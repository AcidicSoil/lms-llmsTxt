{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Clean Dependencies and Normalize Names in pyproject.toml",
        "description": "Remove direct VCS URL dependencies and normalize dependency names to canonical PyPI distribution names to ensure PyPI compliance.",
        "details": "1. Modify `pyproject.toml`. \n2. Identify the `llm-ctx` dependency currently defined as `llm-ctx @ git+...`.\n3. Replace it with a valid PyPI version constraint (e.g., `>=x.y.z` if available on PyPI) or a specific version. If it's not on PyPI, this task blocks until a strategy (vendor or PyPI release of dependency) is decided, but assuming availability or replacement, update it.\n4. Scan for other dependencies and ensure they use canonical names (e.g., change `llms_txt` to `llms-txt` if applicable).\n5. Verify `uv lock` or `pip install` works with the new configuration.",
        "testStrategy": "Run `uv build` to ensure metadata is generated. Run `pip install .` in a fresh environment to verify dependencies resolve without git protocols.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Update pyproject.toml Dependencies",
            "description": "Modify the pyproject.toml file to replace direct Git URLs with standard PyPI version constraints and normalize dependency names.",
            "dependencies": [],
            "details": "Open `pyproject.toml`. Locate the dependency `llm-ctx`. Replace the `git+https://...` syntax with a standard version specifier (e.g., `^0.2.0` or `>=0.2.0`). Additionally, review all other dependencies in the `[project.dependencies]` section. Convert names like `llms_txt` to their canonical form `llms-txt` to avoid ambiguous distribution name warnings during build.",
            "status": "pending",
            "testStrategy": "Manual inspection of pyproject.toml to confirm no URL dependencies remain."
          },
          {
            "id": 2,
            "title": "Validate Dependency Configuration and Build",
            "description": "Verify that the updated dependency configuration allows for a successful build and installation in a clean environment.",
            "dependencies": [
              1
            ],
            "details": "Create a fresh virtual environment. Run `pip install .` (or `uv pip install .`) to confirm that the package installs with the new PyPI-based dependencies. Run a build command (e.g., `python -m build` or `uv build`) and inspect the generated metadata (PKG-INFO) in the source distribution to ensure `Requires-Dist` fields do not contain direct URL references.",
            "status": "pending",
            "testStrategy": "Successful execution of build commands and installation without resolution errors."
          }
        ]
      },
      {
        "id": 2,
        "title": "Configure Setuptools for src Layout Discovery",
        "description": "Explicitly configure package discovery in pyproject.toml to ensure code under `src/` is correctly included in the built wheel.",
        "details": "1. Edit `pyproject.toml`.\n2. Ensure `[tool.setuptools.packages.find]` is present.\n3. Set `where = [\"src\"]`.\n4. Verify `[project.scripts]` points to the correct module path (e.g., `lmstxt = \"lmstudiotxt_generator.cli:main\"`).\n5. Ensure `[build-system]` is correctly defined (usually `setuptools` and `wheel` or `hatchling` etc., stick to existing but ensure config matches layout).",
        "testStrategy": "Run `uv build`. Inspect the generated wheel content (using `unzip -l dist/*.whl`) to verify `lmstudiotxt_generator` directory exists in the root of the wheel.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze Project Structure and Current Configuration",
            "description": "Inspect the file system to verify the 'src' directory existence and read the current 'pyproject.toml' to assess existing build settings.",
            "dependencies": [],
            "details": "Use glob/ls to confirm `src/lmstudiotxt_generator` (or similar) exists. Read `pyproject.toml` to identify the current `[build-system]` and any existing `[tool.setuptools]` configuration to determine the necessary changes.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 2,
            "title": "Configure Build System Backend",
            "description": "Ensure 'pyproject.toml' defines a standard build backend compatible with setuptools and src-layout.",
            "dependencies": [
              1
            ],
            "details": "Edit `pyproject.toml` to verify or add the `[build-system]` table. Ensure `requires` includes `[\"setuptools\", \"wheel\"]` (and `setuptools-scm` if dynamic versioning is used) and `build-backend` is set to `\"setuptools.build_meta\"`.",
            "status": "pending",
            "testStrategy": "Run `uv build` (or `python -m build`) to confirm the build backend initializes correctly."
          },
          {
            "id": 3,
            "title": "Implement Explicit Package Discovery",
            "description": "Add the specific setuptools configuration to restrict package discovery to the 'src' directory.",
            "dependencies": [
              2
            ],
            "details": "In `pyproject.toml`, add or update the `[tool.setuptools.packages.find]` table. Set `where = [\"src\"]` to ensure only the code inside `src/` is packaged, preventing accidental inclusion of root-level scripts or config files.",
            "status": "pending",
            "testStrategy": "None (Verification happens in the final build step)."
          },
          {
            "id": 4,
            "title": "Configure Entry Points",
            "description": "Verify and update the CLI entry point mapping in 'pyproject.toml' to point to the correct location within the src layout.",
            "dependencies": [
              3
            ],
            "details": "Check `[project.scripts]`. Ensure the command (e.g., `lmstxt`) maps to the correct module path, likely `lmstudiotxt_generator.cli:main` or similar. If the package name changed or moved to src, ensure the import path is valid.",
            "status": "pending",
            "testStrategy": "Run `pip install -e .` and check if the `lmstxt` command is available and executable."
          },
          {
            "id": 5,
            "title": "Verify Wheel Structure",
            "description": "Build the distribution package and inspect the contents to confirm the 'src' layout is correctly flattened in the wheel.",
            "dependencies": [
              4
            ],
            "details": "Run `uv build`. Use `unzip -l dist/*.whl` to list the files. Confirm that the top-level directory in the wheel is the package folder (e.g., `lmstudiotxt_generator/`), NOT `src/lmstudiotxt_generator/` and NOT containing extraneous root files.",
            "status": "pending",
            "testStrategy": "Automated script or manual check using unzip/tar to validate file hierarchy inside the artifact."
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Artifact Metadata Validation Script",
        "description": "Create a script or makefile target to validate that built artifacts have correct metadata and are well-formed.",
        "details": "1. Create `scripts/release/validate_metadata.py` or similar.\n2. The script should run `twine check dist/*` (or equivalent `uv` command if available) to ensure README renders correctly and metadata is valid.\n3. Ensure it checks for the absence of direct URL dependencies in the metadata `Requires-Dist` fields.",
        "testStrategy": "Build artifacts with known bad metadata (e.g., direct URL) and ensure script fails. Build clean artifacts and ensure script passes.",
        "priority": "medium",
        "dependencies": [
          2
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Validation Script and Directory Structure",
            "description": "Create the scripts directory and the initial Python script structure for metadata validation.",
            "dependencies": [],
            "details": "Create directory `scripts/release/` if it does not exist. Create `scripts/release/validate_metadata.py`. Set up standard Python boilerplate (argparse to accept dist directory path, logging setup).",
            "status": "pending",
            "testStrategy": "Run `python scripts/release/validate_metadata.py --help` to verify it runs without errors."
          },
          {
            "id": 2,
            "title": "Implement Standard Twine Check Integration",
            "description": "Integrate `twine check` execution into the validation script to verify standard metadata compliance.",
            "dependencies": [
              1
            ],
            "details": "In `validate_metadata.py`, use `subprocess` to run `twine check` on the target artifacts. Ensure the script captures stdout/stderr and propagates the exit code if twine detects invalid reStructuredText or missing fields.",
            "status": "pending",
            "testStrategy": "Build a valid artifact using `uv build` and run the script; ensure it passes the twine check step."
          },
          {
            "id": 3,
            "title": "Implement Metadata Extraction Logic",
            "description": "Add functionality to extract metadata files from Wheel and Source distributions.",
            "dependencies": [
              1
            ],
            "details": "Using `zipfile` for `.whl` and `tarfile` for `.tar.gz`, implement functions to locate and read the `METADATA` (wheel) or `PKG-INFO` (sdist) files from the artifacts in the dist directory.",
            "status": "pending",
            "testStrategy": "Unit test the extraction function with a sample built artifact to ensure it returns the raw metadata string."
          },
          {
            "id": 4,
            "title": "Implement Direct URL Dependency Validation",
            "description": "Parse metadata to detect and reject direct URL dependencies in `Requires-Dist`.",
            "dependencies": [
              3
            ],
            "details": "Parse the extracted metadata headers. Iterate through `Requires-Dist` entries. Implement a regex or parsing check (e.g. using `packaging.requirements`) to identify if any dependency specifies a direct URL (e.g., `@ http://...` or `git+https://...`). Raise an error if found.",
            "status": "pending",
            "testStrategy": "Create a dummy wheel with a direct URL dependency, run the script, and assert that it fails with a specific error message."
          },
          {
            "id": 5,
            "title": "Finalize Script Execution Flow and CI Integration",
            "description": "Combine checks into a main execution loop and handle overall exit codes.",
            "dependencies": [
              2,
              4
            ],
            "details": "Orchestrate the flow: 1. Find all files in `dist/`. 2. Run Twine check. 3. Run Metadata/URL check. 4. Aggregate results. If any check fails, print clear errors and exit with non-zero status code. Update `pyproject.toml` or `Makefile` (if exists) to include a target for this script.",
            "status": "pending",
            "testStrategy": "Run the full script against a clean build (`uv build`) to confirm zero exit code, then against a corrupted build to confirm non-zero exit code."
          }
        ]
      },
      {
        "id": 4,
        "title": "Create Smoke Test Suite for Artifacts",
        "description": "Develop a smoke test script that installs built artifacts (wheel/sdist) and verifies the CLI runs.",
        "details": "1. Create `tests/smoke_test.py`.\n2. Implement `test_wheel_install()`: Creates a venv, installs the `.whl` from `dist/`, and runs `lmstxt --help`.\n3. Implement `test_sdist_install()`: Creates a venv, installs the `.tar.gz` from `dist/`, and runs `lmstxt --help`.\n4. Use `subprocess` to manage venv creation and command execution to ensure total isolation.",
        "testStrategy": "Run `pytest tests/smoke_test.py` (or execute directly `python tests/smoke_test.py`) after running `uv build`. It should pass only if artifacts are valid.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize smoke test script with venv management helpers",
            "description": "Set up the test file and utilities for isolating installations in temporary virtual environments.",
            "dependencies": [],
            "details": "Create `tests/smoke_test.py`. Implement helper functions using `tempfile`, `venv`, and `subprocess` to generate throwaway virtual environments. Create a utility function `run_in_venv(venv_path, command)` to execute shell commands (like `pip install` or the CLI executable) specifically within the context of the created environment.",
            "status": "pending",
            "testStrategy": "Execute the script to verify the helper function successfully creates a directory containing a valid Python executable."
          },
          {
            "id": 2,
            "title": "Implement artifact installation tests for Wheel and Sdist",
            "description": "Write the specific test cases that install the build artifacts and verify the CLI entry point runs correctly.",
            "dependencies": [
              1
            ],
            "details": "In `tests/smoke_test.py`, implement `test_wheel_install()` and `test_sdist_install()`. The logic must: 1. Locate the `.whl` and `.tar.gz` files in `dist/`. 2. Invoke the venv helper. 3. Install the artifact into the venv. 4. Execute `lmstxt --help` and assert exit code 0. Add a `if __name__ == '__main__':` block to run these tests.",
            "status": "pending",
            "testStrategy": "Run `uv build` to generate artifacts, then execute `python tests/smoke_test.py`. Expect success output for both installation types."
          }
        ]
      },
      {
        "id": 5,
        "title": "Update TestPyPI Publishing Workflow",
        "description": "Refine the GitHub Action for TestPyPI to use `uv`, proper secrets, and allow manual dispatch.",
        "details": "1. Edit `.github/workflows/publish-testpypi.yml`.\n2. Ensure `on: workflow_dispatch` with input `release_tag` (or ref).\n3. Use `uv build` to generate artifacts.\n4. Use `uv publish` (or `twine`) targeting `https://test.pypi.org/legacy/`.\n5. Map `UV_PUBLISH_TOKEN` or `TWINE_PASSWORD` to the repo secret `TEST_PYPI_TOKEN`.\n6. Ensure it runs the smoke test (Task 4) before publishing.",
        "testStrategy": "Trigger the workflow manually on a branch. Verify artifacts appear on TestPyPI.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze and Prepare Workflow Structure",
            "description": "Examine the existing GitHub Actions workflows to determine the correct structure for the TestPyPI publishing workflow.",
            "dependencies": [],
            "details": "Check for existing workflows in `.github/workflows/` like `publish-testpypi.yml` or `ci.yml`. Create or update `.github/workflows/publish-testpypi.yml` to define the basic structure, ensuring permissions are set correctly (id-token: write for future OIDC or just contents: read) and define the `workflow_dispatch` trigger with a `release_tag` input option.",
            "status": "pending",
            "testStrategy": "Validate YAML syntax using a linter."
          },
          {
            "id": 2,
            "title": "Implement UV Setup and Build Steps",
            "description": "Configure the workflow to set up Python and install uv, then build the package distribution artifacts.",
            "dependencies": [
              1
            ],
            "details": "In the `publish-testpypi.yml` workflow, add steps to: 1. Checkout code (using `actions/checkout@v4`). 2. Set up Python (using `actions/setup-python@v5`). 3. Install `uv` (e.g., via `pip install uv` or the official action if available/preferred). 4. Run `uv build` to generate sdist and wheel files in the `dist/` directory.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 3,
            "title": "Integrate Smoke Tests into Workflow",
            "description": "Add a step to run the project's smoke tests before publishing to ensure artifact integrity.",
            "dependencies": [
              2
            ],
            "details": "Add a step after the build process but before publishing. This step should create a fresh virtual environment using `uv venv`, install the built wheel from `dist/` (e.g., `uv pip install dist/*.whl`), and run the basic CLI command or import test defined in Task 4 (e.g., `lmstxt --help` or a specific script).",
            "status": "pending",
            "testStrategy": "Workflow fails if the smoke test command returns a non-zero exit code."
          },
          {
            "id": 4,
            "title": "Configure UV Publish to TestPyPI",
            "description": "Add the publishing step using `uv publish` targeting the TestPyPI repository with proper authentication.",
            "dependencies": [
              3
            ],
            "details": "Add the publish step: `uv publish --publish-url https://test.pypi.org/legacy/ dist/*`. Configure authentication by mapping the `UV_PUBLISH_TOKEN` environment variable to the `${{ secrets.TEST_PYPI_TOKEN }}` GitHub secret. Ensure this step only runs if the smoke test passes.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 5,
            "title": "Final Review and Workflow Validation",
            "description": "Verify the complete workflow configuration and document the dispatch parameters.",
            "dependencies": [
              4
            ],
            "details": "Review the final `.github/workflows/publish-testpypi.yml` file to ensure all indentation is correct, secrets are referenced properly, and the `workflow_dispatch` inputs are correctly defined. Verify that the `release_tag` input is used to checkout the specific ref if provided, or default to the current branch.",
            "status": "pending",
            "testStrategy": "Manually trigger the workflow from the GitHub Actions UI on a feature branch to verify it passes up to the publish step (which might fail if secrets aren't set in the fork, but the logic can be verified)."
          }
        ]
      },
      {
        "id": 6,
        "title": "Configure PyPI Trusted Publishing Workflow",
        "description": "Update the release workflow to use OIDC for Trusted Publishing to PyPI on tag push.",
        "details": "1. Edit `.github/workflows/release.yml`.\n2. Set permission `id-token: write`.\n3. Configure environment `name: pypi` and `url: https://pypi.org/p/lmstudio-llmstxt-generator`.\n4. Update the publish step to use `uv publish --trusted-publishing` (or `gh-action-pypi-publish`).\n5. Ensure it triggers on `push: tags: [ 'v*' ]`.\n6. Integrate the smoke test step before the publish job.",
        "testStrategy": "Review workflow syntax using a linter. Actual verification requires pushing a tag (or testing in a fork with TestPyPI configured as the trusted target).",
        "priority": "high",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze and Prepare Release Workflow Structure",
            "description": "Examine the existing .github/workflows directory and prepare the structure for release.yml with required OIDC permissions.",
            "dependencies": [],
            "details": "Check for existing release.yml. If present, review current steps. If absent, create a new file. Define the trigger on 'push' for tags matching 'v*'. Crucially, add the top-level 'permissions' block setting 'id-token: write' to enable OIDC authentication for Trusted Publishing.",
            "status": "pending",
            "testStrategy": "Review the workflow file syntax to ensure permissions are correctly scoped."
          },
          {
            "id": 2,
            "title": "Configure Build Job with UV",
            "description": "Define the build job in release.yml to checkout code, set up Python, and build artifacts using uv.",
            "dependencies": [
              1
            ],
            "details": "In release.yml, add a 'build' job. Steps: 1. Checkout code (v4). 2. Install uv. 3. Setup Python (v5). 4. Run 'uv build'. 5. Upload artifacts (dist/*) using actions/upload-artifact@v4 so they can be used by subsequent jobs (smoke test and publish).",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 3,
            "title": "Integrate Smoke Test Job",
            "description": "Add a smoke test job that runs before publishing to verify the build artifacts work as expected.",
            "dependencies": [
              2
            ],
            "details": "Create a 'smoke-test' job in release.yml that 'needs: build'. Steps: 1. Download artifacts from the build job. 2. Install uv/Python. 3. Run the smoke test suite (e.g., 'python tests/smoke_test.py' or 'pytest tests/smoke_test.py') against the downloaded wheel to ensure viability before publishing.",
            "status": "pending",
            "testStrategy": "Ensure the job fails if the smoke test script exits with a non-zero code."
          },
          {
            "id": 4,
            "title": "Configure PyPI Publish Job with Trusted Publishing",
            "description": "Define the publish job using OIDC authentication targeting the PyPI environment.",
            "dependencies": [
              3
            ],
            "details": "Add a 'publish' job that 'needs: smoke-test'. Define 'environment' with name 'pypi' and url 'https://pypi.org/p/lmstudio-llmstxt-generator'. Use 'pypa/gh-action-pypi-publish@release/v1' (recommended for Trusted Publishing) or 'uv publish --trusted-publishing' if supported. Do NOT use username/password secrets.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 5,
            "title": "Finalize and Validate Workflow Configuration",
            "description": "Combine all jobs into the final workflow file and perform static validation.",
            "dependencies": [
              4
            ],
            "details": "Ensure all jobs (build, smoke-test, publish) are correctly linked with 'needs'. Verify the environment URL matches the specific PyPI project. Commit the .github/workflows/release.yml file. Check that 'contents: read' permission is also present alongside 'id-token: write'.",
            "status": "pending",
            "testStrategy": "Run a linter (e.g., action-validator) on the YAML file locally if possible."
          }
        ]
      },
      {
        "id": 7,
        "title": "Consolidate Release Runbook",
        "description": "Create a comprehensive document describing the release process for maintainers.",
        "details": "1. Create `docs/publishing.md`.\n2. Document prerequisites (PyPI account, Trusted Publisher setup logic).\n3. Document the 'One Command' release flow (tagging).\n4. Document the TestPyPI manual trigger flow.\n5. Include troubleshooting steps for common errors (e.g., metadata rejected).\n6. Link to the CI workflows.",
        "testStrategy": "Peer review the markdown file. Verify links work.",
        "priority": "medium",
        "dependencies": [
          5,
          6
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Establish Documentation Structure and Prerequisites",
            "description": "Create the `docs/publishing.md` file and document the initial setup requirements for maintainers.",
            "dependencies": [],
            "details": "Create a new markdown file at `docs/publishing.md`. Add a 'Prerequisites' section detailing the need for a PyPI account and the specific configuration required for Trusted Publisher setup (OIDC) between GitHub and PyPI. Explain how maintainers should verify their permissions before attempting a release.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 2,
            "title": "Document the Production Release Workflow",
            "description": "Detail the standard 'One Command' release process triggered by git tags.",
            "dependencies": [
              1
            ],
            "details": "In `docs/publishing.md`, describe the production release flow. Explain the versioning convention (Semantic Versioning), how to create a git tag (e.g., `git tag v1.0.0`), and how pushing this tag triggers the GitHub Action defined in `.github/workflows/release.yml`. Include a step-by-step guide for the maintainer to execute this flow.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 3,
            "title": "Document the TestPyPI Workflow",
            "description": "Explain the manual trigger process for deploying to TestPyPI for validation.",
            "dependencies": [
              1
            ],
            "details": "Add a section to `docs/publishing.md` covering the TestPyPI release process. Reference the manual dispatch trigger configured in the CI workflow. Provide instructions on how to use the GitHub UI 'Run workflow' button, input necessary parameters (if any), and verify the artifact on `test.pypi.org`.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 4,
            "title": "Compile Troubleshooting Guide",
            "description": "Create a section for resolving common release failures and errors.",
            "dependencies": [
              2,
              3
            ],
            "details": "Add a 'Troubleshooting' section to the documentation. Address common issues such as 'Metadata Rejected' (often due to version conflicts or invalid `pyproject.toml` configuration), OIDC token failures, and smoke test failures. Provide resolution steps for each scenario, including how to clean up failed tags if necessary.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 5,
            "title": "Finalize References and CI Links",
            "description": "Add direct links to workflows and review the document for completeness.",
            "dependencies": [
              4
            ],
            "details": "Conclude the document by adding direct links to the repository's 'Actions' tab and the specific workflow files (`release.yml`, etc.). Perform a self-review to ensure all paths, commands, and URLs are accurate relative to the project root. Ensure the document is linked from the main `README.md` or `CONTRIBUTING.md` if they exist.",
            "status": "pending",
            "testStrategy": "Peer review the markdown file. Verify all hyperlinks are functional."
          }
        ]
      },
      {
        "id": 8,
        "title": "Add Pre-commit/Local Build Check Script",
        "description": "Create a helper script for developers to run the full build-verify-smoke cycle locally.",
        "details": "1. Create `scripts/release/verify_install.sh` (or `local_check.sh`).\n2. The script should: Clean `dist/`, run `uv build`, run `validate_metadata.py`, and run `smoke_test.py`.\n3. Make it executable.",
        "testStrategy": "Run `./scripts/release/verify_install.sh` locally and ensure it passes when the repo is clean.",
        "priority": "low",
        "dependencies": [
          3,
          4
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Verify Install Script Skeleton",
            "description": "Initialize the verify_install.sh bash script with strict error handling and basic environment checks.",
            "dependencies": [],
            "details": "Create the directory `scripts/release` if it does not exist. Create the file `scripts/release/verify_install.sh` with the shebang `#!/bin/bash` and `set -euo pipefail` to ensure the script exits immediately on error. Add a check to verify that the `uv` command is installed and available in the system PATH.",
            "status": "pending",
            "testStrategy": "Run the script; verify it exists, errors if `uv` is missing, or exits cleanly if nothing else is added yet."
          },
          {
            "id": 2,
            "title": "Implement Clean and Build Logic",
            "description": "Add functionality to clean old artifacts and rebuild the project using uv.",
            "dependencies": [
              1
            ],
            "details": "Update the script to run `rm -rf dist/` to clean previous builds. Add the `uv build` command. Check that the `dist/` directory is created and contains artifacts (e.g., `.whl`, `.tar.gz`) after the build command runs.",
            "status": "pending",
            "testStrategy": "Run the script. Check that `dist/` is cleared and then repopulated with build artifacts."
          },
          {
            "id": 3,
            "title": "Integrate Metadata Validation Step",
            "description": "Invoke the metadata validation Python script to ensure build artifacts are well-formed.",
            "dependencies": [
              2
            ],
            "details": "Add the command to execute `python scripts/release/validate_metadata.py`. Ensure the script relies on the exit code of this Python script to determine pass/fail status. This ensures no artifacts with invalid metadata (e.g., direct URL dependencies) pass the check.",
            "status": "pending",
            "testStrategy": "Temporarily modify `pyproject.toml` to be invalid, run the script, and ensure it fails at this step."
          },
          {
            "id": 4,
            "title": "Integrate Smoke Test Step",
            "description": "Invoke the smoke test script to verify the basic functionality of the built artifacts.",
            "dependencies": [
              3
            ],
            "details": "Add the command to execute `python scripts/release/smoke_test.py`. This step should occur after metadata validation. It ensures that the package built in the previous steps can be installed and imported successfully.",
            "status": "pending",
            "testStrategy": "Run the script and verify that the smoke test python script is executed effectively."
          },
          {
            "id": 5,
            "title": "Finalize Script Permissions and User Experience",
            "description": "Make the script executable and add user-friendly logging for build phases.",
            "dependencies": [
              4
            ],
            "details": "Run `chmod +x scripts/release/verify_install.sh`. Add `echo` statements with ANSI color codes (e.g., Green for success, Red for failure) to clearly demarcate the Build, Validate, and Smoke Test phases. Add a final success message if all steps pass.",
            "status": "pending",
            "testStrategy": "Run `./scripts/release/verify_install.sh` directly from the terminal and verify the output contains formatted status messages."
          }
        ]
      },
      {
        "id": 9,
        "title": "Verify Dynamic Versioning or Version Bump Policy",
        "description": "Ensure the package version aligns with the Git tag automatically or document the bump process.",
        "details": "1. Check `pyproject.toml` for `dynamic = [\"version\"]` or static version.\n2. If static, update the Runbook (Task 7) to include 'Bump version in pyproject.toml' as a step before tagging.\n3. If dynamic versioning is preferred, configure `setuptools_scm` (or equivalent) in `pyproject.toml` and add it to build-system requirements.",
        "testStrategy": "Build a wheel and check the filename version. Ensure it matches the expected tag/commit version.",
        "priority": "medium",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze pyproject.toml for Current Versioning Configuration",
            "description": "Inspect the pyproject.toml file to determine if the version is statically defined or configured for dynamic discovery.",
            "dependencies": [],
            "details": "Read `pyproject.toml` to check the `[project]` table for a `version` key or `dynamic = [\"version\"]`. Check `[build-system]` and `[tool.setuptools]` (or other backend configs) to understand the current setup.",
            "status": "pending",
            "testStrategy": "None (Analysis task)"
          },
          {
            "id": 2,
            "title": "Implement Dynamic Versioning with setuptools_scm",
            "description": "Configure the project to derive its version automatically from Git tags using setuptools_scm.",
            "dependencies": [
              1
            ],
            "details": "Modify `pyproject.toml` to: 1. Remove static `version`. 2. Add `dynamic = [\"version\"]`. 3. Add `setuptools_scm` to `[build-system] requires`. 4. Add `[tool.setuptools_scm]` section (can be empty or with `write_to` config).",
            "status": "pending",
            "testStrategy": "Run `uv build` (or pip install) locally and verify the generated metadata/wheel version matches `git describe --tags`."
          },
          {
            "id": 3,
            "title": "Ensure __version__ Attribute Availability",
            "description": "Ensure the package version is accessible at runtime via `__version__` without hardcoding it.",
            "dependencies": [
              2
            ],
            "details": "Modify `src/lmstudiotxt_generator/__init__.py` (or equivalent top-level init) to use `importlib.metadata.version` to retrieve the installed package version dynamically, handling the `PackageNotFoundError` gracefully.",
            "status": "pending",
            "testStrategy": "Run a python script importing the package and printing `__version__`. It should match the installed version."
          },
          {
            "id": 4,
            "title": "Update Release Documentation / Runbook",
            "description": "Update the project documentation to reflect the new dynamic versioning workflow.",
            "dependencies": [
              2
            ],
            "details": "Create or update a `RELEASE.md` or the Runbook (from Task 7 context) to explain that tagging a commit triggers the version bump, and manual editing of `pyproject.toml` version is no longer needed.",
            "status": "pending",
            "testStrategy": "Review the documentation for clarity and accuracy."
          },
          {
            "id": 5,
            "title": "Verify Wheel Versioning with Smoke Test",
            "description": "Validate that the build process produces artifacts with the correct version derived from the git context.",
            "dependencies": [
              2
            ],
            "details": "Create a temporary git tag (e.g., `v0.0.1-test`), run `uv build`, and inspect the filename of the generated `.whl` in `dist/`. Ensure it matches the tag. Clean up the tag afterwards.",
            "status": "pending",
            "testStrategy": "Manual verification or script checking `dist/` filenames against `git describe` output."
          }
        ]
      },
      {
        "id": 10,
        "title": "End-to-End Release Simulation",
        "description": "Perform a final validation of the entire pipeline using TestPyPI.",
        "details": "1. Bump version to a dev version (e.g., `0.0.1.dev1`).\n2. Trigger the TestPyPI workflow.\n3. Verify upload.\n4. locally run `pip install -i https://test.pypi.org/simple/ lmstudio-llmstxt-generator`.\n5. Verify `lmstxt --help` works.",
        "testStrategy": "Successful installation and execution from TestPyPI.",
        "priority": "medium",
        "dependencies": [
          5,
          7
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Update Version to Development Release Candidate",
            "description": "Bump the package version in pyproject.toml to a unique development version (e.g., 0.0.1.dev1) to allow a non-conflicting upload to TestPyPI.",
            "dependencies": [],
            "details": "Modify the `version` field in `pyproject.toml`. Append a `.devN` suffix to the current version. This ensures that we can verify the upload mechanism without consuming a 'real' semantic version number on TestPyPI, as PyPI files are immutable.",
            "status": "pending",
            "testStrategy": "Check pyproject.toml content and verify `uv build` produces a file with the new version string."
          },
          {
            "id": 2,
            "title": "Trigger TestPyPI Deployment Workflow",
            "description": "Manually trigger the GitHub Action or CI pipeline configured for TestPyPI deployment using the new development version.",
            "dependencies": [
              1
            ],
            "details": "Navigate to the CI/CD dashboard (e.g., GitHub Actions). Locate the 'Publish to TestPyPI' workflow (created in previous tasks). Trigger it manually or by pushing a specific tag (e.g., `v0.0.1.dev1`) depending on the trigger logic defined in the workflow file.",
            "status": "pending",
            "testStrategy": "Monitor the CI job logs for successful completion, specifically looking for the 'Twine upload' or equivalent step success message."
          },
          {
            "id": 3,
            "title": "Verify TestPyPI Upload Integrity",
            "description": "Confirm that the package artifacts have effectively appeared on the TestPyPI repository page with the correct metadata.",
            "dependencies": [
              2
            ],
            "details": "Visit the project URL on TestPyPI (e.g., `https://test.pypi.org/project/lmstudio-llmstxt-generator/`). Verify that the new version `0.0.1.dev1` is listed, the Release Date is current, and the `Description` (README) renders correctly.",
            "status": "pending",
            "testStrategy": "Manual verification on the TestPyPI website."
          },
          {
            "id": 4,
            "title": "Perform Clean Installation from TestPyPI",
            "description": "Create a fresh virtual environment and install the package exclusively from the TestPyPI index to simulate an end-user installation.",
            "dependencies": [
              3
            ],
            "details": "Create a new virtualenv: `python -m venv venv_test`. Activate it. Run `pip install --no-cache-dir --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ lmstudio-llmstxt-generator==0.0.1.dev1`. Note: `--extra-index-url` is often needed for dependencies that don't exist on TestPyPI.",
            "status": "pending",
            "testStrategy": "Ensure pip completes successfully without errors regarding missing dependencies or hash mismatches."
          },
          {
            "id": 5,
            "title": "Validate Installed CLI Functionality",
            "description": "Execute the installed CLI tool within the test environment to confirm the package structure and entry points function correctly.",
            "dependencies": [
              4
            ],
            "details": "Inside the `venv_test`, run the command `lmstxt --help`. Verify that the help text is displayed, indicating that the entry point scripts were correctly generated and the internal imports are working.",
            "status": "pending",
            "testStrategy": "The command should exit with code 0 and display the standard help message, not an ImportError or ModuleNotFoundError."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-12-29T22:13:30.525Z",
      "updated": "2025-12-29T22:13:30.525Z",
      "description": "Tasks for master context"
    }
  },
  "chore-release": {
    "tasks": [
      {
        "id": 1,
        "title": "Clean Dependencies and Normalize Names in pyproject.toml",
        "description": "Remove direct VCS URL dependencies and normalize dependency names to canonical PyPI distribution names to ensure PyPI compliance.",
        "details": "1. Modify `pyproject.toml`. \n2. Identify the `llm-ctx` dependency currently defined as `llm-ctx @ git+...`.\n3. Replace it with a valid PyPI version constraint (e.g., `>=x.y.z` if available on PyPI) or a specific version. If it's not on PyPI, this task blocks until a strategy (vendor or PyPI release of dependency) is decided, but assuming availability or replacement, update it.\n4. Scan for other dependencies and ensure they use canonical names (e.g., change `llms_txt` to `llms-txt` if applicable).\n5. Verify `uv lock` or `pip install` works with the new configuration.",
        "testStrategy": "Run `uv build` to ensure metadata is generated. Run `pip install .` in a fresh environment to verify dependencies resolve without git protocols.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Update pyproject.toml Dependencies",
            "description": "Modify the pyproject.toml file to replace direct Git URLs with standard PyPI version constraints and normalize dependency names.",
            "dependencies": [],
            "details": "Open `pyproject.toml`. Locate the dependency `llm-ctx`. Replace the `git+https://...` syntax with a standard version specifier (e.g., `^0.2.0` or `>=0.2.0`). Additionally, review all other dependencies in the `[project.dependencies]` section. Convert names like `llms_txt` to their canonical form `llms-txt` to avoid ambiguous distribution name warnings during build.",
            "status": "pending",
            "testStrategy": "Manual inspection of pyproject.toml to confirm no URL dependencies remain.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Validate Dependency Configuration and Build",
            "description": "Verify that the updated dependency configuration allows for a successful build and installation in a clean environment.",
            "dependencies": [
              1
            ],
            "details": "Create a fresh virtual environment. Run `pip install .` (or `uv pip install .`) to confirm that the package installs with the new PyPI-based dependencies. Run a build command (e.g., `python -m build` or `uv build`) and inspect the generated metadata (PKG-INFO) in the source distribution to ensure `Requires-Dist` fields do not contain direct URL references.",
            "status": "pending",
            "testStrategy": "Successful execution of build commands and installation without resolution errors.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-30T00:12:38.308Z"
      },
      {
        "id": 2,
        "title": "Configure Setuptools for src Layout Discovery",
        "description": "Explicitly configure package discovery in pyproject.toml to ensure code under `src/` is correctly included in the built wheel.",
        "details": "1. Edit `pyproject.toml`.\n2. Ensure `[tool.setuptools.packages.find]` is present.\n3. Set `where = [\"src\"]`.\n4. Verify `[project.scripts]` points to the correct module path (e.g., `lmstxt = \"lmstudiotxt_generator.cli:main\"`).\n5. Ensure `[build-system]` is correctly defined (usually `setuptools` and `wheel` or `hatchling` etc., stick to existing but ensure config matches layout).",
        "testStrategy": "Run `uv build`. Inspect the generated wheel content (using `unzip -l dist/*.whl`) to verify `lmstudiotxt_generator` directory exists in the root of the wheel.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze Project Structure and Current Configuration",
            "description": "Inspect the file system to verify the 'src' directory existence and read the current 'pyproject.toml' to assess existing build settings.",
            "dependencies": [],
            "details": "Use glob/ls to confirm `src/lmstudiotxt_generator` (or similar) exists. Read `pyproject.toml` to identify the current `[build-system]` and any existing `[tool.setuptools]` configuration to determine the necessary changes.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Configure Build System Backend",
            "description": "Ensure 'pyproject.toml' defines a standard build backend compatible with setuptools and src-layout.",
            "dependencies": [
              1
            ],
            "details": "Edit `pyproject.toml` to verify or add the `[build-system]` table. Ensure `requires` includes `[\"setuptools\", \"wheel\"]` (and `setuptools-scm` if dynamic versioning is used) and `build-backend` is set to `\"setuptools.build_meta\"`.",
            "status": "pending",
            "testStrategy": "Run `uv build` (or `python -m build`) to confirm the build backend initializes correctly.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Explicit Package Discovery",
            "description": "Add the specific setuptools configuration to restrict package discovery to the 'src' directory.",
            "dependencies": [
              2
            ],
            "details": "In `pyproject.toml`, add or update the `[tool.setuptools.packages.find]` table. Set `where = [\"src\"]` to ensure only the code inside `src/` is packaged, preventing accidental inclusion of root-level scripts or config files.",
            "status": "pending",
            "testStrategy": "None (Verification happens in the final build step).",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Configure Entry Points",
            "description": "Verify and update the CLI entry point mapping in 'pyproject.toml' to point to the correct location within the src layout.",
            "dependencies": [
              3
            ],
            "details": "Check `[project.scripts]`. Ensure the command (e.g., `lmstxt`) maps to the correct module path, likely `lmstudiotxt_generator.cli:main` or similar. If the package name changed or moved to src, ensure the import path is valid.",
            "status": "pending",
            "testStrategy": "Run `pip install -e .` and check if the `lmstxt` command is available and executable.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Verify Wheel Structure",
            "description": "Build the distribution package and inspect the contents to confirm the 'src' layout is correctly flattened in the wheel.",
            "dependencies": [
              4
            ],
            "details": "Run `uv build`. Use `unzip -l dist/*.whl` to list the files. Confirm that the top-level directory in the wheel is the package folder (e.g., `lmstudiotxt_generator/`), NOT `src/lmstudiotxt_generator/` and NOT containing extraneous root files.",
            "status": "pending",
            "testStrategy": "Automated script or manual check using unzip/tar to validate file hierarchy inside the artifact.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-30T00:13:30.433Z"
      },
      {
        "id": 3,
        "title": "Implement Artifact Metadata Validation Script",
        "description": "Create a script or makefile target to validate that built artifacts have correct metadata and are well-formed.",
        "details": "1. Create `scripts/release/validate_metadata.py` or similar.\n2. The script should run `twine check dist/*` (or equivalent `uv` command if available) to ensure README renders correctly and metadata is valid.\n3. Ensure it checks for the absence of direct URL dependencies in the metadata `Requires-Dist` fields.",
        "testStrategy": "Build artifacts with known bad metadata (e.g., direct URL) and ensure script fails. Build clean artifacts and ensure script passes.",
        "priority": "medium",
        "dependencies": [
          "2"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Validation Script and Directory Structure",
            "description": "Create the scripts directory and the initial Python script structure for metadata validation.",
            "dependencies": [],
            "details": "Create directory `scripts/release/` if it does not exist. Create `scripts/release/validate_metadata.py`. Set up standard Python boilerplate (argparse to accept dist directory path, logging setup).",
            "status": "pending",
            "testStrategy": "Run `python scripts/release/validate_metadata.py --help` to verify it runs without errors.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Standard Twine Check Integration",
            "description": "Integrate `twine check` execution into the validation script to verify standard metadata compliance.",
            "dependencies": [
              1
            ],
            "details": "In `validate_metadata.py`, use `subprocess` to run `twine check` on the target artifacts. Ensure the script captures stdout/stderr and propagates the exit code if twine detects invalid reStructuredText or missing fields.",
            "status": "pending",
            "testStrategy": "Build a valid artifact using `uv build` and run the script; ensure it passes the twine check step.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Metadata Extraction Logic",
            "description": "Add functionality to extract metadata files from Wheel and Source distributions.",
            "dependencies": [
              1
            ],
            "details": "Using `zipfile` for `.whl` and `tarfile` for `.tar.gz`, implement functions to locate and read the `METADATA` (wheel) or `PKG-INFO` (sdist) files from the artifacts in the dist directory.",
            "status": "pending",
            "testStrategy": "Unit test the extraction function with a sample built artifact to ensure it returns the raw metadata string.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement Direct URL Dependency Validation",
            "description": "Parse metadata to detect and reject direct URL dependencies in `Requires-Dist`.",
            "dependencies": [
              3
            ],
            "details": "Parse the extracted metadata headers. Iterate through `Requires-Dist` entries. Implement a regex or parsing check (e.g. using `packaging.requirements`) to identify if any dependency specifies a direct URL (e.g., `@ http://...` or `git+https://...`). Raise an error if found.",
            "status": "pending",
            "testStrategy": "Create a dummy wheel with a direct URL dependency, run the script, and assert that it fails with a specific error message.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Finalize Script Execution Flow and CI Integration",
            "description": "Combine checks into a main execution loop and handle overall exit codes.",
            "dependencies": [
              2,
              4
            ],
            "details": "Orchestrate the flow: 1. Find all files in `dist/`. 2. Run Twine check. 3. Run Metadata/URL check. 4. Aggregate results. If any check fails, print clear errors and exit with non-zero status code. Update `pyproject.toml` or `Makefile` (if exists) to include a target for this script.",
            "status": "pending",
            "testStrategy": "Run the full script against a clean build (`uv build`) to confirm zero exit code, then against a corrupted build to confirm non-zero exit code.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-30T00:14:42.421Z"
      },
      {
        "id": 4,
        "title": "Create Smoke Test Suite for Artifacts",
        "description": "Develop a smoke test script that installs built artifacts (wheel/sdist) and verifies the CLI runs.",
        "details": "1. Create `tests/smoke_test.py`.\n2. Implement `test_wheel_install()`: Creates a venv, installs the `.whl` from `dist/`, and runs `lmstxt --help`.\n3. Implement `test_sdist_install()`: Creates a venv, installs the `.tar.gz` from `dist/`, and runs `lmstxt --help`.\n4. Use `subprocess` to manage venv creation and command execution to ensure total isolation.",
        "testStrategy": "Run `pytest tests/smoke_test.py` (or execute directly `python tests/smoke_test.py`) after running `uv build`. It should pass only if artifacts are valid.",
        "priority": "high",
        "dependencies": [
          "2"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize smoke test script with venv management helpers",
            "description": "Set up the test file and utilities for isolating installations in temporary virtual environments.",
            "dependencies": [],
            "details": "Create `tests/smoke_test.py`. Implement helper functions using `tempfile`, `venv`, and `subprocess` to generate throwaway virtual environments. Create a utility function `run_in_venv(venv_path, command)` to execute shell commands (like `pip install` or the CLI executable) specifically within the context of the created environment.",
            "status": "pending",
            "testStrategy": "Execute the script to verify the helper function successfully creates a directory containing a valid Python executable.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement artifact installation tests for Wheel and Sdist",
            "description": "Write the specific test cases that install the build artifacts and verify the CLI entry point runs correctly.",
            "dependencies": [
              1
            ],
            "details": "In `tests/smoke_test.py`, implement `test_wheel_install()` and `test_sdist_install()`. The logic must: 1. Locate the `.whl` and `.tar.gz` files in `dist/`. 2. Invoke the venv helper. 3. Install the artifact into the venv. 4. Execute `lmstxt --help` and assert exit code 0. Add a `if __name__ == '__main__':` block to run these tests.",
            "status": "pending",
            "testStrategy": "Run `uv build` to generate artifacts, then execute `python tests/smoke_test.py`. Expect success output for both installation types.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-30T00:18:03.814Z"
      },
      {
        "id": 5,
        "title": "Update TestPyPI Publishing Workflow",
        "description": "Refine the GitHub Action for TestPyPI to use `uv`, proper secrets, and allow manual dispatch.",
        "details": "1. Edit `.github/workflows/publish-testpypi.yml`.\n2. Ensure `on: workflow_dispatch` with input `release_tag` (or ref).\n3. Use `uv build` to generate artifacts.\n4. Use `uv publish` (or `twine`) targeting `https://test.pypi.org/legacy/`.\n5. Map `UV_PUBLISH_TOKEN` or `TWINE_PASSWORD` to the repo secret `TEST_PYPI_TOKEN`.\n6. Ensure it runs the smoke test (Task 4) before publishing.",
        "testStrategy": "Trigger the workflow manually on a branch. Verify artifacts appear on TestPyPI.",
        "priority": "medium",
        "dependencies": [
          "4"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze and Prepare Workflow Structure",
            "description": "Examine the existing GitHub Actions workflows to determine the correct structure for the TestPyPI publishing workflow.",
            "dependencies": [],
            "details": "Check for existing workflows in `.github/workflows/` like `publish-testpypi.yml` or `ci.yml`. Create or update `.github/workflows/publish-testpypi.yml` to define the basic structure, ensuring permissions are set correctly (id-token: write for future OIDC or just contents: read) and define the `workflow_dispatch` trigger with a `release_tag` input option.",
            "status": "pending",
            "testStrategy": "Validate YAML syntax using a linter.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement UV Setup and Build Steps",
            "description": "Configure the workflow to set up Python and install uv, then build the package distribution artifacts.",
            "dependencies": [
              1
            ],
            "details": "In the `publish-testpypi.yml` workflow, add steps to: 1. Checkout code (using `actions/checkout@v4`). 2. Set up Python (using `actions/setup-python@v5`). 3. Install `uv` (e.g., via `pip install uv` or the official action if available/preferred). 4. Run `uv build` to generate sdist and wheel files in the `dist/` directory.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate Smoke Tests into Workflow",
            "description": "Add a step to run the project's smoke tests before publishing to ensure artifact integrity.",
            "dependencies": [
              2
            ],
            "details": "Add a step after the build process but before publishing. This step should create a fresh virtual environment using `uv venv`, install the built wheel from `dist/` (e.g., `uv pip install dist/*.whl`), and run the basic CLI command or import test defined in Task 4 (e.g., `lmstxt --help` or a specific script).",
            "status": "pending",
            "testStrategy": "Workflow fails if the smoke test command returns a non-zero exit code.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Configure UV Publish to TestPyPI",
            "description": "Add the publishing step using `uv publish` targeting the TestPyPI repository with proper authentication.",
            "dependencies": [
              3
            ],
            "details": "Add the publish step: `uv publish --publish-url https://test.pypi.org/legacy/ dist/*`. Configure authentication by mapping the `UV_PUBLISH_TOKEN` environment variable to the `${{ secrets.TEST_PYPI_TOKEN }}` GitHub secret. Ensure this step only runs if the smoke test passes.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Final Review and Workflow Validation",
            "description": "Verify the complete workflow configuration and document the dispatch parameters.",
            "dependencies": [
              4
            ],
            "details": "Review the final `.github/workflows/publish-testpypi.yml` file to ensure all indentation is correct, secrets are referenced properly, and the `workflow_dispatch` inputs are correctly defined. Verify that the `release_tag` input is used to checkout the specific ref if provided, or default to the current branch.",
            "status": "pending",
            "testStrategy": "Manually trigger the workflow from the GitHub Actions UI on a feature branch to verify it passes up to the publish step (which might fail if secrets aren't set in the fork, but the logic can be verified).",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-30T00:19:04.031Z"
      },
      {
        "id": 6,
        "title": "Configure PyPI Trusted Publishing Workflow",
        "description": "Update the release workflow to use OIDC for Trusted Publishing to PyPI on tag push.",
        "details": "1. Edit `.github/workflows/release.yml`.\n2. Set permission `id-token: write`.\n3. Configure environment `name: pypi` and `url: https://pypi.org/p/lmstudio-llmstxt-generator`.\n4. Update the publish step to use `uv publish --trusted-publishing` (or `gh-action-pypi-publish`).\n5. Ensure it triggers on `push: tags: [ 'v*' ]`.\n6. Integrate the smoke test step before the publish job.",
        "testStrategy": "Review workflow syntax using a linter. Actual verification requires pushing a tag (or testing in a fork with TestPyPI configured as the trusted target).",
        "priority": "high",
        "dependencies": [
          "4"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze and Prepare Release Workflow Structure",
            "description": "Examine the existing .github/workflows directory and prepare the structure for release.yml with required OIDC permissions.",
            "dependencies": [],
            "details": "Check for existing release.yml. If present, review current steps. If absent, create a new file. Define the trigger on 'push' for tags matching 'v*'. Crucially, add the top-level 'permissions' block setting 'id-token: write' to enable OIDC authentication for Trusted Publishing.",
            "status": "pending",
            "testStrategy": "Review the workflow file syntax to ensure permissions are correctly scoped.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Configure Build Job with UV",
            "description": "Define the build job in release.yml to checkout code, set up Python, and build artifacts using uv.",
            "dependencies": [
              1
            ],
            "details": "In release.yml, add a 'build' job. Steps: 1. Checkout code (v4). 2. Install uv. 3. Setup Python (v5). 4. Run 'uv build'. 5. Upload artifacts (dist/*) using actions/upload-artifact@v4 so they can be used by subsequent jobs (smoke test and publish).",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate Smoke Test Job",
            "description": "Add a smoke test job that runs before publishing to verify the build artifacts work as expected.",
            "dependencies": [
              2
            ],
            "details": "Create a 'smoke-test' job in release.yml that 'needs: build'. Steps: 1. Download artifacts from the build job. 2. Install uv/Python. 3. Run the smoke test suite (e.g., 'python tests/smoke_test.py' or 'pytest tests/smoke_test.py') against the downloaded wheel to ensure viability before publishing.",
            "status": "pending",
            "testStrategy": "Ensure the job fails if the smoke test script exits with a non-zero code.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Configure PyPI Publish Job with Trusted Publishing",
            "description": "Define the publish job using OIDC authentication targeting the PyPI environment.",
            "dependencies": [
              3
            ],
            "details": "Add a 'publish' job that 'needs: smoke-test'. Define 'environment' with name 'pypi' and url 'https://pypi.org/p/lmstudio-llmstxt-generator'. Use 'pypa/gh-action-pypi-publish@release/v1' (recommended for Trusted Publishing) or 'uv publish --trusted-publishing' if supported. Do NOT use username/password secrets.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Finalize and Validate Workflow Configuration",
            "description": "Combine all jobs into the final workflow file and perform static validation.",
            "dependencies": [
              4
            ],
            "details": "Ensure all jobs (build, smoke-test, publish) are correctly linked with 'needs'. Verify the environment URL matches the specific PyPI project. Commit the .github/workflows/release.yml file. Check that 'contents: read' permission is also present alongside 'id-token: write'.",
            "status": "pending",
            "testStrategy": "Run a linter (e.g., action-validator) on the YAML file locally if possible.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-30T00:19:10.363Z"
      },
      {
        "id": 7,
        "title": "Consolidate Release Runbook",
        "description": "Create a comprehensive document describing the release process for maintainers.",
        "details": "1. Create `docs/publishing.md`.\n2. Document prerequisites (PyPI account, Trusted Publisher setup logic).\n3. Document the 'One Command' release flow (tagging).\n4. Document the TestPyPI manual trigger flow.\n5. Include troubleshooting steps for common errors (e.g., metadata rejected).\n6. Link to the CI workflows.",
        "testStrategy": "Peer review the markdown file. Verify links work.",
        "priority": "medium",
        "dependencies": [
          "5",
          "6"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Establish Documentation Structure and Prerequisites",
            "description": "Create the `docs/publishing.md` file and document the initial setup requirements for maintainers.",
            "dependencies": [],
            "details": "Create a new markdown file at `docs/publishing.md`. Add a 'Prerequisites' section detailing the need for a PyPI account and the specific configuration required for Trusted Publisher setup (OIDC) between GitHub and PyPI. Explain how maintainers should verify their permissions before attempting a release.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Document the Production Release Workflow",
            "description": "Detail the standard 'One Command' release process triggered by git tags.",
            "dependencies": [
              1
            ],
            "details": "In `docs/publishing.md`, describe the production release flow. Explain the versioning convention (Semantic Versioning), how to create a git tag (e.g., `git tag v1.0.0`), and how pushing this tag triggers the GitHub Action defined in `.github/workflows/release.yml`. Include a step-by-step guide for the maintainer to execute this flow.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Document the TestPyPI Workflow",
            "description": "Explain the manual trigger process for deploying to TestPyPI for validation.",
            "dependencies": [
              1
            ],
            "details": "Add a section to `docs/publishing.md` covering the TestPyPI release process. Reference the manual dispatch trigger configured in the CI workflow. Provide instructions on how to use the GitHub UI 'Run workflow' button, input necessary parameters (if any), and verify the artifact on `test.pypi.org`.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Compile Troubleshooting Guide",
            "description": "Create a section for resolving common release failures and errors.",
            "dependencies": [
              2,
              3
            ],
            "details": "Add a 'Troubleshooting' section to the documentation. Address common issues such as 'Metadata Rejected' (often due to version conflicts or invalid `pyproject.toml` configuration), OIDC token failures, and smoke test failures. Provide resolution steps for each scenario, including how to clean up failed tags if necessary.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Finalize References and CI Links",
            "description": "Add direct links to workflows and review the document for completeness.",
            "dependencies": [
              4
            ],
            "details": "Conclude the document by adding direct links to the repository's 'Actions' tab and the specific workflow files (`release.yml`, etc.). Perform a self-review to ensure all paths, commands, and URLs are accurate relative to the project root. Ensure the document is linked from the main `README.md` or `CONTRIBUTING.md` if they exist.",
            "status": "pending",
            "testStrategy": "Peer review the markdown file. Verify all hyperlinks are functional.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-30T00:19:25.045Z"
      },
      {
        "id": 8,
        "title": "Add Pre-commit/Local Build Check Script",
        "description": "Create a helper script for developers to run the full build-verify-smoke cycle locally.",
        "details": "1. Create `scripts/release/verify_install.sh` (or `local_check.sh`).\n2. The script should: Clean `dist/`, run `uv build`, run `validate_metadata.py`, and run `smoke_test.py`.\n3. Make it executable.",
        "testStrategy": "Run `./scripts/release/verify_install.sh` locally and ensure it passes when the repo is clean.",
        "priority": "low",
        "dependencies": [
          "3",
          "4"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Verify Install Script Skeleton",
            "description": "Initialize the verify_install.sh bash script with strict error handling and basic environment checks.",
            "dependencies": [],
            "details": "Create the directory `scripts/release` if it does not exist. Create the file `scripts/release/verify_install.sh` with the shebang `#!/bin/bash` and `set -euo pipefail` to ensure the script exits immediately on error. Add a check to verify that the `uv` command is installed and available in the system PATH.",
            "status": "pending",
            "testStrategy": "Run the script; verify it exists, errors if `uv` is missing, or exits cleanly if nothing else is added yet.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Clean and Build Logic",
            "description": "Add functionality to clean old artifacts and rebuild the project using uv.",
            "dependencies": [
              1
            ],
            "details": "Update the script to run `rm -rf dist/` to clean previous builds. Add the `uv build` command. Check that the `dist/` directory is created and contains artifacts (e.g., `.whl`, `.tar.gz`) after the build command runs.",
            "status": "pending",
            "testStrategy": "Run the script. Check that `dist/` is cleared and then repopulated with build artifacts.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate Metadata Validation Step",
            "description": "Invoke the metadata validation Python script to ensure build artifacts are well-formed.",
            "dependencies": [
              2
            ],
            "details": "Add the command to execute `python scripts/release/validate_metadata.py`. Ensure the script relies on the exit code of this Python script to determine pass/fail status. This ensures no artifacts with invalid metadata (e.g., direct URL dependencies) pass the check.",
            "status": "pending",
            "testStrategy": "Temporarily modify `pyproject.toml` to be invalid, run the script, and ensure it fails at this step.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Integrate Smoke Test Step",
            "description": "Invoke the smoke test script to verify the basic functionality of the built artifacts.",
            "dependencies": [
              3
            ],
            "details": "Add the command to execute `python scripts/release/smoke_test.py`. This step should occur after metadata validation. It ensures that the package built in the previous steps can be installed and imported successfully.",
            "status": "pending",
            "testStrategy": "Run the script and verify that the smoke test python script is executed effectively.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Finalize Script Permissions and User Experience",
            "description": "Make the script executable and add user-friendly logging for build phases.",
            "dependencies": [
              4
            ],
            "details": "Run `chmod +x scripts/release/verify_install.sh`. Add `echo` statements with ANSI color codes (e.g., Green for success, Red for failure) to clearly demarcate the Build, Validate, and Smoke Test phases. Add a final success message if all steps pass.",
            "status": "pending",
            "testStrategy": "Run `./scripts/release/verify_install.sh` directly from the terminal and verify the output contains formatted status messages.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-30T00:24:30.249Z"
      },
      {
        "id": 9,
        "title": "Verify Dynamic Versioning or Version Bump Policy",
        "description": "Ensure the package version aligns with the Git tag automatically or document the bump process.",
        "details": "1. Check `pyproject.toml` for `dynamic = [\"version\"]` or static version.\n2. If static, update the Runbook (Task 7) to include 'Bump version in pyproject.toml' as a step before tagging.\n3. If dynamic versioning is preferred, configure `setuptools_scm` (or equivalent) in `pyproject.toml` and add it to build-system requirements.",
        "testStrategy": "Build a wheel and check the filename version. Ensure it matches the expected tag/commit version.",
        "priority": "medium",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze pyproject.toml for Current Versioning Configuration",
            "description": "Inspect the pyproject.toml file to determine if the version is statically defined or configured for dynamic discovery.",
            "dependencies": [],
            "details": "Read `pyproject.toml` to check the `[project]` table for a `version` key or `dynamic = [\"version\"]`. Check `[build-system]` and `[tool.setuptools]` (or other backend configs) to understand the current setup.",
            "status": "pending",
            "testStrategy": "None (Analysis task)",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Dynamic Versioning with setuptools_scm",
            "description": "Configure the project to derive its version automatically from Git tags using setuptools_scm.",
            "dependencies": [
              1
            ],
            "details": "Modify `pyproject.toml` to: 1. Remove static `version`. 2. Add `dynamic = [\"version\"]`. 3. Add `setuptools_scm` to `[build-system] requires`. 4. Add `[tool.setuptools_scm]` section (can be empty or with `write_to` config).",
            "status": "pending",
            "testStrategy": "Run `uv build` (or pip install) locally and verify the generated metadata/wheel version matches `git describe --tags`.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Ensure __version__ Attribute Availability",
            "description": "Ensure the package version is accessible at runtime via `__version__` without hardcoding it.",
            "dependencies": [
              2
            ],
            "details": "Modify `src/lmstudiotxt_generator/__init__.py` (or equivalent top-level init) to use `importlib.metadata.version` to retrieve the installed package version dynamically, handling the `PackageNotFoundError` gracefully.",
            "status": "pending",
            "testStrategy": "Run a python script importing the package and printing `__version__`. It should match the installed version.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Update Release Documentation / Runbook",
            "description": "Update the project documentation to reflect the new dynamic versioning workflow.",
            "dependencies": [
              2
            ],
            "details": "Create or update a `RELEASE.md` or the Runbook (from Task 7 context) to explain that tagging a commit triggers the version bump, and manual editing of `pyproject.toml` version is no longer needed.",
            "status": "pending",
            "testStrategy": "Review the documentation for clarity and accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Verify Wheel Versioning with Smoke Test",
            "description": "Validate that the build process produces artifacts with the correct version derived from the git context.",
            "dependencies": [
              2
            ],
            "details": "Create a temporary git tag (e.g., `v0.0.1-test`), run `uv build`, and inspect the filename of the generated `.whl` in `dist/`. Ensure it matches the tag. Clean up the tag afterwards.",
            "status": "pending",
            "testStrategy": "Manual verification or script checking `dist/` filenames against `git describe` output.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-30T00:27:09.320Z"
      },
      {
        "id": 10,
        "title": "End-to-End Release Simulation",
        "description": "Perform a final validation of the entire pipeline using TestPyPI.",
        "details": "1. Bump version to a dev version (e.g., `0.0.1.dev1`).\n2. Trigger the TestPyPI workflow.\n3. Verify upload.\n4. locally run `pip install -i https://test.pypi.org/simple/ lmstudio-llmstxt-generator`.\n5. Verify `lmstxt --help` works.",
        "testStrategy": "Successful installation and execution from TestPyPI.",
        "priority": "medium",
        "dependencies": [
          "5",
          "7"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Update Version to Development Release Candidate",
            "description": "Bump the package version in pyproject.toml to a unique development version (e.g., 0.0.1.dev1) to allow a non-conflicting upload to TestPyPI.",
            "dependencies": [],
            "details": "Modify the `version` field in `pyproject.toml`. Append a `.devN` suffix to the current version. This ensures that we can verify the upload mechanism without consuming a 'real' semantic version number on TestPyPI, as PyPI files are immutable.",
            "status": "pending",
            "testStrategy": "Check pyproject.toml content and verify `uv build` produces a file with the new version string.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Trigger TestPyPI Deployment Workflow",
            "description": "Manually trigger the GitHub Action or CI pipeline configured for TestPyPI deployment using the new development version.",
            "dependencies": [
              1
            ],
            "details": "Navigate to the CI/CD dashboard (e.g., GitHub Actions). Locate the 'Publish to TestPyPI' workflow (created in previous tasks). Trigger it manually or by pushing a specific tag (e.g., `v0.0.1.dev1`) depending on the trigger logic defined in the workflow file.",
            "status": "pending",
            "testStrategy": "Monitor the CI job logs for successful completion, specifically looking for the 'Twine upload' or equivalent step success message.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Verify TestPyPI Upload Integrity",
            "description": "Confirm that the package artifacts have effectively appeared on the TestPyPI repository page with the correct metadata.",
            "dependencies": [
              2
            ],
            "details": "Visit the project URL on TestPyPI (e.g., `https://test.pypi.org/project/lmstudio-llmstxt-generator/`). Verify that the new version `0.0.1.dev1` is listed, the Release Date is current, and the `Description` (README) renders correctly.",
            "status": "pending",
            "testStrategy": "Manual verification on the TestPyPI website.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Perform Clean Installation from TestPyPI",
            "description": "Create a fresh virtual environment and install the package exclusively from the TestPyPI index to simulate an end-user installation.",
            "dependencies": [
              3
            ],
            "details": "Create a new virtualenv: `python -m venv venv_test`. Activate it. Run `pip install --no-cache-dir --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ lmstudio-llmstxt-generator==0.0.1.dev1`. Note: `--extra-index-url` is often needed for dependencies that don't exist on TestPyPI.",
            "status": "pending",
            "testStrategy": "Ensure pip completes successfully without errors regarding missing dependencies or hash mismatches.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Validate Installed CLI Functionality",
            "description": "Execute the installed CLI tool within the test environment to confirm the package structure and entry points function correctly.",
            "dependencies": [
              4
            ],
            "details": "Inside the `venv_test`, run the command `lmstxt --help`. Verify that the help text is displayed, indicating that the entry point scripts were correctly generated and the internal imports are working.",
            "status": "pending",
            "testStrategy": "The command should exit with code 0 and display the standard help message, not an ImportError or ModuleNotFoundError.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-30T00:28:29.726Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2025-12-30T00:28:29.728Z",
      "taskCount": 10,
      "completedCount": 10,
      "tags": [
        "chore-release"
      ],
      "created": "2026-01-03T18:28:51.110Z",
      "description": "Tasks for chore-release context",
      "updated": "2026-01-03T18:28:51.110Z"
    }
  },
  "feat-mcp": {
    "tasks": [
      {
        "id": "1",
        "title": "Project Foundation & Configuration Module",
        "description": "Establish the project structure, dependency management, and configuration loading logic.",
        "details": "Initialize the project following the `src/llmstxt_mcp/` structure. Create `config.py` using Pydantic Settings or `python-dotenv` to manage environment variables (e.g., `LLMSTXT_MCP_ALLOWED_ROOT`, `LLMSTXT_MCP_RESOURCE_MAX_CHARS`). Ensure `pyproject.toml` includes dependencies for `mcp` (using `fastmcp` if available/compatible or standard `mcp` SDK) and the local `lmstudio-llmstxt-generator` package. Set up `errors.py` to define custom exception classes like `OutputDirNotAllowedError` and `LMStudioUnavailableError`.\n\nLibraries: `pydantic`, `pydantic-settings`, `mcp`.\nfiles: `src/llmstxt_mcp/config.py`, `src/llmstxt_mcp/errors.py`, `pyproject.toml`.",
        "testStrategy": "Unit tests verifying that environment variables are correctly parsed into the config object and that defaults are applied. Test custom error instantiation.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "complexity": 3,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Create a `pyproject.toml` configuration defining the build system and dependencies including `mcp` and `pydantic`. Implement `src/llmstxt_mcp/config.py` using `pydantic-settings` to parse environment variables like `LLMSTXT_MCP_ALLOWED_ROOT` with safe defaults. Define the custom exception hierarchy in `src/llmstxt_mcp/errors.py`.",
        "updatedAt": "2026-01-03T19:17:10.418Z"
      },
      {
        "id": "2",
        "title": "Data Models & Type Definitions",
        "description": "Define Pydantic models for MCP tool inputs/outputs and internal data structures.",
        "details": "Implement `models.py` to define schemas for `GenerateResult`, `ArtifactRef`, and `ReadArtifactResult`. Use Pydantic to enforce types and constraints. Define `ArtifactName` literals (e.g., `llms.txt`, `llms-full.txt`). Ensure strict typing for tool arguments to leverage MCP's automatic schema generation capabilities.\n\nLibraries: `pydantic`.\nfiles: `src/llmstxt_mcp/models.py`.",
        "testStrategy": "Unit tests ensuring valid data passes validation and invalid data raises ValidationError. Verify JSON serialization formats.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 2,
        "recommendedSubtasks": 0,
        "expansionPrompt": "",
        "updatedAt": "2026-01-03T19:17:54.333Z"
      },
      {
        "id": "3",
        "title": "Security & File Hashing Utilities",
        "description": "Implement path validation security controls and file hashing utilities.",
        "details": "Create `security.py` to implement `validate_output_dir`. This function must resolve paths and ensure they are contained within `LLMSTXT_MCP_ALLOWED_ROOT` to prevent path traversal. Create `hashing.py` to implement `sha256_file` (streaming read) and `read_text_preview` (reads first N chars). These utilities are critical for the generation and artifact access layers.\n\nLibraries: `pathlib`, `hashlib`.\nfiles: `src/llmstxt_mcp/security.py`, `src/llmstxt_mcp/hashing.py`.",
        "testStrategy": "Unit tests: 1) Attempt to access paths outside the allowed root (e.g., `../etc/passwd`) and assert `OutputDirNotAllowedError`. 2) Verify SHA256 matches known values for test files. 3) Verify preview returns correct truncation boolean.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 4,
        "recommendedSubtasks": 2,
        "expansionPrompt": "Implement `validate_output_dir` in `security.py` using `pathlib` to strictly resolve and check path containment against the allowed root. Develop `hashing.py` to provide a streaming SHA256 calculator and a safe text preview reader that handles encoding errors gracefully.",
        "updatedAt": "2026-01-03T19:18:55.104Z"
      },
      {
        "id": "4",
        "title": "In-Memory Run Registry",
        "description": "Implement the storage mechanism to track generation runs and their artifacts.",
        "details": "Create `runs.py` containing a `RunStore` class. This should maintain an in-memory dictionary mapping `run_id` to `RunRecord` objects. Implement methods `put_run(run_record)`, `get_run(run_id)`, and `list_runs(limit)`. Use a thread-safe approach if necessary, though simple dicts are atomic in Python for single operations. This store bridges the generation and reading steps.\n\nfiles: `src/llmstxt_mcp/runs.py`.",
        "testStrategy": "Unit tests: Add runs, retrieve them by ID, and list them with sorting/limiting. Verify `UnknownRunError` when accessing non-existent IDs.",
        "priority": "medium",
        "dependencies": [
          "2"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 3,
        "recommendedSubtasks": 0,
        "expansionPrompt": "",
        "updatedAt": "2026-01-03T19:19:47.633Z"
      },
      {
        "id": "5",
        "title": "Artifact Access Logic",
        "description": "Implement the core logic for resolving, reading, and chunking artifact content.",
        "details": "Create `artifacts.py`. Implement `resource_uri(run_id, artifact)` to generate standard URIs. Implement `read_resource_text` which uses `hashing.read_text_preview` for truncated reads suitable for MCP Resources. Implement `read_artifact_chunk` for the chunking tool, handling `offset` and `limit` to slice file content safely. Ensure dependencies on `runs.py` to resolve `run_id` to file paths.\n\nfiles: `src/llmstxt_mcp/artifacts.py`.",
        "testStrategy": "Unit tests: Create dummy files, perform chunked reads at various offsets (start, middle, end, past end). Verify truncation banners are prepended when reading as a resource.",
        "priority": "high",
        "dependencies": [
          "3",
          "4"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Artifacts Module and URI Logic",
            "description": "Create the artifacts module structure and implement the URI generation helper function. [Updated: 1/3/2026]",
            "dependencies": [],
            "details": "Create `src/llmstxt_mcp/artifacts.py`. Import necessary dependencies including the `RunStore` from `runs.py`. Implement the `resource_uri(run_id: str, artifact_name: str) -> str` function to generate standardized URIs (e.g., `llmstxt://{run_id}/{artifact}`) used by the MCP server to identify resources.",
            "status": "done",
            "testStrategy": "Unit test verifying that `resource_uri` returns the expected string format for given inputs.",
            "parentId": "undefined",
            "updatedAt": "2026-01-03T19:20:34.944Z"
          },
          {
            "id": 2,
            "title": "Implement Truncated Resource Reading",
            "description": "Develop the logic to read artifact files for MCP Resources, ensuring content is truncated if it exceeds limits.",
            "dependencies": [
              1
            ],
            "details": "Implement `read_resource_text(run_id, artifact)`. This function must resolve the file path using `RunStore`, read the text content, and check against `LLMSTXT_MCP_RESOURCE_MAX_CHARS`. If the content exceeds the limit, truncate it and append a footer (e.g., '... truncated') to prevent overloading the MCP client.",
            "status": "done",
            "testStrategy": "Create a dummy file larger than the configured max chars. Call `read_resource_text` and assert the returned string length matches the limit and contains the truncation indicator.",
            "parentId": "undefined",
            "updatedAt": "2026-01-03T19:21:38.433Z"
          },
          {
            "id": 3,
            "title": "Implement Chunked Artifact Reading",
            "description": "Create the logic for reading specific slices of a file based on offset and limit parameters for the reading tool.",
            "dependencies": [
              1
            ],
            "details": "Implement `read_artifact_chunk(run_id, artifact, offset, limit)`. Use python file seeking (`seek`) to navigate to the `offset` and read up to `limit` characters. Handle edge cases such as `offset` exceeding file size (return empty string) or `limit` extending past EOF. Ensure file handles are closed safely.",
            "status": "done",
            "testStrategy": "Create a file with known content (e.g., '0123456789'). specific offsets and limits (e.g., offset=2, limit=3 should return '234') and verify behavior when reading past EOF.",
            "parentId": "undefined",
            "updatedAt": "2026-01-03T19:25:09.120Z"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Implement `resource_uri` generation logic. Create the `read_resource_text` function handling file I/O and truncation. Develop `read_artifact_chunk` with robust offset/limit handling to support pagination of large files.",
        "updatedAt": "2026-01-03T19:25:09.120Z"
      },
      {
        "id": "6",
        "title": "Generator Integration & Serialized Execution",
        "description": "Wrap the external `lmstudiotxt_generator` library with thread-safe execution logic.",
        "details": "In `server.py` (or a dedicated integration module), implement the `llmstxt_generate` tool logic. Import `run_generation` from the external library. Use a `threading.Lock` to ensure only one generation runs at a time (avoiding global config races). Capture exceptions (specifically connectivity errors) and map them to `LMStudioUnavailableError`. Calculate hashes for outputs and register the run in `RunStore`.\n\nLibraries: `lmstudiotxt_generator`.\nfiles: `src/llmstxt_mcp/server.py`.",
        "testStrategy": "Integration test: Mock the external `run_generation` to write temporary files. Verify that the MCP tool wrapper correctly calls the mock, computes metadata, and updates the RunStore.",
        "priority": "high",
        "dependencies": [
          "5"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Thread-Safe Generator Wrapper",
            "description": "Create the integration module and implement the locking mechanism to ensure serial execution of the generator.",
            "dependencies": [],
            "details": "Create `src/llmstxt_mcp/generator.py` (or integrate into `server.py`). Import `run_generation` from `lmstudiotxt_generator`. Instantiate a module-level `threading.Lock`. Define a function `safe_generate` that uses the lock as a context manager to wrap the call to `run_generation`. This ensures that concurrent requests to the MCP server do not trigger race conditions in the external library's global configuration.",
            "status": "done",
            "testStrategy": "Unit test: call the wrapper from multiple threads simultaneously and assert that the critical section is entered serially.",
            "parentId": "undefined",
            "updatedAt": "2026-01-03T19:31:15.917Z"
          },
          {
            "id": 2,
            "title": "Implement Error Translation Layer",
            "description": "Add error handling logic to map external library exceptions to internal domain errors.",
            "dependencies": [
              1
            ],
            "details": "In the generation wrapper, wrap the `run_generation` call in a try/except block. Specifically identify connectivity errors (e.g., connection refused from LM Studio) and raise a `LMStudioUnavailableError` (defined in `errors.py`). Ensure generic exceptions are also caught and wrapped or logged appropriately to prevent server crashes while releasing the lock in a `finally` block.",
            "status": "done",
            "testStrategy": "Unit test: Mock `run_generation` to raise specific exceptions and assert that the wrapper raises the correct internal `LMStudioUnavailableError`.",
            "parentId": "undefined",
            "updatedAt": "2026-01-03T19:36:42.791Z"
          },
          {
            "id": 3,
            "title": "Integrate Output Hashing and RunStore Updates",
            "description": "Process generation outputs, calculate hashes, and register the completed run in the RunStore.",
            "dependencies": [
              1,
              2
            ],
            "details": "Upon successful generation, use the hashing utilities from `src/llmstxt_mcp/hashing.py` to calculate SHA256 checksums for the output files. Construct a `Run` object with the status, timestamp, and artifact metadata. Call the `RunStore` instance to save the run. Return the final result object expected by the MCP tool interface.",
            "status": "done",
            "testStrategy": "Integration test: Verify that a successful generation results in a new entry in `RunStore` with correct file hashes.",
            "parentId": "undefined",
            "updatedAt": "2026-01-03T19:55:09.862Z"
          }
        ],
        "complexity": 6,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Create the integration module wrapping `lmstudiotxt_generator`. Implement a locking mechanism to enforce serial execution. Add error handling to translate external library exceptions into internal domain errors and update the `RunStore` upon completion.",
        "updatedAt": "2026-01-03T19:55:09.862Z"
      },
      {
        "id": "7",
        "title": "MCP Server Tool & Resource Wiring",
        "description": "Configure the FastMCP server instance and register tools and resources.",
        "details": "In `server.py`, initialize `FastMCP`. Register the `llmstxt_generate` and `llmstxt_read_artifact` functions as tools with appropriate descriptions. Register the `llmstxt://runs/{run_id}/{artifact}` pattern as a resource. Ensure the `llmstxt_list_runs` tool is also registered. Wire up the `stdio` and `http` transport capabilities provided by FastMCP/MCP SDK.\n\nLibraries: `fastmcp` (or standard `mcp` SDK depending on preference/availability).\nfiles: `src/llmstxt_mcp/server.py`.",
        "testStrategy": "Functional test: Instantiate the server in a test harness. Call `list_tools` and `list_resources` to verify registration. Simulate a tool call and resource read request.",
        "priority": "high",
        "dependencies": [
          "6"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize FastMCP Server Instance",
            "description": "Set up the basic FastMCP server application shell and entry point. [Updated: 1/3/2026]",
            "dependencies": [],
            "details": "In `src/llmstxt_mcp/server.py`, import `FastMCP` and instantiate the server object with the name 'llmstxt-mcp'. Define the standard `if __name__ == \"__main__\":` block to invoke `mcp.run()`, ensuring it defaults to the stdio transport mechanism for standard MCP communication.",
            "status": "done",
            "testStrategy": "Run the python file directly and verify it enters the wait loop for stdin input (or prints help if arguments are required).",
            "parentId": "undefined",
            "updatedAt": "2026-01-03T20:04:49.758Z"
          },
          {
            "id": 2,
            "title": "Register MCP Tools",
            "description": "Expose the core generation and retrieval functions as MCP tools.",
            "dependencies": [
              1
            ],
            "details": "Import the logic from core modules. Use the `@mcp.tool()` decorator to register `llmstxt_generate`, `llmstxt_read_artifact`, and `llmstxt_list_runs`. Ensure type hints use the Pydantic models defined in `models.py` so that FastMCP generates the correct JSON schemas for the tool definitions.",
            "status": "done",
            "testStrategy": "Unit test that inspects the `mcp.tools` registry to confirm all three tools are present and possess the correct arguments/descriptions.",
            "parentId": "undefined",
            "updatedAt": "2026-01-03T20:20:31.495Z"
          },
          {
            "id": 3,
            "title": "Register Dynamic Resources",
            "description": "Implement the URI pattern for accessing generated artifacts as resources. [Updated: 1/3/2026]",
            "dependencies": [
              1
            ],
            "details": "Use the `@mcp.resource(\"llmstxt://runs/{run_id}/{artifact}\")` decorator to register the resource handler. Implement the underlying function to parse the `run_id` and `artifact` name, retrieve the content from `RunStore`, and return it as a string resource.",
            "status": "done",
            "testStrategy": "Unit test mocking the `RunStore` with a dummy artifact, calling the resource handler with a valid URI, and asserting the content is returned.",
            "parentId": "undefined",
            "updatedAt": "2026-01-03T20:21:32.410Z"
          },
          {
            "id": 4,
            "title": "Server Capability Verification",
            "description": "Verify the complete wiring of tools and resources within the server instance. [Updated: 1/3/2026]",
            "dependencies": [
              2,
              3
            ],
            "details": "Create an integration test harness that loads the `server.py` module. Call the internal MCP introspection methods (e.g., `list_tools`, `list_resource_templates`) to verify that the server is correctly advertising its capabilities before it is deployed to a real client.",
            "status": "done",
            "testStrategy": "Integration test: Instantiate the server and simulate a `tools/list` and `resources/templates/list` JSON-RPC request to validate the response structure.",
            "parentId": "undefined",
            "updatedAt": "2026-01-03T20:22:31.099Z"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Initialize the `FastMCP` server instance. Register the generation and reading tools with their Pydantic schemas. Define the resource URI pattern `llmstxt://runs/{run_id}/{artifact}` and link it to the artifact reader. Configure the entry point to run the server.",
        "updatedAt": "2026-01-03T20:22:31.099Z"
      },
      {
        "id": "8",
        "title": "Stdio-Safe Logging Implementation",
        "description": "Configure logging to ensure no interference with the stdio transport. [Updated: 1/3/2026]",
        "details": "Update `server.py` or `config.py` to configure the Python `logging` module. Ensure the root logger writes to `sys.stderr` and NOT `sys.stdout`, as stdout is reserved for the JSON-RPC protocol in stdio mode. Set log levels based on the configuration.\n\nfiles: `src/llmstxt_mcp/server.py`.",
        "testStrategy": "Manual/Scripted test: Run the server in stdio mode, emit logs, and verify they appear on stderr while the JSON-RPC communication on stdout remains valid JSON.",
        "priority": "medium",
        "dependencies": [
          "7"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 2,
        "recommendedSubtasks": 0,
        "expansionPrompt": "",
        "updatedAt": "2026-01-03T20:26:24.207Z"
      },
      {
        "id": "9",
        "title": "Request Limits & Error Normalization",
        "description": "Harden the server by enforcing limits and ensuring clean error reporting.",
        "details": "Refine `artifacts.py` and `server.py` to enforce `MAX_CHARS` limits on resource reads and chunk sizes. Implement a global error handler or try/except blocks in tool entry points to catch internal exceptions (e.g., `LMStudioUnavailableError`, `OutputDirNotAllowedError`) and return user-friendly error strings instead of stack traces to the MCP client.\n\nfiles: `src/llmstxt_mcp/errors.py`, `src/llmstxt_mcp/server.py`.",
        "testStrategy": "Unit tests: Trigger exceptions and verify the returned error message format. Test boundary conditions for chunk sizes to ensure limits are respected.",
        "priority": "medium",
        "dependencies": [
          "7"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 4,
        "recommendedSubtasks": 2,
        "expansionPrompt": "Implement decorators or middleware to catch domain exceptions and convert them to user-friendly MCP errors. Refactor artifact reading to strictly enforce `MAX_CHARS` limits globally.",
        "updatedAt": "2026-01-03T20:26:56.522Z"
      },
      {
        "id": "10",
        "title": "Packaging & Entry Point Verification",
        "description": "Finalize packaging and verify the CLI entry point works for both transports.",
        "details": "Ensure `pyproject.toml` defines the correct `project.scripts` entry point (e.g., `llmstxt-mcp = llmstxt_mcp.server:main`). Verify that running `llmstxt-mcp` defaults to stdio mode and accepts flags for HTTP mode if implemented. Create a basic README documenting installation and usage.\n\nfiles: `pyproject.toml`, `README.md`.",
        "testStrategy": "Smoke test: Install the package locally (`pip install -e .`) and run the CLI command. Verify it starts up without crashing and prints expected startup logs to stderr.",
        "priority": "low",
        "dependencies": [
          "8",
          "9"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 2,
        "recommendedSubtasks": 0,
        "expansionPrompt": "",
        "updatedAt": "2026-01-03T22:34:55.275Z"
      },
      {
        "id": "11",
        "title": "Fix: Refactor for Asynchronous Processing",
        "description": "To truly remove the timeout in a way that works for tools, the server should not do heavy lifting inside the tool call. It should accept the request, launch a background thread to do the work, and return a \"Processing...\" status immediately.",
        "details": "Requirements:\n- Accept the request\n- Launch a background thread to do the work\n- Return a \"Processing...\" status immediately",
        "testStrategy": "",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Update RunRecord to Include Job Status and Results",
            "description": "Enhance the RunRecord data model to track the state of the background job (e.g., 'pending', 'processing', 'completed', 'failed') and capture any errors or outputs.",
            "dependencies": [],
            "details": "Modify the `RunRecord` class in `src/llmstxt_mcp/runs.py`. Add fields for `status` (Enum), `error_message` (Optional[str]), and ensure it can store results once the background task finishes. Update `RunStore.put_run` or create a `update_run` method to allow updating status during the lifecycle.",
            "status": "done",
            "testStrategy": "Unit test for RunStore and RunRecord to ensure status transitions and metadata updates are correctly persisted in memory.",
            "parentId": "undefined",
            "updatedAt": "2026-01-04T07:54:52.404Z"
          },
          {
            "id": 2,
            "title": "Implement Background Worker Logic",
            "description": "Create a helper function that executes the heavy generation logic and updates the RunStore with results.",
            "dependencies": [
              1
            ],
            "details": "In `src/llmstxt_mcp/server.py` or a new `worker.py`, implement an async-friendly wrapper (using `anyio.to_thread.run_sync` or standard `threading`) that calls the actual generation code. It must catch exceptions, update the run status in `RunStore` to 'completed' or 'failed', and store the resulting artifact paths.",
            "status": "done",
            "testStrategy": "Integration test: Call the worker function directly and verify that the RunStore reflects the correct status and artifact list after completion.",
            "parentId": "undefined",
            "updatedAt": "2026-01-04T07:55:00.636Z"
          },
          {
            "id": 3,
            "title": "Refactor llmstxt_generate Tool to Use Background Tasks",
            "description": "Modify the main generation tool to initialize a run and immediately return the run ID.",
            "dependencies": [
              2
            ],
            "details": "Update the `llmstxt_generate` tool in `server.py`. Instead of awaiting the full generation, it should: 1. Generate a unique `run_id`. 2. Create a 'pending' record in `RunStore`. 3. Schedule the worker logic in the background (using `asyncio.create_task` or a background thread). 4. Return a JSON response containing the `run_id` and a message like 'Generation started'.",
            "status": "done",
            "testStrategy": "Functional test using an MCP client: verify that the tool call returns immediately with a success message while the work happens in the background.",
            "parentId": "undefined",
            "updatedAt": "2026-01-04T07:55:08.953Z"
          },
          {
            "id": 4,
            "title": "Update Resource Provider for Dynamic Run Status",
            "description": "Ensure that reading a run resource provides the current status of the processing job.",
            "dependencies": [
              1,
              3
            ],
            "details": "Modify the resource resolver for `llmstxt://runs/{run_id}/...` in `server.py`. If the user requests the status or the job is still processing, the resource content should reflect the current status (e.g., 'Processing...') instead of failing or blocking. If the job failed, it should return the captured error message.",
            "status": "done",
            "testStrategy": "Resource read test: Attempt to read a resource for a 'processing' run and verify the returned text indicates the current state.",
            "parentId": "undefined",
            "updatedAt": "2026-01-04T07:55:16.681Z"
          },
          {
            "id": 5,
            "title": "Implement Asynchronous Cleanup of Completed Runs",
            "description": "Add logic to prune old runs from the in-memory registry to prevent memory leaks.",
            "dependencies": [
              1
            ],
            "details": "Add a simple TTL (Time To Live) or a maximum capacity check to `RunStore`. Implement a background cleanup routine that periodically removes records for runs that have been completed or failed for a certain duration.",
            "status": "done",
            "testStrategy": "Unit test: Verify that adding runs beyond the capacity or after the TTL causes the oldest/expired runs to be removed from the registry.",
            "parentId": "undefined",
            "updatedAt": "2026-01-04T07:55:25.234Z"
          },
          {
            "id": 6,
            "title": "Update RunRecord Model for State Tracking",
            "description": "Enhance the existing RunRecord model in runs.py to include status fields and lifecycle timestamps.",
            "dependencies": [],
            "details": "Add 'status' (Enum: PENDING, PROCESSING, COMPLETED, FAILED), 'error_message' (Optional string), and 'updated_at' fields to the RunRecord dataclass or Pydantic model.",
            "status": "done",
            "testStrategy": "Unit tests to verify instantiation and field accessibility for the updated RunRecord structure.",
            "updatedAt": "2026-01-04T07:55:33.395Z",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Implement Thread-Safe RunStore Update Mechanism",
            "description": "Modify RunStore to handle concurrent updates to records safely using synchronization primitives.",
            "dependencies": [
              6
            ],
            "details": "Add a threading.Lock to the RunStore class. Implement an update_run(run_id, **kwargs) method that safely updates specific fields in a RunRecord without race conditions.",
            "status": "done",
            "testStrategy": "Concurrent stress test with multiple threads attempting to update the same record ID simultaneously.",
            "parentId": "undefined",
            "updatedAt": "2026-01-04T07:55:43.390Z"
          },
          {
            "id": 8,
            "title": "Develop Background Execution Wrapper",
            "description": "Create a helper function to execute long-running generator tasks in a background thread.",
            "dependencies": [
              7
            ],
            "details": "Use anyio.to_thread.run_sync or threading.Thread to wrap generator logic. The wrapper must update the RunStore status to 'PROCESSING' at start, 'COMPLETED' on success, and 'FAILED' on exception.",
            "status": "done",
            "testStrategy": "Verify status transitions in the RunStore when a dummy background task is executed through the wrapper.",
            "parentId": "undefined",
            "updatedAt": "2026-01-04T07:55:52.937Z"
          },
          {
            "id": 9,
            "title": "Refactor Generate Tool for Immediate Response",
            "description": "Update the llmstxt_generate tool in server.py to initiate background work and return immediately.",
            "dependencies": [
              8
            ],
            "details": "Modify the tool handler to create a new run_id, put a 'PENDING' record in the RunStore, spawn the background wrapper task, and return a JSON response containing the run_id and 'Processing...' message.",
            "status": "done",
            "testStrategy": "Functional test calling the generate tool and checking for immediate HTTP 200/OK while background work continues.",
            "parentId": "undefined",
            "updatedAt": "2026-01-04T07:56:06.316Z"
          },
          {
            "id": 10,
            "title": "Implement Status-Aware Resource Resolvers",
            "description": "Update llmstxt_read_artifact and resource providers to handle runs that are still in progress.",
            "dependencies": [
              9
            ],
            "details": "Check the RunRecord status in resource handlers. If 'PROCESSING', return a status indicator. If 'FAILED', return the error details. Only return artifact content if status is 'COMPLETED'.",
            "status": "done",
            "testStrategy": "Attempt to read an artifact for a 'PROCESSING' run and verify the response correctly indicates work is in progress.",
            "parentId": "undefined",
            "updatedAt": "2026-01-04T07:56:14.353Z"
          },
          {
            "id": 11,
            "title": "Implement In-Memory Record Cleanup Routine",
            "description": "Create a background maintenance task to remove old or stale run records from the RunStore.",
            "dependencies": [
              7
            ],
            "details": "Add a method to RunStore that deletes records older than a configurable TTL (e.g., 24 hours). Implement an optional background loop or trigger this during new run creation to prevent memory leaks.",
            "status": "done",
            "testStrategy": "Add records with manually expired timestamps and verify they are correctly removed after calling the pruning method.",
            "parentId": "undefined",
            "updatedAt": "2026-01-04T07:56:23.989Z"
          }
        ],
        "complexity": 8,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Identify and break down the necessary changes for Task 11 into granular steps. Ensure you cover the following areas: 1) Updating the RunRecord model to track state, 2) Creating a thread-safe RunStore update mechanism, 3) Implementing a background execution wrapper (e.g., using anyio or threading), 4) Refactoring the FastMCP tool to handle immediate returns and background task spawning, 5) Updating resource resolvers to provide status-aware responses, and 6) Implementing a safe cleanup/pruning routine for in-memory records.",
        "updatedAt": "2026-01-04T07:56:23.989Z"
      },
      {
        "id": "12",
        "title": "Artifact Directory Discovery and Resource Exposure",
        "description": "Extend the MCP server to discover and retrieve artifact files from local storage, exposing them as structured resources and tools for client context.",
        "details": "This task transforms the server into a persistent context source by enabling access to the 'artifacts/' directory on disk. 1) Update 'artifacts.py' to include a 'scan_artifacts' function using 'pathlib' to list all '.txt' files in the configured output directory. 2) In 'server.py', implement the 'list_resources' MCP handler to dynamically expose these files as resources with the URI scheme 'llmstxt://artifacts/{filename}'. 3) Implement the 'read_resource' handler to fetch file content, utilizing 'hashing.read_text_preview' for safety and 'security.validate_output_dir' to prevent path traversal. 4) Add a new MCP tool 'llmstxt_list_all_artifacts' that returns metadata (filename, size, last modified) for all existing artifacts, including those from previous server sessions not present in the volatile 'RunStore'. 5) Integrate the 'MAX_CHARS' limit from Task 9 into the directory-based resource reads.",
        "testStrategy": "1) Manually populate the artifacts directory with several .txt files. 2) Use an MCP inspector or test client to call 'list_resources' and verify the 'llmstxt://artifacts/' URIs are correctly generated. 3) Verify that 'read_resource' successfully returns content for valid files and fails for files outside the allowed directory. 4) Verify the 'llmstxt_list_all_artifacts' tool returns files that were created before the current server process started. 5) Confirm that large files are appropriately truncated according to defined limits.",
        "status": "done",
        "dependencies": [
          "3",
          "5",
          "7",
          "9"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Artifact Directory Scanning Logic",
            "description": "Add a function to discover existing artifact files in the configured output directory.",
            "dependencies": [],
            "details": "Update 'src/llmstxt_mcp/artifacts.py' to include 'scan_artifacts'. Use 'pathlib.Path.glob' to identify all '.txt' files. Ensure the path is resolved relative to the configured artifact directory.",
            "status": "done",
            "testStrategy": "Unit test using a temporary directory populated with several .txt and non-.txt files to ensure only artifacts are listed.",
            "updatedAt": "2026-01-05T06:27:05.271Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Define Artifact Metadata Schemas",
            "description": "Create structured data models for artifact metadata to be used by tools and resources.",
            "dependencies": [
              1
            ],
            "details": "Update 'src/llmstxt_mcp/models.py' to include an 'ArtifactMetadata' model with fields: 'filename', 'size_bytes', 'last_modified', and 'uri'. This ensures consistent formatting between the scanner and the API.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2026-01-05T06:31:45.456Z"
          },
          {
            "id": 3,
            "title": "Implement 'llmstxt_list_all_artifacts' MCP Tool",
            "description": "Expose the discovered artifacts via a specialized MCP tool for client discovery.",
            "dependencies": [
              1,
              2
            ],
            "details": "In 'src/llmstxt_mcp/server.py', register a new tool using the FastMCP decorator. The tool should call 'scan_artifacts', map results to 'ArtifactMetadata', and return a list of all persistent files including those from previous sessions.",
            "status": "done",
            "testStrategy": "Manual verification via MCP Inspector by calling the tool and checking if it lists files manually added to the artifacts folder.",
            "parentId": "undefined",
            "updatedAt": "2026-01-05T06:32:30.942Z"
          },
          {
            "id": 4,
            "title": "Implement MCP Resource Handlers",
            "description": "Register dynamic resource listing and reading handlers for the artifact URI scheme.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement '@mcp.list_resources()' and '@mcp.read_resource()' in 'server.py'. Define the URI scheme as 'llmstxt://artifacts/{filename}'. Ensure 'list_resources' populates the list based on current disk state.",
            "status": "done",
            "testStrategy": "Use an MCP client to list resources and verify the URIs match the expected scheme. Attempt to read a specific resource by URI.",
            "parentId": "undefined",
            "updatedAt": "2026-01-05T06:32:30.948Z"
          },
          {
            "id": 5,
            "title": "Integrate Security and Content Truncation",
            "description": "Apply path validation and length limits to artifact resource retrieval.",
            "dependencies": [
              4
            ],
            "details": "In the 'read_resource' handler, wrap file access with 'security.validate_output_dir'. Use 'hashing.read_text_preview' to read content, passing the 'MAX_CHARS' limit from the application configuration to prevent memory issues.",
            "status": "done",
            "testStrategy": "Attempt to access a file outside the artifacts directory via the resource URI to verify rejection. Verify that files exceeding MAX_CHARS are correctly truncated with a preview banner.",
            "parentId": "undefined",
            "updatedAt": "2026-01-05T06:32:30.951Z"
          }
        ],
        "complexity": 4,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Please break Task 12 into subtasks covering: 1) Implementation of 'scan_artifacts' in 'artifacts.py' using pathlib. 2) Updating 'models.py' to include schemas for artifact metadata. 3) Implementing the 'llmstxt_list_all_artifacts' tool in 'server.py'. 4) Implementing MCP resource handlers ('list_resources' and 'read_resource') for the artifact URI scheme. 5) Integration of 'security.validate_output_dir' and 'hashing.read_text_preview' within the resource read flow.",
        "updatedAt": "2026-01-05T06:32:30.951Z"
      },
      {
        "id": "13",
        "title": "Migrate Artifact Directory Path from 'output' to 'artifacts'",
        "description": "Update the default storage and retrieval path for MCP artifacts from './output' to './artifacts' in configuration and tool implementations.",
        "details": "This task involves standardizing the naming convention for the artifact storage directory. 1) In 'src/llmstxt_mcp/config.py', locate the default configuration variable (likely 'DEFAULT_OUTPUT_DIR' or 'OUTPUT_PATH') and update its value to 'artifacts'. 2) In 'src/llmstxt_mcp/server.py', verify that the 'list_all' and 'read_artifacts' tools utilize the updated configuration rather than hardcoded paths. 3) Update any tool descriptions or help text in the MCP server metadata that reference the directory name to ensure they reflect the change. 4) Verify that the logic in 'artifacts.py' (implemented in Task 5 and 12) correctly inherits this change via the config module, ensuring that path resolution via 'pathlib' correctly points to the new directory. 5) Ensure that the generator integration from Task 6 now writes files to the 'artifacts' directory.",
        "testStrategy": "1) Delete any existing './output' directory. 2) Run the MCP server and trigger an artifact generation; verify that the './artifacts' directory is created and contains the new file. 3) Use an MCP client to call 'list_all' and verify it lists files within 'artifacts/'. 4) Call 'read_artifacts' with a filename in the new directory and verify successful content retrieval. 5) Confirm no new files are being written to './output'.",
        "status": "done",
        "dependencies": [
          "5",
          "6",
          "12"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Update Default Directory in Configuration",
            "description": "Modify 'src/llmstxt_mcp/config.py' to change the default artifact directory path from 'output' to 'artifacts'.",
            "dependencies": [],
            "details": "Locate the constant or Pydantic field defining the output path (e.g., 'DEFAULT_OUTPUT_DIR' or 'OUTPUT_PATH') and update its default value to 'artifacts'. Ensure any environment variable mapping is preserved.",
            "status": "done",
            "testStrategy": "Unit test 'config.py' to ensure that the settings object returns 'artifacts' as the default directory path when no environment variable is set.",
            "updatedAt": "2026-01-05T07:16:07.944Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Update MCP Server Tool Metadata",
            "description": "Update tool descriptions and help text in 'src/llmstxt_mcp/server.py' to reference the new directory name.",
            "dependencies": [
              1
            ],
            "details": "Search for string literals or docstrings in 'server.py' that mention the 'output' directory. Update these to 'artifacts' so that MCP client UI reflects the correct path in help text for tools like 'list_all' and 'llmstxt_generate'.",
            "status": "done",
            "testStrategy": "Examine the MCP server metadata (via 'mcp list-tools') to verify that descriptions for artifact-related tools correctly mention 'artifacts'.",
            "parentId": "undefined",
            "updatedAt": "2026-01-05T07:16:07.950Z"
          },
          {
            "id": 3,
            "title": "Align Artifact Discovery and Resource Logic",
            "description": "Ensure 'src/llmstxt_mcp/artifacts.py' correctly uses the updated config for file scanning and URI generation.",
            "dependencies": [
              1
            ],
            "details": "Verify 'scan_artifacts' uses 'config.output_path' instead of hardcoded strings. Update the resource URI logic (e.g., 'llmstxt://artifacts/{filename}') to ensure internal path resolution points to the new directory on disk.",
            "status": "done",
            "testStrategy": "Verify that 'scan_artifacts' successfully finds files placed manually in './artifacts/' and that 'list_resources' returns URIs correctly mapped to that location.",
            "parentId": "undefined",
            "updatedAt": "2026-01-05T07:16:07.953Z"
          },
          {
            "id": 4,
            "title": "Update Generator Execution Integration",
            "description": "Update the 'llmstxt_generate' tool implementation to write generated files to the 'artifacts' directory.",
            "dependencies": [
              1
            ],
            "details": "In 'src/llmstxt_mcp/server.py', ensure the call to 'run_generation' (or the internal generator wrapper) uses the path defined in the configuration. Confirm that file write operations target the updated directory.",
            "status": "done",
            "testStrategy": "Run a generation task through the MCP server and check that the resulting .txt files appear in './artifacts/' and NOT in './output/'.",
            "parentId": "undefined",
            "updatedAt": "2026-01-05T07:16:07.955Z"
          },
          {
            "id": 5,
            "title": "Validate Migration and Cleanup",
            "description": "Perform a full system check of the migration and remove legacy directory references.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Delete the old './output' directory. Run the server and confirm all artifact-related operations (generate, list, read) function correctly using the new path. Update any README or documentation that mentions the folder structure.",
            "status": "done",
            "testStrategy": "End-to-end verification: 1) Generation creates './artifacts'. 2) 'list_resources' identifies the file. 3) 'read_resource' retrieves the file content successfully from the new path.",
            "parentId": "undefined",
            "updatedAt": "2026-01-05T07:16:07.958Z"
          }
        ],
        "updatedAt": "2026-01-05T07:16:07.958Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2026-01-05T07:16:07.959Z",
      "taskCount": 13,
      "completedCount": 13,
      "tags": [
        "feat-mcp"
      ]
    }
  }
}