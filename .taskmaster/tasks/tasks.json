{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Clean Dependencies and Normalize Names in pyproject.toml",
        "description": "Remove direct VCS URL dependencies and normalize dependency names to canonical PyPI distribution names to ensure PyPI compliance.",
        "details": "1. Modify `pyproject.toml`. \n2. Identify the `llm-ctx` dependency currently defined as `llm-ctx @ git+...`.\n3. Replace it with a valid PyPI version constraint (e.g., `>=x.y.z` if available on PyPI) or a specific version. If it's not on PyPI, this task blocks until a strategy (vendor or PyPI release of dependency) is decided, but assuming availability or replacement, update it.\n4. Scan for other dependencies and ensure they use canonical names (e.g., change `llms_txt` to `llms-txt` if applicable).\n5. Verify `uv lock` or `pip install` works with the new configuration.",
        "testStrategy": "Run `uv build` to ensure metadata is generated. Run `pip install .` in a fresh environment to verify dependencies resolve without git protocols.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Update pyproject.toml Dependencies",
            "description": "Modify the pyproject.toml file to replace direct Git URLs with standard PyPI version constraints and normalize dependency names.",
            "dependencies": [],
            "details": "Open `pyproject.toml`. Locate the dependency `llm-ctx`. Replace the `git+https://...` syntax with a standard version specifier (e.g., `^0.2.0` or `>=0.2.0`). Additionally, review all other dependencies in the `[project.dependencies]` section. Convert names like `llms_txt` to their canonical form `llms-txt` to avoid ambiguous distribution name warnings during build.",
            "status": "pending",
            "testStrategy": "Manual inspection of pyproject.toml to confirm no URL dependencies remain."
          },
          {
            "id": 2,
            "title": "Validate Dependency Configuration and Build",
            "description": "Verify that the updated dependency configuration allows for a successful build and installation in a clean environment.",
            "dependencies": [
              1
            ],
            "details": "Create a fresh virtual environment. Run `pip install .` (or `uv pip install .`) to confirm that the package installs with the new PyPI-based dependencies. Run a build command (e.g., `python -m build` or `uv build`) and inspect the generated metadata (PKG-INFO) in the source distribution to ensure `Requires-Dist` fields do not contain direct URL references.",
            "status": "pending",
            "testStrategy": "Successful execution of build commands and installation without resolution errors."
          }
        ]
      },
      {
        "id": 2,
        "title": "Configure Setuptools for src Layout Discovery",
        "description": "Explicitly configure package discovery in pyproject.toml to ensure code under `src/` is correctly included in the built wheel.",
        "details": "1. Edit `pyproject.toml`.\n2. Ensure `[tool.setuptools.packages.find]` is present.\n3. Set `where = [\"src\"]`.\n4. Verify `[project.scripts]` points to the correct module path (e.g., `lmstxt = \"lms_lmstxt.cli:main\"`).\n5. Ensure `[build-system]` is correctly defined (usually `setuptools` and `wheel` or `hatchling` etc., stick to existing but ensure config matches layout).",
        "testStrategy": "Run `uv build`. Inspect the generated wheel content (using `unzip -l dist/*.whl`) to verify `lms_lmstxt` directory exists in the root of the wheel.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze Project Structure and Current Configuration",
            "description": "Inspect the file system to verify the 'src' directory existence and read the current 'pyproject.toml' to assess existing build settings.",
            "dependencies": [],
            "details": "Use glob/ls to confirm `src/lms_lmstxt` (or similar) exists. Read `pyproject.toml` to identify the current `[build-system]` and any existing `[tool.setuptools]` configuration to determine the necessary changes.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 2,
            "title": "Configure Build System Backend",
            "description": "Ensure 'pyproject.toml' defines a standard build backend compatible with setuptools and src-layout.",
            "dependencies": [
              1
            ],
            "details": "Edit `pyproject.toml` to verify or add the `[build-system]` table. Ensure `requires` includes `[\"setuptools\", \"wheel\"]` (and `setuptools-scm` if dynamic versioning is used) and `build-backend` is set to `\"setuptools.build_meta\"`.",
            "status": "pending",
            "testStrategy": "Run `uv build` (or `python -m build`) to confirm the build backend initializes correctly."
          },
          {
            "id": 3,
            "title": "Implement Explicit Package Discovery",
            "description": "Add the specific setuptools configuration to restrict package discovery to the 'src' directory.",
            "dependencies": [
              2
            ],
            "details": "In `pyproject.toml`, add or update the `[tool.setuptools.packages.find]` table. Set `where = [\"src\"]` to ensure only the code inside `src/` is packaged, preventing accidental inclusion of root-level scripts or config files.",
            "status": "pending",
            "testStrategy": "None (Verification happens in the final build step)."
          },
          {
            "id": 4,
            "title": "Configure Entry Points",
            "description": "Verify and update the CLI entry point mapping in 'pyproject.toml' to point to the correct location within the src layout.",
            "dependencies": [
              3
            ],
            "details": "Check `[project.scripts]`. Ensure the command (e.g., `lmstxt`) maps to the correct module path, likely `lms_lmstxt.cli:main` or similar. If the package name changed or moved to src, ensure the import path is valid.",
            "status": "pending",
            "testStrategy": "Run `pip install -e .` and check if the `lmstxt` command is available and executable."
          },
          {
            "id": 5,
            "title": "Verify Wheel Structure",
            "description": "Build the distribution package and inspect the contents to confirm the 'src' layout is correctly flattened in the wheel.",
            "dependencies": [
              4
            ],
            "details": "Run `uv build`. Use `unzip -l dist/*.whl` to list the files. Confirm that the top-level directory in the wheel is the package folder (e.g., `lms_lmstxt/`), NOT `src/lms_lmstxt/` and NOT containing extraneous root files.",
            "status": "pending",
            "testStrategy": "Automated script or manual check using unzip/tar to validate file hierarchy inside the artifact."
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Artifact Metadata Validation Script",
        "description": "Create a script or makefile target to validate that built artifacts have correct metadata and are well-formed.",
        "details": "1. Create `scripts/release/validate_metadata.py` or similar.\n2. The script should run `twine check dist/*` (or equivalent `uv` command if available) to ensure README renders correctly and metadata is valid.\n3. Ensure it checks for the absence of direct URL dependencies in the metadata `Requires-Dist` fields.",
        "testStrategy": "Build artifacts with known bad metadata (e.g., direct URL) and ensure script fails. Build clean artifacts and ensure script passes.",
        "priority": "medium",
        "dependencies": [
          2
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Validation Script and Directory Structure",
            "description": "Create the scripts directory and the initial Python script structure for metadata validation.",
            "dependencies": [],
            "details": "Create directory `scripts/release/` if it does not exist. Create `scripts/release/validate_metadata.py`. Set up standard Python boilerplate (argparse to accept dist directory path, logging setup).",
            "status": "pending",
            "testStrategy": "Run `python scripts/release/validate_metadata.py --help` to verify it runs without errors."
          },
          {
            "id": 2,
            "title": "Implement Standard Twine Check Integration",
            "description": "Integrate `twine check` execution into the validation script to verify standard metadata compliance.",
            "dependencies": [
              1
            ],
            "details": "In `validate_metadata.py`, use `subprocess` to run `twine check` on the target artifacts. Ensure the script captures stdout/stderr and propagates the exit code if twine detects invalid reStructuredText or missing fields.",
            "status": "pending",
            "testStrategy": "Build a valid artifact using `uv build` and run the script; ensure it passes the twine check step."
          },
          {
            "id": 3,
            "title": "Implement Metadata Extraction Logic",
            "description": "Add functionality to extract metadata files from Wheel and Source distributions.",
            "dependencies": [
              1
            ],
            "details": "Using `zipfile` for `.whl` and `tarfile` for `.tar.gz`, implement functions to locate and read the `METADATA` (wheel) or `PKG-INFO` (sdist) files from the artifacts in the dist directory.",
            "status": "pending",
            "testStrategy": "Unit test the extraction function with a sample built artifact to ensure it returns the raw metadata string."
          },
          {
            "id": 4,
            "title": "Implement Direct URL Dependency Validation",
            "description": "Parse metadata to detect and reject direct URL dependencies in `Requires-Dist`.",
            "dependencies": [
              3
            ],
            "details": "Parse the extracted metadata headers. Iterate through `Requires-Dist` entries. Implement a regex or parsing check (e.g. using `packaging.requirements`) to identify if any dependency specifies a direct URL (e.g., `@ http://...` or `git+https://...`). Raise an error if found.",
            "status": "pending",
            "testStrategy": "Create a dummy wheel with a direct URL dependency, run the script, and assert that it fails with a specific error message."
          },
          {
            "id": 5,
            "title": "Finalize Script Execution Flow and CI Integration",
            "description": "Combine checks into a main execution loop and handle overall exit codes.",
            "dependencies": [
              2,
              4
            ],
            "details": "Orchestrate the flow: 1. Find all files in `dist/`. 2. Run Twine check. 3. Run Metadata/URL check. 4. Aggregate results. If any check fails, print clear errors and exit with non-zero status code. Update `pyproject.toml` or `Makefile` (if exists) to include a target for this script.",
            "status": "pending",
            "testStrategy": "Run the full script against a clean build (`uv build`) to confirm zero exit code, then against a corrupted build to confirm non-zero exit code."
          }
        ]
      },
      {
        "id": 4,
        "title": "Create Smoke Test Suite for Artifacts",
        "description": "Develop a smoke test script that installs built artifacts (wheel/sdist) and verifies the CLI runs.",
        "details": "1. Create `tests/smoke_test.py`.\n2. Implement `test_wheel_install()`: Creates a venv, installs the `.whl` from `dist/`, and runs `lmstxt --help`.\n3. Implement `test_sdist_install()`: Creates a venv, installs the `.tar.gz` from `dist/`, and runs `lmstxt --help`.\n4. Use `subprocess` to manage venv creation and command execution to ensure total isolation.",
        "testStrategy": "Run `pytest tests/smoke_test.py` (or execute directly `python tests/smoke_test.py`) after running `uv build`. It should pass only if artifacts are valid.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize smoke test script with venv management helpers",
            "description": "Set up the test file and utilities for isolating installations in temporary virtual environments.",
            "dependencies": [],
            "details": "Create `tests/smoke_test.py`. Implement helper functions using `tempfile`, `venv`, and `subprocess` to generate throwaway virtual environments. Create a utility function `run_in_venv(venv_path, command)` to execute shell commands (like `pip install` or the CLI executable) specifically within the context of the created environment.",
            "status": "pending",
            "testStrategy": "Execute the script to verify the helper function successfully creates a directory containing a valid Python executable."
          },
          {
            "id": 2,
            "title": "Implement artifact installation tests for Wheel and Sdist",
            "description": "Write the specific test cases that install the build artifacts and verify the CLI entry point runs correctly.",
            "dependencies": [
              1
            ],
            "details": "In `tests/smoke_test.py`, implement `test_wheel_install()` and `test_sdist_install()`. The logic must: 1. Locate the `.whl` and `.tar.gz` files in `dist/`. 2. Invoke the venv helper. 3. Install the artifact into the venv. 4. Execute `lmstxt --help` and assert exit code 0. Add a `if __name__ == '__main__':` block to run these tests.",
            "status": "pending",
            "testStrategy": "Run `uv build` to generate artifacts, then execute `python tests/smoke_test.py`. Expect success output for both installation types."
          }
        ]
      },
      {
        "id": 5,
        "title": "Update TestPyPI Publishing Workflow",
        "description": "Refine the GitHub Action for TestPyPI to use `uv`, proper secrets, and allow manual dispatch.",
        "details": "1. Edit `.github/workflows/publish-testpypi.yml`.\n2. Ensure `on: workflow_dispatch` with input `release_tag` (or ref).\n3. Use `uv build` to generate artifacts.\n4. Use `uv publish` (or `twine`) targeting `https://test.pypi.org/legacy/`.\n5. Map `UV_PUBLISH_TOKEN` or `TWINE_PASSWORD` to the repo secret `TEST_PYPI_TOKEN`.\n6. Ensure it runs the smoke test (Task 4) before publishing.",
        "testStrategy": "Trigger the workflow manually on a branch. Verify artifacts appear on TestPyPI.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze and Prepare Workflow Structure",
            "description": "Examine the existing GitHub Actions workflows to determine the correct structure for the TestPyPI publishing workflow.",
            "dependencies": [],
            "details": "Check for existing workflows in `.github/workflows/` like `publish-testpypi.yml` or `ci.yml`. Create or update `.github/workflows/publish-testpypi.yml` to define the basic structure, ensuring permissions are set correctly (id-token: write for future OIDC or just contents: read) and define the `workflow_dispatch` trigger with a `release_tag` input option.",
            "status": "pending",
            "testStrategy": "Validate YAML syntax using a linter."
          },
          {
            "id": 2,
            "title": "Implement UV Setup and Build Steps",
            "description": "Configure the workflow to set up Python and install uv, then build the package distribution artifacts.",
            "dependencies": [
              1
            ],
            "details": "In the `publish-testpypi.yml` workflow, add steps to: 1. Checkout code (using `actions/checkout@v4`). 2. Set up Python (using `actions/setup-python@v5`). 3. Install `uv` (e.g., via `pip install uv` or the official action if available/preferred). 4. Run `uv build` to generate sdist and wheel files in the `dist/` directory.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 3,
            "title": "Integrate Smoke Tests into Workflow",
            "description": "Add a step to run the project's smoke tests before publishing to ensure artifact integrity.",
            "dependencies": [
              2
            ],
            "details": "Add a step after the build process but before publishing. This step should create a fresh virtual environment using `uv venv`, install the built wheel from `dist/` (e.g., `uv pip install dist/*.whl`), and run the basic CLI command or import test defined in Task 4 (e.g., `lmstxt --help` or a specific script).",
            "status": "pending",
            "testStrategy": "Workflow fails if the smoke test command returns a non-zero exit code."
          },
          {
            "id": 4,
            "title": "Configure UV Publish to TestPyPI",
            "description": "Add the publishing step using `uv publish` targeting the TestPyPI repository with proper authentication.",
            "dependencies": [
              3
            ],
            "details": "Add the publish step: `uv publish --publish-url https://test.pypi.org/legacy/ dist/*`. Configure authentication by mapping the `UV_PUBLISH_TOKEN` environment variable to the `${{ secrets.TEST_PYPI_TOKEN }}` GitHub secret. Ensure this step only runs if the smoke test passes.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 5,
            "title": "Final Review and Workflow Validation",
            "description": "Verify the complete workflow configuration and document the dispatch parameters.",
            "dependencies": [
              4
            ],
            "details": "Review the final `.github/workflows/publish-testpypi.yml` file to ensure all indentation is correct, secrets are referenced properly, and the `workflow_dispatch` inputs are correctly defined. Verify that the `release_tag` input is used to checkout the specific ref if provided, or default to the current branch.",
            "status": "pending",
            "testStrategy": "Manually trigger the workflow from the GitHub Actions UI on a feature branch to verify it passes up to the publish step (which might fail if secrets aren't set in the fork, but the logic can be verified)."
          }
        ]
      },
      {
        "id": 6,
        "title": "Configure PyPI Trusted Publishing Workflow",
        "description": "Update the release workflow to use OIDC for Trusted Publishing to PyPI on tag push.",
        "details": "1. Edit `.github/workflows/release.yml`.\n2. Set permission `id-token: write`.\n3. Configure environment `name: pypi` and `url: https://pypi.org/p/lmstudio-lmstxt-generator`.\n4. Update the publish step to use `uv publish --trusted-publishing` (or `gh-action-pypi-publish`).\n5. Ensure it triggers on `push: tags: [ 'v*' ]`.\n6. Integrate the smoke test step before the publish job.",
        "testStrategy": "Review workflow syntax using a linter. Actual verification requires pushing a tag (or testing in a fork with TestPyPI configured as the trusted target).",
        "priority": "high",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze and Prepare Release Workflow Structure",
            "description": "Examine the existing .github/workflows directory and prepare the structure for release.yml with required OIDC permissions.",
            "dependencies": [],
            "details": "Check for existing release.yml. If present, review current steps. If absent, create a new file. Define the trigger on 'push' for tags matching 'v*'. Crucially, add the top-level 'permissions' block setting 'id-token: write' to enable OIDC authentication for Trusted Publishing.",
            "status": "pending",
            "testStrategy": "Review the workflow file syntax to ensure permissions are correctly scoped."
          },
          {
            "id": 2,
            "title": "Configure Build Job with UV",
            "description": "Define the build job in release.yml to checkout code, set up Python, and build artifacts using uv.",
            "dependencies": [
              1
            ],
            "details": "In release.yml, add a 'build' job. Steps: 1. Checkout code (v4). 2. Install uv. 3. Setup Python (v5). 4. Run 'uv build'. 5. Upload artifacts (dist/*) using actions/upload-artifact@v4 so they can be used by subsequent jobs (smoke test and publish).",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 3,
            "title": "Integrate Smoke Test Job",
            "description": "Add a smoke test job that runs before publishing to verify the build artifacts work as expected.",
            "dependencies": [
              2
            ],
            "details": "Create a 'smoke-test' job in release.yml that 'needs: build'. Steps: 1. Download artifacts from the build job. 2. Install uv/Python. 3. Run the smoke test suite (e.g., 'python tests/smoke_test.py' or 'pytest tests/smoke_test.py') against the downloaded wheel to ensure viability before publishing.",
            "status": "pending",
            "testStrategy": "Ensure the job fails if the smoke test script exits with a non-zero code."
          },
          {
            "id": 4,
            "title": "Configure PyPI Publish Job with Trusted Publishing",
            "description": "Define the publish job using OIDC authentication targeting the PyPI environment.",
            "dependencies": [
              3
            ],
            "details": "Add a 'publish' job that 'needs: smoke-test'. Define 'environment' with name 'pypi' and url 'https://pypi.org/p/lmstudio-lmstxt-generator'. Use 'pypa/gh-action-pypi-publish@release/v1' (recommended for Trusted Publishing) or 'uv publish --trusted-publishing' if supported. Do NOT use username/password secrets.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 5,
            "title": "Finalize and Validate Workflow Configuration",
            "description": "Combine all jobs into the final workflow file and perform static validation.",
            "dependencies": [
              4
            ],
            "details": "Ensure all jobs (build, smoke-test, publish) are correctly linked with 'needs'. Verify the environment URL matches the specific PyPI project. Commit the .github/workflows/release.yml file. Check that 'contents: read' permission is also present alongside 'id-token: write'.",
            "status": "pending",
            "testStrategy": "Run a linter (e.g., action-validator) on the YAML file locally if possible."
          }
        ]
      },
      {
        "id": 7,
        "title": "Consolidate Release Runbook",
        "description": "Create a comprehensive document describing the release process for maintainers.",
        "details": "1. Create `docs/publishing.md`.\n2. Document prerequisites (PyPI account, Trusted Publisher setup logic).\n3. Document the 'One Command' release flow (tagging).\n4. Document the TestPyPI manual trigger flow.\n5. Include troubleshooting steps for common errors (e.g., metadata rejected).\n6. Link to the CI workflows.",
        "testStrategy": "Peer review the markdown file. Verify links work.",
        "priority": "medium",
        "dependencies": [
          5,
          6
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Establish Documentation Structure and Prerequisites",
            "description": "Create the `docs/publishing.md` file and document the initial setup requirements for maintainers.",
            "dependencies": [],
            "details": "Create a new markdown file at `docs/publishing.md`. Add a 'Prerequisites' section detailing the need for a PyPI account and the specific configuration required for Trusted Publisher setup (OIDC) between GitHub and PyPI. Explain how maintainers should verify their permissions before attempting a release.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 2,
            "title": "Document the Production Release Workflow",
            "description": "Detail the standard 'One Command' release process triggered by git tags.",
            "dependencies": [
              1
            ],
            "details": "In `docs/publishing.md`, describe the production release flow. Explain the versioning convention (Semantic Versioning), how to create a git tag (e.g., `git tag v1.0.0`), and how pushing this tag triggers the GitHub Action defined in `.github/workflows/release.yml`. Include a step-by-step guide for the maintainer to execute this flow.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 3,
            "title": "Document the TestPyPI Workflow",
            "description": "Explain the manual trigger process for deploying to TestPyPI for validation.",
            "dependencies": [
              1
            ],
            "details": "Add a section to `docs/publishing.md` covering the TestPyPI release process. Reference the manual dispatch trigger configured in the CI workflow. Provide instructions on how to use the GitHub UI 'Run workflow' button, input necessary parameters (if any), and verify the artifact on `test.pypi.org`.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 4,
            "title": "Compile Troubleshooting Guide",
            "description": "Create a section for resolving common release failures and errors.",
            "dependencies": [
              2,
              3
            ],
            "details": "Add a 'Troubleshooting' section to the documentation. Address common issues such as 'Metadata Rejected' (often due to version conflicts or invalid `pyproject.toml` configuration), OIDC token failures, and smoke test failures. Provide resolution steps for each scenario, including how to clean up failed tags if necessary.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 5,
            "title": "Finalize References and CI Links",
            "description": "Add direct links to workflows and review the document for completeness.",
            "dependencies": [
              4
            ],
            "details": "Conclude the document by adding direct links to the repository's 'Actions' tab and the specific workflow files (`release.yml`, etc.). Perform a self-review to ensure all paths, commands, and URLs are accurate relative to the project root. Ensure the document is linked from the main `README.md` or `CONTRIBUTING.md` if they exist.",
            "status": "pending",
            "testStrategy": "Peer review the markdown file. Verify all hyperlinks are functional."
          }
        ]
      },
      {
        "id": 8,
        "title": "Add Pre-commit/Local Build Check Script",
        "description": "Create a helper script for developers to run the full build-verify-smoke cycle locally.",
        "details": "1. Create `scripts/release/verify_install.sh` (or `local_check.sh`).\n2. The script should: Clean `dist/`, run `uv build`, run `validate_metadata.py`, and run `smoke_test.py`.\n3. Make it executable.",
        "testStrategy": "Run `./scripts/release/verify_install.sh` locally and ensure it passes when the repo is clean.",
        "priority": "low",
        "dependencies": [
          3,
          4
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Verify Install Script Skeleton",
            "description": "Initialize the verify_install.sh bash script with strict error handling and basic environment checks.",
            "dependencies": [],
            "details": "Create the directory `scripts/release` if it does not exist. Create the file `scripts/release/verify_install.sh` with the shebang `#!/bin/bash` and `set -euo pipefail` to ensure the script exits immediately on error. Add a check to verify that the `uv` command is installed and available in the system PATH.",
            "status": "pending",
            "testStrategy": "Run the script; verify it exists, errors if `uv` is missing, or exits cleanly if nothing else is added yet."
          },
          {
            "id": 2,
            "title": "Implement Clean and Build Logic",
            "description": "Add functionality to clean old artifacts and rebuild the project using uv.",
            "dependencies": [
              1
            ],
            "details": "Update the script to run `rm -rf dist/` to clean previous builds. Add the `uv build` command. Check that the `dist/` directory is created and contains artifacts (e.g., `.whl`, `.tar.gz`) after the build command runs.",
            "status": "pending",
            "testStrategy": "Run the script. Check that `dist/` is cleared and then repopulated with build artifacts."
          },
          {
            "id": 3,
            "title": "Integrate Metadata Validation Step",
            "description": "Invoke the metadata validation Python script to ensure build artifacts are well-formed.",
            "dependencies": [
              2
            ],
            "details": "Add the command to execute `python scripts/release/validate_metadata.py`. Ensure the script relies on the exit code of this Python script to determine pass/fail status. This ensures no artifacts with invalid metadata (e.g., direct URL dependencies) pass the check.",
            "status": "pending",
            "testStrategy": "Temporarily modify `pyproject.toml` to be invalid, run the script, and ensure it fails at this step."
          },
          {
            "id": 4,
            "title": "Integrate Smoke Test Step",
            "description": "Invoke the smoke test script to verify the basic functionality of the built artifacts.",
            "dependencies": [
              3
            ],
            "details": "Add the command to execute `python scripts/release/smoke_test.py`. This step should occur after metadata validation. It ensures that the package built in the previous steps can be installed and imported successfully.",
            "status": "pending",
            "testStrategy": "Run the script and verify that the smoke test python script is executed effectively."
          },
          {
            "id": 5,
            "title": "Finalize Script Permissions and User Experience",
            "description": "Make the script executable and add user-friendly logging for build phases.",
            "dependencies": [
              4
            ],
            "details": "Run `chmod +x scripts/release/verify_install.sh`. Add `echo` statements with ANSI color codes (e.g., Green for success, Red for failure) to clearly demarcate the Build, Validate, and Smoke Test phases. Add a final success message if all steps pass.",
            "status": "pending",
            "testStrategy": "Run `./scripts/release/verify_install.sh` directly from the terminal and verify the output contains formatted status messages."
          }
        ]
      },
      {
        "id": 9,
        "title": "Verify Dynamic Versioning or Version Bump Policy",
        "description": "Ensure the package version aligns with the Git tag automatically or document the bump process.",
        "details": "1. Check `pyproject.toml` for `dynamic = [\"version\"]` or static version.\n2. If static, update the Runbook (Task 7) to include 'Bump version in pyproject.toml' as a step before tagging.\n3. If dynamic versioning is preferred, configure `setuptools_scm` (or equivalent) in `pyproject.toml` and add it to build-system requirements.",
        "testStrategy": "Build a wheel and check the filename version. Ensure it matches the expected tag/commit version.",
        "priority": "medium",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze pyproject.toml for Current Versioning Configuration",
            "description": "Inspect the pyproject.toml file to determine if the version is statically defined or configured for dynamic discovery.",
            "dependencies": [],
            "details": "Read `pyproject.toml` to check the `[project]` table for a `version` key or `dynamic = [\"version\"]`. Check `[build-system]` and `[tool.setuptools]` (or other backend configs) to understand the current setup.",
            "status": "pending",
            "testStrategy": "None (Analysis task)"
          },
          {
            "id": 2,
            "title": "Implement Dynamic Versioning with setuptools_scm",
            "description": "Configure the project to derive its version automatically from Git tags using setuptools_scm.",
            "dependencies": [
              1
            ],
            "details": "Modify `pyproject.toml` to: 1. Remove static `version`. 2. Add `dynamic = [\"version\"]`. 3. Add `setuptools_scm` to `[build-system] requires`. 4. Add `[tool.setuptools_scm]` section (can be empty or with `write_to` config).",
            "status": "pending",
            "testStrategy": "Run `uv build` (or pip install) locally and verify the generated metadata/wheel version matches `git describe --tags`."
          },
          {
            "id": 3,
            "title": "Ensure __version__ Attribute Availability",
            "description": "Ensure the package version is accessible at runtime via `__version__` without hardcoding it.",
            "dependencies": [
              2
            ],
            "details": "Modify `src/lms_lmstxt/__init__.py` (or equivalent top-level init) to use `importlib.metadata.version` to retrieve the installed package version dynamically, handling the `PackageNotFoundError` gracefully.",
            "status": "pending",
            "testStrategy": "Run a python script importing the package and printing `__version__`. It should match the installed version."
          },
          {
            "id": 4,
            "title": "Update Release Documentation / Runbook",
            "description": "Update the project documentation to reflect the new dynamic versioning workflow.",
            "dependencies": [
              2
            ],
            "details": "Create or update a `RELEASE.md` or the Runbook (from Task 7 context) to explain that tagging a commit triggers the version bump, and manual editing of `pyproject.toml` version is no longer needed.",
            "status": "pending",
            "testStrategy": "Review the documentation for clarity and accuracy."
          },
          {
            "id": 5,
            "title": "Verify Wheel Versioning with Smoke Test",
            "description": "Validate that the build process produces artifacts with the correct version derived from the git context.",
            "dependencies": [
              2
            ],
            "details": "Create a temporary git tag (e.g., `v0.0.1-test`), run `uv build`, and inspect the filename of the generated `.whl` in `dist/`. Ensure it matches the tag. Clean up the tag afterwards.",
            "status": "pending",
            "testStrategy": "Manual verification or script checking `dist/` filenames against `git describe` output."
          }
        ]
      },
      {
        "id": 10,
        "title": "End-to-End Release Simulation",
        "description": "Perform a final validation of the entire pipeline using TestPyPI.",
        "details": "1. Bump version to a dev version (e.g., `0.0.1.dev1`).\n2. Trigger the TestPyPI workflow.\n3. Verify upload.\n4. locally run `pip install -i https://test.pypi.org/simple/ lmstudio-lmstxt-generator`.\n5. Verify `lmstxt --help` works.",
        "testStrategy": "Successful installation and execution from TestPyPI.",
        "priority": "medium",
        "dependencies": [
          5,
          7
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Update Version to Development Release Candidate",
            "description": "Bump the package version in pyproject.toml to a unique development version (e.g., 0.0.1.dev1) to allow a non-conflicting upload to TestPyPI.",
            "dependencies": [],
            "details": "Modify the `version` field in `pyproject.toml`. Append a `.devN` suffix to the current version. This ensures that we can verify the upload mechanism without consuming a 'real' semantic version number on TestPyPI, as PyPI files are immutable.",
            "status": "pending",
            "testStrategy": "Check pyproject.toml content and verify `uv build` produces a file with the new version string."
          },
          {
            "id": 2,
            "title": "Trigger TestPyPI Deployment Workflow",
            "description": "Manually trigger the GitHub Action or CI pipeline configured for TestPyPI deployment using the new development version.",
            "dependencies": [
              1
            ],
            "details": "Navigate to the CI/CD dashboard (e.g., GitHub Actions). Locate the 'Publish to TestPyPI' workflow (created in previous tasks). Trigger it manually or by pushing a specific tag (e.g., `v0.0.1.dev1`) depending on the trigger logic defined in the workflow file.",
            "status": "pending",
            "testStrategy": "Monitor the CI job logs for successful completion, specifically looking for the 'Twine upload' or equivalent step success message."
          },
          {
            "id": 3,
            "title": "Verify TestPyPI Upload Integrity",
            "description": "Confirm that the package artifacts have effectively appeared on the TestPyPI repository page with the correct metadata.",
            "dependencies": [
              2
            ],
            "details": "Visit the project URL on TestPyPI (e.g., `https://test.pypi.org/project/lmstudio-lmstxt-generator/`). Verify that the new version `0.0.1.dev1` is listed, the Release Date is current, and the `Description` (README) renders correctly.",
            "status": "pending",
            "testStrategy": "Manual verification on the TestPyPI website."
          },
          {
            "id": 4,
            "title": "Perform Clean Installation from TestPyPI",
            "description": "Create a fresh virtual environment and install the package exclusively from the TestPyPI index to simulate an end-user installation.",
            "dependencies": [
              3
            ],
            "details": "Create a new virtualenv: `python -m venv venv_test`. Activate it. Run `pip install --no-cache-dir --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ lmstudio-lmstxt-generator==0.0.1.dev1`. Note: `--extra-index-url` is often needed for dependencies that don't exist on TestPyPI.",
            "status": "pending",
            "testStrategy": "Ensure pip completes successfully without errors regarding missing dependencies or hash mismatches."
          },
          {
            "id": 5,
            "title": "Validate Installed CLI Functionality",
            "description": "Execute the installed CLI tool within the test environment to confirm the package structure and entry points function correctly.",
            "dependencies": [
              4
            ],
            "details": "Inside the `venv_test`, run the command `lmstxt --help`. Verify that the help text is displayed, indicating that the entry point scripts were correctly generated and the internal imports are working.",
            "status": "pending",
            "testStrategy": "The command should exit with code 0 and display the standard help message, not an ImportError or ModuleNotFoundError."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-12-29T22:13:30.525Z",
      "updated": "2025-12-29T22:13:30.525Z",
      "description": "Tasks for master context"
    }
  },
  "chore-release": {
    "tasks": [
      {
        "id": 1,
        "title": "Clean Dependencies and Normalize Names in pyproject.toml",
        "description": "Remove direct VCS URL dependencies and normalize dependency names to canonical PyPI distribution names to ensure PyPI compliance.",
        "details": "1. Modify `pyproject.toml`. \n2. Identify the `llm-ctx` dependency currently defined as `llm-ctx @ git+...`.\n3. Replace it with a valid PyPI version constraint (e.g., `>=x.y.z` if available on PyPI) or a specific version. If it's not on PyPI, this task blocks until a strategy (vendor or PyPI release of dependency) is decided, but assuming availability or replacement, update it.\n4. Scan for other dependencies and ensure they use canonical names (e.g., change `llms_txt` to `llms-txt` if applicable).\n5. Verify `uv lock` or `pip install` works with the new configuration.",
        "testStrategy": "Run `uv build` to ensure metadata is generated. Run `pip install .` in a fresh environment to verify dependencies resolve without git protocols.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Update pyproject.toml Dependencies",
            "description": "Modify the pyproject.toml file to replace direct Git URLs with standard PyPI version constraints and normalize dependency names.",
            "dependencies": [],
            "details": "Open `pyproject.toml`. Locate the dependency `llm-ctx`. Replace the `git+https://...` syntax with a standard version specifier (e.g., `^0.2.0` or `>=0.2.0`). Additionally, review all other dependencies in the `[project.dependencies]` section. Convert names like `llms_txt` to their canonical form `llms-txt` to avoid ambiguous distribution name warnings during build.",
            "status": "pending",
            "testStrategy": "Manual inspection of pyproject.toml to confirm no URL dependencies remain.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Validate Dependency Configuration and Build",
            "description": "Verify that the updated dependency configuration allows for a successful build and installation in a clean environment.",
            "dependencies": [
              1
            ],
            "details": "Create a fresh virtual environment. Run `pip install .` (or `uv pip install .`) to confirm that the package installs with the new PyPI-based dependencies. Run a build command (e.g., `python -m build` or `uv build`) and inspect the generated metadata (PKG-INFO) in the source distribution to ensure `Requires-Dist` fields do not contain direct URL references.",
            "status": "pending",
            "testStrategy": "Successful execution of build commands and installation without resolution errors.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-30T00:12:38.308Z"
      },
      {
        "id": 2,
        "title": "Configure Setuptools for src Layout Discovery",
        "description": "Explicitly configure package discovery in pyproject.toml to ensure code under `src/` is correctly included in the built wheel.",
        "details": "1. Edit `pyproject.toml`.\n2. Ensure `[tool.setuptools.packages.find]` is present.\n3. Set `where = [\"src\"]`.\n4. Verify `[project.scripts]` points to the correct module path (e.g., `lmstxt = \"lms_lmstxt.cli:main\"`).\n5. Ensure `[build-system]` is correctly defined (usually `setuptools` and `wheel` or `hatchling` etc., stick to existing but ensure config matches layout).",
        "testStrategy": "Run `uv build`. Inspect the generated wheel content (using `unzip -l dist/*.whl`) to verify `lms_lmstxt` directory exists in the root of the wheel.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze Project Structure and Current Configuration",
            "description": "Inspect the file system to verify the 'src' directory existence and read the current 'pyproject.toml' to assess existing build settings.",
            "dependencies": [],
            "details": "Use glob/ls to confirm `src/lms_lmstxt` (or similar) exists. Read `pyproject.toml` to identify the current `[build-system]` and any existing `[tool.setuptools]` configuration to determine the necessary changes.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Configure Build System Backend",
            "description": "Ensure 'pyproject.toml' defines a standard build backend compatible with setuptools and src-layout.",
            "dependencies": [
              1
            ],
            "details": "Edit `pyproject.toml` to verify or add the `[build-system]` table. Ensure `requires` includes `[\"setuptools\", \"wheel\"]` (and `setuptools-scm` if dynamic versioning is used) and `build-backend` is set to `\"setuptools.build_meta\"`.",
            "status": "pending",
            "testStrategy": "Run `uv build` (or `python -m build`) to confirm the build backend initializes correctly.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Explicit Package Discovery",
            "description": "Add the specific setuptools configuration to restrict package discovery to the 'src' directory.",
            "dependencies": [
              2
            ],
            "details": "In `pyproject.toml`, add or update the `[tool.setuptools.packages.find]` table. Set `where = [\"src\"]` to ensure only the code inside `src/` is packaged, preventing accidental inclusion of root-level scripts or config files.",
            "status": "pending",
            "testStrategy": "None (Verification happens in the final build step).",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Configure Entry Points",
            "description": "Verify and update the CLI entry point mapping in 'pyproject.toml' to point to the correct location within the src layout.",
            "dependencies": [
              3
            ],
            "details": "Check `[project.scripts]`. Ensure the command (e.g., `lmstxt`) maps to the correct module path, likely `lms_lmstxt.cli:main` or similar. If the package name changed or moved to src, ensure the import path is valid.",
            "status": "pending",
            "testStrategy": "Run `pip install -e .` and check if the `lmstxt` command is available and executable.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Verify Wheel Structure",
            "description": "Build the distribution package and inspect the contents to confirm the 'src' layout is correctly flattened in the wheel.",
            "dependencies": [
              4
            ],
            "details": "Run `uv build`. Use `unzip -l dist/*.whl` to list the files. Confirm that the top-level directory in the wheel is the package folder (e.g., `lms_lmstxt/`), NOT `src/lms_lmstxt/` and NOT containing extraneous root files.",
            "status": "pending",
            "testStrategy": "Automated script or manual check using unzip/tar to validate file hierarchy inside the artifact.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-30T00:13:30.433Z"
      },
      {
        "id": 3,
        "title": "Implement Artifact Metadata Validation Script",
        "description": "Create a script or makefile target to validate that built artifacts have correct metadata and are well-formed.",
        "details": "1. Create `scripts/release/validate_metadata.py` or similar.\n2. The script should run `twine check dist/*` (or equivalent `uv` command if available) to ensure README renders correctly and metadata is valid.\n3. Ensure it checks for the absence of direct URL dependencies in the metadata `Requires-Dist` fields.",
        "testStrategy": "Build artifacts with known bad metadata (e.g., direct URL) and ensure script fails. Build clean artifacts and ensure script passes.",
        "priority": "medium",
        "dependencies": [
          "2"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Validation Script and Directory Structure",
            "description": "Create the scripts directory and the initial Python script structure for metadata validation.",
            "dependencies": [],
            "details": "Create directory `scripts/release/` if it does not exist. Create `scripts/release/validate_metadata.py`. Set up standard Python boilerplate (argparse to accept dist directory path, logging setup).",
            "status": "pending",
            "testStrategy": "Run `python scripts/release/validate_metadata.py --help` to verify it runs without errors.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Standard Twine Check Integration",
            "description": "Integrate `twine check` execution into the validation script to verify standard metadata compliance.",
            "dependencies": [
              1
            ],
            "details": "In `validate_metadata.py`, use `subprocess` to run `twine check` on the target artifacts. Ensure the script captures stdout/stderr and propagates the exit code if twine detects invalid reStructuredText or missing fields.",
            "status": "pending",
            "testStrategy": "Build a valid artifact using `uv build` and run the script; ensure it passes the twine check step.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Metadata Extraction Logic",
            "description": "Add functionality to extract metadata files from Wheel and Source distributions.",
            "dependencies": [
              1
            ],
            "details": "Using `zipfile` for `.whl` and `tarfile` for `.tar.gz`, implement functions to locate and read the `METADATA` (wheel) or `PKG-INFO` (sdist) files from the artifacts in the dist directory.",
            "status": "pending",
            "testStrategy": "Unit test the extraction function with a sample built artifact to ensure it returns the raw metadata string.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement Direct URL Dependency Validation",
            "description": "Parse metadata to detect and reject direct URL dependencies in `Requires-Dist`.",
            "dependencies": [
              3
            ],
            "details": "Parse the extracted metadata headers. Iterate through `Requires-Dist` entries. Implement a regex or parsing check (e.g. using `packaging.requirements`) to identify if any dependency specifies a direct URL (e.g., `@ http://...` or `git+https://...`). Raise an error if found.",
            "status": "pending",
            "testStrategy": "Create a dummy wheel with a direct URL dependency, run the script, and assert that it fails with a specific error message.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Finalize Script Execution Flow and CI Integration",
            "description": "Combine checks into a main execution loop and handle overall exit codes.",
            "dependencies": [
              2,
              4
            ],
            "details": "Orchestrate the flow: 1. Find all files in `dist/`. 2. Run Twine check. 3. Run Metadata/URL check. 4. Aggregate results. If any check fails, print clear errors and exit with non-zero status code. Update `pyproject.toml` or `Makefile` (if exists) to include a target for this script.",
            "status": "pending",
            "testStrategy": "Run the full script against a clean build (`uv build`) to confirm zero exit code, then against a corrupted build to confirm non-zero exit code.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-30T00:14:42.421Z"
      },
      {
        "id": 4,
        "title": "Create Smoke Test Suite for Artifacts",
        "description": "Develop a smoke test script that installs built artifacts (wheel/sdist) and verifies the CLI runs.",
        "details": "1. Create `tests/smoke_test.py`.\n2. Implement `test_wheel_install()`: Creates a venv, installs the `.whl` from `dist/`, and runs `lmstxt --help`.\n3. Implement `test_sdist_install()`: Creates a venv, installs the `.tar.gz` from `dist/`, and runs `lmstxt --help`.\n4. Use `subprocess` to manage venv creation and command execution to ensure total isolation.",
        "testStrategy": "Run `pytest tests/smoke_test.py` (or execute directly `python tests/smoke_test.py`) after running `uv build`. It should pass only if artifacts are valid.",
        "priority": "high",
        "dependencies": [
          "2"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize smoke test script with venv management helpers",
            "description": "Set up the test file and utilities for isolating installations in temporary virtual environments.",
            "dependencies": [],
            "details": "Create `tests/smoke_test.py`. Implement helper functions using `tempfile`, `venv`, and `subprocess` to generate throwaway virtual environments. Create a utility function `run_in_venv(venv_path, command)` to execute shell commands (like `pip install` or the CLI executable) specifically within the context of the created environment.",
            "status": "pending",
            "testStrategy": "Execute the script to verify the helper function successfully creates a directory containing a valid Python executable.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement artifact installation tests for Wheel and Sdist",
            "description": "Write the specific test cases that install the build artifacts and verify the CLI entry point runs correctly.",
            "dependencies": [
              1
            ],
            "details": "In `tests/smoke_test.py`, implement `test_wheel_install()` and `test_sdist_install()`. The logic must: 1. Locate the `.whl` and `.tar.gz` files in `dist/`. 2. Invoke the venv helper. 3. Install the artifact into the venv. 4. Execute `lmstxt --help` and assert exit code 0. Add a `if __name__ == '__main__':` block to run these tests.",
            "status": "pending",
            "testStrategy": "Run `uv build` to generate artifacts, then execute `python tests/smoke_test.py`. Expect success output for both installation types.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-30T00:18:03.814Z"
      },
      {
        "id": 5,
        "title": "Update TestPyPI Publishing Workflow",
        "description": "Refine the GitHub Action for TestPyPI to use `uv`, proper secrets, and allow manual dispatch.",
        "details": "1. Edit `.github/workflows/publish-testpypi.yml`.\n2. Ensure `on: workflow_dispatch` with input `release_tag` (or ref).\n3. Use `uv build` to generate artifacts.\n4. Use `uv publish` (or `twine`) targeting `https://test.pypi.org/legacy/`.\n5. Map `UV_PUBLISH_TOKEN` or `TWINE_PASSWORD` to the repo secret `TEST_PYPI_TOKEN`.\n6. Ensure it runs the smoke test (Task 4) before publishing.",
        "testStrategy": "Trigger the workflow manually on a branch. Verify artifacts appear on TestPyPI.",
        "priority": "medium",
        "dependencies": [
          "4"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze and Prepare Workflow Structure",
            "description": "Examine the existing GitHub Actions workflows to determine the correct structure for the TestPyPI publishing workflow.",
            "dependencies": [],
            "details": "Check for existing workflows in `.github/workflows/` like `publish-testpypi.yml` or `ci.yml`. Create or update `.github/workflows/publish-testpypi.yml` to define the basic structure, ensuring permissions are set correctly (id-token: write for future OIDC or just contents: read) and define the `workflow_dispatch` trigger with a `release_tag` input option.",
            "status": "pending",
            "testStrategy": "Validate YAML syntax using a linter.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement UV Setup and Build Steps",
            "description": "Configure the workflow to set up Python and install uv, then build the package distribution artifacts.",
            "dependencies": [
              1
            ],
            "details": "In the `publish-testpypi.yml` workflow, add steps to: 1. Checkout code (using `actions/checkout@v4`). 2. Set up Python (using `actions/setup-python@v5`). 3. Install `uv` (e.g., via `pip install uv` or the official action if available/preferred). 4. Run `uv build` to generate sdist and wheel files in the `dist/` directory.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate Smoke Tests into Workflow",
            "description": "Add a step to run the project's smoke tests before publishing to ensure artifact integrity.",
            "dependencies": [
              2
            ],
            "details": "Add a step after the build process but before publishing. This step should create a fresh virtual environment using `uv venv`, install the built wheel from `dist/` (e.g., `uv pip install dist/*.whl`), and run the basic CLI command or import test defined in Task 4 (e.g., `lmstxt --help` or a specific script).",
            "status": "pending",
            "testStrategy": "Workflow fails if the smoke test command returns a non-zero exit code.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Configure UV Publish to TestPyPI",
            "description": "Add the publishing step using `uv publish` targeting the TestPyPI repository with proper authentication.",
            "dependencies": [
              3
            ],
            "details": "Add the publish step: `uv publish --publish-url https://test.pypi.org/legacy/ dist/*`. Configure authentication by mapping the `UV_PUBLISH_TOKEN` environment variable to the `${{ secrets.TEST_PYPI_TOKEN }}` GitHub secret. Ensure this step only runs if the smoke test passes.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Final Review and Workflow Validation",
            "description": "Verify the complete workflow configuration and document the dispatch parameters.",
            "dependencies": [
              4
            ],
            "details": "Review the final `.github/workflows/publish-testpypi.yml` file to ensure all indentation is correct, secrets are referenced properly, and the `workflow_dispatch` inputs are correctly defined. Verify that the `release_tag` input is used to checkout the specific ref if provided, or default to the current branch.",
            "status": "pending",
            "testStrategy": "Manually trigger the workflow from the GitHub Actions UI on a feature branch to verify it passes up to the publish step (which might fail if secrets aren't set in the fork, but the logic can be verified).",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-30T00:19:04.031Z"
      },
      {
        "id": 6,
        "title": "Configure PyPI Trusted Publishing Workflow",
        "description": "Update the release workflow to use OIDC for Trusted Publishing to PyPI on tag push.",
        "details": "1. Edit `.github/workflows/release.yml`.\n2. Set permission `id-token: write`.\n3. Configure environment `name: pypi` and `url: https://pypi.org/p/lmstudio-lmstxt-generator`.\n4. Update the publish step to use `uv publish --trusted-publishing` (or `gh-action-pypi-publish`).\n5. Ensure it triggers on `push: tags: [ 'v*' ]`.\n6. Integrate the smoke test step before the publish job.",
        "testStrategy": "Review workflow syntax using a linter. Actual verification requires pushing a tag (or testing in a fork with TestPyPI configured as the trusted target).",
        "priority": "high",
        "dependencies": [
          "4"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze and Prepare Release Workflow Structure",
            "description": "Examine the existing .github/workflows directory and prepare the structure for release.yml with required OIDC permissions.",
            "dependencies": [],
            "details": "Check for existing release.yml. If present, review current steps. If absent, create a new file. Define the trigger on 'push' for tags matching 'v*'. Crucially, add the top-level 'permissions' block setting 'id-token: write' to enable OIDC authentication for Trusted Publishing.",
            "status": "pending",
            "testStrategy": "Review the workflow file syntax to ensure permissions are correctly scoped.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Configure Build Job with UV",
            "description": "Define the build job in release.yml to checkout code, set up Python, and build artifacts using uv.",
            "dependencies": [
              1
            ],
            "details": "In release.yml, add a 'build' job. Steps: 1. Checkout code (v4). 2. Install uv. 3. Setup Python (v5). 4. Run 'uv build'. 5. Upload artifacts (dist/*) using actions/upload-artifact@v4 so they can be used by subsequent jobs (smoke test and publish).",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate Smoke Test Job",
            "description": "Add a smoke test job that runs before publishing to verify the build artifacts work as expected.",
            "dependencies": [
              2
            ],
            "details": "Create a 'smoke-test' job in release.yml that 'needs: build'. Steps: 1. Download artifacts from the build job. 2. Install uv/Python. 3. Run the smoke test suite (e.g., 'python tests/smoke_test.py' or 'pytest tests/smoke_test.py') against the downloaded wheel to ensure viability before publishing.",
            "status": "pending",
            "testStrategy": "Ensure the job fails if the smoke test script exits with a non-zero code.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Configure PyPI Publish Job with Trusted Publishing",
            "description": "Define the publish job using OIDC authentication targeting the PyPI environment.",
            "dependencies": [
              3
            ],
            "details": "Add a 'publish' job that 'needs: smoke-test'. Define 'environment' with name 'pypi' and url 'https://pypi.org/p/lmstudio-lmstxt-generator'. Use 'pypa/gh-action-pypi-publish@release/v1' (recommended for Trusted Publishing) or 'uv publish --trusted-publishing' if supported. Do NOT use username/password secrets.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Finalize and Validate Workflow Configuration",
            "description": "Combine all jobs into the final workflow file and perform static validation.",
            "dependencies": [
              4
            ],
            "details": "Ensure all jobs (build, smoke-test, publish) are correctly linked with 'needs'. Verify the environment URL matches the specific PyPI project. Commit the .github/workflows/release.yml file. Check that 'contents: read' permission is also present alongside 'id-token: write'.",
            "status": "pending",
            "testStrategy": "Run a linter (e.g., action-validator) on the YAML file locally if possible.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-30T00:19:10.363Z"
      },
      {
        "id": 7,
        "title": "Consolidate Release Runbook",
        "description": "Create a comprehensive document describing the release process for maintainers.",
        "details": "1. Create `docs/publishing.md`.\n2. Document prerequisites (PyPI account, Trusted Publisher setup logic).\n3. Document the 'One Command' release flow (tagging).\n4. Document the TestPyPI manual trigger flow.\n5. Include troubleshooting steps for common errors (e.g., metadata rejected).\n6. Link to the CI workflows.",
        "testStrategy": "Peer review the markdown file. Verify links work.",
        "priority": "medium",
        "dependencies": [
          "5",
          "6"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Establish Documentation Structure and Prerequisites",
            "description": "Create the `docs/publishing.md` file and document the initial setup requirements for maintainers.",
            "dependencies": [],
            "details": "Create a new markdown file at `docs/publishing.md`. Add a 'Prerequisites' section detailing the need for a PyPI account and the specific configuration required for Trusted Publisher setup (OIDC) between GitHub and PyPI. Explain how maintainers should verify their permissions before attempting a release.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Document the Production Release Workflow",
            "description": "Detail the standard 'One Command' release process triggered by git tags.",
            "dependencies": [
              1
            ],
            "details": "In `docs/publishing.md`, describe the production release flow. Explain the versioning convention (Semantic Versioning), how to create a git tag (e.g., `git tag v1.0.0`), and how pushing this tag triggers the GitHub Action defined in `.github/workflows/release.yml`. Include a step-by-step guide for the maintainer to execute this flow.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Document the TestPyPI Workflow",
            "description": "Explain the manual trigger process for deploying to TestPyPI for validation.",
            "dependencies": [
              1
            ],
            "details": "Add a section to `docs/publishing.md` covering the TestPyPI release process. Reference the manual dispatch trigger configured in the CI workflow. Provide instructions on how to use the GitHub UI 'Run workflow' button, input necessary parameters (if any), and verify the artifact on `test.pypi.org`.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Compile Troubleshooting Guide",
            "description": "Create a section for resolving common release failures and errors.",
            "dependencies": [
              2,
              3
            ],
            "details": "Add a 'Troubleshooting' section to the documentation. Address common issues such as 'Metadata Rejected' (often due to version conflicts or invalid `pyproject.toml` configuration), OIDC token failures, and smoke test failures. Provide resolution steps for each scenario, including how to clean up failed tags if necessary.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Finalize References and CI Links",
            "description": "Add direct links to workflows and review the document for completeness.",
            "dependencies": [
              4
            ],
            "details": "Conclude the document by adding direct links to the repository's 'Actions' tab and the specific workflow files (`release.yml`, etc.). Perform a self-review to ensure all paths, commands, and URLs are accurate relative to the project root. Ensure the document is linked from the main `README.md` or `CONTRIBUTING.md` if they exist.",
            "status": "pending",
            "testStrategy": "Peer review the markdown file. Verify all hyperlinks are functional.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-30T00:19:25.045Z"
      },
      {
        "id": 8,
        "title": "Add Pre-commit/Local Build Check Script",
        "description": "Create a helper script for developers to run the full build-verify-smoke cycle locally.",
        "details": "1. Create `scripts/release/verify_install.sh` (or `local_check.sh`).\n2. The script should: Clean `dist/`, run `uv build`, run `validate_metadata.py`, and run `smoke_test.py`.\n3. Make it executable.",
        "testStrategy": "Run `./scripts/release/verify_install.sh` locally and ensure it passes when the repo is clean.",
        "priority": "low",
        "dependencies": [
          "3",
          "4"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Verify Install Script Skeleton",
            "description": "Initialize the verify_install.sh bash script with strict error handling and basic environment checks.",
            "dependencies": [],
            "details": "Create the directory `scripts/release` if it does not exist. Create the file `scripts/release/verify_install.sh` with the shebang `#!/bin/bash` and `set -euo pipefail` to ensure the script exits immediately on error. Add a check to verify that the `uv` command is installed and available in the system PATH.",
            "status": "pending",
            "testStrategy": "Run the script; verify it exists, errors if `uv` is missing, or exits cleanly if nothing else is added yet.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Clean and Build Logic",
            "description": "Add functionality to clean old artifacts and rebuild the project using uv.",
            "dependencies": [
              1
            ],
            "details": "Update the script to run `rm -rf dist/` to clean previous builds. Add the `uv build` command. Check that the `dist/` directory is created and contains artifacts (e.g., `.whl`, `.tar.gz`) after the build command runs.",
            "status": "pending",
            "testStrategy": "Run the script. Check that `dist/` is cleared and then repopulated with build artifacts.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate Metadata Validation Step",
            "description": "Invoke the metadata validation Python script to ensure build artifacts are well-formed.",
            "dependencies": [
              2
            ],
            "details": "Add the command to execute `python scripts/release/validate_metadata.py`. Ensure the script relies on the exit code of this Python script to determine pass/fail status. This ensures no artifacts with invalid metadata (e.g., direct URL dependencies) pass the check.",
            "status": "pending",
            "testStrategy": "Temporarily modify `pyproject.toml` to be invalid, run the script, and ensure it fails at this step.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Integrate Smoke Test Step",
            "description": "Invoke the smoke test script to verify the basic functionality of the built artifacts.",
            "dependencies": [
              3
            ],
            "details": "Add the command to execute `python scripts/release/smoke_test.py`. This step should occur after metadata validation. It ensures that the package built in the previous steps can be installed and imported successfully.",
            "status": "pending",
            "testStrategy": "Run the script and verify that the smoke test python script is executed effectively.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Finalize Script Permissions and User Experience",
            "description": "Make the script executable and add user-friendly logging for build phases.",
            "dependencies": [
              4
            ],
            "details": "Run `chmod +x scripts/release/verify_install.sh`. Add `echo` statements with ANSI color codes (e.g., Green for success, Red for failure) to clearly demarcate the Build, Validate, and Smoke Test phases. Add a final success message if all steps pass.",
            "status": "pending",
            "testStrategy": "Run `./scripts/release/verify_install.sh` directly from the terminal and verify the output contains formatted status messages.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-30T00:24:30.249Z"
      },
      {
        "id": 9,
        "title": "Verify Dynamic Versioning or Version Bump Policy",
        "description": "Ensure the package version aligns with the Git tag automatically or document the bump process.",
        "details": "1. Check `pyproject.toml` for `dynamic = [\"version\"]` or static version.\n2. If static, update the Runbook (Task 7) to include 'Bump version in pyproject.toml' as a step before tagging.\n3. If dynamic versioning is preferred, configure `setuptools_scm` (or equivalent) in `pyproject.toml` and add it to build-system requirements.",
        "testStrategy": "Build a wheel and check the filename version. Ensure it matches the expected tag/commit version.",
        "priority": "medium",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze pyproject.toml for Current Versioning Configuration",
            "description": "Inspect the pyproject.toml file to determine if the version is statically defined or configured for dynamic discovery.",
            "dependencies": [],
            "details": "Read `pyproject.toml` to check the `[project]` table for a `version` key or `dynamic = [\"version\"]`. Check `[build-system]` and `[tool.setuptools]` (or other backend configs) to understand the current setup.",
            "status": "pending",
            "testStrategy": "None (Analysis task)",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Dynamic Versioning with setuptools_scm",
            "description": "Configure the project to derive its version automatically from Git tags using setuptools_scm.",
            "dependencies": [
              1
            ],
            "details": "Modify `pyproject.toml` to: 1. Remove static `version`. 2. Add `dynamic = [\"version\"]`. 3. Add `setuptools_scm` to `[build-system] requires`. 4. Add `[tool.setuptools_scm]` section (can be empty or with `write_to` config).",
            "status": "pending",
            "testStrategy": "Run `uv build` (or pip install) locally and verify the generated metadata/wheel version matches `git describe --tags`.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Ensure __version__ Attribute Availability",
            "description": "Ensure the package version is accessible at runtime via `__version__` without hardcoding it.",
            "dependencies": [
              2
            ],
            "details": "Modify `src/lms_lmstxt/__init__.py` (or equivalent top-level init) to use `importlib.metadata.version` to retrieve the installed package version dynamically, handling the `PackageNotFoundError` gracefully.",
            "status": "pending",
            "testStrategy": "Run a python script importing the package and printing `__version__`. It should match the installed version.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Update Release Documentation / Runbook",
            "description": "Update the project documentation to reflect the new dynamic versioning workflow.",
            "dependencies": [
              2
            ],
            "details": "Create or update a `RELEASE.md` or the Runbook (from Task 7 context) to explain that tagging a commit triggers the version bump, and manual editing of `pyproject.toml` version is no longer needed.",
            "status": "pending",
            "testStrategy": "Review the documentation for clarity and accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Verify Wheel Versioning with Smoke Test",
            "description": "Validate that the build process produces artifacts with the correct version derived from the git context.",
            "dependencies": [
              2
            ],
            "details": "Create a temporary git tag (e.g., `v0.0.1-test`), run `uv build`, and inspect the filename of the generated `.whl` in `dist/`. Ensure it matches the tag. Clean up the tag afterwards.",
            "status": "pending",
            "testStrategy": "Manual verification or script checking `dist/` filenames against `git describe` output.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-30T00:27:09.320Z"
      },
      {
        "id": 10,
        "title": "End-to-End Release Simulation",
        "description": "Perform a final validation of the entire pipeline using TestPyPI.",
        "details": "1. Bump version to a dev version (e.g., `0.0.1.dev1`).\n2. Trigger the TestPyPI workflow.\n3. Verify upload.\n4. locally run `pip install -i https://test.pypi.org/simple/ lmstudio-lmstxt-generator`.\n5. Verify `lmstxt --help` works.",
        "testStrategy": "Successful installation and execution from TestPyPI.",
        "priority": "medium",
        "dependencies": [
          "5",
          "7"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Update Version to Development Release Candidate",
            "description": "Bump the package version in pyproject.toml to a unique development version (e.g., 0.0.1.dev1) to allow a non-conflicting upload to TestPyPI.",
            "dependencies": [],
            "details": "Modify the `version` field in `pyproject.toml`. Append a `.devN` suffix to the current version. This ensures that we can verify the upload mechanism without consuming a 'real' semantic version number on TestPyPI, as PyPI files are immutable.",
            "status": "pending",
            "testStrategy": "Check pyproject.toml content and verify `uv build` produces a file with the new version string.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Trigger TestPyPI Deployment Workflow",
            "description": "Manually trigger the GitHub Action or CI pipeline configured for TestPyPI deployment using the new development version.",
            "dependencies": [
              1
            ],
            "details": "Navigate to the CI/CD dashboard (e.g., GitHub Actions). Locate the 'Publish to TestPyPI' workflow (created in previous tasks). Trigger it manually or by pushing a specific tag (e.g., `v0.0.1.dev1`) depending on the trigger logic defined in the workflow file.",
            "status": "pending",
            "testStrategy": "Monitor the CI job logs for successful completion, specifically looking for the 'Twine upload' or equivalent step success message.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Verify TestPyPI Upload Integrity",
            "description": "Confirm that the package artifacts have effectively appeared on the TestPyPI repository page with the correct metadata.",
            "dependencies": [
              2
            ],
            "details": "Visit the project URL on TestPyPI (e.g., `https://test.pypi.org/project/lmstudio-lmstxt-generator/`). Verify that the new version `0.0.1.dev1` is listed, the Release Date is current, and the `Description` (README) renders correctly.",
            "status": "pending",
            "testStrategy": "Manual verification on the TestPyPI website.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Perform Clean Installation from TestPyPI",
            "description": "Create a fresh virtual environment and install the package exclusively from the TestPyPI index to simulate an end-user installation.",
            "dependencies": [
              3
            ],
            "details": "Create a new virtualenv: `python -m venv venv_test`. Activate it. Run `pip install --no-cache-dir --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ lmstudio-lmstxt-generator==0.0.1.dev1`. Note: `--extra-index-url` is often needed for dependencies that don't exist on TestPyPI.",
            "status": "pending",
            "testStrategy": "Ensure pip completes successfully without errors regarding missing dependencies or hash mismatches.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Validate Installed CLI Functionality",
            "description": "Execute the installed CLI tool within the test environment to confirm the package structure and entry points function correctly.",
            "dependencies": [
              4
            ],
            "details": "Inside the `venv_test`, run the command `lmstxt --help`. Verify that the help text is displayed, indicating that the entry point scripts were correctly generated and the internal imports are working.",
            "status": "pending",
            "testStrategy": "The command should exit with code 0 and display the standard help message, not an ImportError or ModuleNotFoundError.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-30T00:28:29.726Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2025-12-30T00:28:29.728Z",
      "taskCount": 10,
      "completedCount": 10,
      "tags": [
        "chore-release"
      ],
      "created": "2026-01-03T18:28:51.110Z",
      "description": "Tasks for chore-release context",
      "updated": "2026-01-03T18:28:51.110Z"
    }
  },
  "feat-mcp": {
    "tasks": [
      {
        "id": 1,
        "title": "Project Foundation & Configuration Module",
        "description": "Establish the project structure, dependency management, and configuration loading logic.",
        "details": "Initialize the project following the `src/lms_lmstxt_mcp/` structure. Create `config.py` using Pydantic Settings or `python-dotenv` to manage environment variables (e.g., `LLMSTXT_MCP_ALLOWED_ROOT`, `LLMSTXT_MCP_RESOURCE_MAX_CHARS`). Ensure `pyproject.toml` includes dependencies for `mcp` (using `fastmcp` if available/compatible or standard `mcp` SDK) and the local `lmstudio-lmstxt-generator` package. Set up `errors.py` to define custom exception classes like `OutputDirNotAllowedError` and `LMStudioUnavailableError`.\n\nLibraries: `pydantic`, `pydantic-settings`, `mcp`.\nfiles: `src/lms_lmstxt_mcp/config.py`, `src/lms_lmstxt_mcp/errors.py`, `pyproject.toml`.",
        "testStrategy": "Unit tests verifying that environment variables are correctly parsed into the config object and that defaults are applied. Test custom error instantiation.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "complexity": 3,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Create a `pyproject.toml` configuration defining the build system and dependencies including `mcp` and `pydantic`. Implement `src/lms_lmstxt_mcp/config.py` using `pydantic-settings` to parse environment variables like `LLMSTXT_MCP_ALLOWED_ROOT` with safe defaults. Define the custom exception hierarchy in `src/lms_lmstxt_mcp/errors.py`.",
        "updatedAt": "2026-01-03T19:17:10.418Z"
      },
      {
        "id": 2,
        "title": "Data Models & Type Definitions",
        "description": "Define Pydantic models for MCP tool inputs/outputs and internal data structures.",
        "details": "Implement `models.py` to define schemas for `GenerateResult`, `ArtifactRef`, and `ReadArtifactResult`. Use Pydantic to enforce types and constraints. Define `ArtifactName` literals (e.g., `llms.txt`, `llms-full.txt`). Ensure strict typing for tool arguments to leverage MCP's automatic schema generation capabilities.\n\nLibraries: `pydantic`.\nfiles: `src/lms_lmstxt_mcp/models.py`.",
        "testStrategy": "Unit tests ensuring valid data passes validation and invalid data raises ValidationError. Verify JSON serialization formats.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 2,
        "recommendedSubtasks": 0,
        "expansionPrompt": "",
        "updatedAt": "2026-01-03T19:17:54.333Z"
      },
      {
        "id": 3,
        "title": "Security & File Hashing Utilities",
        "description": "Implement path validation security controls and file hashing utilities.",
        "details": "Create `security.py` to implement `validate_output_dir`. This function must resolve paths and ensure they are contained within `LLMSTXT_MCP_ALLOWED_ROOT` to prevent path traversal. Create `hashing.py` to implement `sha256_file` (streaming read) and `read_text_preview` (reads first N chars). These utilities are critical for the generation and artifact access layers.\n\nLibraries: `pathlib`, `hashlib`.\nfiles: `src/lms_lmstxt_mcp/security.py`, `src/lms_lmstxt_mcp/hashing.py`.",
        "testStrategy": "Unit tests: 1) Attempt to access paths outside the allowed root (e.g., `../etc/passwd`) and assert `OutputDirNotAllowedError`. 2) Verify SHA256 matches known values for test files. 3) Verify preview returns correct truncation boolean.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 4,
        "recommendedSubtasks": 2,
        "expansionPrompt": "Implement `validate_output_dir` in `security.py` using `pathlib` to strictly resolve and check path containment against the allowed root. Develop `hashing.py` to provide a streaming SHA256 calculator and a safe text preview reader that handles encoding errors gracefully.",
        "updatedAt": "2026-01-03T19:18:55.104Z"
      },
      {
        "id": 4,
        "title": "In-Memory Run Registry",
        "description": "Implement the storage mechanism to track generation runs and their artifacts.",
        "details": "Create `runs.py` containing a `RunStore` class. This should maintain an in-memory dictionary mapping `run_id` to `RunRecord` objects. Implement methods `put_run(run_record)`, `get_run(run_id)`, and `list_runs(limit)`. Use a thread-safe approach if necessary, though simple dicts are atomic in Python for single operations. This store bridges the generation and reading steps.\n\nfiles: `src/lms_lmstxt_mcp/runs.py`.",
        "testStrategy": "Unit tests: Add runs, retrieve them by ID, and list them with sorting/limiting. Verify `UnknownRunError` when accessing non-existent IDs.",
        "priority": "medium",
        "dependencies": [
          "2"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 3,
        "recommendedSubtasks": 0,
        "expansionPrompt": "",
        "updatedAt": "2026-01-03T19:19:47.633Z"
      },
      {
        "id": 5,
        "title": "Artifact Access Logic",
        "description": "Implement the core logic for resolving, reading, and chunking artifact content.",
        "details": "Create `artifacts.py`. Implement `resource_uri(run_id, artifact)` to generate standard URIs. Implement `read_resource_text` which uses `hashing.read_text_preview` for truncated reads suitable for MCP Resources. Implement `read_artifact_chunk` for the chunking tool, handling `offset` and `limit` to slice file content safely. Ensure dependencies on `runs.py` to resolve `run_id` to file paths.\n\nfiles: `src/lms_lmstxt_mcp/artifacts.py`.",
        "testStrategy": "Unit tests: Create dummy files, perform chunked reads at various offsets (start, middle, end, past end). Verify truncation banners are prepended when reading as a resource.",
        "priority": "high",
        "dependencies": [
          "3",
          "4"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Artifacts Module and URI Logic",
            "description": "Create the artifacts module structure and implement the URI generation helper function. [Updated: 1/3/2026]",
            "dependencies": [],
            "details": "Create `src/lms_lmstxt_mcp/artifacts.py`. Import necessary dependencies including the `RunStore` from `runs.py`. Implement the `resource_uri(run_id: str, artifact_name: str) -> str` function to generate standardized URIs (e.g., `lmstxt://{run_id}/{artifact}`) used by the MCP server to identify resources.",
            "status": "done",
            "testStrategy": "Unit test verifying that `resource_uri` returns the expected string format for given inputs.",
            "parentId": "undefined",
            "updatedAt": "2026-01-03T19:20:34.944Z"
          },
          {
            "id": 2,
            "title": "Implement Truncated Resource Reading",
            "description": "Develop the logic to read artifact files for MCP Resources, ensuring content is truncated if it exceeds limits.",
            "dependencies": [
              1
            ],
            "details": "Implement `read_resource_text(run_id, artifact)`. This function must resolve the file path using `RunStore`, read the text content, and check against `LLMSTXT_MCP_RESOURCE_MAX_CHARS`. If the content exceeds the limit, truncate it and append a footer (e.g., '... truncated') to prevent overloading the MCP client.",
            "status": "done",
            "testStrategy": "Create a dummy file larger than the configured max chars. Call `read_resource_text` and assert the returned string length matches the limit and contains the truncation indicator.",
            "parentId": "undefined",
            "updatedAt": "2026-01-03T19:21:38.433Z"
          },
          {
            "id": 3,
            "title": "Implement Chunked Artifact Reading",
            "description": "Create the logic for reading specific slices of a file based on offset and limit parameters for the reading tool.",
            "dependencies": [
              1
            ],
            "details": "Implement `read_artifact_chunk(run_id, artifact, offset, limit)`. Use python file seeking (`seek`) to navigate to the `offset` and read up to `limit` characters. Handle edge cases such as `offset` exceeding file size (return empty string) or `limit` extending past EOF. Ensure file handles are closed safely.",
            "status": "done",
            "testStrategy": "Create a file with known content (e.g., '0123456789'). specific offsets and limits (e.g., offset=2, limit=3 should return '234') and verify behavior when reading past EOF.",
            "parentId": "undefined",
            "updatedAt": "2026-01-03T19:25:09.120Z"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Implement `resource_uri` generation logic. Create the `read_resource_text` function handling file I/O and truncation. Develop `read_artifact_chunk` with robust offset/limit handling to support pagination of large files.",
        "updatedAt": "2026-01-03T19:25:09.120Z"
      },
      {
        "id": 6,
        "title": "Generator Integration & Serialized Execution",
        "description": "Wrap the external `lms_lmstxt` library with thread-safe execution logic.",
        "details": "In `server.py` (or a dedicated integration module), implement the `lmstxt_generate` tool logic. Import `run_generation` from the external library. Use a `threading.Lock` to ensure only one generation runs at a time (avoiding global config races). Capture exceptions (specifically connectivity errors) and map them to `LMStudioUnavailableError`. Calculate hashes for outputs and register the run in `RunStore`.\n\nLibraries: `lms_lmstxt`.\nfiles: `src/lms_lmstxt_mcp/server.py`.",
        "testStrategy": "Integration test: Mock the external `run_generation` to write temporary files. Verify that the MCP tool wrapper correctly calls the mock, computes metadata, and updates the RunStore.",
        "priority": "high",
        "dependencies": [
          "5"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Thread-Safe Generator Wrapper",
            "description": "Create the integration module and implement the locking mechanism to ensure serial execution of the generator.",
            "dependencies": [],
            "details": "Create `src/lms_lmstxt_mcp/generator.py` (or integrate into `server.py`). Import `run_generation` from `lms_lmstxt`. Instantiate a module-level `threading.Lock`. Define a function `safe_generate` that uses the lock as a context manager to wrap the call to `run_generation`. This ensures that concurrent requests to the MCP server do not trigger race conditions in the external library's global configuration.",
            "status": "done",
            "testStrategy": "Unit test: call the wrapper from multiple threads simultaneously and assert that the critical section is entered serially.",
            "parentId": "undefined",
            "updatedAt": "2026-01-03T19:31:15.917Z"
          },
          {
            "id": 2,
            "title": "Implement Error Translation Layer",
            "description": "Add error handling logic to map external library exceptions to internal domain errors.",
            "dependencies": [
              1
            ],
            "details": "In the generation wrapper, wrap the `run_generation` call in a try/except block. Specifically identify connectivity errors (e.g., connection refused from LM Studio) and raise a `LMStudioUnavailableError` (defined in `errors.py`). Ensure generic exceptions are also caught and wrapped or logged appropriately to prevent server crashes while releasing the lock in a `finally` block.",
            "status": "done",
            "testStrategy": "Unit test: Mock `run_generation` to raise specific exceptions and assert that the wrapper raises the correct internal `LMStudioUnavailableError`.",
            "parentId": "undefined",
            "updatedAt": "2026-01-03T19:36:42.791Z"
          },
          {
            "id": 3,
            "title": "Integrate Output Hashing and RunStore Updates",
            "description": "Process generation outputs, calculate hashes, and register the completed run in the RunStore.",
            "dependencies": [
              1,
              2
            ],
            "details": "Upon successful generation, use the hashing utilities from `src/lms_lmstxt_mcp/hashing.py` to calculate SHA256 checksums for the output files. Construct a `Run` object with the status, timestamp, and artifact metadata. Call the `RunStore` instance to save the run. Return the final result object expected by the MCP tool interface.",
            "status": "done",
            "testStrategy": "Integration test: Verify that a successful generation results in a new entry in `RunStore` with correct file hashes.",
            "parentId": "undefined",
            "updatedAt": "2026-01-03T19:55:09.862Z"
          }
        ],
        "complexity": 6,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Create the integration module wrapping `lms_lmstxt`. Implement a locking mechanism to enforce serial execution. Add error handling to translate external library exceptions into internal domain errors and update the `RunStore` upon completion.",
        "updatedAt": "2026-01-03T19:55:09.862Z"
      },
      {
        "id": 7,
        "title": "MCP Server Tool & Resource Wiring",
        "description": "Configure the FastMCP server instance and register tools and resources.",
        "details": "In `server.py`, initialize `FastMCP`. Register the `lmstxt_generate` and `lmstxt_read_artifact` functions as tools with appropriate descriptions. Register the `lmstxt://runs/{run_id}/{artifact}` pattern as a resource. Ensure the `lmstxt_list_runs` tool is also registered. Wire up the `stdio` and `http` transport capabilities provided by FastMCP/MCP SDK.\n\nLibraries: `fastmcp` (or standard `mcp` SDK depending on preference/availability).\nfiles: `src/lms_lmstxt_mcp/server.py`.",
        "testStrategy": "Functional test: Instantiate the server in a test harness. Call `list_tools` and `list_resources` to verify registration. Simulate a tool call and resource read request.",
        "priority": "high",
        "dependencies": [
          "6"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize FastMCP Server Instance",
            "description": "Set up the basic FastMCP server application shell and entry point. [Updated: 1/3/2026]",
            "dependencies": [],
            "details": "In `src/lms_lmstxt_mcp/server.py`, import `FastMCP` and instantiate the server object with the name 'lmstxt-mcp'. Define the standard `if __name__ == \"__main__\":` block to invoke `mcp.run()`, ensuring it defaults to the stdio transport mechanism for standard MCP communication.",
            "status": "done",
            "testStrategy": "Run the python file directly and verify it enters the wait loop for stdin input (or prints help if arguments are required).",
            "parentId": "undefined",
            "updatedAt": "2026-01-03T20:04:49.758Z"
          },
          {
            "id": 2,
            "title": "Register MCP Tools",
            "description": "Expose the core generation and retrieval functions as MCP tools.",
            "dependencies": [
              1
            ],
            "details": "Import the logic from core modules. Use the `@mcp.tool()` decorator to register `lmstxt_generate`, `lmstxt_read_artifact`, and `lmstxt_list_runs`. Ensure type hints use the Pydantic models defined in `models.py` so that FastMCP generates the correct JSON schemas for the tool definitions.",
            "status": "done",
            "testStrategy": "Unit test that inspects the `mcp.tools` registry to confirm all three tools are present and possess the correct arguments/descriptions.",
            "parentId": "undefined",
            "updatedAt": "2026-01-03T20:20:31.495Z"
          },
          {
            "id": 3,
            "title": "Register Dynamic Resources",
            "description": "Implement the URI pattern for accessing generated artifacts as resources. [Updated: 1/3/2026]",
            "dependencies": [
              1
            ],
            "details": "Use the `@mcp.resource(\"lmstxt://runs/{run_id}/{artifact}\")` decorator to register the resource handler. Implement the underlying function to parse the `run_id` and `artifact` name, retrieve the content from `RunStore`, and return it as a string resource.",
            "status": "done",
            "testStrategy": "Unit test mocking the `RunStore` with a dummy artifact, calling the resource handler with a valid URI, and asserting the content is returned.",
            "parentId": "undefined",
            "updatedAt": "2026-01-03T20:21:32.410Z"
          },
          {
            "id": 4,
            "title": "Server Capability Verification",
            "description": "Verify the complete wiring of tools and resources within the server instance. [Updated: 1/3/2026]",
            "dependencies": [
              2,
              3
            ],
            "details": "Create an integration test harness that loads the `server.py` module. Call the internal MCP introspection methods (e.g., `list_tools`, `list_resource_templates`) to verify that the server is correctly advertising its capabilities before it is deployed to a real client.",
            "status": "done",
            "testStrategy": "Integration test: Instantiate the server and simulate a `tools/list` and `resources/templates/list` JSON-RPC request to validate the response structure.",
            "parentId": "undefined",
            "updatedAt": "2026-01-03T20:22:31.099Z"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Initialize the `FastMCP` server instance. Register the generation and reading tools with their Pydantic schemas. Define the resource URI pattern `lmstxt://runs/{run_id}/{artifact}` and link it to the artifact reader. Configure the entry point to run the server.",
        "updatedAt": "2026-01-03T20:22:31.099Z"
      },
      {
        "id": 8,
        "title": "Stdio-Safe Logging Implementation",
        "description": "Configure logging to ensure no interference with the stdio transport. [Updated: 1/3/2026]",
        "details": "Update `server.py` or `config.py` to configure the Python `logging` module. Ensure the root logger writes to `sys.stderr` and NOT `sys.stdout`, as stdout is reserved for the JSON-RPC protocol in stdio mode. Set log levels based on the configuration.\n\nfiles: `src/lms_lmstxt_mcp/server.py`.",
        "testStrategy": "Manual/Scripted test: Run the server in stdio mode, emit logs, and verify they appear on stderr while the JSON-RPC communication on stdout remains valid JSON.",
        "priority": "medium",
        "dependencies": [
          "7"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 2,
        "recommendedSubtasks": 0,
        "expansionPrompt": "",
        "updatedAt": "2026-01-03T20:26:24.207Z"
      },
      {
        "id": 9,
        "title": "Request Limits & Error Normalization",
        "description": "Harden the server by enforcing limits and ensuring clean error reporting.",
        "details": "Refine `artifacts.py` and `server.py` to enforce `MAX_CHARS` limits on resource reads and chunk sizes. Implement a global error handler or try/except blocks in tool entry points to catch internal exceptions (e.g., `LMStudioUnavailableError`, `OutputDirNotAllowedError`) and return user-friendly error strings instead of stack traces to the MCP client.\n\nfiles: `src/lms_lmstxt_mcp/errors.py`, `src/lms_lmstxt_mcp/server.py`.",
        "testStrategy": "Unit tests: Trigger exceptions and verify the returned error message format. Test boundary conditions for chunk sizes to ensure limits are respected.",
        "priority": "medium",
        "dependencies": [
          "7"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 4,
        "recommendedSubtasks": 2,
        "expansionPrompt": "Implement decorators or middleware to catch domain exceptions and convert them to user-friendly MCP errors. Refactor artifact reading to strictly enforce `MAX_CHARS` limits globally.",
        "updatedAt": "2026-01-03T20:26:56.522Z"
      },
      {
        "id": 10,
        "title": "Packaging & Entry Point Verification",
        "description": "Finalize packaging and verify the CLI entry point works for both transports.",
        "details": "Ensure `pyproject.toml` defines the correct `project.scripts` entry point (e.g., `lmstxt-mcp = lms_lmstxt_mcp.server:main`). Verify that running `lmstxt-mcp` defaults to stdio mode and accepts flags for HTTP mode if implemented. Create a basic README documenting installation and usage.\n\nfiles: `pyproject.toml`, `README.md`.",
        "testStrategy": "Smoke test: Install the package locally (`pip install -e .`) and run the CLI command. Verify it starts up without crashing and prints expected startup logs to stderr.",
        "priority": "low",
        "dependencies": [
          "8",
          "9"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 2,
        "recommendedSubtasks": 0,
        "expansionPrompt": "",
        "updatedAt": "2026-01-03T22:34:55.275Z"
      },
      {
        "id": 11,
        "title": "Fix: Refactor for Asynchronous Processing",
        "description": "To truly remove the timeout in a way that works for tools, the server should not do heavy lifting inside the tool call. It should accept the request, launch a background thread to do the work, and return a \"Processing...\" status immediately.",
        "details": "Requirements:\n- Accept the request\n- Launch a background thread to do the work\n- Return a \"Processing...\" status immediately",
        "testStrategy": "",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Update RunRecord to Include Job Status and Results",
            "description": "Enhance the RunRecord data model to track the state of the background job (e.g., 'pending', 'processing', 'completed', 'failed') and capture any errors or outputs.",
            "dependencies": [],
            "details": "Modify the `RunRecord` class in `src/lms_lmstxt_mcp/runs.py`. Add fields for `status` (Enum), `error_message` (Optional[str]), and ensure it can store results once the background task finishes. Update `RunStore.put_run` or create a `update_run` method to allow updating status during the lifecycle.",
            "status": "done",
            "testStrategy": "Unit test for RunStore and RunRecord to ensure status transitions and metadata updates are correctly persisted in memory.",
            "parentId": "undefined",
            "updatedAt": "2026-01-04T07:54:52.404Z"
          },
          {
            "id": 2,
            "title": "Implement Background Worker Logic",
            "description": "Create a helper function that executes the heavy generation logic and updates the RunStore with results.",
            "dependencies": [
              1
            ],
            "details": "In `src/lms_lmstxt_mcp/server.py` or a new `worker.py`, implement an async-friendly wrapper (using `anyio.to_thread.run_sync` or standard `threading`) that calls the actual generation code. It must catch exceptions, update the run status in `RunStore` to 'completed' or 'failed', and store the resulting artifact paths.",
            "status": "done",
            "testStrategy": "Integration test: Call the worker function directly and verify that the RunStore reflects the correct status and artifact list after completion.",
            "parentId": "undefined",
            "updatedAt": "2026-01-04T07:55:00.636Z"
          },
          {
            "id": 3,
            "title": "Refactor lmstxt_generate Tool to Use Background Tasks",
            "description": "Modify the main generation tool to initialize a run and immediately return the run ID.",
            "dependencies": [
              2
            ],
            "details": "Update the `lmstxt_generate` tool in `server.py`. Instead of awaiting the full generation, it should: 1. Generate a unique `run_id`. 2. Create a 'pending' record in `RunStore`. 3. Schedule the worker logic in the background (using `asyncio.create_task` or a background thread). 4. Return a JSON response containing the `run_id` and a message like 'Generation started'.",
            "status": "done",
            "testStrategy": "Functional test using an MCP client: verify that the tool call returns immediately with a success message while the work happens in the background.",
            "parentId": "undefined",
            "updatedAt": "2026-01-04T07:55:08.953Z"
          },
          {
            "id": 4,
            "title": "Update Resource Provider for Dynamic Run Status",
            "description": "Ensure that reading a run resource provides the current status of the processing job.",
            "dependencies": [
              1,
              3
            ],
            "details": "Modify the resource resolver for `lmstxt://runs/{run_id}/...` in `server.py`. If the user requests the status or the job is still processing, the resource content should reflect the current status (e.g., 'Processing...') instead of failing or blocking. If the job failed, it should return the captured error message.",
            "status": "done",
            "testStrategy": "Resource read test: Attempt to read a resource for a 'processing' run and verify the returned text indicates the current state.",
            "parentId": "undefined",
            "updatedAt": "2026-01-04T07:55:16.681Z"
          },
          {
            "id": 5,
            "title": "Implement Asynchronous Cleanup of Completed Runs",
            "description": "Add logic to prune old runs from the in-memory registry to prevent memory leaks.",
            "dependencies": [
              1
            ],
            "details": "Add a simple TTL (Time To Live) or a maximum capacity check to `RunStore`. Implement a background cleanup routine that periodically removes records for runs that have been completed or failed for a certain duration.",
            "status": "done",
            "testStrategy": "Unit test: Verify that adding runs beyond the capacity or after the TTL causes the oldest/expired runs to be removed from the registry.",
            "parentId": "undefined",
            "updatedAt": "2026-01-04T07:55:25.234Z"
          },
          {
            "id": 6,
            "title": "Update RunRecord Model for State Tracking",
            "description": "Enhance the existing RunRecord model in runs.py to include status fields and lifecycle timestamps.",
            "dependencies": [],
            "details": "Add 'status' (Enum: PENDING, PROCESSING, COMPLETED, FAILED), 'error_message' (Optional string), and 'updated_at' fields to the RunRecord dataclass or Pydantic model.",
            "status": "done",
            "testStrategy": "Unit tests to verify instantiation and field accessibility for the updated RunRecord structure.",
            "updatedAt": "2026-01-04T07:55:33.395Z",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Implement Thread-Safe RunStore Update Mechanism",
            "description": "Modify RunStore to handle concurrent updates to records safely using synchronization primitives.",
            "dependencies": [
              6
            ],
            "details": "Add a threading.Lock to the RunStore class. Implement an update_run(run_id, **kwargs) method that safely updates specific fields in a RunRecord without race conditions.",
            "status": "done",
            "testStrategy": "Concurrent stress test with multiple threads attempting to update the same record ID simultaneously.",
            "parentId": "undefined",
            "updatedAt": "2026-01-04T07:55:43.390Z"
          },
          {
            "id": 8,
            "title": "Develop Background Execution Wrapper",
            "description": "Create a helper function to execute long-running generator tasks in a background thread.",
            "dependencies": [
              7
            ],
            "details": "Use anyio.to_thread.run_sync or threading.Thread to wrap generator logic. The wrapper must update the RunStore status to 'PROCESSING' at start, 'COMPLETED' on success, and 'FAILED' on exception.",
            "status": "done",
            "testStrategy": "Verify status transitions in the RunStore when a dummy background task is executed through the wrapper.",
            "parentId": "undefined",
            "updatedAt": "2026-01-04T07:55:52.937Z"
          },
          {
            "id": 9,
            "title": "Refactor Generate Tool for Immediate Response",
            "description": "Update the lmstxt_generate tool in server.py to initiate background work and return immediately.",
            "dependencies": [
              8
            ],
            "details": "Modify the tool handler to create a new run_id, put a 'PENDING' record in the RunStore, spawn the background wrapper task, and return a JSON response containing the run_id and 'Processing...' message.",
            "status": "done",
            "testStrategy": "Functional test calling the generate tool and checking for immediate HTTP 200/OK while background work continues.",
            "parentId": "undefined",
            "updatedAt": "2026-01-04T07:56:06.316Z"
          },
          {
            "id": 10,
            "title": "Implement Status-Aware Resource Resolvers",
            "description": "Update lmstxt_read_artifact and resource providers to handle runs that are still in progress.",
            "dependencies": [
              9
            ],
            "details": "Check the RunRecord status in resource handlers. If 'PROCESSING', return a status indicator. If 'FAILED', return the error details. Only return artifact content if status is 'COMPLETED'.",
            "status": "done",
            "testStrategy": "Attempt to read an artifact for a 'PROCESSING' run and verify the response correctly indicates work is in progress.",
            "parentId": "undefined",
            "updatedAt": "2026-01-04T07:56:14.353Z"
          },
          {
            "id": 11,
            "title": "Implement In-Memory Record Cleanup Routine",
            "description": "Create a background maintenance task to remove old or stale run records from the RunStore.",
            "dependencies": [
              7
            ],
            "details": "Add a method to RunStore that deletes records older than a configurable TTL (e.g., 24 hours). Implement an optional background loop or trigger this during new run creation to prevent memory leaks.",
            "status": "done",
            "testStrategy": "Add records with manually expired timestamps and verify they are correctly removed after calling the pruning method.",
            "parentId": "undefined",
            "updatedAt": "2026-01-04T07:56:23.989Z"
          }
        ],
        "complexity": 8,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Identify and break down the necessary changes for Task 11 into granular steps. Ensure you cover the following areas: 1) Updating the RunRecord model to track state, 2) Creating a thread-safe RunStore update mechanism, 3) Implementing a background execution wrapper (e.g., using anyio or threading), 4) Refactoring the FastMCP tool to handle immediate returns and background task spawning, 5) Updating resource resolvers to provide status-aware responses, and 6) Implementing a safe cleanup/pruning routine for in-memory records.",
        "updatedAt": "2026-01-04T07:56:23.989Z"
      },
      {
        "id": 12,
        "title": "Artifact Directory Discovery and Resource Exposure",
        "description": "Extend the MCP server to discover and retrieve artifact files from local storage, exposing them as structured resources and tools for client context.",
        "details": "This task transforms the server into a persistent context source by enabling access to the 'artifacts/' directory on disk. 1) Update 'artifacts.py' to include a 'scan_artifacts' function using 'pathlib' to list all '.txt' files in the configured output directory. 2) In 'server.py', implement the 'list_resources' MCP handler to dynamically expose these files as resources with the URI scheme 'lmstxt://artifacts/{filename}'. 3) Implement the 'read_resource' handler to fetch file content, utilizing 'hashing.read_text_preview' for safety and 'security.validate_output_dir' to prevent path traversal. 4) Add a new MCP tool 'lmstxt_list_all_artifacts' that returns metadata (filename, size, last modified) for all existing artifacts, including those from previous server sessions not present in the volatile 'RunStore'. 5) Integrate the 'MAX_CHARS' limit from Task 9 into the directory-based resource reads.",
        "testStrategy": "1) Manually populate the artifacts directory with several .txt files. 2) Use an MCP inspector or test client to call 'list_resources' and verify the 'lmstxt://artifacts/' URIs are correctly generated. 3) Verify that 'read_resource' successfully returns content for valid files and fails for files outside the allowed directory. 4) Verify the 'lmstxt_list_all_artifacts' tool returns files that were created before the current server process started. 5) Confirm that large files are appropriately truncated according to defined limits.",
        "status": "done",
        "dependencies": [
          "3",
          "5",
          "7",
          "9"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Artifact Directory Scanning Logic",
            "description": "Add a function to discover existing artifact files in the configured output directory.",
            "dependencies": [],
            "details": "Update 'src/lms_lmstxt_mcp/artifacts.py' to include 'scan_artifacts'. Use 'pathlib.Path.glob' to identify all '.txt' files. Ensure the path is resolved relative to the configured artifact directory.",
            "status": "done",
            "testStrategy": "Unit test using a temporary directory populated with several .txt and non-.txt files to ensure only artifacts are listed.",
            "updatedAt": "2026-01-05T06:27:05.271Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Define Artifact Metadata Schemas",
            "description": "Create structured data models for artifact metadata to be used by tools and resources.",
            "dependencies": [
              1
            ],
            "details": "Update 'src/lms_lmstxt_mcp/models.py' to include an 'ArtifactMetadata' model with fields: 'filename', 'size_bytes', 'last_modified', and 'uri'. This ensures consistent formatting between the scanner and the API.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2026-01-05T06:31:45.456Z"
          },
          {
            "id": 3,
            "title": "Implement 'lmstxt_list_all_artifacts' MCP Tool",
            "description": "Expose the discovered artifacts via a specialized MCP tool for client discovery.",
            "dependencies": [
              1,
              2
            ],
            "details": "In 'src/lms_lmstxt_mcp/server.py', register a new tool using the FastMCP decorator. The tool should call 'scan_artifacts', map results to 'ArtifactMetadata', and return a list of all persistent files including those from previous sessions.",
            "status": "done",
            "testStrategy": "Manual verification via MCP Inspector by calling the tool and checking if it lists files manually added to the artifacts folder.",
            "parentId": "undefined",
            "updatedAt": "2026-01-05T06:32:30.942Z"
          },
          {
            "id": 4,
            "title": "Implement MCP Resource Handlers",
            "description": "Register dynamic resource listing and reading handlers for the artifact URI scheme.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement '@mcp.list_resources()' and '@mcp.read_resource()' in 'server.py'. Define the URI scheme as 'lmstxt://artifacts/{filename}'. Ensure 'list_resources' populates the list based on current disk state.",
            "status": "done",
            "testStrategy": "Use an MCP client to list resources and verify the URIs match the expected scheme. Attempt to read a specific resource by URI.",
            "parentId": "undefined",
            "updatedAt": "2026-01-05T06:32:30.948Z"
          },
          {
            "id": 5,
            "title": "Integrate Security and Content Truncation",
            "description": "Apply path validation and length limits to artifact resource retrieval.",
            "dependencies": [
              4
            ],
            "details": "In the 'read_resource' handler, wrap file access with 'security.validate_output_dir'. Use 'hashing.read_text_preview' to read content, passing the 'MAX_CHARS' limit from the application configuration to prevent memory issues.",
            "status": "done",
            "testStrategy": "Attempt to access a file outside the artifacts directory via the resource URI to verify rejection. Verify that files exceeding MAX_CHARS are correctly truncated with a preview banner.",
            "parentId": "undefined",
            "updatedAt": "2026-01-05T06:32:30.951Z"
          }
        ],
        "complexity": 4,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Please break Task 12 into subtasks covering: 1) Implementation of 'scan_artifacts' in 'artifacts.py' using pathlib. 2) Updating 'models.py' to include schemas for artifact metadata. 3) Implementing the 'lmstxt_list_all_artifacts' tool in 'server.py'. 4) Implementing MCP resource handlers ('list_resources' and 'read_resource') for the artifact URI scheme. 5) Integration of 'security.validate_output_dir' and 'hashing.read_text_preview' within the resource read flow.",
        "updatedAt": "2026-01-05T06:32:30.951Z"
      },
      {
        "id": 13,
        "title": "Migrate Artifact Directory Path from 'output' to 'artifacts'",
        "description": "Update the default storage and retrieval path for MCP artifacts from './output' to './artifacts' in configuration and tool implementations.",
        "details": "This task involves standardizing the naming convention for the artifact storage directory. 1) In 'src/lms_lmstxt_mcp/config.py', locate the default configuration variable (likely 'DEFAULT_OUTPUT_DIR' or 'OUTPUT_PATH') and update its value to 'artifacts'. 2) In 'src/lms_lmstxt_mcp/server.py', verify that the 'list_all' and 'read_artifacts' tools utilize the updated configuration rather than hardcoded paths. 3) Update any tool descriptions or help text in the MCP server metadata that reference the directory name to ensure they reflect the change. 4) Verify that the logic in 'artifacts.py' (implemented in Task 5 and 12) correctly inherits this change via the config module, ensuring that path resolution via 'pathlib' correctly points to the new directory. 5) Ensure that the generator integration from Task 6 now writes files to the 'artifacts' directory.",
        "testStrategy": "1) Delete any existing './output' directory. 2) Run the MCP server and trigger an artifact generation; verify that the './artifacts' directory is created and contains the new file. 3) Use an MCP client to call 'list_all' and verify it lists files within 'artifacts/'. 4) Call 'read_artifacts' with a filename in the new directory and verify successful content retrieval. 5) Confirm no new files are being written to './output'.",
        "status": "done",
        "dependencies": [
          "5",
          "6",
          "12"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Update Default Directory in Configuration",
            "description": "Modify 'src/lms_lmstxt_mcp/config.py' to change the default artifact directory path from 'output' to 'artifacts'.",
            "dependencies": [],
            "details": "Locate the constant or Pydantic field defining the output path (e.g., 'DEFAULT_OUTPUT_DIR' or 'OUTPUT_PATH') and update its default value to 'artifacts'. Ensure any environment variable mapping is preserved.",
            "status": "done",
            "testStrategy": "Unit test 'config.py' to ensure that the settings object returns 'artifacts' as the default directory path when no environment variable is set.",
            "updatedAt": "2026-01-05T07:16:07.944Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Update MCP Server Tool Metadata",
            "description": "Update tool descriptions and help text in 'src/lms_lmstxt_mcp/server.py' to reference the new directory name.",
            "dependencies": [
              1
            ],
            "details": "Search for string literals or docstrings in 'server.py' that mention the 'output' directory. Update these to 'artifacts' so that MCP client UI reflects the correct path in help text for tools like 'list_all' and 'lmstxt_generate'.",
            "status": "done",
            "testStrategy": "Examine the MCP server metadata (via 'mcp list-tools') to verify that descriptions for artifact-related tools correctly mention 'artifacts'.",
            "parentId": "undefined",
            "updatedAt": "2026-01-05T07:16:07.950Z"
          },
          {
            "id": 3,
            "title": "Align Artifact Discovery and Resource Logic",
            "description": "Ensure 'src/lms_lmstxt_mcp/artifacts.py' correctly uses the updated config for file scanning and URI generation.",
            "dependencies": [
              1
            ],
            "details": "Verify 'scan_artifacts' uses 'config.output_path' instead of hardcoded strings. Update the resource URI logic (e.g., 'lmstxt://artifacts/{filename}') to ensure internal path resolution points to the new directory on disk.",
            "status": "done",
            "testStrategy": "Verify that 'scan_artifacts' successfully finds files placed manually in './artifacts/' and that 'list_resources' returns URIs correctly mapped to that location.",
            "parentId": "undefined",
            "updatedAt": "2026-01-05T07:16:07.953Z"
          },
          {
            "id": 4,
            "title": "Update Generator Execution Integration",
            "description": "Update the 'lmstxt_generate' tool implementation to write generated files to the 'artifacts' directory.",
            "dependencies": [
              1
            ],
            "details": "In 'src/lms_lmstxt_mcp/server.py', ensure the call to 'run_generation' (or the internal generator wrapper) uses the path defined in the configuration. Confirm that file write operations target the updated directory.",
            "status": "done",
            "testStrategy": "Run a generation task through the MCP server and check that the resulting .txt files appear in './artifacts/' and NOT in './output/'.",
            "parentId": "undefined",
            "updatedAt": "2026-01-05T07:16:07.955Z"
          },
          {
            "id": 5,
            "title": "Validate Migration and Cleanup",
            "description": "Perform a full system check of the migration and remove legacy directory references.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Delete the old './output' directory. Run the server and confirm all artifact-related operations (generate, list, read) function correctly using the new path. Update any README or documentation that mentions the folder structure.",
            "status": "done",
            "testStrategy": "End-to-end verification: 1) Generation creates './artifacts'. 2) 'list_resources' identifies the file. 3) 'read_resource' retrieves the file content successfully from the new path.",
            "parentId": "undefined",
            "updatedAt": "2026-01-05T07:16:07.958Z"
          }
        ],
        "updatedAt": "2026-01-05T07:16:07.958Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2026-01-05T07:16:07.959Z",
      "taskCount": 13,
      "completedCount": 13,
      "tags": [
        "feat-mcp"
      ],
      "created": "2026-01-06T16:42:55.011Z",
      "description": "Tasks for feat-mcp context",
      "updated": "2026-01-06T16:42:55.011Z"
    }
  },
  "chore-dist-name": {
    "tasks": [
      {
        "id": 12,
        "title": "Research and Select New Distribution Name",
        "description": "Research and selection process for the Python package name. 'lms-lmstxt' has been selected as the new distribution name to align with the repository name and maintain consistency across the project.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "details": null,
        "testStrategy": "Availability was tested by running 'pip index versions lms-lmstxt' and confirming that no packages were returned. Compliance was verified against the PyPA specifications (PEP 508) for naming conventions, ensuring underscores are used for imports while hyphens are used for the distribution name (e.g., 'lms-lmstxt' as distribution vs 'import lms_lmstxt' as package). Success is also contingent on the name matching the GitHub repository name 'lms-lmstxt'. \n\nChecked using:\n$ pip index versions lms-lmstxt\nERROR: No matching distributions found for lms-lmstxt\n\nVerified PEP 508 normalization rules where '-' is canonical for distributions and '_' is canonical for importable packages/modules in Python source code (src/lms_lmstxt).\",\n  \"details\": \"The research phase originally considered 'lmstxt' but has been finalized with 'lms-lmstxt' to match the repository name. This choice provides better context for users regarding the origin of the tool while remaining concise. Availability has been confirmed via PyPI index lookups.\n\nKey Selection Criteria Met:\n- Availability: 'lms-lmstxt' is not registered on PyPI.\n- PEP 508: Follows normalization rules (lowercase, alphanumeric with hyphens allowed).\n- Consistency: Matches the repository name 'lms-lmstxt'.\",\n  \"subtasks\": [\n    {\n      \"id\": 1,\n      \"title\": \"Brainstorm potential distribution names\",\n      \"description\": \"Evaluate candidates like 'lmstxt', 'lmstxt', and 'lms-lmstxt' for brevity and clarity.\",\n      \"dependencies\": [],\n      \"details\": \"Comparison was made between 'lmstudio-lmstxt-generator' (current) and shorter alternatives. 'lmstxt' was selected for its direct association with the LLM-readable text format.\",\n      \"status\": \"done\",\n      \"testStrategy\": null\n    },\n    {\n      \"id\": 2,\n      \"title\": \"Verify 'lmstxt' availability on PyPI\",\n      \"description\": \"Ensure the chosen name is available for registration and does not conflict with existing packages.\",\n      \"dependencies\": [],\n      \"details\": \"Verified via 'pip index versions lmstxt' and manual PyPI search. No matching distributions were found, making it safe for adoption.\",\n      \"status\": \"done\",\n      \"testStrategy\": null\n    },\n    {\n      \"id\": 3,\n      \"title\": \"Confirm PEP 508 normalization compliance\",\n      \"description\": \"Validate that the name follows the standard for Python distribution names.\",\n      \"dependencies\": [],\n      \"details\": \"The name 'lmstxt' consists only of lowercase alphanumeric characters, which is the canonical form recommended by PEP 508 and PEP 621.\",\n      \"status\": \"done\",\n      \"testStrategy\": null\n    },\n    {\n      \"id\": 4,\n      \"title\": \"Update final selection to 'lms-lmstxt'\",\n      \"description\": \"Adjust the selection from 'lmstxt' to 'lms-lmstxt' to maintain consistency with the repository name.\",\n      \"dependencies\": [\n        1,\n        2,\n        3\n      ],\n      \"details\": \"While 'lmstxt' was the initial choice, 'lms-lmstxt' was finalized to ensure the PyPI package name matches the GitHub repository exactly.\",\n      \"status\": \"done\",\n      \"testStrategy\": \"Manual verification that the repository name and intended package name are identical.\"\n    },\n    {\n      \"id\": 5,\n      \"title\": \"Verify 'lms-lmstxt' availability on PyPI\",\n      \"description\": \"Check the PyPI index for the final selected name 'lms-lmstxt'.\",\n      \"dependencies\": [\n        4\n      ],\n      \"details\": \"Run 'pip index versions lms-lmstxt' to ensure no existing package uses this name.\",\n      \"status\": \"done\",\n      \"testStrategy\": \"Execution of 'pip index versions lms-lmstxt' returned no results.\"\n    },\n    {\n      \"id\": 6,\n      \"title\": \"Confirm PEP 508 compliance for 'lms-lmstxt'\",\n      \"description\": \"Ensure the hyphenated name follows Python packaging standards and establish the import name.\",\n      \"dependencies\": [\n        4\n      ],\n      \"details\": \"Confirmed that 'lms-lmstxt' is a valid distribution name. The corresponding Python import package will be 'lms_lmstxt' (normalized with underscores).\",\n      \"status\": \"done\",\n      \"testStrategy\": \"Cross-referenced with PEP 508 and PEP 621 naming specifications.\"\n    }\n  ]\n}\n}",
        "subtasks": [
          {
            "id": 1,
            "title": "Brainstorm potential distribution names",
            "description": "Evaluate candidates like 'lmstxt', 'lmstxt', and 'lms-lmstxt' for brevity and clarity.",
            "dependencies": [],
            "details": "Comparison was made between 'lmstudio-lmstxt-generator' (current) and shorter alternatives. 'lmstxt' was selected for its direct association with the LLM-readable text format.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Verify 'lmstxt' availability on PyPI",
            "description": "Ensure the chosen name is available for registration and does not conflict with existing packages.",
            "dependencies": [],
            "details": "Verified via 'pip index versions lmstxt' and manual PyPI search. No matching distributions were found, making it safe for adoption.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Confirm PEP 508 normalization compliance",
            "description": "Validate that the name follows the standard for Python distribution names.",
            "dependencies": [],
            "details": "The name 'lmstxt' consists only of lowercase alphanumeric characters, which is the canonical form recommended by PEP 508 and PEP 621.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-06T16:53:50.485Z"
      },
      {
        "id": 13,
        "title": "Update [project].name in pyproject.toml",
        "description": "Formally change the distribution name in the project configuration file.",
        "details": "1. Open 'pyproject.toml'.\n2. Locate the '[project]' section.\n3. Update the 'name' field to the selected new distribution name.\n4. Ensure it remains a static string as per PEP 621.",
        "testStrategy": "Run 'python -m build' and check the generated .whl and .tar.gz filenames in the 'dist/' directory to ensure they reflect the new name.",
        "priority": "high",
        "dependencies": [
          "12"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-06T17:05:12.032Z"
      },
      {
        "id": 14,
        "title": "Rename CLI Entry Points in pyproject.toml",
        "description": "Decide and implement changes to the command names users type in the terminal.",
        "details": "1. Locate '[project.scripts]' in 'pyproject.toml'.\n2. Review 'lmstxt' and 'lmstxt-mcp'.\n3. If aligning with the new distribution name, update the keys (the command names).\n4. Update the values (module:function mapping) if the module structure is planned to change.",
        "testStrategy": "After local installation, verify that the new command names work (e.g., 'new-command --help') and the old ones (if removed) are no longer available.",
        "priority": "medium",
        "dependencies": [
          "13"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-06T17:05:12.035Z"
      },
      {
        "id": 15,
        "title": "Rename Internal Source Package and Modules",
        "description": "Rename the directories under 'src/' to match the new distribution name for internal consistency.",
        "details": "1. Identify the current package directory (likely 'src/lmstudio_lmstxt_generator').\n2. Rename this directory to the new normalized name (e.g., 'src/lmstxt').\n3. Use underscores for the directory name if the distribution name has hyphens (Python import convention).",
        "testStrategy": "Confirm the directory structure has changed and ensure no 'pyc' files remain from the old structure.",
        "priority": "medium",
        "dependencies": [
          "14"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-06T17:05:12.038Z"
      },
      {
        "id": 16,
        "title": "Update Internal Code Imports",
        "description": "Search and replace all instances of the old package name in import statements and code references.",
        "details": "1. Search for 'from lmstudio_lmstxt_generator ...' and 'import lmstudio_lmstxt_generator'.\n2. Update them to the new package name.\n3. Check '__init__.py' files for any relative import or metadata exports that might be affected.",
        "testStrategy": "Run existing unit tests to ensure imports are correctly resolved and the application logic is intact.",
        "priority": "high",
        "dependencies": [
          "15"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-06T17:05:12.041Z"
      },
      {
        "id": 17,
        "title": "Update Documentation and Installation Snippets",
        "description": "Refresh the README and any other documentation with the new distribution name and installation instructions.",
        "details": "1. Update 'pip install' commands in README.md.\n2. Update 'pipx' or 'uv' usage examples.\n3. Update any mentions of the project name in the description or header sections.\n4. Update repository badges if they contain the old name (e.g., PyPI version badges).",
        "testStrategy": "Manual review of documentation to ensure all references to the old name are gone.",
        "priority": "medium",
        "dependencies": [
          "13"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-06T17:05:12.043Z"
      },
      {
        "id": 18,
        "title": "Update Release Automation and CI/CD Config",
        "description": "Ensure the GitHub Actions or other CI/CD pipelines correctly handle the new package name for publishing.",
        "details": "1. Check '.github/workflows/' for any environment variables referencing the old name.\n2. Update Trusted Publisher settings on PyPI if applicable to authorize the new project name.\n3. Update any release scripts that specifically target the old distribution.",
        "testStrategy": "Trigger a dry-run of the CI/CD pipeline if possible, or verify YAML configuration values.",
        "priority": "high",
        "dependencies": [
          "13"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-06T17:05:12.046Z"
      },
      {
        "id": 19,
        "title": "Create Migration Strategy for PyPI (If Applicable)",
        "description": "Plan the deprecation of the old package name on PyPI by creating a transition package.",
        "details": "1. Prepare a final release for 'lmstudio-lmstxt-generator'.\n2. Set the new distribution as a dependency in the old package's 'pyproject.toml'.\n3. Add a post-install message or a deprecation warning in the old package to notify users.",
        "testStrategy": "Verify 'install_requires' in the old package points to the new package name in a local environment.",
        "priority": "low",
        "dependencies": [
          "13"
        ],
        "status": "deferred",
        "subtasks": [],
        "updatedAt": "2026-01-06T17:05:18.341Z"
      },
      {
        "id": 20,
        "title": "Local Build and Install Verification",
        "description": "Perform a final end-to-end local installation check in a clean environment.",
        "details": "1. Clear 'dist/' and 'build/' folders.\n2. Run 'python -m build'.\n3. Create a fresh virtual environment ('python -m venv venv_test').\n4. Install the wheel: 'pip install dist/*.whl'.\n5. Verify commands: 'lmstxt --help' (or new command names).",
        "testStrategy": "The installation should complete without errors and the CLI tool should execute successfully in the clean environment.",
        "priority": "high",
        "dependencies": [
          "16",
          "18"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-06T17:05:12.049Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2026-01-06T17:05:18.342Z",
      "taskCount": 9,
      "completedCount": 8,
      "tags": [
        "chore-dist-name"
      ],
      "created": "2026-01-08T02:14:43.900Z",
      "description": "Tasks for chore-dist-name context",
      "updated": "2026-01-08T02:14:43.900Z"
    }
  },
  "chore-update": {
    "tasks": [
      {
        "id": 1,
        "title": "Define Event Models and Schemas",
        "description": "Create the core event definitions and typing structure to serve as the foundation for the streaming architecture.",
        "details": "Implement `src/lms_llmsTxt_mcp/events.py`. Define a `RunEvent` Pydantic model containing `run_id`, `sequence_number`, `timestamp`, `type`, and `payload`. Create an `EventType` Enum including `run.started`, `progress`, `log`, `artifact.partial`, `run.completed`, `run.failed`, and `run.canceled`. Define payload schemas for each event type to ensure strict typing. Ensure serialization/deserialization logic is robust.",
        "testStrategy": "Unit tests ensuring all event types can be instantiated, serialized to JSON, and deserialized back to the correct Pydantic models. Validate required fields.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define EventType Enum",
            "description": "Create the EventType enumeration to categorize the different stages and types of events in the system.",
            "dependencies": [],
            "details": "Create `src/lms_llmsTxt_mcp/events.py` if it does not exist. Define a Python Enum named `EventType` inheriting from `str` and `Enum`. It must include members: `RUN_STARTED = 'run.started'`, `PROGRESS = 'progress'`, `LOG = 'log'`, `ARTIFACT_PARTIAL = 'artifact.partial'`, `RUN_COMPLETED = 'run.completed'`, `RUN_FAILED = 'run.failed'`, and `RUN_CANCELED = 'run.canceled'`.",
            "status": "pending",
            "testStrategy": "Verify that all required enum members exist and string values match the specification."
          },
          {
            "id": 2,
            "title": "Define Event Payload Models",
            "description": "Implement specific Pydantic models for the payload of each event type to ensure strict schema validation.",
            "dependencies": [
              1
            ],
            "details": "In `src/lms_llmsTxt_mcp/events.py`, define Pydantic models for payloads: `RunStartedPayload` (inputs), `ProgressPayload` (percentage, current_step), `LogPayload` (level, message), `ArtifactPartialPayload` (delta, artifact_id), `RunCompletedPayload` (result), `RunFailedPayload` (error_code, message, traceback), and `RunCanceledPayload` (reason). Ensure all inherit from `pydantic.BaseModel`.",
            "status": "pending",
            "testStrategy": "Unit tests ensuring valid data instantiates correctly and invalid data raises ValidationError."
          },
          {
            "id": 3,
            "title": "Create RunEvent Model",
            "description": "Define the main envelope model that wraps the specific payload and provides metadata like timestamp and sequence number.",
            "dependencies": [
              1,
              2
            ],
            "details": "In `src/lms_llmsTxt_mcp/events.py`, define `RunEvent`. It must have fields: `run_id` (str), `sequence_number` (int), `timestamp` (datetime, defaulting to `datetime.utcnow`), `type` (EventType), and `payload` (Union of the payload models defined in subtask 2). Use a Pydantic discriminator if possible for polymorphic parsing, or ensure validation checks the payload matches the type.",
            "status": "pending",
            "testStrategy": "Test instantiation of RunEvent with various payloads to ensure type/payload matching."
          },
          {
            "id": 4,
            "title": "Implement Serialization Utilities",
            "description": "Add functionality to serialize events to JSON strings compatible with SSE (Server-Sent Events) or standard JSON lines.",
            "dependencies": [
              3
            ],
            "details": "Add a method `to_json()` to `RunEvent` (or a standalone utility function) that dumps the model using `model_dump_json()`. Ensure that `datetime` objects are serialized to ISO format strings. Create a factory method `from_json(json_str)` to reconstruct the object, handling polymorphic payload resolution.",
            "status": "pending",
            "testStrategy": "Round-trip tests: Serialize an event to JSON and deserialize it back, checking for equality."
          },
          {
            "id": 5,
            "title": "Create Unit Tests for Event Schemas",
            "description": "Implement a comprehensive test suite to validate the robustness of the event models.",
            "dependencies": [
              3,
              4
            ],
            "details": "Create `tests/test_events.py`. Include tests for: 1. Successful creation of every event type. 2. Validation errors for missing fields. 3. Mismatches between `type` and `payload` (if validation logic exists). 4. JSON serialization format (verifying fields like `timestamp` are formatted correctly).",
            "status": "pending",
            "testStrategy": null
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement In-Process Event Bus",
        "description": "Build a thread-safe publish/subscribe mechanism to manage event flow for individual runs.",
        "details": "Implement `src/lms_llmsTxt_mcp/event_bus.py`. The `EventBus` class should support `subscribe(run_id) -> Iterator` and `publish(event)`. Use `queue.Queue` or `asyncio.Queue` (depending on the async nature of the server) to handle buffering. Implement logic to handle backpressure or bounded buffers to prevent memory leaks if consumers are slow. Include a `close_run(run_id)` method to clean up resources.",
        "testStrategy": "Unit tests verifying that multiple subscribers receive events in order. Test fan-out capabilities (one publisher, multiple subscribers). Test buffer limits and cleanup on run close.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Core EventBus Pub/Sub with asyncio",
            "description": "Create the basic EventBus class structure and implement the core mechanism for publishing events and allowing components to subscribe via asyncio queues.",
            "dependencies": [],
            "details": "Create `src/lms_llmsTxt_mcp/event_bus.py`. Define the `EventBus` class. Implement the `subscribe(run_id)` method which should initialize a bounded `asyncio.Queue` for the specific run and return an async generator/iterator to yield events. Implement the `publish(event)` method to iterate through all active queues for the given `run_id` and put the event into them. Ensure the queues use a reasonable maxsize to handle backpressure.",
            "status": "pending",
            "testStrategy": "Unit test verifying that a message published to a specific run_id is correctly received by an async subscriber."
          },
          {
            "id": 2,
            "title": "Implement Subscriber Management and Cleanup Logic",
            "description": "Add lifecycle management to the EventBus to handle adding/removing subscribers and properly cleaning up resources when a run is finished.",
            "dependencies": [
              1
            ],
            "details": "Extend `src/lms_llmsTxt_mcp/event_bus.py`. Implement internal storage (e.g., `Dict[str, List[Queue]]`) to track subscribers per run. Implement `close_run(run_id)` which should signal completion to all subscriber queues (e.g., via a sentinel value or closing the queue) and remove the run from the internal registry to free memory. Ensure thread-safety principles are applied if the bus is accessed from multiple contexts.",
            "status": "pending",
            "testStrategy": "Test subscribing multiple listeners, then calling close_run; verify all listeners terminate gracefully and the registry is empty."
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Completion Controller",
        "description": "Create a controller to enforce deterministic run completion semantics, ensuring exactly one terminal event is emitted.",
        "details": "Implement `src/lms_llmsTxt_mcp/completion.py`. Create `CompletionController` class that wraps the EventBus publishing logic for terminal states. It should track the state of a run and prevent invalid transitions (e.g., emitting 'completed' after 'failed'). Implement methods: `mark_failed(error)`, `mark_completed(metadata)`, `mark_canceled(reason)`. Ensure `ensure_terminal_on_exit()` pattern handles worker thread termination.",
        "testStrategy": "Unit tests simulating race conditions (e.g., success and error called nearly simultaneously). Verify only the first terminal event is emitted and subsequent attempts are ignored.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Run States and Transition Logic",
            "description": "Define the enumeration of possible run states and implement the validation logic for state transitions.",
            "dependencies": [],
            "details": "In `src/lms_llmsTxt_mcp/completion.py`, create a `RunState` enum (e.g., PENDING, RUNNING, COMPLETED, FAILED, CANCELED). Implement a private method `_can_transition(current, new)` to strictly enforce invalid state changes (e.g., terminal states cannot transition to anything else).",
            "status": "pending",
            "testStrategy": "Unit tests verifying that illegal transitions return False and legal ones return True."
          },
          {
            "id": 2,
            "title": "Create CompletionController Class Skeleton",
            "description": "Scaffold the CompletionController class with initialization logic and Event Bus integration.",
            "dependencies": [
              1
            ],
            "details": "Create the `CompletionController` class in `src/lms_llmsTxt_mcp/completion.py`. The constructor should accept an `EventBus` instance and a `run_id`. Initialize an internal lock (e.g., `threading.Lock`) to ensure thread safety for state updates.",
            "status": "pending",
            "testStrategy": "Verify the class instantiates correctly and stores the reference to the EventBus."
          },
          {
            "id": 3,
            "title": "Implement Terminal State Methods",
            "description": "Implement the core methods to mark the run as completed, failed, or canceled.",
            "dependencies": [
              2
            ],
            "details": "Implement `mark_completed(metadata)`, `mark_failed(error)`, and `mark_canceled(reason)`. Each method must acquire the lock, check if the transition is valid, update the internal state, and publish the specific terminal event to the `EventBus`. If the run is already terminal, these calls should be ignored or log a warning, but not raise an exception.",
            "status": "pending",
            "testStrategy": "Simulate concurrent calls to different mark methods and ensure only the first one wins (publishes an event)."
          },
          {
            "id": 4,
            "title": "Implement Context Manager for Automatic Termination",
            "description": "Create a context manager mechanism to ensure a terminal event is always emitted when a worker exits.",
            "dependencies": [
              3
            ],
            "details": "Implement `ensure_terminal_on_exit()` as a context manager (`__enter__` and `__exit__`). In `__exit__`, check if the run is still in a non-terminal state. If so, default to `mark_failed` with a generic 'Unexpected termination' error. This handles cases where code crashes or returns without explicitly setting a status.",
            "status": "pending",
            "testStrategy": "Use the context manager in a test block that raises an unhandled exception. Verify that `mark_failed` is called automatically."
          },
          {
            "id": 5,
            "title": "Integrate with Event Definitions",
            "description": "Ensure the events emitted match the expected schema defined in the project.",
            "dependencies": [
              3
            ],
            "details": "Review the event schemas (likely defined in `events.py` or similar). Ensure that `mark_completed` emits a `run.completed` event, `mark_failed` emits `run.failed`, and `mark_canceled` emits `run.canceled` with the correct payload structures (e.g., including error messages or result metadata).",
            "status": "pending",
            "testStrategy": "inspect the specific event objects published to the mock EventBus to ensure payload correctness."
          }
        ]
      },
      {
        "id": 4,
        "title": "Refactor Pipeline for Milestone Emission",
        "description": "Instrument the existing synchronous generation pipeline to emit status events at key milestones.",
        "details": "Modify `src/lms_llmsTxt/pipeline.py` or create a wrapper in `src/lms_llmsTxt_mcp/runner.py` (`run_llmstxt_with_events`). Inject the `EventBus` or a callback function into the `run_generation` logic. Emit events for: 'start', 'repo_fetched', 'analysis_started', 'writing_files', and 'done'. Ensure existing file I/O behavior remains unchanged.",
        "testStrategy": "Integration test running the pipeline with a mock EventBus. Verify that the expected sequence of milestone events is received while the actual artifacts are still created on disk.",
        "priority": "medium",
        "dependencies": [
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Instrument Core Pipeline with Event Hooks",
            "description": "Modify the core pipeline logic to support an optional callback mechanism for status updates without altering synchronous behavior.",
            "dependencies": [],
            "details": "Analyze `src/lms_llmsTxt/pipeline.py`. Update the main entry function (e.g., `run_generation`) to accept an optional `on_event` callable. Insert calls to this function at key execution points: 'start', 'repo_fetched', 'analysis_started', 'writing_files', and 'done'. Ensure the function defaults to no-op if no callback is provided to maintain backward compatibility.",
            "status": "pending",
            "testStrategy": "Unit test the modified pipeline function with a mock callback to verify it is called the expected number of times with correct arguments."
          },
          {
            "id": 2,
            "title": "Implement Event-Aware Runner Adapter",
            "description": "Create a wrapper function in the MCP layer to bridge the core pipeline's new callback interface with the system EventBus.",
            "dependencies": [
              1
            ],
            "details": "Implement `run_llmstxt_with_events` in `src/lms_llmsTxt_mcp/runner.py`. This function should accept a `run_id` and the `EventBus` instance. It invokes the modified pipeline from Subtask 1, passing a lambda/callback that wraps the raw strings into formatted Event objects and calls `event_bus.publish()`. Ensure it handles context passing correctly.",
            "status": "pending",
            "testStrategy": "Unit test the adapter logic to ensure it correctly translates string tokens from the pipeline into structured Event objects sent to the mock bus."
          },
          {
            "id": 3,
            "title": "Verify Milestone Emission via Integration Tests",
            "description": "Develop and run integration tests to confirm the full sequence of events is emitted during a generation run.",
            "dependencies": [
              2
            ],
            "details": "Create an integration test in the test suite that sets up a mock EventBus and runs `run_llmstxt_with_events`. Verify the sequence: start -> repo_fetched -> analysis_started -> writing_files -> done. Check that the pipeline still produces the actual output files on disk (or mocked FS) to ensure no regression in functionality.",
            "status": "pending",
            "testStrategy": null
          }
        ]
      },
      {
        "id": 5,
        "title": "Integrate DSPy Streaming Hooks",
        "description": "Connect DSPy's internal streaming mechanisms to the EventBus to provide real-time feedback.",
        "details": "Research the installed `dspy` version's streaming capabilities (e.g., `streamify` or callbacks). In `src/lms_llmsTxt/lmstudio.py` or the runner, attach a listener to the DSPy LM or Program. Map DSPy internal events (token generation, intermediate reasoning steps) to `progress` or `log` events in the `EventBus`. Implement throttling for token-level events to avoid flooding the bus.",
        "testStrategy": "Integration test with a mocked LLM backend or a small local model. Verify that token/status events are flowing through to the EventBus during a generation task.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Prototype DSPy Streaming Interface",
            "description": "Investigate the specific streaming capabilities of the installed DSPy version and verify connectivity with the backend.",
            "dependencies": [],
            "details": "Create a standalone script or unit test to verify how the `dspy.LM` or `dspy.Module` yields tokens. Determine if `stream=True` returns a generator or requires a callback handler. Confirm compatibility with the LM Studio backend usage to ensure tokens can be intercepted in real-time.",
            "status": "pending",
            "testStrategy": "Create a temporary script `scripts/test_dspy_stream.py` that connects to LM Studio and prints tokens to stdout as they are generated."
          },
          {
            "id": 2,
            "title": "Implement DSPy to EventBus Adapter",
            "description": "Develop the logic to translate DSPy's internal generation events into the project's standard Event objects.",
            "dependencies": [
              1
            ],
            "details": "Create a wrapper or callback class in `src/lms_llmsTxt/lmstudio.py` that intercepts token generation. Map these raw inputs into the `RunEvent` schema (specifically `progress` events for tokens and `log` events for reasoning steps), ensuring correct timestamps and run IDs are attached to each event payload.",
            "status": "pending",
            "testStrategy": "Unit test the adapter by feeding it mock DSPy token streams and asserting that it produces correctly formatted `RunEvent` objects."
          },
          {
            "id": 3,
            "title": "Integrate Streaming with Throttling Logic",
            "description": "Hook the adapter into the main execution runner and apply rate-limiting to high-frequency events.",
            "dependencies": [
              2
            ],
            "details": "Modify the runner to use the streaming adapter during execution. Implement a throttling mechanism (e.g., publish only every 100ms or every 5 tokens) for `progress` events to prevent EventBus congestion. Ensure the final result is still captured accurately regardless of throttling.",
            "status": "pending",
            "testStrategy": "Integration test: Run a generation task and verify that the EventBus receives a stream of events that are spaced out according to the throttling logic, rather than one event per token."
          }
        ]
      },
      {
        "id": 6,
        "title": "Implement Streaming MCP Tool",
        "description": "Create the new `lmstxt_generate_llms_txt_stream` tool that exposes the event stream to MCP clients.",
        "details": "In `src/lms_llmsTxt_mcp/streaming_tool.py`, define the new tool function using `FastMCP` decorators. This function should: 1. Generate a `run_id`. 2. Initialize the `EventBus` for this run. 3. Spawn the background worker (using the runner from Task 4). 4. Return an iterator/generator that yields SSE-formatted events from the bus to the client. Ensure it handles the terminal event to close the stream.",
        "testStrategy": "End-to-end test using an MCP client (or mock). Call the streaming tool and assert that a stream of JSON events is received, ending with a terminal event.",
        "priority": "high",
        "dependencies": [
          5
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold Streaming Tool and Initialization",
            "description": "Create the `streaming_tool.py` module and define the FastMCP tool entry point `lmstxt_generate_llms_txt_stream`, including unique run ID generation and EventBus setup.",
            "dependencies": [],
            "details": "Create `src/lms_llmsTxt_mcp/streaming_tool.py`. Import `FastMCP` and the `EventBus` class. Define the tool function `lmstxt_generate_llms_txt_stream` with appropriate arguments (url, options). Inside the function, generate a UUID v4 `run_id` and initialize the `EventBus` for this specific run. Return the `run_id` and initialized bus for the next steps.",
            "status": "pending",
            "testStrategy": "Unit test the function to verify it returns a valid UUID and initializes an EventBus instance without errors."
          },
          {
            "id": 2,
            "title": "Implement Background Runner Execution",
            "description": "Implement the logic to spawn the LLM generation runner in a non-blocking background thread to prevent blocking the async event loop.",
            "dependencies": [
              1
            ],
            "details": "Integrate the runner logic (from Task 4). Use `asyncio.create_task` combined with `asyncio.to_thread` (or a `ThreadPoolExecutor`) to execute the synchronous generation process in the background. Pass the `run_id`, `event_bus`, and input arguments to the runner. Ensure exceptions in the thread are caught and published as failure events to the bus.",
            "status": "pending",
            "testStrategy": "Mock the runner function and verify that calling the tool spawns the thread and that the main async loop remains responsive."
          },
          {
            "id": 3,
            "title": "Implement Event-to-SSE Bridge Generator",
            "description": "Create the asynchronous generator that consumes events from the EventBus subscription and yields them to the client until a terminal event is reached.",
            "dependencies": [
              2
            ],
            "details": "Implement an `async for` loop iterating over `event_bus.subscribe(run_id)`. Transform internal event objects into the required SSE format (or JSON dictionaries if FastMCP handles wrapping). Detect terminal events (completed, failed, canceled) to break the loop and perform cleanup. This generator is the return value of the tool function.",
            "status": "pending",
            "testStrategy": "Simulate a sequence of events (start, progress, complete) in the EventBus and verify the generator yields them correctly and stops after the terminal event."
          }
        ]
      },
      {
        "id": 7,
        "title": "Implement Run Cancellation Support",
        "description": "Add mechanisms to interrupt a running generation task and clean up resources.",
        "details": "Extend `EventBus` and `Runner` to support cancellation. Add a `cancel_run` method in the bus that sets a flag. Update the generation loop in `pipeline.py` (or the wrapper) to check this flag periodically and abort execution. Ensure `CompletionController.mark_canceled` is called. Create a new tool `lmstxt_cancel_run(run_id)` in `server.py`.",
        "testStrategy": "Start a long-running generation task, then immediately call the cancel tool. Verify that the stream receives a `run.canceled` event and the background thread terminates.",
        "priority": "medium",
        "dependencies": [
          6
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Core Cancellation Logic in EventBus and Runner",
            "description": "Modify the EventBus to support a cancellation state and update the execution loop to respect it.",
            "dependencies": [],
            "details": "Modify the `EventBus` class to include a thread-safe cancellation mechanism (e.g., `threading.Event`). Update the main generation loop in `pipeline.py` to check `event_bus.is_canceled()` at each step. If true, break the loop and trigger `CompletionController.mark_canceled` to ensure the correct terminal event is emitted and resources are released.",
            "status": "pending",
            "testStrategy": "Create a unit test with a mock long-running loop that checks the flag. Trigger the cancellation from a separate thread and assert the loop terminates early with a 'canceled' status."
          },
          {
            "id": 2,
            "title": "Implement lmstxt_cancel_run Tool",
            "description": "Expose the cancellation functionality via a new MCP tool in the server.",
            "dependencies": [
              1
            ],
            "details": "In `src/lms_llmsTxt_mcp/server.py`, register a new tool `lmstxt_cancel_run(run_id: str)`. This function should look up the active `EventBus` or context for the provided `run_id`. If found, call the cancellation method implemented in the previous subtask. Handle edge cases where the run ID does not exist or the task is already finished.",
            "status": "pending",
            "testStrategy": "Integration test: Start a generation task via the API, immediately call `lmstxt_cancel_run` with the returned run ID, and verify the stream ends with a cancellation event."
          }
        ]
      },
      {
        "id": 8,
        "title": "Register Tools and Update Server Entry Point",
        "description": "Expose the new streaming tools in the main server definition while preserving backward compatibility.",
        "details": "Update `src/lms_llmsTxt_mcp/server.py`. Import and register `lmstxt_generate_llms_txt_stream` and other streaming variants. Ensure existing tools (`lmstxt_generate_llms_txt`, `lmstxt_list_runs`) remain registered and functional. Verify that the `RunStore` is still updated so legacy polling clients still work even if the run was started via streaming (optional, but good for hybrid use).",
        "testStrategy": "Regression testing: Verify legacy tools still return immediate `run_id` and polling works. Verify new tools stream data. Check server startup logs for successful registration.",
        "priority": "high",
        "dependencies": [
          6
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze and Refactor Existing Server Tool Registration",
            "description": "Examine `src/lms_llmsTxt_mcp/server.py` to understand the current `FastMCP` instantiation and tool registration pattern. Identify if refactoring is needed to support both synchronous and asynchronous/streaming tool definitions side-by-side.",
            "dependencies": [],
            "details": "Read `src/lms_llmsTxt_mcp/server.py`. Ensure the `mcp` server instance (likely `FastMCP`) is configured correctly to accept generator-based tools if specific configuration is required, or verify that the existing instance supports standard python `async def` and generators. Plan the import statements for the new tools.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 2,
            "title": "Import and Register Streaming Generation Tool",
            "description": "Import the `lmstxt_generate_llms_txt_stream` function from the streaming module and register it as an MCP tool.",
            "dependencies": [
              1
            ],
            "details": "In `src/lms_llmsTxt_mcp/server.py`, add the import `from .streaming_tool import lmstxt_generate_llms_txt_stream` (adjusting path based on Task 6 output). Use the `@mcp.tool()` decorator (or `mcp.add_tool()`) to expose it. Ensure the tool description accurately reflects that it returns a stream of events.",
            "status": "pending",
            "testStrategy": "Check `mcp list-tools` output to see the new tool."
          },
          {
            "id": 3,
            "title": "Implement Hybrid RunStore Updates",
            "description": "Ensure that the new streaming tool updates the shared `RunStore` so that legacy polling tools can still observe the run status.",
            "dependencies": [
              2
            ],
            "details": "Verify or modify the logic within the imported streaming handler or wrap it in `server.py` to ensure that when a streaming run is initialized, it is also added to the global `run_store` used by `lmstxt_list_runs`. This ensures hybrid compatibility where a user starts a stream but lists runs via the legacy tool.",
            "status": "pending",
            "testStrategy": "Start a streaming run, then immediately call `lmstxt_list_runs` to verify the run appears in the list."
          },
          {
            "id": 4,
            "title": "Register Cancellation Tool",
            "description": "Import and register the `lmstxt_cancel_run` tool to allow clients to interrupt streaming or long-running tasks.",
            "dependencies": [
              2
            ],
            "details": "In `src/lms_llmsTxt_mcp/server.py`, implement or import the cancellation logic. Register `lmstxt_cancel_run(run_id: str)` as a tool. This tool should look up the run in the `RunStore` or `EventBus` registry and trigger the cancellation flag.",
            "status": "pending",
            "testStrategy": "Verify the tool exists in the server capabilities."
          },
          {
            "id": 5,
            "title": "Verify Server Startup and Tool Registry",
            "description": "Perform a sanity check on the server entry point to ensure no import cycles or registration conflicts occur with the new additions.",
            "dependencies": [
              2,
              4
            ],
            "details": "Run the server locally (`mcp run src/lms_llmsTxt_mcp/server.py` or equivalent). Check logs for any FastMCP warnings regarding tool schema generation. Verify that both legacy tools (`lmstxt_generate_llms_txt`) and new tools (`lmstxt_generate_llms_txt_stream`, `lmstxt_cancel_run`) are listed.",
            "status": "pending",
            "testStrategy": "Run the server and query the list of tools via an MCP client inspector."
          }
        ]
      },
      {
        "id": 9,
        "title": "Harden Completion Determinism",
        "description": "Ensure robustness against worker crashes, network issues, or ambiguous LLM finish reasons.",
        "details": "Enhance `CompletionController` and `Runner`. Wrap the worker thread execution in a broad `try/finally` block. In `finally`, call `completion_controller.ensure_terminal_on_exit()` to guarantee a terminal event (failed or completed) is always emitted if one hasn't been sent yet. Add logic to ignore `finish_reason` if it conflicts with explicit stream markers.",
        "testStrategy": "Fault injection testing: Simulate an unhandled exception in the worker thread. Verify that a `run.failed` event is emitted to the stream.",
        "priority": "medium",
        "dependencies": [
          3,
          6
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Global Exception Handling in Worker Thread",
            "description": "Wrap the worker thread execution in a robust try/finally structure to guarantee terminal event emission.",
            "dependencies": [],
            "details": "In `src/lms_llmsTxt_mcp/runner.py`, encapsulate the payload execution within a top-level `try/except/finally` block. In the `finally` clause, invoke `completion_controller.ensure_terminal_on_exit()` to ensure that if the thread dies unexpectedly (e.g., OOM, unhandled exception), a terminal event is still sent to the client.",
            "status": "pending",
            "testStrategy": "Strict fault injection where a `RuntimeError` is raised explicitly within the worker logic; assert that the EventBus receives a `run.failed` message despite the crash."
          },
          {
            "id": 2,
            "title": "Refactor CompletionController for Idempotency and State Validation",
            "description": "Enhance the CompletionController to safely handle concurrent state transitions and ambiguous finish reasons.",
            "dependencies": [
              1
            ],
            "details": "Update `src/lms_llmsTxt_mcp/completion.py`. Add a `threading.Lock` around state transitions. Implement `ensure_terminal_on_exit` which checks if `is_terminal` is false before emitting a fallback failure. Add logic to ignore `finish_reason` provided by the LLM API if explicit stream markers indicate a different state (e.g., successful parsing despite 'length' stop reason).",
            "status": "pending",
            "testStrategy": "Parallel unit tests invoking `mark_failed` and `mark_completed` simultaneously on the same controller instance; verify exactly one terminal event is published."
          }
        ]
      },
      {
        "id": 10,
        "title": "Implement ODA Step Runner (Foundation)",
        "description": "Create the basic structure for the Observe-Decide-Act loop to demonstrate chaining.",
        "details": "Create `src/lms_llmsTxt_mcp/agent_loop.py`. Implement a simple `AgentLoop` class that takes a list of step definitions. It should subscribe to the `EventBus` of the current step, wait for a terminal event, and strictly based on that event (Completed vs Failed), decide to execute the next step or abort. This is the foundation for the Post-MVP feature.",
        "testStrategy": "Unit test with mock steps. Sequence: Step 1 succeeds -> Step 2 triggers. Step 1 fails -> Loop aborts. Verify no polling is used.",
        "priority": "low",
        "dependencies": [
          6
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Step Interface and Mock Implementation",
            "description": "Create abstract base classes or protocols for 'Step' definitions to be used by the AgentLoop.",
            "dependencies": [],
            "details": "Define a `Step` protocol/interface in `src/lms_llmsTxt_mcp/interfaces.py` (or within `agent_loop.py` if preferred) that requires an `execute(context)` method. Create a `MockStep` class for testing that simulates work and emits events via the EventBus. This ensures the loop has a consistent contract to interact with steps.",
            "status": "pending",
            "testStrategy": "Verify the interface enforces required methods. Test MockStep emits expected start/end events."
          },
          {
            "id": 2,
            "title": "Implement AgentLoop Class Structure",
            "description": "Scaffold the AgentLoop class including initialization with step definitions and EventBus connection.",
            "dependencies": [
              1
            ],
            "details": "Create `src/lms_llmsTxt_mcp/agent_loop.py`. Define `AgentLoop` class. Constructor should accept `steps: List[Step]`, `event_bus: EventBus`, and `run_id: str`. Implement internal state tracking for `current_step_index` and `status` (e.g., idle, running, completed, failed).",
            "status": "pending",
            "testStrategy": "Unit test initialization to ensure correct storage of steps and event bus reference."
          },
          {
            "id": 3,
            "title": "Implement Event Subscription and Monitoring Logic",
            "description": "Add logic to subscribe to the EventBus and listen for terminal events from the current step.",
            "dependencies": [
              2
            ],
            "details": "Implement a `_monitor_step(step_id)` method (or async equivalent) in `AgentLoop`. It should subscribe to the `EventBus` for the specific step execution. It needs to filter for `step.completed` or `step.failed` events (or the equivalent terminal events defined in the CompletionController) to trigger the decision logic.",
            "status": "pending",
            "testStrategy": "Mock the EventBus. Publish a completion event and verify the monitor method detects it."
          },
          {
            "id": 4,
            "title": "Implement ODA Decision and Execution Logic",
            "description": "Create the core loop mechanics to transition between steps based on previous step outcomes.",
            "dependencies": [
              3
            ],
            "details": "Implement the `run()` or `start()` method. Logic: 1. Execute current step. 2. Wait for terminal event (via monitor). 3. If Success -> increment index and repeat. 4. If Failure -> Abort loop and emit loop failure. 5. If No more steps -> Emit loop completion. Use a simple iterator or while loop structure.",
            "status": "pending",
            "testStrategy": "Test with a sequence of 2 mock steps. Verify Step 2 only starts after Step 1 succeeds. Test failure in Step 1 prevents Step 2."
          },
          {
            "id": 5,
            "title": "Integrate Cancellation Handling in Loop",
            "description": "Ensure the loop respects the run cancellation flag/event between steps.",
            "dependencies": [
              4
            ],
            "details": "Enhance the loop execution logic in `agent_loop.py` to check for a cancellation signal (either via a flag on the `EventBus` or a specific event) before starting the next step. If cancelled, terminate the loop immediately and clean up.",
            "status": "pending",
            "testStrategy": "Start a loop with 3 steps. Trigger cancellation during step 1. Verify step 2 is never executed."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2026-01-15T00:25:56.942Z",
      "updated": "2026-01-15T00:39:31.722Z",
      "description": "Tasks for chore-update context"
    }
  },
  "chore_update-v2": {
    "tasks": [
      {
        "id": 1,
        "title": "Establish Project Structure and Configuration Module",
        "description": "Initialize the Python project structure and implement the configuration loader to handle environment variables, schema versioning, and DSPy settings.",
        "details": "Create the directory structure as proposed (src/config, src/core_types, etc.). Implement `src/config/loader.py` using `pydantic-settings` to load configuration from environment variables and `.env` files. Define a `Config` class that includes settings for LLM providers (API keys, model names), caching strategies (path, type), and concurrency limits. Ensure strict validation of configuration values. Create `src/config/schema.py` to hold version constants.\n\nLibraries: `pydantic`, `pydantic-settings`, `python-dotenv`.\nStructure:\n- `src/config/__init__.py`\n- `src/config/loader.py`\n- `tests/unit/test_config.py`",
        "testStrategy": "Unit tests ensuring valid configs load correctly, missing required vars raise errors, and default values are applied.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Directory Structure",
            "description": "Create the root source directory and necessary package subdirectories to establish the project layout.",
            "dependencies": [],
            "details": "Create the following directories: `src/config`, `src/core_types`, `src/io`, `src/analysis`, `src/orchestration`, and `tests/unit`. Add empty `__init__.py` files in each `src` subdirectory to make them importable Python packages.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 2,
            "title": "Define Version Schema Constants",
            "description": "Create the schema module to hold versioning constants for the application configuration and artifacts.",
            "dependencies": [
              1
            ],
            "details": "Create `src/config/schema.py`. Define constants such as `CONFIG_VERSION` (e.g., '1.0.0') and potentially other static schema-related identifiers that the configuration loader or other modules might need to reference.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 3,
            "title": "Implement Base Configuration Model",
            "description": "Develop the Pydantic settings model to define the structure of the application configuration.",
            "dependencies": [
              2
            ],
            "details": "In `src/config/loader.py`, import `BaseSettings` from `pydantic-settings`. Define a `Config` class that includes fields for: LLM providers (api_key, model_name, base_url), caching (cache_dir, cache_type), and concurrency_limit (int). Ensure appropriate types and default values (e.g., concurrency_limit=5).",
            "status": "pending",
            "testStrategy": "Unit test to verify default values are assigned when no env vars are present."
          },
          {
            "id": 4,
            "title": "Implement Environment Loading Logic",
            "description": "Configure the settings model to load values from environment variables and .env files.",
            "dependencies": [
              3
            ],
            "details": "Update the `Config` class in `src/config/loader.py` to use `model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8')`. Ensure that environment variables (e.g., `LLM_API_KEY`) correctly override defaults. Add a utility function `get_settings()` to return a cached instance using `lru_cache`.",
            "status": "pending",
            "testStrategy": "Unit test using a temporary .env file to verify values are loaded correctly."
          },
          {
            "id": 5,
            "title": "Create Configuration Unit Tests",
            "description": "Write a comprehensive test suite to validate configuration loading, validation rules, and error handling.",
            "dependencies": [
              4
            ],
            "details": "Create `tests/unit/test_config.py`. Include tests for: 1. Successful loading of valid defaults. 2. Correct loading from environment variables (using `monkeypatch` or similar). 3. Validation errors when required fields (like API keys if no default) are missing. 4. Type validation failures (e.g., string for concurrency limit).",
            "status": "pending",
            "testStrategy": "Run pytest to ensure all configuration tests pass."
          }
        ]
      },
      {
        "id": 2,
        "title": "Define Core Type Definitions and Event Models",
        "description": "Create Pydantic models for repo analysis artifacts, streaming events, and completion signals to ensure type safety across the application.",
        "details": "Implement `src/core_types/analysis_artifacts.py` using Pydantic `BaseModel`. Define `RepoAnalysisArtifact` with fields for purpose, key concepts, constraints, and 'remember bullets'. Add a `version` field. Implement `src/core_types/events.py` to define `StreamEvent` (partial updates) and `CompletionEvent` (final signal with checksum). Ensure models support serialization to/from JSON.\n\nLibraries: `pydantic`.\nStructure:\n- `src/core_types/analysis_artifacts.py`\n- `src/core_types/events.py`",
        "testStrategy": "Unit tests verifying schema validation, JSON serialization/deserialization, and handling of invalid data types.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Core Types Package",
            "description": "Create the core_types package structure and add a common base model for consistent configuration.",
            "dependencies": [],
            "details": "Create directory `src/core_types` if it doesn't exist. Add `src/core_types/__init__.py`. Create `src/core_types/base.py` defining a `CoreModel` class inheriting from `pydantic.BaseModel` with `model_config` set to `{'populate_by_name': True, 'arbitrary_types_allowed': True}` to ensure consistent behavior across all derived models.",
            "status": "pending",
            "testStrategy": "Verify importability of the package and instantiation of the base model."
          },
          {
            "id": 2,
            "title": "Define Repository Analysis Artifact Models",
            "description": "Implement the Pydantic models representing the output of the repository analysis.",
            "dependencies": [
              1
            ],
            "details": "Create `src/core_types/analysis_artifacts.py`. Define `RepoAnalysisArtifact` inheriting from `CoreModel`. Include fields: `purpose` (str), `key_concepts` (List[str]), `constraints` (List[str]), `remember_bullets` (List[str]), and `version` (str, default='1.0.0'). Add docstrings and field descriptions for better schema generation.",
            "status": "pending",
            "testStrategy": "Unit test verifying `RepoAnalysisArtifact` validation logic and correct field types."
          },
          {
            "id": 3,
            "title": "Define Event and Signal Models",
            "description": "Implement the Pydantic models for streaming events and completion signals.",
            "dependencies": [
              1
            ],
            "details": "Create `src/core_types/events.py`. Define `StreamEvent` with fields like `type` (Literal['chunk']), `content` (str), and `timestamp` (float). Define `CompletionEvent` with fields `type` (Literal['completion']), `checksum` (str), and `artifact` (Optional[RepoAnalysisArtifact]). Ensure types are discriminated unions if needed for polymorphic handling.",
            "status": "pending",
            "testStrategy": "Unit test verifying serialization of events and ensuring required fields are present."
          },
          {
            "id": 4,
            "title": "Implement JSON Serialization Helpers",
            "description": "Ensure models have robust methods or config for JSON serialization/deserialization.",
            "dependencies": [
              2,
              3
            ],
            "details": "In `src/core_types/base.py` or specific model files, add helper methods `to_json()` and `from_json()` if Pydantic's default `model_dump_json` needs wrapper logic (e.g. for pretty printing or specific encoding). Ensure `CompletionEvent` can properly serialize the nested `RepoAnalysisArtifact`.",
            "status": "pending",
            "testStrategy": "Unit test round-tripping (object -> json -> object) for complex nested structures."
          },
          {
            "id": 5,
            "title": "Create Unit Tests for Core Types",
            "description": "Develop comprehensive unit tests to validate the integrity of the type definitions.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Create `tests/core_types/test_models.py`. Write tests to: 1) Validate valid data for `RepoAnalysisArtifact`, 2) Assert failure on invalid data types (e.g. list instead of str), 3) Verify default values (like version), 4) Test `StreamEvent` and `CompletionEvent` instantiation and JSON conversion.",
            "status": "pending",
            "testStrategy": "Run `pytest tests/core_types/test_models.py` and check for 100% pass rate."
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement DSPy Runtime Initialization and Adapter Setup",
        "description": "Configure the DSPy runtime with `dspy-ai` 3.x, setting up the language model and structured output adapters.",
        "details": "Create `src/dspy_runtime/runtime.py`. Implement a `configure_dspy(config)` function that initializes `dspy.settings.configure` with the LM defined in the config. Use `dspy-ai` (latest 3.x). Implement `src/dspy_runtime/adapters.py` to wrap DSPy's adapter functionality, ensuring JSON schema enforcement for structured outputs. This layer acts as the bridge between the raw LLM and the typed Pydantic models.\n\nLibraries: `dspy-ai`.\nRef: `dspy.configure`, `dspy.TypedPredictor` (or equivalent adapter pattern in v3).",
        "testStrategy": "Integration test: Initialize DSPy with a mock/test LM and verify it can process a basic request. Verify settings are applied globally or contextually as needed.",
        "priority": "high",
        "dependencies": [
          1,
          2
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure DSPy Runtime and LM Settings",
            "description": "Initialize the core DSPy runtime environment and configure the language model settings using the global configuration object.",
            "dependencies": [],
            "details": "Create `src/dspy_runtime/runtime.py`. Implement the `configure_dspy(config)` function. This function must inspect the provided configuration (model name, API keys, parameters) and call `dspy.configure` (or `dspy.settings.configure` in v3). Instantiate the appropriate Language Model client (e.g., `dspy.LM`) based on the config and set it as the default context for the application lifetime.",
            "status": "pending",
            "testStrategy": "Integration test: Call `configure_dspy` with a mock configuration and verify that `dspy.settings.lm` (or the v3 equivalent global state) reflects the initialized model instance correctly."
          },
          {
            "id": 2,
            "title": "Implement DSPy Structured Output Adapters",
            "description": "Develop the adapter layer that enforces Pydantic schemas on LLM outputs within the DSPy framework.",
            "dependencies": [
              1
            ],
            "details": "Create `src/dspy_runtime/adapters.py`. Implement logic to wrap DSPy's prediction mechanisms (specifically focusing on `TypedPredictor` or the functional signature pattern in v3) to enforce JSON schema compliance. This layer acts as the interface between the raw LM string output and the application's Pydantic models, handling the serialization of the schema into the prompt and the deserialization of the response.",
            "status": "pending",
            "testStrategy": "Unit test: Define a simple Pydantic model. Use the adapter to process a mocked raw JSON string from an LLM and verify it instantiates the correct Pydantic object with valid fields."
          }
        ]
      },
      {
        "id": 4,
        "title": "Develop IO Module for Repo Context Extraction",
        "description": "Implement functionality to read repository files, extract metadata, and prepare the context for the analysis program.",
        "details": "Create `src/io/repo_reader.py`. Implement functions to traverse a directory (respecting .gitignore if possible), read text files, and construct a `RepoContext` object. Implement `src/io/artifact_writer.py` to save `RepoAnalysisArtifact` to JSON files with deterministic ordering. Ensure file reading handles encoding errors gracefully.\n\nLibraries: `pathlib`, `pathspec` (for gitignore).\nStructure:\n- `src/io/repo_reader.py`\n- `src/io/artifact_writer.py`",
        "testStrategy": "Unit tests with temporary directory fixtures containing various file types. verify `RepoContext` structure and content.",
        "priority": "medium",
        "dependencies": [
          2
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Repository Traversal and Gitignore Logic",
            "description": "Develop the file system traversal mechanism that identifies valid source files while respecting .gitignore rules.",
            "dependencies": [],
            "details": "In `src/io/repo_reader.py`, implement a `traverse_repo(root_path: Path)` function. Use `pathspec` to parse `.gitignore` files. Use `pathlib` to walk the directory tree, filtering out ignored files, hidden directories (e.g., `.git`), and non-text extensions if defined. Return a list of valid file paths.",
            "status": "pending",
            "testStrategy": "Create a temporary directory structure with a `.gitignore` file and nested folders. Assert that the traversal function excludes ignored files and returns the correct set of paths."
          },
          {
            "id": 2,
            "title": "Implement Safe File Reading and Artifact Writer",
            "description": "Implement robust file reading with encoding fallback and the functionality to serialize analysis artifacts to JSON.",
            "dependencies": [
              1
            ],
            "details": "Add functionality to `src/io/repo_reader.py` to read file contents, trying `utf-8` then `latin-1`, and handling `UnicodeDecodeError`. Construct the `RepoContext`. Create `src/io/artifact_writer.py` to serialize `RepoAnalysisArtifact` objects to JSON with deterministic key sorting for consistency.",
            "status": "pending",
            "testStrategy": "Unit tests using files with various encodings (including invalid UTF-8) to verify graceful degradation. Verify that `artifact_writer` produces valid, deterministically ordered JSON output."
          }
        ]
      },
      {
        "id": 5,
        "title": "Create DSPy Analysis Program with Structured Outputs",
        "description": "Implement the core DSPy signatures and modules to generate the repository analysis using TypedPredictors/Adapters.",
        "details": "Create `src/analysis_program/signatures.py` defining `dspy.Signature` classes that map `RepoContext` input to `RepoAnalysisArtifact` output fields. Use type hints extensively. Create `src/analysis_program/program.py` containing a `dspy.Module` that utilizes `dspy.TypedPredictor` (or current v3 equivalent for structured output) to execute the signatures. Ensure the module returns a strictly validated Pydantic object.\n\nRef: DSPy TypedPredictor / Adapters.",
        "testStrategy": "Integration tests using a small fixture repo content. Mock the LLM response to return valid/invalid JSON and verify the program returns the correct Pydantic object or raises specific errors.",
        "priority": "high",
        "dependencies": [
          3,
          4
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define DSPy Signatures and Prompt Instructions",
            "description": "Implement the DSPy signature classes that map repository context to the required analysis artifacts, focusing on prompt engineering via docstrings.",
            "dependencies": [],
            "details": "Create `src/analysis_program/signatures.py`. Import `RepoAnalysisArtifact` from `src.core_types` and `RepoContext` from the relevant module. Define a class `GenerateRepoAnalysis(dspy.Signature)` with input fields for repository context (e.g., file tree, readme content) and an output field typed as `RepoAnalysisArtifact`. Write extensive docstrings for the signature and its fields to act as the system prompt and field-level instructions for the LLM.",
            "status": "pending",
            "testStrategy": "Unit tests verifying that `dspy` correctly inspects the signature fields and types, and that the Pydantic models are correctly referenced within the signature schema."
          },
          {
            "id": 2,
            "title": "Implement TypedPredictor Module Logic",
            "description": "Develop the executable DSPy module that integrates the defined signatures with a TypedPredictor to ensure structured Pydantic outputs.",
            "dependencies": [
              1
            ],
            "details": "Create `src/analysis_program/program.py`. Define a class `RepoAnalysisProgram(dspy.Module)`. inside `__init__`, instantiate a `dspy.TypedPredictor` (or equivalent v3 structured adapter) using the signature defined in the previous subtask. Implement the `forward` method to accept `RepoContext` data, pass it to the predictor, and return the validated `RepoAnalysisArtifact` object. Ensure error handling wraps the prediction call to catch schema validation failures.",
            "status": "pending",
            "testStrategy": "Integration tests using `dspy.DummyLM` or a mocked LLM response to verify that the module successfully parses JSON into the Pydantic object and handles malformed outputs by raising appropriate exceptions."
          }
        ]
      },
      {
        "id": 6,
        "title": "Implement Validation and Retry Logic",
        "description": "Wrap the analysis program with a validation loop that catches schema errors and retries generation.",
        "details": "Enhance `src/analysis_program/validator.py`. Implement a logic flow that inspects the raw output or caught Pydantic validation errors. If validation fails, retry the DSPy call (potentially with an updated prompt containing the error message) up to N times (defined in config). If all retries fail, return a structured error artifact.\n\nDetails: Use DSPy's built-in retry mechanisms or implement a custom loop around the `TypedPredictor`.",
        "testStrategy": "Unit tests with mocked failures (1st attempt fails, 2nd succeeds) to verify retry logic. Test exhaustion of retries leading to a graceful failure state.",
        "priority": "medium",
        "dependencies": [
          5
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Retry Configuration and Error Types",
            "description": "Add retry count configuration to the Config model and define a specific RetryError exception.",
            "dependencies": [],
            "details": "Update `src/config.py` (or equivalent) to include `max_retries` (default 3). Create `src/core_types/exceptions.py` to define `MaxRetriesExceededError` which will carry the last validation failure message. This ensures the retry logic is configurable and errors are typed.",
            "status": "pending",
            "testStrategy": "Unit test verifying config defaults and exception initialization."
          },
          {
            "id": 2,
            "title": "Create Validation Wrapper Class",
            "description": "Scaffold the Validator class in `src/analysis_program/validator.py`.",
            "dependencies": [
              1
            ],
            "details": "Create a class `Validator` that accepts a `dspy.Module` (the analysis program) and the config object. Implement the `__init__` method. Define the main method signature `validate_and_run(input_context: RepoContext) -> RepoAnalysisArtifact`.",
            "status": "pending",
            "testStrategy": "Unit test instantiation of the Validator class with a mock module."
          },
          {
            "id": 3,
            "title": "Implement Retry Loop Logic",
            "description": "Implement the core retry loop that invokes the DSPy program and catches validation errors.",
            "dependencies": [
              2
            ],
            "details": "Inside `validate_and_run`, create a loop running `max_retries` times. Call the DSPy module. Wrap the call in a try/except block specifically catching `dspy.dsp.modules.dsp_program.DSPyAssertionError` (or generic Pydantic `ValidationError` depending on how `TypedPredictor` fails). If successful, return the artifact.",
            "status": "pending",
            "testStrategy": "Unit test with a mock function that succeeds on the first try."
          },
          {
            "id": 4,
            "title": "Implement Error Feedback Construction",
            "description": "Construct an updated prompt or feedback mechanism when validation fails.",
            "dependencies": [
              3
            ],
            "details": "When a validation error is caught in the loop, extract the error message (e.g., 'missing field X'). If using `dspy.Assert` or `Suggest`, this might be handled internally, but for manual retries, log the error and potentially update the call signature if the DSPy signature allows for 'previous_errors'. For this task, ensure the error is logged and the retry counter increments.",
            "status": "pending",
            "testStrategy": "Unit test with a mock that fails once then succeeds, ensuring the loop continues."
          },
          {
            "id": 5,
            "title": "Implement Fallback and Final Error Handling",
            "description": "Handle the case where all retries are exhausted by returning a structured error or raising a specific exception.",
            "dependencies": [
              3,
              4
            ],
            "details": "If the loop finishes without success, catch the final exception. Construct a `RepoAnalysisArtifact` with a specific 'error' state or raise the `MaxRetriesExceededError` defined in subtask 1. Ensure the system doesn't crash silently. The return value must adhere to the expected return type or control flow.",
            "status": "pending",
            "testStrategy": "Unit test where the mock fails every time, verifying the final exception or error artifact is returned."
          }
        ]
      },
      {
        "id": 7,
        "title": "Integrate Deterministic Caching",
        "description": "Enable and configure DSPy's caching mechanism to improve performance and cost efficiency for repeated runs.",
        "details": "Modify `src/dspy_runtime/cache.py` (or configuration) to enable local caching. Ensure the cache key includes a hash of the input `RepoContext`, the configuration state, and the signature version. This ensures that changes to the code or prompt invalidate the cache. Provide options in `Config` to clear or disable the cache.\n\nRef: `dspy.settings.configure(cache=True)` or specific backend configuration.",
        "testStrategy": "Integration test: Run the analysis twice on the same input. Verify the second run is significantly faster/instant and does not trigger an actual LLM call (inspect logs/mocks).",
        "priority": "medium",
        "dependencies": [
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Caching Configuration Schema",
            "description": "Add caching-specific fields to the main configuration model to control cache behavior, location, and status.",
            "dependencies": [],
            "details": "Update `src/config/models.py` (or `loader.py`) to include a `CacheConfig` nested model. Fields should include `enabled` (bool), `cache_path` (str, default to `~/.cache/lms-llmsTxt`), and `in_memory` (bool). Ensure environment variables like `DSPY_CACHE_ENABLED` can override these settings.",
            "status": "pending",
            "testStrategy": "Unit test verifying that config loads correctly with default values and can be overridden by env vars."
          },
          {
            "id": 2,
            "title": "Implement Cache Initialization Logic",
            "description": "Create the logic to enable DSPy's native caching mechanism based on the loaded configuration.",
            "dependencies": [
              1
            ],
            "details": "Create `src/dspy_runtime/cache.py`. Implement a function `configure_cache(config: Config)` that calls `dspy.settings.configure` or specific backend cache settings if using `dspy-ai` v2.5+. If the config enables caching, ensure the file path is created. Add logic to clear the cache if a specific flag is passed.",
            "status": "pending",
            "testStrategy": "Integration test: Call `configure_cache`, then run a dummy DSPy predictor. Check if the cache file is created at the specified path."
          },
          {
            "id": 3,
            "title": "Implement RepoContext Hashing Utility",
            "description": "Create a deterministic hashing utility for the repository context to be used as part of the cache key.",
            "dependencies": [],
            "details": "In `src/utils/hashing.py` (create if needed), implement `compute_repo_hash(context: RepoContext) -> str`. This function should serialize the `RepoContext` (files, content, structure) in a deterministic order (e.g., sorting keys) and return a SHA-256 hash. This ensures that any change in the repository content invalidates the cache for downstream tasks.",
            "status": "pending",
            "testStrategy": "Unit test: Create two identical RepoContext objects and verify they produce the same hash. Modify one file in the context and verify the hash changes."
          },
          {
            "id": 4,
            "title": "Wrap Predictors with Context-Aware Caching",
            "description": "Ensure that the DSPy predictors or modules use the repository hash in their input signatures to force cache invalidation on code changes.",
            "dependencies": [
              3
            ],
            "details": "Modify the main analysis program (likely in `src/agent/analysis.py` or similar) or create a wrapper. When invoking the DSPy module, inject the `repo_hash` computed in step 3 as a field in the input signature (even if not used by the LLM prompt directly, or add it to the prompt metadata) so that DSPy's lookup key is unique per repository state. Alternatively, explore `dspy`'s manual cache key overrides if available.",
            "status": "pending",
            "testStrategy": "Integration test: Run the analysis with `cache=True`. Run again -> hit. Change repo content -> run -> miss (new computation)."
          },
          {
            "id": 5,
            "title": "Add Cache Management CLI Commands",
            "description": "Expose functionality to clear or inspect the cache via the application's interface.",
            "dependencies": [
              2
            ],
            "details": "In the main CLI entry point (e.g., `src/main.py`), add a `--clear-cache` flag or a subcommand. This should invoke a `clear_cache()` function from `src/dspy_runtime/cache.py` that removes the underlying SQLite or JSON cache files defined in the config.",
            "status": "pending",
            "testStrategy": "Manual/Scripted test: Populate cache, run CLI with --clear-cache, verify cache files are removed or size is reset."
          }
        ]
      },
      {
        "id": 8,
        "title": "Build Orchestration Layer with Streaming Support",
        "description": "Implement the pipeline orchestrator that manages the flow from input reading to analysis to output writing, supporting event streaming.",
        "details": "Create `src/orchestration/pipeline.py` and `src/orchestration/streaming.py`. Implement `run_pipeline` for batch mode and `stream_run_pipeline` for async/generator mode. The streaming function should yield `StreamEvent` objects (e.g., 'reading files', 'analyzing', 'validating') and end with a `CompletionEvent`. Ensure the completion event contains a checksum of the final artifact. Use python `asyncio` if concurrent file reading is implemented.\n\nStructure:\n- `src/orchestration/pipeline.py`",
        "testStrategy": "End-to-End test: Call `stream_run_pipeline` and collect all yielded events. Verify sequence order and presence of the final `CompletionEvent`.",
        "priority": "high",
        "dependencies": [
          6,
          7
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Pipeline Events and Async Skeleton",
            "description": "Establish the orchestration module structure and define specific event classes for pipeline stages.",
            "dependencies": [],
            "details": "Create `src/orchestration/streaming.py`. Import base event models from `src/core_types/events.py`. Define specific event subclasses (or factory methods) for distinct pipeline stages: 'ReadingFiles', 'Analyzing', 'Validating', and 'Writing'. Define the empty async generator signature for `stream_run_pipeline`.",
            "status": "pending",
            "testStrategy": "Verify that the module imports correctly and that event classes can be instantiated with valid payloads."
          },
          {
            "id": 2,
            "title": "Implement Async Stream Execution Logic",
            "description": "Develop the core asynchronous generator that orchestrates data flow between IO and Analysis modules.",
            "dependencies": [
              1
            ],
            "details": "In `src/orchestration/pipeline.py`, implement `stream_run_pipeline`. Use `asyncio` to await file reading from `src/io`. Yield status events. Pass context to the analysis program (Task 6). Yield validation events. Ensure exceptions are caught and yielded as error events within the stream.",
            "status": "pending",
            "testStrategy": "Unit test the generator with mocked IO and Analysis modules to ensure it yields the expected sequence of events."
          },
          {
            "id": 3,
            "title": "Implement Batch Mode and Final Completion Event",
            "description": "Create the synchronous wrapper for batch execution and implement final artifact checksumming.",
            "dependencies": [
              2
            ],
            "details": "Implement `run_pipeline` to consume `stream_run_pipeline` completely and return the final artifact. Add logic to the end of the stream to serialize the final `RepoAnalysisArtifact`, calculate a SHA-256 checksum, and yield a `CompletionEvent` containing this hash before closing the stream.",
            "status": "pending",
            "testStrategy": "End-to-End test: Run the full pipeline in batch mode, asserting the final object matches the checksum provided in the completion event."
          }
        ]
      },
      {
        "id": 9,
        "title": "Create Evaluation Harness for Quality Metrics",
        "description": "Develop a framework to evaluate the quality of the generated analysis against defined metrics.",
        "details": "Create `src/optimization/datasets.py` to load evaluation examples (pairs of repo inputs and 'gold standard' analysis). Create `src/optimization/metrics.py` to define scoring functions (e.g., semantic similarity of 'key concepts', JSON schema compliance score, rubric-based LLM grading). Implement `src/optimization/eval.py` to run the current program against the dataset and report scores.",
        "testStrategy": "Unit tests for metric functions. Integration test running the eval harness on a dummy dataset to ensure it generates a report.",
        "priority": "low",
        "dependencies": [
          8
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Dataset Management and Evaluation Runner",
            "description": "Develop the infrastructure to load evaluation datasets and the main execution loop for running the analysis program against these datasets.",
            "dependencies": [],
            "details": "Create `src/optimization/datasets.py` to define the `EvaluationExample` model and functions to load pairs of repository inputs and gold-standard analysis outputs (e.g., from JSONL files). Implement the skeleton of `src/optimization/eval.py` to iterate through loaded datasets, execute the analysis program (mocked or actual), and prepare the results for scoring.",
            "status": "pending",
            "testStrategy": "Unit tests for `datasets.py` to ensure correct parsing of data files. Integration test for `eval.py` to verify it can iterate through a dummy dataset without errors."
          },
          {
            "id": 2,
            "title": "Implement Quality Metrics and Scoring Functions",
            "description": "Define and implement the specific scoring logic to assess the quality of generated analysis against the gold standard examples.",
            "dependencies": [
              1
            ],
            "details": "Create `src/optimization/metrics.py`. Implement specific scoring functions such as `semantic_similarity` (comparing embedding distance of 'key concepts') and `json_compliance` (validating schema adherence). Integrate these metrics into the `src/optimization/eval.py` loop created in the previous task to generate a final report.",
            "status": "pending",
            "testStrategy": "Unit tests for individual metric functions in `metrics.py` using synthetic inputs with known high and low scores to verify calculation accuracy."
          }
        ]
      },
      {
        "id": 10,
        "title": "Implement MIPROv2 Optimizer Integration",
        "description": "Integrate DSPy's MIPROv2 optimizer to automatically improve prompts based on the evaluation metrics.",
        "details": "Create `src/optimization/compile.py`. Use `dspy.MIPROv2` (or current equivalent in `dspy-ai` 3.x) to compile the `analysis_program`. Setup the optimization loop that takes the training set and metric from Task 9, runs the optimizer, and saves the optimized program/instructions to a file. Add a flag in `Config` to load these optimized instructions in production.\n\nRef: `dspy.MIPROv2(metric=...)`, `teleprompter.compile(...)`.",
        "testStrategy": "Integration test: Run a short optimization loop (few iterations) on a small dataset to verify the code path works and artifacts are saved.",
        "priority": "low",
        "dependencies": [
          9
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure MIPROv2 Optimizer and Compilation Logic",
            "description": "Initialize the MIPROv2 optimizer in `src/optimization/compile.py` with appropriate prompt generation and teacher settings.",
            "dependencies": [],
            "details": "Create `src/optimization/compile.py`. Instantiate `dspy.MIPROv2` targeting the `analysis_program`. Configure hyperparameters such as `num_candidates`, `init_temperature`, and `prompt_model` (likely using a stronger model like GPT-4 for the optimization process). Define the `compile_program` function signature that accepts the module to be optimized.",
            "status": "pending",
            "testStrategy": "Unit test verifying that the `dspy.MIPROv2` class is instantiated with the correct parameters and does not raise configuration errors."
          },
          {
            "id": 2,
            "title": "Integrate Evaluation Metrics and Training Data",
            "description": "Connect the evaluation metric and training dataset to the MIPROv2 optimization loop to drive the prompt search.",
            "dependencies": [
              1
            ],
            "details": "Import the custom metric function and training data loader (from Task 9). Ensure the metric signature is compatible with DSPy's optimizer requirements. Pass the metric and training set into the `mipro_optimizer.compile(...)` call. Implement a basic execution block to run the optimization process.",
            "status": "pending",
            "testStrategy": "Integration test: Run the optimization loop with a very small subset of data (e.g., 2 examples) and 1 iteration to verify the metric is called and returns a score."
          },
          {
            "id": 3,
            "title": "Persist Optimized Instructions and Enable Loading",
            "description": "Implement logic to save the optimized DSPy program to a file and update the configuration to load it in production.",
            "dependencies": [
              2
            ],
            "details": "After optimization, call `compiled_program.save()` to write the optimized instructions to `data/optimized_prompts.json`. Modify `src/config.py` to add a `use_optimized_prompts` boolean flag. Update `src/dspy_runtime/runtime.py` to check this flag and, if true, load the saved state into the `analysis_program` instance during initialization.",
            "status": "pending",
            "testStrategy": "Test saving a dummy program to a file, then reloading it into a fresh instance and verifying that the prompt signatures/instructions match the saved state."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2026-01-15T01:12:59.399Z",
      "updated": "2026-01-15T01:17:50.556Z",
      "description": "Tasks for chore_update-v2 context"
    }
  }
}