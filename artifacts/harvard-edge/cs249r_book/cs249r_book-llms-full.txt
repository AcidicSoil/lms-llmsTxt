# llms-full (private-aware)
> Built from GitHub files and website pages. Large files may be truncated.

--- tinytorch/site/getting-started.md ---
# Getting Started with TinyTorch

```{warning} Early Explorer Territory

You're ahead of the curve. TinyTorch is functional but still being refined. Expect rough edges, incomplete documentation, and things that might change. If you proceed, you're helping us shape this by finding what works and what doesn't.

**Best approach right now:** Browse the code and concepts. For hands-on building, check back when we announce classroom readiness (Summer/Fall 2026).

Questions or feedback? [Join the discussion](https://github.com/harvard-edge/cs249r_book/discussions/1076)
```

```{note} Prerequisites Check
This guide requires **Python programming** (classes, functions, NumPy basics) and **basic linear algebra** (matrix multiplication).
```

## The Journey

TinyTorch follows a simple pattern: **build modules, unlock milestones, recreate ML history**.

```{mermaid}
:align: center
graph LR
    A[Install] --> B[Setup]
    B --> C[Start Module]
    C --> D[Complete Module]
    D --> E[Run Milestone]
    E --> C

    style A fill:#e3f2fd
    style B fill:#e3f2fd
    style C fill:#fff3e0
    style D fill:#f0fdf4
    style E fill:#fce4ec
```

As you complete modules, you unlock milestones that recreate landmark moments in ML history‚Äîusing YOUR code.

## Step 1: Install & Setup (2 Minutes)

`````{tab-set}
````{tab-item} macOS / Linux
```bash
# Install TinyTorch (run from a project folder like ~/projects)
curl -sSL mlsysbook.ai/tinytorch/install.sh | bash

# Activate and verify
cd tinytorch
source .venv/bin/activate
tito setup
```
````

````{tab-item} Windows
TinyTorch works on Windows using **Git Bash** (included with Git for Windows).

**Step 1: Install Git for Windows** (if you don't have it)
- Download from [git-scm.com/download/win](https://git-scm.com/download/win)
- Run the installer with default options

**Step 2: Open Git Bash**
- Search "Git Bash" in the Start menu and open it

**Step 3: Install TinyTorch**
```bash
# In Git Bash (run from a project folder like ~/projects)
curl -sSL mlsysbook.ai/tinytorch/install.sh | bash

# Activate and verify
cd tinytorch
source .venv/Scripts/activate
tito setup
```
````
`````

**What this does:**
- Checks your system (Python 3.8+, git)
- Downloads TinyTorch to a `tinytorch/` folder
- Creates an isolated virtual environment
- Installs all dependencies
- Verifies installation

**Check your version:**
```bash
tito --version
```

**Update TinyTorch:**
```bash
tito system update
```

## Step 2: Your First Module (15 Minutes)

Let's build Module 01 (Tensor)‚Äîthe foundation of all neural networks.

### Start the module

```bash
tito module start 01
```

This opens the module notebook and tracks your progress.

### Work in the notebook

Edit `modules/01_tensor/tensor.ipynb` in Jupyter:

```bash
jupyter lab modules/01_tensor/tensor.ipynb
```

You'll implement:
- N-dimensional array creation
- Mathematical operations (add, multiply, matmul)
- Shape manipulation (reshape, transpose)

### Complete the module

When your implementation is ready, export it to the TinyTorch package:

```bash
tito module complete 01
```

Your code is now importable:

```python
from tinytorch.core.tensor import Tensor  # YOUR implementation!
x = Tensor([1, 2, 3])
```

## Step 3: Your First Milestone

Now for the payoff! After completing the required modules (01-03), run a milestone:

```bash
tito milestone run perceptron
```

The milestone uses YOUR implementations to recreate Rosenblatt's 1958 Perceptron:

```text
Checking prerequisites for Milestone 01...
All required modules completed!

Testing YOUR implementations...
  * Tensor import successful
  * Activations import successful
  * Layers import successful
YOUR TinyTorch is ready!

+----------------------- Milestone 01 (1958) -----------------------+
|  Milestone 01: Perceptron (1958)                                  |
|  Frank Rosenblatt's First Neural Network                          |
|                                                                   |
|  Running: milestones/01_1958_perceptron/01_rosenblatt_forward.py  |
|  All code uses YOUR TinyTorch implementations!                    |
+-------------------------------------------------------------------+

Starting Milestone 01...

Assembling perceptron with YOUR TinyTorch modules...
   * Linear layer: 2 -> 1 (YOUR Module 03!)
   * Activation: Sigmoid (YOUR Module 02!)

+-------------------- Achievement Unlocked --------------------+
|  MILESTONE ACHIEVED!                                         |
|                                                              |
|  You completed Milestone 01: Perceptron (1958)               |
|  Frank Rosenblatt's First Neural Network                     |
|                                                              |
|  What makes this special:                                    |
|  - Every tensor operation: YOUR Tensor class                 |
|  - Every layer: YOUR Linear implementation                   |
|  - Every activation: YOUR Sigmoid function                   |
+--------------------------------------------------------------+
```

You're recreating ML history with your own code. *By Module 19, you'll benchmark against MLPerf‚Äîthe industry standard for ML performance.*

## The Pattern Continues

As you complete more modules, you unlock more milestones:

| Modules Completed | Milestone | What You Recreate |
|-------------------|-----------|-------------------|
| 01-03 | Perceptron (1958) | First neural network (forward pass) |
| 01-03 | XOR Crisis (1969) | The limitation that triggered AI Winter |
| 01-08 | MLP Revival (1986) | Backprop solves XOR + real digit recognition |
| 01-09 | CNN Revolution (1998) | Convolutions for spatial understanding |
| 01-08 + 11-13 | Transformers (2017) | Language generation with attention |
| 01-08 + 14-19 | MLPerf (2018) | Production optimization pipeline |

See all milestones and their requirements:

```bash
tito milestone list
```

## Quick Reference

Here are the commands you'll use throughout your journey:

```bash
# Module workflow
tito module start <N>       # Start working on module N
tito module complete <N>    # Export module to package
tito module status          # See your progress across all modules

# Milestones
tito milestone list         # See all milestones & requirements
tito milestone run <name>   # Run a milestone with your code

# Utilities
tito setup                  # First-time setup (safe to re-run)
tito system update                 # Update TinyTorch (your work is preserved)
tito --help                 # Full command reference
```

## Module Progression

TinyTorch has 20 modules organized in progressive tiers:

| Tier | Modules | Focus | Time Estimate |
|------|---------|-------|---------------|
| **Foundation** | 01-08 | Core ML infrastructure (tensors, dataloader, autograd, training) | ~18-24 hours |
| **Architecture** | 09-13 | Neural architectures (CNNs, transformers) | ~15-20 hours |
| **Optimization** | 14-19 | Production optimization (profiling, quantization) | ~18-24 hours |
| **Capstone** | 20 | Torch Olympics Competition | ~8-10 hours |

**Total: ~60-80 hours** over 14-18 weeks (4-6 hours/week pace).

See the module descriptions in this guide for detailed prerequisites and learning objectives.

## Join the Community (Optional)

After setup, join the global TinyTorch community:

```bash
tito community login        # Join the community
```

The community features include progress tracking and connecting with other builders.

## For Instructors & TAs

```{note}
Classroom support with NBGrader integration is coming (target: Summer/Fall 2026). TinyTorch works for self-paced learning today.
```

**What's Planned:**
- Automated assignment generation with solutions removed
- Auto-grading against test suites
- Progress tracking across all 20 modules
- Grade export to CSV for LMS integration

**Interested in early adoption?** [Join the discussion](https://github.com/harvard-edge/cs249r_book/discussions/1076) to share your use case.

**Ready to start?** Run `tito module start 01` and begin building!


## Links discovered
- [Join the discussion](https://github.com/harvard-edge/cs249r_book/discussions/1076)
- [git-scm.com/download/win](https://git-scm.com/download/win)

--- tinytorch/site/tito/overview.md ---
# TITO Command Reference

<div style="background: #f8f9fa; padding: 2rem; border-radius: 0.5rem; margin: 2rem 0; text-align: center;">
<h2 style="margin: 0 0 1rem 0; color: #495057;">Master the TinyTorch CLI</h2>
<p style="margin: 0; font-size: 1.1rem; color: #6c757d;">Complete command reference for building ML systems efficiently</p>
</div>

**Purpose**: Quick reference for all TITO commands. Find the right command for every task in your ML systems engineering journey.

## Quick Start: Three Commands You Need

<div style="display: grid; grid-template-columns: 1fr; gap: 1rem; margin: 2rem 0;">

<div style="background: #e3f2fd; padding: 1.5rem; border-radius: 0.5rem; border-left: 4px solid #2196f3;">
<h4 style="margin: 0 0 0.5rem 0; color: #1976d2;">1. Check Your Environment</h4>
<code style="background: #263238; color: #ffffff; padding: 0.5rem; border-radius: 0.25rem; display: block; margin: 0.5rem 0;">tito system health</code>
<p style="margin: 0.5rem 0 0 0; font-size: 0.9rem; color: #64748b;">Verify your setup is ready for development</p>
</div>

<div style="background: #fffbeb; padding: 1.5rem; border-radius: 0.5rem; border-left: 4px solid #f59e0b;">
<h4 style="margin: 0 0 0.5rem 0; color: #d97706;">2. Build & Export Modules</h4>
<code style="background: #263238; color: #ffffff; padding: 0.5rem; border-radius: 0.25rem; display: block; margin: 0.5rem 0;">tito module complete 01</code>
<p style="margin: 0.5rem 0 0 0; font-size: 0.9rem; color: #64748b;">Export your module to the TinyTorch package</p>
</div>

<div style="background: #f3e5f5; padding: 1.5rem; border-radius: 0.5rem; border-left: 4px solid #9c27b0;">
<h4 style="margin: 0 0 0.5rem 0; color: #7b1fa2;">3. Run Historical Milestones</h4>
<code style="background: #263238; color: #ffffff; padding: 0.5rem; border-radius: 0.25rem; display: block; margin: 0.5rem 0;">tito milestone run 03</code>
<p style="margin: 0.5rem 0 0 0; font-size: 0.9rem; color: #64748b;">Recreate ML history with YOUR code</p>
</div>

</div>


##  Commands by User Role

TinyTorch serves three types of users. Choose your path:

<div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 1.5rem; margin: 2rem 0;">

<div style="background: #e3f2fd; padding: 1.5rem; border-radius: 0.5rem; border-left: 4px solid #2196f3;">
<h3 style="margin: 0 0 1rem 0; color: #1976d2;"> Student / Learner</h3>
<p style="margin: 0 0 1rem 0; font-size: 0.9rem; color: #37474f;">You're learning ML systems by building from scratch</p>

**Your Workflow:**
```bash
# Start learning
tito module start 01

# Complete modules
tito module complete 01

# Validate with history
tito milestone run 03

# Track progress
tito module status
```

**Key Commands:**
- `tito module` - Build components
- `tito milestone` - Validate
- `tito module status` - Track progress

</div>

<div style="background: #fff3e0; padding: 1.5rem; border-radius: 0.5rem; border-left: 4px solid #f57c00;">
<h3 style="margin: 0 0 1rem 0; color: #e65100;"> Instructor</h3>
<p style="margin: 0 0 1rem 0; font-size: 0.9rem; color: #37474f;">You're teaching ML systems engineering</p>

**Your Workflow:**
```bash
# Initialize nbgrader environment
tito nbgrader init

# Generate assignments
tito nbgrader generate 01

# Distribute to students
tito nbgrader release 01

# Collect & grade
tito nbgrader collect 01
tito nbgrader autograde 01

# Provide feedback
tito nbgrader feedback 01

# View analytics & export grades
tito nbgrader status
tito nbgrader analytics 01
tito nbgrader report
```

**Key Commands:**
- `tito nbgrader` - Assignment management
- `tito module` - Test implementations
- `tito milestone` - Validate setups

</div>

<div style="background: #f3e5f5; padding: 1.5rem; border-radius: 0.5rem; border-left: 4px solid #9c27b0;">
<h3 style="margin: 0 0 1rem 0; color: #7b1fa2;">üë©üíª Developer / Contributor</h3>
<p style="margin: 0 0 1rem 0; font-size: 0.9rem; color: #37474f;">You're contributing to TinyTorch modules</p>

**Your Workflow:**
```bash
# Edit source code
# src/01_tensor/01_tensor.py

# Export to notebooks & package
tito dev export 01
tito dev export --all

# Test implementations
tito dev test --unit
tito dev test --inline --module 01

# Before merging
tito dev test --all
```

**Key Commands:**
- `tito dev test` - Run tests
- `tito dev export` - Rebuild curriculum
- `tito milestone` - Validate with history

</div>

</div>


## Complete Command Reference

### Setup Command

**Purpose**: First-time environment and profile setup

| Command | Description | Guide |
|---------|-------------|-------|
| `tito setup` | Set up development environment (venv, packages, profile) | [Module Workflow](modules.md) |

### System Commands

**Purpose**: Environment health, validation, and configuration

| Command | Description | Guide |
|---------|-------------|-------|
| `tito system health` | Environment health check and validation | [Module Workflow](modules.md) |
| `tito system info` | System resources (paths, disk, memory) | [Module Workflow](modules.md) |
| `tito system jupyter` | Start Jupyter Lab server | [Module Workflow](modules.md) |
| `tito system update` | Check for and install updates | [Module Workflow](modules.md) |
| `tito system reset` | Reset TinyTorch to pristine state | [Troubleshooting](troubleshooting.md) |

### Module Commands

**Purpose**: Build-from-scratch workflow (your main development cycle)

| Command | Description | Guide |
|---------|-------------|-------|
| `tito module list` | List all available modules | [Module Workflow](modules.md) |
| `tito module start XX` | Begin working on a module (first time) | [Module Workflow](modules.md) |
| `tito module view XX` | Open module notebook (no status update) | [Module Workflow](modules.md) |
| `tito module resume XX` | Continue working on a module | [Module Workflow](modules.md) |
| `tito module complete XX` | Test, export, and track module completion | [Module Workflow](modules.md) |
| `tito module test XX` | Run module tests independently | [Module Workflow](modules.md) |
| `tito module status` | View module completion progress | [Module Workflow](modules.md) |
| `tito module reset XX` | Reset module to clean state | [Module Workflow](modules.md) |

**See**: [Module Workflow Guide](modules.md) for complete details

### Milestone Commands

**Purpose**: Run historical ML recreations with YOUR implementations

| Command | Description | Guide |
|---------|-------------|-------|
| `tito milestone list` | Show all 6 historical milestones (1958-2018) | [Milestone System](milestones.md) |
| `tito milestone run XX` | Run milestone with prerequisite checking | [Milestone System](milestones.md) |
| `tito milestone info XX` | Get detailed milestone information | [Milestone System](milestones.md) |
| `tito milestone status` | View milestone progress and achievements | [Milestone System](milestones.md) |
| `tito milestone timeline` | Visual timeline of your journey | [Milestone System](milestones.md) |
| `tito milestone test XX` | Test milestone achievement requirements | [Milestone System](milestones.md) |
| `tito milestone demo XX` | Run milestone capability demonstration | [Milestone System](milestones.md) |

**See**: [Milestone System Guide](milestones.md) for complete details

### Progress & Data Commands

**Purpose**: Track progress and manage user data

| Command | Description | Guide |
|---------|-------------|-------|
| `tito module status` | View module completion progress | [Progress & Data](data.md) |
| `tito milestone status` | View milestone achievements | [Progress & Data](data.md) |
| `tito module reset XX` | Reset a specific module | [Progress & Data](data.md) |

**See**: [Progress & Data Management](data.md) for complete details

### Community Commands

**Purpose**: Join the global TinyTorch community and track your progress

| Command | Description | Guide |
|---------|-------------|-------|
| `tito community login` | Log in to the community via web browser | [Community Guide](../community.md) |
| `tito community logout` | Log out of the community | [Community Guide](../community.md) |
| `tito community profile` | View/edit your community profile | [Community Guide](../community.md) |
| `tito community status` | Show login status and user info | [Community Guide](../community.md) |
| `tito community map` | Open global community map | [Community Guide](../community.md) |

**See**: [Community Guide](../community.md) for complete details

### Benchmark Commands

**Purpose**: Validate setup and measure performance

| Command | Description | Guide |
|---------|-------------|-------|
| `tito benchmark baseline` | Quick setup validation ("Hello World") | [Troubleshooting](troubleshooting.md) |
| `tito benchmark capstone` | Full Module 20 performance evaluation | [Troubleshooting](troubleshooting.md) |

**See**: [Troubleshooting](troubleshooting.md) for validation and benchmark help

### Developer Commands

**Purpose**: Source code development, testing, and contribution (for developers only)

| Command | Description | Use Case |
|---------|-------------|----------|
| `tito dev export <module>` | Export src/ ‚Üí modules/ ‚Üí tinytorch/ | After editing source files |
| `tito dev export --all` | Export all modules | After major refactoring |
| `tito dev test` | Run unit tests (default) | Quick validation |
| `tito dev test --inline` | Run inline tests from src/ | After editing modules |
| `tito dev test --unit` | Run pytest unit tests | Component validation |
| `tito dev test --cli` | Run CLI tests | After CLI changes |
| `tito dev test --integration` | Run integration tests | Cross-module validation |
| `tito dev test --e2e` | Run end-to-end tests | User journey validation |
| `tito dev test --milestone` | Run milestone tests | Full package validation |
| `tito dev test --all` | Run all test types | Before PRs |
| `tito dev test --release` | Full release validation (destructive) | Before releases only |

**Note**: These commands work with `src/XX_name/XX_name.py` files and are for TinyTorch contributors/developers.
**Students** use `tito module` commands to work with generated notebooks.

**See**: [Developer Testing Guide](testing.md) for complete testing documentation

**Directory Structure:**
```
src/              ‚Üê Developers edit here (Python source)
modules/          ‚Üê Students use these (generated notebooks)
tinytorch/        ‚Üê Package code (auto-generated)
```


## Command Groups by Task

### First-Time Setup

```bash
# Install TinyTorch (downloads and sets up everything)
curl -sSL mlsysbook.ai/tinytorch/install.sh | bash
cd tinytorch
source .venv/bin/activate

# First-time profile setup
tito setup

# Verify environment
tito system health
```

### Student Workflow (Learning)

```bash
# Start or continue a module
tito module start 01      # First time
tito module resume 01     # Continue later

# Export when complete
tito module complete 01

# Check progress
tito module status
```

### Developer Workflow (Contributing)

```bash
# Edit source files in src/
vim src/01_tensor/01_tensor.py

# Export to notebooks + package
tito dev export 01

# Run tests
tito dev test --unit           # Quick validation
tito dev test --inline --module 01   # Test specific module

# Test implementation
python -c "from tinytorch import Tensor; print(Tensor([1,2,3]))"

# Before PR: run all tests
tito dev test --all
```

### Achievement & Validation

```bash
# See available milestones
tito milestone list

# Get details
tito milestone info 03

# Run milestone
tito milestone run 03

# View achievements
tito milestone status
```

### Progress Management

```bash
# View all progress
tito module status
tito milestone status
```


## Typical Session Flow

Here's what a typical TinyTorch session looks like:

<div style="background: #f8f9fa; padding: 1.5rem; border: 1px solid #dee2e6; border-radius: 0.5rem; margin: 1.5rem 0;">

**1. Start Session**
```bash
cd tinytorch
source .venv/bin/activate
tito system health         # Verify environment
```

**2. Work on Module**
```bash
tito module start 03       # Or: tito module resume 03
# Edit in Jupyter Lab...
```

**3. Export & Test**
```bash
tito module complete 03
```

**4. Run Milestone (when prerequisites met)**
```bash
tito milestone list        # Check if ready
tito milestone run 03      # Run with YOUR code
```

**5. Track Progress**
```bash
tito module status         # See module progress
tito milestone status      # See milestone achievements
```

</div>


## Command Help

Every command has detailed help text:

```bash
# Top-level help
tito --help

# Command group help
tito module --help
tito milestone --help

# Specific command help
tito module complete --help
tito milestone run --help
```


## Detailed Guides

- **[Module Workflow](modules.md)** - Complete guide to building and exporting modules
- **[Milestone System](milestones.md)** - Running historical ML recreations
- **[Progress & Data](data.md)** - Managing your learning journey
- **[Developer Testing](testing.md)** - Complete testing infrastructure guide
- **[Troubleshooting](troubleshooting.md)** - Common issues and solutions


## Related Resources

- **[Getting Started Guide](../getting-started.md)** - Complete setup and first steps
- **[Module Workflow](modules.md)** - Day-to-day development cycle
- **[Datasets Guide](../datasets.md)** - Understanding TinyTorch datasets


*Master these commands and you'll build ML systems with confidence. Every command is designed to accelerate your learning and keep you focused on what matters: building production-quality ML frameworks from scratch.*


## Links discovered
- [Module Workflow](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/site/tito/modules.md)
- [Troubleshooting](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/site/tito/troubleshooting.md)
- [Module Workflow Guide](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/site/tito/modules.md)
- [Milestone System](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/site/tito/milestones.md)
- [Milestone System Guide](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/site/tito/milestones.md)
- [Progress & Data](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/site/tito/data.md)
- [Progress & Data Management](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/site/tito/data.md)
- [Community Guide](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/site/community.md)
- [Developer Testing Guide](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/site/tito/testing.md)
- [Developer Testing](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/site/tito/testing.md)
- [Getting Started Guide](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/site/getting-started.md)
- [Datasets Guide](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/site/datasets.md)

--- tinytorch/tests/milestones/QUICKSTART.md ---
# üöÄ Milestones Quick Start

## Run All Tests

```bash
pytest tests/milestones/test_learning_verification.py -v
```

**Expected**: ‚úÖ 5 passed in ~90 seconds

---

## Run Individual Milestones

### 1Ô∏è‚É£ Perceptron (1958)
```bash
pytest tests/milestones/test_learning_verification.py::test_perceptron_learning -v
```
**Tests**: Linear classification, gradient descent basics

### 2Ô∏è‚É£ XOR (1986)
```bash
pytest tests/milestones/test_learning_verification.py::test_xor_learning -v
```
**Tests**: Backpropagation, hidden layers, non-linearity

### 3Ô∏è‚É£ MLP Digits (1989)
```bash
pytest tests/milestones/test_learning_verification.py::test_mlp_digits_learning -v
```
**Tests**: Multi-class classification, real data, generalization

### 4Ô∏è‚É£ CNN (1998)
```bash
pytest tests/milestones/test_learning_verification.py::test_cnn_learning -v
```
**Tests**: Convolution, spatial structure, parameter efficiency

### 5Ô∏è‚É£ Transformer (2017)
```bash
pytest tests/milestones/test_learning_verification.py::test_transformer_learning -v
```
**Tests**: Attention, embeddings, positional encoding, sequence processing

---

## What Each Test Verifies

| Test | Loss ‚Üì | Accuracy | Gradients | Key Innovation |
|------|--------|----------|-----------|----------------|
| Perceptron | >50% | >90% | 2/2 | Automatic learning |
| XOR | >50% | >90% | 8/8 | Non-linearity |
| MLP | >50% | >80% | 6/6 | Real data scaling |
| CNN | >50% | >80% | 6/6 | Spatial structure |
| Transformer | >50% | 100% | 19/19 | Attention mechanism |

---

## Understanding the Output

### ‚úÖ Success Example
```
üìä Learning Verification Results:
‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
‚îÇ Metric              ‚îÇ Value    ‚îÇ Status  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Final Test Accuracy ‚îÇ 82.0%    ‚îÇ ‚úÖ PASS ‚îÇ
‚îÇ Loss Decrease       ‚îÇ 68.1%    ‚îÇ ‚úÖ PASS ‚îÇ
‚îÇ Gradients Flowing   ‚îÇ 6/6      ‚îÇ ‚úÖ PASS ‚îÇ
‚îÇ Weights Updated     ‚îÇ 6/6      ‚îÇ ‚úÖ PASS ‚îÇ
‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ

‚úÖ CNN LEARNING VERIFIED
```

### ‚ùå Failure Example
```
‚ùå LEARNING VERIFICATION FAILED
   ‚Ä¢ Test accuracy too low: 45.0% < 80.0%
   ‚Ä¢ Loss didn't decrease enough: 30% < 50%
```

---

## Debugging Failed Tests

### No Gradients
```python
# Check if gradients exist
for name, param in model.named_parameters():
    if param.grad is None:
        print(f"‚ùå {name} has no gradient!")
    else:
        print(f"‚úÖ {name}: grad mean = {param.grad.data.abs().mean():.6f}")
```

### Loss Not Decreasing
```python
# Check learning rate
optimizer = SGD(model.parameters(), lr=0.01)  # Try different values

# Check if optimizer is stepping
optimizer.zero_grad()
loss.backward()
optimizer.step()  # Don't forget this!
```

### Low Accuracy
```python
# Check model output
predictions = model(X_test)
print(f"Predictions shape: {predictions.shape}")
print(f"Predictions range: [{predictions.min():.3f}, {predictions.max():.3f}]")

# Check labels
print(f"Labels shape: {y_test.shape}")
print(f"Unique labels: {set(y_test.data.flatten())}")
```

---

## Fair Comparisons

### MLP vs CNN (both on TinyDigits)

**Matched training budget**:
```python
batch_size = 32  # Same
epochs = 25      # Same
samples = 1000   # Same dataset
updates = 775    # Same number of gradient steps
```

**Results**:
- MLP: 82.0% accuracy, 52.3% loss decrease
- CNN: 82.0% accuracy, 68.1% loss decrease

**Conclusion**: CNN learns more efficiently (better loss reduction) with 10√ó fewer parameters

---

## Common Issues

### Issue: "Test functions should return None"
**Status**: ‚ö†Ô∏è Warning (not an error)
**Fix**: Not needed - tests still pass
**Explanation**: Tests return bool for programmatic use

### Issue: Tests are slow
**Expected**: ~90 seconds for all 5 tests
**Reason**: Actually training models (not mocked)
**Benefit**: Real verification that learning works

### Issue: Accuracy varies slightly
**Expected**: ¬±2% variation across runs
**Reason**: Random initialization, data shuffling
**Fix**: Tests use thresholds (e.g., >80% not ==82%)

---

## File Structure

```
tests/milestones/
‚îú‚îÄ‚îÄ test_learning_verification.py  # Main test file
‚îú‚îÄ‚îÄ README.md                       # Full documentation
‚îú‚îÄ‚îÄ PROGRESSION.md                  # How milestones connect
‚îú‚îÄ‚îÄ QUICKSTART.md                   # This file
‚îú‚îÄ‚îÄ CURRENT_STATUS.md              # Implementation notes
‚îî‚îÄ‚îÄ WHY_SEQUENCE_REVERSAL.md       # Transformer debugging notes
```

---

## Next Steps

1. **Run the tests**: `pytest tests/milestones/ -v`
2. **Read the progression**: See `PROGRESSION.md` for how they connect
3. **Understand the code**: Each test is ~100 lines, well-commented
4. **Try modifications**: Change architectures, hyperparameters
5. **Build your own**: Use these as templates for new milestones

---

## Quick Reference

**All tests pass?** ‚úÖ TinyTorch implements 60+ years of neural network history correctly!

**Some tests fail?** See debugging section above or check `CURRENT_STATUS.md`

**Want to understand connections?** Read `PROGRESSION.md`

**Want full details?** Read `README.md`

---

## One-Liner Summary

```bash
# Verify TinyTorch implements neural network history correctly
pytest tests/milestones/ -v && echo "‚úÖ 60+ years of ML history verified!"
```


--- book/tools/scripts/docs/README.md ---
# Scripts Directory

This directory contains various Python scripts used for book maintenance and processing.

## Available Scripts

### Figure Caption Improvement
The `improve_figure_captions.py` script provides automated caption enhancement using local Ollama LLM models:

```bash
# Improve all captions (recommended)
python3 scripts/improve_figure_captions.py -d contents/core/

# Analysis and utilities
python3 scripts/improve_figure_captions.py --analyze -d contents/core/
python3 scripts/improve_figure_captions.py --build-map -d contents/core/
```

üìñ **Full documentation**: See [`FIGURE_CAPTIONS.md`](FIGURE_CAPTIONS.md) for complete usage guide, model selection, and troubleshooting.

### Cross-Reference Generation
The `cross_refs/` directory contains scripts for generating AI-powered cross-references with explanations.

üìñ **Full documentation**: See [`cross_refs/RECIPE.md`](/tools/scripts/cross_refs/RECIPE.md) for complete workflow.
## Python Dependencies

All Python dependencies are managed through the root-level `requirements.txt` file. This ensures consistent package versions across all scripts and the GitHub Actions workflow.

### Adding New Dependencies

When adding new Python scripts that require external packages:

1. Add the required packages to `requirements.txt` at the project root
2. Include version constraints where appropriate (e.g., `>=1.0.0`)
3. Add comments to group related packages
4. Test locally with: `pip install -r requirements.txt`

### Current Dependencies

The current dependencies include:

- **Quarto/Jupyter**: `jupyterlab-quarto`, `jupyter`
- **NLP**: `nltk` (with stopwords and punkt data)
- **AI Integration**: `openai`, `gradio`
- **Document Processing**: `pybtex`, `pypandoc`, `pyyaml`
- **Image Processing**: `Pillow`
- **Validation**: `jsonschema`
- **Utilities**: `absl-py`

### Subdirectory Requirements Files

Some subdirectories have their own `requirements.txt` files for specific workflows:

- `scripts/genai/requirements.txt` - AI-specific dependencies
- `scripts/publish/requirements.txt` - Publishing dependencies

These are kept for reference but the main workflow uses the root `requirements.txt`.

### GitHub Actions Integration

The GitHub Actions workflow automatically:

1. Caches Python packages for faster builds
2. Installs all dependencies from `requirements.txt`
3. Downloads required NLTK data
4. Reports cache status in build summaries

Cache is invalidated when `requirements.txt` changes, ensuring dependencies stay up-to-date.

## Pre-commit Setup

The project uses pre-commit hooks for code quality checks. The hooks run automatically on commit and include:

- **Spell checking** with codespell
- **YAML validation** for `_quarto-html.yml` and `_quarto-pdf.yml`
- **Markdown formatting** and linting
- **Bibliography formatting** with bibtex-tidy
- **Custom Python scripts** for section ID management and unreferenced label detection

### Setup Instructions

1. **Install pre-commit** (included in requirements.txt):
   ```bash
   pip install -r requirements.txt
   ```

2. **Install the git hooks**:
   ```bash
   pre-commit install
   ```

3. **Run manually** (optional):
   ```bash
   # Run on all files
   pre-commit run --all-files

   # Run on specific files
   pre-commit run --files path/to/file.qmd
   ```

### Troubleshooting

- **NLTK data issues**: The hooks automatically download required NLTK data, but if you encounter issues, you can manually run:
  ```python
  import nltk
  nltk.download('stopwords')
  nltk.download('punkt')
  ```

- **Python environment**: The hooks use isolated Python environments with the specified dependencies, so they should work regardless of your local Python setup.


## Links discovered
- [`FIGURE_CAPTIONS.md`](https://github.com/harvard-edge/cs249r_book/blob/dev/book/tools/scripts/docs/FIGURE_CAPTIONS.md)
- [`cross_refs/RECIPE.md`](https://github.com/harvard-edge/cs249r_book/blob/dev/tools/scripts/cross_refs/RECIPE.md)

--- book/docs/BINDER.md ---
# Book Binder CLI

The **Book Binder** is a self-contained, lightning-fast development CLI for the MLSysBook project. It provides streamlined commands for building, previewing, and managing the book in both HTML and PDF formats.

## Quick Start

```bash
# First time setup
./binder setup

# Welcome and overview
./binder hello

# Build a single chapter (HTML)
./binder build intro

# Build multiple chapters together (HTML)
./binder build intro,ml_systems

# Preview a chapter (builds and opens in browser)
./binder preview intro

# Build the complete book (HTML)
./binder build

# Build the complete book (PDF)
./binder pdf

# Build a single chapter (PDF) - SELECTIVE BUILD
./binder pdf intro
# ‚Ü≥ Automatically comments out all chapters except index.qmd and introduction.qmd

# Publish the book
./binder publish

# Get help
./binder help
```

## Installation

The binder is a Python script located in the project root. Make sure it's executable:

```bash
chmod +x binder
```

**Dependencies**: Python 3.6+ (uses only standard library modules)

## Command Reference

### ‚ö° Core Commands

Intuitive commands that work on both individual chapters and the entire book.

| Command | Description | Example |
|---------|-------------|---------|
| `build [chapter[,ch2,...]]` | Build book or chapter(s) in HTML | `./binder build intro,ml_systems` |
| `preview [chapter[,ch2,...]]` | Preview book or chapter(s) | `./binder preview ops` |
| `pdf [chapter[,ch2,...]]` | Build book or chapter(s) in PDF | `./binder pdf intro` |

**Smart defaults**: No target = entire book, with target = specific chapter(s)

### üìö Full Book Examples

| Command | Description | Example |
|---------|-------------|---------|
| `build` | Build complete book (HTML) | `./binder build` |
| `preview` | Preview complete book | `./binder preview` |
| `pdf` | Build complete book (PDF) | `./binder pdf` |
| `publish` | Build and publish book | `./binder publish` |

### üîß Management Commands

| Command | Description | Example |
|---------|-------------|---------|
| `setup` | Configure environment | `./binder setup` |
| `clean` | Clean configs & artifacts | `./binder clean` |
| `switch <format>` | Switch active config | `./binder switch pdf` |
| `status` | Show current status | `./binder status` |
| `list` | List available chapters | `./binder list` |
| `doctor` | Run comprehensive health check | `./binder doctor` |
| `about` | Show project information | `./binder about` |
| `help` | Show help information | `./binder help` |

### üöÄ Shortcuts

All commands have convenient shortcuts:

| Shortcut | Command |
|----------|---------|
| `b` | `build` |
| `p` | `preview` |
| `pdf` | `pdf` |
| `epub` | `epub` |
| `l` | `list` |
| `s` | `status` |
| `d` | `doctor` |
| `h` | `help` |

## Chapter Names

Chapters can be referenced by their short names. Common examples:

- `intro` ‚Üí Introduction chapter
- `ml_systems` ‚Üí Machine Learning Systems chapter
- `dl_primer` ‚Üí Deep Learning Primer chapter
- `training` ‚Üí Training chapter
- `ops` ‚Üí MLOps chapter

Use `./binder list` to see all available chapters.

## Build Outputs

| Format | Output Location | Description |
|--------|-----------------|-------------|
| HTML | `build/html/` | Website format with navigation |
| PDF | `build/pdf/` | Academic book format |

## üöÄ Publishing

The `publish` command provides two modes based on how you call it:

### 1. Interactive Mode (Default)

When called without arguments, `publish` runs the interactive wizard:

```bash
# Interactive publishing wizard
./binder publish
```

**What interactive mode does:**

1. **üîç Pre-flight checks** - Verifies git status and branch
2. **üßπ Cleans** - Removes previous builds
3. **üìö Builds HTML** - Creates web version
4. **üìÑ Builds PDF** - Creates downloadable version
5. **üì¶ Copies PDF** - Moves PDF to assets directory
6. **üíæ Commits** - Adds PDF to git
7. **üöÄ Pushes** - Triggers GitHub Actions deployment

### 2. Command-Line Trigger Mode

When called with arguments, `publish` triggers the GitHub Actions workflow directly:

```bash
# Trigger GitHub Actions workflow
./binder publish "Description" [COMMIT_HASH]

# With options
./binder publish "Add new chapter" abc123def --type patch --no-ai
```

**What command-line mode does:**

1. **üîç Validates environment** - Checks GitHub CLI, authentication, branch
2. **‚úÖ Validates commit** - Ensures the dev commit exists (if provided)
3. **üöÄ Triggers workflow** - Uses GitHub CLI to trigger the publish-live workflow
4. **üìä Provides feedback** - Shows monitoring links and next steps

**Options:**
- `--type patch|minor|major` - Release type (default: minor)
- `--no-ai` - Disable AI release notes
- `--yes` - Skip confirmation prompts

**Requirements:**
- GitHub CLI installed and authenticated (`gh auth login`)
- Must be on main or dev branch
- Dev commit must exist (if provided)

### Publishing Workflow:

```bash
# Development workflow
./binder preview intro          # Preview a chapter
./binder build                  # Build complete HTML
./binder pdf                    # Build complete PDF
./binder publish                # Publish to the world
```

### After Publishing:

- **üåê Web version**: Available at https://harvard-edge.github.io/cs249r_book
- **üìÑ PDF download**: Available at https://harvard-edge.github.io/cs249r_book/assets/downloads/Machine-Learning-Systems.pdf
- **üìà GitHub Actions**: Monitors build progress at https://github.com/harvard-edge/cs249r_book/actions

### Requirements:

- Must be on `main` branch
- No uncommitted changes
- Git repository properly configured

## Advanced Features

### Unified Multi-Chapter Builds

The binder supports building multiple chapters together in a single Quarto render command:

```bash
# Build multiple chapters together (HTML)
./binder build intro,ml_systems

# Build multiple chapters together (PDF)
./binder pdf intro,ml_systems

# Preview multiple chapters together
./binder preview intro,ml_systems
```

**Benefits:**
- ‚úÖ **Faster builds**: Single Quarto process instead of multiple
- ‚úÖ **Shared context**: Dependencies loaded once
- ‚úÖ **Unified processing**: Cross-references and quizzes processed together
- ‚úÖ **Better UX**: Single browser window opens with complete site

### Fast Build Mode

Fast builds use selective rendering to only build essential files plus target chapters:

**HTML Fast Build** (project.render):
```yaml
render:
  - index.qmd
  - 404.qmd
  - contents/frontmatter/
  - contents/core/target-chapter.qmd
```

**PDF Fast Build** (comments out unused chapters):
```yaml
chapters:
  - index.qmd
  - contents/frontmatter/foreword.qmd
  - contents/core/target-chapter.qmd
  # - contents/core/other-chapter.qmd  # Commented for fast build
```

#### Selective PDF Chapter Building

When you run `./binder pdf intro`, the system automatically:

1. **Creates a backup** of the original PDF configuration
2. **Comments out all chapters** except the target chapter and essential files
3. **Builds only the selected content**:
   - ‚úÖ `index.qmd` (always included)
   - ‚úÖ `contents/core/introduction/introduction.qmd` (target chapter)
   - ‚ùå `contents/backmatter/glossary/glossary.qmd` (commented out)
   - ‚ùå `contents/backmatter/references.qmd` (commented out)
4. **Restores the original configuration** after build completion

**Example output:**
```bash
./binder pdf intro

üìÑ Building chapter(s) as PDF: intro
üöÄ Building 1 chapters (pdf)
‚ö° Setting up fast build mode...
üìã Files to build: 2 files
‚úì - index.qmd
‚úì - contents/core/introduction/introduction.qmd
‚úì Fast build mode configured (PDF/EPUB)
```

This ensures that in Binder environments, you get exactly what you need: a PDF containing only the index and your target chapter, with all other chapters automatically commented out during the build process.

#### Cloud Binder Compatibility

The selective PDF build system works seamlessly in cloud environments like [mybinder.org](https://mybinder.org):

**For cloud Binder users:**
```bash
# In a Jupyter terminal or notebook cell
!./binder pdf intro

# Or using the Python CLI directly
!python binder pdf intro
```

**Key benefits for cloud environments:**
- ‚úÖ **Reduced memory usage** - Only builds essential chapters
- ‚úÖ **Faster build times** - Skips unnecessary content
- ‚úÖ **Automatic cleanup** - Restores configuration after build
- ‚úÖ **No manual editing** - Everything is automated

**What gets built:**
- Always includes `index.qmd` for proper book structure
- Includes your target chapter (e.g., `introduction.qmd`)
- Comments out all other chapters automatically
- Comments out backmatter (glossary, references) for minimal builds

### Configuration Management

The binder automatically manages Quarto configurations:

- **`_quarto-html.yml`**: Website build configuration
- **`_quarto-pdf.yml`**: Academic PDF build configuration
- **`_quarto.yml`**: **Symlink** to active configuration (currently ‚Üí `config/_quarto-html.yml`)

**Important**: The `_quarto.yml` file is a symlink that points to the active configuration. This allows the binder to quickly switch between HTML and PDF build modes without copying files.

**Quarto Executable**: The system quarto executable (`/Applications/quarto/bin/quarto`) is NOT a symlink - it's a regular executable file.

Use `./binder switch <format>` to change the active configuration symlink.

## Development Workflow

### Typical Chapter Development

```bash
# 1. Start development on a chapter
./binder preview intro

# 2. Make edits, save files (auto-rebuild in preview mode)

# 3. Build multiple related chapters together
./binder build intro,ml_systems html

# 4. Check full book before committing
./binder build * pdf
```

### Before Committing

```bash
# Clean up any build artifacts
./binder clean

# Run health check
./binder doctor

# Build full book to ensure everything works
./binder build
./binder pdf
```

## Troubleshooting

### Common Issues

**"Chapter not found"**
- Use `./binder list` to see available chapters
- Check that the chapter QMD file exists
- Verify the chapter path in configuration files

**"Build artifacts detected"**
- Run `./binder clean` to remove temporary files
- Use `./binder doctor` to verify system health

**"Config not clean"**
- The binder detected a previous fast build configuration
- Run `./binder clean` to restore normal configuration

**"Symlink issues"**
- If `_quarto.yml` is not a symlink: `ln -sf config/_quarto-html.yml book/_quarto.yml`
- Check current symlink target: `ls -la book/_quarto.yml`
- The symlink should point to either `config/_quarto-html.yml` or `config/_quarto-pdf.yml`

### Performance Tips

- Use fast builds (`./binder build chapter html`) for development
- Use unified builds (`./binder build ch1,ch2 html`) for multiple chapters
- Only use full builds (`./binder build * format`) for final verification
- Preview mode auto-rebuilds on file changes

## üöÄ Publishing

The `publish` command provides a complete publishing workflow:

```bash
# One-command publishing
./binder publish
```

**What it does:**
1. **Validates environment** - Checks Git status, tools, and dependencies
2. **Manages branches** - Merges `dev` to `main` with confirmation
3. **Plans release** - Suggests version bump (patch/minor/major)
4. **Builds everything** - PDF first, then HTML (ensures PDF is available)
5. **Creates release** - Git tag, AI-generated release notes, GitHub release
6. **Deploys** - Copies PDF to assets, commits, pushes to production

**Features:**
- ü§ñ **AI-powered release notes** (requires Ollama)
- üìä **Smart version suggestions** based on changes
- üõ°Ô∏è **Safety checks** and confirmations
- üéØ **Step-by-step wizard** with clear progress

For more details, see:
- [BUILD.md](BUILD.md) - Complete build instructions
- [DEVELOPMENT.md](DEVELOPMENT.md) - Development setup and workflow


## Links discovered
- [mybinder.org](https://mybinder.org)
- [BUILD.md](https://github.com/harvard-edge/cs249r_book/blob/dev/book/docs/BUILD.md)
- [DEVELOPMENT.md](https://github.com/harvard-edge/cs249r_book/blob/dev/book/docs/DEVELOPMENT.md)

--- book/docs/BUILD.md ---
# üõ† How to Build the Book Locally

Welcome! üëã If you‚Äôre here, you‚Äôre probably trying to **build the Machine Learning Systems book locally** on your own machine.

This guide will walk you through **how to get set up manually**, especially if you're not using GitHub Actions or Docker. We'll cover what tools you need, why you need them, and how to test everything is working.

## üöÄ Quick Start (Recommended)

For most users, the easiest way is using our **Book Binder CLI**:

```bash
# First time setup
./binder setup

# System health check
./binder doctor

# Quick chapter preview (HTML with live reload)
./binder preview intro

# Build specific chapter(s)
./binder build intro                    # Single chapter (HTML)
./binder build intro,ml_systems         # Multiple chapters (HTML)

# Build complete book
./binder build                          # Complete book (HTML)
./binder pdf                            # Complete book (PDF)
./binder epub                           # Complete book (EPUB)

# Get help
./binder help
```

The `binder` tool automatically handles all dependencies, configuration, and build processes for you!

---

## üîß Manual Setup (Advanced)

## üìö What Are We Trying to Build?

This project is written using [**Quarto**](https://quarto.org), which lets us render:

- A website (HTML version of the book)
- A typeset PDF (for printable reading)

By default, Quarto can build the HTML version pretty easily. But **building the PDF version** is a bit trickier ‚Äî it requires LaTeX, Inkscape, and a few other tools to properly render graphics and fonts.

---

## ‚úÖ What You'll Need (And Why)

| Tool | Why It's Needed | Version |
|------|------------------|---------|
| **Quarto** | The core tool that converts the `.qmd` files into HTML/PDF | 1.7.31+ |
| **Python** | Required for Book Binder CLI and build scripts | 3.9+ |
| **Python packages** | Dependencies (see `tools/dependencies/requirements.txt`) | See below |
| **R** | Some chapters include R code chunks and R-based plots | 4.0+ |
| **R packages** | Supporting packages (defined in `tools/dependencies/install_packages.R`) | Latest |
| **TinyTeX + TeX Live** | Needed for LaTeX ‚Üí PDF rendering | Latest |
| **Inkscape** | Converts `.svg` diagrams into `.pdf` (especially TikZ) | 1.0+ |
| **Ghostscript** | Compresses large PDF files | Latest |
| **System libraries** | Fonts and rendering support (Linux systems) | Various |

Don't worry ‚Äî this guide will walk you through installing all of them, step by step.

### Python Dependencies

The project uses a modern Python packaging setup with `pyproject.toml`. Core dependencies include:

**Core Build Dependencies:**
- `jupyterlab-quarto>=0.3.0` - Quarto integration
- `jupyter>=1.0.0` - Jupyter notebook support
- `pybtex>=0.24.0` - Bibliography processing
- `pypandoc>=1.11` - Document conversion
- `pyyaml>=6.0` - Configuration management
- `rich>=13.0.0` - CLI formatting and output

**Data Processing:**
- `pandas>=2.0.0` - Data manipulation
- `numpy>=1.24.0` - Numerical computing
- `Pillow>=9.0.0` - Image processing

**Additional Tools:**
- `openai>=1.0.0` - AI-assisted content tools
- `gradio>=4.0.0` - Interactive interfaces
- `ghostscript>=0.7` - PDF compression
- `pre-commit>=3.0.0` - Code quality hooks

For the complete list, see `tools/dependencies/requirements.txt` and `pyproject.toml`.

---

## üêß Setting Things Up on **Linux**

### 1. üîß Install Quarto

Quarto is what drives the entire build process.

```sh
wget https://github.com/quarto-dev/quarto-cli/releases/download/v1.7.31/quarto-1.7.31-linux-amd64.deb
sudo dpkg -i quarto-1.7.31-linux-amd64.deb
```

Test it with:

```sh
quarto --version
```

---

### 2. üìä Install R

If you're using Ubuntu or Debian:

```sh
sudo apt-get update
sudo apt-get install -y r-base
```

Test R:

```sh
R --version
```

---

### 3. üì¶ Install Required R Packages

Once R is installed, open it by typing `R`, then run:

```r
install.packages("remotes")
source("tools/dependencies/install_packages.R")
```

This installs everything the book needs to render code, plots, etc. The R package dependencies are centrally managed in `tools/dependencies/install_packages.R`.

---

### 4. ‚úíÔ∏è Install TinyTeX (LaTeX Distribution)

TinyTeX is a lightweight version of TeX Live, which Quarto uses to generate PDFs.

```sh
quarto install tinytex
```

Then add it to your shell:

```sh
echo 'export PATH=$HOME/.TinyTeX/bin/x86_64-linux:$PATH' >> ~/.bashrc
source ~/.bashrc
```

---

### 5. üß∞ Install Additional TeX Live Packages (for diagrams, fonts, etc.)

These give us broader LaTeX support:

```sh
sudo apt-get install -y texlive-latex-recommended texlive-fonts-recommended texlive-latex-extra \
  texlive-pictures texlive-luatex
```

---

### 6. üñºÔ∏è Install Inkscape

This is needed to convert `.svg` images into `.pdf` (especially for TikZ diagrams).

```sh
sudo add-apt-repository ppa:inkscape.dev/stable -y
sudo apt-get update
sudo apt-get install -y inkscape
```

Test with:

```sh
inkscape --version
```

---

### 7. üìâ Install Ghostscript (for compressing the final PDF)

```sh
sudo apt-get install -y ghostscript
```

---

### 8. üêç Install Python 3.9+ and Dependencies

```sh
sudo apt-get install -y python3 python3-pip python3-venv
```

Test with:

```sh
python3 --version    # Should be 3.9 or higher
pip3 --version
```

### 9. üì¶ Install Python Dependencies

The project uses modern Python packaging. Install all dependencies with:

```sh
# Option 1: Using pip (recommended)
pip install -r requirements.txt

# Option 2: Install in development mode (includes CLI as command)
pip install -e .

# Option 3: Using a virtual environment (best practice)
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

**What gets installed:**
- Book Binder CLI and all build tools
- Jupyter and Quarto integration packages
- Data processing libraries (pandas, numpy)
- AI/ML tools for content assistance
- Pre-commit hooks for code quality

The `requirements.txt` file points to `tools/dependencies/requirements.txt`, which contains all production and development dependencies.

---

### 10. üß™ Test That It All Works

Once you've installed everything, run the health check:

```sh
./binder doctor
```

This will verify:
- ‚úÖ Quarto installation
- ‚úÖ Python and dependencies
- ‚úÖ R and required packages
- ‚úÖ LaTeX and TinyTeX
- ‚úÖ Inkscape and Ghostscript
- ‚úÖ Configuration files
- ‚úÖ Build directory structure

If everything passes, you're ready to build the book!

---

## üß± How to Build the Book

Navigate to the root folder of the project:

```sh
cd path/to/MLSysBook
```

### üöÄ **Dual-Configuration System**

The book uses a **dual-configuration approach** that automatically switches between optimized settings for different output formats:

- **`quarto/config/_quarto-html.yml`** ‚Üí Optimized for interactive website (clean navigation, TikZ‚ÜíSVG, cross-references)
- **`quarto/config/_quarto-pdf.yml`** ‚Üí Optimized for academic PDF (full citations, LaTeX rendering, book structure)
- **`quarto/config/_quarto-epub.yml`** ‚Üí Optimized for EPUB (e-reader format, reflowable content)

The Binder CLI automatically handles configuration switching using symlinks ‚Äî **no manual file management needed!**

---

### üîπ **Build Commands (Book Binder CLI)**

The **recommended way** to build the book is using the Book Binder CLI:

#### Build Complete Book
```sh
./binder build                  # Complete website (HTML)
./binder pdf                    # Complete book (PDF)
./binder epub                   # Complete e-book (EPUB)
```

#### Build Specific Chapter(s)
```sh
./binder build intro                    # Single chapter (HTML)
./binder build intro,ml_systems         # Multiple chapters (HTML)
./binder pdf intro                      # Single chapter (PDF, selective build)
```

#### Preview Mode (Live Reload)
```sh
./binder preview                        # Preview complete book
./binder preview intro                  # Preview specific chapter
./binder preview intro,ml_systems       # Preview multiple chapters
```

#### Management Commands
```sh
./binder clean                  # Clean build artifacts
./binder status                 # Show current status
./binder list                   # List all available chapters
./binder doctor                 # Run comprehensive health check
./binder help                   # Show all commands
```

**Output Locations:**
- **HTML:** `build/html/`
- **PDF:** `build/pdf/`
- **EPUB:** `build/epub/`

---

### üîπ **Advanced: Direct Quarto Commands**

If you need direct control without the Binder CLI:

#### Website (HTML) version:
```sh
cd quarto
ln -sf config/_quarto-html.yml _quarto.yml
quarto render --to html
```

#### PDF version:
```sh
cd quarto
ln -sf config/_quarto-pdf.yml _quarto.yml
quarto render --to=titlepage-pdf
```

#### EPUB version:
```sh
cd quarto
ln -sf config/_quarto-epub.yml _quarto.yml
quarto render --to epub
```

**Important:** The Binder CLI is strongly recommended as it:
- ‚úÖ Handles configuration switching automatically
- ‚úÖ Manages build artifacts and cleanup
- ‚úÖ Provides progress indicators
- ‚úÖ Validates system health
- ‚úÖ Supports fast/selective builds

---

## ü™ü Setup on **Windows**

### Prerequisites
- Windows 10 or later
- Administrator access for some installations

### 1. Install Quarto
Download and install from [quarto.org](https://quarto.org/docs/download/)

### 2. Install Python 3.9+
Download from [python.org](https://www.python.org/downloads/) or use Windows Store.

**Important:** Check "Add Python to PATH" during installation.

### 3. Install R
Download from [CRAN](https://cran.r-project.org/)

### 4. Install R Packages
Open R and run:
```r
install.packages("remotes")
source("tools/dependencies/install_packages.R")
```

### 5. Install TinyTeX
From R console:
```r
install.packages("tinytex")
tinytex::install_tinytex()
```

### 6. Install Inkscape, Ghostscript (Using Chocolatey)
Open PowerShell (as Administrator):
```powershell
# Install Chocolatey if not already installed
Set-ExecutionPolicy Bypass -Scope Process -Force
[System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072
iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))

# Install tools
choco install inkscape ghostscript -y
```

### 7. Install Python Dependencies
Open Command Prompt or PowerShell in the project directory:
```powershell
# Create virtual environment (recommended)
python -m venv .venv
.venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 8. Test Everything Works
Run the health check:
```powershell
python binder doctor
```

Or test building:
```powershell
python binder build intro
python binder pdf
```

---

## üí° Troubleshooting Tips

### Common Installation Issues

**Quarto not found?**
```sh
# Verify installation
quarto --version

# Check PATH (Linux/macOS)
echo $PATH | grep quarto

# Reinstall if needed
# Linux: sudo dpkg -i quarto-*.deb
# macOS: brew install --cask quarto
# Windows: Download from quarto.org
```

**Python version issues?**
```sh
# Check Python version (must be 3.9+)
python --version
python3 --version

# Use specific version if multiple installed
python3.9 --version
```

**Dependencies not installing?**
```sh
# Upgrade pip first
pip install --upgrade pip setuptools wheel

# Try with verbose output
pip install -r requirements.txt -v

# If SSL errors occur
pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org -r requirements.txt
```

### Build Issues

**PDF build fails?**
- Verify LaTeX is installed: `pdflatex --version`
- Verify Inkscape is installed: `inkscape --version`
- Check TinyTeX path: `tinytex::tinytex_root()` in R
- Try rebuilding from scratch:
  ```sh
  ./binder clean
  ./binder pdf
  ```

**Chapter not found?**
```sh
# List all available chapters
./binder list

# Use exact chapter names (case-sensitive)
./binder build intro    # ‚úì correct
./binder build Intro    # ‚úó wrong
```

**Build artifacts detected?**
```sh
# Clean all build artifacts
./binder clean

# Check status
./binder status

# Run health check
./binder doctor
```

**Configuration issues?**
```sh
# Check current configuration
ls -la quarto/_quarto.yml

# Should be a symlink to config/_quarto-html.yml or config/_quarto-pdf.yml
# If not, recreate:
cd quarto
ln -sf config/_quarto-html.yml _quarto.yml
```

### System-Specific Issues

**macOS: Inkscape not in PATH?**
```sh
# Add Inkscape to PATH
echo 'export PATH="/Applications/Inkscape.app/Contents/MacOS:$PATH"' >> ~/.zshrc
source ~/.zshrc
```

**Linux: Missing system libraries?**
```sh
# Install common missing libraries
sudo apt-get install -y libcairo2-dev libharfbuzz-dev libfribidi-dev \
  libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev
```

**Windows: Permission errors?**
```powershell
# Run PowerShell as Administrator
# Disable execution policy temporarily
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process
```

### Getting Help

If you're still having issues:

1. **Run the health check**: `./binder doctor`
2. **Check the logs**: Look for detailed error messages
3. **Consult documentation**:
   - [BINDER.md](BINDER.md) - Binder CLI guide
   - [DEVELOPMENT.md](DEVELOPMENT.md) - Development setup
4. **Ask for help**:
   - GitHub Discussions: https://github.com/harvard-edge/cs249r_book/discussions
   - GitHub Issues: https://github.com/harvard-edge/cs249r_book/issues

---

## üì¶ Modern Python Packaging

The project uses modern Python packaging standards with `pyproject.toml`:

### Project Structure
```
MLSysBook/
‚îú‚îÄ‚îÄ pyproject.toml              # Python project configuration
‚îú‚îÄ‚îÄ requirements.txt            # Points to tools/dependencies/requirements.txt
‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îî‚îÄ‚îÄ dependencies/
‚îÇ       ‚îú‚îÄ‚îÄ requirements.txt    # Actual dependencies
‚îÇ       ‚îî‚îÄ‚îÄ install_packages.R  # R dependencies
‚îî‚îÄ‚îÄ cli/                        # Modular CLI package
    ‚îú‚îÄ‚îÄ main.py                 # CLI entry point
    ‚îú‚îÄ‚îÄ commands/               # Command implementations
    ‚îú‚îÄ‚îÄ core/                   # Core functionality
    ‚îî‚îÄ‚îÄ utils/                  # Utilities
```

### Installation Options

**Standard Installation (Recommended):**
```sh
pip install -r requirements.txt
```

**Development Installation:**
```sh
# Installs package in editable mode with CLI as command
pip install -e .

# Now you can use:
binder build
mlsysbook build  # Alternative command name
```

**With Optional Dependencies:**
```sh
# Install with AI features
pip install -e ".[ai]"

# Install with development tools
pip install -e ".[dev]"

# Install everything
pip install -e ".[ai,dev]"
```

### Key Features

The `pyproject.toml` defines:
- **Minimum Python version**: 3.9+
- **Core dependencies**: Listed in `dependencies` section
- **Optional dependencies**: AI tools, dev tools, build tools
- **Entry points**: `binder` and `mlsysbook` commands
- **Code quality tools**: Black, isort, pylint, mypy configurations
- **Testing setup**: Pytest with coverage

### Benefits
- ‚úÖ Standards-compliant packaging
- ‚úÖ Proper dependency management
- ‚úÖ CLI installed as system command
- ‚úÖ Supports pip, poetry, and other tools
- ‚úÖ Easy distribution and installation

---

## üéâ That's It!

Once everything is set up, you'll be able to:

### Development Workflow
- üöÄ **Preview changes locally** with live reload: `./binder preview intro`
- üî® **Build individual chapters** for fast iteration: `./binder build intro`
- üìö **Build complete book** in multiple formats: `./binder build`, `./binder pdf`, `./binder epub`
- üîç **Validate your setup** anytime: `./binder doctor`
- üßπ **Clean up artifacts**: `./binder clean`

### Contributing
- üìù **Make edits** to chapter content in `quarto/contents/`
- ‚úÖ **Test locally** before committing
- ü§ù **Follow best practices** with pre-commit hooks
- üí™ **Contribute like a pro** to the open-source book

### Next Steps
1. Read [BINDER.md](BINDER.md) for complete CLI reference
2. Check [DEVELOPMENT.md](DEVELOPMENT.md) for development workflow
3. Review [contribute.md](contribute.md) for contribution guidelines
4. Join discussions at [GitHub Discussions](https://github.com/harvard-edge/cs249r_book/discussions)

---

## üìñ Additional Resources

### Documentation
- **[BINDER.md](BINDER.md)** - Complete Book Binder CLI reference
- **[DEVELOPMENT.md](DEVELOPMENT.md)** - Development guidelines and workflow
- **[contribute.md](contribute.md)** - Contribution guidelines
- **[PUBLISH_LIVE_WORKFLOW.md](PUBLISH_LIVE_WORKFLOW.md)** - Publishing workflow

### Community
- **[GitHub Discussions](https://github.com/harvard-edge/cs249r_book/discussions)** - Ask questions and share knowledge
- **[GitHub Issues](https://github.com/harvard-edge/cs249r_book/issues)** - Report bugs and request features
- **[MLSysBook.ai](https://mlsysbook.ai)** - Main website and learning platform

### Tools and Scripts
The `tools/scripts/` directory contains various utilities:
- **`content/`** - Content management tools
- **`cross_refs/`** - Cross-reference management
- **`genai/`** - AI-assisted content tools
- **`glossary/`** - Glossary management
- **`maintenance/`** - System maintenance scripts
- **`publish/`** - Publishing and deployment tools

Run `./binder help` to see all available commands and their descriptions.

---

## üôè Contributing

We welcome contributions! The easiest way to get started:

1. **Fork and clone** the repository
2. **Set up your environment**: `./binder setup`
3. **Make your changes** to content or code
4. **Test locally**: `./binder preview <chapter>`
5. **Submit a pull request**

For detailed contribution guidelines, see [contribute.md](contribute.md).

---

**Last Updated**: October 2025
**Project**: Machine Learning Systems - Principles and Practices
**Website**: https://mlsysbook.ai


## Links discovered
- [**Quarto**](https://quarto.org)
- [quarto.org](https://quarto.org/docs/download/)
- [python.org](https://www.python.org/downloads/)
- [CRAN](https://cran.r-project.org/)
- [BINDER.md](https://github.com/harvard-edge/cs249r_book/blob/dev/book/docs/BINDER.md)
- [DEVELOPMENT.md](https://github.com/harvard-edge/cs249r_book/blob/dev/book/docs/DEVELOPMENT.md)
- [contribute.md](https://github.com/harvard-edge/cs249r_book/blob/dev/book/docs/contribute.md)
- [GitHub Discussions](https://github.com/harvard-edge/cs249r_book/discussions)
- [PUBLISH_LIVE_WORKFLOW.md](https://github.com/harvard-edge/cs249r_book/blob/dev/book/docs/PUBLISH_LIVE_WORKFLOW.md)
- [GitHub Issues](https://github.com/harvard-edge/cs249r_book/issues)
- [MLSysBook.ai](https://mlsysbook.ai)

--- book/docs/CODE_OF_CONDUCT.md ---
# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, caste, color, religion, or sexual
identity and orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
* Focusing on what is best not just for us as individuals, but for the overall
  community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or advances of
  any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email address,
  without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies when
an individual is officially representing the community in public spaces.
Examples of representing our community include using an official e-mail address,
posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at
vj@eecs.harvard.edu or nkhoshnevis@g.harvard.edu.
All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series of
actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or permanent
ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior, harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within the
community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.1, available at
<https://www.contributor-covenant.org/version/2/1/code_of_conduct.html>.

Community Impact Guidelines were inspired by
[Mozilla's code of conduct enforcement ladder][https://github.com/mozilla/inclusion].

For answers to common questions about this code of conduct, see the FAQ at
<https://www.contributor-covenant.org/faq>. Translations are available at <https://www.contributor-covenant.org/translations>.

[homepage]: https://www.contributor-covenant.org


--- book/docs/CONTAINER_BUILDS.md ---
# Containerized Build System

## Overview

This document describes the containerized build system for MLSysBook that significantly reduces build times from 45 minutes to 5-10 minutes for Linux builds.

## Architecture

### Container Strategy
- **Linux builds**: Use pre-built container with all dependencies
- **Windows builds**: Keep traditional approach (unchanged)
- **Container registry**: GitHub Container Registry (ghcr.io)

### Performance Benefits
```
Current Linux Build (45 minutes):
‚îú‚îÄ‚îÄ Install system packages (5-10 min)
‚îú‚îÄ‚îÄ Install TeX Live (15-20 min)
‚îú‚îÄ‚îÄ Install R packages (5-10 min)
‚îú‚îÄ‚îÄ Install Python packages (2-5 min)
‚îú‚îÄ‚îÄ Install Quarto (1-2 min)
‚îî‚îÄ‚îÄ Build content (5-10 min)

Containerized Linux Build (5-10 minutes):
‚îú‚îÄ‚îÄ Pull container (30 seconds)
‚îú‚îÄ‚îÄ Checkout code (30 seconds)
‚îî‚îÄ‚îÄ Build content (5-10 min)
```

## Files

### Core Files
- `docker/linux/Dockerfile` - A single Dockerfile for Linux builds.
- `docker/linux/README.md` - Linux container documentation
- `docker/linux/.dockerignore` - Build exclusions
- `docker/windows/Dockerfile` - A single Dockerfile for Windows builds.
- `docker/windows/README.md` - Windows container documentation
- `docker/windows/.dockerignore` - Build exclusions

### Container Lifecycle
1. **Build**: Weekly automatic rebuilds + manual triggers
   - Linux container: Sunday 12am
   - Windows container: Sunday 2am
2. **Storage**: GitHub Container Registry (ghcr.io)
3. **Usage**: Pulled fresh for each build job
4. **Cleanup**: GitHub manages old images automatically

## Usage

### Registry Paths
- **Linux Registry**: `ghcr.io/harvard-edge/cs249r_book/quarto-linux`
- **Windows Registry**: `ghcr.io/harvard-edge/cs249r_book/quarto-windows`

### Manual Builds
You can build the containers locally using these commands:
- **Linux**:
  ```bash
  docker build -f docker/linux/Dockerfile -t mlsysbook-linux .
  ```
- **Windows**:
  ```powershell
  docker build -f docker/windows/Dockerfile -t mlsysbook-windows .
  ```

### Manual Build Test
```bash
# Test containerized build
gh workflow run quarto-build-container.yml --field os=ubuntu-latest --field format=html
```

### Container Information
- **Linux Registry**: `ghcr.io/harvard-edge/cs249r_book/quarto-linux`
- **Windows Registry**: `ghcr.io/harvard-edge/cs249r_book/quarto-windows`
- **Tags**: `latest`, `main`, `dev`, branch-specific tags
- **Linux Size**: ~2-3GB (includes TeX Live, R, Python packages)
- **Windows Size**: ~4-5GB (includes Windows Server Core + dependencies)

## Workflow Integration

### Current Workflows
- `quarto-build-baremetal.yml` - Original workflow (brute force approach, legacy)
- `quarto-build-container.yml` - Containerized version (fast path, recommended)
- `build-linux-container.yml` - Linux container management
- `build-windows-container.yml` - Windows container management

### Migration Status
1. **‚úÖ Phase 1**: Containerized builds tested and validated
2. **‚úÖ Phase 2**: Performance significantly improved (45min ‚Üí 5-10min)
3. **‚úÖ Phase 3**: Container workflow is now the primary build method

## Container Contents

### Pre-installed Dependencies

#### Linux Container
- **System**: Ubuntu 22.04 with all required libraries
- **TeX Live**: Full distribution (texlive-full)
- **R**: R-base with all required packages
- **Python**: Python 3.13 with all requirements
- **Quarto**: Version 1.7.31
- **Tools**: Inkscape, Ghostscript, fonts

#### Windows Container
- **System**: Windows Server Core 2022
- **TeX Live**: MiKTeX distribution
- **R**: R-base with all required packages
- **Python**: Python 3.x with all requirements
- **Quarto**: Version 1.7.31
- **Tools**: Inkscape, Ghostscript, Chocolatey package manager

### Environment Variables
```bash
R_LIBS_USER=/usr/local/lib/R/library
QUARTO_LOG_LEVEL=INFO
PYTHONIOENCODING=utf-8
LANG=en_US.UTF-8
LC_ALL=en_US.UTF-8
```

## Troubleshooting

### Container Build Issues
1. Check container build logs in Actions
2. Verify dependency files are up to date
3. Test locally with `docker build -t test .`

### Build Issues
1. Check if container exists: `ghcr.io/harvard-edge/cs249r_book/quarto-linux:latest`
2. Verify container has all dependencies
3. Compare with traditional build logs

### Performance Issues
1. Monitor container pull times
2. Check disk space in container
3. Verify memory allocation

## Future Enhancements

### Potential Improvements
1. **Multi-stage builds** for smaller images
2. **Windows containers** for Windows builds
3. **Layer optimization** for faster pulls
4. **Parallel builds** for multiple formats

### Monitoring
- Container build frequency
- Build time improvements
- Error rates vs traditional builds
- Resource usage optimization

## Rollback Plan

If issues arise:
1. Keep original `quarto-build-baremetal.yml` as backup
2. Switch back to traditional builds immediately
3. Debug container issues separately
4. Re-enable when resolved

## Security

### Container Security
- Uses official Ubuntu base image
- Minimal attack surface
- Regular base image updates
- GitHub security scanning enabled

### Access Control
- Container registry access via GitHub Actions
- No external dependencies
- All builds run in isolated containers

### Building the Containers

To build the containers, use the standard `docker build` command:

```bash
# For Linux
docker build -f docker/linux/Dockerfile -t mlsysbook-linux .

# For Windows
docker build -f docker/windows/Dockerfile -t mlsysbook-windows .
```


--- book/docs/CONTRIBUTING.md ---
# Guidelines for contributing to the project

The Machine Learning Systems with TinyML project welcomes contributions from everyone. This project is maintained by a community of contributors from around the world. We appreciate your help!

Your contributions are welcome and can encompass a variety of tasks, such as:

- Identifying and reporting any bugs in the examples
- Correcting typographical errors in the documentation
- Contributing additional examples
- Authoring a new chapter
- Suggesting topics for new chapters
- Enhancing the accessibility of the material

If you are unsure about whether a contribution is appropriate, feel free to open an [issue](https://github.com/harvard-edge/cs249r_book/issues) and ask.

## How to contribute

### Open an issue

If there is an open issue for the contribution you would like to make, please comment on the issue to let us know you are working on it. If there is no open issue, please open one to let us know you are working on the contribution.

### Fork the repository

Fork the repository on GitHub and clone your fork to your local machine. We are following GitHub flow for collaboration. Please make sure that your main branch is up to date with the upstream main branch before you start working on your contribution.

### Clone the forked repository

```bash
git clone https://github.com/YOUR_USERNAME/cs249r_book.git
```

### Navigate to the repository

```bash
cd cs249r_book
```

### Add the upstream remote

```bash
git remote add upstream https://github.com/harvard-edge/cs249r_book.git
```

Please note that the upstream remote is read-only. You will not be able to push to the upstream remote. You will only be able to push to your forked repository (you will use a Pull Request for merging your code to upstream). However, you will be able to pull from the upstream remote to keep your forked repository up to date with the upstream repository.

### Create a new branch

Create a branch for your contribution. The branch name should start with issue number and be descriptive of the contribution you are making. For example, if you are fixing a typo in the documentation, the branch name could be `iss14-fix-typo-in-documentation`. If you are adding a new example, the branch name could be `iss5-add-new-example`. Following this naming convention will help us keep track of the ongoing contributions.

### Make your changes

Make your changes to the code or documentation. Please make sure that your changes are consistent with the style of the rest of the code or documentation.

- The `content` directory subfolders that each represent a chapter in the book. Each chapter folder contains the source files and documents to render the book. Any new files should be added to the `content` directory in its appropriate folder. Please create a new folder if needed. Make sure that the path in the `_quarto-html.yml` and `_quarto-pdf.yml` files is updated to include the new folder.

- Each chapter folder also include an images folder. The images folder has 4 subfolders: `png`, `pdf`, `svg`, and `jpg`. Please add your images to the appropriate folder. This is important to keep the images organized and to make sure that the images are rendered correctly in the book.

### Commit your changes

```bash
git add .
git commit -m "your commit message"
```

### Render the book

Please render the book to make sure that your contribution is rendered correctly and do not raise an error or warnings. We are using [quarto](https://quarto.org/docs/get-started/) to render the book.  You can render the book by running the following command in the terminal:

```bash
quarto render
```

### Push your changes to your forked repository

```bash
git push origin your-branch-name
```

### Open a Pull Request (PR)

Please submit the PRs to the `dev`  branch, not `main`.

Open a Pull Request (PR) to merge your changes to the upstream repository. Please add a brief description of your contribution to the PR. Please include the issue number in the description. For example, `Fix typo in the documentation (issue #14)`.

Opening an early PR is encouraged. This will allow us to provide feedback on your contribution and help you improve it. Moreover, GitHub Actions will run on your PR and will generate the book, so you can download the book and make sure that your contribution is rendered correctly.

- If your PR is a work in progress, please add `[WIP]` to the title of the PR. This will let us know that you are still working on your contribution and that you are not ready for a review or merge yet.

For a more detailed guide on the CS249r documentation process and peer review,
check [here](https://docs.google.com/document/d/1izDoWwFLnV8XK2FYCl23_9KYL_7EQ5OWLo-PCNUGle0).

## Contributor Recognition

We use [All Contributors](https://allcontributors.org) to recognize everyone who helps improve the book.

### How to Recognize a Contributor

After merging a PR or resolving an issue, comment:

```
@all-contributors please add @username for TYPE
```

### Contribution Types

| Type | Emoji | Use For |
|------|-------|---------|
| `doc` | üìñ | Wrote or improved content |
| `review` | üëÄ | Reviewed chapters or PRs |
| `translation` | üåç | Translated content |
| `design` | üé® | Created diagrams or figures |
| `bug` | üêõ | Found errors or typos |
| `ideas` | üí° | Suggested improvements |

### Example

```
@all-contributors please add @contributor for doc, review
```


## Links discovered
- [issue](https://github.com/harvard-edge/cs249r_book/issues)
- [quarto](https://quarto.org/docs/get-started/)
- [here](https://docs.google.com/document/d/1izDoWwFLnV8XK2FYCl23_9KYL_7EQ5OWLo-PCNUGle0)
- [All Contributors](https://allcontributors.org)

--- book/docs/DEVELOPMENT.md ---
# MLSysBook Development Guide

This guide covers the development workflow, automated cleanup system, and best practices for contributing to the Machine Learning Systems book.

## üéØ Essential Commands (Daily Use)

```bash
./binder clean      # Clean build artifacts
./binder build      # Build HTML book
./binder doctor     # Health check & diagnostics
./binder preview    # Live preview with hot reload
./binder pdf        # Build PDF
```

## üöÄ Quick Start

```bash
# First time setup
./binder setup              # Configure environment and tools

# Daily workflow (most common commands)
./binder clean              # Clean build artifacts
./binder build              # Build HTML (complete book)
./binder doctor             # Health check

# Preview & development
./binder preview intro      # Preview a chapter with live reload
./binder build intro        # Build specific chapter
```

## üßπ Automated Cleanup System

This project includes an **automated cleanup system** that runs before every commit to ensure a clean repository.

### What Gets Cleaned Automatically

The cleanup system removes:
- **Build artifacts**: `*.html`, `*.pdf`, `*.tex`, `*.aux`, `*.log`, `*.toc`
- **Cache directories**: `.quarto/`, `site_libs/`, `index_files/` (legacy)
- **Python artifacts**: `__pycache__/`, `*.pyc`, `*.pyo`
- **System files**: `.DS_Store`, `Thumbs.db`, `*.swp`
- **Editor files**: `*~`, `.#*`
- **Debug files**: `debug.log`, `error.log`

### Manual Cleanup Commands

```bash
# Regular cleanup (recommended before commits)
./binder clean

# See what files will be cleaned (safe preview)
git status
git clean -xdn

# Deep clean (removes all build artifacts)
./binder clean
git clean -xdf
```

### Pre-Commit Hook

The git pre-commit hook automatically:
1. üîç **Scans for build artifacts** in staged files
2. üßπ **Runs cleanup** if artifacts are detected
3. ‚ö†Ô∏è **Warns about large files** (>1MB)
4. üö® **Blocks commits** with potential secrets
5. ‚úÖ **Allows clean commits** to proceed

#### Bypassing the Hook (Emergency)

```bash
# Only if absolutely necessary
git commit --no-verify -m "Emergency commit"
```

## üî® Building the Book

### Build Commands

```bash
# Using binder (recommended)
./binder build - html          # Build HTML version
./binder build - pdf           # Build PDF version
./binder publish               # Build and publish

# Using binder (recommended)
./binder build                 # HTML version
./binder pdf                   # PDF version
./binder epub                  # EPUB version
```

### Development Workflow

```bash
# Preview a chapter (fastest)
./binder preview intro

# Build complete book
./binder build - html

# Publish to the world
./binder publish
```

### Environment Setup

The `./binder setup` command provides a complete environment configuration:

**What it does:**
1. **Checks environment** - Verifies all required tools and versions
2. **Installs dependencies** - Auto-installs missing tools (Quarto, GitHub CLI, Ollama)
3. **Configures Git** - Sets up user name, email, and GitHub username
4. **Sets preferences** - Configures build format and browser behavior
5. **Tests setup** - Builds a test chapter to verify everything works

**Features:**
- üõ†Ô∏è **Automatic tool installation** (Homebrew, apt, pip)
- üë§ **Interactive Git configuration**
- ‚öôÔ∏è **User preference setup**
- üß™ **Built-in testing** to verify setup

```bash
# Run setup
./binder setup

# Get welcome and overview
./binder hello
```

### Development Server

```bash
# Start live preview server
./binder preview

# The server will automatically reload when you save changes
```

### Build Outputs

- **HTML**: `build/html/index.html` (main output directory)
- **PDF**: `build/pdf/` (PDF output directory)
- **PDF**: `book/index.pdf` (in book directory)
- **Artifacts**: Automatically cleaned by git hooks

## üöÄ Publishing

The `./binder publish` command provides a complete publishing workflow:

**Step-by-step process:**
1. **Environment validation** - Checks Git status, tools, and dependencies
2. **Branch management** - Merges `dev` to `main` with confirmation
3. **Release planning** - Suggests version bump based on changes
4. **Build process** - PDF first, then HTML (ensures PDF availability)
5. **Release creation** - Git tag, AI-generated release notes, GitHub release
6. **Deployment** - Copies PDF to assets, commits, pushes to production

**Features:**
- ü§ñ **AI-powered release notes** (requires Ollama)
- üìä **Smart version suggestions** (patch/minor/major)
- üõ°Ô∏è **Safety checks** and confirmations
- üéØ **Step-by-step wizard** with clear progress

```bash
# One-command publishing
./binder publish
```

### Manual Publishing Steps

If you prefer to do it step by step:

```bash
# 1. Ensure you're on main branch
git checkout main
git merge dev

# 2. Build both formats
./binder build - html
./binder build - pdf

# 3. Copy PDF to assets
cp build/pdf/Machine-Learning-Systems.pdf assets/

# 4. Commit and push
git add assets/downloads/Machine-Learning-Systems.pdf
git commit -m "Add PDF to assets"
git push origin main
```

### Publishing Requirements

- ‚úÖ Must be on `main` branch
- ‚úÖ No uncommitted changes
- ‚úÖ All builds successful
- ‚úÖ Git repository properly configured

### After Publishing

The GitHub Actions workflow will:
- üîÑ Run quality checks
- üèóÔ∏è Build all formats (Linux + Windows)
- üöÄ Deploy to GitHub Pages
- üì¶ Create release assets

**Monitor progress**: https://github.com/harvard-edge/cs249r_book/actions

## üîç Project Health Checks

### Quick Status Check

```bash
./binder doctor     # Overall project health
./binder status     # Detailed project status
git status          # Git repository status
```

### Comprehensive Testing

```bash
./binder doctor     # Run comprehensive health check
quarto check        # Validate Quarto configuration
```

### Example Health Check Output

```
üîç Checking project health...

üìä Project Structure:
  QMD files: 45
  Bibliography files: 20
  Quiz files: 18

üóÇÔ∏è Git Status:
  Repository is clean

üì¶ Dependencies:
  ‚úÖ Quarto: 1.4.x
  ‚úÖ Python: 3.x
```

## üìù Content Development

### Chapter Structure

```
book/contents/
‚îú‚îÄ‚îÄ core/                    # Main content chapters
‚îÇ   ‚îú‚îÄ‚îÄ introduction/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ introduction.qmd
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ introduction.bib
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ introduction_quizzes.json
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ frontmatter/            # Preface, about, etc.
‚îú‚îÄ‚îÄ backmatter/             # References, appendices
‚îî‚îÄ‚îÄ labs/                   # Hands-on exercises
```

### Working with Minimal Configuration

For faster development, you can work with a minimal set of chapters:

1. **Edit `book/_quarto-html.yml`**: Comment out chapters you're not working on
2. **Edit bibliography section**: Comment out unused `.bib` files
3. **Build faster**: Only active chapters will be processed

```yaml
chapters:
  - index.qmd
  - contents/core/introduction/introduction.qmd
  # - contents/core/ml_systems/ml_systems.qmd    # Commented out
  # - contents/core/dl_primer/dl_primer.qmd      # Commented out
```

### Restoring Full Configuration

Simply uncomment the chapters and bibliography entries you want to restore.

## üîß Troubleshooting

### Common Issues

1. **Build fails with missing files**
   ```bash
   make clean          # Clean artifacts
   make check          # Verify structure
   ```

2. **Git hook blocks commit**
   ```bash
   make clean          # Remove artifacts
   git status          # Check what's staged
   ```

3. **Slow builds**
   ```bash
   make clean-deep     # Full cleanup
   # Use minimal configuration
   ```

4. **Permission denied on scripts**
   ```bash
   make setup-hooks    # Fix permissions
   ```

### Getting Help

```bash
./binder help       # Show all commands
./binder --help     # Detailed help
```

## üéØ Best Practices

### Before Starting Work

```bash
git pull            # Get latest changes
./binder clean      # Clean workspace
./binder doctor     # Verify health
```

### Daily Development Workflow

```bash
# 1. Clean and build
./binder clean
./binder build

# 2. Start development server
./binder preview

# 3. Make changes to .qmd files
# 4. Preview updates automatically

# 5. When ready to commit
git add .           # Pre-commit hook runs automatically
git commit -m "Your message"
```

### Before Major Changes

```bash
./binder clean      # Full cleanup
./binder build      # Clean build
./binder doctor     # Run all checks
```

### Release Preparation

```bash
./binder doctor     # Comprehensive validation
./binder build      # Build HTML
./binder pdf        # Build PDF
./binder epub       # Build EPUB
```

## ‚öôÔ∏è Configuration Files

- **`quarto/config/_quarto-html.yml`**: HTML website configuration
- **`quarto/config/_quarto-pdf.yml`**: PDF book configuration
- **`binder`**: Book Binder CLI (build and development tool)
- **`.git/hooks/pre-commit`**: Automated cleanup hook
- **`.gitignore`**: Ignored file patterns

## üóÇÔ∏è Scripts Organization

The `tools/scripts/` directory is organized into logical categories:

```
tools/scripts/
‚îú‚îÄ‚îÄ build/           # Build and development scripts (clean.sh, etc.)
‚îú‚îÄ‚îÄ content/         # Content management tools
‚îú‚îÄ‚îÄ maintenance/     # System maintenance scripts
‚îú‚îÄ‚îÄ testing/         # Test and validation scripts
‚îú‚îÄ‚îÄ utilities/       # General utility scripts
‚îú‚îÄ‚îÄ docs/            # Script documentation
‚îú‚îÄ‚îÄ genai/           # AI and generation tools
‚îú‚îÄ‚îÄ cross_refs/      # Cross-reference management
‚îú‚îÄ‚îÄ quarto_publish/  # Publishing workflows
‚îî‚îÄ‚îÄ ai_menu/         # AI menu tools
```

Each directory has its own README.md with specific usage instructions.

## ü§ù Contributing

1. **Fork and clone** the repository
2. **Run setup**: `make setup-hooks && make install`
3. **Make changes** with the development workflow above
4. **Test thoroughly**: `make test && make build-all`
5. **Submit pull request** with clean commits

The automated cleanup system ensures that your commits will be clean and won't include build artifacts, making code reviews easier and keeping the repository tidy.

## üìû Support

If you encounter issues with the development workflow:
1. Check this guide first
2. Run `make check` for diagnostics
3. Review the cleanup script output with `make clean-dry`
4. Ask for help in project discussions


--- tinytorch/milestones/extras/03_quickdemo.py ---
#!/usr/bin/env python3
"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë           üöÄ MILESTONE 05.3: TinyTalks Quick Demo (2-Minute Training)        ‚ïë
‚ïë           Watch Your Transformer Learn to Answer Questions Live!             ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

üìö HISTORICAL CONTEXT:
This demo shows the magic of transformer learning in real-time. Watch as
random noise becomes coherent answers - the same progression that happens
(at much larger scale) when training GPT, Claude, and other LLMs.

üéØ WHAT YOU'RE BUILDING:
A live training dashboard using YOUR Tinyüî•Torch implementations!
See model responses evolve from gibberish to coherent answers in ~2 minutes.

Features:
- Smaller model (~50K params) for fast training
- Live dashboard showing training progress
- Rotating prompts to show diverse capabilities
- Learning progression display (gibberish ‚Üí coherent)

‚úÖ REQUIRED MODULES (Run after Module 13):
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
  Module 01 (Tensor)         : YOUR data structure for all computations
  Module 04 (Losses)         : YOUR CrossEntropyLoss for training
  Module 07 (Optimizers)     : YOUR Adam optimizer for fast convergence
  Module 10 (Tokenization)   : YOUR CharTokenizer for text ‚Üí tokens
  Module 13 (Transformer)    : YOUR GPT model for generation
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üèóÔ∏è ARCHITECTURE (Live Training Pipeline):
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                    LIVE TRAINING DASHBOARD                    ‚îÇ
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
    ‚îÇ  ‚îÇ Progress    ‚îÇ  ‚îÇ Learning Progression                 ‚îÇ   ‚îÇ
    ‚îÇ  ‚îÇ Epoch: 3/8  ‚îÇ  ‚îÇ Q: What is 2+2?                      ‚îÇ   ‚îÇ
    ‚îÇ  ‚îÇ Loss: 1.234 ‚îÇ  ‚îÇ   Ep1: sdfj3kj... (gibberish)        ‚îÇ   ‚îÇ
    ‚îÇ  ‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 40%  ‚îÇ  ‚îÇ   Ep2: four is th... (learning)      ‚îÇ   ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ   Ep3: 4 (correct!)                  ‚îÇ   ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
    ‚îÇ  ‚îÇ Systems     ‚îÇ                                             ‚îÇ
    ‚îÇ  ‚îÇ Tokens/s    ‚îÇ  YOUR GPT model processes batches and       ‚îÇ
    ‚îÇ  ‚îÇ Memory      ‚îÇ  updates weights using YOUR optimizer       ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                             ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

# =============================================================================
# üìä YOUR MODULES IN ACTION
# =============================================================================
#
# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
# ‚îÇ What You Built      ‚îÇ How It's Used Here             ‚îÇ Systems Impact              ‚îÇ
# ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
# ‚îÇ Module 04: Loss     ‚îÇ Computes cross-entropy loss    ‚îÇ Guides learning direction   ‚îÇ
# ‚îÇ                     ‚îÇ to measure prediction error    ‚îÇ (gradient information)      ‚îÇ
# ‚îÇ                     ‚îÇ                                ‚îÇ                             ‚îÇ
# ‚îÇ Module 07: Adam     ‚îÇ Updates weights with adaptive  ‚îÇ Fast convergence in 2 min   ‚îÇ
# ‚îÇ                     ‚îÇ learning rates                 ‚îÇ (vs hours with vanilla SGD) ‚îÇ
# ‚îÇ                     ‚îÇ                                ‚îÇ                             ‚îÇ
# ‚îÇ Module 10: Tokenize ‚îÇ Converts Q&A text to tokens    ‚îÇ Character-level enables     ‚îÇ
# ‚îÇ                     ‚îÇ for model processing           ‚îÇ learning any vocabulary     ‚îÇ
# ‚îÇ                     ‚îÇ                                ‚îÇ                             ‚îÇ
# ‚îÇ Module 13: GPT      ‚îÇ Learns to predict next token   ‚îÇ Watch answers evolve from   ‚îÇ
# ‚îÇ                     ‚îÇ given question context         ‚îÇ noise to coherent text!     ‚îÇ
# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
#
# =============================================================================

üí° KEY INSIGHT:
This demo shows the same learning progression as GPT training:
  Epoch 1: "sdf3kj2l" (random weights = random output)
  Epoch 4: "fouris" (learning patterns, merging words)
  Epoch 8: "4" or "four" (correct answers!)

üìä EXPECTED RESULTS:
  Training time: ~2 minutes
  Final loss: ~1.0-1.5 (down from ~4.0)
  Visible progression: gibberish ‚Üí partial ‚Üí coherent answers
"""

import sys
import os
import time
import numpy as np
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# Rich for live dashboard
from rich.console import Console
from rich.layout import Layout
from rich.panel import Panel
from rich.table import Table
from rich.live import Live
from rich.text import Text
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn
from rich import box

# TinyTorch imports
from tinytorch.core.tensor import Tensor
from tinytorch.core.optimizers import Adam
from tinytorch.core.losses import CrossEntropyLoss
from tinytorch.core.transformers import GPT
from tinytorch.core.tokenization import CharTokenizer

console = Console()

# =============================================================================
# Configuration - Optimized for ~2 minute training
# =============================================================================

CONFIG = {
    # Model (smaller for speed)
    "n_layer": 2,
    "n_head": 2,
    "n_embd": 64,
    "max_seq_len": 32,  # Shorter sequences for speed

    # Training (optimized for ~2 min on pure Python)
    "epochs": 8,
    "batches_per_epoch": 30,
    "batch_size": 8,
    "learning_rate": 0.003,  # Balanced LR for stable convergence

    # Display
    "update_interval": 5,  # Update dashboard every N batches
}

# Test prompts to show model learning (3 prompts for better progression display)
TEST_PROMPTS = [
    "Q: What is 2+2?\nA:",
    "Q: What color is the sky?\nA:",
    "Q: Say hello\nA:",
]


# =============================================================================
# Dataset
# =============================================================================

class TinyTalksDataset:
    """Simple character-level dataset from TinyTalks."""

    def __init__(self, data_path: Path, seq_len: int):
        self.seq_len = seq_len

        # Load text
        with open(data_path, 'r') as f:
            self.text = f.read()

        # Create tokenizer and build vocabulary
        self.tokenizer = CharTokenizer()
        self.tokenizer.build_vocab([self.text])

        # Tokenize entire text
        self.tokens = self.tokenizer.encode(self.text)

    def __len__(self):
        return len(self.tokens) - self.seq_len

    def get_batch(self, batch_size: int):
        """Get random batch of sequences."""
        indices = np.random.randint(0, len(self) - 1, size=batch_size)

        inputs = []
        targets = []

        for idx in indices:
            seq = self.tokens[idx:idx + self.seq_len + 1]
            inputs.append(seq[:-1])
            targets.append(seq[1:])

        return (
            Tensor(np.array(inputs)),
            Tensor(np.array(targets))
        )


# =============================================================================
# Text Generation
# =============================================================================

def generate_response(model, tokenizer, prompt: str, max_tokens: int = 30) -> str:
    """Generate text from prompt."""
    # Encode prompt
    tokens = tokenizer.encode(prompt)

    for _ in range(max_tokens):
        # Prepare input
        context = tokens[-CONFIG["max_seq_len"]:]
        x = Tensor(np.array([context]))

        # Forward pass
        logits = model.forward(x)

        # Get next token probabilities
        last_logits = logits.data[0, -1, :]

        # Temperature sampling
        temperature = 0.8
        last_logits = last_logits / temperature
        exp_logits = np.exp(last_logits - np.max(last_logits))
        probs = exp_logits / np.sum(exp_logits)

        # Sample
        next_token = np.random.choice(len(probs), p=probs)
        tokens.append(next_token)

        # Stop at newline (end of answer)
        if tokenizer.decode([next_token]) == '\n':
            break

    # Decode and extract answer
    full_text = tokenizer.decode(tokens)

    # Get just the answer part
    if "A:" in full_text:
        answer = full_text.split("A:")[-1].strip()
        # Clean up - take first line
        answer = answer.split('\n')[0].strip()
        return answer if answer else "(empty)"

    return full_text[len(prompt):].strip() or "(empty)"


# =============================================================================
# Dashboard Layout
# =============================================================================

def make_layout() -> Layout:
    """Create the dashboard layout."""
    layout = Layout()

    layout.split_column(
        Layout(name="header", size=3),
        Layout(name="main", ratio=1),
        Layout(name="footer", size=3),
    )

    layout["main"].split_row(
        Layout(name="left", ratio=1),
        Layout(name="outputs", ratio=2),
    )

    layout["left"].split_column(
        Layout(name="progress", ratio=2),
        Layout(name="stats", ratio=1),
    )

    return layout


def make_header() -> Panel:
    """Create header panel."""
    return Panel(
        Text("TinyTalks Quick Demo - Watch Your Transformer Learn!",
             style="bold cyan", justify="center"),
        box=box.ROUNDED,
        style="cyan",
    )


def make_progress_panel(epoch: int, total_epochs: int, batch: int,
                        total_batches: int, loss: float, elapsed: float) -> Panel:
    """Create training progress panel."""
    # Calculate overall progress
    total_steps = total_epochs * total_batches
    current_step = (epoch - 1) * total_batches + batch
    progress_pct = (current_step / total_steps) * 100

    # Progress bar
    bar_width = 20
    filled = int(bar_width * progress_pct / 100)
    bar = "‚ñà" * filled + "‚ñë" * (bar_width - filled)

    # Estimate time remaining
    if current_step > 0:
        time_per_step = elapsed / current_step
        remaining_steps = total_steps - current_step
        eta = remaining_steps * time_per_step
        eta_str = f"{eta:.0f}s"
    else:
        eta_str = "..."

    content = Text()
    content.append(f"Epoch: {epoch}/{total_epochs}\n", style="bold")
    content.append(f"Batch: {batch}/{total_batches}\n")
    content.append(f"Loss: {loss:.3f}\n\n", style="yellow")
    content.append(f"{bar} {progress_pct:.0f}%\n\n", style="green")
    content.append(f"Elapsed: {elapsed:.0f}s\n")
    content.append(f"ETA: {eta_str}")

    return Panel(
        content,
        title="[bold]Training Progress[/bold]",
        border_style="green",
        box=box.ROUNDED,
    )


def make_outputs_panel(responses: dict, epoch: int) -> Panel:
    """Create model outputs panel showing all epoch responses as a log."""
    content = Text()

    # Show all 3 prompts with full epoch history
    for i, prompt in enumerate(TEST_PROMPTS):
        q = prompt.split('\n')[0]
        content.append(f"{q}\n", style="cyan bold")

        # Show all epochs completed so far
        for ep in range(1, epoch + 1):
            key = f"epoch_{ep}_{i}"
            response = responses.get(key, "...")
            # Most recent epoch is highlighted
            style = "white" if ep == epoch else "dim"
            content.append(f"  Ep{ep}: ", style="yellow")
            # Truncate long responses to fit
            display_response = response[:25] + "..." if len(response) > 25 else response
            content.append(f"{display_response}\n", style=style)

        content.append("\n")

    return Panel(
        content,
        title=f"[bold]Learning Progression (Epoch {epoch})[/bold]",
        border_style="blue",
        box=box.ROUNDED,
    )


def make_stats_panel(stats: dict) -> Panel:
    """Create systems stats panel."""
    content = Text()

    content.append("Performance Metrics\n", style="bold")
    content.append(f"  Tokens/sec: {stats.get('tokens_per_sec', 0):.1f}\n")
    content.append(f"  Batch time: {stats.get('batch_time_ms', 0):.0f}ms\n")
    content.append(f"  Memory: {stats.get('memory_mb', 0):.1f}MB\n\n")

    content.append("Model Stats\n", style="bold")
    content.append(f"  Parameters: {stats.get('params', 0):,}\n")
    content.append(f"  Vocab size: {stats.get('vocab_size', 0)}\n")

    return Panel(
        content,
        title="[bold]Systems[/bold]",
        border_style="magenta",
        box=box.ROUNDED,
    )


def make_footer(message: str = "") -> Panel:
    """Create footer panel."""
    if not message:
        message = "Training in progress... Watch the model learn to answer questions!"

    return Panel(
        Text(message, style="dim", justify="center"),
        box=box.ROUNDED,
        style="dim",
    )


# =============================================================================
# Main Training Loop
# =============================================================================

def main():
    """Main training function with live dashboard."""

    # Welcome
    console.print()
    console.print(Panel.fit(
        "[bold cyan]TinyTalks Quick Demo[/bold cyan]\n\n"
        "Watch a transformer learn to answer questions in real-time!\n"
        "The model starts with random weights (gibberish output)\n"
        "and learns to produce coherent answers.\n\n"
        "[dim]Training time: ~2 minutes[/dim]",
        title="Welcome",
        border_style="cyan",
    ))
    console.print()

    # Load dataset
    project_root = Path(__file__).parent.parent.parent
    data_path = project_root / "datasets" / "tinytalks" / "splits" / "train.txt"

    if not data_path.exists():
        console.print(f"[red]Error: Dataset not found at {data_path}[/red]")
        console.print("[yellow]Please ensure TinyTalks dataset is available.[/yellow]")
        return

    console.print(f"[dim]Loading dataset from {data_path}...[/dim]")
    dataset = TinyTalksDataset(data_path, CONFIG["max_seq_len"])
    console.print(f"[green]‚úì[/green] Loaded {len(dataset.text):,} characters, vocab size: {dataset.tokenizer.vocab_size}")

    # Create model
    console.print("[dim]Creating model...[/dim]")
    model = GPT(
        vocab_size=dataset.tokenizer.vocab_size,
        embed_dim=CONFIG["n_embd"],
        num_heads=CONFIG["n_head"],
        num_layers=CONFIG["n_layer"],
        max_seq_len=CONFIG["max_seq_len"],
    )

    # Count parameters
    param_count = sum(p.data.size for p in model.parameters())
    console.print(f"[green]‚úì[/green] Model created: {param_count:,} parameters")
    console.print(f"[dim]  {CONFIG['n_layer']} layers, {CONFIG['n_head']} heads, {CONFIG['n_embd']} embed dim[/dim]")

    # Setup training
    optimizer = Adam(model.parameters(), lr=CONFIG["learning_rate"])
    criterion = CrossEntropyLoss()

    console.print()
    console.print("[bold green]Starting training with live dashboard...[/bold green]")
    console.print("[dim]Press Ctrl+C to stop early[/dim]")
    console.print()
    time.sleep(1)

    # Storage for responses and stats
    responses = {}
    stats = {
        "params": param_count,
        "vocab_size": dataset.tokenizer.vocab_size,
        "tokens_per_sec": 0,
        "batch_time_ms": 0,
        "memory_mb": param_count * 4 / (1024 * 1024),  # Rough estimate
    }

    # Create layout
    layout = make_layout()

    # Training loop with live display
    start_time = time.time()
    current_loss = 0.0
    total_tokens = 0

    try:
        with Live(layout, console=console, refresh_per_second=4) as live:
            for epoch in range(1, CONFIG["epochs"] + 1):
                epoch_loss = 0.0

                for batch_idx in range(1, CONFIG["batches_per_epoch"] + 1):
                    batch_start = time.time()

                    # Get batch
                    inputs, targets = dataset.get_batch(CONFIG["batch_size"])

                    # Forward pass
                    logits = model.forward(inputs)

                    # Reshape for loss
                    batch_size, seq_len, vocab_size = logits.shape
                    logits_flat = logits.reshape(batch_size * seq_len, vocab_size)
                    targets_flat = targets.reshape(-1)

                    # Compute loss
                    loss = criterion(logits_flat, targets_flat)

                    # Backward pass
                    loss.backward()

                    # Update
                    optimizer.step()
                    optimizer.zero_grad()

                    # Track loss and stats
                    batch_loss = float(loss.data)
                    epoch_loss += batch_loss
                    current_loss = epoch_loss / batch_idx

                    # Update systems stats
                    batch_time = time.time() - batch_start
                    tokens_in_batch = batch_size * seq_len
                    total_tokens += tokens_in_batch
                    elapsed = time.time() - start_time

                    stats["batch_time_ms"] = batch_time * 1000
                    stats["tokens_per_sec"] = total_tokens / elapsed if elapsed > 0 else 0

                    # Update dashboard
                    layout["header"].update(make_header())
                    layout["progress"].update(make_progress_panel(
                        epoch, CONFIG["epochs"],
                        batch_idx, CONFIG["batches_per_epoch"],
                        current_loss, elapsed
                    ))
                    layout["stats"].update(make_stats_panel(stats))
                    layout["outputs"].update(make_outputs_panel(responses, epoch))
                    layout["footer"].update(make_footer())

                # End of epoch - generate sample responses
                for i, prompt in enumerate(TEST_PROMPTS):
                    response = generate_response(model, dataset.tokenizer, prompt)
                    responses[f"epoch_{epoch}_{i}"] = response

                # Update display with new responses
                layout["outputs"].update(make_outputs_panel(responses, epoch))

                # Show epoch completion message
                layout["footer"].update(make_footer(
                    f"Epoch {epoch} complete! Loss: {current_loss:.3f}"
                ))

        # Training complete
        total_time = time.time() - start_time

        console.print()
        console.print(Panel.fit(
            f"[bold green]Training Complete![/bold green]\n\n"
            f"Total time: {total_time:.1f} seconds\n"
            f"Final loss: {current_loss:.3f}\n"
            f"Epochs: {CONFIG['epochs']}\n\n"
            "[cyan]Watch how your transformer learned to talk![/cyan]",
            title="Success",
            border_style="green",
        ))

        # Show learning progression for all prompts
        console.print()
        console.print("[bold]Full Learning Progression:[/bold]")
        console.print()

        for i, prompt in enumerate(TEST_PROMPTS):
            q = prompt.split('\n')[0]
            table = Table(box=box.ROUNDED, title=q)
            table.add_column("Epoch", style="cyan")
            table.add_column("Response", style="white")

            for epoch in range(1, CONFIG["epochs"] + 1):
                key = f"epoch_{epoch}_{i}"
                resp = responses.get(key, "...")
                table.add_row(str(epoch), resp)

            console.print(table)
            console.print()

    except KeyboardInterrupt:
        console.print("\n[yellow]Training stopped by user[/yellow]")


if __name__ == "__main__":
    main()


--- tinytorch/datasets/tinytalks/examples/demo_usage.py ---
"""
TinyTalks Dataset Usage Examples

Demonstrates how to load and use the TinyTalks dataset for training
transformer models.

Usage:
    python examples/demo_usage.py
"""

from pathlib import Path


def example1_load_full_dataset():
    """Example 1: Load the full dataset"""
    print("=" * 60)
    print("Example 1: Loading Full Dataset")
    print("=" * 60)
    
    dataset_path = Path(__file__).parent.parent / "tinytalks_v1.txt"
    
    with open(dataset_path, 'r', encoding='utf-8') as f:
        text = f.read()
    
    print(f"‚úì Loaded dataset from: {dataset_path.name}")
    print(f"  Total size: {len(text)} characters")
    print(f"  Total lines: {len(text.splitlines())} lines")
    
    # Show first 300 characters
    print(f"\n  First 300 characters:")
    print(f"  {'-' * 58}")
    print(f"  {text[:300]}...")
    
    return text


def example2_load_train_split():
    """Example 2: Load training split only"""
    print("\n" + "=" * 60)
    print("Example 2: Loading Training Split")
    print("=" * 60)
    
    train_path = Path(__file__).parent.parent / "splits" / "train.txt"
    
    with open(train_path, 'r', encoding='utf-8') as f:
        train_text = f.read()
    
    print(f"‚úì Loaded training split from: {train_path.name}")
    print(f"  Size: {len(train_text)} characters")
    
    return train_text


def example3_parse_qa_pairs():
    """Example 3: Parse Q&A pairs from text"""
    print("\n" + "=" * 60)
    print("Example 3: Parsing Q&A Pairs")
    print("=" * 60)
    
    dataset_path = Path(__file__).parent.parent / "tinytalks_v1.txt"
    
    with open(dataset_path, 'r', encoding='utf-8') as f:
        text = f.read()
    
    # Parse Q&A pairs
    qa_pairs = []
    blocks = text.strip().split('\n\n')
    
    for block in blocks:
        lines = block.strip().split('\n')
        if len(lines) == 2:
            q_line = lines[0]
            a_line = lines[1]
            
            if q_line.startswith('Q: ') and a_line.startswith('A: '):
                question = q_line[3:]  # Remove "Q: "
                answer = a_line[3:]    # Remove "A: "
                qa_pairs.append((question, answer))
    
    print(f"‚úì Parsed {len(qa_pairs)} Q&A pairs")
    print(f"\n  First 5 pairs:")
    print(f"  {'-' * 58}")
    for i, (q, a) in enumerate(qa_pairs[:5], 1):
        print(f"\n  {i}. Q: {q}")
        print(f"     A: {a}")
    
    return qa_pairs


def example4_character_tokenization():
    """Example 4: Character-level tokenization"""
    print("\n" + "=" * 60)
    print("Example 4: Character-Level Tokenization")
    print("=" * 60)
    
    dataset_path = Path(__file__).parent.parent / "tinytalks_v1.txt"
    
    with open(dataset_path, 'r', encoding='utf-8') as f:
        text = f.read()
    
    # Build character vocabulary
    vocab = sorted(set(text))
    char_to_idx = {ch: i for i, ch in enumerate(vocab)}
    idx_to_char = {i: ch for i, ch in enumerate(vocab)}
    
    print(f"‚úì Built character vocabulary")
    print(f"  Vocabulary size: {len(vocab)}")
    print(f"  Characters: {repr(''.join(vocab[:20]))}")
    
    # Encode a sample
    sample = "Q: Hello! A: Hi there!"
    encoded = [char_to_idx[ch] for ch in sample]
    
    print(f"\n  Sample text: {sample}")
    print(f"  Encoded: {encoded[:20]}...")
    
    # Decode back
    decoded = ''.join([idx_to_char[idx] for idx in encoded])
    print(f"  Decoded: {decoded}")
    
    assert sample == decoded, "Encoding/decoding mismatch!"
    print(f"  ‚úì Encoding/decoding verified")
    
    return vocab, char_to_idx, idx_to_char


def example5_prepare_for_transformer():
    """Example 5: Prepare data for transformer training"""
    print("\n" + "=" * 60)
    print("Example 5: Preparing Data for Transformer")
    print("=" * 60)
    
    # Load training data
    train_path = Path(__file__).parent.parent / "splits" / "train.txt"
    
    with open(train_path, 'r', encoding='utf-8') as f:
        train_text = f.read()
    
    # Build vocabulary
    vocab = sorted(set(train_text))
    char_to_idx = {ch: i for i, ch in enumerate(vocab)}
    
    print(f"‚úì Prepared data for training")
    print(f"  Training text size: {len(train_text)} characters")
    print(f"  Vocabulary size: {len(vocab)}")
    
    # Show example sequence creation
    seq_length = 32
    sample_seq = train_text[:seq_length]
    sample_target = train_text[1:seq_length+1]
    
    print(f"\n  Example input sequence (first {seq_length} chars):")
    print(f"    {repr(sample_seq)}")
    print(f"\n  Example target sequence (shifted by 1):")
    print(f"    {repr(sample_target)}")
    
    return train_text, vocab, char_to_idx


def example6_using_with_tinytorch():
    """Example 6: Using with TinyTorch (pseudocode)"""
    print("\n" + "=" * 60)
    print("Example 6: Using with TinyTorch (Pseudocode)")
    print("=" * 60)
    
    print("""
  # Import TinyTorch components
  from tinytorch.models.transformer import GPT
  from tinytorch.text.tokenization import CharTokenizer
  from tinytorch.core.optimizers import Adam
  from tinytorch.core.losses import CrossEntropyLoss
  
  # Load dataset
  with open('datasets/tinytalks/splits/train.txt', 'r') as f:
      train_text = f.read()
  
  # Initialize tokenizer
  tokenizer = CharTokenizer()
  tokenizer.fit(train_text)
  
  # Initialize model
  model = GPT(
      vocab_size=len(tokenizer),
      embed_dim=128,
      num_layers=4,
      num_heads=4,
      max_seq_len=64
  )
  
  # Initialize optimizer and loss
  optimizer = Adam(model.parameters(), lr=0.001)
  criterion = CrossEntropyLoss()
  
  # Training loop (simplified)
  for epoch in range(10):
      # ... create batches from train_text ...
      # ... forward pass ...
      # ... compute loss ...
      # ... backward pass ...
      # ... optimizer step ...
      print(f"Epoch {epoch+1}, Loss: {loss}")
  
  # Generate text
  prompt = "Q: What is your name?"
  response = model.generate(prompt, tokenizer)
  print(response)
  """)
    
    print(f"\n  See milestones/05_2017_transformer/tinybot_demo.py")
    print(f"  for a complete working example!")


def main():
    """Run all examples"""
    print("\n")
    print("*" * 60)
    print("  TinyTalks Dataset - Usage Examples")
    print("*" * 60)
    
    # Run examples
    text = example1_load_full_dataset()
    train_text = example2_load_train_split()
    qa_pairs = example3_parse_qa_pairs()
    vocab, char_to_idx, idx_to_char = example4_character_tokenization()
    train_text, vocab, char_to_idx = example5_prepare_for_transformer()
    example6_using_with_tinytorch()
    
    print("\n" + "=" * 60)
    print("  ‚úÖ All examples completed successfully!")
    print("=" * 60)
    print()


if __name__ == "__main__":
    main()



--- book/tools/scripts/cross_refs/HOW_TO_REGENERATE_XREFS.md ---
# How to Regenerate Cross-References

This guide explains how to regenerate all cross-references for the MLSysBook using Gemma 2 27B model.

## Prerequisites

1. **Ollama**: Ensure Ollama is installed and running
   ```bash
   # Check if Ollama is running
   ollama list

   # If gemma2:27b is not available, pull it
   ollama pull gemma2:27b
   ```

2. **Python Dependencies** (optional for enhanced generation):
   ```bash
   pip install sentence-transformers scikit-learn numpy torch pyyaml pypandoc requests
   ```

## Method 1: Simple Regeneration Script (Recommended)

Run the provided regeneration script:

```bash
cd /Users/VJ/GitHub/MLSysBook
python3 tools/scripts/cross_refs/regenerate_xrefs.py
```

This script will:
1. Check if Ollama is running with gemma2:27b
2. Clean up all existing _xrefs.json files
3. Generate base cross-references using production_xref_generator.py
4. Enhance explanations with Gemma 2 if available
5. Save files to `quarto/contents/core/[chapter]/[chapter]_xrefs.json`

## Method 2: Using Production Generator Directly

```bash
cd tools/scripts/cross_refs
python3 production_xref_generator.py
```

This generates cross-references without LLM enhancement but is faster.

## Method 3: Using manage_cross_references.py (Advanced)

For more control with specific models:

```bash
cd tools/scripts/cross_refs
python3 manage_cross_references.py \
  --generate \
  --model sentence-t5-base \
  --output cross_refs.json \
  --dirs ../../../quarto/contents/core/ \
  --explain \
  --ollama-model gemma2:27b
```

## Files Generated

The scripts generate individual `_xrefs.json` files for each chapter:
- `introduction/introduction_xrefs.json`
- `ml_systems/ml_systems_xrefs.json`
- `dl_primer/dl_primer_xrefs.json`
- ... (one for each chapter)

## Structure of _xrefs.json Files

Each file contains:
```json
{
  "cross_references": {
    "sec-[section-id]": [
      {
        "target_chapter": "chapter_name",
        "target_section": "sec-target-id",
        "connection_type": "prerequisite|foundation|extends|complements",
        "concepts": ["concept1", "concept2"],
        "strength": 0.0-1.0,
        "quality": 0.0-1.0,
        "explanation": "Brief explanation",
        "placement": "chapter_start|section_start|section_end|sidebar",
        "priority": 1-3
      }
    ]
  }
}
```

## Using the Cross-References

The `inject-xrefs.lua` filter automatically uses these files during PDF/HTML generation:

```bash
# Build with cross-references
./binder pdf intro  # Builds introduction chapter with cross-refs
./binder pdf all    # Builds entire book with cross-refs
```

## Configuration

### Default Model
The default model is set to `gemma2:27b` in:
- `tools/scripts/cross_refs/manage_cross_references.py`
- `tools/scripts/cross_refs/llm_enhanced_xrefs.py`

To change the default, modify the `selected_model` variable or use `--ollama-model` flag.

### Hybrid Mode Settings
The inject-xrefs.lua filter uses hybrid mode by default, configured with:
- `MAX_CHAPTER_REFS = 8` - Maximum references in chapter-level box
- `MAX_SECTION_REFS = 3` - Maximum references per section
- `PRIORITY_THRESHOLD = 2` - Show priority 1-2 in sections
- `STRENGTH_THRESHOLD = 0.25` - Show connections >25% strength

## Troubleshooting

1. **Ollama not running**: Start Ollama first
   ```bash
   ollama serve
   ```

2. **Model not available**: Pull the model
   ```bash
   ollama pull gemma2:27b
   ```

3. **Python dependencies missing**: The production generator works without ML libraries
   ```bash
   python3 production_xref_generator.py
   ```

4. **Files not in correct location**: Ensure you're in the MLSysBook root directory

## Notes

- Generation takes ~2-5 minutes depending on whether LLM enhancement is enabled
- The hybrid placement mode reduces visual clutter by ~60% while maintaining pedagogical value
- Cross-references point to specific sections, not just chapters
- Each reference includes both chapter name and section number (e.g., "**ML Systems**: (¬ß2.1)")


--- tinytorch/modules/README.md ---
# TinyTorch Modules

Your workspace for building ML systems from scratch.

## üöÄ Getting Started

```bash
tito module start 01    # Start Module 01: Tensors
```

This creates the module folder and opens Jupyter Lab.

## üìã Workflow

1. `tito module start XX` - Create and start a module
2. Work in Jupyter Lab - Build your implementation
3. `tito module complete XX` - Test and export your work

## üìä Progress

```bash
tito module status      # View your progress
tito module list        # See all available modules
```

Each module builds on previous ones. Complete them in order to build your ML framework!


--- .github/TINYTORCH_ISSUE_TEMPLATE/module_architecture_improvement.md ---
---
name: üìö Module Architecture: Break Complex Modules into Digestible Sub-Components
about: Suggest breaking down large monolithic modules into smaller, focused pieces while maintaining educational cohesion
title: "Break [MODULE_NAME] into smaller sub-components while maintaining module unity"
labels: ["enhancement", "education", "architecture", "modules"]
assignees: []
---

## üìö **Educational Problem**

Several TinyTorch modules have grown quite large (1000+ lines), making them difficult for students to navigate, understand, and debug. While the modules work well as cohesive educational units, the individual development files can be overwhelming.

**Current Complex Modules:**
- `02_tensor/tensor_dev.py`: 1,578 lines
- `15_mlops/mlops_dev.py`: 1,667 lines
- `13_kernels/kernels_dev.py`: 1,381 lines
- `05_dense/dense_dev.py`: 907 lines

## üéØ **Proposed Solution**

Break each complex module into **smaller, focused subcomponents** while maintaining the module structure and educational flow. Think "bite-sized pieces that still work as a whole."

### Example: Breaking Down `02_tensor` Module

**Current Structure:**
```
modules/02_tensor/
‚îú‚îÄ‚îÄ tensor_dev.py          # 1,578 lines - everything in one file
‚îú‚îÄ‚îÄ module.yaml
‚îî‚îÄ‚îÄ README.md
```

**Proposed Structure:**
```
modules/02_tensor/
‚îú‚îÄ‚îÄ parts/
‚îÇ   ‚îú‚îÄ‚îÄ 01_foundations.py     # Mathematical foundations & tensor theory
‚îÇ   ‚îú‚îÄ‚îÄ 02_creation.py        # Tensor creation & initialization
‚îÇ   ‚îú‚îÄ‚îÄ 03_operations.py      # Core arithmetic operations
‚îÇ   ‚îú‚îÄ‚îÄ 04_broadcasting.py    # Broadcasting & shape manipulation
‚îÇ   ‚îú‚îÄ‚îÄ 05_advanced.py        # Advanced operations & edge cases
‚îÇ   ‚îî‚îÄ‚îÄ 06_integration.py     # Integration tests & complete examples
‚îú‚îÄ‚îÄ tensor_dev.py             # Main orchestrator that imports all parts
‚îú‚îÄ‚îÄ module.yaml
‚îî‚îÄ‚îÄ README.md
```

### Example: Breaking Down `15_mlops` Module

**Current Structure:**
```
modules/15_mlops/
‚îú‚îÄ‚îÄ mlops_dev.py          # 1,667 lines - entire MLOps pipeline
‚îú‚îÄ‚îÄ module.yaml
‚îî‚îÄ‚îÄ README.md
```

**Proposed Structure:**
```
modules/15_mlops/
‚îú‚îÄ‚îÄ parts/
‚îÇ   ‚îú‚îÄ‚îÄ 01_monitoring.py      # Model and data drift detection
‚îÇ   ‚îú‚îÄ‚îÄ 02_deployment.py      # Model serving & API endpoints
‚îÇ   ‚îú‚îÄ‚îÄ 03_pipeline.py        # Continuous learning workflows
‚îÇ   ‚îú‚îÄ‚îÄ 04_registry.py        # Model versioning & registry
‚îÇ   ‚îú‚îÄ‚îÄ 05_alerting.py        # Alert systems & notifications
‚îÇ   ‚îî‚îÄ‚îÄ 06_integration.py     # Full MLOps pipeline integration
‚îú‚îÄ‚îÄ mlops_dev.py              # Main orchestrator
‚îú‚îÄ‚îÄ module.yaml
‚îî‚îÄ‚îÄ README.md
```

## üèóÔ∏è **Implementation Strategy**

### 1. **Maintain Module Unity**
- Keep the main `{module}_dev.py` file as the **primary entry point**
- Use imports to bring all subcomponents together
- Ensure the module still "feels like one cohesive lesson"

### 2. **Logical Decomposition**
- Break modules by **conceptual boundaries**, not arbitrary line counts
- Each subcomponent should be **self-contained** but **integrate seamlessly**
- Maintain the **Build ‚Üí Use ‚Üí Optimize** educational flow across parts

### 3. **Educational Benefits**
- **Easier navigation**: Students can focus on specific concepts
- **Better debugging**: Smaller files are easier to troubleshoot
- **Clearer progression**: Natural learning checkpoints within modules
- **Maintained cohesion**: Everything still works together as intended

### 4. **Technical Implementation**
```python
# Main module file (e.g., tensor_dev.py)
"""
TinyTorch Tensor Module - Complete Implementation
Students work through parts/ directory, then see integration here.
"""

# Import all sub-components
from .parts.foundations import *
from .parts.creation import *
from .parts.operations import *
from .parts.broadcasting import *
from .parts.advanced import *

# Integration and final examples
from .parts.integration import run_complete_tensor_demo

# Expose the complete Tensor class
__all__ = ['Tensor', 'run_complete_tensor_demo']
```

## üéì **Educational Advantages**

1. **Bite-sized Learning**: Students can master one concept at a time
2. **Natural Progression**: Clear path through complex topics
3. **Better Testing**: Each part can have focused inline tests
4. **Easier Review**: Instructors can review specific components
5. **Maintained Flow**: Module still tells one coherent story

## üîß **Implementation Notes**

- This is **architectural improvement**, not feature addition
- Maintains all existing functionality and educational goals
- **Backward compatible**: Current workflows continue to work
- Each module can be migrated independently
- Priority should be given to largest/most complex modules first

## üìã **Success Criteria**

- [ ] No single subcomponent exceeds ~300 lines
- [ ] Each part has clear educational purpose
- [ ] Main module file remains functional entry point
- [ ] All inline tests continue to pass
- [ ] Students report improved navigation and understanding
- [ ] Module still "feels like one lesson" despite internal structure

## üéØ **Priority Modules for Migration**

1. **`02_tensor`** (1,578 lines) - Foundation module, affects all others
2. **`15_mlops`** (1,667 lines) - Complex capstone module
3. **`13_kernels`** (1,381 lines) - Performance engineering module
4. **`11_training`** (estimated 1,000+ lines) - Core training pipeline

---

**This enhancement will make TinyTorch more student-friendly while maintaining its educational integrity and systematic learning progression.**


--- tinytorch/site/modules/01_tensor_ABOUT.md ---
../../src/01_tensor/ABOUT.md

--- tinytorch/site/modules/02_activations_ABOUT.md ---
../../src/02_activations/ABOUT.md

--- tinytorch/site/modules/03_layers_ABOUT.md ---
../../src/03_layers/ABOUT.md

--- tinytorch/site/modules/04_losses_ABOUT.md ---
../../src/04_losses/ABOUT.md

--- tinytorch/site/modules/05_dataloader_ABOUT.md ---
../../src/05_dataloader/ABOUT.md

--- tinytorch/site/modules/06_autograd_ABOUT.md ---
../../src/06_autograd/ABOUT.md

--- tinytorch/site/modules/07_optimizers_ABOUT.md ---
../../src/07_optimizers/ABOUT.md

--- tinytorch/site/modules/08_training_ABOUT.md ---
../../src/08_training/ABOUT.md

--- tinytorch/site/tiers/architecture.md ---
# Architecture Tier (Modules 09-13)

**Build modern neural architectures‚Äîfrom computer vision to language models.**


## What You'll Learn

The Architecture tier teaches you how to build the neural network architectures that power modern AI. You'll implement CNNs for computer vision and transformers for language understanding, building on the foundational training infrastructure from the previous tier.

**By the end of this tier, you'll understand:**
- Why convolutional layers are essential for computer vision
- How attention mechanisms enable transformers to understand sequences
- What embeddings do to represent discrete tokens as continuous vectors
- How modern architectures compose these components into powerful systems


## Module Progression

```{mermaid}
:align: center
:caption: "**Architecture Module Flow.** Two parallel tracks branch from Foundation: vision (Convolutions) and language (Tokenization through Transformers)."
graph TB
 F[ Foundation<br/>Tensor, DataLoader, Autograd, Training]

 F --> M09[09. Convolutions<br/>Conv2d + Pooling]
 M09 --> VISION[ Computer Vision<br/>CNNs unlock spatial intelligence]

 F --> M10[10. Tokenization<br/>Text ‚Üí integers]
 M10 --> M11[11. Embeddings<br/>Integers ‚Üí vectors]
 M11 --> M12[12. Attention<br/>Context-aware representations]
 M12 --> M13[13. Transformers<br/>Complete architecture]

 M13 --> LLM[ Language Models<br/>Transformers generate text]

 style F fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
 style M09 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px
 style M10 fill:#e1bee7,stroke:#6a1b9a,stroke-width:3px
 style M11 fill:#e1bee7,stroke:#6a1b9a,stroke-width:3px
 style M12 fill:#ce93d8,stroke:#4a148c,stroke-width:3px
 style M13 fill:#ba68c8,stroke:#4a148c,stroke-width:4px
 style VISION fill:#fef3c7,stroke:#f59e0b,stroke-width:3px
 style LLM fill:#fef3c7,stroke:#f59e0b,stroke-width:3px
```


## Why This Order?

The Architecture tier branches into two parallel tracks‚Äî**Vision** and **Language**‚Äîbecause these domains have fundamentally different data structures and operations. But both follow the same principle: **build components in the order they compose**.

### Vision Track: Spatial Processing (09)

**Convolutions (09)** stands alone because CNNs have a relatively simple pipeline:
- Images come in, convolutions extract spatial features, pooling reduces dimensions
- One module gives you everything needed for computer vision

### Language Track: Sequential Processing (10-13)

**Tokenization (10) ‚Üí Embeddings (11) ‚Üí Attention (12) ‚Üí Transformers (13)**

Language requires more infrastructure, and the order is non-negotiable:
1. **Tokenization** converts text to integers‚Äîyou can't process raw strings
2. **Embeddings** convert integers to vectors‚Äîattention needs continuous representations
3. **Attention** computes context-aware representations‚Äîthe core transformer operation
4. **Transformers** compose attention with MLPs and normalization‚Äîthe complete architecture

Each step transforms the data representation:
```
"hello" ‚Üí [72, 101, 108, 108, 111] ‚Üí [[0.1, 0.3, ...], [...]] ‚Üí attention ‚Üí output
  text       token IDs                  embeddings           transformer
```

### Why Not Merge Them?

Vision and language students have different goals. A computer vision engineer building image classifiers doesn't need tokenization; an NLP engineer building chatbots doesn't need convolutions. Parallel tracks let students focus on their domain while building on shared foundations.


## Module Details

### 09. Convolutions - Convolutional Neural Networks

**What it is**: Conv2d (convolutional layers) and pooling operations for processing images.

**Why it matters**: CNNs revolutionized computer vision by exploiting spatial structure. Understanding convolutions, kernels, and pooling is essential for image processing and beyond.

**What you'll build**: Conv2d, MaxPool2d, and related operations with proper gradient computation.

**Systems focus**: Spatial operations, memory layout (channels), computational intensity

**Historical impact**: This module enables **Milestone 04 (1998 CNN Revolution)** - achieving 75%+ accuracy on CIFAR-10 with YOUR implementations.


### 10. Tokenization - From Text to Numbers

**What it is**: Converting text into integer sequences that neural networks can process.

**Why it matters**: Neural networks operate on numbers, not text. Tokenization is the bridge between human language and machine learning‚Äîunderstanding vocabulary, encoding, and decoding is fundamental.

**What you'll build**: Character-level and subword tokenizers with vocabulary management and encoding/decoding.

**Systems focus**: Vocabulary management, encoding schemes, out-of-vocabulary handling


### 11. Embeddings - Learning Representations

**What it is**: Learned mappings from discrete tokens (words, characters) to continuous vectors.

**Why it matters**: Embeddings transform sparse, discrete representations into dense, semantic vectors. Understanding embeddings is crucial for NLP, recommendation systems, and any domain with categorical data.

**What you'll build**: Embedding layers with proper initialization and gradient computation.

**Systems focus**: Lookup tables, gradient backpropagation through indices, initialization


### 12. Attention - Context-Aware Representations

**What it is**: Self-attention mechanisms that let each token attend to all other tokens in a sequence.

**Why it matters**: Attention is the breakthrough that enabled modern LLMs. It allows models to capture long-range dependencies and contextual relationships that RNNs struggled with.

**What you'll build**: Scaled dot-product attention, multi-head attention, and causal masking for autoregressive generation.

**Systems focus**: O(n¬≤) memory/compute, masking strategies, numerical stability


### 13. Transformers - The Modern Architecture

**What it is**: Complete transformer architecture combining embeddings, attention, and feedforward layers.

**Why it matters**: Transformers power GPT, BERT, and virtually all modern LLMs. Understanding their architecture‚Äîpositional encodings, layer normalization, residual connections‚Äîis essential for AI engineering.

**What you'll build**: A complete decoder-only transformer (GPT-style) for autoregressive text generation.

**Systems focus**: Layer composition, residual connections, generation loop

**Historical impact**: This module enables **Milestone 05 (2017 Transformer Era)** - generating coherent text with YOUR attention implementation.


## What You Can Build After This Tier

```{mermaid}
:align: center
:caption: "**Architecture Tier Milestones.** After completing modules 09-13, you unlock computer vision (1998 CNN) and language understanding (2017 Transformer) breakthroughs."
timeline
 title Historical Achievements Unlocked
 1998 : CNN Revolution : 75%+ accuracy on CIFAR-10 with spatial intelligence
 2017 : Transformer Era : Text generation with attention mechanisms
```

After completing the Architecture tier, you'll be able to:

- **Milestone 04 (1998)**: Build CNNs that achieve 75%+ accuracy on CIFAR-10 (color images)
- **Milestone 05 (2017)**: Implement transformers that generate coherent text responses
- Train on real datasets (MNIST, CIFAR-10, text corpora)
- Understand why modern architectures (ResNets, Vision Transformers, LLMs) work


## Prerequisites

**Required**:
- ** Foundation Tier** (Modules 01-08) completed
- Understanding of tensors, data loaders, autograd, and training loops
- Basic understanding of images (height, width, channels)
- Basic understanding of text/language concepts

**Helpful but not required**:
- Computer vision concepts (convolution, feature maps)
- NLP concepts (tokens, vocabulary, sequence modeling)


## Time Commitment

**Per module**: 4-6 hours (implementation + exercises + datasets)

**Total tier**: ~30-40 hours for complete mastery

**Recommended pace**: 1 module per week (2 modules/week for intensive study)


## Learning Approach

Each module follows the **Build ‚Üí Use ‚Üí Reflect** cycle with **real datasets**:

1. **Build**: Implement the architecture component (Conv2d, attention, transformers)
2. **Use**: Train on real data (CIFAR-10 images, text corpora)
3. **Reflect**: Analyze systems trade-offs (memory vs accuracy, speed vs quality)


## Key Achievements

### Milestone 04: CNN Revolution (1998)

**After Module 09**, you'll recreate Yann LeCun's breakthrough:

```bash
cd milestones/04_1998_cnn
python 02_lecun_cifar10.py # 75%+ accuracy on CIFAR-10
```

**What makes this special**: You're not just importing `torch.nn.Conv2d`‚Äîyou built the entire convolutional architecture from scratch.

### Milestone 05: Transformer Era (2017)

**After Module 13**, you'll implement the attention revolution:

```bash
cd milestones/05_2017_transformer
python 01_vaswani_generation.py # Text generation with YOUR transformer
```

**What makes this special**: Your attention implementation powers the same architecture behind GPT, ChatGPT, and modern LLMs.


## Two Parallel Tracks

The Architecture tier splits into two parallel paths that can be learned in any order:

**Vision Track (Module 09)**:
- Convolutions (Conv2d + Pooling)
- Enables computer vision applications
- Culminates in CNN milestone

**Language Track (Modules 10-13)**:
- Tokenization ‚Üí Embeddings ‚Üí Attention ‚Üí Transformers
- Enables natural language processing
- Culminates in Transformer milestone

**Recommendation**: Complete both tracks in order (09‚Üí10‚Üí11‚Üí12‚Üí13), but you can prioritize the track that interests you more.


## Next Steps

**Ready to build modern architectures?**

```bash
# Start the Architecture tier with vision
tito module start 09_convolutions

# Or jump to language models
tito module start 10_tokenization
```

**Or explore other tiers:**

- **[ Foundation Tier](foundation)** (Modules 01-08): Mathematical foundations
- **[ Optimization Tier](optimization)** (Modules 14-19): Production-ready performance
- **[ Torch Olympics](olympics)** (Module 20): Compete in ML systems challenges


**[‚Üê Back to Home](../intro)** ‚Ä¢ **[Milestone System](../tito/milestones)**


## Links discovered
- [Foundation Tier](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/site/tiers/foundation.md)
- [Optimization Tier](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/site/tiers/optimization.md)
- [Torch Olympics](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/site/tiers/olympics.md)
- [‚Üê Back to Home](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/site/intro.md)
- [Milestone System](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/site/tito/milestones.md)

--- book/tools/scripts/cross_refs/concept_xrefs_generator.py ---
#!/usr/bin/env python3
"""
Concept-Driven Cross-Reference Generator
========================================

Generates intelligent cross-references between textbook chapters using structured concept maps.
Creates *_xrefs.json files for each chapter that can be consumed by Quarto Lua filters.

Unlike text similarity approaches, this uses the structured concept maps (YAML files) to find:
- Exact concept matches between chapters
- Educational progressions (prerequisites ‚Üí applications)
- Methodological connections (theory ‚Üí implementation)
- Hierarchical relationships (primary ‚Üí secondary concepts)

USAGE:
    python3 concept_xrefs_generator.py -d /path/to/chapters/
    python3 concept_xrefs_generator.py -d /path/to/chapters/ --max-refs 3 --min-overlap 2

OUTPUT:
    Creates *_xrefs.json files in each chapter directory:
    - introduction/introduction_xrefs.json
    - ml_systems/ml_systems_xrefs.json
    - training/training_xrefs.json
    etc.

CONCEPT RELATIONSHIPS:
    1. Exact Matches: Same technical terms across chapters
    2. Educational Flow: Prerequisites ‚Üí Advanced topics
    3. Application Links: Theory chapters ‚Üí Implementation chapters
    4. Methodology Connections: Techniques ‚Üí Use cases

REQUIREMENTS:
    pip install pyyaml

The generated JSON files follow this structure:
{
  "cross_references": {
    "sec-training-distributed": [
      {
        "target_chapter": "hw_acceleration",
        "target_section": "sec-gpu-computing",
        "connection_type": "implementation",
        "concepts": ["Distributed Training", "GPU Computing"],
        "explanation": "Distributed training benefits from GPU acceleration",
        "strength": 0.87
      }
    ]
  }
}
"""

import json
import yaml
import argparse
import sys
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple
from dataclasses import dataclass
from collections import defaultdict
import re

@dataclass
class ConceptMap:
    """Represents a chapter's concept map."""
    chapter: str
    file_path: str
    source_file: str
    generated_date: str
    primary_concepts: List[str]
    secondary_concepts: List[str]
    technical_terms: List[str]
    methodologies: List[str]
    applications: List[str]
    keywords: List[str]
    topics_covered: List[Dict]

@dataclass
class CrossReference:
    """Represents a cross-reference between sections."""
    target_chapter: str
    target_section: str
    connection_type: str
    concepts: List[str]
    explanation: str
    strength: float

class ConceptXRefGenerator:
    def __init__(self, chapters_dir: str, max_refs: int = 5, min_overlap: int = 2):
        self.chapters_dir = Path(chapters_dir)
        self.max_refs = max_refs
        self.min_overlap = min_overlap
        self.concept_maps: Dict[str, ConceptMap] = {}
        self.chapter_order = []

    def load_concept_maps(self) -> None:
        """Load all concept maps from chapter directories."""
        print("üîç Loading concept maps...")

        for chapter_dir in sorted(self.chapters_dir.iterdir()):
            if not chapter_dir.is_dir():
                continue

            chapter_name = chapter_dir.name
            concept_file = chapter_dir / f"{chapter_name}_concepts.yml"

            if not concept_file.exists():
                print(f"‚ö†Ô∏è  No concept file found: {concept_file}")
                continue

            try:
                with open(concept_file, 'r', encoding='utf-8') as f:
                    data = yaml.safe_load(f)

                if 'concept_map' not in data:
                    print(f"‚ö†Ô∏è  Invalid concept file format: {concept_file}")
                    continue

                concept_map_data = data['concept_map']

                concept_map = ConceptMap(
                    chapter=chapter_name,
                    file_path=str(concept_file),
                    source_file=concept_map_data.get('source', f'{chapter_name}.qmd'),
                    generated_date=concept_map_data.get('generated_date', ''),
                    primary_concepts=concept_map_data.get('primary_concepts', []),
                    secondary_concepts=concept_map_data.get('secondary_concepts', []),
                    technical_terms=concept_map_data.get('technical_terms', []),
                    methodologies=concept_map_data.get('methodologies', []),
                    applications=concept_map_data.get('applications', []),
                    keywords=concept_map_data.get('keywords', []),
                    topics_covered=concept_map_data.get('topics_covered', [])
                )

                self.concept_maps[chapter_name] = concept_map
                self.chapter_order.append(chapter_name)
                print(f"  ‚úÖ {chapter_name} ({len(concept_map.primary_concepts)} primary concepts)")

            except Exception as e:
                print(f"‚ùå Error loading {concept_file}: {e}")

        print(f"üìö Loaded {len(self.concept_maps)} concept maps")

    def find_concept_overlaps(self, source_chapter: str, target_chapter: str) -> Tuple[List[str], float]:
        """Find overlapping concepts between two chapters with fuzzy matching."""
        source_map = self.concept_maps[source_chapter]
        target_map = self.concept_maps[target_chapter]

        # Collect all concepts from each chapter with normalization
        def normalize_concept(concept: str) -> str:
            """Normalize concepts for better matching."""
            return concept.lower().strip()

        def collect_concepts(concept_map: ConceptMap) -> Set[str]:
            concepts = set()
            concepts.update([normalize_concept(c) for c in concept_map.primary_concepts])
            concepts.update([normalize_concept(c) for c in concept_map.secondary_concepts])
            concepts.update([normalize_concept(c) for c in concept_map.technical_terms])
            concepts.update([normalize_concept(c) for c in concept_map.methodologies])
            concepts.update([normalize_concept(c) for c in concept_map.applications])
            concepts.update([normalize_concept(c) for c in concept_map.keywords])
            return concepts

        source_concepts = collect_concepts(source_map)
        target_concepts = collect_concepts(target_map)

        # Find exact overlaps
        exact_overlaps = source_concepts & target_concepts

        # Find fuzzy overlaps (substring matching)
        fuzzy_overlaps = set()
        for source_concept in source_concepts:
            if source_concept in exact_overlaps:
                continue
            for target_concept in target_concepts:
                if target_concept in exact_overlaps:
                    continue
                # Check for substring matches (both ways)
                if (len(source_concept) > 4 and source_concept in target_concept) or \
                   (len(target_concept) > 4 and target_concept in source_concept):
                    fuzzy_overlaps.add(f"{source_concept}~{target_concept}")

        # Combine overlaps
        all_overlaps = list(exact_overlaps) + [match.split('~')[0] for match in fuzzy_overlaps]

        # Calculate weighted strength
        # Exact matches get full weight, fuzzy matches get 0.7 weight
        exact_weight = len(exact_overlaps)
        fuzzy_weight = len(fuzzy_overlaps) * 0.7
        total_weight = exact_weight + fuzzy_weight

        # Use smaller set for denominator to boost strength
        min_concepts = min(len(source_concepts), len(target_concepts))
        strength = total_weight / min_concepts if min_concepts > 0 else 0.0

        return all_overlaps, strength

    def determine_connection_type(self, source_chapter: str, target_chapter: str, overlaps: List[str]) -> str:
        """Determine the type of connection between chapters."""
        source_map = self.concept_maps[source_chapter]
        target_map = self.concept_maps[target_chapter]

        # Educational progression patterns
        educational_flow = {
            'introduction': 'foundation',
            'dl_primer': 'foundation',
            'ml_systems': 'architecture',
            'dnn_architectures': 'architecture',
            'frameworks': 'tools',
            'training': 'implementation',
            'efficient_ai': 'optimization',
            'optimizations': 'optimization',
            'hw_acceleration': 'implementation',
            'benchmarking': 'evaluation',
            'ops': 'deployment',
            'ondevice_learning': 'specialization',
            'robust_ai': 'specialization',
            'privacy_security': 'specialization',
            'responsible_ai': 'ethics',
            'sustainable_ai': 'ethics',
            'ai_for_good': 'applications',
            'workflow': 'process',
            'emerging_topics': 'frontier',
            'generative_ai': 'frontier',
            'frontiers': 'frontier',
            'conclusion': 'summary'
        }

        source_type = educational_flow.get(source_chapter, 'general')
        target_type = educational_flow.get(target_chapter, 'general')

        # Determine connection type based on chapter types and overlaps
        if source_type == 'foundation' and target_type in ['architecture', 'implementation']:
            return 'prerequisite'
        elif source_type == 'architecture' and target_type == 'implementation':
            return 'implementation'
        elif source_type == 'implementation' and target_type == 'optimization':
            return 'enhancement'
        elif source_type in ['foundation', 'architecture'] and target_type == 'applications':
            return 'application'
        elif any(concept in source_map.methodologies for concept in overlaps):
            return 'methodological'
        elif any(concept in source_map.technical_terms for concept in overlaps):
            return 'technical'
        elif any(concept in source_map.applications for concept in overlaps):
            return 'practical'
        else:
            return 'conceptual'

    def generate_explanation(self, source_chapter: str, target_chapter: str,
                           connection_type: str, overlaps: List[str]) -> str:
        """Generate explanation for the cross-reference."""
        overlap_text = ", ".join(overlaps[:3])
        if len(overlaps) > 3:
            overlap_text += f" and {len(overlaps) - 3} more"

        explanations = {
            'prerequisite': f"Builds on foundational concepts: {overlap_text}",
            'implementation': f"Shows practical implementation of: {overlap_text}",
            'enhancement': f"Optimizes and enhances: {overlap_text}",
            'application': f"Demonstrates real-world applications of: {overlap_text}",
            'methodological': f"Shares methodological approaches: {overlap_text}",
            'technical': f"Uses related technical concepts: {overlap_text}",
            'practical': f"Provides practical examples of: {overlap_text}",
            'conceptual': f"Explores related concepts: {overlap_text}"
        }

        return explanations.get(connection_type, f"Related through: {overlap_text}")

    def get_section_mapping(self, chapter: str) -> Dict[str, str]:
        """Get section ID mapping for a chapter (simplified for now)."""
        # In a full implementation, this would parse the .qmd file to extract actual section IDs
        # For now, we'll use common patterns based on chapter topics
        section_patterns = {
            'introduction': ['sec-introduction-ai-pervasiveness', 'sec-introduction-ai-evolution'],
            'ml_systems': ['sec-ml-systems-overview', 'sec-ml-systems-cloud', 'sec-ml-systems-edge'],
            'training': ['sec-training-distributed', 'sec-training-optimization'],
            'hw_acceleration': ['sec-hw-acceleration-gpu-computing', 'sec-hw-acceleration-tpu'],
            'frameworks': ['sec-frameworks-tensorflow', 'sec-frameworks-pytorch'],
            # Add more as needed
        }

        sections = section_patterns.get(chapter, [f'sec-{chapter}-overview'])
        return {section: section for section in sections}

    def generate_cross_references(self) -> Dict[str, Dict]:
        """Generate cross-references for all chapters."""
        print("\nüîó Generating cross-references...")

        all_xrefs = {}

        for source_chapter in self.chapter_order:
            chapter_xrefs = {}
            source_sections = self.get_section_mapping(source_chapter)

            print(f"  üìù Processing {source_chapter}...")

            # Find connections to other chapters
            connections = []

            for target_chapter in self.chapter_order:
                if source_chapter == target_chapter:
                    continue

                overlaps, strength = self.find_concept_overlaps(source_chapter, target_chapter)

                if len(overlaps) >= self.min_overlap and strength > 0.05:
                    connection_type = self.determine_connection_type(source_chapter, target_chapter, overlaps)
                    explanation = self.generate_explanation(source_chapter, target_chapter, connection_type, overlaps)
                    target_sections = self.get_section_mapping(target_chapter)

                    # Create cross-reference
                    xref = CrossReference(
                        target_chapter=target_chapter,
                        target_section=list(target_sections.keys())[0],  # Use first section for now
                        connection_type=connection_type,
                        concepts=overlaps[:5],  # Limit to top 5 concepts
                        explanation=explanation,
                        strength=strength
                    )

                    connections.append((xref, strength))

            # Sort by strength and take top connections
            connections.sort(key=lambda x: x[1], reverse=True)
            top_connections = connections[:self.max_refs]

            # Group by source sections (for now, assign to first section)
            if source_sections and top_connections:
                first_section = list(source_sections.keys())[0]
                chapter_xrefs[first_section] = []

                for xref, _ in top_connections:
                    chapter_xrefs[first_section].append({
                        'target_chapter': xref.target_chapter,
                        'target_section': xref.target_section,
                        'connection_type': xref.connection_type,
                        'concepts': xref.concepts,
                        'explanation': xref.explanation,
                        'strength': round(xref.strength, 3)
                    })

            all_xrefs[source_chapter] = {
                'cross_references': chapter_xrefs,
                'generated_date': '2025-01-12',
                'generator': 'concept_xrefs_generator.py',
                'total_connections': len([xref for xrefs in chapter_xrefs.values() for xref in xrefs])
            }

            print(f"    ‚úÖ Generated {len([xref for xrefs in chapter_xrefs.values() for xref in xrefs])} connections")

        return all_xrefs

    def save_xref_files(self, all_xrefs: Dict[str, Dict]) -> None:
        """Save cross-reference files for each chapter."""
        print("\nüíæ Saving cross-reference files...")

        for chapter, xrefs in all_xrefs.items():
            chapter_dir = self.chapters_dir / chapter
            xref_file = chapter_dir / f"{chapter}_xrefs.json"

            try:
                with open(xref_file, 'w', encoding='utf-8') as f:
                    json.dump(xrefs, f, indent=2, ensure_ascii=False)
                print(f"  ‚úÖ {xref_file}")

            except Exception as e:
                print(f"  ‚ùå Failed to save {xref_file}: {e}")

    def run(self) -> None:
        """Run the complete cross-reference generation process."""
        print("üöÄ Concept-Driven Cross-Reference Generator")
        print("=" * 50)

        self.load_concept_maps()
        all_xrefs = self.generate_cross_references()
        self.save_xref_files(all_xrefs)

        total_connections = sum(xref['total_connections'] for xref in all_xrefs.values())
        print(f"\nüéâ Generated {total_connections} cross-references across {len(all_xrefs)} chapters")
        print("‚úÖ Cross-reference generation complete!")

def main():
    parser = argparse.ArgumentParser(
        description="Generate concept-driven cross-references for ML Systems textbook",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )

    parser.add_argument(
        '-d', '--chapters-dir',
        required=True,
        help='Directory containing chapter folders with concept maps'
    )

    parser.add_argument(
        '--max-refs',
        type=int,
        default=5,
        help='Maximum cross-references per chapter (default: 5)'
    )

    parser.add_argument(
        '--min-overlap',
        type=int,
        default=2,
        help='Minimum concept overlap to create connection (default: 2)'
    )

    args = parser.parse_args()

    if not Path(args.chapters_dir).exists():
        print(f"‚ùå Chapters directory does not exist: {args.chapters_dir}")
        sys.exit(1)

    generator = ConceptXRefGenerator(
        args.chapters_dir,
        max_refs=args.max_refs,
        min_overlap=args.min_overlap
    )

    generator.run()

if __name__ == "__main__":
    main()


--- book/tools/scripts/content/extract_concepts.py ---
#!/usr/bin/env python3
"""
extract_concepts.py

Extracts key concepts and topics from .qmd chapters by analyzing:
1. Section headers
2. Bold terms (**term**)
3. Footnote definitions ([^fn-name])
4. Terms in definition blocks
5. Figure and table captions

This helps build an accurate knowledge map of what each chapter actually covers.

Usage:
    python extract_concepts.py -f path/to/chapter.qmd
    python extract_concepts.py -d path/to/core/
"""

import os
import re
import argparse
from pathlib import Path
from collections import defaultdict

def extract_concepts_from_file(file_path):
    """
    Extracts key concepts from a .qmd file.

    Returns:
        dict with:
        - headers: list of (level, text) tuples
        - bold_terms: list of bolded terms
        - footnotes: list of footnote names/topics
        - definitions: list of defined terms
        - figures: list of figure topics
    """
    concepts = {
        'headers': [],
        'bold_terms': set(),
        'footnotes': [],
        'definitions': [],
        'figures': [],
        'introduces': set()  # Key introduced concepts
    }

    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
        lines = content.split('\n')

    # Extract headers
    for line in lines:
        match = re.match(r'^(#{1,6})\s+(.*)', line)
        if match:
            level = len(match.group(1))
            text = match.group(2).strip()
            # Remove {#sec-...} labels
            text = re.sub(r'\{#.*?\}', '', text).strip()
            concepts['headers'].append((level, text))

    # Extract bold terms (often definitions)
    bold_pattern = r'\*\*([^*]+)\*\*'
    for match in re.finditer(bold_pattern, content):
        term = match.group(1).strip()
        if len(term) > 2 and not term.startswith('Note'):
            concepts['bold_terms'].add(term)

    # Extract footnote definitions
    footnote_pattern = r'\[\^fn-([^\]]+)\]:\s*(.+?)(?=\n\n|\[\^|\Z)'
    for match in re.finditer(footnote_pattern, content, re.DOTALL):
        name = match.group(1)
        definition = match.group(2).strip()[:100]  # First 100 chars
        concepts['footnotes'].append(f"{name}: {definition}")

    # Extract definition blocks (common patterns)
    # Pattern: "X is defined as..." or "X refers to..."
    definition_patterns = [
        r'(\w[\w\s]+?)\s+is defined as',
        r'(\w[\w\s]+?)\s+refers to',
        r'(\w[\w\s]+?)\s+is a (?:type|kind|form) of',
        r'We define\s+(\w[\w\s]+?)\s+as',
    ]

    for pattern in definition_patterns:
        for match in re.finditer(pattern, content, re.IGNORECASE):
            term = match.group(1).strip()
            if len(term) < 50:  # Reasonable length for a term
                concepts['definitions'].append(term)

    # Extract figure captions
    figure_pattern = r'!\[([^\]]+)\]'
    for match in re.finditer(figure_pattern, content):
        caption = match.group(1).strip()
        if caption:
            concepts['figures'].append(caption[:100])

    # Identify key introduced concepts (heuristic)
    # Look for phrases like "introduce", "present", "explore"
    intro_patterns = [
        r'we (?:will |now )?introduce\s+(\w[\w\s,]+)',
        r'introduces?\s+(\w[\w\s,]+)',
        r'explore\s+(\w[\w\s,]+)',
        r'present\s+(\w[\w\s,]+)',
        r'discuss\s+(\w[\w\s,]+)',
    ]

    for pattern in intro_patterns:
        for match in re.finditer(pattern, content[:5000], re.IGNORECASE):  # Check first part
            concepts['introduces'].add(match.group(1).strip())

    return concepts

def process_chapter(file_path):
    """Process a single chapter and return formatted summary."""
    concepts = extract_concepts_from_file(file_path)
    chapter_name = Path(file_path).stem

    summary = []
    summary.append(f"\n### {chapter_name.replace('_', ' ').title()}")

    # Main topics from level 2 headers
    main_topics = [text for level, text in concepts['headers'] if level == 2 and not text.startswith('Purpose')]
    if main_topics:
        summary.append("**Main Topics:**")
        for topic in main_topics[:10]:  # Limit to 10
            summary.append(f"- {topic}")

    # Key concepts from bold terms
    key_terms = sorted(list(concepts['bold_terms']))[:15]  # Top 15 terms
    if key_terms:
        summary.append("\n**Key Terms:**")
        summary.append(", ".join(key_terms))

    # Introduced concepts
    if concepts['introduces']:
        summary.append("\n**Introduces:**")
        for concept in sorted(list(concepts['introduces']))[:10]:
            summary.append(f"- {concept}")

    return "\n".join(summary)

def main():
    parser = argparse.ArgumentParser(description="Extract concepts from .qmd files.")
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('-f', '--file', help='Path to a single .qmd file')
    group.add_argument('-d', '--directory', help='Directory containing .qmd files')
    args = parser.parse_args()

    if args.file:
        files = [Path(args.file)]
    else:
        # Get chapters in order
        chapter_order = [
            'introduction', 'ml_systems', 'dl_primer', 'dnn_architectures',
            'workflow', 'data_engineering', 'frameworks', 'training',
            'efficient_ai', 'optimizations', 'hw_acceleration', 'benchmarking',
            'ops', 'ondevice_learning', 'robust_ai', 'privacy_security',
            'responsible_ai', 'sustainable_ai', 'ai_for_good', 'conclusion'
        ]

        files = []
        base_dir = Path(args.directory)
        for chapter in chapter_order:
            chapter_file = base_dir / chapter / f"{chapter}.qmd"
            if chapter_file.exists():
                files.append(chapter_file)

    print("# Knowledge Map v2 - Extracted from Actual Content\n")

    for i, file_path in enumerate(files, 1):
        print(f"\n## Chapter {i}: {process_chapter(file_path)}")

if __name__ == "__main__":
    main()


--- LICENSE.md ---
Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International

This work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.
To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/


--- tinytorch/CONTRIBUTING.md ---
# Contributing to TinyTorch üî•

Thank you for your interest in contributing to TinyTorch! This educational ML framework is designed to teach systems engineering principles through hands-on implementation.

## üéØ Contributing Philosophy

TinyTorch is an **educational framework** where every contribution should:
- **Enhance learning** - Make concepts clearer for students
- **Maintain pedagogical flow** - Preserve the learning progression
- **Follow systems thinking** - Emphasize memory, performance, and scaling
- **Keep it simple** - Educational clarity over production complexity

## üöÄ Getting Started

### Development Setup

1. **Clone and setup environment**:
   ```bash
   git clone https://github.com/harvard-edge/cs249r_book.git
   cd cs249r_book/tinytorch
   python -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   pip install -r requirements.txt
   pip install -e .
   ```

2. **Verify installation**:
   ```bash
   tito --version       # Check TinyTorch version
   tito system health   # Verify environment
   tito module status   # See module progress
   ```

3. **Read the development guidelines**:
   - `CONTRIBUTING.md` - Development standards (this file)
   - `INSTRUCTOR.md` - Educational context and teaching approach
   - `README.md` - Repository structure and project overview

## üõ†Ô∏è Types of Contributions

### 1. **Module Improvements**
- Fix bugs in educational implementations
- Improve documentation and explanations
- Add better examples or visualizations
- Enhance systems analysis sections

### 2. **Testing & Validation**
- Add test cases for edge conditions
- Improve checkpoint validation
- Enhance integration tests
- Fix failing test cases

### 3. **Documentation**
- Improve module explanations
- Add better ML systems insights
- Create additional examples
- Fix typos and clarity issues

### 4. **Examples & Demos**
- Create new working examples
- Improve existing example performance
- Add visualization and analysis
- Fix broken demonstrations

## üìã Development Process

### **MANDATORY: Follow Git Workflow Standards**

```bash
# 1. Always use virtual environment
source .venv/bin/activate

# 2. Create feature branch (NEVER work on dev/main directly)
git checkout dev
git pull origin dev
git checkout -b feature/your-improvement

# 3. Make changes following standards in CONTRIBUTING.md
# 4. Test thoroughly
pytest tests/
tito module test 01

# 5. Commit with descriptive messages (NO auto-attribution)
git add .
git commit -m "Fix tensor broadcasting bug in Module 02

- Resolve shape mismatch in batch operations
- Add comprehensive test cases
- Update documentation with edge cases"

# 6. Merge to dev when complete
git checkout dev
git merge feature/your-improvement
git branch -d feature/your-improvement
```

### **Critical Policies - NO EXCEPTIONS**
- ‚úÖ Always use virtual environment (`.venv`)
- ‚úÖ Always work on feature branches
- ‚úÖ Always test before committing
- üö® **NEVER add Co-Authored-By or automated attribution**
- üö® **NEVER add "Generated with Claude Code"**
- üö® **Only project owner adds attribution when needed**

## üß™ Testing Requirements

All contributions must pass:

1. **Module Tests** (run tests for a specific module):
   ```bash
   pytest tests/NN_name/             # e.g., pytest tests/01_tensor/
   tito module test NN               # e.g., tito module test 01
   ```

2. **Integration Tests**:
   ```bash
   pytest tests/integration/
   ```

3. **Milestone Verification** (end-to-end examples):
   ```bash
   python milestones/02_1969_xor/02_xor_solved.py
   python milestones/04_1998_cnn/01_lecun_tinydigits.py
   ```

## üìù Code Standards

### Module Development

**For Students** (using the framework):
- **File Format**: Work in `modules/NN_name/name.ipynb` notebooks in Jupyter Lab
- **Location**: Notebooks are in `modules/NN_name/` directories (e.g., `modules/01_tensor/tensor.ipynb`)
- **Testing**: Run tests inline as you build
- **Export**: Use `tito module complete N` to export to package

**For Contributors** (improving the framework):
- **Source Files**: Edit `src/NN_name/NN_name.py` files (source of truth, e.g., `src/01_tensor/01_tensor.py`)
- **Notebooks**: Generated from source files using `tito src export`
- **Structure**: Follow the standardized module structure
- **Testing**: Include immediate testing after each implementation
- **Systems Analysis**: MANDATORY memory and performance analysis
- **Documentation**: Clear explanations for educational value

### Code Quality
- **Clean Code**: Readable, well-commented implementations
- **Educational Focus**: Prioritize clarity over optimization
- **Error Handling**: Helpful error messages for students
- **Type Hints**: Where they enhance understanding

## üéì Educational Guidelines

### What Makes a Good Contribution

‚úÖ **Good Examples**:
- Fixes a bug that confuses students
- Adds memory profiling to show systems concepts
- Improves explanation of complex ML concepts
- Creates working example that achieves good performance

‚ùå **Avoid These**:
- Overly complex optimizations that obscure learning
- Breaking changes that disrupt module progression
- Adding dependencies that complicate setup
- Removing educational scaffolding

### Systems Focus
Every contribution should emphasize:
- **Memory usage** and optimization
- **Computational complexity** analysis
- **Performance characteristics**
- **Scaling behavior** and bottlenecks
- **Production implications**

## üêõ Bug Reports

When reporting bugs, include:

1. **Version**: Run `tito --version` to get TinyTorch version
2. **Environment**: OS, Python version, virtual environment status
3. **Module**: Which module/checkpoint is affected
4. **Steps to Reproduce**: Exact commands and inputs
5. **Expected vs Actual**: What should happen vs what happens
6. **Error Messages**: Full stack traces if applicable
7. **Testing**: Did you run the module tests?

```bash
# Always include this information
tito --version
python --version
echo $VIRTUAL_ENV
tito system health
```

## üåü Feature Requests

For new features, please:

1. **Check existing issues** - Avoid duplicates
2. **Explain educational value** - How does this help students learn?
3. **Consider module progression** - Where does this fit?
4. **Propose implementation** - High-level approach
5. **Systems implications** - Memory, performance, scaling considerations

## üí¨ Communication

- **Issues**: Use GitHub Issues for bugs and feature requests
- **Discussions**: GitHub Discussions for questions and ideas
- **Documentation**: Check `README.md` for project structure and guides
- **Development**: Follow `CONTRIBUTING.md` for complete standards

## üèÜ Recognition

Contributors who follow these guidelines and make valuable educational improvements will be acknowledged in:
- Module documentation where appropriate
- Release notes for significant contributions
- Course materials when contributions enhance learning

## üè∑Ô∏è Releases (Maintainers Only)

TinyTorch follows [semantic versioning](https://semver.org/):

| Release Type | Version Change | When to Use |
|--------------|----------------|-------------|
| **patch** | 0.1.0 ‚Üí 0.1.1 | Bug fixes, typos, small updates |
| **minor** | 0.1.x ‚Üí 0.2.0 | New features, module improvements |
| **major** | 0.x.x ‚Üí 1.0.0 | Breaking changes, stable API |

### Release Process

Releases are created via the `tinytorch-publish-live.yml` GitHub Actions workflow:

1. Maintainer triggers workflow from GitHub Actions
2. Select release type (patch/minor/major)
3. Enter release description
4. Workflow automatically:
   - Bumps version in code
   - Runs tests and preflight checks
   - Merges dev ‚Üí main
   - Deploys to tinytorch.org
   - Creates git tag (e.g., v0.1.1)
   - Creates GitHub Release with notes
   - Publishes to PyPI

### For Contributors

**You don't need to bump versions.** Maintainers handle versioning during the release process. Just focus on:
- Writing good code
- Following the contribution guidelines
- Using conventional commit messages (`fix:`, `feat:`, `docs:`)

Your commits will be included in the next release with appropriate version bump.

## üìö Resources

### Essential Reading
- **`CONTRIBUTING.md`** - Development standards and workflow (this file)
- **`INSTRUCTOR.md`** - Educational context and teaching approach
- **`README.md`** - Repository structure and project overview

### Quick References
- **Module Structure**: See any `src/NN_name/` directory (e.g., `src/01_tensor/`)
- **Testing Patterns**: Check `tests/NN_name/` directories (e.g., `tests/01_tensor/`)
- **Example Code**: Look at `milestones/` for end-to-end working examples

---

## üèÜ Contributor Recognition

We use [All Contributors](https://allcontributors.org) to recognize everyone who helps improve TinyTorch.

### How to Recognize a Contributor

After merging a PR or resolving an issue, comment:

```
@all-contributors please add @username for TYPE
```

### Contribution Types

| Type | Emoji | Use For |
|------|-------|---------|
| `bug` | üêõ | Found a bug or issue |
| `code` | üíª | Submitted code |
| `doc` | üìñ | Improved documentation |
| `ideas` | üí° | Suggested improvements |
| `test` | üß™ | Added tests |
| `review` | üëÄ | Reviewed PRs |

### Examples

```
@all-contributors please add @AmirAlasady for bug
@all-contributors please add @student123 for code, doc
```

---

**Remember**: TinyTorch is about teaching students to understand ML systems by building them. Every contribution should enhance that educational mission! üéìüî•

**Questions?** Check the docs or open a GitHub Discussion.


## Links discovered
- [semantic versioning](https://semver.org/)
- [All Contributors](https://allcontributors.org)

--- tinytorch/paper/benchmark_quick.py ---
#!/usr/bin/env python3
"""
Quick benchmark for Table 3 - uses reasonable approximations for slow operations
"""

import time
import numpy as np
import torch

def time_op(func, warmup=2, runs=5):
    """Time an operation"""
    for _ in range(warmup):
        func()
    times = []
    for _ in range(runs):
        start = time.perf_counter()
        func()
        times.append(time.perf_counter() - start)
    return np.mean(times)

# 1. MatMul benchmark
print("=== MatMul (1K√ó1K) ===")
a_pt = torch.randn(1000, 1000)
b_pt = torch.randn(1000, 1000)
pt_mm_time = time_op(lambda: torch.mm(a_pt, b_pt))
print(f"PyTorch: {pt_mm_time*1000:.1f} ms")

# Naive triple loop matmul
a_np = a_pt.numpy()
b_np = b_pt.numpy()
def naive_mm_single():
    result = np.zeros((1000, 1000))
    for i in range(1000):
        for j in range(1000):
            result[i, j] = np.dot(a_np[i, :], b_np[:, j])  # Inner loop uses numpy dot
    return result

tt_mm_time = time_op(naive_mm_single, warmup=1, runs=3)
print(f"TinyTorch: {tt_mm_time*1000:.0f} ms")
print(f"Ratio: {tt_mm_time/pt_mm_time:.0f}√ó\n")

# 2. Conv2d benchmark - use tiny batch to estimate
print("=== Conv2d (CIFAR batch - estimated from small run) ===")
batch_full = 128
batch_tiny = 1  # Just 1 image for timing

input_pt = torch.randn(batch_tiny, 3, 32, 32)
conv_pt = torch.nn.Conv2d(3, 32, 5, bias=False)
pt_conv_time_tiny = time_op(lambda: conv_pt(input_pt))
pt_conv_time_full = pt_conv_time_tiny * batch_full  # Linear scaling
print(f"PyTorch (batch={batch_full}): {pt_conv_time_full*1000:.0f} ms")

# Naive conv2d with 7 nested loops
input_np = input_pt.numpy()
weight_np = conv_pt.weight.detach().numpy()

def naive_conv2d():
    B, C_in, H, W = input_np.shape
    C_out, _, K_h, K_w = weight_np.shape
    H_out, W_out = H - K_h + 1, W - K_w + 1
    output = np.zeros((B, C_out, H_out, W_out))

    for b in range(B):
        for c_out in range(C_out):
            for h in range(H_out):
                for w in range(W_out):
                    for c_in in range(C_in):
                        for kh in range(K_h):
                            for kw in range(K_w):
                                output[b, c_out, h, w] += \
                                    input_np[b, c_in, h+kh, w+kw] * \
                                    weight_np[c_out, c_in, kh, kw]
    return output

tt_conv_time_tiny = time_op(naive_conv2d, warmup=0, runs=1)
tt_conv_time_full = tt_conv_time_tiny * batch_full
print(f"TinyTorch (batch={batch_full}): {tt_conv_time_full:.1f} s")
print(f"Ratio: {tt_conv_time_full/pt_conv_time_full:.0f}√ó\n")

# 3. Softmax benchmark - pure Python loops
print("=== Softmax (10K elements) ===")
x_pt = torch.randn(10000)
pt_soft_time = time_op(lambda: torch.nn.functional.softmax(x_pt, dim=0), runs=20)
print(f"PyTorch: {pt_soft_time*1000:.3f} ms")

x_np = x_pt.numpy()
def pure_python_softmax():
    """Pure Python softmax without numpy vectorization"""
    n = len(x_np)
    # Find max
    max_val = x_np[0]
    for i in range(1, n):
        if x_np[i] > max_val:
            max_val = x_np[i]

    # Compute exp and sum
    exp_vals = []
    sum_exp = 0.0
    for i in range(n):
        exp_val = np.exp(x_np[i] - max_val)
        exp_vals.append(exp_val)
        sum_exp += exp_val

    # Normalize
    result = [e / sum_exp for e in exp_vals]
    return result

tt_soft_time = time_op(pure_python_softmax, warmup=1, runs=5)
print(f"TinyTorch: {tt_soft_time*1000:.0f} ms")
print(f"Ratio: {tt_soft_time/pt_soft_time:.0f}√ó\n")

# Generate LaTeX table
print("="*60)
print("LaTeX Table:")
print("="*60)
print(r"\begin{table}[t]")
print(r"\centering")
print(r"\caption{Runtime comparison: TinyTorch vs PyTorch (CPU).}")
print(r"\label{tab:performance}")
print(r"\small")
print(r"\begin{tabular}{@{}lrrr@{}}")
print(r"\toprule")
print(r"Operation & TinyTorch & PyTorch & Ratio \\")
print(r"\midrule")
print(f"\\texttt{{matmul}} (1K$\\times$1K) & {tt_mm_time*1000:.0f} ms & {pt_mm_time*1000:.1f} ms & {tt_mm_time/pt_mm_time:.0f}$\\times$ \\\\")
print(f"\\texttt{{conv2d}} (CIFAR batch) & {tt_conv_time_full:.1f} s & {pt_conv_time_full*1000:.0f} ms & {tt_conv_time_full/pt_conv_time_full:.0f}$\\times$ \\\\")
print(f"\\texttt{{softmax}} (10K elem) & {tt_soft_time*1000:.0f} ms & {pt_soft_time*1000:.2f} ms & {tt_soft_time/pt_soft_time:.0f}$\\times$ \\\\")
print(r"\bottomrule")
print(r"\end{tabular}")
print(r"\end{table}")

print("\n" + "="*60)
print(f"Summary: {tt_mm_time/pt_mm_time:.0f}√ó (matmul), {tt_conv_time_full/pt_conv_time_full:.0f}√ó (conv2d), {tt_soft_time/pt_soft_time:.0f}√ó (softmax)")
print(f"Average slowdown: {np.mean([tt_mm_time/pt_mm_time, tt_conv_time_full/pt_conv_time_full, tt_soft_time/pt_soft_time]):.0f}√ó")


--- tinytorch/src/19_benchmarking/19_benchmarking.py ---
# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.17.1
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# %% [markdown]
"""
# Module 19: Benchmarking - Performance Measurement Infrastructure

Welcome to Module 19! You'll build the benchmarking infrastructure for systematic ML performance evaluation.

**Note on hasattr() Usage:** This module uses hasattr() throughout for duck-typing and polymorphic benchmarking. This is legitimate because benchmarking frameworks must work with ANY model type (PyTorch, TinyTorch, custom) with different method names.

## üîó Prerequisites & Progress
**You've Built**: Complete ML framework with profiling, acceleration, quantization, and compression
**You'll Build**: TorchPerf benchmarking system for fair model comparison and performance evaluation
**You'll Enable**: Systematic optimization combination and competitive performance evaluation

**Connection Map**:
```
Individual Optimizations (M14-18) ‚Üí Benchmarking (M19) ‚Üí Module 20 (Capstone)
(techniques)                        (evaluation)         (application)
```

## üéØ Learning Objectives
By the end of this module, you will:
1. Implement professional benchmarking infrastructure with statistical rigor
2. Learn to combine optimization techniques strategically (order matters!)
3. Build the TorchPerf class - a standardized performance evaluation framework
4. Understand ablation studies and systematic performance evaluation

Let's get started!

## üì¶ Where This Code Lives in the Final Package

**Learning Side:** You work in `modules/19_benchmarking/benchmarking_dev.py`
**Building Side:** Code exports to `tinytorch.perf.benchmarking`

```python
# Final package structure:
from tinytorch.perf.benchmarking import Benchmark, OlympicEvent

# For capstone submission:
benchmark = Benchmark([baseline_model, optimized_model],
                     [{"name": "baseline"}, {"name": "optimized"}])
results = benchmark.run_latency_benchmark()
```

**Why this matters:**
- **Learning:** Complete benchmarking ecosystem in one focused module for rigorous evaluation
- **TorchPerf Olympics:** The Benchmark class provides the standardized framework for capstone submissions
- **Consistency:** All benchmarking operations and reporting in benchmarking.benchmark
- **Integration:** Works seamlessly with optimization modules (M14-18) for complete systems evaluation
"""

# %% nbgrader={"grade": false, "grade_id": "imports", "solution": true}
#| default_exp perf.benchmarking
#| export

# Constants for benchmarking defaults
DEFAULT_WARMUP_RUNS = 5  # Default warmup runs for JIT compilation and cache warming
DEFAULT_MEASUREMENT_RUNS = 10  # Default measurement runs for statistical significance

# %% [markdown]
"""
## üìã Module Dependencies

**Prerequisites**: Modules 01-18 (Complete TinyTorch framework)

**External Dependencies**:
- `numpy` (for numerical operations)
- `time`, `statistics` (for measurements)
- `tracemalloc` (for memory profiling)
- `matplotlib` (optional, for visualization)

**TinyTorch Dependencies**:
- `tinytorch.core.tensor` (Tensor class)
- `tinytorch.core.layers` (Linear layer)
- `tinytorch.perf.profiling` (Profiler from Module 14)

**Dependency Flow**:
```
Profiling (M14) ‚Üí Benchmarking (M19)
       ‚Üì
‚Üí Module 20 (Capstone)
```

Students completing this module will have built professional
benchmarking infrastructure for systematic performance evaluation.
"""

# %% [markdown]
"""
## üèÖ Looking Ahead

The benchmarking tools you build here will be used in Module 20's capstone project, where you'll apply optimization techniques competitively. For now, focus on building reliable, fair measurement infrastructure.
"""

# %% [markdown]
"""
## üí° Introduction: What is Fair Benchmarking?

Benchmarking in ML systems isn't just timing code - it's about making fair, reproducible comparisons that guide real optimization decisions. Think of it like standardized testing: everyone takes the same test under the same conditions.

Consider comparing three models: a base CNN, a quantized version, and a pruned version. Without proper benchmarking, you might conclude the quantized model is "fastest" because you measured it when your CPU was idle, while testing the others during peak system load. Fair benchmarking controls for these variables.

The challenge: ML models have multiple competing objectives (accuracy vs speed vs memory), measurements can be noisy, and "faster" depends on your hardware and use case.

### Benchmarking as a Systems Engineering Discipline

Professional ML benchmarking requires understanding measurement uncertainty and controlling for confounding factors:

**Statistical Foundations**: We need enough measurements to achieve statistical significance. Running a model once tells you nothing about its true performance - you need distributions.

**System Noise Sources**:
- **Thermal throttling**: CPU frequency drops when hot
- **Background processes**: OS interrupts and other applications
- **Memory pressure**: Garbage collection, cache misses
- **Network interference**: For distributed models

**Fair Comparison Requirements**:
- Same hardware configuration
- Same input data distributions
- Same measurement methodology
- Statistical significance testing

This module builds infrastructure that addresses all these challenges while generating actionable insights for optimization decisions.
"""

# %% [markdown]
"""
## üìê Foundations: Statistics for Performance Engineering

Benchmarking is applied statistics. We measure noisy processes (model inference) and need to extract reliable insights about their true performance characteristics.

### Central Limit Theorem in Practice

When you run a model many times, the distribution of measurements approaches normal (regardless of the underlying noise distribution). This lets us:
- Compute confidence intervals for the true mean
- Detect statistically significant differences between models
- Control for measurement variance

```
Single measurement: Meaningless
Few measurements: Unreliable
Many measurements: Statistical confidence
```

### Multi-Objective Optimization Theory

ML systems exist on a **Pareto frontier** - you can't simultaneously maximize accuracy and minimize latency without trade-offs. Good benchmarks reveal this frontier:

```
Accuracy
    ^
    |  A .     <- Model A: High accuracy, high latency
    |
    |    B .  <- Model B: Balanced trade-off
    |
    |      C .<- Model C: Low accuracy, low latency
    |__________> Latency (lower is better)
```

The goal: Find the optimal operating point for your specific constraints.

### Measurement Uncertainty and Error Propagation

Every measurement has uncertainty. When combining metrics (like accuracy per joule), uncertainties compound:

- **Systematic errors**: Consistent bias (timer overhead, warmup effects)
- **Random errors**: Statistical noise (thermal variation, OS scheduling)
- **Propagated errors**: How uncertainty spreads through calculations

Professional benchmarking quantifies and minimizes these uncertainties.
"""

# %%
#| export
import numpy as np
import time
import statistics
import os
import tracemalloc
from typing import Dict, List, Tuple, Any, Optional, Callable, Union
from dataclasses import dataclass, field
from pathlib import Path
import json
import platform
from contextlib import contextmanager
import warnings

from tinytorch.core.tensor import Tensor
from tinytorch.core.layers import Linear

# Optional dependency for visualization only
try:
    import matplotlib.pyplot as plt
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    # Create minimal fallback for when matplotlib is not available
    class plt:
        @staticmethod
        def subplots(*args, **kwargs):
            return None, None
        @staticmethod
        def figure(*args, **kwargs):
            return None
        @staticmethod
        def scatter(*args, **kwargs):
            pass
        @staticmethod
        def annotate(*args, **kwargs):
            pass
        @staticmethod
        def xlabel(*args, **kwargs):
            pass
        @staticmethod
        def ylabel(*args, **kwargs):
            pass
        @staticmethod
        def title(*args, **kwargs):
            pass
        @staticmethod
        def grid(*args, **kwargs):
            pass
        @staticmethod
        def tight_layout(*args, **kwargs):
            pass
        @staticmethod
        def savefig(*args, **kwargs):
            pass
        @staticmethod
        def show(*args, **kwargs):
            pass

# Import Profiler from Module 14 for measurement reuse
from tinytorch.perf.profiling import Profiler

# %%
from enum import Enum

#| export
class OlympicEvent(Enum):
    """
    Performance evaluation event categories for systematic optimization benchmarking.

    Each event optimizes for different objectives with specific constraints,
    enabling structured comparison of optimization strategies.
    """
    LATENCY_SPRINT = "latency_sprint"      # Minimize latency (accuracy >= 85%)
    MEMORY_CHALLENGE = "memory_challenge"   # Minimize memory (accuracy >= 85%)
    ACCURACY_CONTEST = "accuracy_contest"   # Maximize accuracy (latency < 100ms, memory < 10MB)
    ALL_AROUND = "all_around"               # Best balanced score across all metrics
    EXTREME_PUSH = "extreme_push"           # Most aggressive optimization (accuracy >= 80%)

# %% [markdown]
"""
## üèóÔ∏è Implementation: Building Professional Benchmarking Infrastructure

We'll build a comprehensive benchmarking system that handles statistical analysis, multi-dimensional comparison, and automated reporting. Each component builds toward production-quality evaluation tools.

### Benchmark Architecture Overview

```
Benchmark Architecture:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Profiler (Module 14)                    ‚îÇ
‚îÇ ‚Ä¢ Base measurement tools                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ BenchmarkResult                         ‚îÇ
‚îÇ ‚Ä¢ Statistical container for measurements‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Benchmark                               ‚îÇ
‚îÇ ‚Ä¢ Uses Profiler + multi-model comparison‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ BenchmarkSuite                          ‚îÇ
‚îÇ ‚Ä¢ Multi-metric comprehensive evaluation ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ MLPerf                                  ‚îÇ
‚îÇ ‚Ä¢ Standardized industry-style benchmarks‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key Architectural Decision**: The `Benchmark` class reuses `Profiler` from Module 14 for individual model measurements, then adds statistical comparison across multiple models. This demonstrates proper systems architecture - build once, reuse everywhere!

Each level adds capability while maintaining statistical rigor at the foundation.
"""

# %% [markdown]
"""
### BenchmarkResult - Statistical Analysis Container

Before measuring anything, we need a robust container that stores measurements and computes statistical properties. This is the foundation of all our benchmarking.

### Why Statistical Analysis Matters

Single measurements are meaningless in performance engineering. Consider timing a model:
- Run 1: 1.2ms (CPU was idle)
- Run 2: 3.1ms (background process started)
- Run 3: 1.4ms (CPU returned to normal)

Without statistics, which number do you trust? BenchmarkResult solves this by:
- Computing confidence intervals for the true mean
- Detecting outliers and measurement noise
- Providing uncertainty estimates for decision making

### Statistical Properties We Track

```
Raw measurements: [1.2, 3.1, 1.4, 1.3, 1.5, 1.1, 1.6]
                           ‚Üì
        Statistical Analysis
                           ‚Üì
Mean: 1.46ms ¬± 0.25ms (95% confidence interval)
Median: 1.4ms (less sensitive to outliers)
CV: 17% (coefficient of variation - relative noise)
```

The confidence interval tells us: "We're 95% confident the true mean latency is between 1.21ms and 1.71ms." This guides optimization decisions with statistical backing.
"""

# %% nbgrader={"grade": false, "grade_id": "benchmark-dataclass", "solution": true}
#| export

@dataclass
class BenchmarkResult:
    """
    Container for benchmark measurements with statistical analysis.

    TODO: Implement a robust result container that stores measurements and metadata

    APPROACH:
    1. Store raw measurements and computed statistics
    2. Include metadata about test conditions
    3. Provide methods for statistical analysis
    4. Support serialization for result persistence

    EXAMPLE:
    >>> result = BenchmarkResult("model_accuracy", [0.95, 0.94, 0.96])
    >>> print(f"Mean: {result.mean:.3f} ¬± {result.std:.3f}")
    Mean: 0.950 ¬± 0.010

    HINTS:
    - Use statistics module for robust mean/std calculations
    - Store both raw data and summary statistics
    - Include confidence intervals for professional reporting
    """
    ### BEGIN SOLUTION
    metric_name: str
    values: List[float]
    metadata: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        """Compute statistics after initialization."""
        if not self.values:
            raise ValueError(
                f"Empty values list for BenchmarkResult\n"
                f"  ‚ùå Cannot compute statistics: values=[] (0 measurements)\n"
                f"  üí° BenchmarkResult needs data to compute mean, std, percentiles\n"
                f"  üîß Add measurements: BenchmarkResult('{self.metric_name}', [1.2, 1.3, 1.1])"
            )

        self.mean = statistics.mean(self.values)
        self.std = statistics.stdev(self.values) if len(self.values) > 1 else 0.0
        self.median = statistics.median(self.values)
        self.min_val = min(self.values)
        self.max_val = max(self.values)
        self.count = len(self.values)

        # 95% confidence interval for the mean
        if len(self.values) > 1:
            t_score = 1.96  # Approximate for large samples
            margin_error = t_score * (self.std / np.sqrt(self.count))
            self.ci_lower = self.mean - margin_error
            self.ci_upper = self.mean + margin_error
        else:
            self.ci_lower = self.ci_upper = self.mean

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            'metric_name': self.metric_name,
            'values': self.values,
            'mean': self.mean,
            'std': self.std,
            'median': self.median,
            'min': self.min_val,
            'max': self.max_val,
            'count': self.count,
            'ci_lower': self.ci_lower,
            'ci_upper': self.ci_upper,
            'metadata': self.metadata
        }

    def __str__(self) -> str:
        return f"{self.metric_name}: {self.mean:.4f} ¬± {self.std:.4f} (n={self.count})"
    ### END SOLUTION

# %% [markdown]
"""
### üß™ Unit Test: BenchmarkResult

This test validates our BenchmarkResult class correctly computes statistical properties from measurements.

**What we're testing**: Statistical calculations (mean, std, confidence intervals)
**Why it matters**: Reliable statistics are the foundation of fair benchmarking
**Expected**: Correct statistics and proper handling of edge cases
"""

# %% nbgrader={"grade": true, "grade_id": "test-benchmark-result", "locked": true, "points": 10}
def test_unit_benchmark_result():
    """üß™ Test BenchmarkResult statistical calculations."""
    print("üß™ Unit Test: BenchmarkResult...")

    # Test basic statistics
    values = [1.0, 2.0, 3.0, 4.0, 5.0]
    result = BenchmarkResult("test_metric", values)

    assert result.mean == 3.0
    assert abs(result.std - statistics.stdev(values)) < 1e-10
    assert result.median == 3.0
    assert result.min_val == 1.0
    assert result.max_val == 5.0
    assert result.count == 5

    # Test confidence intervals
    assert result.ci_lower < result.mean < result.ci_upper

    # Test serialization
    result_dict = result.to_dict()
    assert result_dict['metric_name'] == "test_metric"
    assert result_dict['mean'] == 3.0

    print("‚úÖ BenchmarkResult works correctly!")

if __name__ == "__main__":
    test_unit_benchmark_result()

# %% [markdown]
"""
## üèóÔ∏è High-Precision Timing Infrastructure

Accurate timing is the foundation of performance benchmarking. System clocks have different precision and behavior, so we need a robust timing mechanism.

### Timing Challenges in Practice

Consider what happens when you time a function:
```
User calls: time.time()
            ‚Üì
Operating System scheduling delays (Œºs to ms)
            ‚Üì
Timer system call overhead (~1Œºs)
            ‚Üì
Hardware clock resolution (ns to Œºs)
            ‚Üì
Your measurement
```

For microsecond-precision timing, each of these can introduce significant error.

### Why perf_counter() Matters

Python's `time.perf_counter()` is specifically designed for interval measurement:
- **Monotonic**: Never goes backwards (unaffected by system clock adjustments)
- **High resolution**: Typically nanosecond precision
- **Low overhead**: Optimized system call

### Timing Best Practices

```
Context Manager Pattern:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  with timer():  ‚îÇ ‚Üê Start timing
‚îÇ    operation()  ‚îÇ ‚Üê Your code runs
‚îÇ  # End timing   ‚îÇ ‚Üê Automatic cleanup
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
elapsed = timer.elapsed
```

This pattern ensures timing starts/stops correctly even if exceptions occur.
"""

# %% nbgrader={"grade": false, "grade_id": "timer-context", "solution": true}
#| export
@contextmanager
def precise_timer():
    """
    High-precision timing context manager for benchmarking.

    TODO: Implement a context manager that provides accurate timing measurements

    APPROACH:
    1. Use time.perf_counter() for high precision
    2. Handle potential interruptions and system noise
    3. Return elapsed time when context exits
    4. Provide warmup capability for JIT compilation

    Yields:
        Timer object with .elapsed attribute (set after context exits)

    EXAMPLE:
    >>> with precise_timer() as timer:
    ...     time.sleep(0.1)  # Some operation
    >>> print(f"Elapsed: {timer.elapsed:.4f}s")
    Elapsed: 0.1001s

    HINTS:
    - perf_counter() is monotonic and high-resolution
    - Store start time in __enter__, compute elapsed in __exit__
    - Handle any exceptions gracefully
    """
    ### BEGIN SOLUTION
    class Timer:
        def __init__(self):
            self.elapsed = 0.0
            self.start_time = None

    timer = Timer()
    timer.start_time = time.perf_counter()

    try:
        yield timer
    finally:
        timer.elapsed = time.perf_counter() - timer.start_time
    ### END SOLUTION

# %% [markdown]
"""
### üß™ Unit Test: Precise Timer

This test validates our timing context manager provides accurate measurements.

**What we're testing**: High-precision timing with perf_counter
**Why it matters**: Accurate timing is essential for reliable benchmarks
**Expected**: Measurements close to actual sleep durations
"""

# %% nbgrader={"grade": true, "grade_id": "test-precise-timer", "locked": true, "points": 5}
def test_unit_precise_timer():
    """üß™ Test precise_timer context manager."""
    print("üß™ Unit Test: precise_timer...")

    # Test basic timing
    with precise_timer() as timer:
        time.sleep(0.01)  # 10ms sleep

    # Should be close to 0.01 seconds (allow some variance)
    assert 0.005 < timer.elapsed < 0.05, f"Expected ~0.01s, got {timer.elapsed}s"

    # Test multiple uses
    times = []
    for _ in range(3):
        with precise_timer() as timer:
            time.sleep(0.001)  # 1ms sleep
        times.append(timer.elapsed)

    # All times should be reasonably close
    assert all(0.0005 < t < 0.01 for t in times)

    print("‚úÖ precise_timer works correctly!")

if __name__ == "__main__":
    test_unit_precise_timer()

# %% [markdown]
"""
### Benchmark Class - Core Measurement Engine

The Benchmark class implements the core measurement logic for different metrics. It handles the complex orchestration of multiple models, datasets, and measurement protocols.

### Benchmark Architecture Overview

```
Benchmark Execution Flow:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Models    ‚îÇ    ‚îÇ   Datasets   ‚îÇ    ‚îÇ Measurement     ‚îÇ
‚îÇ [M1, M2...] ‚îÇ ‚Üí  ‚îÇ [D1, D2...]  ‚îÇ ‚Üí  ‚îÇ Protocol        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                               ‚Üì
                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                           ‚îÇ        Benchmark Loop           ‚îÇ
                           ‚îÇ 1. Warmup runs (JIT, cache)     ‚îÇ
                           ‚îÇ 2. Measurement runs (statistics)‚îÇ
                           ‚îÇ 3. System info capture          ‚îÇ
                           ‚îÇ 4. Result aggregation           ‚îÇ
                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                        ‚Üì
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚îÇ          BenchmarkResult           ‚îÇ
                        ‚îÇ ‚Ä¢ Statistical analysis             ‚îÇ
                        ‚îÇ ‚Ä¢ Confidence intervals             ‚îÇ
                        ‚îÇ ‚Ä¢ Metadata (system, conditions)    ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Why Warmup Runs Matter

Modern systems have multiple layers of adaptation:
- **JIT compilation**: Code gets faster after being run several times
- **CPU frequency scaling**: Processors ramp up under load
- **Cache warming**: Data gets loaded into faster memory
- **Branch prediction**: CPU learns common execution paths

Without warmup, your first few measurements don't represent steady-state performance.

### Multiple Benchmark Types

Different metrics require different measurement strategies:

**Latency Benchmarking**:
- Focus: Time per inference
- Key factors: Input size, model complexity, hardware utilization
- Measurement: High-precision timing of forward pass

**Accuracy Benchmarking**:
- Focus: Quality of predictions
- Key factors: Dataset representativeness, evaluation protocol
- Measurement: Correct predictions / total predictions

**Memory Benchmarking**:
- Focus: Peak and average memory usage
- Key factors: Model size, batch size, intermediate activations
- Measurement: Process memory monitoring during inference
"""

# %% [markdown]
"""
### Benchmark.__init__ - Setting Up the Measurement Engine

The Benchmark constructor configures the measurement infrastructure: models to test,
datasets for evaluation, and system metadata for reproducibility. It reuses the
Profiler from Module 14 for individual model measurements.

```
Benchmark Setup:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Models    ‚îÇ     ‚îÇ   Datasets   ‚îÇ     ‚îÇ  Profiler   ‚îÇ
‚îÇ [M1, M2...] ‚îÇ ‚îÄ‚îÄ> ‚îÇ [D1, D2...]  ‚îÇ ‚îÄ‚îÄ> ‚îÇ (Module 14) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚Üì
                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                 ‚îÇ  System Metadata ‚îÇ
                 ‚îÇ ‚Ä¢ platform       ‚îÇ
                 ‚îÇ ‚Ä¢ processor      ‚îÇ
                 ‚îÇ ‚Ä¢ python version ‚îÇ
                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```
"""

# %% nbgrader={"grade": false, "grade_id": "benchmark-init", "solution": true}
#| export
class Benchmark:
    """
    Professional benchmarking system for ML models and operations.

    Provides latency, accuracy, and memory benchmarking with statistical
    rigor. Reuses Profiler from Module 14 for individual measurements
    and adds multi-model comparison with confidence intervals.

    EXAMPLE:
    >>> benchmark = Benchmark(models=[model1, model2], datasets=[test_data])
    >>> results = benchmark.run_accuracy_benchmark()
    """

    def __init__(self, models: List[Any], datasets: List[Any],
                 warmup_runs: int = DEFAULT_WARMUP_RUNS, measurement_runs: int = DEFAULT_MEASUREMENT_RUNS):
        """
        Initialize benchmark with models and datasets.

        TODO: Set up the benchmark runner with models, datasets, and system metadata

        APPROACH:
        1. Store models and datasets for benchmarking
        2. Configure warmup and measurement run counts
        3. Initialize Profiler from Module 14 for measurements
        4. Capture system information for reproducibility

        HINTS:
        - Use platform module for system info
        - os.cpu_count() can return None, use fallback
        """
        ### BEGIN SOLUTION
        self.models = models
        self.datasets = datasets
        self.warmup_runs = warmup_runs
        self.measurement_runs = measurement_runs
        self.results = {}

        # Use Profiler from Module 14 for measurements
        self.profiler = Profiler()

        # System information for metadata (using Python standard library)
        self.system_info = {
            'platform': platform.platform(),
            'processor': platform.processor(),
            'python_version': platform.python_version(),
            'cpu_count': os.cpu_count() or 1,  # os.cpu_count() can return None
        }
        # Note: System total memory not available via standard library
        # Process memory measurement uses tracemalloc (via Profiler)
        ### END SOLUTION

# %% [markdown]
"""
### üß™ Unit Test: Benchmark.__init__

**What we're testing**: Benchmark initialization with models, datasets, and system metadata
**Why it matters**: Proper setup ensures reproducible benchmarking conditions
**Expected**: All attributes initialized, system info captured
"""

# %% nbgrader={"grade": true, "grade_id": "test-benchmark-init", "locked": true, "points": 5}
def test_unit_benchmark_init():
    """üß™ Test Benchmark initialization."""
    print("üß™ Unit Test: Benchmark.__init__...")

    class MockModel:
        def __init__(self, name):
            self.name = name
        def forward(self, x):
            return x

    models = [MockModel("m1"), MockModel("m2")]
    datasets = [{"data": "test"}]

    benchmark = Benchmark(models, datasets, warmup_runs=3, measurement_runs=5)

    assert len(benchmark.models) == 2
    assert len(benchmark.datasets) == 1
    assert benchmark.warmup_runs == 3
    assert benchmark.measurement_runs == 5
    assert isinstance(benchmark.results, dict)
    assert 'platform' in benchmark.system_info
    assert 'processor' in benchmark.system_info
    assert 'python_version' in benchmark.system_info
    assert 'cpu_count' in benchmark.system_info
    assert benchmark.profiler is not None

    print("‚úÖ Benchmark.__init__ works correctly!")

if __name__ == "__main__":
    test_unit_benchmark_init()

# %% [markdown]
"""
### Benchmark.run_latency_benchmark - Measuring Inference Speed

Latency benchmarking measures how long each model takes to process input. We use
the Profiler for warmup, then collect multiple individual measurements for
statistical analysis via BenchmarkResult.

```
Latency Measurement Flow:
Input Tensor ‚îÄ‚îÄ> Warmup Runs (discard) ‚îÄ‚îÄ> Measurement Runs ‚îÄ‚îÄ> BenchmarkResult
                 (JIT, cache warming)      (collect times)      (mean, std, CI)
```
"""

# %% nbgrader={"grade": false, "grade_id": "benchmark-latency", "solution": true}
#| export
    # --- Benchmark.run_latency_benchmark ---
def benchmark_run_latency_benchmark(self, input_shape: Tuple[int, ...] = (1, 28, 28)) -> Dict[str, BenchmarkResult]:
    """
    Benchmark model inference latency using Profiler.

    TODO: Measure inference latency for each model with statistical rigor

    APPROACH:
    1. Create input tensor matching input_shape
    2. Use Profiler for initial warmup measurement
    3. Collect multiple individual latency measurements
    4. Wrap results in BenchmarkResult for statistical analysis

    HINTS:
    - Use self.profiler.measure_latency() for warmup
    - Collect self.measurement_runs individual measurements
    - Include system_info in metadata
    """
    ### BEGIN SOLUTION
    results = {}

    for i, model in enumerate(self.models):
        model_name = getattr(model, 'name', f'model_{i}')

        # Create input tensor for profiling
        from tinytorch.core.tensor import Tensor
        input_tensor = Tensor(np.random.randn(*input_shape).astype(np.float32))

        # Use Profiler to measure latency with proper warmup and iterations
        latency_ms = self.profiler.measure_latency(
            model,
            input_tensor,
            warmup=self.warmup_runs,
            iterations=self.measurement_runs
        )

        # Profiler returns single median value
        # For BenchmarkResult, we need multiple measurements
        # Run additional measurements for statistical analysis
        latencies = []
        for _ in range(self.measurement_runs):
            single_latency = self.profiler.measure_latency(
                model, input_tensor, warmup=0, iterations=1
            )
            latencies.append(single_latency)

        results[model_name] = BenchmarkResult(
            f"{model_name}_latency_ms",
            latencies,
            metadata={'input_shape': input_shape, **self.system_info}
        )

    return results
    ### END SOLUTION

Benchmark.run_latency_benchmark = benchmark_run_latency_benchmark

# %% [markdown]
"""
### üß™ Unit Test: Benchmark.run_latency_benchmark

**What we're testing**: Latency measurement across multiple models
**Why it matters**: Accurate latency data guides deployment decisions
**Expected**: BenchmarkResult for each model with positive latency values
"""

# %% nbgrader={"grade": true, "grade_id": "test-benchmark-latency", "locked": true, "points": 10}
def test_unit_benchmark_latency():
    """üß™ Test Benchmark latency measurement."""
    print("üß™ Unit Test: Benchmark.run_latency_benchmark...")

    class MockModel:
        def __init__(self, name):
            self.name = name
        def forward(self, x):
            time.sleep(0.001)
            return x

    models = [MockModel("fast"), MockModel("slow")]
    benchmark = Benchmark(models, [{"data": "test"}], warmup_runs=1, measurement_runs=3)

    results = benchmark.run_latency_benchmark()
    assert len(results) == 2
    assert "fast" in results
    assert "slow" in results
    assert all(isinstance(r, BenchmarkResult) for r in results.values())
    assert all(r.mean > 0 for r in results.values())

    print("‚úÖ Benchmark.run_latency_benchmark works correctly!")

if __name__ == "__main__":
    test_unit_benchmark_latency()

# %% [markdown]
"""
### Benchmark.run_accuracy_benchmark - Measuring Prediction Quality

Accuracy benchmarking evaluates model correctness across datasets. Models with
an `evaluate` method are tested directly; otherwise, accuracy is simulated for
demonstration purposes.

```
Accuracy Measurement:
Model ‚îÄ‚îÄ> Dataset 1 ‚îÄ‚îÄ> accuracy_1 ‚îÄ‚îÄ‚îê
      ‚îÄ‚îÄ> Dataset 2 ‚îÄ‚îÄ> accuracy_2 ‚îÄ‚îÄ‚îº‚îÄ‚îÄ> BenchmarkResult
      ‚îÄ‚îÄ> Dataset N ‚îÄ‚îÄ> accuracy_N ‚îÄ‚îÄ‚îò    (mean, std across datasets)
```
"""

# %% nbgrader={"grade": false, "grade_id": "benchmark-accuracy", "solution": true}
#| export
    # --- Benchmark.run_accuracy_benchmark ---
def benchmark_run_accuracy_benchmark(self) -> Dict[str, BenchmarkResult]:
    """
    Benchmark model accuracy across datasets.

    TODO: Evaluate each model on each dataset and collect accuracy scores

    APPROACH:
    1. Iterate over all models and datasets
    2. Use model.evaluate() if available, otherwise simulate
    3. Clamp accuracy to [0, 1] range
    4. Wrap results in BenchmarkResult

    HINTS:
    - Use hasattr(model, 'evaluate') for duck-typing
    - Different models get different base accuracies for simulation
    """
    ### BEGIN SOLUTION
    results = {}

    for i, model in enumerate(self.models):
        model_name = getattr(model, 'name', f'model_{i}')
        accuracies = []

        for dataset in self.datasets:
            # Simulate accuracy measurement
            # In practice, this would evaluate the model on the dataset
            try:
                if hasattr(model, 'evaluate'):
                    accuracy = model.evaluate(dataset)
                else:
                    # Simulate accuracy for demonstration
                    base_accuracy = 0.85 + i * 0.05  # Different models have different base accuracies
                    accuracy = base_accuracy + np.random.normal(0, 0.02)  # Add noise
                    accuracy = max(0.0, min(1.0, accuracy))  # Clamp to [0, 1]
            except Exception:
                # Fallback simulation
                accuracy = 0.80 + np.random.normal(0, 0.05)
                accuracy = max(0.0, min(1.0, accuracy))

            accuracies.append(accuracy)

        results[model_name] = BenchmarkResult(
            f"{model_name}_accuracy",
            accuracies,
            metadata={'num_datasets': len(self.datasets), **self.system_info}
        )

    return results
    ### END SOLUTION

Benchmark.run_accuracy_benchmark = benchmark_run_accuracy_benchmark

# %% [markdown]
"""
### üß™ Unit Test: Benchmark.run_accuracy_benchmark

**What we're testing**: Accuracy evaluation across models and datasets
**Why it matters**: Accuracy is the primary quality metric for ML models
**Expected**: Accuracy values in [0, 1] range for each model
"""

# %% nbgrader={"grade": true, "grade_id": "test-benchmark-accuracy", "locked": true, "points": 10}
def test_unit_benchmark_accuracy():
    """üß™ Test Benchmark accuracy measurement."""
    print("üß™ Unit Test: Benchmark.run_accuracy_benchmark...")

    class MockModel:
        def __init__(self, name):
            self.name = name
        def forward(self, x):
            return x

    models = [MockModel("model_a"), MockModel("model_b")]
    datasets = [{"d": "1"}, {"d": "2"}]
    benchmark = Benchmark(models, datasets, warmup_runs=1, measurement_runs=3)

    results = benchmark.run_accuracy_benchmark()
    assert len(results) == 2
    assert all(isinstance(r, BenchmarkResult) for r in results.values())
    assert all(0 <= r.mean <= 1 for r in results.values())

    print("‚úÖ Benchmark.run_accuracy_benchmark works correctly!")

if __name__ == "__main__":
    test_unit_benchmark_accuracy()

# %% [markdown]
"""
### Benchmark.run_memory_benchmark - Measuring Resource Consumption

Memory benchmarking tracks how much RAM each model consumes during inference.
We use the Profiler's memory measurement, falling back to parameter-count
estimation when tracemalloc reports minimal usage.

```
Memory Measurement:
Model ‚îÄ‚îÄ> Profiler.measure_memory() ‚îÄ‚îÄ> peak_memory_mb
                                         ‚Üì
                            If < 1.0 MB detected:
                            count_parameters() * 4 bytes
                                         ‚Üì
                                  BenchmarkResult
```
"""

# %% nbgrader={"grade": false, "grade_id": "benchmark-memory", "solution": true}
#| export
    # --- Benchmark.run_memory_benchmark ---
def benchmark_run_memory_benchmark(self, input_shape: Tuple[int, ...] = (1, 28, 28)) -> Dict[str, BenchmarkResult]:
    """
    Benchmark model memory usage using Profiler.

    TODO: Measure memory consumption for each model across multiple runs

    APPROACH:
    1. Use self.profiler.measure_memory() for each model
    2. Fall back to parameter-count estimation if tracemalloc reports < 1 MB
    3. Collect self.measurement_runs samples
    4. Wrap results in BenchmarkResult

    HINTS:
    - memory_stats['peak_memory_mb'] is the primary metric
    - Estimate memory as param_count * 4 / (1024**2) for float32
    """
    ### BEGIN SOLUTION
    results = {}

    for i, model in enumerate(self.models):
        model_name = getattr(model, 'name', f'model_{i}')
        memory_usages = []

        for run in range(self.measurement_runs):
            # Use Profiler to measure memory
            memory_stats = self.profiler.measure_memory(model, input_shape)
            # Use peak_memory_mb as the primary metric
            memory_used = memory_stats['peak_memory_mb']

            # If no significant memory change detected, estimate from parameters
            if memory_used < 1.0:
                param_count = self.profiler.count_parameters(model)
                memory_used = param_count * 4 / (1024**2)  # 4 bytes per float32

            memory_usages.append(max(0, memory_used))

        results[model_name] = BenchmarkResult(
            f"{model_name}_memory_mb",
            memory_usages,
            metadata={'input_shape': input_shape, **self.system_info}
        )

    return results
    ### END SOLUTION

Benchmark.run_memory_benchmark = benchmark_run_memory_benchmark

# %% [markdown]
"""
### üß™ Unit Test: Benchmark.run_memory_benchmark

**What we're testing**: Memory usage measurement across multiple models
**Why it matters**: Memory constraints determine deployment feasibility on edge devices
**Expected**: Non-negative memory values for each model
"""

# %% nbgrader={"grade": true, "grade_id": "test-benchmark-memory", "locked": true, "points": 10}
def test_unit_benchmark_memory():
    """üß™ Test Benchmark memory measurement."""
    print("üß™ Unit Test: Benchmark.run_memory_benchmark...")

    class MockModel:
        def __init__(self, name):
            self.name = name
        def forward(self, x):
            return x

    models = [MockModel("small"), MockModel("large")]
    benchmark = Benchmark(models, [{"data": "test"}], warmup_runs=1, measurement_runs=3)

    results = benchmark.run_memory_benchmark()
    assert len(results) == 2
    assert all(isinstance(r, BenchmarkResult) for r in results.values())
    assert all(r.mean >= 0 for r in results.values())

    print("‚úÖ Benchmark.run_memory_benchmark works correctly!")

if __name__ == "__main__":
    test_unit_benchmark_memory()

# %% [markdown]
"""
### Benchmark.compare_models - Cross-Model Comparison

The compare_models method dispatches to the appropriate benchmark type and
formats results into a structured list of dictionaries for easy comparison.
This is the primary interface for multi-model evaluation.
"""

# %% nbgrader={"grade": false, "grade_id": "benchmark-compare", "solution": true}
#| export
    # --- Benchmark.compare_models ---
def benchmark_compare_models(self, metric: str = "latency"):
    """
    Compare models across a specific metric.

    TODO: Dispatch to the appropriate benchmark and format comparison results

    APPROACH:
    1. Select benchmark type based on metric string
    2. Run the selected benchmark
    3. Format results into list of dicts for easy comparison

    HINTS:
    - Support 'latency', 'accuracy', 'memory' metrics
    - Return list of dicts with model, metric, mean, std, ci_lower, ci_upper, count
    """
    ### BEGIN SOLUTION
    if metric == "latency":
        results = self.run_latency_benchmark()
    elif metric == "accuracy":
        results = self.run_accuracy_benchmark()
    elif metric == "memory":
        results = self.run_memory_benchmark()
    else:
        raise ValueError(
            f"Unknown benchmark metric: '{metric}'\n"
            f"  ‚ùå Metric '{metric}' is not supported\n"
            f"  üí° compare_models() supports three metrics: latency (timing), memory (bytes), accuracy (correctness)\n"
            f"  üîß Use: compare_models(metric='latency') or 'memory' or 'accuracy'"
        )

    # Return structured list of dicts for easy comparison
    # (No pandas dependency - students can convert to DataFrame if needed)
    comparison_data = []
    for model_name, result in results.items():
        comparison_data.append({
            'model': model_name.replace(f'_{metric}', '').replace('_ms', '').replace('_mb', ''),
            'metric': metric,
            'mean': result.mean,
            'std': result.std,
            'ci_lower': result.ci_lower,
            'ci_upper': result.ci_upper,
            'count': result.count
        })

    return comparison_data
    ### END SOLUTION

Benchmark.compare_models = benchmark_compare_models

# %% [markdown]
"""
### üß™ Unit Test: Benchmark (Full Class Integration)

This test validates our Benchmark class measures latency, accuracy, and memory correctly,
and that compare_models dispatches properly.

**What we're testing**: Multi-model benchmarking with different metrics
**Why it matters**: Reliable comparisons guide optimization decisions
**Expected**: Consistent results across multiple benchmark types
"""

# %% nbgrader={"grade": true, "grade_id": "test-benchmark", "locked": true, "points": 15}
def test_unit_benchmark():
    """üß™ Test Benchmark class functionality."""
    print("üß™ Unit Test: Benchmark...")

    # Create mock models for testing
    class MockModel:
        def __init__(self, name):
            self.name = name

        def forward(self, x):
            time.sleep(0.001)  # Simulate computation
            return x

    models = [MockModel("fast_model"), MockModel("slow_model")]
    datasets = [{"data": "test1"}, {"data": "test2"}]

    benchmark = Benchmark(models, datasets, warmup_runs=2, measurement_runs=3)

    # Test latency benchmark
    latency_results = benchmark.run_latency_benchmark()
    assert len(latency_results) == 2
    assert "fast_model" in latency_results
    assert all(isinstance(result, BenchmarkResult) for result in latency_results.values())

    # Test accuracy benchmark
    accuracy_results = benchmark.run_accuracy_benchmark()
    assert len(accuracy_results) == 2
    assert all(0 <= result.mean <= 1 for result in accuracy_results.values())

    # Test memory benchmark
    memory_results = benchmark.run_memory_benchmark()
    assert len(memory_results) == 2
    assert all(result.mean >= 0 for result in memory_results.values())

    # Test comparison (returns list of dicts, not DataFrame)
    comparison_data = benchmark.compare_models("latency")
    assert len(comparison_data) == 2
    assert isinstance(comparison_data, list)
    assert all(isinstance(item, dict) for item in comparison_data)
    assert "model" in comparison_data[0]
    assert "mean" in comparison_data[0]

    print("‚úÖ Benchmark works correctly!")

if __name__ == "__main__":
    test_unit_benchmark()

# %% [markdown]
"""
### BenchmarkSuite - Comprehensive Multi-Metric Evaluation

The BenchmarkSuite orchestrates multiple benchmark types and generates comprehensive reports. This is where individual measurements become actionable engineering insights.

### Why Multi-Metric Analysis Matters

Single metrics mislead. Consider these three models:
- **Model A**: 95% accuracy, 100ms latency, 50MB memory
- **Model B**: 90% accuracy, 20ms latency, 10MB memory
- **Model C**: 85% accuracy, 10ms latency, 5MB memory

Which is "best"? It depends on your constraints:
- **Server deployment**: Model A (accuracy matters most)
- **Mobile app**: Model C (memory/latency critical)
- **Edge device**: Model B (balanced trade-off)

### Multi-Dimensional Comparison Workflow

```
BenchmarkSuite Execution Pipeline:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Models     ‚îÇ ‚Üê Input: List of models to compare
‚îÇ [M1,M2,M3]   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Metric Types ‚îÇ ‚Üê Run each benchmark type
‚îÇ ‚Ä¢ Latency    ‚îÇ
‚îÇ ‚Ä¢ Accuracy   ‚îÇ
‚îÇ ‚Ä¢ Memory     ‚îÇ
‚îÇ ‚Ä¢ Energy     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Result       ‚îÇ ‚Üê Aggregate into unified view
‚îÇ Aggregation  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Analysis &   ‚îÇ ‚Üê Generate insights
‚îÇ Reporting    ‚îÇ   ‚Ä¢ Best performer per metric
‚îÇ              ‚îÇ   ‚Ä¢ Trade-off analysis
‚îÇ              ‚îÇ   ‚Ä¢ Use case recommendations
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Pareto Frontier Analysis

The suite automatically identifies Pareto-optimal solutions - models that aren't strictly dominated by others across all metrics. This reveals the true trade-off space for optimization decisions.

### Energy Efficiency Modeling

Since direct energy measurement requires specialized hardware, we estimate energy based on computational complexity and memory usage. This provides actionable insights for battery-powered deployments.
"""

# %% [markdown]
"""
### BenchmarkSuite.__init__ - Setting Up Multi-Metric Evaluation

The BenchmarkSuite constructor creates the evaluation infrastructure, including
a Benchmark instance for measurements and an output directory for reports and plots.
"""

# %% nbgrader={"grade": false, "grade_id": "benchsuite-init", "solution": true}
#| export
class BenchmarkSuite:
    """
    Comprehensive benchmark suite for ML systems evaluation.

    Orchestrates multiple benchmark types (latency, accuracy, memory, energy)
    and generates reports with visualizations and recommendations.

    EXAMPLE:
    >>> suite = BenchmarkSuite(models, datasets)
    >>> report = suite.run_full_benchmark()
    >>> suite.generate_report(report)
    """

    def __init__(self, models: List[Any], datasets: List[Any],
                 output_dir: str = "benchmark_results"):
        """
        Initialize comprehensive benchmark suite.

        TODO: Set up the suite with models, datasets, output directory, and a Benchmark instance

        APPROACH:
        1. Store models and datasets
        2. Create output directory (use Path, mkdir with exist_ok)
        3. Create Benchmark instance for measurements
        4. Initialize empty results dict

        HINTS:
        - Use Path(output_dir) for cross-platform paths
        - The Benchmark instance handles individual model measurements
        """
        ### BEGIN SOLUTION
        self.models = models
        self.datasets = datasets
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

        self.benchmark = Benchmark(models, datasets)
        self.results = {}
        ### END SOLUTION

# %% [markdown]
"""
### üß™ Unit Test: BenchmarkSuite.__init__

**What we're testing**: Suite initialization with output directory and Benchmark instance
**Why it matters**: Proper setup ensures results can be saved and compared
**Expected**: All attributes initialized, output directory created
"""

# %% nbgrader={"grade": true, "grade_id": "test-benchsuite-init", "locked": true, "points": 5}
def test_unit_benchsuite_init():
    """üß™ Test BenchmarkSuite initialization."""
    print("üß™ Unit Test: BenchmarkSuite.__init__...")

    class MockModel:
        def __init__(self, name):
            self.name = name
        def forward(self, x):
            return x

    import tempfile
    with tempfile.TemporaryDirectory() as tmp_dir:
        models = [MockModel("m1")]
        datasets = [{"d": "1"}]
        suite = BenchmarkSuite(models, datasets, output_dir=tmp_dir)

        assert len(suite.models) == 1
        assert len(suite.datasets) == 1
        assert suite.output_dir == Path(tmp_dir)
        assert isinstance(suite.benchmark, Benchmark)
        assert isinstance(suite.results, dict)

    print("‚úÖ BenchmarkSuite.__init__ works correctly!")

if __name__ == "__main__":
    test_unit_benchsuite_init()

# %% [markdown]
"""
### BenchmarkSuite.run_full_benchmark - Orchestrating All Measurements

The run_full_benchmark method runs all four benchmark categories (latency, accuracy,
memory, energy) in sequence, collecting comprehensive results for each model.

```
Run Full Benchmark Pipeline:
Models ‚îÄ‚îÄ> Latency Benchmark ‚îÄ‚îÄ‚îê
       ‚îÄ‚îÄ> Accuracy Benchmark ‚îÄ‚îÄ‚îº‚îÄ‚îÄ> self.results dict
       ‚îÄ‚îÄ> Memory Benchmark   ‚îÄ‚îÄ‚î§    (keyed by metric type)
       ‚îÄ‚îÄ> Energy Estimation  ‚îÄ‚îÄ‚îò
```
"""

# %% nbgrader={"grade": false, "grade_id": "benchsuite-run", "solution": true}
#| export
    # --- BenchmarkSuite.run_full_benchmark ---
def benchsuite_run_full_benchmark(self) -> Dict[str, Dict[str, BenchmarkResult]]:
    """
    Run all benchmark categories.

    TODO: Orchestrate latency, accuracy, memory, and energy benchmarks

    APPROACH:
    1. Run self.benchmark.run_latency_benchmark()
    2. Run self.benchmark.run_accuracy_benchmark()
    3. Run self.benchmark.run_memory_benchmark()
    4. Run self._estimate_energy_efficiency()
    5. Store all results in self.results dict

    HINTS:
    - Print progress messages for each benchmark type
    - Return the complete results dict
    """
    ### BEGIN SOLUTION
    print("üß™ Running comprehensive benchmark suite...")

    # Run all benchmark types
    print("  üìä Measuring latency...")
    self.results['latency'] = self.benchmark.run_latency_benchmark()

    print("  üéØ Measuring accuracy...")
    self.results['accuracy'] = self.benchmark.run_accuracy_benchmark()

    print("  üíæ Measuring memory usage...")
    self.results['memory'] = self.benchmark.run_memory_benchmark()

    # Simulate energy benchmark (would require specialized hardware)
    print("  ‚ö° Estimating energy efficiency...")
    self.results['energy'] = self._estimate_energy_efficiency()

    return self.results
    ### END SOLUTION

BenchmarkSuite.run_full_benchmark = benchsuite_run_full_benchmark

# %% [markdown]
"""
### üß™ Unit Test: BenchmarkSuite.run_full_benchmark

**What we're testing**: Orchestration of all four benchmark types
**Why it matters**: Complete evaluation requires all metrics measured consistently
**Expected**: Results dict with keys for latency, accuracy, memory, energy
"""

# %% nbgrader={"grade": true, "grade_id": "test-benchsuite-run", "locked": true, "points": 15}
def test_unit_benchsuite_run():
    """üß™ Test BenchmarkSuite.run_full_benchmark."""
    print("üß™ Unit Test: BenchmarkSuite.run_full_benchmark...")

    class MockModel:
        def __init__(self, name):
            self.name = name
        def forward(self, x):
            time.sleep(0.001)
            return x

    import tempfile
    with tempfile.TemporaryDirectory() as tmp_dir:
        models = [MockModel("m1"), MockModel("m2")]
        suite = BenchmarkSuite(models, [{"d": "1"}], output_dir=tmp_dir)

        results = suite.run_full_benchmark()

        assert 'latency' in results
        assert 'accuracy' in results
        assert 'memory' in results
        assert 'energy' in results
        for metric_results in results.values():
            assert len(metric_results) == 2
            assert all(isinstance(r, BenchmarkResult) for r in metric_results.values())

    print("‚úÖ BenchmarkSuite.run_full_benchmark works correctly!")

# Note: test_unit_benchsuite_run() is called at the bottom of the module
# after all BenchmarkSuite methods (including _estimate_energy_efficiency) are patched.

# %% [markdown]
"""
### BenchmarkSuite._estimate_energy_efficiency - Energy Modeling

Since direct energy measurement requires specialized hardware (power meters, RAPL),
we estimate energy from latency and memory usage. This simplified model captures the
key relationship: energy is proportional to power (memory-related) multiplied by time (latency).

```
Energy Estimation Model:
energy = base_cost + (latency/1000) * 2.0 + memory * 0.01   (Joules)
         ‚Üë            ‚Üë                      ‚Üë
         Fixed        Time component          Memory component
         overhead     (active power)          (static power)
```
"""

# %% nbgrader={"grade": false, "grade_id": "benchsuite-energy", "solution": true}
#| export
    # --- BenchmarkSuite._estimate_energy_efficiency ---
def _benchsuite_estimate_energy_efficiency(self) -> Dict[str, BenchmarkResult]:
    """
    Estimate energy efficiency (simplified simulation).

    TODO: Estimate energy from latency and memory measurements

    APPROACH:
    1. Check if latency and memory results are available
    2. Combine latency and memory into energy estimate per measurement
    3. Fall back to simulated values if prerequisites missing
    4. Wrap results in BenchmarkResult

    HINTS:
    - Energy model: energy = 0.1 + (lat/1000)*2.0 + mem*0.01
    - Use zip() to pair latency and memory measurements
    """
    ### BEGIN SOLUTION
    energy_results = {}

    for i, model in enumerate(self.models):
        model_name = getattr(model, 'name', f'model_{i}')

        # Energy roughly correlates with latency * memory usage
        if 'latency' in self.results and 'memory' in self.results:
            latency_result = self.results['latency'].get(model_name)
            memory_result = self.results['memory'].get(model_name)

            if latency_result and memory_result:
                # Energy ‚àù power √ó time, power ‚àù memory usage
                energy_values = []
                for lat, mem in zip(latency_result.values, memory_result.values):
                    # Simplified energy model: energy = base + latency_factor * time + memory_factor * memory
                    energy = 0.1 + (lat / 1000) * 2.0 + mem * 0.01  # Joules
                    energy_values.append(energy)

                energy_results[model_name] = BenchmarkResult(
                    f"{model_name}_energy_joules",
                    energy_values,
                    metadata={'estimated': True, **self.benchmark.system_info}
                )

    # Fallback if no latency/memory results
    if not energy_results:
        for i, model in enumerate(self.models):
            model_name = getattr(model, 'name', f'model_{i}')
            # Simulate energy measurements
            energy_values = [0.5 + np.random.normal(0, 0.1) for _ in range(5)]
            energy_results[model_name] = BenchmarkResult(
                f"{model_name}_energy_joules",
                energy_values,
                metadata={'estimated': True, **self.benchmark.system_info}
            )

    return energy_results
    ### END SOLUTION

BenchmarkSuite._estimate_energy_efficiency = _benchsuite_estimate_energy_efficiency

# %% [markdown]
"""
### üß™ Unit Test: BenchmarkSuite._estimate_energy_efficiency

**What we're testing**: Energy estimation from latency and memory data
**Why it matters**: Energy awareness is critical for edge/mobile deployment
**Expected**: Positive energy values for each model
"""

# %% nbgrader={"grade": true, "grade_id": "test-benchsuite-energy", "locked": true, "points": 5}
def test_unit_benchsuite_energy():
    """üß™ Test BenchmarkSuite energy estimation."""
    print("üß™ Unit Test: BenchmarkSuite._estimate_energy_efficiency...")

    class MockModel:
        def __init__(self, name):
            self.name = name
        def forward(self, x):
            time.sleep(0.001)
            return x

    import tempfile
    with tempfile.TemporaryDirectory() as tmp_dir:
        models = [MockModel("m1")]
        suite = BenchmarkSuite(models, [{"d": "1"}], output_dir=tmp_dir)

        # Populate latency and memory first
        suite.results['latency'] = suite.benchmark.run_latency_benchmark()
        suite.results['memory'] = suite.benchmark.run_memory_benchmark()

        energy = suite._estimate_energy_efficiency()
        assert len(energy) >= 1
        assert all(isinstance(r, BenchmarkResult) for r in energy.values())
        assert all(r.mean > 0 for r in energy.values())

    print("‚úÖ BenchmarkSuite._estimate_energy_efficiency works correctly!")

if __name__ == "__main__":
    test_unit_benchsuite_energy()

# %% [markdown]
"""
### BenchmarkSuite.plot_results - Visualization

The plot_results method generates a 2x2 grid of bar charts comparing models
across all four metrics. The best performer in each category is highlighted green.
"""

# %% nbgrader={"grade": false, "grade_id": "benchsuite-plot", "solution": true}
#| export
    # --- BenchmarkSuite.plot_results and plot_pareto_frontier ---
def benchsuite_plot_results(self, save_plots: bool = True):
    """
    Generate visualization plots for benchmark results.

    TODO: Create 2x2 bar chart grid comparing models across metrics

    APPROACH:
    1. Check that results exist and matplotlib is available
    2. Create 2x2 subplot grid for latency, accuracy, memory, energy
    3. Plot bar charts with error bars (std)
    4. Highlight best performer in green
    5. Save and show plots

    HINTS:
    - For latency/memory/energy, lower is better
    - For accuracy, higher is better
    - Use alpha=0.7 for bars, capsize=5 for error bars
    """
    ### BEGIN SOLUTION
    if not self.results:
        print("No results to plot. Run benchmark first.")
        return

    if not MATPLOTLIB_AVAILABLE:
        print("‚ö†Ô∏è matplotlib not available - skipping plots. Install with: pip install matplotlib")
        return

    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('ML Model Benchmark Results', fontsize=16, fontweight='bold')

    # Plot each metric type
    metrics = ['latency', 'accuracy', 'memory', 'energy']
    units = ['ms', 'accuracy', 'MB', 'J']

    for idx, (metric, unit) in enumerate(zip(metrics, units)):
        ax = axes[idx // 2, idx % 2]

        if metric in self.results:
            model_names = []
            means = []
            stds = []

            for model_name, result in self.results[metric].items():
                clean_name = model_name.replace(f'_{metric}', '').replace('_ms', '').replace('_mb', '').replace('_joules', '')
                model_names.append(clean_name)
                means.append(result.mean)
                stds.append(result.std)

            bars = ax.bar(model_names, means, yerr=stds, capsize=5, alpha=0.7)
            ax.set_title(f'{metric.capitalize()} Comparison')
            ax.set_ylabel(f'{metric.capitalize()} ({unit})')
            ax.tick_params(axis='x', rotation=45)

            # Color bars by performance (green = better)
            if metric in ['latency', 'memory', 'energy']:  # Lower is better
                best_idx = means.index(min(means))
            else:  # Higher is better (accuracy)
                best_idx = means.index(max(means))

            for i, bar in enumerate(bars):
                if i == best_idx:
                    bar.set_color('green')
                    bar.set_alpha(0.8)
        else:
            ax.text(0.5, 0.5, f'No {metric} data', ha='center', va='center', transform=ax.transAxes)
            ax.set_title(f'{metric.capitalize()} Comparison')

    plt.tight_layout()

    if save_plots:
        plot_path = self.output_dir / 'benchmark_comparison.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        print(f"üìä Plots saved to {plot_path}")

    plt.show()
    ### END SOLUTION

BenchmarkSuite.plot_results = benchsuite_plot_results

def benchsuite_plot_pareto_frontier(self, x_metric: str = 'latency', y_metric: str = 'accuracy'):
    """Plot Pareto frontier for two competing objectives."""
    if not MATPLOTLIB_AVAILABLE:
        print("‚ö†Ô∏è matplotlib not available - skipping plots. Install with: pip install matplotlib")
        return

    if x_metric not in self.results or y_metric not in self.results:
        print(f"Missing data for {x_metric} or {y_metric}")
        return

    plt.figure(figsize=(10, 8))

    x_values = []
    y_values = []
    model_names = []

    for model_name in self.results[x_metric].keys():
        clean_name = model_name.replace(f'_{x_metric}', '').replace('_ms', '').replace('_mb', '').replace('_joules', '')
        if clean_name in [mn.replace(f'_{y_metric}', '') for mn in self.results[y_metric].keys()]:
            x_val = self.results[x_metric][model_name].mean

            # Find corresponding y value
            y_key = None
            for key in self.results[y_metric].keys():
                if clean_name in key:
                    y_key = key
                    break

            if y_key:
                y_val = self.results[y_metric][y_key].mean
                x_values.append(x_val)
                y_values.append(y_val)
                model_names.append(clean_name)

    # Plot points
    plt.scatter(x_values, y_values, s=100, alpha=0.7)

    # Label points
    for i, name in enumerate(model_names):
        plt.annotate(name, (x_values[i], y_values[i]),
                    xytext=(5, 5), textcoords='offset points')

    # Determine if lower or higher is better for each metric
    x_lower_better = x_metric in ['latency', 'memory', 'energy']
    y_lower_better = y_metric in ['latency', 'memory', 'energy']

    plt.xlabel(f'{x_metric.capitalize()} ({"lower" if x_lower_better else "higher"} is better)')
    plt.ylabel(f'{y_metric.capitalize()} ({"lower" if y_lower_better else "higher"} is better)')
    plt.title(f'Pareto Frontier: {x_metric.capitalize()} vs {y_metric.capitalize()}')
    plt.grid(True, alpha=0.3)

    # Save plot
    plot_path = self.output_dir / f'pareto_{x_metric}_vs_{y_metric}.png'
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    print(f"üìä Pareto plot saved to {plot_path}")
    plt.show()

BenchmarkSuite.plot_pareto_frontier = benchsuite_plot_pareto_frontier

# %% [markdown]
"""
### üß™ Unit Test: BenchmarkSuite.plot_results

**What we're testing**: Visualization generation (graceful handling when matplotlib unavailable)
**Why it matters**: Visual comparisons make benchmark results actionable
**Expected**: No errors when plotting (or graceful fallback message)
"""

# %% nbgrader={"grade": true, "grade_id": "test-benchsuite-plot", "locked": true, "points": 10}
def test_unit_benchsuite_plot():
    """üß™ Test BenchmarkSuite plotting."""
    print("üß™ Unit Test: BenchmarkSuite.plot_results...")

    class MockModel:
        def __init__(self, name):
            self.name = name
        def forward(self, x):
            time.sleep(0.001)
            return x

    import tempfile
    with tempfile.TemporaryDirectory() as tmp_dir:
        models = [MockModel("m1"), MockModel("m2")]
        suite = BenchmarkSuite(models, [{"d": "1"}], output_dir=tmp_dir)
        suite.run_full_benchmark()

        # Should not raise even without matplotlib display
        try:
            import matplotlib
            matplotlib.use('Agg')  # Non-interactive backend
            suite.plot_results(save_plots=True)
        except Exception:
            pass  # Plotting is optional

    # Test with no results
    import tempfile
    with tempfile.TemporaryDirectory() as tmp_dir:
        suite2 = BenchmarkSuite([MockModel("m1")], [{"d": "1"}], output_dir=tmp_dir)
        suite2.plot_results()  # Should print "No results" without error

    print("‚úÖ BenchmarkSuite.plot_results works correctly!")

if __name__ == "__main__":
    test_unit_benchsuite_plot()

# %% [markdown]
"""
### BenchmarkSuite.generate_report - Actionable Insights

The generate_report method compiles all benchmark results into a structured
markdown report with system information, per-metric summaries, best performers,
trade-off analysis, and deployment recommendations.

```
Report Generation Pipeline:
Results Dict ‚îÄ‚îÄ> System Info Section ‚îÄ‚îÄ> Per-Metric Summaries ‚îÄ‚îÄ> Trade-off Analysis
                                                                         ‚Üì
                                                              Recommendations Section
                                                                         ‚Üì
                                                              Save to benchmark_report.md
```

We'll build this in three steps: format the per-metric results summary,
compute trade-off recommendations, then compose the full report.
"""

# %% [markdown]
"""
#### Step 1: Format Per-Metric Results Summary

For each metric type, identify the best performer and list all model scores.
"""

# %% nbgrader={"grade": false, "grade_id": "benchsuite-format-results", "solution": true}
#| export
def _benchsuite_format_results_summary(self) -> List[str]:
    """
    Format per-metric results into report lines.

    Returns:
        List of markdown-formatted lines

    TODO: Summarize each metric with best performer and detailed scores

    APPROACH:
    1. For each metric type in self.results:
       a. Determine if lower or higher is better
       b. Find the best performer (min for latency/memory/energy, max for accuracy)
       c. List all models with mean ¬± std
    """
    ### BEGIN SOLUTION
    lines = []
    lines.append("## Benchmark Results Summary")
    lines.append("")

    for metric_type, results in self.results.items():
        lines.append(f"### {metric_type.capitalize()} Results")
        lines.append("")

        # Find best performer
        if metric_type in ['latency', 'memory', 'energy']:
            best_model = min(results.items(), key=lambda x: x[1].mean)
            comparison_text = "fastest" if metric_type == 'latency' else "most efficient"
        else:
            best_model = max(results.items(), key=lambda x: x[1].mean)
            comparison_text = "most accurate"

        lines.append(f"**Best performer**: {best_model[0]} ({comparison_text})")
        lines.append("")

        for model_name, result in results.items():
            clean_name = model_name.replace(f'_{metric_type}', '').replace('_ms', '').replace('_mb', '').replace('_joules', '')
            lines.append(f"- **{clean_name}**: {result.mean:.4f} ¬± {result.std:.4f}")
        lines.append("")

    return lines
    ### END SOLUTION

BenchmarkSuite._format_results_summary = _benchsuite_format_results_summary

# %% [markdown]
"""
#### Step 2: Compute Trade-off Recommendations

Analyze accuracy vs speed trade-offs and generate use-case recommendations.
"""

# %% nbgrader={"grade": false, "grade_id": "benchsuite-format-recs", "solution": true}
#| export
def _benchsuite_format_recommendations(self) -> List[str]:
    """
    Generate recommendation lines from benchmark results.

    Returns:
        List of markdown-formatted recommendation lines

    TODO: Compute trade-off scores and generate use-case recommendations

    APPROACH:
    1. If latency and accuracy results exist, normalize and compute combined scores
    2. Find best overall trade-off model
    3. Add use-case recommendations (max accuracy, min latency, production)

    HINTS:
    - Normalize: 1 - (val - min) / (max - min) for lower-is-better
    - Normalize: (val - min) / (max - min) for higher-is-better
    """
    ### BEGIN SOLUTION
    lines = []
    lines.append("## Recommendations")
    lines.append("")

    if len(self.results) >= 2:
        if 'latency' in self.results and 'accuracy' in self.results:
            lines.append("### Accuracy vs Speed Trade-off")

            latency_results = self.results['latency']
            accuracy_results = self.results['accuracy']

            scores = {}
            for model_name in latency_results.keys():
                clean_name = model_name.replace('_latency', '').replace('_ms', '')

                acc_key = None
                for key in accuracy_results.keys():
                    if clean_name in key:
                        acc_key = key
                        break

                if acc_key:
                    lat_vals = [r.mean for r in latency_results.values()]
                    acc_vals = [r.mean for r in accuracy_results.values()]

                    norm_latency = 1 - (latency_results[model_name].mean - min(lat_vals)) / (max(lat_vals) - min(lat_vals) + 1e-8)
                    norm_accuracy = (accuracy_results[acc_key].mean - min(acc_vals)) / (max(acc_vals) - min(acc_vals) + 1e-8)

                    scores[clean_name] = (norm_latency + norm_accuracy) / 2

            if scores:
                best_overall = max(scores.items(), key=lambda x: x[1])
                lines.append(f"- **Best overall trade-off**: {best_overall[0]} (score: {best_overall[1]:.3f})")
                lines.append("")

    lines.append("### Usage Recommendations")
    if 'accuracy' in self.results and 'latency' in self.results:
        acc_results = self.results['accuracy']
        lat_results = self.results['latency']

        best_acc_model = max(acc_results.items(), key=lambda x: x[1].mean)
        best_lat_model = min(lat_results.items(), key=lambda x: x[1].mean)

        lines.append(f"- **For maximum accuracy**: Use {best_acc_model[0].replace('_accuracy', '')}")
        lines.append(f"- **For minimum latency**: Use {best_lat_model[0].replace('_latency_ms', '')}")
        lines.append("- **For production deployment**: Consider the best overall trade-off model above")

    return lines
    ### END SOLUTION

BenchmarkSuite._format_recommendations = _benchsuite_format_recommendations

# %% [markdown]
"""
#### Step 3: Compose the Full Report

Combine system info, results summary, and recommendations into a complete
markdown report and save it to disk.
"""

# %% nbgrader={"grade": false, "grade_id": "benchsuite-report", "solution": true}
#| export
def benchsuite_generate_report(self) -> str:
    """
    Generate comprehensive benchmark report.

    TODO: Compose _format_results_summary and _format_recommendations into a full report

    APPROACH:
    1. Add report header and system information
    2. Call self._format_results_summary() for per-metric data
    3. Call self._format_recommendations() for trade-off analysis
    4. Save to output_dir/benchmark_report.md
    """
    ### BEGIN SOLUTION
    if not self.results:
        return "No benchmark results available. Run benchmark first."

    report_lines = []
    report_lines.append("# ML Model Benchmark Report")
    report_lines.append("=" * 50)
    report_lines.append("")

    # System information
    report_lines.append("## System Information")
    system_info = self.benchmark.system_info
    for key, value in system_info.items():
        report_lines.append(f"- {key}: {value}")
    report_lines.append("")

    # Results summary (from helper)
    report_lines.extend(self._format_results_summary())

    # Recommendations (from helper)
    report_lines.extend(self._format_recommendations())

    report_lines.append("")
    report_lines.append("---")
    report_lines.append("Report generated by TinyTorch Benchmarking Suite")

    # Save report
    report_text = "\n".join(report_lines)
    report_path = self.output_dir / 'benchmark_report.md'
    with open(report_path, 'w') as f:
        f.write(report_text)

    print(f"üìÑ Report saved to {report_path}")
    return report_text
    ### END SOLUTION

BenchmarkSuite.generate_report = benchsuite_generate_report

# %% [markdown]
"""
### üß™ Unit Test: BenchmarkSuite._format_results_summary

**What we're testing**: Per-metric results formatting with best performer identification
**Why it matters**: Correct summaries help engineers quickly identify winners
**Expected**: Markdown lines with metric headers, best performers, and model scores
"""

# %% nbgrader={"grade": true, "grade_id": "test-benchsuite-format-results", "locked": true, "points": 3}
def test_unit_benchsuite_format_results():
    """üß™ Test BenchmarkSuite._format_results_summary implementation."""
    print("üß™ Unit Test: BenchmarkSuite._format_results_summary...")

    class MockModel:
        def __init__(self, name):
            self.name = name
        def forward(self, x):
            return x * 0.5

    import tempfile
    with tempfile.TemporaryDirectory() as tmp_dir:
        models = [MockModel("fast_model"), MockModel("accurate_model")]
        suite = BenchmarkSuite(models, [{"data": "test"}], output_dir=tmp_dir)
        suite.run_full_benchmark()

        lines = suite._format_results_summary()

        # Should return a list of strings
        assert isinstance(lines, list), f"Expected list, got {type(lines)}"
        assert len(lines) > 0, "Should produce at least some lines"

        # Should contain results summary header
        text = "\n".join(lines)
        assert "Results Summary" in text, "Should contain 'Results Summary'"
        assert "Best performer" in text, "Should identify best performer"

    print("‚úÖ BenchmarkSuite._format_results_summary works correctly!")

if __name__ == "__main__":
    test_unit_benchsuite_format_results()

# %% [markdown]
"""
### üß™ Unit Test: BenchmarkSuite._format_recommendations

**What we're testing**: Trade-off analysis and use-case recommendation generation
**Why it matters**: Wrong recommendations lead to wrong deployment decisions
**Expected**: Markdown lines with trade-off scores and use-case guidance
"""

# %% nbgrader={"grade": true, "grade_id": "test-benchsuite-format-recs", "locked": true, "points": 3}
def test_unit_benchsuite_format_recs():
    """üß™ Test BenchmarkSuite._format_recommendations implementation."""
    print("üß™ Unit Test: BenchmarkSuite._format_recommendations...")

    class MockModel:
        def __init__(self, name):
            self.name = name
        def forward(self, x):
            return x * 0.5

    import tempfile
    with tempfile.TemporaryDirectory() as tmp_dir:
        models = [MockModel("fast_model"), MockModel("accurate_model")]
        suite = BenchmarkSuite(models, [{"data": "test"}], output_dir=tmp_dir)
        suite.run_full_benchmark()

        lines = suite._format_recommendations()

        assert isinstance(lines, list), f"Expected list, got {type(lines)}"
        text = "\n".join(lines)
        assert "Recommendations" in text, "Should contain 'Recommendations'"

    print("‚úÖ BenchmarkSuite._format_recommendations works correctly!")

if __name__ == "__main__":
    test_unit_benchsuite_format_recs()

# %% [markdown]
"""
### üß™ Unit Test: BenchmarkSuite (Full Class Integration)

This test validates our BenchmarkSuite runs comprehensive multi-metric evaluation
and generates valid reports with recommendations.

**What we're testing**: Full benchmark suite with report generation
**Why it matters**: Comprehensive evaluation enables informed optimization decisions
**Expected**: Complete results across all metrics with valid reports
"""

# %% nbgrader={"grade": true, "grade_id": "test-benchmark-suite", "locked": true, "points": 15}
def test_unit_benchmark_suite():
    """üß™ Test BenchmarkSuite comprehensive functionality."""
    print("üß™ Unit Test: BenchmarkSuite...")

    # Create mock models
    class MockModel:
        def __init__(self, name):
            self.name = name

        def forward(self, x):
            time.sleep(0.001)
            return x

    models = [MockModel("efficient_model"), MockModel("accurate_model")]
    datasets = [{"test": "data"}]

    # Create temporary directory for test output
    import tempfile
    with tempfile.TemporaryDirectory() as tmp_dir:
        suite = BenchmarkSuite(models, datasets, output_dir=tmp_dir)

        # Run full benchmark
        results = suite.run_full_benchmark()

        # Verify all benchmark types completed
        assert 'latency' in results
        assert 'accuracy' in results
        assert 'memory' in results
        assert 'energy' in results

        # Verify results structure
        for metric_results in results.values():
            assert len(metric_results) == 2  # Two models
            assert all(isinstance(result, BenchmarkResult) for result in metric_results.values())

        # Test report generation
        report = suite.generate_report()
        assert "Benchmark Report" in report
        assert "System Information" in report
        assert "Recommendations" in report

        # Verify files are created
        output_path = Path(tmp_dir)
        assert (output_path / 'benchmark_report.md').exists()

    print("‚úÖ BenchmarkSuite works correctly!")

if __name__ == "__main__":
    test_unit_benchmark_suite()

# %% [markdown]
"""
### MLPerf - Standardized Industry Benchmarking

MLPerf provides standardized benchmarks that enable fair comparison across different systems, similar to how MLPerf works for larger models. This is crucial for reproducible research and industry adoption.

### Why Standardization Matters

Without standards, every team benchmarks differently:
- Different datasets, input sizes, measurement protocols
- Different accuracy metrics, latency definitions
- Different hardware configurations, software stacks

This makes it impossible to compare results across papers, products, or research groups.

### MLPerf Benchmark Architecture

```
MLPerf Benchmark Structure:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  Benchmark Definition                   ‚îÇ
‚îÇ ‚Ä¢ Standard datasets (CIFAR-10, Speech Commands, etc.)   ‚îÇ
‚îÇ ‚Ä¢ Fixed input shapes and data types                     ‚îÇ
‚îÇ ‚Ä¢ Target accuracy and latency thresholds                ‚îÇ
‚îÇ ‚Ä¢ Measurement protocol (warmup, runs, etc.)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 Execution Protocol                      ‚îÇ
‚îÇ 1. Model registration and validation                    ‚îÇ
‚îÇ 2. Warmup phase (deterministic random inputs)           ‚îÇ
‚îÇ 3. Measurement phase (statistical sampling)             ‚îÇ
‚îÇ 4. Accuracy evaluation (ground truth comparison)        ‚îÇ
‚îÇ 5. Compliance checking (thresholds, statistical tests)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Compliance Determination                   ‚îÇ
‚îÇ PASS: accuracy ‚â• target AND latency ‚â§ target            ‚îÇ
‚îÇ FAIL: Either constraint violated                        ‚îÇ
‚îÇ Report: Detailed metrics + system information           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Standard Benchmark Tasks

**Keyword Spotting**: Wake word detection from audio
- Input: 1-second 16kHz audio samples
- Task: Binary classification (keyword present/absent)
- Target: 90% accuracy, <100ms latency

**Visual Wake Words**: Person detection in images
- Input: 96√ó96 RGB images
- Task: Binary classification (person present/absent)
- Target: 80% accuracy, <200ms latency

**Anomaly Detection**: Industrial sensor monitoring
- Input: 640-element sensor feature vectors
- Task: Binary classification (anomaly/normal)
- Target: 85% accuracy, <50ms latency

### Reproducibility Requirements

All MLPerf benchmarks use:
- **Fixed random seeds**: Deterministic input generation
- **Standardized hardware**: Reference implementations for comparison
- **Statistical validation**: Multiple runs with confidence intervals
- **Compliance reporting**: Machine-readable results format
"""

# %% [markdown]
"""
### MLPerf.__init__ - Configuring Standard Benchmarks

The MLPerf constructor sets up four standardized benchmark tasks, each with
fixed input shapes, target accuracy, and maximum latency thresholds. Using a
fixed random seed ensures reproducible results across different systems.

```
Standard MLPerf Benchmarks:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Benchmark           ‚îÇ Input Shape      ‚îÇ Acc Tgt ‚îÇ Lat Tgt  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ keyword_spotting     ‚îÇ (1, 16000)       ‚îÇ 90%     ‚îÇ <100ms   ‚îÇ
‚îÇ visual_wake_words   ‚îÇ (1, 96, 96, 3)   ‚îÇ 80%     ‚îÇ <200ms   ‚îÇ
‚îÇ anomaly_detection   ‚îÇ (1, 640)         ‚îÇ 85%     ‚îÇ <50ms    ‚îÇ
‚îÇ image_classification‚îÇ (1, 32, 32, 3)   ‚îÇ 75%     ‚îÇ <150ms   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```
"""

# %% nbgrader={"grade": false, "grade_id": "tinymlperf-init", "solution": true}
#| export
class MLPerf:
    """
    MLPerf-style standardized benchmarking for edge ML systems.

    Provides fixed benchmark configurations with target thresholds,
    standardized measurement protocols, and compliance reporting.

    EXAMPLE:
    >>> perf = MLPerf()
    >>> results = perf.run_standard_benchmark(model, 'keyword_spotting')
    >>> perf.generate_compliance_report(results)
    """

    def __init__(self, random_seed: int = 42):
        """
        Initialize MLPerf benchmark suite.

        TODO: Set up standard benchmark configurations with fixed seeds

        APPROACH:
        1. Store random seed and initialize numpy RNG
        2. Define benchmark configs with input_shape, target_accuracy, max_latency_ms

        HINTS:
        - Each benchmark is a dict with 'input_shape', 'target_accuracy', 'max_latency_ms', 'description'
        - keyword_spotting uses (1, 16000) for 1 second of 16kHz audio
        """
        ### BEGIN SOLUTION
        self.random_seed = random_seed
        np.random.seed(random_seed)

        # Standard MLPerf benchmark configurations
        self.benchmarks = {
            'keyword_spotting': {
                'input_shape': (1, 16000),  # 1 second of 16kHz audio
                'target_accuracy': 0.90,
                'max_latency_ms': 100,
                'description': 'Wake word detection'
            },
            'visual_wake_words': {
                'input_shape': (1, 96, 96, 3),  # 96x96 RGB image
                'target_accuracy': 0.80,
                'max_latency_ms': 200,
                'description': 'Person detection in images'
            },
            'anomaly_detection': {
                'input_shape': (1, 640),  # Machine sensor data
                'target_accuracy': 0.85,
                'max_latency_ms': 50,
                'description': 'Industrial anomaly detection'
            },
            'image_classification': {
                'input_shape': (1, 32, 32, 3),  # CIFAR-10 style
                'target_accuracy': 0.75,
                'max_latency_ms': 150,
                'description': 'Tiny image classification'
            }
        }
        ### END SOLUTION

# %% [markdown]
"""
### üß™ Unit Test: MLPerf.__init__

**What we're testing**: Benchmark configuration setup with all four standard tasks
**Why it matters**: Correct configurations ensure fair, standardized comparisons
**Expected**: Four benchmarks with proper input shapes and thresholds
"""

# %% nbgrader={"grade": true, "grade_id": "test-tinymlperf-init", "locked": true, "points": 5}
def test_unit_mlperf_init():
    """üß™ Test MLPerf initialization."""
    print("üß™ Unit Test: MLPerf.__init__...")

    perf = MLPerf(random_seed=42)

    assert perf.random_seed == 42
    assert len(perf.benchmarks) == 4
    assert 'keyword_spotting' in perf.benchmarks
    assert 'visual_wake_words' in perf.benchmarks
    assert 'anomaly_detection' in perf.benchmarks
    assert 'image_classification' in perf.benchmarks

    # Verify config structure
    for name, config in perf.benchmarks.items():
        assert 'input_shape' in config
        assert 'target_accuracy' in config
        assert 'max_latency_ms' in config
        assert 'description' in config
        assert 0 < config['target_accuracy'] <= 1.0
        assert config['max_latency_ms'] > 0

    print("‚úÖ MLPerf.__init__ works correctly!")

if __name__ == "__main__":
    test_unit_mlperf_init()

# %% [markdown]
"""
### MLPerf._run_latency_test - Measuring Inference Latency

This helper runs the latency measurement phase: warmup, then timed inference
for each test input. Returns lists of latencies (ms) and model predictions.

```
Latency Test Protocol:
Test Inputs ‚îÄ‚îÄ> Warmup Phase (10%) ‚îÄ‚îÄ> Measurement Phase (100%) ‚îÄ‚îÄ> latencies[], predictions[]
                (discard timing)       (collect per-input timing)
```
"""

# %% nbgrader={"grade": false, "grade_id": "tinymlperf-latency", "solution": true}
#| export
    # --- MLPerf._run_latency_test ---
def _mlperf_run_latency_test(self, model: Any, test_inputs: List[Any],
                                  benchmark_name: str, num_runs: int) -> Tuple[List[float], List[Any]]:
    """
    Run latency measurement phase with warmup.

    TODO: Implement warmup and measurement phases for latency testing

    APPROACH:
    1. Warmup phase: run 10% of inputs without timing
    2. Measurement phase: time each inference with precise_timer
    3. Use duck-typing (forward/predict/callable) for model invocation
    4. Return latencies in ms and predictions list

    HINTS:
    - warmup_runs = max(1, num_runs // 10)
    - Use precise_timer() context manager
    - Convert elapsed seconds to ms: timer.elapsed * 1000
    """
    ### BEGIN SOLUTION
    # Warmup phase (10% of runs)
    warmup_runs = max(1, num_runs // 10)
    print(f"   Warming up ({warmup_runs} runs)...")
    for i in range(warmup_runs):
        if hasattr(model, 'forward'):
            model.forward(test_inputs[i])
        elif hasattr(model, 'predict'):
            model.predict(test_inputs[i])
        elif callable(model):
            model(test_inputs[i])

    # Measurement phase
    print(f"   Measuring performance ({num_runs} runs)...")
    latencies = []
    predictions = []

    for i, test_input in enumerate(test_inputs):
        with precise_timer() as timer:
            try:
                if hasattr(model, 'forward'):
                    output = model.forward(test_input)
                elif hasattr(model, 'predict'):
                    output = model.predict(test_input)
                elif callable(model):
                    output = model(test_input)
                else:
                    # Simulate prediction
                    output = np.random.rand(2) if benchmark_name in ['keyword_spotting', 'visual_wake_words'] else np.random.rand(10)

                predictions.append(output)
            except Exception:
                # Fallback simulation
                predictions.append(np.random.rand(2))

        latencies.append(timer.elapsed * 1000)  # Convert to ms

    return latencies, predictions
    ### END SOLUTION

MLPerf._run_latency_test = _mlperf_run_latency_test

# %% [markdown]
"""
### üß™ Unit Test: MLPerf._run_latency_test

**What we're testing**: Warmup and measurement phase execution
**Why it matters**: Proper warmup eliminates cold-start bias in measurements
**Expected**: Positive latency values and predictions for each input
"""

# %% nbgrader={"grade": true, "grade_id": "test-tinymlperf-latency", "locked": true, "points": 10}
def test_unit_mlperf_latency():
    """üß™ Test MLPerf latency measurement phase."""
    print("üß™ Unit Test: MLPerf._run_latency_test...")

    class MockModel:
        def __init__(self, name):
            self.name = name
        def forward(self, x):
            time.sleep(0.001)
            return np.random.rand(2)

    perf = MLPerf(random_seed=42)
    model = MockModel("test")

    test_inputs = [np.random.randn(1, 16000).astype(np.float32) for _ in range(5)]
    latencies, predictions = perf._run_latency_test(model, test_inputs, 'keyword_spotting', 5)

    assert len(latencies) == 5
    assert len(predictions) == 5
    assert all(lat > 0 for lat in latencies)

    print("‚úÖ MLPerf._run_latency_test works correctly!")

if __name__ == "__main__":
    test_unit_mlperf_latency()

# %% [markdown]
"""
### MLPerf._run_accuracy_test - Evaluating Prediction Quality

This helper calculates accuracy by comparing model predictions against synthetic
ground truth labels. It handles both binary classification (keyword spotting,
visual wake words) and multi-class classification (image classification,
anomaly detection).

We'll build this in two steps: first a helper to extract a clean prediction
array from various output formats, then the accuracy calculation itself.
"""

# %% [markdown]
"""
#### Step 1: Extract Prediction Array

Model outputs can be TinyTorch Tensors, numpy arrays, or plain Python objects.
This helper normalizes them into a flat numpy array for label extraction.
"""

# %% nbgrader={"grade": false, "grade_id": "tinymlperf-extract-pred", "solution": true}
#| export
def _extract_pred_array(pred) -> np.ndarray:
    """
    Extract a flat numpy array from a model prediction.

    Args:
        pred: Raw prediction (Tensor, numpy array, or list)

    Returns:
        Flattened numpy array of prediction values

    TODO: Normalize various prediction formats into a flat numpy array

    APPROACH:
    1. If pred has .data attribute (TinyTorch Tensor), use it
    2. Otherwise convert to numpy array
    3. Flatten if multi-dimensional
    """
    ### BEGIN SOLUTION
    if hasattr(pred, 'data'):
        pred_array = pred.data
    else:
        pred_array = np.array(pred)

    # Convert to numpy array if needed (handle memoryview objects)
    if not isinstance(pred_array, np.ndarray):
        pred_array = np.array(pred_array)

    if len(pred_array.shape) > 1:
        pred_array = pred_array.flatten()

    return pred_array
    ### END SOLUTION

# %% [markdown]
"""
#### Step 2: Calculate Accuracy

Use _extract_pred_array to get clean predictions, then compare against
synthetic ground truth for binary and multi-class tasks.
"""

# %% nbgrader={"grade": false, "grade_id": "tinymlperf-memory", "solution": true}
#| export
def _mlperf_run_accuracy_test(self, model: Any, predictions: List[Any],
                                    benchmark_name: str, num_runs: int) -> float:
    """
    Calculate accuracy from predictions against synthetic ground truth.

    TODO: Implement accuracy calculation using _extract_pred_array helper

    APPROACH:
    1. Generate synthetic ground truth using fixed seed
    2. For binary tasks: use _extract_pred_array, compare class scores
    3. For multi-class: use _extract_pred_array, take argmax
    4. Add realistic noise based on model name

    HINTS:
    - keyword_spotting and visual_wake_words are binary (2 classes)
    - image_classification has 10 classes, anomaly_detection has 5
    """
    ### BEGIN SOLUTION
    np.random.seed(self.random_seed)
    if benchmark_name in ['keyword_spotting', 'visual_wake_words']:
        # Binary classification
        true_labels = np.random.randint(0, 2, num_runs)
        predicted_labels = []
        for pred in predictions:
            pred_array = _extract_pred_array(pred)
            if len(pred_array) >= 2:
                predicted_labels.append(1 if pred_array[1] > pred_array[0] else 0)
            else:
                predicted_labels.append(1 if pred_array[0] > 0.5 else 0)
    else:
        # Multi-class classification
        num_classes = 10 if benchmark_name == 'image_classification' else 5
        true_labels = np.random.randint(0, num_classes, num_runs)
        predicted_labels = []
        for pred in predictions:
            pred_array = _extract_pred_array(pred)
            predicted_labels.append(np.argmax(pred_array) % num_classes)

    # Calculate accuracy
    correct_predictions = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == pred)
    accuracy = correct_predictions / num_runs

    # Add realistic noise based on model complexity
    model_name = getattr(model, 'name', 'unknown_model')
    if 'efficient' in model_name.lower():
        accuracy = min(0.95, accuracy + 0.1)
    elif 'accurate' in model_name.lower():
        accuracy = min(0.98, accuracy + 0.2)

    return accuracy
    ### END SOLUTION

MLPerf._run_accuracy_test = _mlperf_run_accuracy_test

# %% [markdown]
"""
### üß™ Unit Test: _extract_pred_array

**What we're testing**: Prediction array extraction from various output formats
**Why it matters**: Models return Tensors, numpy arrays, or lists ‚Äî we need to handle all
**Expected**: Always returns a flat numpy array regardless of input format
"""

# %% nbgrader={"grade": true, "grade_id": "test-extract-pred", "locked": true, "points": 3}
def test_unit_extract_pred_array():
    """üß™ Test _extract_pred_array helper."""
    print("üß™ Unit Test: _extract_pred_array...")

    # Test with plain numpy array
    result = _extract_pred_array(np.array([0.3, 0.7]))
    assert isinstance(result, np.ndarray), f"Expected ndarray, got {type(result)}"
    assert result.shape == (2,), f"Expected shape (2,), got {result.shape}"

    # Test with 2D array (should flatten)
    result_2d = _extract_pred_array(np.array([[0.3, 0.7]]))
    assert len(result_2d.shape) == 1, "Should flatten multi-dimensional input"

    # Test with list
    result_list = _extract_pred_array([0.3, 0.7])
    assert isinstance(result_list, np.ndarray), "Should convert list to ndarray"

    print("‚úÖ _extract_pred_array works correctly!")

if __name__ == "__main__":
    test_unit_extract_pred_array()

# %% [markdown]
"""
### üß™ Unit Test: MLPerf._run_accuracy_test

**What we're testing**: Accuracy calculation for binary and multi-class tasks
**Why it matters**: Accuracy determines whether a model meets compliance thresholds
**Expected**: Accuracy value between 0 and 1
"""

# %% nbgrader={"grade": true, "grade_id": "test-tinymlperf-accuracy", "locked": true, "points": 10}
def test_unit_mlperf_accuracy():
    """üß™ Test MLPerf accuracy calculation."""
    print("üß™ Unit Test: MLPerf._run_accuracy_test...")

    class MockModel:
        def __init__(self, name):
            self.name = name

    perf = MLPerf(random_seed=42)
    model = MockModel("test_model")

    # Binary classification predictions
    predictions = [np.random.rand(2) for _ in range(10)]
    accuracy = perf._run_accuracy_test(model, predictions, 'keyword_spotting', 10)
    assert 0 <= accuracy <= 1

    # Multi-class predictions
    predictions_mc = [np.random.rand(10) for _ in range(10)]
    accuracy_mc = perf._run_accuracy_test(model, predictions_mc, 'image_classification', 10)
    assert 0 <= accuracy_mc <= 1

    print("‚úÖ MLPerf._run_accuracy_test works correctly!")

if __name__ == "__main__":
    test_unit_mlperf_accuracy()

# %% [markdown]
"""
### MLPerf.run_standard_benchmark - Complete Benchmark Execution

This method orchestrates a complete standardized benchmark: input generation,
latency testing, accuracy evaluation, and compliance determination. It composes
the `_run_latency_test` and `_run_accuracy_test` helpers into the full protocol.

```
run_standard_benchmark Pipeline:
Config Lookup ‚îÄ‚îÄ> Generate Inputs ‚îÄ‚îÄ> _run_latency_test() ‚îÄ‚îÄ> _run_accuracy_test()
                  (deterministic)     (warmup + measure)      (evaluate quality)
                                                                     ‚Üì
                                                          Compile Results Dict
                                                          (accuracy, latency, compliance)
```
"""

# %% nbgrader={"grade": false, "grade_id": "tinymlperf-run", "solution": true}
#| export
    # --- MLPerf.run_standard_benchmark ---
def mlperf_run_standard_benchmark(self, model: Any, benchmark_name: str,
                              num_runs: int = 100) -> Dict[str, Any]:
    """
    Run a standardized MLPerf benchmark.

    TODO: Orchestrate input generation, latency test, accuracy test, and compliance check

    APPROACH:
    1. Validate benchmark_name and get config
    2. Generate deterministic test inputs using seeded random
    3. Call self._run_latency_test() for timing
    4. Call self._run_accuracy_test() for quality
    5. Compile results with compliance determination

    HINTS:
    - Use np.random.seed(self.random_seed + i) for each input
    - Audio data: np.random.randn, Image data: np.random.randint(0,256)/255
    - compliant = accuracy_met AND latency_met
    """
    ### BEGIN SOLUTION
    if benchmark_name not in self.benchmarks:
        available = list(self.benchmarks.keys())
        raise ValueError(
            f"Unknown MLPerf benchmark: '{benchmark_name}'\n"
            f"  ‚ùå '{benchmark_name}' is not a registered benchmark\n"
            f"  üí° MLPerf defines standard edge ML benchmarks for reproducible comparison\n"
            f"  üîß Choose from: {available}"
        )

    config = self.benchmarks[benchmark_name]
    print(f"üß™ Running MLPerf {benchmark_name} benchmark...")
    print(f"   Target: {config['target_accuracy']:.1%} accuracy, "
          f"<{config['max_latency_ms']}ms latency")

    # Generate standardized test inputs
    input_shape = config['input_shape']
    test_inputs = []
    for i in range(num_runs):
        # Use deterministic random generation for reproducibility
        np.random.seed(self.random_seed + i)
        if len(input_shape) == 2:  # Audio/sequence data
            test_input = np.random.randn(*input_shape).astype(np.float32)
        else:  # Image data
            test_input = np.random.randint(0, 256, input_shape).astype(np.float32) / 255.0
        test_inputs.append(test_input)

    # Run latency and accuracy tests using helpers
    latencies, predictions = self._run_latency_test(model, test_inputs, benchmark_name, num_runs)
    accuracy = self._run_accuracy_test(model, predictions, benchmark_name, num_runs)

    # Compile results
    mean_latency = float(np.mean(latencies))
    accuracy_met = bool(accuracy >= config['target_accuracy'])
    latency_met = bool(mean_latency <= config['max_latency_ms'])

    results = {
        'benchmark_name': benchmark_name,
        'model_name': getattr(model, 'name', 'unknown_model'),
        'accuracy': float(accuracy),
        'mean_latency_ms': mean_latency,
        'std_latency_ms': float(np.std(latencies)),
        'p50_latency_ms': float(np.percentile(latencies, 50)),
        'p90_latency_ms': float(np.percentile(latencies, 90)),
        'p99_latency_ms': float(np.percentile(latencies, 99)),
        'max_latency_ms': float(np.max(latencies)),
        'throughput_fps': float(1000 / mean_latency),
        'target_accuracy': float(config['target_accuracy']),
        'target_latency_ms': float(config['max_latency_ms']),
        'accuracy_met': accuracy_met,
        'latency_met': latency_met,
        'compliant': accuracy_met and latency_met,
        'num_runs': int(num_runs),
        'random_seed': int(self.random_seed)
    }

    print(f"   Results: {accuracy:.1%} accuracy, {np.mean(latencies):.1f}ms latency")
    print(f"   Compliance: {'‚úÖ PASS' if results['compliant'] else '‚ùå FAIL'}")

    return results
    ### END SOLUTION

MLPerf.run_standard_benchmark = mlperf_run_standard_benchmark

def mlperf_run_all_benchmarks(self, model: Any) -> Dict[str, Dict[str, Any]]:
    """Run all MLPerf benchmarks on a model."""
    all_results = {}

    print(f"üöÄ Running full MLPerf suite on {getattr(model, 'name', 'model')}...")
    print("=" * 60)

    for benchmark_name in self.benchmarks.keys():
        try:
            results = self.run_standard_benchmark(model, benchmark_name)
            all_results[benchmark_name] = results
            print()
        except Exception as e:
            print(f"   ‚ùå Failed to run {benchmark_name}: {e}")
            all_results[benchmark_name] = {'error': str(e)}

    return all_results

MLPerf.run_all_benchmarks = mlperf_run_all_benchmarks

# %% [markdown]
"""
### üß™ Unit Test: MLPerf.run_standard_benchmark

**What we're testing**: Complete benchmark execution with compliance determination
**Why it matters**: The full pipeline must produce valid, reproducible results
**Expected**: Results dict with all required metrics and compliance flags
"""

# %% nbgrader={"grade": true, "grade_id": "test-tinymlperf-run", "locked": true, "points": 15}
def test_unit_mlperf_run():
    """üß™ Test MLPerf standard benchmark execution."""
    print("üß™ Unit Test: MLPerf.run_standard_benchmark...")

    class MockModel:
        def __init__(self, name):
            self.name = name
        def forward(self, x):
            time.sleep(0.001)
            if hasattr(x, 'shape') and len(x.shape) == 2:
                return np.random.rand(2)
            return np.random.rand(10)

    perf = MLPerf(random_seed=42)
    model = MockModel("test_model")

    result = perf.run_standard_benchmark(model, 'keyword_spotting', num_runs=5)

    required_keys = ['accuracy', 'mean_latency_ms', 'throughput_fps', 'compliant',
                     'accuracy_met', 'latency_met', 'p50_latency_ms', 'p99_latency_ms']
    assert all(key in result for key in required_keys)
    assert 0 <= result['accuracy'] <= 1
    assert result['mean_latency_ms'] > 0
    assert result['throughput_fps'] > 0
    assert isinstance(result['compliant'], bool)

    # Test invalid benchmark name
    try:
        perf.run_standard_benchmark(model, 'nonexistent')
        assert False, "Should have raised ValueError"
    except ValueError:
        pass

    print("‚úÖ MLPerf.run_standard_benchmark works correctly!")

if __name__ == "__main__":
    test_unit_mlperf_run()

# %% [markdown]
"""
### MLPerf.generate_compliance_report - Scorecard Generation

The compliance report compiles results from multiple benchmarks into both
machine-readable JSON and human-readable markdown formats, with overall
compliance determination.

```
Report Generation:
Results Dict ‚îÄ‚îÄ> Count compliant benchmarks ‚îÄ‚îÄ> JSON report (structured data)
                                              ‚îÄ‚îÄ> Markdown summary (human-readable)
                                              ‚îÄ‚îÄ> Overall: COMPLIANT/NON-COMPLIANT
```

We'll build this in two steps: compile the structured report data,
then format it into a human-readable summary.
"""

# %% [markdown]
"""
#### Step 1: Compile Structured Report Data

Process raw benchmark results into a structured dictionary with compliance
statistics, ready for JSON serialization.
"""

# %% nbgrader={"grade": false, "grade_id": "tinymlperf-compile-data", "solution": true}
#| export
def _mlperf_compile_report_data(self, results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
    """
    Compile benchmark results into structured report data.

    Args:
        results: Raw benchmark results dict

    Returns:
        Structured report_data dict with benchmarks and summary

    TODO: Process results into a structured dict with compliance stats

    APPROACH:
    1. Initialize report_data with version, seed, timestamp
    2. Loop through results, skipping errors
    3. Count compliant benchmarks and compute compliance_rate
    4. Store per-benchmark metrics

    HINTS:
    - overall_compliant = compliance_rate == 1.0
    - Set model_name from first successful result
    """
    ### BEGIN SOLUTION
    compliant_benchmarks = []
    total_benchmarks = 0

    report_data = {
        'mlperf_version': '1.0',
        'random_seed': self.random_seed,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'model_name': 'unknown',
        'benchmarks': {},
        'summary': {}
    }

    for benchmark_name, result in results.items():
        if 'error' not in result:
            total_benchmarks += 1
            if result.get('compliant', False):
                compliant_benchmarks.append(benchmark_name)

            if report_data['model_name'] == 'unknown':
                report_data['model_name'] = result.get('model_name', 'unknown')

            report_data['benchmarks'][benchmark_name] = {
                'accuracy': result['accuracy'],
                'mean_latency_ms': result['mean_latency_ms'],
                'p99_latency_ms': result['p99_latency_ms'],
                'throughput_fps': result['throughput_fps'],
                'target_accuracy': result['target_accuracy'],
                'target_latency_ms': result['target_latency_ms'],
                'accuracy_met': result['accuracy_met'],
                'latency_met': result['latency_met'],
                'compliant': result['compliant']
            }

    if total_benchmarks > 0:
        compliance_rate = len(compliant_benchmarks) / total_benchmarks
        report_data['summary'] = {
            'total_benchmarks': total_benchmarks,
            'compliant_benchmarks': len(compliant_benchmarks),
            'compliance_rate': compliance_rate,
            'overall_compliant': compliance_rate == 1.0,
            'compliant_benchmark_names': compliant_benchmarks
        }

    return report_data
    ### END SOLUTION

MLPerf._compile_report_data = _mlperf_compile_report_data

# %% [markdown]
"""
#### Step 2: Format Human-Readable Summary

Convert structured report data into a markdown compliance summary.
"""

# %% nbgrader={"grade": false, "grade_id": "tinymlperf-format-summary", "solution": true}
#| export
def _mlperf_format_compliance_summary(self, report_data: Dict[str, Any]) -> str:
    """
    Format report data into a human-readable markdown summary.

    Args:
        report_data: Structured report dict from _compile_report_data

    Returns:
        Markdown-formatted summary string

    TODO: Generate markdown summary from structured report data

    APPROACH:
    1. Add header with model name and date
    2. Show overall COMPLIANT/NON-COMPLIANT status
    3. List each benchmark with PASS/FAIL and metrics
    """
    ### BEGIN SOLUTION
    summary_lines = []
    summary_lines.append("# MLPerf Compliance Report")
    summary_lines.append("=" * 40)
    summary_lines.append(f"Model: {report_data['model_name']}")
    summary_lines.append(f"Date: {report_data['timestamp']}")
    summary_lines.append("")

    if report_data['summary']:
        overall = report_data['summary']['overall_compliant']
        rate = report_data['summary']['compliance_rate']
        compliant_count = report_data['summary']['compliant_benchmarks']
        total = report_data['summary']['total_benchmarks']

        summary_lines.append(f"## Overall Result: {'‚úÖ COMPLIANT' if overall else '‚ùå NON-COMPLIANT'}")
        summary_lines.append(f"Compliance Rate: {rate:.1%} ({compliant_count}/{total})")
        summary_lines.append("")

        summary_lines.append("## Benchmark Details:")
        for benchmark_name, result in report_data['benchmarks'].items():
            status = "‚úÖ PASS" if result['compliant'] else "‚ùå FAIL"
            summary_lines.append(f"- **{benchmark_name}**: {status}")
            summary_lines.append(f"  - Accuracy: {result['accuracy']:.1%} (target: {result['target_accuracy']:.1%})")
            summary_lines.append(f"  - Latency: {result['mean_latency_ms']:.1f}ms (target: <{result['target_latency_ms']}ms)")
            summary_lines.append("")
    else:
        summary_lines.append("No successful benchmark runs.")

    return "\n".join(summary_lines)
    ### END SOLUTION

MLPerf._format_compliance_summary = _mlperf_format_compliance_summary

# %% [markdown]
"""
#### Step 3: Compose the Full Compliance Report

Combine data compilation, JSON serialization, and summary formatting.
"""

# %% nbgrader={"grade": false, "grade_id": "tinymlperf-scorecard", "solution": true}
#| export
def mlperf_generate_compliance_report(self, results: Dict[str, Dict[str, Any]],
                                           output_path: str = "mlperf_report.json") -> str:
    """
    Generate MLPerf compliance report.

    TODO: Compose _compile_report_data and _format_compliance_summary

    APPROACH:
    1. Compile structured data with self._compile_report_data(results)
    2. Save JSON report with json.dump
    3. Format summary with self._format_compliance_summary(report_data)
    4. Save summary markdown alongside JSON
    """
    ### BEGIN SOLUTION
    # Compile structured report data
    report_data = self._compile_report_data(results)

    # Save JSON report
    with open(output_path, 'w') as f:
        json.dump(report_data, f, indent=2)

    # Generate and save human-readable summary
    summary_text = self._format_compliance_summary(report_data)

    summary_path = output_path.replace('.json', '_summary.md')
    with open(summary_path, 'w') as f:
        f.write(summary_text)

    print(f"üìÑ MLPerf report saved to {output_path}")
    print(f"üìÑ Summary saved to {summary_path}")

    return summary_text
    ### END SOLUTION

MLPerf.generate_compliance_report = mlperf_generate_compliance_report

# %% [markdown]
"""
### üß™ Unit Test: MLPerf._compile_report_data

**What we're testing**: Structured data compilation from raw benchmark results
**Why it matters**: Correct data structure is the foundation for both JSON and markdown reports
**Expected**: Dict with benchmarks, summary, compliance stats
"""

# %% nbgrader={"grade": true, "grade_id": "test-tinymlperf-compile", "locked": true, "points": 3}
def test_unit_mlperf_compile_data():
    """üß™ Test MLPerf._compile_report_data implementation."""
    print("üß™ Unit Test: MLPerf._compile_report_data...")

    perf = MLPerf(random_seed=42)

    # Simulate results from run_standard_benchmark
    mock_results = {
        'keyword_spotting': {
            'accuracy': 0.92, 'mean_latency_ms': 50.0, 'p99_latency_ms': 80.0,
            'throughput_fps': 20.0, 'target_accuracy': 0.90, 'target_latency_ms': 100,
            'accuracy_met': True, 'latency_met': True, 'compliant': True,
            'model_name': 'test_model'
        }
    }

    report_data = perf._compile_report_data(mock_results)

    assert 'benchmarks' in report_data, "Should have 'benchmarks' key"
    assert 'summary' in report_data, "Should have 'summary' key"
    assert report_data['summary']['total_benchmarks'] == 1
    assert report_data['summary']['overall_compliant'] == True
    assert report_data['model_name'] == 'test_model'

    print("‚úÖ MLPerf._compile_report_data works correctly!")

if __name__ == "__main__":
    test_unit_mlperf_compile_data()

# %% [markdown]
"""
### üß™ Unit Test: MLPerf._format_compliance_summary

**What we're testing**: Markdown summary generation from structured report data
**Why it matters**: Human-readable reports are what engineers actually read
**Expected**: Markdown string with COMPLIANT/NON-COMPLIANT status and benchmark details
"""

# %% nbgrader={"grade": true, "grade_id": "test-tinymlperf-format", "locked": true, "points": 3}
def test_unit_mlperf_format_summary():
    """üß™ Test MLPerf._format_compliance_summary implementation."""
    print("üß™ Unit Test: MLPerf._format_compliance_summary...")

    perf = MLPerf(random_seed=42)

    report_data = {
        'model_name': 'test_model',
        'timestamp': '2025-01-01 00:00:00',
        'summary': {
            'total_benchmarks': 1, 'compliant_benchmarks': 1,
            'compliance_rate': 1.0, 'overall_compliant': True,
            'compliant_benchmark_names': ['keyword_spotting']
        },
        'benchmarks': {
            'keyword_spotting': {
                'accuracy': 0.92, 'mean_latency_ms': 50.0,
                'target_accuracy': 0.90, 'target_latency_ms': 100,
                'compliant': True
            }
        }
    }

    summary = perf._format_compliance_summary(report_data)

    assert isinstance(summary, str), f"Expected string, got {type(summary)}"
    assert "COMPLIANT" in summary, "Should contain compliance status"
    assert "keyword_spotting" in summary, "Should list benchmark names"
    assert "PASS" in summary, "Compliant benchmark should show PASS"

    print("‚úÖ MLPerf._format_compliance_summary works correctly!")

if __name__ == "__main__":
    test_unit_mlperf_format_summary()

# %% [markdown]
"""
### üß™ Unit Test: MLPerf (Full Class Integration)

This test validates our MLPerf class provides standardized benchmarking
with proper compliance reporting.

**What we're testing**: Industry-standard benchmark protocols and compliance reporting
**Why it matters**: Standardized benchmarks enable fair cross-system comparison
**Expected**: Proper metrics, compliance checking, and report generation
"""

# %% nbgrader={"grade": true, "grade_id": "test-tinymlperf", "locked": true, "points": 10}
def test_unit_mlperf():
    """üß™ Test MLPerf standardized benchmarking."""
    print("üß™ Unit Test: MLPerf...")

    # Create mock model for testing
    class MockModel:
        def __init__(self, name):
            self.name = name

        def forward(self, x):
            time.sleep(0.001)  # Simulate computation
            # Return appropriate output shape for different benchmarks
            if hasattr(x, 'shape'):
                if len(x.shape) == 2:  # Audio/sequence
                    return np.random.rand(2)  # Binary classification
                else:  # Image
                    return np.random.rand(10)  # Multi-class
            return np.random.rand(2)

    model = MockModel("test_model")
    perf = MLPerf(random_seed=42)

    # Test individual benchmark
    result = perf.run_standard_benchmark(model, 'keyword_spotting', num_runs=5)

    # Verify result structure
    required_keys = ['accuracy', 'mean_latency_ms', 'throughput_fps', 'compliant']
    assert all(key in result for key in required_keys)
    assert 0 <= result['accuracy'] <= 1
    assert result['mean_latency_ms'] > 0
    assert result['throughput_fps'] > 0

    # Test full benchmark suite (with fewer runs for speed)
    import tempfile
    with tempfile.TemporaryDirectory() as tmp_dir:
        # Run subset of benchmarks for testing
        subset_results = {}
        for benchmark in ['keyword_spotting', 'image_classification']:
            subset_results[benchmark] = perf.run_standard_benchmark(model, benchmark, num_runs=3)

        # Test compliance report generation
        report_path = f"{tmp_dir}/test_report.json"
        summary = perf.generate_compliance_report(subset_results, report_path)

        # Verify report was created
        assert Path(report_path).exists()
        assert "MLPerf Compliance Report" in summary
        assert "Compliance Rate" in summary

    print("‚úÖ MLPerf works correctly!")

if __name__ == "__main__":
    test_unit_mlperf()

# %% [markdown]
"""
## üîß Integration: Building Complete Benchmark Workflows

Now we'll integrate all our benchmarking components into complete workflows that demonstrate professional ML systems evaluation. This integration shows how to combine statistical rigor with practical insights.

The integration layer connects individual measurements into actionable engineering insights. This is where benchmarking becomes a decision-making tool rather than just data collection.

### Workflow Architecture

```
Integration Workflow Pipeline:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Model Variants  ‚îÇ    ‚îÇ Optimization    ‚îÇ    ‚îÇ Use Case        ‚îÇ
‚îÇ ‚Ä¢ Base model    ‚îÇ ‚Üí  ‚îÇ Techniques      ‚îÇ ‚Üí  ‚îÇ Analysis        ‚îÇ
‚îÇ ‚Ä¢ Quantized     ‚îÇ    ‚îÇ ‚Ä¢ Accuracy loss ‚îÇ    ‚îÇ ‚Ä¢ Mobile        ‚îÇ
‚îÇ ‚Ä¢ Pruned        ‚îÇ    ‚îÇ ‚Ä¢ Speed gain    ‚îÇ    ‚îÇ ‚Ä¢ Server        ‚îÇ
‚îÇ ‚Ä¢ Distilled     ‚îÇ    ‚îÇ ‚Ä¢ Memory save   ‚îÇ    ‚îÇ ‚Ä¢ Edge          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

This workflow helps answer questions like:
- "Which optimization gives the best accuracy/latency trade-off?"
- "What's the memory budget impact of each technique?"
- "Which model should I deploy for mobile vs server?"
"""

# %% [markdown]
"""
### Optimization Comparison Engine

Before implementing the comparison function, let's understand what makes optimization comparison challenging and valuable.

### Why Optimization Comparison is Complex

When you optimize a model, you're making trade-offs across multiple dimensions simultaneously:

```
Optimization Impact Matrix:
                   Accuracy    Latency    Memory    Energy
Quantization        -5%        +2.1x      +2.0x     +1.8x
Pruning            -2%        +1.4x      +3.2x     +1.3x
Knowledge Distill. -8%        +1.9x      +1.5x     +1.7x
```

The challenge: Which is "best"? It depends entirely on your deployment constraints.

### Multi-Objective Decision Framework

Our comparison engine implements a decision framework that:

1. **Measures all dimensions**: Don't optimize in isolation
2. **Calculates efficiency ratios**: Accuracy per MB, accuracy per ms
3. **Identifies Pareto frontiers**: Models that aren't dominated in all metrics
4. **Generates use-case recommendations**: Tailored to specific constraints

### Recommendation Algorithm

```
For each use case:
‚îú‚îÄ‚îÄ Latency-critical (real-time apps)
‚îÇ   ‚îî‚îÄ‚îÄ Optimize: min(latency) subject to accuracy > threshold
‚îú‚îÄ‚îÄ Memory-constrained (mobile/IoT)
‚îÇ   ‚îî‚îÄ‚îÄ Optimize: min(memory) subject to accuracy > threshold
‚îú‚îÄ‚îÄ Accuracy-preservation (quality-critical)
‚îÇ   ‚îî‚îÄ‚îÄ Optimize: max(accuracy) subject to latency < threshold
‚îî‚îÄ‚îÄ Balanced (general deployment)
    ‚îî‚îÄ‚îÄ Optimize: weighted combination of all factors
```

This principled approach ensures recommendations match real deployment needs.
"""

# %% [markdown]
"""
### _collect_base_metrics - Extracting Baseline Performance

This helper extracts the base model's mean performance across all metrics from
the benchmark results. It establishes the reference point for improvement calculations.
"""

# %% nbgrader={"grade": false, "grade_id": "benchmark-collect-base", "solution": true}
#| export
def _collect_base_metrics(base_name: str, benchmark_results: Dict) -> Dict[str, float]:
    """
    Extract base model metrics from benchmark results.

    TODO: Find the base model's mean value for each metric type

    APPROACH:
    1. Iterate over each metric type (latency, accuracy, memory, energy)
    2. Find the result whose key contains base_name
    3. Store result.mean in a dict keyed by metric type

    HINTS:
    - Use 'base_name in model_name' to match the base model
    """
    ### BEGIN SOLUTION
    base_metrics = {}
    for metric_type, results in benchmark_results.items():
        for model_name, result in results.items():
            if base_name in model_name:
                base_metrics[metric_type] = result.mean
                break
    return base_metrics
    ### END SOLUTION

# %% [markdown]
"""
### üß™ Unit Test: _collect_base_metrics

**What we're testing**: Extraction of base model's mean metrics from benchmark results
**Why it matters**: Accurate baselines are essential for meaningful improvement ratios
**Expected**: Dict with metric types as keys and mean values as floats
"""

# %% nbgrader={"grade": true, "grade_id": "test-collect-base", "locked": true, "points": 5}
def test_unit_collect_base_metrics():
    """üß™ Test _collect_base_metrics helper."""
    print("üß™ Unit Test: _collect_base_metrics...")

    # Simulate benchmark results
    mock_results = {
        'latency': {'base_latency_ms': BenchmarkResult('base_latency_ms', [10.0, 11.0, 12.0])},
        'accuracy': {'base_accuracy': BenchmarkResult('base_accuracy', [0.9, 0.91, 0.89])},
    }

    metrics = _collect_base_metrics('base', mock_results)
    assert 'latency' in metrics
    assert 'accuracy' in metrics
    assert abs(metrics['latency'] - 11.0) < 0.01
    assert abs(metrics['accuracy'] - 0.9) < 0.01

    print("‚úÖ _collect_base_metrics works correctly!")

if __name__ == "__main__":
    test_unit_collect_base_metrics()

# %% [markdown]
"""
### _calculate_improvements - Computing Speedup and Retention Ratios

This helper computes improvement ratios for each optimized model relative to
the baseline. For latency/memory/energy (lower is better), it calculates
base/optimized as the speedup factor. For accuracy, it calculates
optimized/base as the retention ratio.

```
Improvement Calculation:
Latency:  speedup = base_latency / opt_latency  (>1 means faster)
Memory:   speedup = base_memory / opt_memory     (>1 means smaller)
Accuracy: retention = opt_accuracy / base_accuracy (closer to 1 is better)
```
"""

# %% nbgrader={"grade": false, "grade_id": "benchmark-calc-improvements", "solution": true}
#| export
def _calculate_improvements(base_metrics: Dict[str, float], opt_metrics: Dict[str, float]) -> Dict[str, float]:
    """
    Calculate improvement ratios for an optimized model vs baseline.

    TODO: Compute speedup ratios for latency/memory/energy and retention for accuracy

    APPROACH:
    1. For latency, memory, energy: improvement = base / optimized
    2. For accuracy: retention = optimized / base
    3. Handle division by zero with fallback to 1.0

    HINTS:
    - Check opt_metrics[metric] > 0 before dividing
    - Use f'{metric_type}_speedup' as key names
    """
    ### BEGIN SOLUTION
    improvements = {}
    for metric_type in ['latency', 'memory', 'energy']:
        if metric_type in base_metrics and metric_type in opt_metrics:
            # For these metrics, lower is better, so improvement = base/optimized
            if opt_metrics[metric_type] > 0:
                improvements[f'{metric_type}_speedup'] = base_metrics[metric_type] / opt_metrics[metric_type]
            else:
                improvements[f'{metric_type}_speedup'] = 1.0

    if 'accuracy' in base_metrics and 'accuracy' in opt_metrics:
        # Accuracy retention (higher is better)
        improvements['accuracy_retention'] = opt_metrics['accuracy'] / base_metrics['accuracy']

    return improvements
    ### END SOLUTION

# %% [markdown]
"""
### üß™ Unit Test: _calculate_improvements

**What we're testing**: Improvement ratio calculations for all metric types
**Why it matters**: Correct ratios drive optimization recommendations
**Expected**: Speedup > 1 when optimized is better, retention near 1.0
"""

# %% nbgrader={"grade": true, "grade_id": "test-calc-improvements", "locked": true, "points": 5}
def test_unit_calculate_improvements():
    """üß™ Test _calculate_improvements helper."""
    print("üß™ Unit Test: _calculate_improvements...")

    base = {'latency': 10.0, 'memory': 100.0, 'accuracy': 0.90}
    opt = {'latency': 5.0, 'memory': 50.0, 'accuracy': 0.85}

    improvements = _calculate_improvements(base, opt)

    assert abs(improvements['latency_speedup'] - 2.0) < 0.01  # 10/5 = 2x
    assert abs(improvements['memory_speedup'] - 2.0) < 0.01   # 100/50 = 2x
    assert abs(improvements['accuracy_retention'] - 0.9444) < 0.01  # 0.85/0.90

    # Test with zero (edge case)
    opt_zero = {'latency': 0.0, 'memory': 50.0, 'accuracy': 0.85}
    imp_zero = _calculate_improvements(base, opt_zero)
    assert imp_zero['latency_speedup'] == 1.0  # Fallback

    print("‚úÖ _calculate_improvements works correctly!")

if __name__ == "__main__":
    test_unit_calculate_improvements()

# %% [markdown]
"""
### _generate_recommendations - Deployment-Specific Guidance

This helper analyzes improvement ratios across all optimized models to generate
recommendations for four deployment scenarios: latency-critical, memory-constrained,
accuracy-preservation, and balanced deployment.
"""

# %% nbgrader={"grade": false, "grade_id": "benchmark-gen-recs", "solution": true}
#| export
def _generate_recommendations(all_improvements: Dict[str, Dict[str, float]]) -> Dict[str, Dict]:
    """
    Generate deployment recommendations from improvement data.

    TODO: Find best model for each deployment scenario

    APPROACH:
    1. Track best latency, memory, accuracy, and overall scores
    2. For overall: weight speedups equally but accuracy retention at 5x
    3. Cap speedup at 5.0x to avoid outlier domination
    4. Return recommendation dict with model, reason, use_case

    HINTS:
    - Iterate over all_improvements items (opt_name -> improvements dict)
    - Overall score = (sum of capped speedups + accuracy_retention * 5) / count
    """
    ### BEGIN SOLUTION
    best_latency = None
    best_memory = None
    best_accuracy = None
    best_overall = None

    best_latency_score = 0
    best_memory_score = 0
    best_accuracy_score = 0
    best_overall_score = 0

    for opt_name, improvements in all_improvements.items():
        # Latency recommendation
        if 'latency_speedup' in improvements and improvements['latency_speedup'] > best_latency_score:
            best_latency_score = improvements['latency_speedup']
            best_latency = opt_name

        # Memory recommendation
        if 'memory_speedup' in improvements and improvements['memory_speedup'] > best_memory_score:
            best_memory_score = improvements['memory_speedup']
            best_memory = opt_name

        # Accuracy recommendation
        if 'accuracy_retention' in improvements and improvements['accuracy_retention'] > best_accuracy_score:
            best_accuracy_score = improvements['accuracy_retention']
            best_accuracy = opt_name

        # Overall balance (considering all factors)
        overall_score = 0
        count = 0
        for key, value in improvements.items():
            if 'speedup' in key:
                overall_score += min(value, 5.0)  # Cap speedup at 5x to avoid outliers
                count += 1
            elif 'retention' in key:
                overall_score += value * 5  # Weight accuracy retention heavily
                count += 1

        if count > 0:
            overall_score /= count
            if overall_score > best_overall_score:
                best_overall_score = overall_score
                best_overall = opt_name

    return {
        'for_latency_critical': {
            'model': best_latency,
            'reason': f"Best latency improvement: {best_latency_score:.2f}x faster",
            'use_case': "Real-time applications, edge devices with strict timing requirements"
        },
        'for_memory_constrained': {
            'model': best_memory,
            'reason': f"Best memory reduction: {best_memory_score:.2f}x smaller",
            'use_case': "Mobile devices, IoT sensors, embedded systems"
        },
        'for_accuracy_preservation': {
            'model': best_accuracy,
            'reason': f"Best accuracy retention: {best_accuracy_score:.1%} of original",
            'use_case': "Applications where quality cannot be compromised"
        },
        'for_balanced_deployment': {
            'model': best_overall,
            'reason': f"Best overall trade-off (score: {best_overall_score:.2f})",
            'use_case': "General production deployment with multiple constraints"
        }
    }
    ### END SOLUTION

# %% [markdown]
"""
### üß™ Unit Test: _generate_recommendations

**What we're testing**: Recommendation generation from improvement data
**Why it matters**: Correct recommendations guide deployment decisions
**Expected**: Four recommendation categories with appropriate model selections
"""

# %% nbgrader={"grade": true, "grade_id": "test-gen-recs", "locked": true, "points": 5}
def test_unit_generate_recommendations():
    """üß™ Test _generate_recommendations helper."""
    print("üß™ Unit Test: _generate_recommendations...")

    improvements = {
        'quantized': {'latency_speedup': 2.0, 'memory_speedup': 3.0, 'accuracy_retention': 0.95},
        'pruned': {'latency_speedup': 1.5, 'memory_speedup': 4.0, 'accuracy_retention': 0.98},
    }

    recs = _generate_recommendations(improvements)

    assert 'for_latency_critical' in recs
    assert 'for_memory_constrained' in recs
    assert 'for_accuracy_preservation' in recs
    assert 'for_balanced_deployment' in recs

    # Quantized has best latency speedup (2.0 > 1.5)
    assert recs['for_latency_critical']['model'] == 'quantized'
    # Pruned has best memory speedup (4.0 > 3.0)
    assert recs['for_memory_constrained']['model'] == 'pruned'
    # Pruned has best accuracy retention (0.98 > 0.95)
    assert recs['for_accuracy_preservation']['model'] == 'pruned'

    print("‚úÖ _generate_recommendations works correctly!")

if __name__ == "__main__":
    test_unit_generate_recommendations()

# %% [markdown]
"""
### analyze_optimization_techniques - Composition Function

This is the main entry point that composes `_collect_base_metrics`,
`_calculate_improvements`, and `_generate_recommendations` into a complete
optimization comparison workflow.

```
analyze_optimization_techniques Pipeline:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Run Full   ‚îÇ ‚îÄ‚îÄ> ‚îÇ _collect_base_metrics‚îÇ ‚îÄ‚îÄ> ‚îÇ For each opt model: ‚îÇ
‚îÇ Benchmark  ‚îÇ     ‚îÇ (extract baseline)   ‚îÇ     ‚îÇ _calculate_improvements‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                          ‚Üì
                                               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                               ‚îÇ_generate_recommendations‚îÇ
                                               ‚îÇ (deploy guidance)    ‚îÇ
                                               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```
"""

# %% nbgrader={"grade": false, "grade_id": "benchmark-comparison", "solution": true}
#| export
def analyze_optimization_techniques(base_model: Any, optimized_models: List[Any],
                                  datasets: List[Any]) -> Dict[str, Any]:
    """
    Compare base model against various optimization techniques.

    TODO: Compose helpers to run benchmarks, calculate improvements, generate recommendations

    APPROACH:
    1. Run BenchmarkSuite on [base_model] + optimized_models
    2. Use _collect_base_metrics() for baseline
    3. Use _calculate_improvements() for each optimized model
    4. Use _generate_recommendations() for deployment guidance
    5. Print summary and return results

    Args:
        base_model: Baseline model (unoptimized)
        optimized_models: List of models with different optimizations applied
        datasets: List of datasets for evaluation

    Returns:
        Dictionary with 'base_metrics', 'optimized_results', 'improvements', 'recommendations'

    EXAMPLE:
    >>> results = analyze_optimization_techniques(base_model, [quant, pruned], datasets)
    >>> print(results['recommendations'])
    """
    ### BEGIN SOLUTION
    all_models = [base_model] + optimized_models
    suite = BenchmarkSuite(all_models, datasets)

    print("üß™ Running optimization comparison benchmark...")
    benchmark_results = suite.run_full_benchmark()

    # Extract base model performance using helper
    base_name = getattr(base_model, 'name', 'model_0')
    base_metrics = _collect_base_metrics(base_name, benchmark_results)

    # Initialize comparison results
    comparison_results = {
        'base_model': base_name,
        'base_metrics': base_metrics,
        'optimized_results': {},
        'improvements': {},
        'efficiency_metrics': {},
        'recommendations': {}
    }

    for opt_model in optimized_models:
        opt_name = getattr(opt_model, 'name', f'optimized_model_{len(comparison_results["optimized_results"])}')

        # Find results for this optimized model
        opt_metrics = {}
        for metric_type, results in benchmark_results.items():
            for model_name, result in results.items():
                if opt_name in model_name:
                    opt_metrics[metric_type] = result.mean
                    break

        comparison_results['optimized_results'][opt_name] = opt_metrics

        # Calculate improvements using helper
        improvements = _calculate_improvements(base_metrics, opt_metrics)
        comparison_results['improvements'][opt_name] = improvements

        # Calculate efficiency metrics
        efficiency = {}
        if 'accuracy' in opt_metrics:
            if 'memory' in opt_metrics and opt_metrics['memory'] > 0:
                efficiency['accuracy_per_mb'] = opt_metrics['accuracy'] / opt_metrics['memory']
            if 'latency' in opt_metrics and opt_metrics['latency'] > 0:
                efficiency['accuracy_per_ms'] = opt_metrics['accuracy'] / opt_metrics['latency']

        comparison_results['efficiency_metrics'][opt_name] = efficiency

    # Generate recommendations using helper
    recommendations = _generate_recommendations(comparison_results['improvements'])
    comparison_results['recommendations'] = recommendations

    # Print summary
    print("\nüìä Optimization Comparison Results:")
    print("=" * 50)

    for opt_name, improvements in comparison_results['improvements'].items():
        print(f"\n{opt_name}:")
        for metric, value in improvements.items():
            if 'speedup' in metric:
                print(f"  {metric}: {value:.2f}x improvement")
            elif 'retention' in metric:
                print(f"  {metric}: {value:.1%}")

    print("\nüéØ Recommendations:")
    for use_case, rec in recommendations.items():
        if rec['model']:
            print(f"  {use_case}: {rec['model']} - {rec['reason']}")

    return comparison_results
    ### END SOLUTION

# %% [markdown]
"""
### üß™ Unit Test: analyze_optimization_techniques (Full Integration)

This test validates the complete optimization comparison workflow generates
useful recommendations from benchmark data.

**What we're testing**: Multi-model comparison with recommendation generation
**Why it matters**: Guides engineers to choose the right optimization for their use case
**Expected**: Valid comparisons and actionable recommendations
"""

# %% nbgrader={"grade": true, "grade_id": "test-optimization-comparison", "locked": true, "points": 10}
def test_unit_optimization_comparison():
    """üß™ Test optimization comparison functionality."""
    print("üß™ Unit Test: analyze_optimization_techniques...")

    # Create mock models with different characteristics
    class MockModel:
        def __init__(self, name, latency_factor=1.0, accuracy_factor=1.0, memory_factor=1.0):
            self.name = name
            self.latency_factor = latency_factor
            self.accuracy_factor = accuracy_factor
            self.memory_factor = memory_factor

        def forward(self, x):
            time.sleep(0.001 * self.latency_factor)
            return x

    # Base model and optimized variants
    base_model = MockModel("base_model", latency_factor=1.0, accuracy_factor=1.0, memory_factor=1.0)
    quantized_model = MockModel("quantized_model", latency_factor=0.7, accuracy_factor=0.95, memory_factor=0.5)
    pruned_model = MockModel("pruned_model", latency_factor=0.8, accuracy_factor=0.98, memory_factor=0.3)

    datasets = [{"test": "data"}]

    # Run comparison
    results = analyze_optimization_techniques(base_model, [quantized_model, pruned_model], datasets)

    # Verify results structure
    assert 'base_model' in results
    assert 'optimized_results' in results
    assert 'improvements' in results
    assert 'recommendations' in results

    # Verify improvements were calculated
    assert len(results['improvements']) == 2  # Two optimized models

    # Verify recommendations were generated
    recommendations = results['recommendations']
    assert 'for_latency_critical' in recommendations
    assert 'for_memory_constrained' in recommendations
    assert 'for_accuracy_preservation' in recommendations
    assert 'for_balanced_deployment' in recommendations

    print("‚úÖ analyze_optimization_techniques works correctly!")

if __name__ == "__main__":
    test_unit_optimization_comparison()

# %% [markdown]
"""
## üìä Systems Analysis: Benchmark Variance and Optimization Trade-offs

Let's understand the key systems concept of measurement variance and optimization trade-offs.
"""

# %%
def analyze_benchmark_variance():
    """üìä Analyze measurement variance and confidence intervals."""
    print("üìä Analyzing Benchmark Variance")
    print("=" * 60)

    # Simulate benchmarking with different sample sizes
    sample_sizes = [5, 10, 20, 50, 100]
    true_latency = 10.0  # True mean latency in ms
    noise_std = 1.5  # Standard deviation of measurement noise

    print("Effect of Sample Size on Confidence Interval Width:\n")
    print(f"{'Samples':<10} {'Mean (ms)':<15} {'CI Width (ms)':<15} {'Relative Error':<15}")
    print("-" * 60)

    for n_samples in sample_sizes:
        # Simulate measurements
        measurements = np.random.normal(true_latency, noise_std, n_samples)
        mean_latency = np.mean(measurements)
        std_latency = np.std(measurements)

        # Calculate 95% confidence interval
        t_score = 1.96
        margin_error = t_score * (std_latency / np.sqrt(n_samples))
        ci_width = 2 * margin_error
        relative_error = ci_width / mean_latency * 100

        print(f"{n_samples:<10} {mean_latency:<15.2f} {ci_width:<15.2f} {relative_error:<15.1f}%")

    print("\nüí° Key Insights:")
    print("   ‚Ä¢ More samples reduce confidence interval width")
    print("   ‚Ä¢ CI width decreases with ‚àön (diminishing returns)")
    print("   ‚Ä¢ 20-50 samples typically sufficient for <10% error")
    print("   ‚Ä¢ Statistical rigor requires measuring variance, not just mean")

if __name__ == "__main__":
    analyze_benchmark_variance()

# %%
def analyze_optimization_tradeoffs():
    """üìä Analyze trade-offs between different optimization techniques."""
    print("\nüìä Analyzing Optimization Trade-offs")
    print("=" * 60)

    # Simulated optimization results
    optimizations = {
        'Baseline': {'accuracy': 0.89, 'latency_ms': 45, 'memory_mb': 12, 'energy_j': 2.0},
        'Quantization (INT8)': {'accuracy': 0.88, 'latency_ms': 30, 'memory_mb': 3, 'energy_j': 1.3},
        'Pruning (70%)': {'accuracy': 0.87, 'latency_ms': 35, 'memory_mb': 4, 'energy_j': 1.5},
        'Both (INT8 + 70%)': {'accuracy': 0.85, 'latency_ms': 22, 'memory_mb': 1, 'energy_j': 0.9},
    }

    # Calculate efficiency metrics
    print("\nEfficiency Metrics (higher is better):\n")
    print(f"{'Technique':<25} {'Acc/MB':<12} {'Acc/ms':<12} {'Acc/J':<12}")
    print("-" * 60)

    baseline = optimizations['Baseline']

    for name, metrics in optimizations.items():
        acc_per_mb = metrics['accuracy'] / metrics['memory_mb']
        acc_per_ms = metrics['accuracy'] / metrics['latency_ms']
        acc_per_j = metrics['accuracy'] / metrics['energy_j']

        print(f"{name:<25} {acc_per_mb:<12.3f} {acc_per_ms:<12.4f} {acc_per_j:<12.3f}")

    print("\nPareto Frontier Analysis:")
    print("   ‚Ä¢ Quantization: Best memory efficiency (0.293 acc/MB)")
    print("   ‚Ä¢ Pruning: Balanced trade-off")
    print("   ‚Ä¢ Combined: Maximum resource efficiency, highest accuracy loss")

    print("\nüí° Key Insights:")
    print("   ‚Ä¢ No single optimization dominates all metrics")
    print("   ‚Ä¢ Combined optimizations compound benefits and risks")
    print("   ‚Ä¢ Choose based on deployment constraints (memory vs speed vs accuracy)")
    print("   ‚Ä¢ Pareto frontier reveals non-dominated solutions")

if __name__ == "__main__":
    analyze_optimization_tradeoffs()

# %% [markdown]
"""
## üìä MLPerf Principles - Industry-Standard Benchmarking

MLPerf (created by MLCommons) is the industry-standard ML benchmarking framework. Understanding these principles grounds your capstone competition in professional methodology.

### Core Principles

**Reproducibility:** Fixed hardware specs, software versions, random seeds, and multiple runs for statistical validity.

**Standardization:** Fixed models and datasets enable fair comparison. MLPerf has two divisions:
- **Closed:** Same models/datasets, optimize systems (hardware/software)
- **Open:** Modify models/algorithms, show innovation

**MLPerf:** Edge device benchmarks (<1MB models, <100ms latency, <10mW power) that inspire the capstone.

### Key Takeaways

1. Document everything for reproducibility
2. Use same baseline for fair comparison
3. Measure multiple metrics (accuracy, latency, memory, energy)
4. Optimize for real deployment constraints

The capstone project follows MLPerf-style principles!
"""

# %% [markdown]
"""
## üìä Combination Strategies

Strategic optimization combines multiple techniques for different performance goals. The order matters: quantize-then-prune may preserve accuracy better, while prune-then-quantize may be faster.

### Ablation Studies

Professional ML engineers use ablation studies to understand each optimization's contribution:

```
Baseline:           Accuracy: 89%, Latency: 45ms, Memory: 12MB
+ Quantization:     Accuracy: 88%, Latency: 30ms, Memory: 3MB   (Œî: -1%, -33%, -75%)
+ Pruning:          Accuracy: 87%, Latency: 22ms, Memory: 2MB   (Œî: -1%, -27%, -33%)
+ Kernel Fusion:    Accuracy: 87%, Latency: 18ms, Memory: 2MB   (Œî: 0%, -18%, 0%)
```

You'll apply these strategies with specific optimization targets in Module 20's capstone project.
"""

# %% [markdown]
"""
## üß™ Module Integration Test

Final validation that our complete benchmarking system works correctly and integrates properly with all TinyTorch components.

This comprehensive test validates the entire benchmarking ecosystem and ensures it's ready for production use in the final capstone project.
"""

# %% nbgrader={"grade": true, "grade_id": "test-module", "locked": true, "points": 10}
def test_module():
    """üß™ Module Test: Complete Integration

    Comprehensive test of entire benchmarking module functionality.

    This final test runs before module summary to ensure:
    - All benchmarking components work together correctly
    - Statistical analysis provides reliable results
    - Integration with optimization modules functions properly
    - Professional reporting generates actionable insights
    """
    print("üß™ RUNNING MODULE INTEGRATION TEST")
    print("=" * 50)

    # Run all unit tests
    print("Running unit tests...")
    test_unit_benchmark_result()
    test_unit_precise_timer()
    test_unit_benchmark_init()
    test_unit_benchmark_latency()
    test_unit_benchmark_accuracy()
    test_unit_benchmark_memory()
    test_unit_benchmark()
    test_unit_benchsuite_init()
    test_unit_benchsuite_run()
    test_unit_benchsuite_energy()
    test_unit_benchsuite_plot()
    test_unit_benchsuite_format_results()
    test_unit_benchsuite_format_recs()
    test_unit_benchmark_suite()
    test_unit_mlperf_init()
    test_unit_mlperf_latency()
    test_unit_extract_pred_array()
    test_unit_mlperf_accuracy()
    test_unit_mlperf_run()
    test_unit_mlperf_compile_data()
    test_unit_mlperf_format_summary()
    test_unit_mlperf()
    test_unit_collect_base_metrics()
    test_unit_calculate_improvements()
    test_unit_generate_recommendations()
    test_unit_optimization_comparison()

    print("\nRunning integration scenarios...")

    # Test realistic benchmarking workflow
    print("üß™ Integration Test: Complete benchmarking workflow...")

    # Create realistic test models
    class RealisticModel:
        def __init__(self, name, characteristics):
            self.name = name
            self.characteristics = characteristics

        def forward(self, x):
            # Simulate different model behaviors
            base_time = self.characteristics.get('base_latency', 0.001)
            variance = self.characteristics.get('variance', 0.0001)
            memory_factor = self.characteristics.get('memory_factor', 1.0)

            # Simulate realistic computation
            time.sleep(max(0, base_time + np.random.normal(0, variance)))

            # Simulate memory usage
            if hasattr(x, 'shape'):
                temp_size = int(np.prod(x.shape) * memory_factor)
                temp_data = np.random.randn(temp_size)
                _ = np.sum(temp_data)  # Use the data

            return x

        def evaluate(self, dataset):
            # Simulate evaluation
            base_acc = self.characteristics.get('base_accuracy', 0.85)
            return base_acc + np.random.normal(0, 0.02)

        def parameters(self):
            # Simulate parameter count - return Tensor objects for compatibility
            from tinytorch.core.tensor import Tensor
            param_count = self.characteristics.get('param_count', 1000000)
            return [Tensor(np.random.randn(param_count))]

    # Create test model suite
    models = [
        RealisticModel("efficient_model", {
            'base_latency': 0.001,
            'base_accuracy': 0.82,
            'memory_factor': 0.5,
            'param_count': 500000
        }),
        RealisticModel("accurate_model", {
            'base_latency': 0.003,
            'base_accuracy': 0.95,
            'memory_factor': 2.0,
            'param_count': 2000000
        }),
        RealisticModel("balanced_model", {
            'base_latency': 0.002,
            'base_accuracy': 0.88,
            'memory_factor': 1.0,
            'param_count': 1000000
        })
    ]

    datasets = [{"test_data": f"dataset_{i}"} for i in range(3)]

    # Test 1: Comprehensive benchmark suite
    print("  Testing comprehensive benchmark suite...")
    suite = BenchmarkSuite(models, datasets)
    results = suite.run_full_benchmark()

    assert 'latency' in results
    assert 'accuracy' in results
    assert 'memory' in results
    assert 'energy' in results

    # Verify all models were tested
    for result_type in results.values():
        assert len(result_type) == len(models)

    # Test 2: Statistical analysis
    print("  Testing statistical analysis...")
    for result_type, model_results in results.items():
        for model_name, result in model_results.items():
            assert isinstance(result, BenchmarkResult)
            assert result.count > 0
            assert result.std >= 0
            assert result.ci_lower <= result.mean <= result.ci_upper

    # Test 3: Report generation
    print("  Testing report generation...")
    report = suite.generate_report()
    assert "Benchmark Report" in report
    assert "System Information" in report
    assert "Recommendations" in report

    # Test 4: MLPerf compliance
    print("  Testing MLPerf compliance...")
    perf = MLPerf(random_seed=42)
    perf_results = perf.run_standard_benchmark(models[0], 'keyword_spotting', num_runs=5)

    required_keys = ['accuracy', 'mean_latency_ms', 'compliant', 'target_accuracy']
    assert all(key in perf_results for key in required_keys)
    assert 0 <= perf_results['accuracy'] <= 1
    assert perf_results['mean_latency_ms'] > 0

    # Test 5: Optimization comparison
    print("  Testing optimization comparison...")
    comparison_results = analyze_optimization_techniques(
        models[0], models[1:], datasets[:1]
    )

    assert 'base_model' in comparison_results
    assert 'improvements' in comparison_results
    assert 'recommendations' in comparison_results
    assert len(comparison_results['improvements']) == 2

    # Test 6: Cross-platform compatibility
    print("  Testing cross-platform compatibility...")
    system_info = {
        'platform': platform.platform(),
        'processor': platform.processor(),
        'python_version': platform.python_version()
    }

    # Verify system information is captured
    benchmark = Benchmark(models[:1], datasets[:1])
    assert all(key in benchmark.system_info for key in system_info.keys())

    print("‚úÖ End-to-end benchmarking workflow works!")

    print("\n" + "=" * 50)
    print("üéâ ALL TESTS PASSED! Module ready for export.")
    print("Run: tito module complete 19")

# %% [markdown]
"""
## ü§î ML Systems Reflection Questions

Answer these to deepen your understanding of benchmarking and performance engineering:

### 1. Statistical Confidence in Measurements
You implemented BenchmarkResult with confidence intervals for measurements.
If you run 20 trials and get mean latency 5.2ms with std dev 0.8ms:
- What's the 95% confidence interval for the true mean? [_____ ms, _____ ms]
- How many more trials would you need to halve the confidence interval width? _____ total trials

### 2. Measurement Overhead Analysis
Your precise_timer context manager has microsecond precision, but models run for milliseconds.
For a model that takes 1ms to execute:
- If timer overhead is 10Œºs, what's the relative error? _____%
- At what model latency does timer overhead become negligible (<1%)? _____ ms

### 3. Benchmark Configuration Trade-offs
Your optimize_benchmark_configuration() function tested different warmup/measurement combinations.
For a CI/CD pipeline that runs 100 benchmarks per day:
- Fast config (3s each): _____ minutes total daily
- Accurate config (15s each): _____ minutes total daily
- What's the key trade-off you're making? [accuracy/precision/development velocity]

### 4. MLPerf Compliance Metrics
You implemented MLPerf-style standardized benchmarks with target thresholds.
If a model achieves 89% accuracy (target: 90%) and 120ms latency (target: <100ms):
- Is it compliant? [Yes/No] _____
- Which constraint is more critical for edge deployment? [accuracy/latency]
- How would you prioritize optimization? [accuracy first/latency first/balanced]

### 5. Optimization Comparison Analysis
Your analyze_optimization_techniques() generates recommendations for different use cases.
Given three optimized models:
- Quantized: 0.8√ó memory, 2√ó speed, 0.95√ó accuracy
- Pruned: 0.3√ó memory, 1.5√ó speed, 0.98√ó accuracy
- Distilled: 0.6√ó memory, 1.8√ó speed, 0.92√ó accuracy

For a mobile app with 50MB model size limit and <100ms latency requirement:
- Which optimization offers best memory reduction? _____
- Which balances all constraints best? _____
- What's the key insight about optimization trade-offs? [no free lunch/specialization wins/measurement guides decisions]
"""

# %% [markdown]
"""
## ‚≠ê Aha Moment: Measurement Enables Optimization

**What you built:** A benchmarking system with warmup, statistics, and reproducibility.

**Why it matters:** "Premature optimization is the root of all evil"‚Äîbut you can't optimize
without measuring! Your benchmarking system produces reliable, comparable numbers: warmup
iterations eliminate cold-start effects, multiple runs give confidence intervals.

This is how production ML teams make decisions: measure, compare, improve, repeat.
"""

# %%
def demo_benchmarking():
    """üéØ See professional benchmarking in action."""
    print("üéØ AHA MOMENT: Measurement Enables Optimization")
    print("=" * 45)

    # Create a simple model and input
    layer = Linear(512, 256)
    x = Tensor(np.random.randn(32, 512))

    # Benchmark with proper methodology
    benchmark = Benchmark(
        models=[layer],
        datasets=[(x, None)],
        warmup_runs=3,
        measurement_runs=10
    )

    results = benchmark.run_latency_benchmark(input_shape=(32, 512))
    result = list(results.values())[0]

    print(f"Model: Linear(512 ‚Üí 256)")
    print(f"Batch: 32 samples")
    print(f"\nBenchmark Results (10 iterations):")
    print(f"  Mean latency: {result.mean*1000:.2f} ms")
    print(f"  Std dev:      {result.std*1000:.2f} ms")
    print(f"  Min:          {result.min_val*1000:.2f} ms")
    print(f"  Max:          {result.max_val*1000:.2f} ms")

    print("\n‚ú® Reliable measurements guide optimization decisions!")

# %%
if __name__ == "__main__":
    test_module()
    print("\n")
    demo_benchmarking()

# %% [markdown]
"""
## üöÄ MODULE SUMMARY: Benchmarking

Congratulations! You've built a professional benchmarking system that rivals industry-standard evaluation frameworks!

### Key Accomplishments
- Built comprehensive benchmarking infrastructure with BenchmarkResult, Benchmark, and BenchmarkSuite classes
- Implemented statistical rigor with confidence intervals, variance analysis, and measurement optimization
- Created MLPerf-style standardized benchmarks for reproducible cross-system comparison
- Developed optimization comparison workflows that generate actionable recommendations
- All tests pass ‚úÖ (validated by `test_module()`)

### Systems Engineering Insights Gained
- **Measurement Science**: Statistical significance requires proper sample sizes and variance control
- **Benchmark Design**: Standardized protocols enable fair comparison across different systems
- **Trade-off Analysis**: Pareto frontiers reveal optimization opportunities and constraints
- **Production Integration**: Automated reporting transforms measurements into engineering decisions

### Ready for Systems Capstone
Your benchmarking implementation enables comprehensive systems evaluation, demonstrating your complete optimization toolkit. This is where all 19 modules come together!

Export with: `tito module complete 19`

**Next**: Milestone 5 (Systems Capstone) will demonstrate the complete ML systems engineering workflow!
"""


--- tinytorch/src/19_benchmarking/ABOUT.md ---
---
file_format: mystnb
kernelspec:
  name: python3
---

# Module 19: Benchmarking

:::{admonition} Module Info
:class: note

**OPTIMIZATION TIER** | Difficulty: ‚óè‚óè‚óè‚óã | Time: 5-7 hours | Prerequisites: 01-18

This module assumes familiarity with the complete TinyTorch stack (Modules 01-13), profiling (Module 14), and optimization techniques (Modules 15-18). You should understand how to build, profile, and optimize models before tackling systematic benchmarking and statistical comparison of optimizations.
:::

`````{only} html
````{grid} 1 1 3 3
:gutter: 3

```{grid-item-card} üéß Audio Overview

Listen to an AI-generated overview.

<audio controls style="width: 100%; height: 54px; margin-top: auto;">
  <source src="https://github.com/harvard-edge/cs249r_book/releases/download/tinytorch-audio-v0.1.1/19_benchmarking.mp3" type="audio/mpeg">
</audio>
```

```{grid-item-card} üöÄ Launch Binder

Run interactively in your browser.

<a href="https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?labpath=tinytorch%2Fmodules%2F19_benchmarking%2Fbenchmarking.ipynb" target="_blank" style="display: flex; align-items: center; justify-content: center; width: 100%; height: 54px; margin-top: auto; background: #f97316; color: white; text-align: center; text-decoration: none; border-radius: 27px; font-size: 14px; box-sizing: border-box;">Open in Binder ‚Üí</a>
```

```{grid-item-card} üìÑ View Source

Browse the source code on GitHub.

<a href="https://github.com/harvard-edge/cs249r_book/blob/main/tinytorch/src/19_benchmarking/19_benchmarking.py" target="_blank" style="display: flex; align-items: center; justify-content: center; width: 100%; height: 54px; margin-top: auto; background: #14b8a6; color: white; text-align: center; text-decoration: none; border-radius: 27px; font-size: 14px; box-sizing: border-box;">View on GitHub ‚Üí</a>
```

````

```{raw} html
<style>
.slide-viewer-container {
  margin: 0.5rem 0 1.5rem 0;
  background: #0f172a;
  border-radius: 1rem;
  overflow: hidden;
  box-shadow: 0 4px 20px rgba(0,0,0,0.15);
}
.slide-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: 0.6rem 1rem;
  background: rgba(255,255,255,0.03);
}
.slide-title {
  display: flex;
  align-items: center;
  gap: 0.5rem;
  color: #94a3b8;
  font-weight: 500;
  font-size: 0.85rem;
}
.slide-subtitle {
  color: #64748b;
  font-weight: 400;
  font-size: 0.75rem;
}
.slide-toolbar {
  display: flex;
  align-items: center;
  gap: 0.375rem;
}
.slide-toolbar button {
  background: transparent;
  border: none;
  color: #64748b;
  width: 32px;
  height: 32px;
  border-radius: 0.375rem;
  cursor: pointer;
  font-size: 1.1rem;
  transition: all 0.15s;
  display: flex;
  align-items: center;
  justify-content: center;
}
.slide-toolbar button:hover {
  background: rgba(249, 115, 22, 0.15);
  color: #f97316;
}
.slide-nav-group {
  display: flex;
  align-items: center;
}
.slide-page-info {
  color: #64748b;
  font-size: 0.75rem;
  padding: 0 0.5rem;
  font-weight: 500;
}
.slide-zoom-group {
  display: flex;
  align-items: center;
  margin-left: 0.25rem;
  padding-left: 0.5rem;
  border-left: 1px solid rgba(255,255,255,0.1);
}
.slide-canvas-wrapper {
  display: flex;
  justify-content: center;
  align-items: center;
  padding: 0.5rem 1rem 1rem 1rem;
  min-height: 380px;
  background: #0f172a;
}
.slide-canvas {
  max-width: 100%;
  max-height: 350px;
  height: auto;
  border-radius: 0.5rem;
  box-shadow: 0 4px 24px rgba(0,0,0,0.4);
}
.slide-progress-wrapper {
  padding: 0 1rem 0.5rem 1rem;
}
.slide-progress-bar {
  height: 3px;
  background: rgba(255,255,255,0.08);
  border-radius: 1.5px;
  overflow: hidden;
  cursor: pointer;
}
.slide-progress-fill {
  height: 100%;
  background: #f97316;
  border-radius: 1.5px;
  transition: width 0.2s ease;
}
.slide-loading {
  color: #f97316;
  font-size: 0.9rem;
  display: flex;
  align-items: center;
  gap: 0.5rem;
}
.slide-loading::before {
  content: '';
  width: 18px;
  height: 18px;
  border: 2px solid rgba(249, 115, 22, 0.2);
  border-top-color: #f97316;
  border-radius: 50%;
  animation: slide-spin 0.8s linear infinite;
}
@keyframes slide-spin {
  to { transform: rotate(360deg); }
}
.slide-footer {
  display: flex;
  justify-content: center;
  gap: 0.5rem;
  padding: 0.6rem 1rem;
  background: rgba(255,255,255,0.02);
  border-top: 1px solid rgba(255,255,255,0.05);
}
.slide-footer a {
  display: inline-flex;
  align-items: center;
  gap: 0.375rem;
  background: #f97316;
  color: white;
  padding: 0.4rem 0.9rem;
  border-radius: 2rem;
  text-decoration: none;
  font-weight: 500;
  font-size: 0.75rem;
  transition: all 0.15s;
}
.slide-footer a:hover {
  background: #ea580c;
  color: white;
}
.slide-footer a.secondary {
  background: transparent;
  color: #94a3b8;
  border: 1px solid rgba(255,255,255,0.15);
}
.slide-footer a.secondary:hover {
  background: rgba(255,255,255,0.05);
  color: #f8fafc;
}
@media (max-width: 600px) {
  .slide-header { flex-direction: column; gap: 0.5rem; padding: 0.5rem 0.75rem; }
  .slide-toolbar button { width: 28px; height: 28px; }
  .slide-canvas-wrapper { min-height: 260px; padding: 0.5rem; }
  .slide-canvas { max-height: 220px; }
}
</style>

<div class="slide-viewer-container" id="slide-viewer-19_benchmarking">
  <div class="slide-header">
    <div class="slide-title">
      <span>üî•</span>
      <span>Slide Deck</span>
      <span class="slide-subtitle">¬∑ AI-generated</span>
    </div>
    <div class="slide-toolbar">
      <div class="slide-nav-group">
        <button onclick="slideNav('19_benchmarking', -1)" title="Previous">‚Äπ</button>
        <span class="slide-page-info"><span id="slide-num-19_benchmarking">1</span> / <span id="slide-count-19_benchmarking">-</span></span>
        <button onclick="slideNav('19_benchmarking', 1)" title="Next">‚Ä∫</button>
      </div>
      <div class="slide-zoom-group">
        <button onclick="slideZoom('19_benchmarking', -0.25)" title="Zoom out">‚àí</button>
        <button onclick="slideZoom('19_benchmarking', 0.25)" title="Zoom in">+</button>
      </div>
    </div>
  </div>
  <div class="slide-canvas-wrapper">
    <div id="slide-loading-19_benchmarking" class="slide-loading">Loading slides...</div>
    <canvas id="slide-canvas-19_benchmarking" class="slide-canvas" style="display:none;"></canvas>
  </div>
  <div class="slide-progress-wrapper">
    <div class="slide-progress-bar" onclick="slideProgress('19_benchmarking', event)">
      <div class="slide-progress-fill" id="slide-progress-19_benchmarking" style="width: 0%;"></div>
    </div>
  </div>
  <div class="slide-footer">
    <a href="../_static/slides/19_benchmarking.pdf" download>‚¨á Download</a>
    <a href="#" onclick="slideFullscreen('19_benchmarking'); return false;" class="secondary">‚õ∂ Fullscreen</a>
  </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.min.js"></script>
<script>
(function() {
  if (window.slideViewersInitialized) return;
  window.slideViewersInitialized = true;

  pdfjsLib.GlobalWorkerOptions.workerSrc = 'https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.worker.min.js';

  window.slideViewers = {};

  window.initSlideViewer = function(id, pdfUrl) {
    const viewer = { pdf: null, page: 1, scale: 1.3, rendering: false, pending: null };
    window.slideViewers[id] = viewer;

    const canvas = document.getElementById('slide-canvas-' + id);
    const ctx = canvas.getContext('2d');

    function render(num) {
      viewer.rendering = true;
      viewer.pdf.getPage(num).then(function(page) {
        const viewport = page.getViewport({scale: viewer.scale});
        canvas.height = viewport.height;
        canvas.width = viewport.width;
        page.render({canvasContext: ctx, viewport: viewport}).promise.then(function() {
          viewer.rendering = false;
          if (viewer.pending !== null) { render(viewer.pending); viewer.pending = null; }
        });
      });
      document.getElementById('slide-num-' + id).textContent = num;
      document.getElementById('slide-progress-' + id).style.width = (num / viewer.pdf.numPages * 100) + '%';
    }

    function queue(num) { if (viewer.rendering) viewer.pending = num; else render(num); }

    pdfjsLib.getDocument(pdfUrl).promise.then(function(pdf) {
      viewer.pdf = pdf;
      document.getElementById('slide-count-' + id).textContent = pdf.numPages;
      document.getElementById('slide-loading-' + id).style.display = 'none';
      canvas.style.display = 'block';
      render(1);
    }).catch(function() {
      document.getElementById('slide-loading-' + id).innerHTML = 'Unable to load. <a href="' + pdfUrl + '" style="color:#f97316;">Download PDF</a>';
    });

    viewer.queue = queue;
  };

  window.slideNav = function(id, dir) {
    const v = window.slideViewers[id];
    if (!v || !v.pdf) return;
    const newPage = v.page + dir;
    if (newPage >= 1 && newPage <= v.pdf.numPages) { v.page = newPage; v.queue(newPage); }
  };

  window.slideZoom = function(id, delta) {
    const v = window.slideViewers[id];
    if (!v) return;
    v.scale = Math.max(0.5, Math.min(3, v.scale + delta));
    v.queue(v.page);
  };

  window.slideProgress = function(id, event) {
    const v = window.slideViewers[id];
    if (!v || !v.pdf) return;
    const bar = event.currentTarget;
    const pct = (event.clientX - bar.getBoundingClientRect().left) / bar.offsetWidth;
    const newPage = Math.max(1, Math.min(v.pdf.numPages, Math.ceil(pct * v.pdf.numPages)));
    if (newPage !== v.page) { v.page = newPage; v.queue(newPage); }
  };

  window.slideFullscreen = function(id) {
    const el = document.getElementById('slide-viewer-' + id);
    if (el.requestFullscreen) el.requestFullscreen();
    else if (el.webkitRequestFullscreen) el.webkitRequestFullscreen();
  };
})();

initSlideViewer('19_benchmarking', '../_static/slides/19_benchmarking.pdf');
</script>
```
`````

## Overview

Benchmarking transforms performance optimization from guesswork into engineering discipline. You have learned individual optimization techniques in Modules 14-18, but how do you know which optimizations actually work? How do you compare a quantized model against a pruned one? How do you ensure your measurements are statistically valid rather than random noise?

In this module, you will build the infrastructure that powers TorchPerf Olympics, the capstone competition framework. You will implement professional benchmarking tools that measure latency, accuracy, and memory with statistical rigor, generate Pareto frontiers showing optimization trade-offs, and produce reproducible results that guide real engineering decisions.

By the end, you will have the evaluation framework needed to systematically combine optimizations and compete in the capstone challenge.

## Learning Objectives

```{tip} By completing this module, you will:

- **Implement** professional benchmarking infrastructure with statistical analysis including confidence intervals and variance control
- **Master** multi-objective optimization trade-offs between accuracy, latency, and memory through Pareto frontier analysis
- **Understand** measurement uncertainty, warmup protocols, and reproducibility requirements for fair model comparison
- **Connect** optimization techniques from Modules 14-18 into systematic evaluation workflows for the TorchPerf Olympics capstone
```

## What You'll Build

```{mermaid}
:align: center
:caption: Benchmarking Infrastructure
flowchart TB
    subgraph "Benchmarking Infrastructure"
        A["BenchmarkResult<br/>Statistical container"]
        B["Benchmark<br/>Single metric runner"]
        C["BenchmarkSuite<br/>Multi-metric evaluation"]
        D["TorchPerf Olympics<br/>Competition framework"]
    end

    E["Models to Compare"] --> B
    F["Test Datasets"] --> B
    B --> A
    A --> C
    C --> D

    style A fill:#e1f5ff
    style B fill:#fff3cd
    style C fill:#f8d7da
    style D fill:#d4edda
```

**Implementation roadmap:**

| Part | What You'll Implement | Key Concept |
|------|----------------------|-------------|
| 1 | `BenchmarkResult` dataclass | Statistical analysis with confidence intervals |
| 2 | `precise_timer()` context manager | High-precision timing for microsecond measurements |
| 3 | `Benchmark.run_latency_benchmark()` | Warmup protocols and latency measurement |
| 4 | `Benchmark.run_accuracy_benchmark()` | Model quality evaluation across datasets |
| 5 | `Benchmark.run_memory_benchmark()` | Peak memory tracking during inference |
| 6 | `BenchmarkSuite` for multi-metric analysis | Pareto frontier generation and trade-off visualization |

**The pattern you'll enable:**

```python
# Compare baseline vs optimized model with statistical rigor
benchmark = Benchmark([baseline_model, optimized_model])
latency_results = benchmark.run_latency_benchmark()
# Output: baseline: 12.3ms ¬± 0.8ms, optimized: 4.1ms ¬± 0.3ms (67% reduction, p < 0.01)
```

### What You're NOT Building (Yet)

To keep this module focused, you will **not** implement:

- Hardware-specific benchmarks (GPU profiling requires CUDA, covered in production frameworks)
- Energy measurement (requires specialized hardware like power meters)
- Distributed benchmarking (multi-node coordination is beyond scope)
- Automated hyperparameter tuning for optimization (that is Module 20: Capstone)

**You are building the statistical foundation for fair comparison.** Advanced deployment scenarios come in production systems.

## API Reference

This section documents the benchmarking API you will implement. Use it as your reference while building the statistical analysis and measurement infrastructure.

### BenchmarkResult Dataclass

```python
BenchmarkResult(metric_name: str, values: List[float], metadata: Dict[str, Any] = {})
```

Statistical container for benchmark measurements with automatic computation of mean, standard deviation, median, and 95% confidence intervals.

**Properties computed in `__post_init__`:**

| Property | Type | Description |
|----------|------|-------------|
| `mean` | `float` | Average of all measurements |
| `std` | `float` | Standard deviation (0 if single measurement) |
| `median` | `float` | Middle value, less sensitive to outliers |
| `min_val` | `float` | Minimum observed value |
| `max_val` | `float` | Maximum observed value |
| `count` | `int` | Number of measurements |
| `ci_lower` | `float` | Lower bound of 95% confidence interval |
| `ci_upper` | `float` | Upper bound of 95% confidence interval |

**Methods:**

| Method | Signature | Description |
|--------|-----------|-------------|
| `to_dict` | `to_dict() -> Dict[str, Any]` | Serialize to dictionary for JSON export |
| `__str__` | Returns formatted summary | `"metric: mean ¬± std (n=count)"` |

### Timing Context Manager

```python
with precise_timer() as timer:
    # Your code to measure
    ...
# Access elapsed time
elapsed_seconds = timer.elapsed
```

High-precision timing context manager using `time.perf_counter()` for monotonic, nanosecond-resolution measurements.

### Benchmark Class

```python
Benchmark(models: List[Any], datasets: List[Any],
          warmup_runs: int = 5, measurement_runs: int = 10)
```

Core benchmarking engine for single-metric evaluation across multiple models.

**Parameters:**
- `models`: List of models to benchmark (supports any object with forward/predict/__call__)
- `datasets`: List of datasets for accuracy benchmarking (required)
- `warmup_runs`: Number of warmup iterations before measurement (default: 5)
- `measurement_runs`: Number of measurement iterations for statistical analysis (default: 10)

**Core Methods:**

| Method | Signature | Description |
|--------|-----------|-------------|
| `run_latency_benchmark` | `run_latency_benchmark(input_shape=(1,28,28)) -> Dict[str, BenchmarkResult]` | Measure inference time per model |
| `run_accuracy_benchmark` | `run_accuracy_benchmark() -> Dict[str, BenchmarkResult]` | Measure prediction accuracy on datasets |
| `run_memory_benchmark` | `run_memory_benchmark(input_shape=(1,28,28)) -> Dict[str, BenchmarkResult]` | Track peak memory usage during inference |
| `compare_models` | `compare_models(metric: str = "latency") -> List[Dict]` | Compare models across a specific metric |

### BenchmarkSuite Class

```python
BenchmarkSuite(models: List[Any], datasets: List[Any], output_dir: str = "benchmark_results")
```

Comprehensive multi-metric evaluation suite for generating Pareto frontiers and optimization trade-off analysis.

| Method | Signature | Description |
|--------|-----------|-------------|
| `run_full_benchmark` | `run_full_benchmark() -> Dict[str, Dict[str, BenchmarkResult]]` | Run all benchmark types and aggregate results |
| `plot_pareto_frontier` | `plot_pareto_frontier(x_metric='latency', y_metric='accuracy')` | Plot Pareto frontier for two competing objectives |
| `plot_results` | `plot_results(save_plots=True)` | Generate visualization plots for benchmark results |
| `generate_report` | `generate_report() -> str` | Generate comprehensive benchmark report |

## Core Concepts

This section covers the fundamental principles that make benchmarking scientifically valid. Understanding these concepts separates professional performance engineering from naive timing measurements.

### Benchmarking Methodology

Single measurements are meaningless in performance engineering. Consider timing a model inference once: you might get 1.2ms. Run it again and you might get 3.1ms because a background process started. Which number represents the model's true performance? Neither.

Professional benchmarking treats measurements as samples from a noisy distribution. The true performance is the distribution's mean, and your job is to estimate it with statistical confidence. This requires understanding measurement variance and controlling for confounding factors.

The methodology follows a structured protocol:

**Warmup Phase**: Modern systems adapt to workloads. JIT compilers optimize hot code paths after several executions. CPU frequency scales up under sustained load. Caches fill with frequently accessed data. Without warmup, your first measurements capture cold-start behavior, not steady-state performance.

```python
# Warmup protocol in action
for _ in range(warmup_runs):
    _ = model(input_data)  # Run but discard measurements
```

**Measurement Phase**: After warmup, run the operation multiple times and collect timing data. The Central Limit Theorem tells us that with enough samples, the sample mean approaches the true mean, and we can compute confidence intervals.

Here is how the Benchmark class implements the complete protocol:

```python
def run_latency_benchmark(self, input_shape: Tuple[int, ...] = (1, 28, 28)) -> Dict[str, BenchmarkResult]:
    """Benchmark model inference latency using Profiler."""
    results = {}

    for i, model in enumerate(self.models):
        model_name = getattr(model, 'name', f'model_{i}')
        latencies = []

        # Create input tensor for this benchmark
        input_data = Tensor(np.random.randn(*input_shape).astype(np.float32))

        # Warmup runs (discard results)
        for _ in range(self.warmup_runs):
            if hasattr(model, 'forward'):
                _ = model.forward(input_data)
            elif callable(model):
                _ = model(input_data)

        # Measurement runs (collect statistics)
        for _ in range(self.measurement_runs):
            with precise_timer() as timer:
                if hasattr(model, 'forward'):
                    _ = model.forward(input_data)
                elif callable(model):
                    _ = model(input_data)
            latencies.append(timer.elapsed)

        results[model_name] = BenchmarkResult(
            metric_name=f"{model_name}_latency",
            values=latencies,
            metadata={'input_shape': input_shape, **self.system_info}
        )

    return results
```

Notice the clear separation between warmup (discarded) and measurement (collected) phases. This ensures measurements reflect steady-state performance.

**Statistical Analysis**: Raw measurements get transformed into confidence intervals that quantify uncertainty. The 95% confidence interval tells you: "If we ran this benchmark 100 times, 95 of those runs would produce a mean within this range."

### Metrics Selection

Different optimization goals require different metrics. Choosing the wrong metric leads to optimizing for the wrong objective.

**Latency (Time per Inference)**: Measures how fast a model processes a single input. Critical for real-time systems like autonomous vehicles where a prediction must complete in 30ms before the next camera frame arrives. Measured in milliseconds or microseconds.

**Throughput (Inputs per Second)**: Measures total processing capacity. Critical for batch processing systems like translating millions of documents. Higher throughput means more efficient hardware utilization. Measured in samples per second or frames per second.

**Accuracy (Prediction Quality)**: Measures how often the model makes correct predictions. The fundamental quality metric. No point having a 1ms model if it is wrong 50% of the time. Measured as percentage of correct predictions on a held-out test set.

**Memory Footprint**: Measures peak RAM usage during inference. Critical for edge devices with limited memory. A 100MB model cannot run on a device with 64MB RAM. Measured in megabytes.

**Model Size**: Measures storage size of model parameters. Critical for over-the-air updates and storage-constrained devices. A 500MB model takes minutes to download on slow networks. Measured in megabytes.

The key insight: **these metrics trade off against each other**. Quantization reduces memory but may reduce accuracy. Pruning reduces latency but may require retraining. Professional benchmarking reveals these trade-offs quantitatively.

### Statistical Validity

Statistics transforms noisy measurements into reliable conclusions. Without statistical validity, you cannot distinguish true performance differences from random noise.

**Why Variance Matters**: Consider two models. Model A runs in 10.2ms, 10.3ms, 10.1ms (low variance). Model B runs in 9.5ms, 12.1ms, 8.9ms (high variance). Is B faster? Looking at means (10.2ms vs 10.2ms) suggests they are identical, but B's high variance makes it unpredictable. Statistical analysis reveals both the mean and the reliability.

The BenchmarkResult class computes statistical properties automatically:

```python
@dataclass
class BenchmarkResult:
    metric_name: str
    values: List[float]
    metadata: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        """Compute statistics after initialization."""
        if not self.values:
            raise ValueError("BenchmarkResult requires at least one measurement.")

        self.mean = statistics.mean(self.values)
        self.std = statistics.stdev(self.values) if len(self.values) > 1 else 0.0
        self.median = statistics.median(self.values)
        self.min_val = min(self.values)
        self.max_val = max(self.values)
        self.count = len(self.values)

        # 95% confidence interval for the mean
        if len(self.values) > 1:
            t_score = 1.96  # Approximate for large samples
            margin_error = t_score * (self.std / np.sqrt(self.count))
            self.ci_lower = self.mean - margin_error
            self.ci_upper = self.mean + margin_error
        else:
            self.ci_lower = self.ci_upper = self.mean
```

The confidence interval calculation uses the standard error of the mean (std / sqrt(n)) scaled by the t-score. For large samples, a t-score of 1.96 corresponds to 95% confidence. This means: "We are 95% confident the true mean latency lies between ci_lower and ci_upper."

**Coefficient of Variation**: The ratio std / mean measures relative noise. A CV of 0.05 means standard deviation is 5% of the mean, indicating stable measurements. A CV of 0.30 means 30% relative noise, indicating unstable or noisy measurements that need more samples.

**Outlier Detection**: Extreme values can skew the mean. The median is robust to outliers. If mean and median differ significantly, investigate outliers. They might indicate thermal throttling, background processes, or measurement errors.

### Reproducibility

Reproducibility means another engineer can run your benchmark and get the same results. Without reproducibility, your optimization insights are not transferable.

**System Metadata**: Recording system configuration ensures results are interpreted correctly. A benchmark on a 2020 laptop will differ from a 2024 server, not because the model changed, but because the hardware did.

```python
def __init__(self, models: List[Any], datasets: List[Any] = None,
             warmup_runs: int = DEFAULT_WARMUP_RUNS,
             measurement_runs: int = DEFAULT_MEASUREMENT_RUNS):
    self.models = models
    self.datasets = datasets or []
    self.warmup_runs = warmup_runs
    self.measurement_runs = measurement_runs

    # Capture system information for reproducibility
    self.system_info = {
        'platform': platform.platform(),
        'python_version': platform.python_version(),
        'cpu_count': os.cpu_count() or 1,
    }
```

This metadata gets embedded in every BenchmarkResult, allowing you to understand why a benchmark run produced specific numbers.

**Controlled Environment**: Background processes, thermal state, and power settings all affect measurements. Professional benchmarking controls these factors:

- Close unnecessary applications before benchmarking
- Let the system reach thermal equilibrium (run warmup)
- Use the same hardware configuration across runs
- Document any anomalies or environmental changes

**Versioning**: Record the versions of all dependencies. A NumPy update might change BLAS library behavior, affecting performance. Recording versions ensures results remain interpretable months later.

### Reporting Results

Raw data is useless without clear communication. Professional benchmarking generates reports that guide engineering decisions.

**Comparison Tables**: Show mean, standard deviation, and confidence intervals for each model on each metric. This lets stakeholders quickly identify winners and understand uncertainty.

**Pareto Frontiers**: When metrics trade off, visualize the Pareto frontier to show which models are optimal for different constraints. A model is Pareto-optimal if no other model is better on all metrics simultaneously.

Consider three models:
- Model A: 10ms latency, 90% accuracy
- Model B: 15ms latency, 95% accuracy
- Model C: 12ms latency, 91% accuracy

Model C is dominated by Model A (faster and only 1% less accurate). It is not Pareto-optimal. Models A and B are both Pareto-optimal: if you want maximum accuracy, choose B; if you want minimum latency, choose A.

**Visualization**: Scatter plots reveal relationships between metrics. Plot latency vs accuracy and you immediately see the trade-off frontier. Add annotations showing model names and you have an actionable decision tool.

**Statistical Significance**: Report not just means but confidence intervals. Saying "Model A is 2ms faster" is incomplete. Saying "Model A is 2ms faster with 95% confidence interval [1.5ms, 2.5ms], p < 0.01" provides the statistical rigor needed for engineering decisions.

## Common Errors

These are the mistakes students commonly make when implementing benchmarking infrastructure. Understanding these patterns will save you debugging time.

### Insufficient Measurement Runs

**Error**: Running a benchmark only 3 times produces unreliable statistics.

**Symptom**: Results change dramatically between benchmark runs. Confidence intervals are extremely wide.

**Cause**: The Central Limit Theorem requires sufficient samples. With only 3 measurements, the sample mean is a poor estimator of the true mean.

**Fix**: Use at least 10 measurement runs (the default in this module). For high-variance operations, increase to 30+ runs.

```python
# BAD: Only 3 measurements
benchmark = Benchmark(models, measurement_runs=3)  # Unreliable!

# GOOD: 10+ measurements
benchmark = Benchmark(models, measurement_runs=10)  # Statistical confidence
```

### Skipping Warmup

**Error**: Measuring performance without warmup captures cold-start behavior, not steady-state performance.

**Symptom**: First measurement is much slower than subsequent ones. Results do not reflect production performance.

**Cause**: JIT compilation, cache warming, and CPU frequency scaling all require several iterations to stabilize.

**Fix**: Always run warmup iterations before measurement.

```python
# Already handled in Benchmark class
for _ in range(self.warmup_runs):
    _ = model(input_data)  # Warmup (discarded)

for _ in range(self.measurement_runs):
    # Now measure steady-state performance
```

### Comparing Different Input Shapes

**Error**: Benchmarking Model A on 28x28 images and Model B on 224x224 images, then comparing latency.

**Symptom**: Misleading conclusions about which model is faster.

**Cause**: Larger inputs require more computation. You are measuring input size effects, not model efficiency.

**Fix**: Use identical input shapes across all models in a benchmark.

```python
# Ensure all models use same input shape
results = benchmark.run_latency_benchmark(input_shape=(1, 28, 28))
```

### Ignoring Variance

**Error**: Reporting only the mean, ignoring standard deviation and confidence intervals.

**Symptom**: Cannot determine if performance differences are statistically significant or just noise.

**Cause**: Treating measurements as deterministic when they are actually stochastic.

**Fix**: Always report confidence intervals, not just means.

```python
# BenchmarkResult automatically computes confidence intervals
result = BenchmarkResult("latency", measurements)
print(f"{result.mean:.3f}ms ¬± {result.std:.3f}ms, 95% CI: [{result.ci_lower:.3f}, {result.ci_upper:.3f}]")
```

## Production Context

### Your Implementation vs. Industry Benchmarks

Your TinyTorch benchmarking infrastructure implements the same statistical principles used in production ML benchmarking frameworks. The difference is scale and automation.

| Feature | Your Implementation | MLPerf / Industry |
|---------|---------------------|-------------------|
| **Statistical Analysis** | Mean, std, 95% CI | Same + hypothesis testing, ANOVA |
| **Metrics** | Latency, accuracy, memory | Same + energy, throughput, tail latency |
| **Warmup Protocol** | Fixed warmup runs | Same + adaptive warmup until convergence |
| **Reproducibility** | System metadata | Same + hardware specs, thermal state |
| **Automation** | Manual benchmark runs | CI/CD integration, regression detection |
| **Scale** | Single machine | Distributed benchmarks across clusters |

### Code Comparison

The following shows equivalent benchmarking patterns in TinyTorch and production frameworks like MLPerf.

`````{tab-set}
````{tab-item} Your TinyTorch
```python
from tinytorch.perf.benchmarking import Benchmark

# Setup models and benchmark
benchmark = Benchmark(
    models=[baseline_model, optimized_model],
    warmup_runs=5,
    measurement_runs=10
)

# Run latency benchmark
results = benchmark.run_latency_benchmark(input_shape=(1, 28, 28))

# Analyze results
for model_name, result in results.items():
    print(f"{model_name}: {result.mean*1000:.2f}ms ¬± {result.std*1000:.2f}ms")
```
````

````{tab-item} MLPerf (Industry Standard)
```python
import mlperf_loadgen as lg

# Configure benchmark scenario
settings = lg.TestSettings()
settings.scenario = lg.TestScenario.SingleStream
settings.mode = lg.TestMode.PerformanceOnly

# Run standardized benchmark
sut = SystemUnderTest(model)
lg.StartTest(sut, qsl, settings)

# Results include latency percentiles, throughput, accuracy
```
````
`````

Let's understand the comparison:

- **Line 1-3 (Setup)**: TinyTorch uses a simple class-based API. MLPerf uses a loadgen library with standardized scenarios (SingleStream, Server, MultiStream, Offline). Both ensure fair comparison.
- **Line 6-8 (Configuration)**: TinyTorch exposes warmup_runs and measurement_runs directly. MLPerf abstracts this into TestSettings with scenario-specific defaults. Same concept, different abstraction level.
- **Line 11-13 (Execution)**: TinyTorch returns BenchmarkResult objects with statistics. MLPerf logs results to standardized formats that compare across hardware vendors. Both provide statistical analysis.
- **Statistical Rigor**: Both use repeated measurements, warmup, and confidence intervals. TinyTorch teaches the foundations; MLPerf adds industry-specific requirements.

```{tip} What's Identical

The statistical methodology, warmup protocols, and reproducibility requirements are identical. Production frameworks add automation, standardization across organizations, and hardware-specific optimizations. Understanding TinyTorch benchmarking gives you the foundation to work with any industry benchmarking framework.
```

### Why Benchmarking Matters at Scale

```{code-cell} python3
:tags: [remove-input, remove-output]
from myst_nb import glue

# Production cost savings from latency reduction
scale_annual_cost = 50_000_000
scale_latency_reduction = 0.10
scale_savings = scale_annual_cost * scale_latency_reduction
glue("scale_savings", f"${scale_savings / 1_000_000:.0f} million")
```

Production ML systems operate at scales where small performance differences compound into massive resource consumption:

- **Cost**: A data center running 10,000 GPUs 24/7 consumes $50 million in electricity annually. Reducing latency 10% saves {glue:text}`scale_savings` per year.
- **User Experience**: Search engines must return results in under 200ms. A 50ms latency reduction is the difference between keeping or losing users.
- **Sustainability**: Training GPT-3 consumed 1,287 MWh of energy, equivalent to the annual energy use of 120 US homes. Optimization reduces carbon footprint.

Fair benchmarking ensures optimization efforts focus on changes that produce measurable, statistically significant improvements.

## Check Your Understanding

```{code-cell} python3
:tags: [remove-input, remove-output]
import math

# Q1: Statistical Significance
# Baseline: mean=12.5ms, std=1.2ms, n=10
# Optimized: mean=11.8ms, std=1.5ms, n=10
q1_n = 10
q1_bl_mean = 12.5
q1_bl_std = 1.2
q1_bl_margin = 1.96 * (q1_bl_std / math.sqrt(q1_n))
q1_bl_ci_lo = q1_bl_mean - q1_bl_margin
q1_bl_ci_hi = q1_bl_mean + q1_bl_margin

q1_opt_mean = 11.8
q1_opt_std = 1.5
q1_opt_margin = 1.96 * (q1_opt_std / math.sqrt(q1_n))
q1_opt_ci_lo = q1_opt_mean - q1_opt_margin
q1_opt_ci_hi = q1_opt_mean + q1_opt_margin

q1_mean_diff = q1_bl_mean - q1_opt_mean

glue("q1_bl_margin", f"{q1_bl_margin:.2f}")
glue("q1_bl_ci_lo", f"{q1_bl_ci_lo:.2f}")
glue("q1_bl_ci_hi", f"{q1_bl_ci_hi:.2f}")
glue("q1_opt_margin", f"{q1_opt_margin:.2f}")
glue("q1_opt_ci_lo", f"{q1_opt_ci_lo:.2f}")
glue("q1_opt_ci_hi", f"{q1_opt_ci_hi:.2f}")
glue("q1_mean_diff", f"{q1_mean_diff:.1f}")

# Q2: Sample Size Calculation
# std=2.0ms, target margin=¬±0.5ms
q2_std = 2.0
q2_target = 0.5
q2_sqrt_n = 1.96 * q2_std / q2_target
q2_n = q2_sqrt_n ** 2
q2_n_ceil = math.ceil(q2_n)

glue("q2_sqrt_n", f"{q2_sqrt_n:.2f}")
glue("q2_n_raw", f"{q2_n:.1f}")
glue("q2_n_ceil", f"{q2_n_ceil}")

# Q3: Warmup Impact
# Without warmup measurements
q3_no_warmup = [15.2, 12.1, 10.8, 10.5, 10.6, 10.4]
q3_warmup = [10.5, 10.6, 10.4, 10.7, 10.5, 10.6]
q3_nw_mean = sum(q3_no_warmup) / len(q3_no_warmup)
q3_w_mean = sum(q3_warmup) / len(q3_warmup)
q3_latency_diff = q3_nw_mean - q3_w_mean
q3_latency_pct = q3_latency_diff / q3_nw_mean * 100

# Std values are given as approximate in the text (1.8 and 0.1)
q3_nw_std = 1.8
q3_w_std = 0.1
q3_var_reduction = (q3_nw_std - q3_w_std) / q3_nw_std * 100

glue("q3_nw_mean", f"{q3_nw_mean:.1f}ms")
glue("q3_w_mean", f"{q3_w_mean:.2f}ms")
glue("q3_latency_diff", f"{q3_latency_diff:.2f}ms")
glue("q3_latency_pct", f"{q3_latency_pct:.0f}%")
glue("q3_var_reduction", f"{q3_var_reduction:.0f}%")

# Q5: Measurement Overhead
# 1Œºs timer overhead, 50Œºs operation, 1000 measurements
q5_op_us = 50
q5_overhead_us = 1
q5_n_measurements = 1000
q5_total_op_us = q5_op_us * q5_n_measurements
q5_total_overhead_us = q5_overhead_us * q5_n_measurements
q5_total_op_ms = q5_total_op_us / 1000
q5_total_overhead_ms = q5_total_overhead_us / 1000
q5_total_measured_ms = q5_total_op_ms + q5_total_overhead_ms
q5_overhead_pct = (q5_total_overhead_ms / q5_total_measured_ms) * 100

glue("q5_total_op_us", f"{q5_total_op_us:,}")
glue("q5_total_op_ms", f"{q5_total_op_ms:.0f}ms")
glue("q5_total_overhead_us", f"{q5_total_overhead_us:,}")
glue("q5_total_overhead_ms", f"{q5_total_overhead_ms:.0f}ms")
glue("q5_total_measured_ms", f"{q5_total_measured_ms:.0f}ms")
glue("q5_overhead_pct", f"{q5_overhead_pct:.2f}%")
```

Test your understanding of benchmarking statistics and methodology with these quantitative questions.

**Q1: Statistical Significance**

You benchmark a baseline model and an optimized model 10 times each. Baseline: mean=12.5ms, std=1.2ms. Optimized: mean=11.8ms, std=1.5ms. Is the optimized model statistically significantly faster?

```{admonition} Answer
:class: dropdown

**Calculate 95% confidence intervals:**

Baseline: CI = mean ¬± 1.96 * (std / sqrt(n)) = 12.5 ¬± 1.96 * (1.2 / sqrt(10)) = 12.5 ¬± {glue:text}`q1_bl_margin` = [{glue:text}`q1_bl_ci_lo`, {glue:text}`q1_bl_ci_hi`]

Optimized: CI = 11.8 ¬± 1.96 * (1.5 / sqrt(10)) = 11.8 ¬± {glue:text}`q1_opt_margin` = [{glue:text}`q1_opt_ci_lo`, {glue:text}`q1_opt_ci_hi`]

**Result**: The confidence intervals OVERLAP (baseline goes as low as {glue:text}`q1_bl_ci_lo`, optimized goes as high as {glue:text}`q1_opt_ci_hi`). This means the difference is **NOT statistically significant** at the 95% confidence level. You cannot confidently claim the optimized model is faster.

**Lesson**: Always compute confidence intervals. A {glue:text}`q1_mean_diff`ms difference in means might seem meaningful, but with these variances and sample sizes, it could be random noise.
```

**Q2: Sample Size Calculation**

You measure latency with standard deviation of 2.0ms. How many measurements do you need to achieve a 95% confidence interval width of ¬±0.5ms?

```{admonition} Answer
:class: dropdown

**Confidence interval formula**: margin = 1.96 * (std / sqrt(n))

**Solve for n**: 0.5 = 1.96 * (2.0 / sqrt(n))

sqrt(n) = 1.96 * 2.0 / 0.5 = {glue:text}`q2_sqrt_n`

n = {glue:text}`q2_sqrt_n`¬≤ = **{glue:text}`q2_n_raw` ‚âà {glue:text}`q2_n_ceil` measurements**

**Lesson**: Achieving tight confidence intervals requires many measurements. Quadrupling precision (from ¬±1.0ms to ¬±0.5ms) requires 4x more samples (15 to 60). This is why professional benchmarks run hundreds of iterations.
```

**Q3: Warmup Impact**

Without warmup, your measurements are: [15.2, 12.1, 10.8, 10.5, 10.6, 10.4] ms. With 3 warmup runs discarded, your measurements are: [10.5, 10.6, 10.4, 10.7, 10.5, 10.6] ms. How much does warmup reduce measured latency and variance?

```{admonition} Answer
:class: dropdown

**Without warmup:**
- Mean = (15.2 + 12.1 + 10.8 + 10.5 + 10.6 + 10.4) / 6 = **{glue:text}`q3_nw_mean`**
- Std = 1.8ms (high variance due to warmup effects)

**With warmup:**
- Mean = (10.5 + 10.6 + 10.4 + 10.7 + 10.5 + 10.6) / 6 = **{glue:text}`q3_w_mean`**
- Std = 0.1ms (low variance, stable measurements)

**Impact:**
- Latency reduced: {glue:text}`q3_nw_mean` - {glue:text}`q3_w_mean` = **{glue:text}`q3_latency_diff` ({glue:text}`q3_latency_pct` reduction)**
- Variance reduced: 1.8 ‚Üí 0.1ms = **{glue:text}`q3_var_reduction` reduction in noise**

**Lesson**: Warmup eliminates cold-start effects and dramatically reduces measurement variance. Without warmup, you are measuring system startup behavior, not steady-state performance.
```

**Q4: Pareto Frontier**

You have three models with (latency, accuracy): A=(5ms, 88%), B=(8ms, 92%), C=(6ms, 89%). Which are Pareto-optimal?

```{admonition} Answer
:class: dropdown

**Pareto-optimal definition**: A model is Pareto-optimal if no other model is better on ALL metrics simultaneously.

**Analysis:**
- Model A vs B: A is faster (5ms < 8ms) but less accurate (88% < 92%). Neither dominates. Both Pareto-optimal.
- Model A vs C: A is faster (5ms < 6ms) but less accurate (88% < 89%). Neither dominates. Both Pareto-optimal.
- Model B vs C: B is slower (8ms > 6ms) but more accurate (92% > 89%). Neither dominates. Both Pareto-optimal.

**Result**: **All three models are Pareto-optimal**. None is strictly dominated by another.

**Lesson**: The Pareto frontier shows trade-off options. If you need minimum latency, choose A. If you need maximum accuracy, choose B. If you want balanced performance, choose C.
```

**Q5: Measurement Overhead**

Your timer has 1Œºs overhead per measurement. You measure a 50Œºs operation 1000 times. What percentage of measured time is overhead?

```{admonition} Answer
:class: dropdown

**Total true operation time**: 50Œºs √ó 1000 = {glue:text}`q5_total_op_us`Œºs = {glue:text}`q5_total_op_ms`

**Total timer overhead**: 1Œºs √ó 1000 = {glue:text}`q5_total_overhead_us`Œºs = {glue:text}`q5_total_overhead_ms`

**Total measured time**: {glue:text}`q5_total_op_ms` + {glue:text}`q5_total_overhead_ms` = {glue:text}`q5_total_measured_ms`

**Overhead percentage**: ({glue:text}`q5_total_overhead_ms` / {glue:text}`q5_total_measured_ms`) √ó 100% = **{glue:text}`q5_overhead_pct`**

**Lesson**: Timer overhead is negligible for operations longer than ~50Œºs, but becomes significant for microsecond-scale operations. This is why we use `time.perf_counter()` with nanosecond resolution and minimal overhead. For operations under 10Œºs, consider measuring batches and averaging.
```

## Further Reading

For students who want to understand the academic and industry foundations of ML benchmarking:

### Seminal Papers

- **MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance** - Mattson et al. (2020). The definitive industry benchmark framework that establishes fair comparison methodology across hardware vendors. Your benchmarking infrastructure follows the same statistical principles MLPerf uses for standardized evaluation. [arXiv:1910.01500](https://arxiv.org/abs/1910.01500)

- **How to Benchmark Code Execution Times on Intel IA-32 and IA-64 Instruction Set Architectures** - Paoloni (2010). White paper explaining low-level timing mechanisms, measurement overhead, and sources of timing variance. Essential reading for understanding what happens when you call `time.perf_counter()`. [Intel](https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/ia-32-ia-64-benchmark-code-execution-paper.pdf)

- **Statistical Tests for Comparing Performance** - Georges et al. (2007). Academic treatment of statistical methodology for benchmarking, including hypothesis testing, sample size calculation, and handling measurement noise. [ACM OOPSLA](https://doi.org/10.1145/1297027.1297033)

- **Benchmarking Deep Learning Inference on Mobile Devices** - Ignatov et al. (2018). Comprehensive study of mobile ML benchmarking challenges including thermal throttling, battery constraints, and heterogeneous hardware. Shows how benchmarking methodology changes for resource-constrained devices. [arXiv:1812.01328](https://arxiv.org/abs/1812.01328)

### Additional Resources

- **Industry Standard**: [MLCommons MLPerf](https://mlcommons.org/en/inference-edge-11/) - Browse actual benchmark results across hardware vendors to see professional benchmarking in practice
- **Textbook**: "The Art of Computer Systems Performance Analysis" by Raj Jain - Comprehensive treatment of experimental design, statistical analysis, and benchmarking methodology for systems engineering

## What's Next

```{seealso} Coming Up: Module 20 - Capstone

Apply everything you have learned in Modules 01-19 to compete in the TorchPerf Olympics! You will strategically combine optimization techniques, benchmark your results, and compete for the fastest, smallest, or most accurate model on the leaderboard.
```

**Preview - How Your Benchmarking Gets Used in the Capstone:**

| Competition Event | Metric Optimized | Your Benchmark In Action |
|-------------------|------------------|--------------------------|
| **Latency Sprint** | Minimize inference time | `benchmark.run_latency_benchmark()` determines winners |
| **Memory Challenge** | Minimize model size | `benchmark.run_memory_benchmark()` tracks footprint |
| **Accuracy Contest** | Maximize accuracy under constraints | `benchmark.run_accuracy_benchmark()` validates quality |
| **All-Around** | Balanced Pareto frontier | `suite.plot_pareto_frontier()` finds optimal trade-offs |

## Get Started

```{tip} Interactive Options

- **[Launch Binder](https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?urlpath=lab/tree/tinytorch/modules/19_benchmarking/benchmarking.ipynb)** - Run interactively in browser, no setup required
- **[View Source](https://github.com/harvard-edge/cs249r_book/blob/main/tinytorch/src/19_benchmarking/19_benchmarking.py)** - Browse the implementation code
```

```{warning} Save Your Progress

Binder sessions are temporary. Download your completed notebook when done, or clone the repository for persistent local work.
```


## Links discovered
- [arXiv:1910.01500](https://arxiv.org/abs/1910.01500)
- [Intel](https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/ia-32-ia-64-benchmark-code-execution-paper.pdf)
- [ACM OOPSLA](https://doi.org/10.1145/1297027.1297033)
- [arXiv:1812.01328](https://arxiv.org/abs/1812.01328)
- [MLCommons MLPerf](https://mlcommons.org/en/inference-edge-11/)
- [Launch Binder](https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?urlpath=lab/tree/tinytorch/modules/19_benchmarking/benchmarking.ipynb)
- [View Source](https://github.com/harvard-edge/cs249r_book/blob/main/tinytorch/src/19_benchmarking/19_benchmarking.py)
- [Open in Binder ‚Üí](https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?labpath=tinytorch%2Fmodules%2F19_benchmarking%2Fbenchmarking.ipynb)
- [View on GitHub ‚Üí](https://github.com/harvard-edge/cs249r_book/blob/main/tinytorch/src/19_benchmarking/19_benchmarking.py)
- [‚¨á Download](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/src/_static/slides/19_benchmarking.pdf)

--- tinytorch/tito/commands/benchmark.py ---
"""
Tinyüî•Torch Benchmark Commands

Run baseline and capstone benchmarks, with automatic submission prompts.
"""

import json
import time
import platform
from argparse import ArgumentParser, Namespace
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import numpy as np

from rich.panel import Panel
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn
from rich.prompt import Prompt, Confirm
from rich.console import Console

from .base import BaseCommand
from ..core.exceptions import TinyTorchCLIError


class BenchmarkCommand(BaseCommand):
    """Benchmark commands - baseline and capstone performance evaluation."""

    @property
    def name(self) -> str:
        return "benchmark"

    @property
    def description(self) -> str:
        return "Run benchmarks - baseline (setup validation) and capstone (full performance)"

    def add_arguments(self, parser: ArgumentParser) -> None:
        """Add benchmark subcommands."""
        subparsers = parser.add_subparsers(
            dest='benchmark_command',
            help='Benchmark operations',
            metavar='COMMAND'
        )

        # Baseline benchmark
        baseline_parser = subparsers.add_parser(
            'baseline',
            help='Run baseline benchmark (quick setup validation)'
        )
        baseline_parser.add_argument(
            '--skip-submit',
            action='store_true',
            help='Skip submission prompt after benchmark'
        )

        # Capstone benchmark
        capstone_parser = subparsers.add_parser(
            'capstone',
            help='Run capstone benchmark (full Module 20 performance evaluation)'
        )
        capstone_parser.add_argument(
            '--track',
            choices=['speed', 'compression', 'accuracy', 'efficiency', 'all'],
            default='all',
            help='Which track to benchmark (default: all)'
        )
        capstone_parser.add_argument(
            '--skip-submit',
            action='store_true',
            help='Skip submission prompt after benchmark'
        )

    def run(self, args: Namespace) -> int:
        """Execute benchmark command."""
        if not args.benchmark_command:
            self.console.print("[yellow]Please specify a benchmark command: baseline or capstone[/yellow]")
            return 1

        if args.benchmark_command == 'baseline':
            return self._run_baseline(args)
        elif args.benchmark_command == 'capstone':
            return self._run_capstone(args)
        else:
            self.console.print(f"[red]Unknown benchmark command: {args.benchmark_command}[/red]")
            return 1

    def _get_reference_times(self) -> Dict[str, float]:
        """
        Get reference times for normalization (SPEC-style).

        Reference system: Mid-range laptop (Intel i5-8th gen, 16GB RAM)
        These times represent expected performance on reference hardware.
        Results are normalized: normalized_score = reference_time / actual_time

        Returns:
            Dict with reference times in milliseconds for each benchmark
        """
        return {
            "tensor_ops": 0.8,      # Reference: 0.8ms for tensor operations
            "matmul": 2.5,          # Reference: 2.5ms for matrix multiply
            "forward_pass": 6.7,    # Reference: 6.7ms for forward pass
            "total": 10.0           # Reference: 10.0ms total
        }

    def _run_baseline(self, args: Namespace) -> int:
        """Run baseline benchmark - lightweight setup validation."""
        console = self.console

        console.print(Panel(
            "[bold cyan]üéØ Baseline Benchmark[/bold cyan]\n\n"
            "Running lightweight benchmarks to validate your setup...\n"
            "[dim]Results are normalized to a reference system for fair comparison.[/dim]",
            title="Baseline Benchmark",
            border_style="cyan"
        ))

        # Run baseline benchmarks
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console
        ) as progress:
            task = progress.add_task("Running baseline benchmarks...", total=None)

            # Benchmark 1: Tensor operations
            progress.update(task, description="[cyan]Testing tensor operations...")
            tensor_time = self._benchmark_tensor_ops()

            # Benchmark 2: Matrix multiply
            progress.update(task, description="[cyan]Testing matrix multiplication...")
            matmul_time = self._benchmark_matmul()

            # Benchmark 3: Simple forward pass
            progress.update(task, description="[cyan]Testing forward pass...")
            forward_time = self._benchmark_forward_pass()

            progress.update(task, completed=True)

        # Get reference times for normalization (SPEC-style)
        reference = self._get_reference_times()

        # Calculate normalized scores (SPEC-style: reference_time / actual_time)
        # Higher normalized score = better performance
        tensor_normalized = reference["tensor_ops"] / max(tensor_time, 0.001)
        matmul_normalized = reference["matmul"] / max(matmul_time, 0.001)
        forward_normalized = reference["forward_pass"] / max(forward_time, 0.001)

        # Overall normalized score (geometric mean for fairness)
        total_time = tensor_time + matmul_time + forward_time
        total_normalized = reference["total"] / max(total_time, 0.001)

        # Convert to 0-100 score scale
        # Reference system = 100 points, faster systems > 100, slower < 100
        score = min(100, int(100 * total_normalized))

        # Store both raw and normalized metrics
        raw_metrics = {
            "tensor_ops_ms": tensor_time,
            "matmul_ms": matmul_time,
            "forward_pass_ms": forward_time,
            "total_ms": total_time
        }

        normalized_metrics = {
            "tensor_ops_normalized": tensor_normalized,
            "matmul_normalized": matmul_normalized,
            "forward_pass_normalized": forward_normalized,
            "total_normalized": total_normalized,
            "score": score
        }

        # Display results
        results_table = Table(title="Baseline Benchmark Results", show_header=True, header_style="bold cyan")
        results_table.add_column("Metric", style="cyan")
        results_table.add_column("Time", justify="right", style="green")
        results_table.add_column("Normalized", justify="right", style="yellow")
        results_table.add_column("Status", justify="center")

        results_table.add_row(
            "Tensor Operations",
            f"{tensor_time:.2f} ms",
            f"{tensor_normalized:.2f}x",
            "‚úÖ"
        )
        results_table.add_row(
            "Matrix Multiply",
            f"{matmul_time:.2f} ms",
            f"{matmul_normalized:.2f}x",
            "‚úÖ"
        )
        results_table.add_row(
            "Forward Pass",
            f"{forward_time:.2f} ms",
            f"{forward_normalized:.2f}x",
            "‚úÖ"
        )
        results_table.add_row("", "", "", "")
        results_table.add_row(
            "[bold]Total[/bold]",
            f"{total_time:.2f} ms",
            f"{total_normalized:.2f}x",
            "‚úÖ"
        )
        results_table.add_row(
            "[bold]Score[/bold]",
            "",
            f"[bold]{score}/100[/bold]",
            "üéØ"
        )

        console.print("\n")
        console.print(results_table)

        # Show normalization info
        console.print(f"\n[dim]üìä Normalization: Results normalized to reference system[/dim]")
        console.print(f"[dim]   Reference: {reference['total']:.1f}ms total time[/dim]")
        console.print(f"[dim]   Your system: {total_time:.2f}ms ({total_normalized:.2f}x vs reference)[/dim]")

        # Create results dict
        results = {
            "benchmark_type": "baseline",
            "timestamp": datetime.now().isoformat(),
            "system_info": self._get_system_info(),
            "reference_system": {
                "description": "Mid-range laptop (Intel i5-8th gen, 16GB RAM)",
                "times_ms": reference
            },
            "raw_metrics": raw_metrics,
            "normalized_metrics": normalized_metrics,
            "metrics": {
                **raw_metrics,
                **normalized_metrics
            }
        }

        # Save results
        benchmark_dir = Path(".tito") / "benchmarks"
        benchmark_dir.mkdir(parents=True, exist_ok=True)
        timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = benchmark_dir / f"baseline_{timestamp_str}.json"

        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)

        console.print(f"\n[green]‚úÖ Results saved to: {results_file}[/green]")

        # Success message
        console.print(Panel(
            f"[bold green]üéâ Baseline Benchmark Complete![/bold green]\n\n"
            f"üìä Your Score: [bold]{score}/100[/bold]\n"
            f"‚úÖ Setup verified and working!\n\n"
            f"üí° Run [cyan]tito benchmark capstone[/cyan] after Module 20 for full benchmarks",
            title="Success",
            border_style="green"
        ))

        # Prompt for submission
        if not args.skip_submit:
            self._prompt_submission(results, "baseline")

        return 0

    def _run_capstone(self, args: Namespace) -> int:
        """Run capstone benchmark - full Module 20 performance evaluation."""
        console = self.console

        console.print(Panel(
            "[bold cyan]üèÜ Capstone Benchmark[/bold cyan]\n\n"
            "Running full benchmark suite from Module 20...",
            title="Capstone Benchmark",
            border_style="cyan"
        ))

        # Check if Module 20 is available
        try:
            from tinytorch.perf.benchmarking import Benchmark
        except ImportError:
            console.print(Panel(
                "[red]‚ùå Module 19 (Benchmarking) not available[/red]\n\n"
                "Please complete Module 19 first:\n"
                "  [cyan]tito module complete 19[/cyan]",
                title="Error",
                border_style="red"
            ))
            return 1

        # Check if Module 20 competition code is available
        try:
            from tinytorch.competition.submit import OlympicEvent, generate_submission
        except ImportError:
            console.print(Panel(
                "[yellow]‚ö†Ô∏è  Module 20 (Capstone) not complete[/yellow]\n\n"
                "Running simplified capstone benchmarks...\n"
                "For full benchmarks, complete Module 20 first:\n"
                "  [cyan]tito module complete 20[/cyan]",
                title="Warning",
                border_style="yellow"
            ))
            # Fall back to simplified benchmarks
            return self._run_simplified_capstone(args)

        # Run full capstone benchmarks
        console.print("[cyan]Running full capstone benchmark suite...[/cyan]")
        console.print("[dim]This may take a few minutes...[/dim]\n")

        # For now, create a placeholder that shows the structure
        # In production, this would use actual models and Module 19's Benchmark class
        results = {
            "benchmark_type": "capstone",
            "timestamp": datetime.now().isoformat(),
            "system_info": self._get_system_info(),
            "track": args.track,
            "metrics": {
                "speed": {
                    "latency_ms": 45.2,
                    "throughput_ops_per_sec": 22.1,
                    "score": 92
                },
                "compression": {
                    "model_size_mb": 12.4,
                    "compression_ratio": 4.2,
                    "score": 88
                },
                "accuracy": {
                    "accuracy_percent": 87.5,
                    "score": 95
                },
                "efficiency": {
                    "memory_mb": 8.3,
                    "energy_score": 85,
                    "score": 85
                }
            },
            "overall_score": 90
        }

        # Save results
        benchmark_dir = Path(".tito") / "benchmarks"
        benchmark_dir.mkdir(parents=True, exist_ok=True)
        timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = benchmark_dir / f"capstone_{timestamp_str}.json"

        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)

        # Display results
        self._display_capstone_results(results)

        console.print(f"\n[green]‚úÖ Results saved to: {results_file}[/green]")

        # Prompt for submission
        if not args.skip_submit:
            self._prompt_submission(results, "capstone")

        return 0

    def _run_simplified_capstone(self, args: Namespace) -> int:
        """Run simplified capstone benchmarks when Module 20 isn't complete."""
        console = self.console

        console.print("[yellow]Running simplified capstone benchmarks...[/yellow]\n")

        # Run basic benchmarks
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console
        ) as progress:
            task = progress.add_task("Running benchmarks...", total=None)

            progress.update(task, description="[cyan]Testing performance...")
            time.sleep(1)  # Simulate benchmark time

        results = {
            "benchmark_type": "capstone_simplified",
            "timestamp": datetime.now().isoformat(),
            "system_info": self._get_system_info(),
            "note": "Simplified benchmarks - complete Module 20 for full suite",
            "metrics": {
                "basic_score": 75
            }
        }

        # Save results
        benchmark_dir = Path(".tito") / "benchmarks"
        benchmark_dir.mkdir(parents=True, exist_ok=True)
        timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = benchmark_dir / f"capstone_simplified_{timestamp_str}.json"

        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)

        console.print(f"\n[green]‚úÖ Results saved to: {results_file}[/green]")
        console.print("[yellow]üí° Complete Module 20 for full capstone benchmarks[/yellow]")

        return 0

    def _benchmark_tensor_ops(self) -> float:
        """Benchmark basic tensor operations."""
        import time

        # Create tensors
        a = np.random.randn(100, 100).astype(np.float32)
        b = np.random.randn(100, 100).astype(np.float32)

        # Warmup
        for _ in range(5):
            _ = a + b
            _ = a * b

        # Benchmark
        start = time.perf_counter()
        for _ in range(100):
            _ = a + b
            _ = a * b
            _ = np.sum(a)
        end = time.perf_counter()

        return (end - start) * 1000 / 100  # Convert to milliseconds per operation

    def _benchmark_matmul(self) -> float:
        """Benchmark matrix multiplication."""
        import time

        a = np.random.randn(100, 100).astype(np.float32)
        b = np.random.randn(100, 100).astype(np.float32)

        # Warmup
        for _ in range(5):
            _ = np.dot(a, b)

        # Benchmark
        start = time.perf_counter()
        for _ in range(50):
            _ = np.dot(a, b)
        end = time.perf_counter()

        return (end - start) * 1000 / 50  # milliseconds per matmul

    def _benchmark_forward_pass(self) -> float:
        """Benchmark simple forward pass simulation."""
        import time

        # Simulate a simple forward pass
        x = np.random.randn(1, 784).astype(np.float32)
        w1 = np.random.randn(784, 128).astype(np.float32)
        w2 = np.random.randn(128, 10).astype(np.float32)

        # Warmup
        for _ in range(5):
            h = np.maximum(0, np.dot(x, w1))  # ReLU
            _ = np.dot(h, w2)

        # Benchmark
        start = time.perf_counter()
        for _ in range(20):
            h = np.maximum(0, np.dot(x, w1))
            _ = np.dot(h, w2)
        end = time.perf_counter()

        return (end - start) * 1000 / 20  # milliseconds per forward pass

    def _get_system_info(self) -> Dict[str, str]:
        """Get system information."""
        return {
            "platform": platform.platform(),
            "processor": platform.processor(),
            "python_version": platform.python_version(),
            "cpu_count": str(platform.processor() or "unknown")
        }

    def _display_capstone_results(self, results: Dict[str, Any]) -> None:
        """Display capstone benchmark results."""
        console = self.console

        results_table = Table(title="Capstone Benchmark Results", show_header=True, header_style="bold cyan")
        results_table.add_column("Track", style="cyan")
        results_table.add_column("Metric", style="yellow")
        results_table.add_column("Value", justify="right", style="green")
        results_table.add_column("Score", justify="right", style="magenta")

        metrics = results.get("metrics", {})

        if "speed" in metrics:
            speed = metrics["speed"]
            results_table.add_row("Speed", "Latency", f"{speed['latency_ms']:.2f} ms", f"{speed['score']}/100")
            results_table.add_row("", "Throughput", f"{speed['throughput_ops_per_sec']:.2f} ops/s", "")

        if "compression" in metrics:
            comp = metrics["compression"]
            results_table.add_row("Compression", "Model Size", f"{comp['model_size_mb']:.2f} MB", f"{comp['score']}/100")
            results_table.add_row("", "Compression Ratio", f"{comp['compression_ratio']:.1f}x", "")

        if "accuracy" in metrics:
            acc = metrics["accuracy"]
            results_table.add_row("Accuracy", "Accuracy", f"{acc['accuracy_percent']:.1f}%", f"{acc['score']}/100")

        if "efficiency" in metrics:
            eff = metrics["efficiency"]
            results_table.add_row("Efficiency", "Memory", f"{eff['memory_mb']:.2f} MB", f"{eff['score']}/100")

        results_table.add_row("", "", "", "")
        results_table.add_row("[bold]Overall[/bold]", "", "", f"[bold]{results.get('overall_score', 0)}/100[/bold]")

        console.print("\n")
        console.print(results_table)

        console.print(Panel(
            f"[bold green]üèÜ Capstone Benchmark Complete![/bold green]\n\n"
            f"üìä Overall Score: [bold]{results.get('overall_score', 0)}/100[/bold]",
            title="Success",
            border_style="green"
        ))

    def _prompt_submission(self, results: Dict[str, Any], benchmark_type: str) -> None:
        """Prompt user to submit benchmark results."""
        console = self.console

        console.print("\n")
        submit = Confirm.ask(
            f"[cyan]Would you like to submit your {benchmark_type} benchmark results to the community?[/cyan]",
            default=True
        )

        if submit:
            # Collect submission configuration
            console.print("\n[cyan]Submission Configuration:[/cyan]")

            # Check if user is in community
            community_data = self._get_community_data()
            if not community_data:
                console.print("[yellow]‚ö†Ô∏è  You're not in the community yet.[/yellow]")
                join = Confirm.ask("Would you like to join the community first?", default=True)
                if join:
                    console.print("\n[cyan]Run: [bold]tito community join[/bold][/cyan]")
                    return

            # Additional submission options
            include_system_info = Confirm.ask(
                "Include system information in submission?",
                default=True
            )

            anonymous = Confirm.ask(
                "Submit anonymously?",
                default=False
            )

            # Create submission data
            submission = {
                "benchmark_type": benchmark_type,
                "timestamp": results["timestamp"],
                "metrics": results["metrics"],
                "include_system_info": include_system_info,
                "anonymous": anonymous
            }

            if include_system_info:
                submission["system_info"] = results.get("system_info", {})

            # Save submission
            submission_dir = Path(".tito") / "submissions"
            submission_dir.mkdir(parents=True, exist_ok=True)
            timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
            submission_file = submission_dir / f"{benchmark_type}_submission_{timestamp_str}.json"

            with open(submission_file, 'w') as f:
                json.dump(submission, f, indent=2)

            console.print(f"\n[green]‚úÖ Submission prepared: {submission_file}[/green]")

            # Stub: Try to submit to website
            self._submit_to_website(submission)

            config = self._get_config()
            if not config.get("website", {}).get("enabled", False):
                console.print("[cyan]üí° Submission saved locally. Community leaderboard coming soon![/cyan]")

    def _get_community_data(self) -> Optional[Dict[str, Any]]:
        """Get user's community profile from ~/.tinytorch (flat structure)."""
        from pathlib import Path
        profile_file = Path.home() / ".tinytorch" / "profile.json"
        if profile_file.exists():
            try:
                with open(profile_file, 'r') as f:
                    return json.load(f)
            except Exception:
                return None
        return None

    def _get_config(self) -> Dict[str, Any]:
        """Get community configuration."""
        config_file = self.config.project_root / ".tinytorch" / "config.json"
        default_config = {
            "website": {
                "base_url": "https://tinytorch.ai",
                "community_map_url": "https://tinytorch.ai/map",
                "api_url": None,  # Set when API is available
                "enabled": False  # Set to True when website integration is ready
            },
            "local": {
                "enabled": True,  # Always use local storage
                "auto_sync": False  # Auto-sync to website when enabled
            }
        }

        if config_file.exists():
            try:
                with open(config_file, 'r') as f:
                    user_config = json.load(f)
                    # Merge with defaults
                    default_config.update(user_config)
                    return default_config
            except Exception:
                pass

        # Create default config if it doesn't exist
        config_file.parent.mkdir(parents=True, exist_ok=True)
        with open(config_file, 'w') as f:
            json.dump(default_config, f, indent=2)

        return default_config

    def _submit_to_website(self, submission: Dict[str, Any]) -> None:
        """Stub: Submit benchmark results to website (local for now, website integration later)."""
        config = self._get_config()

        if not config.get("website", {}).get("enabled", False):
            # Website integration not enabled, just store locally
            return

        api_url = config.get("website", {}).get("api_url")
        if api_url:
            # TODO: Implement API call when website is ready
            # Example:
            # import requests
            # try:
            #     response = requests.post(
            #         f"{api_url}/api/benchmarks/submit",
            #         json=submission,
            #         timeout=30,  # 30 second timeout for benchmark submissions
            #         headers={"Content-Type": "application/json"}
            #     )
            #     response.raise_for_status()
            #     self.console.print("[green]‚úÖ Submitted to community leaderboard![/green]")
            # except requests.Timeout:
            #     self.console.print("[yellow]‚ö†Ô∏è  Submission timed out. Saved locally.[/yellow]")
            #     self.console.print("[dim]You can submit later or try again.[/dim]")
            # except requests.RequestException as e:
            #     self.console.print(f"[yellow]‚ö†Ô∏è  Could not submit to website: {e}[/yellow]")
            #     self.console.print("[dim]Your submission is saved locally and can be submitted later.[/dim]")
            pass


--- tinytorch/datasets/tinytalks/CHANGELOG.md ---
# Changelog

All notable changes to the TinyTalks dataset will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [1.0.0] - 2025-01-28

### Added
- Initial release of TinyTalks dataset
- 301 Q&A pairs across 5 difficulty levels:
  - Level 1: Greetings & Identity (47 pairs)
  - Level 2: Simple Facts (82 pairs)
  - Level 3: Basic Math (45 pairs)
  - Level 4: Common Sense Reasoning (87 pairs)
  - Level 5: Multi-turn Context (40 pairs)
- Train/validation/test splits (70/15/15)
- Comprehensive README with usage examples
- DATASHEET.md following "Datasheets for Datasets" best practices
- CC BY 4.0 license
- Generation script (`scripts/generate_tinytalks.py`)
- Validation script (`scripts/validate_dataset.py`)
- Statistics script (`scripts/stats.py`)
- Example usage script (`examples/demo_usage.py`)

### Dataset Statistics
- Total size: ~17.5 KB
- Character vocabulary: 65 unique characters
- Word vocabulary: 865 unique words
- Average question length: 4.8 words (21.6 characters)
- Average answer length: 6.1 words (29.0 characters)

### Validation
- ‚úÖ UTF-8 encoding
- ‚úÖ Unix line endings (LF)
- ‚úÖ No duplicate questions
- ‚úÖ No empty questions or answers
- ‚úÖ Proper punctuation
- ‚úÖ Balanced splits with no overlap

---

## [Unreleased]

### Planned for v1.1.0
- Add 50 more Level 4-5 pairs for better reasoning
- Expand math questions to include simple multiplication tables
- Add more conversational context pairs

### Planned for v2.0.0
- Multi-language support (Spanish, French)
- Expanded to 500+ pairs
- Add difficulty scores for each Q&A pair

### Planned for v3.0.0
- Expand to 1,000+ pairs
- Add more complex reasoning tasks
- Include multi-hop questions
- Add entity recognition annotations

---

## Version History

- **1.0.0** (2025-01-28) - Initial release


## Links discovered
- [Keep a Changelog](https://keepachangelog.com/en/1.0.0/)
- [Semantic Versioning](https://semver.org/spec/v2.0.0.html)

--- tinytorch/tests/19_benchmarking/test_benchmark_core.py ---
"""
Module 19: Benchmarking Core Tests
===================================

These tests verify that benchmarking tools work correctly.

WHY THESE TESTS MATTER:
-----------------------
Benchmarking is how we measure and compare model performance.
If benchmarking is broken:
- We can't measure throughput (tokens/second)
- We can't compare optimization techniques
- We can't validate our optimizations work

WHAT WE TEST:
-------------
1. MLPerf can run benchmarks
2. Metrics are computed correctly
3. Results are reproducible
"""

import pytest
import numpy as np
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from tinytorch.core.tensor import Tensor
from tinytorch.core.layers import Linear
from tinytorch.perf.benchmarking import Benchmark, MLPerf


class TestBenchmarkBasics:
    """Test basic benchmarking functionality."""

    def test_benchmark_import(self):
        """Verify Benchmark can be imported."""
        assert Benchmark is not None
        assert MLPerf is not None

    def test_benchmark_can_instantiate(self):
        """Verify Benchmark can be created."""
        # Create simple dummy model
        class DummyModel:
            def forward(self, x):
                return x

        models = [DummyModel()]
        datasets = [[(Tensor(np.random.randn(10, 10)), Tensor(np.zeros(10)))]]

        bench = Benchmark(models, datasets)
        assert bench is not None

    def test_measure_throughput(self):
        """
        WHAT: Verify throughput measurement works.

        WHY: Throughput (items/second) is a key performance metric.
        """
        # Simple model
        class SimpleModel:
            def __init__(self):
                self.layer = Linear(10, 10)

            def forward(self, x):
                return self.layer.forward(x)

        model = SimpleModel()
        models = [model]
        datasets = [[(Tensor(np.random.randn(10, 10)), Tensor(np.zeros(10)))]]

        bench = Benchmark(models, datasets)
        results = bench.run_latency_benchmark(input_shape=(1, 10))

        assert len(results) > 0, "Benchmark should produce results"
        for model_name, result in results.items():
            assert result.mean > 0, (
                f"Latency should be positive, got {result.mean}"
            )


class TestMLPerf:
    """Test MLPerf benchmark suite."""

    def test_mlperf_can_run(self):
        """
        WHAT: Verify MLPerf benchmark suite can execute.

        WHY: This is the capstone benchmarking tool students build.
        """
        # Create and run minimal benchmark
        mlperf = MLPerf()

        # Should at least be able to list available benchmarks
        if hasattr(mlperf, 'list_benchmarks'):
            benchmarks = mlperf.list_benchmarks()
            assert isinstance(benchmarks, (list, dict)), (
                "list_benchmarks should return a list or dict"
            )


class TestBenchmarkMetrics:
    """Test that benchmark metrics are computed correctly."""

    def test_latency_is_positive(self):
        """Latency must always be positive."""
        class SimpleModel:
            def forward(self, x):
                return x * 2

        model = SimpleModel()
        x = Tensor(np.random.randn(10))
        datasets = [[(x, None)]]

        bench = Benchmark([model], datasets)
        results = bench.run_latency_benchmark(input_shape=(10,))

        assert len(results) > 0, "Should produce results"
        for name, result in results.items():
            assert result.mean > 0, "Latency must be positive"

    def test_multiple_runs_are_consistent(self):
        """
        WHAT: Verify benchmark results are reasonably consistent.

        WHY: Benchmarks should be reproducible. Large variance
        means we can't trust the measurements.
        """
        class SimpleModel:
            def __init__(self):
                self.layer = Linear(10, 10)

            def forward(self, x):
                return self.layer.forward(x)

        model = SimpleModel()
        x = Tensor(np.random.randn(1, 10))
        datasets = [[(x, None)]]

        bench = Benchmark([model], datasets, measurement_runs=10)
        results = bench.run_latency_benchmark(input_shape=(1, 10))

        # Check that we get results with reasonable variance
        for name, result in results.items():
            # Coefficient of variation should be reasonable (std/mean < 100%)
            if result.mean > 0:
                cv = result.std / result.mean
                assert cv < 1.0, (
                    f"Benchmark results too variable!\n"
                    f"  Mean: {result.mean}, Std: {result.std}, CV: {cv}\n"
                    "Coefficient of variation should be < 100%."
                )


if __name__ == "__main__":
    pytest.main([__file__, "-v"])


--- tinytorch/tests/19_benchmarking/test_benchmarking_integration.py ---
#!/usr/bin/env python3
"""
Integration tests for Module 19: Benchmarking
Tests MLPerf-style benchmarking and performance measurement
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

def test_benchmarking_integration():
    """Test benchmarking system integration."""
    # TODO: Implement integration tests
    # - Test benchmark runner
    # - Test performance metrics collection
    # - Test result validation
    # - Test comparison with baselines
    # - Test leaderboard submission
    pass

if __name__ == "__main__":
    test_benchmarking_integration()
    print("‚úÖ Benchmarking integration tests (pending implementation)")


--- book/quarto/_extensions/mlsysbook-ext/margin-video/CHANGELOG.md ---
# Changelog

All notable changes to the Margin Video extension will be documented in this file.

## [1.0.0] - 2024-12-08

### Added
- Initial release of margin-video extension
- YouTube video embedding as margin notes
- Automatic video numbering in HTML output
- QR code generation for PDF output
- Format-specific rendering (HTML vs PDF)
- YouTube URL validation with clear error messages
- Support for multiple YouTube URL formats:
  - `youtube.com/watch?v=ID`
  - `youtu.be/ID`
  - `youtube.com/embed/ID`
- Configuration options via kwargs:
  - `aspect-ratio`: Custom video aspect ratio (default: "16/9")
  - `start`: Start time in seconds
  - `autoplay`: Enable autoplay (default: false)
- Comprehensive documentation and examples
- Error handling for missing or invalid arguments

### Features
- **HTML Output**: Responsive iframe with configurable aspect ratio
- **PDF Output**: QR code with margin note and clickable link
- **Validation**: YouTube-only support with helpful error messages
- **Flexibility**: Configurable video parameters and styling
- **Documentation**: Complete README with usage examples

### Technical Details
- Quarto >= 1.2.0 required
- MIT licensed
- Self-contained extension with no external dependencies


--- .codespell-ignore-words.txt ---
heros
fith
curren
aline
gage
akumulators
ans
titel
socio
ser
currenty
initialy
assertin
mor
dota
appen
multline
ource
bu
sting
te
crossreference
linez
als
fo
listin
nam
ture
handlin
panting
occurence
socio-economic
rin
nd
ure
<<<<<<< Updated upstream
=======
ure
>>>>>>> Stashed changes


--- README.md ---
# Machine Learning Systems
*Principles and Practices of Engineering Artificially Intelligent Systems*

<p align="center">
  <a href="README.md">English</a> ‚Ä¢
  <a href="README/README_zh.md">‰∏≠Êñá</a> ‚Ä¢
  <a href="README/README_ja.md">Êó•Êú¨Ë™û</a> ‚Ä¢
  <a href="README/README_ko.md">ÌïúÍµ≠Ïñ¥</a>
</p>

<div align="center">

<p align="center">

  [![Book](https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/book-validate-dev.yml?branch=dev&label=Book&logo=githubactions&cacheSeconds=300)](https://github.com/harvard-edge/cs249r_book/actions/workflows/book-validate-dev.yml)
  [![TinyTorch](https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/tinytorch-validate-dev.yml?branch=dev&label=TinyTorch&logo=python&cacheSeconds=300)](https://github.com/harvard-edge/cs249r_book/actions/workflows/tinytorch-validate-dev.yml)
  ![Updated](https://img.shields.io/github/last-commit/harvard-edge/cs249r_book/dev?label=Updated&logo=git&cacheSeconds=300)

  [![License](https://img.shields.io/badge/License-CC--BY--NC--ND%204.0-blue.svg)](https://github.com/harvard-edge/cs249r_book/blob/dev/LICENSE.md)
  [![Cite](https://img.shields.io/badge/Cite-IEEE%202024-blue?logo=ieee)](#-citation--license)
  [![Fund Us](https://img.shields.io/badge/Fund%20Us-Open%20Collective-blue.svg?logo=open-collective)](https://opencollective.com/mlsysbook)

</p>

<p align="center">

  <!-- Reader Navigation -->
  **[üìñ Read Online](https://mlsysbook.ai)** ‚Ä¢
  **[Tinyüî•Torch](https://mlsysbook.ai/tinytorch)** ‚Ä¢
  **[üìÑ Download PDF](https://mlsysbook.ai/assets/downloads/Machine-Learning-Systems.pdf)** ‚Ä¢
  **[üìì Download EPUB](https://mlsysbook.ai/epub)** ‚Ä¢
  **[üåê Explore Ecosystem](https://mlsysbook.org)**

</p>

üìö **Hardcopy edition coming 2026 with MIT Press.**

</div>

---

## Mission

**The world is rushing to build AI systems. It is not engineering them.**

That gap is what we mean by AI engineering.

**AI engineering is the discipline of building efficient, reliable, safe, and robust intelligent systems that operate in the real world, not just models in isolation.**

**Our mission:** Establish AI engineering as a foundational discipline, alongside software engineering and computer engineering, by teaching how to design, build, and evaluate end to end intelligent systems. The long term impact of AI will be shaped by engineers who can turn ideas into working, dependable systems.

---

## What‚Äôs in this repo

This repository is the open learning stack for AI systems engineering.

It includes the textbook source, TinyTorch, hardware kits, and upcoming co-labs that connect principles to runnable code and real devices.

---

## Start Here

Choose a path based on your goal.

**READ** Start with the [textbook](https://mlsysbook.ai). Try [Chapter 1](https://www.mlsysbook.ai/book/contents/core/introduction/introduction.html) and the [Benchmarking chapter](https://mlsysbook.ai/book/contents/core/benchmarking/benchmarking.html).

**BUILD** Start TinyTorch with the [getting started guide](https://mlsysbook.ai/tinytorch/getting-started.html). Begin with Module 01 and work up from CNNs to transformers and the MLPerf benchmarks.

**DEPLOY** Pick a [hardware kit](https://mlsysbook.ai/kits) and run the labs on Arduino, Raspberry Pi, and other edge devices.

**CONNECT** Say hello in [Discussions](https://github.com/harvard-edge/cs249r_book/discussions). We will do our best to reply.

---

## The Learning Stack

The learning stack below shows how the textbook connects to hands on work and deployment. Read the textbook, then pick your path:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                               ‚îÇ
‚îÇ                           MACHINE LEARNING SYSTEMS                            ‚îÇ
‚îÇ                              Read the Textbook                                ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îÇ                    Theory ‚Ä¢ Concepts ‚Ä¢ Best Practices                         ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                        ‚îÇ
                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                          ‚îÇ             ‚îÇ             ‚îÇ
                          ‚ñº             ‚ñº             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                            HANDS-ON ACTIVITIES                                ‚îÇ
‚îÇ                           (pick one or all)                                   ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ     ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ    SOFTWARE     ‚îÇ      ‚îÇ    TINYTORCH    ‚îÇ      ‚îÇ    HARDWARE     ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ    CO-LABS      ‚îÇ      ‚îÇ    FRAMEWORK    ‚îÇ      ‚îÇ      LABS       ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ EXPLORE         ‚îÇ      ‚îÇ BUILD           ‚îÇ      ‚îÇ DEPLOY          ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ Run controlled  ‚îÇ      ‚îÇ Understand      ‚îÇ      ‚îÇ Engineer under  ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ experiments on  ‚îÇ      ‚îÇ frameworks by   ‚îÇ      ‚îÇ real constraints‚îÇ     ‚îÇ
‚îÇ     ‚îÇ latency, memory,‚îÇ      ‚îÇ implementing    ‚îÇ      ‚îÇ memory, power,  ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ energy, cost    ‚îÇ      ‚îÇ them            ‚îÇ      ‚îÇ timing, safety  ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ (coming 2026)   ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ Arduino, Pi     ‚îÇ     ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îÇ           EXPLORE                  BUILD                   DEPLOY             ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                        ‚îÇ
                                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                               ‚îÇ
‚îÇ                                  AI OLYMPICS                                  ‚îÇ
‚îÇ                                 Prove Mastery                                 ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îÇ       Compete across all tracks ‚Ä¢ University teams ‚Ä¢ Public leaderboards      ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îÇ                                (coming 2026)                                  ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

| | Component | What You Do | Link |
|--|-----------|-------------|------|
| **READ** | [üìñ Textbook](https://mlsysbook.ai) | Understand ML systems concepts | [book/](book/README.md) |
| **EXPLORE** | üîÆ Software Co-Labs | Run controlled experiments on latency, memory, energy, cost | *Coming 2026* |
| **BUILD** | [üî• TinyTorch](https://mlsysbook.ai/tinytorch) | Understand frameworks by implementing them | [tinytorch/](tinytorch/README.md) |
| **DEPLOY** | [üîß Hardware Kits](https://mlsysbook.ai/kits) | Engineer under real constraints: memory, power, timing, safety | [kits/](kits/README.md) |
| **PROVE** | üèÜ AI Olympics | Compete and benchmark across all tracks | *Coming 2026* |

**What each path teaches:**
- **EXPLORE** teaches *why* ‚Äî Understand tradeoffs. Change batch sizes, precision, model architectures and see how latency, memory, and accuracy shift.
- **BUILD** teaches *how* ‚Äî Understand internals. Implement autograd, optimizers, and attention from scratch to see how TensorFlow and PyTorch actually work.
- **DEPLOY** teaches *where* ‚Äî Understand constraints. Face real memory limits, power budgets, and latency requirements on actual hardware.

---

## What You Will Learn

This textbook teaches you to think at the intersection of machine learning and systems engineering. Each chapter bridges algorithmic concepts with the infrastructure that makes them work in practice.

### The ML ‚Üî Systems Bridge

| ML Concept | Systems Concept | What You Learn |
|------------|-----------------|----------------|
| Model parameters | Memory constraints | How to fit large models on resource-limited devices |
| Inference latency | Hardware acceleration | How GPUs, TPUs, and accelerators execute neural networks |
| Training convergence | Compute efficiency | How mixed-precision and optimization techniques reduce cost |
| Model accuracy | Quantization and pruning | How to compress models while preserving performance |
| Data requirements | Pipeline infrastructure | How to build efficient data loading and preprocessing |
| Model deployment | MLOps practices | How to monitor, version, and update models in production |
| Privacy constraints | On-device learning | How to train and adapt models without sending data to the cloud |

### Book Structure

| Part | Focus | Chapters |
|------|-------|----------|
| **I. Foundations** | Core concepts | Introduction, ML Systems, DL Primer, Architectures |
| **II. Design** | Building blocks | Workflow, Data Engineering, Frameworks, Training |
| **III. Performance** | Making it fast | Efficient AI, Optimizations, HW Acceleration, Benchmarking |
| **IV. Deployment** | Making it work | MLOps, On-device Learning, Privacy, Robustness |
| **V. Trust** | Making it right | Responsible AI, Sustainable AI, AI for Good |
| **VI. Frontiers** | What's next | Emerging trends and future directions |

---

## What Makes This Different

This is a living textbook. We keep it updated as the field grows, with community input along the way.

AI may feel like it is moving at lightning speed, but the engineering building blocks that make it work do not change as quickly as the headlines. This project is built around those stable foundations.

Think of it like LEGO. New sets arrive all the time, but the bricks themselves stay the same. Once you learn how the bricks fit together, you can build anything. Here, those "AI bricks" are the solid systems principles that make AI work.

Whether you are reading a chapter, running a lab, or sharing feedback, you are helping make these ideas more accessible to the next learner.

### Research to Teaching Loop

We use the same loop for research and teaching: define the system problem, build a reference implementation, benchmark it, then turn it into curriculum and tooling so others can reproduce and extend it.

| Loop Step | Research Artifacts | Teaching Artifacts |
|-----------|-------------------|-------------------|
| **Measure** | Benchmarks, suites, metrics | Benchmarking chapter, assignments |
| **Build** | Reference systems, compilers, runtimes | TinyTorch modules, co-labs |
| **Deploy** | Hardware targets, constraints, reliability | Hardware labs, kits |

---

## Support This Work

We are working toward **1 million learners by 2030** so that AI engineering becomes a shared, teachable discipline, not a collection of isolated practices. Every star, share, and contribution helps move this effort forward.

### Why GitHub Stars Matter

<div align="center">

*What gets measured gets improved.*

Each star is a learner, educator, or supporter who believes AI systems should be engineered with rigor and real world constraints in mind.

[![Stars](https://img.shields.io/github/stars/harvard-edge/cs249r_book?style=for-the-badge&logo=github&color=gold)](https://github.com/harvard-edge/cs249r_book/stargazers)

[![Star History Chart](https://api.star-history.com/svg?repos=harvard-edge/cs249r_book&type=Date)](https://star-history.com/#harvard-edge/cs249r_book&Date)

1 learner ‚Üí 10 learners ‚Üí 100 learners ‚Üí 1,000 learners ‚Üí **10,000 learners** ‚Üí 100,000 learners ‚Üí **1M learners**

</div>

Stars are not the goal. They are a signal.

A visible, growing community makes it easier for universities, foundations, and industry partners to adopt this material, donate hardware, and fund workshops. That momentum lowers the barrier for the next institution, the next classroom, and the next cohort of learners.

Support raised through this signal flows into [Open Collective](https://opencollective.com/mlsysbook) and funds concrete outcomes such as TinyML4D workshops, hardware kits for underserved classrooms, and the infrastructure required to keep this resource free and open.

One click can unlock the next classroom, the next contributor, and the next generation of AI engineers.

### Fund the Mission

<div align="center">

All contributions go to [Open Collective](https://opencollective.com/mlsysbook), a transparent fund that supports educational outreach.

[![Open Collective](https://img.shields.io/badge/üíù%20Support%20AI%20Education-Open%20Collective-blue.svg?style=for-the-badge)](https://opencollective.com/mlsysbook)

</div>

---

## Community and Resources

| Resource | Description |
|---|---|
| [üìñ **Textbook**](https://mlsysbook.ai) | Interactive online textbook |
| [üî• **TinyTorch**](https://mlsysbook.ai/tinytorch) | Build ML frameworks from scratch |
| [üîß **Hardware Kits**](https://mlsysbook.ai/kits) | Deploy to Arduino, Raspberry Pi, edge devices |
| [üåê **Ecosystem**](https://mlsysbook.org) | Resources, workshops, and community |
| [üí¨ **Discussions**](https://github.com/harvard-edge/cs249r_book/discussions) | Questions and ideas |

---

## Contributing

We welcome contributions to the book, TinyTorch, and hardware kits!

| I want to... | Go here |
|--------------|---------|
| Fix a typo or improve a chapter | [book/docs/CONTRIBUTING.md](book/docs/CONTRIBUTING.md) |
| Add a TinyTorch module or fix a bug | [tinytorch/CONTRIBUTING.md](tinytorch/CONTRIBUTING.md) |
| Improve hardware labs | [kits/README.md](kits/README.md) |
| Report an issue | [GitHub Issues](https://github.com/harvard-edge/cs249r_book/issues) |
| Ask a question | [GitHub Discussions](https://github.com/harvard-edge/cs249r_book/discussions) |

---

## Citation & License

### Citation
```bibtex
@inproceedings{reddi2024mlsysbook,
  title        = {MLSysBook.AI: Principles and Practices of Machine Learning Systems Engineering},
  author       = {Reddi, Vijay Janapa},
  booktitle    = {2024 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ ISSS)},
  pages        = {41--42},
  year         = {2024},
  organization = {IEEE},
  url          = {https://mlsysbook.org}
}
```

### License

This project uses a dual-license structure:

| Component | License | What It Means |
|-----------|---------|---------------|
| **Book content** | [CC BY-NC-ND 4.0](LICENSE.md) | Share freely with attribution; no commercial use; no derivatives |
| **TinyTorch code** | [Apache 2.0](tinytorch/LICENSE) | Use, modify, and distribute freely; includes patent protection |

The textbook content (chapters, figures, explanations) is educational material that should circulate with attribution and without commercial exploitation. The software framework is a tool designed to be easy for anyone to use, modify, or integrate into their own projects.

---

## Contributors

Thanks goes to these wonderful people who have contributed to making this resource better for everyone!

**Legend:** ü™≤ Bug Hunter ¬∑ üßë‚Äçüíª Code Contributor ¬∑ ‚úçÔ∏è Doc Wizard ¬∑ üé® Design Artist ¬∑ üß† Idea Spark ¬∑ üîé Code Reviewer ¬∑ üß™ Test Tinkerer ¬∑ üõ†Ô∏è Tool Builder

### üìñ Textbook Contributors

<!-- BOOK-CONTRIBUTORS-START -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
<table>
  <tbody>
    <tr>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/profvjreddi"><img src="https://avatars.githubusercontent.com/profvjreddi?v=4?s=50" width="50px;" alt="Vijay Janapa Reddi"/><br /><sub><b>Vijay Janapa Reddi</b></sub></a><br />ü™≤ üßë‚Äçüíª üé® ‚úçÔ∏è üß† üîé üß™ üõ†Ô∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/Mjrovai"><img src="https://avatars.githubusercontent.com/Mjrovai?v=4?s=50" width="50px;" alt="Marcelo Rovai"/><br /><sub><b>Marcelo Rovai</b></sub></a><br />üßë‚Äçüíª üé® üß™</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/GabrielAmazonas"><img src="https://avatars.githubusercontent.com/GabrielAmazonas?v=4?s=50" width="50px;" alt="Gabriel Amazonas"/><br /><sub><b>Gabriel Amazonas</b></sub></a><br />ü™≤ ‚úçÔ∏è üß†</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/kai4avaya"><img src="https://avatars.githubusercontent.com/kai4avaya?v=4?s=50" width="50px;" alt="Kai Kleinbard"/><br /><sub><b>Kai Kleinbard</b></sub></a><br />üßë‚Äçüíª üõ†Ô∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/didier-durand"><img src="https://avatars.githubusercontent.com/didier-durand?v=4?s=50" width="50px;" alt="Didier Durand"/><br /><sub><b>Didier Durand</b></sub></a><br />‚úçÔ∏è ü™≤</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/hzeljko"><img src="https://avatars.githubusercontent.com/hzeljko?v=4?s=50" width="50px;" alt="Zeljko Hrcek"/><br /><sub><b>Zeljko Hrcek</b></sub></a><br />üßë‚Äçüíª</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/jasonjabbour"><img src="https://avatars.githubusercontent.com/jasonjabbour?v=4?s=50" width="50px;" alt="Jason Jabbour"/><br /><sub><b>Jason Jabbour</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/uchendui"><img src="https://avatars.githubusercontent.com/uchendui?v=4?s=50" width="50px;" alt="Ikechukwu Uchendu"/><br /><sub><b>Ikechukwu Uchendu</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/Naeemkh"><img src="https://avatars.githubusercontent.com/Naeemkh?v=4?s=50" width="50px;" alt="Naeem Khoshnevis"/><br /><sub><b>Naeem Khoshnevis</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/Sara-Khosravi"><img src="https://avatars.githubusercontent.com/Sara-Khosravi?v=4?s=50" width="50px;" alt="Sara Khosravi"/><br /><sub><b>Sara Khosravi</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/V0XNIHILI"><img src="https://avatars.githubusercontent.com/V0XNIHILI?v=4?s=50" width="50px;" alt="Douwe den Blanken"/><br /><sub><b>Douwe den Blanken</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/18jeffreyma"><img src="https://avatars.githubusercontent.com/18jeffreyma?v=4?s=50" width="50px;" alt="Jeffrey Ma"/><br /><sub><b>Jeffrey Ma</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/shanzehbatool"><img src="https://avatars.githubusercontent.com/shanzehbatool?v=4?s=50" width="50px;" alt="shanzehbatool"/><br /><sub><b>shanzehbatool</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/eliasab16"><img src="https://avatars.githubusercontent.com/eliasab16?v=4?s=50" width="50px;" alt="Elias"/><br /><sub><b>Elias</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/JaredP94"><img src="https://avatars.githubusercontent.com/JaredP94?v=4?s=50" width="50px;" alt="Jared Ping"/><br /><sub><b>Jared Ping</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/ishapira1"><img src="https://avatars.githubusercontent.com/ishapira1?v=4?s=50" width="50px;" alt="Itai Shapira"/><br /><sub><b>Itai Shapira</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/8863743b4f26c1a20e730fcf7ebc3bc0?d=identicon&s=100?v=4?s=50" width="50px;" alt="Maximilian Lam"/><br /><sub><b>Maximilian Lam</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/jaysonzlin"><img src="https://avatars.githubusercontent.com/jaysonzlin?v=4?s=50" width="50px;" alt="Jayson Lin"/><br /><sub><b>Jayson Lin</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/sophiacho1"><img src="https://avatars.githubusercontent.com/sophiacho1?v=4?s=50" width="50px;" alt="Sophia Cho"/><br /><sub><b>Sophia Cho</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/andreamurillomtz"><img src="https://avatars.githubusercontent.com/andreamurillomtz?v=4?s=50" width="50px;" alt="Andrea"/><br /><sub><b>Andrea</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/alxrod"><img src="https://avatars.githubusercontent.com/alxrod?v=4?s=50" width="50px;" alt="Alex Rodriguez"/><br /><sub><b>Alex Rodriguez</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/korneelf1"><img src="https://avatars.githubusercontent.com/korneelf1?v=4?s=50" width="50px;" alt="Korneel Van den Berghe"/><br /><sub><b>Korneel Van den Berghe</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/foundingnimo"><img src="https://avatars.githubusercontent.com/foundingnimo?v=4?s=50" width="50px;" alt="Nimo"/><br /><sub><b>Nimo</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/colbybanbury"><img src="https://avatars.githubusercontent.com/colbybanbury?v=4?s=50" width="50px;" alt="Colby Banbury"/><br /><sub><b>Colby Banbury</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/zishenwan"><img src="https://avatars.githubusercontent.com/zishenwan?v=4?s=50" width="50px;" alt="Zishen Wan"/><br /><sub><b>Zishen Wan</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/mmaz"><img src="https://avatars.githubusercontent.com/mmaz?v=4?s=50" width="50px;" alt="Mark Mazumder"/><br /><sub><b>Mark Mazumder</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/ma3mool"><img src="https://avatars.githubusercontent.com/ma3mool?v=4?s=50" width="50px;" alt="Abdulrahman Mahmoud"/><br /><sub><b>Abdulrahman Mahmoud</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/DivyaAmirtharaj"><img src="https://avatars.githubusercontent.com/DivyaAmirtharaj?v=4?s=50" width="50px;" alt="Divya Amirtharaj"/><br /><sub><b>Divya Amirtharaj</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/srivatsankrishnan"><img src="https://avatars.githubusercontent.com/srivatsankrishnan?v=4?s=50" width="50px;" alt="Srivatsan Krishnan"/><br /><sub><b>Srivatsan Krishnan</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/arnaumarin"><img src="https://avatars.githubusercontent.com/arnaumarin?v=4?s=50" width="50px;" alt="marin-llobet"/><br /><sub><b>marin-llobet</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/aptl26"><img src="https://avatars.githubusercontent.com/aptl26?v=4?s=50" width="50px;" alt="Aghyad Deeb"/><br /><sub><b>Aghyad Deeb</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/James-QiuHaoran"><img src="https://avatars.githubusercontent.com/James-QiuHaoran?v=4?s=50" width="50px;" alt="Haoran Qiu"/><br /><sub><b>Haoran Qiu</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/Ekhao"><img src="https://avatars.githubusercontent.com/Ekhao?v=4?s=50" width="50px;" alt="Emil Njor"/><br /><sub><b>Emil Njor</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/ELSuitorHarvard"><img src="https://avatars.githubusercontent.com/ELSuitorHarvard?v=4?s=50" width="50px;" alt="ELSuitorHarvard"/><br /><sub><b>ELSuitorHarvard</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/kaiM0ves"><img src="https://avatars.githubusercontent.com/kaiM0ves?v=4?s=50" width="50px;" alt="kaiM0ves"/><br /><sub><b>kaiM0ves</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/oishib"><img src="https://avatars.githubusercontent.com/oishib?v=4?s=50" width="50px;" alt="oishib"/><br /><sub><b>oishib</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/jared-ni"><img src="https://avatars.githubusercontent.com/jared-ni?v=4?s=50" width="50px;" alt="Jared Ni"/><br /><sub><b>Jared Ni</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/AditiR-42"><img src="https://avatars.githubusercontent.com/AditiR-42?v=4?s=50" width="50px;" alt="Aditi Raju"/><br /><sub><b>Aditi Raju</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/MichaelSchnebly"><img src="https://avatars.githubusercontent.com/MichaelSchnebly?v=4?s=50" width="50px;" alt="Michael Schnebly"/><br /><sub><b>Michael Schnebly</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/VThuong99"><img src="https://avatars.githubusercontent.com/VThuong99?v=4?s=50" width="50px;" alt="Thuong Duong"/><br /><sub><b>Thuong Duong</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/leo47007"><img src="https://avatars.githubusercontent.com/leo47007?v=4?s=50" width="50px;" alt="Yu-Shun Hsiao"/><br /><sub><b>Yu-Shun Hsiao</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/BaeHenryS"><img src="https://avatars.githubusercontent.com/BaeHenryS?v=4?s=50" width="50px;" alt="Henry Bae"/><br /><sub><b>Henry Bae</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/eimlav"><img src="https://avatars.githubusercontent.com/eimlav?v=4?s=50" width="50px;" alt="Eimhin Laverty"/><br /><sub><b>Eimhin Laverty</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/jaywonchung"><img src="https://avatars.githubusercontent.com/jaywonchung?v=4?s=50" width="50px;" alt="Jae-Won Chung"/><br /><sub><b>Jae-Won Chung</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/ShvetankPrakash"><img src="https://avatars.githubusercontent.com/ShvetankPrakash?v=4?s=50" width="50px;" alt="Shvetank Prakash"/><br /><sub><b>Shvetank Prakash</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/marcozennaro"><img src="https://avatars.githubusercontent.com/marcozennaro?v=4?s=50" width="50px;" alt="Marco Zennaro"/><br /><sub><b>Marco Zennaro</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/aryatschand"><img src="https://avatars.githubusercontent.com/aryatschand?v=4?s=50" width="50px;" alt="Arya Tschand"/><br /><sub><b>Arya Tschand</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/arbass22"><img src="https://avatars.githubusercontent.com/arbass22?v=4?s=50" width="50px;" alt="Andrew Bass"/><br /><sub><b>Andrew Bass</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/pongtr"><img src="https://avatars.githubusercontent.com/pongtr?v=4?s=50" width="50px;" alt="Pong Trairatvorakul"/><br /><sub><b>Pong Trairatvorakul</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/euranofshin"><img src="https://avatars.githubusercontent.com/euranofshin?v=4?s=50" width="50px;" alt="Eura Nofshin"/><br /><sub><b>Eura Nofshin</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/0c931fcfd03cd548d44c90602dd773ba?d=identicon&s=100?v=4?s=50" width="50px;" alt="Matthew Stewart"/><br /><sub><b>Matthew Stewart</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/af39c27c6090c50a1921a9b6366e81cc?d=identicon&s=100?v=4?s=50" width="50px;" alt="Emeka Ezike"/><br /><sub><b>Emeka Ezike</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/jianqingdu"><img src="https://avatars.githubusercontent.com/jianqingdu?v=4?s=50" width="50px;" alt="jianqingdu"/><br /><sub><b>jianqingdu</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/jzhou1318"><img src="https://avatars.githubusercontent.com/jzhou1318?v=4?s=50" width="50px;" alt="Jennifer Zhou"/><br /><sub><b>Jennifer Zhou</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/vitasam"><img src="https://avatars.githubusercontent.com/vitasam?v=4?s=50" width="50px;" alt="The Random DIY"/><br /><sub><b>The Random DIY</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/468ef35acc69f3266efd700992daa369?d=identicon&s=100?v=4?s=50" width="50px;" alt="Fatima Shah"/><br /><sub><b>Fatima Shah</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/BrunoScaglione"><img src="https://avatars.githubusercontent.com/BrunoScaglione?v=4?s=50" width="50px;" alt="Bruno Scaglione"/><br /><sub><b>Bruno Scaglione</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/Allen-Kuang"><img src="https://avatars.githubusercontent.com/Allen-Kuang?v=4?s=50" width="50px;" alt="Allen-Kuang"/><br /><sub><b>Allen-Kuang</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/4ad8cdf19eb3b666ace97d3eedb19278?d=identicon&s=100?v=4?s=50" width="50px;" alt="Tess314"/><br /><sub><b>Tess314</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/taunoe"><img src="https://avatars.githubusercontent.com/taunoe?v=4?s=50" width="50px;" alt="Tauno Erik"/><br /><sub><b>Tauno Erik</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/gnodipac886"><img src="https://avatars.githubusercontent.com/gnodipac886?v=4?s=50" width="50px;" alt="gnodipac886"/><br /><sub><b>gnodipac886</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/serco425"><img src="https://avatars.githubusercontent.com/serco425?v=4?s=50" width="50px;" alt="Sercan Ayg√ºn"/><br /><sub><b>Sercan Ayg√ºn</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/TheHiddenLayer"><img src="https://avatars.githubusercontent.com/TheHiddenLayer?v=4?s=50" width="50px;" alt="TheHiddenLayer"/><br /><sub><b>TheHiddenLayer</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/Gjain234"><img src="https://avatars.githubusercontent.com/Gjain234?v=4?s=50" width="50px;" alt="Gauri Jain"/><br /><sub><b>Gauri Jain</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/FinAminToastCrunch"><img src="https://avatars.githubusercontent.com/FinAminToastCrunch?v=4?s=50" width="50px;" alt="Fin Amin"/><br /><sub><b>Fin Amin</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/alex-oesterling"><img src="https://avatars.githubusercontent.com/alex-oesterling?v=4?s=50" width="50px;" alt="Alex Oesterling"/><br /><sub><b>Alex Oesterling</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/AbenezerKb"><img src="https://avatars.githubusercontent.com/AbenezerKb?v=4?s=50" width="50px;" alt="Abenezer Angamo"/><br /><sub><b>Abenezer Angamo</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/BravoBaldo"><img src="https://avatars.githubusercontent.com/BravoBaldo?v=4?s=50" width="50px;" alt="Baldassarre Cesarano"/><br /><sub><b>Baldassarre Cesarano</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/Jahnic-kb"><img src="https://avatars.githubusercontent.com/Jahnic-kb?v=4?s=50" width="50px;" alt="Jahnic Beck"/><br /><sub><b>Jahnic Beck</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/aethernavshulkraven-allain"><img src="https://avatars.githubusercontent.com/aethernavshulkraven-allain?v=4?s=50" width="50px;" alt="‡§Ö‡§∞‡§®‡§µ ‡§∂‡•Å‡§ï‡•ç‡§≤‡§æ | Arnav Shukla"/><br /><sub><b>‡§Ö‡§∞‡§®‡§µ ‡§∂‡•Å‡§ï‡•ç‡§≤‡§æ | Arnav Shukla</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/RinZ27"><img src="https://avatars.githubusercontent.com/RinZ27?v=4?s=50" width="50px;" alt="Rin"/><br /><sub><b>Rin</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/bilgeacun"><img src="https://avatars.githubusercontent.com/bilgeacun?v=4?s=50" width="50px;" alt="Bilge Acun"/><br /><sub><b>Bilge Acun</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/atcheng2"><img src="https://avatars.githubusercontent.com/atcheng2?v=4?s=50" width="50px;" alt="Andy Cheng"/><br /><sub><b>Andy Cheng</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/arighosh05"><img src="https://avatars.githubusercontent.com/arighosh05?v=4?s=50" width="50px;" alt="Aritra Ghosh"/><br /><sub><b>Aritra Ghosh</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/abigailswallow"><img src="https://avatars.githubusercontent.com/abigailswallow?v=4?s=50" width="50px;" alt="abigailswallow"/><br /><sub><b>abigailswallow</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/YangZhou1997"><img src="https://avatars.githubusercontent.com/YangZhou1997?v=4?s=50" width="50px;" alt="Yang Zhou"/><br /><sub><b>Yang Zhou</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/XaicuL"><img src="https://avatars.githubusercontent.com/XaicuL?v=4?s=50" width="50px;" alt="JEON HYUNJUN(Luciano)"/><br /><sub><b>JEON HYUNJUN(Luciano)</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/emmanuel2406"><img src="https://avatars.githubusercontent.com/emmanuel2406?v=4?s=50" width="50px;" alt="Emmanuel Rassou"/><br /><sub><b>Emmanuel Rassou</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/jasonlyik"><img src="https://avatars.githubusercontent.com/jasonlyik?v=4?s=50" width="50px;" alt="Jason Yik"/><br /><sub><b>Jason Yik</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/jessicaquaye"><img src="https://avatars.githubusercontent.com/jessicaquaye?v=4?s=50" width="50px;" alt="Jessica Quaye"/><br /><sub><b>Jessica Quaye</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/cursoragent"><img src="https://avatars.githubusercontent.com/cursoragent?v=4?s=50" width="50px;" alt="Cursor Agent"/><br /><sub><b>Cursor Agent</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/happyappledog"><img src="https://avatars.githubusercontent.com/happyappledog?v=4?s=50" width="50px;" alt="happyappledog"/><br /><sub><b>happyappledog</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/snuggs"><img src="https://avatars.githubusercontent.com/snuggs?v=4?s=50" width="50px;" alt="Snuggs"/><br /><sub><b>Snuggs</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/swilcock0"><img src="https://avatars.githubusercontent.com/swilcock0?v=4?s=50" width="50px;" alt="Sam Wilcock"/><br /><sub><b>Sam Wilcock</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/sjohri20"><img src="https://avatars.githubusercontent.com/sjohri20?v=4?s=50" width="50px;" alt="Shreya Johri"/><br /><sub><b>Shreya Johri</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/skmur"><img src="https://avatars.githubusercontent.com/skmur?v=4?s=50" width="50px;" alt="Sonia Murthy"/><br /><sub><b>Sonia Murthy</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/fc4f3460cdfb9365ab59bdeafb06413e?d=identicon&s=100?v=4?s=50" width="50px;" alt="Costin-Andrei Oncescu"/><br /><sub><b>Costin-Andrei Oncescu</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/0d6b8616427d8b19d425c9808692e347?d=identicon&s=100?v=4?s=50" width="50px;" alt="formlsysbookissue"/><br /><sub><b>formlsysbookissue</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/7cd8d5dfd83071f23979019d97655dc5?d=identicon&s=100?v=4?s=50" width="50px;" alt="Annie Laurie Cook"/><br /><sub><b>Annie Laurie Cook</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/5aa037840c0ca11ee42784ed4843c655?d=identicon&s=100?v=4?s=50" width="50px;" alt="Parampreet Singh"/><br /><sub><b>Parampreet Singh</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/b15b6e0e9adf58099905c1a0fd474cb9?d=identicon&s=100?v=4?s=50" width="50px;" alt="Vijay Edupuganti"/><br /><sub><b>Vijay Edupuganti</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/f88052cca4f401d9b0f43aed0a53434a?d=identicon&s=100?v=4?s=50" width="50px;" alt="Jothi Ramaswamy"/><br /><sub><b>Jothi Ramaswamy</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/35a8d9ffd03f05e79a2c6ce6206a56f2?d=identicon&s=100?v=4?s=50" width="50px;" alt="Batur Arslan"/><br /><sub><b>Batur Arslan</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/bd53d146aa888548c8db4da02bf81e7a?d=identicon&s=100?v=4?s=50" width="50px;" alt="Curren Iyer"/><br /><sub><b>Curren Iyer</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/8d8410338458e08bd5e4b96f58e1c217?d=identicon&s=100?v=4?s=50" width="50px;" alt="Edward Jin"/><br /><sub><b>Edward Jin</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/28c6123d2c9f75578d3ccdedb0df3d11?d=identicon&s=100?v=4?s=50" width="50px;" alt="Tess Watt"/><br /><sub><b>Tess Watt</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/ef139181fe00190f21730f6912532e9e?d=identicon&s=100?v=4?s=50" width="50px;" alt="bluebaer7"/><br /><sub><b>bluebaer7</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/f5d58ba6aa9b00189d4c018d370e8f43?d=identicon&s=100?v=4?s=50" width="50px;" alt="yanjingl"/><br /><sub><b>yanjingl</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/a5a47df988ab1720dd706062e523ca32?d=identicon&s=100?v=4?s=50" width="50px;" alt="a-saraf"/><br /><sub><b>a-saraf</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/c2dc311aa8122d5f5f061e1db14682b1?d=identicon&s=100?v=4?s=50" width="50px;" alt="songhan"/><br /><sub><b>songhan</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/4814aad67982ab07a69006a1ce9d2a72?d=identicon&s=100?v=4?s=50" width="50px;" alt="jvijay"/><br /><sub><b>jvijay</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/43b1feff77c8a95fd581774fb8ec891f?d=identicon&s=100?v=4?s=50" width="50px;" alt="Zishen"/><br /><sub><b>Zishen</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/BunningsWarehouseOfficial"><img src="https://avatars.githubusercontent.com/u/49220945?v=4?v=4?s=50" width="50px;" alt="Kristian Rado≈°"/><br /><sub><b>Kristian Rado≈°</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/minhdang26403"><img src="https://avatars.githubusercontent.com/u/86156224?v=4?v=4?s=50" width="50px;" alt="Dang Truong"/><br /><sub><b>Dang Truong</b></sub></a><br />üßë‚Äçüíª</td>
    </tr>
  </tbody>
</table>

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->
<!-- BOOK-CONTRIBUTORS-END -->

---

### üî• TinyTorch Contributors

<!-- TINYTORCH-CONTRIBUTORS-START -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
<table>
  <tbody>
    <tr>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/profvjreddi"><img src="https://avatars.githubusercontent.com/profvjreddi?v=4?s=50" width="50px;" alt="Vijay Janapa Reddi"/><br /><sub><b>Vijay Janapa Reddi</b></sub></a><br />ü™≤ üßë‚Äçüíª üé® ‚úçÔ∏è üß† üîé üß™ üõ†Ô∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/kai4avaya"><img src="https://avatars.githubusercontent.com/kai4avaya?v=4?s=50" width="50px;" alt="kai"/><br /><sub><b>kai</b></sub></a><br />ü™≤ üßë‚Äçüíª üé® ‚úçÔ∏è üß™</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/minhdang26403"><img src="https://avatars.githubusercontent.com/minhdang26403?v=4?s=50" width="50px;" alt="Dang Truong"/><br /><sub><b>Dang Truong</b></sub></a><br />ü™≤ üßë‚Äçüíª ‚úçÔ∏è üß™</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/didier-durand"><img src="https://avatars.githubusercontent.com/didier-durand?v=4?s=50" width="50px;" alt="Didier Durand"/><br /><sub><b>Didier Durand</b></sub></a><br />ü™≤ üßë‚Äçüíª ‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/karthikdani"><img src="https://avatars.githubusercontent.com/karthikdani?v=4?s=50" width="50px;" alt="Karthik Dani"/><br /><sub><b>Karthik Dani</b></sub></a><br />ü™≤ üßë‚Äçüíª</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/avikde"><img src="https://avatars.githubusercontent.com/avikde?v=4?s=50" width="50px;" alt="Avik De"/><br /><sub><b>Avik De</b></sub></a><br />ü™≤ üß™</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/Takosaga"><img src="https://avatars.githubusercontent.com/Takosaga?v=4?s=50" width="50px;" alt="Takosaga"/><br /><sub><b>Takosaga</b></sub></a><br />ü™≤ ‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/rnjema"><img src="https://avatars.githubusercontent.com/rnjema?v=4?s=50" width="50px;" alt="rnjema"/><br /><sub><b>rnjema</b></sub></a><br />üßë‚Äçüíª üõ†Ô∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/joeswagson"><img src="https://avatars.githubusercontent.com/joeswagson?v=4?s=50" width="50px;" alt="joeswagson"/><br /><sub><b>joeswagson</b></sub></a><br />üßë‚Äçüíª üõ†Ô∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/AndreaMattiaGaravagno"><img src="https://avatars.githubusercontent.com/u/22458187?v=4?v=4?s=50" width="50px;" alt="AndreaMattiaGaravagno"/><br /><sub><b>AndreaMattiaGaravagno</b></sub></a><br />üßë‚Äçüíª ‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/AmirAlasady"><img src="https://avatars.githubusercontent.com/AmirAlasady?v=4?s=50" width="50px;" alt="Amir Alasady"/><br /><sub><b>Amir Alasady</b></sub></a><br />ü™≤</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/jettythek"><img src="https://avatars.githubusercontent.com/jettythek?v=4?s=50" width="50px;" alt="jettythek"/><br /><sub><b>jettythek</b></sub></a><br />üßë‚Äçüíª</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/wz1114841863"><img src="https://avatars.githubusercontent.com/wz1114841863?v=4?s=50" width="50px;" alt="wzz"/><br /><sub><b>wzz</b></sub></a><br />ü™≤</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/ngbolin"><img src="https://avatars.githubusercontent.com/u/9389997?v=4?v=4?s=50" width="50px;" alt="Ng Bo Lin"/><br /><sub><b>Ng Bo Lin</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/keo-dara"><img src="https://avatars.githubusercontent.com/u/175544368?v=4?v=4?s=50" width="50px;" alt="keo-dara"/><br /><sub><b>keo-dara</b></sub></a><br />ü™≤</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/Kobra299"><img src="https://avatars.githubusercontent.com/u/4283156?v=4?v=4?s=50" width="50px;" alt="Wayne Norman"/><br /><sub><b>Wayne Norman</b></sub></a><br />ü™≤</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/lalalostcode"><img src="https://avatars.githubusercontent.com/u/149884766?v=4?v=4?s=50" width="50px;" alt="Ilham Rafiqin"/><br /><sub><b>Ilham Rafiqin</b></sub></a><br />ü™≤</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/oscarf189"><img src="https://avatars.githubusercontent.com/u/28113740?v=4?v=4?s=50" width="50px;" alt="Oscar Flores"/><br /><sub><b>Oscar Flores</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/harishb00a"><img src="https://avatars.githubusercontent.com/harishb00a?v=4?s=50" width="50px;" alt="harishb00a"/><br /><sub><b>harishb00a</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/sotoblanco"><img src="https://avatars.githubusercontent.com/u/46135649?v=4?v=4?s=50" width="50px;" alt="Pastor Soto"/><br /><sub><b>Pastor Soto</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
  </tbody>
</table>

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->
<!-- TINYTORCH-CONTRIBUTORS-END -->

---

### üõ†Ô∏è Hardware Kits Contributors

<!-- KITS-CONTRIBUTORS-START -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
<table>
  <tbody>
    <tr>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/profvjreddi"><img src="https://avatars.githubusercontent.com/profvjreddi?v=4?s=50" width="50px;" alt="Vijay Janapa Reddi"/><br /><sub><b>Vijay Janapa Reddi</b></sub></a><br />ü™≤ üßë‚Äçüíª üé® ‚úçÔ∏è üß™ üõ†Ô∏è</td>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/Mjrovai"><img src="https://avatars.githubusercontent.com/Mjrovai?v=4?s=50" width="50px;" alt="Marcelo Rovai"/><br /><sub><b>Marcelo Rovai</b></sub></a><br />‚úçÔ∏è üßë‚Äçüíª üé® </td>
    </tr>
  </tbody>
</table>

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->
<!-- KITS-CONTRIBUTORS-END -->

---

### üß™ Labs Contributors

<!-- LABS-CONTRIBUTORS-START -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
<table>
  <tbody>
    <tr>
      <td align="center" valign="top" width="11.11%"><a href="https://github.com/profvjreddi"><img src="https://avatars.githubusercontent.com/profvjreddi?v=4?s=50" width="50px;" alt="Vijay Janapa Reddi"/><br /><sub><b>Vijay Janapa Reddi</b></sub></a><br />üßë‚Äçüíª üé® ‚úçÔ∏è</td>
    </tr>
  </tbody>
</table>

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->
<!-- LABS-CONTRIBUTORS-END -->
---

<div align="center">

**[‚≠ê Star us on GitHub](https://github.com/harvard-edge/cs249r_book#support-this-work) ‚Ä¢ [‚úâÔ∏è Subscribe](https://buttondown.email/mlsysbook) ‚Ä¢ [üí¨ Join discussions](https://github.com/harvard-edge/cs249r_book/discussions) ‚Ä¢ [üåê Visit mlsysbook.ai](https://mlsysbook.ai)**

**Made with ‚ù§Ô∏è for AI engineers**<br>
*in the making, around the world* üåé
</div>


## Links discovered
- [![Book](https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/book-validate-dev.yml?branch=dev&label=Book&logo=githubactions&cacheSeconds=300)
- [![TinyTorch](https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/tinytorch-validate-dev.yml?branch=dev&label=TinyTorch&logo=python&cacheSeconds=300)
- [Updated](https://img.shields.io/github/last-commit/harvard-edge/cs249r_book/dev?label=Updated&logo=git&cacheSeconds=300)
- [![License](https://img.shields.io/badge/License-CC--BY--NC--ND%204.0-blue.svg)
- [![Cite](https://img.shields.io/badge/Cite-IEEE%202024-blue?logo=ieee)
- [![Fund Us](https://img.shields.io/badge/Fund%20Us-Open%20Collective-blue.svg?logo=open-collective)
- [üìñ Read Online](https://mlsysbook.ai)
- [Tinyüî•Torch](https://mlsysbook.ai/tinytorch)
- [üìÑ Download PDF](https://mlsysbook.ai/assets/downloads/Machine-Learning-Systems.pdf)
- [üìì Download EPUB](https://mlsysbook.ai/epub)
- [üåê Explore Ecosystem](https://mlsysbook.org)
- [textbook](https://mlsysbook.ai)
- [Chapter 1](https://www.mlsysbook.ai/book/contents/core/introduction/introduction.html)
- [Benchmarking chapter](https://mlsysbook.ai/book/contents/core/benchmarking/benchmarking.html)
- [getting started guide](https://mlsysbook.ai/tinytorch/getting-started.html)
- [hardware kit](https://mlsysbook.ai/kits)
- [Discussions](https://github.com/harvard-edge/cs249r_book/discussions)
- [üìñ Textbook](https://mlsysbook.ai)
- [book/](https://github.com/harvard-edge/cs249r_book/blob/dev/book/README.md)
- [üî• TinyTorch](https://mlsysbook.ai/tinytorch)
- [tinytorch/](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/README.md)
- [üîß Hardware Kits](https://mlsysbook.ai/kits)
- [kits/](https://github.com/harvard-edge/cs249r_book/blob/dev/kits/README.md)
- [![Stars](https://img.shields.io/github/stars/harvard-edge/cs249r_book?style=for-the-badge&logo=github&color=gold)
- [![Star History Chart](https://api.star-history.com/svg?repos=harvard-edge/cs249r_book&type=Date)
- [Open Collective](https://opencollective.com/mlsysbook)
- [![Open Collective](https://img.shields.io/badge/üíù%20Support%20AI%20Education-Open%20Collective-blue.svg?style=for-the-badge)
- [üìñ **Textbook**](https://mlsysbook.ai)
- [üî• **TinyTorch**](https://mlsysbook.ai/tinytorch)
- [üîß **Hardware Kits**](https://mlsysbook.ai/kits)
- [üåê **Ecosystem**](https://mlsysbook.org)
- [üí¨ **Discussions**](https://github.com/harvard-edge/cs249r_book/discussions)
- [book/docs/CONTRIBUTING.md](https://github.com/harvard-edge/cs249r_book/blob/dev/book/docs/CONTRIBUTING.md)
- [tinytorch/CONTRIBUTING.md](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/CONTRIBUTING.md)
- [kits/README.md](https://github.com/harvard-edge/cs249r_book/blob/dev/kits/README.md)
- [GitHub Issues](https://github.com/harvard-edge/cs249r_book/issues)
- [GitHub Discussions](https://github.com/harvard-edge/cs249r_book/discussions)
- [CC BY-NC-ND 4.0](https://github.com/harvard-edge/cs249r_book/blob/dev/LICENSE.md)
- [Apache 2.0](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/LICENSE.md)
- [‚≠ê Star us on GitHub](https://github.com/harvard-edge/cs249r_book#support-this-work)
- [‚úâÔ∏è Subscribe](https://buttondown.email/mlsysbook)
- [üí¨ Join discussions](https://github.com/harvard-edge/cs249r_book/discussions)
- [üåê Visit mlsysbook.ai](https://mlsysbook.ai)
- [English](https://github.com/harvard-edge/cs249r_book/blob/dev/README.md)
- [‰∏≠Êñá](https://github.com/harvard-edge/cs249r_book/blob/dev/README/README_zh.md)
- [Êó•Êú¨Ë™û](https://github.com/harvard-edge/cs249r_book/blob/dev/README/README_ja.md)
- [ÌïúÍµ≠Ïñ¥](https://github.com/harvard-edge/cs249r_book/blob/dev/README/README_ko.md)
- [<img src="https://avatars.githubusercontent.com/profvjreddi?v=4?s=50" width="50px;" alt="Vijay Janapa Reddi"/><br /><sub><b>Vijay Janapa Reddi</b></sub>](https://github.com/profvjreddi)
- [<img src="https://avatars.githubusercontent.com/Mjrovai?v=4?s=50" width="50px;" alt="Marcelo Rovai"/><br /><sub><b>Marcelo Rovai</b></sub>](https://github.com/Mjrovai)
- [<img src="https://avatars.githubusercontent.com/GabrielAmazonas?v=4?s=50" width="50px;" alt="Gabriel Amazonas"/><br /><sub><b>Gabriel Amazonas</b></sub>](https://github.com/GabrielAmazonas)
- [<img src="https://avatars.githubusercontent.com/kai4avaya?v=4?s=50" width="50px;" alt="Kai Kleinbard"/><br /><sub><b>Kai Kleinbard</b></sub>](https://github.com/kai4avaya)
- [<img src="https://avatars.githubusercontent.com/didier-durand?v=4?s=50" width="50px;" alt="Didier Durand"/><br /><sub><b>Didier Durand</b></sub>](https://github.com/didier-durand)
- [<img src="https://avatars.githubusercontent.com/hzeljko?v=4?s=50" width="50px;" alt="Zeljko Hrcek"/><br /><sub><b>Zeljko Hrcek</b></sub>](https://github.com/hzeljko)
- [<img src="https://avatars.githubusercontent.com/jasonjabbour?v=4?s=50" width="50px;" alt="Jason Jabbour"/><br /><sub><b>Jason Jabbour</b></sub>](https://github.com/jasonjabbour)
- [<img src="https://avatars.githubusercontent.com/uchendui?v=4?s=50" width="50px;" alt="Ikechukwu Uchendu"/><br /><sub><b>Ikechukwu Uchendu</b></sub>](https://github.com/uchendui)
- [<img src="https://avatars.githubusercontent.com/Naeemkh?v=4?s=50" width="50px;" alt="Naeem Khoshnevis"/><br /><sub><b>Naeem Khoshnevis</b></sub>](https://github.com/Naeemkh)
- [<img src="https://avatars.githubusercontent.com/Sara-Khosravi?v=4?s=50" width="50px;" alt="Sara Khosravi"/><br /><sub><b>Sara Khosravi</b></sub>](https://github.com/Sara-Khosravi)
- [<img src="https://avatars.githubusercontent.com/V0XNIHILI?v=4?s=50" width="50px;" alt="Douwe den Blanken"/><br /><sub><b>Douwe den Blanken</b></sub>](https://github.com/V0XNIHILI)
- [<img src="https://avatars.githubusercontent.com/18jeffreyma?v=4?s=50" width="50px;" alt="Jeffrey Ma"/><br /><sub><b>Jeffrey Ma</b></sub>](https://github.com/18jeffreyma)
- [<img src="https://avatars.githubusercontent.com/shanzehbatool?v=4?s=50" width="50px;" alt="shanzehbatool"/><br /><sub><b>shanzehbatool</b></sub>](https://github.com/shanzehbatool)
- [<img src="https://avatars.githubusercontent.com/eliasab16?v=4?s=50" width="50px;" alt="Elias"/><br /><sub><b>Elias</b></sub>](https://github.com/eliasab16)
- [<img src="https://avatars.githubusercontent.com/JaredP94?v=4?s=50" width="50px;" alt="Jared Ping"/><br /><sub><b>Jared Ping</b></sub>](https://github.com/JaredP94)
- [<img src="https://avatars.githubusercontent.com/ishapira1?v=4?s=50" width="50px;" alt="Itai Shapira"/><br /><sub><b>Itai Shapira</b></sub>](https://github.com/ishapira1)
- [<img src="https://www.gravatar.com/avatar/8863743b4f26c1a20e730fcf7ebc3bc0?d=identicon&s=100?v=4?s=50" width="50px;" alt="Maximilian Lam"/><br /><sub><b>Maximilian Lam</b></sub>](https://github.com/harvard-edge/cs249r_book/graphs/contributors)
- [<img src="https://avatars.githubusercontent.com/jaysonzlin?v=4?s=50" width="50px;" alt="Jayson Lin"/><br /><sub><b>Jayson Lin</b></sub>](https://github.com/jaysonzlin)
- [<img src="https://avatars.githubusercontent.com/sophiacho1?v=4?s=50" width="50px;" alt="Sophia Cho"/><br /><sub><b>Sophia Cho</b></sub>](https://github.com/sophiacho1)
- [<img src="https://avatars.githubusercontent.com/andreamurillomtz?v=4?s=50" width="50px;" alt="Andrea"/><br /><sub><b>Andrea</b></sub>](https://github.com/andreamurillomtz)
- [<img src="https://avatars.githubusercontent.com/alxrod?v=4?s=50" width="50px;" alt="Alex Rodriguez"/><br /><sub><b>Alex Rodriguez</b></sub>](https://github.com/alxrod)
- [<img src="https://avatars.githubusercontent.com/korneelf1?v=4?s=50" width="50px;" alt="Korneel Van den Berghe"/><br /><sub><b>Korneel Van den Berghe</b></sub>](https://github.com/korneelf1)
- [<img src="https://avatars.githubusercontent.com/foundingnimo?v=4?s=50" width="50px;" alt="Nimo"/><br /><sub><b>Nimo</b></sub>](https://github.com/foundingnimo)
- [<img src="https://avatars.githubusercontent.com/colbybanbury?v=4?s=50" width="50px;" alt="Colby Banbury"/><br /><sub><b>Colby Banbury</b></sub>](https://github.com/colbybanbury)
- [<img src="https://avatars.githubusercontent.com/zishenwan?v=4?s=50" width="50px;" alt="Zishen Wan"/><br /><sub><b>Zishen Wan</b></sub>](https://github.com/zishenwan)
- [<img src="https://avatars.githubusercontent.com/mmaz?v=4?s=50" width="50px;" alt="Mark Mazumder"/><br /><sub><b>Mark Mazumder</b></sub>](https://github.com/mmaz)
- [<img src="https://avatars.githubusercontent.com/ma3mool?v=4?s=50" width="50px;" alt="Abdulrahman Mahmoud"/><br /><sub><b>Abdulrahman Mahmoud</b></sub>](https://github.com/ma3mool)
- [<img src="https://avatars.githubusercontent.com/DivyaAmirtharaj?v=4?s=50" width="50px;" alt="Divya Amirtharaj"/><br /><sub><b>Divya Amirtharaj</b></sub>](https://github.com/DivyaAmirtharaj)
- [<img src="https://avatars.githubusercontent.com/srivatsankrishnan?v=4?s=50" width="50px;" alt="Srivatsan Krishnan"/><br /><sub><b>Srivatsan Krishnan</b></sub>](https://github.com/srivatsankrishnan)
- [<img src="https://avatars.githubusercontent.com/arnaumarin?v=4?s=50" width="50px;" alt="marin-llobet"/><br /><sub><b>marin-llobet</b></sub>](https://github.com/arnaumarin)
- [<img src="https://avatars.githubusercontent.com/aptl26?v=4?s=50" width="50px;" alt="Aghyad Deeb"/><br /><sub><b>Aghyad Deeb</b></sub>](https://github.com/aptl26)
- [<img src="https://avatars.githubusercontent.com/James-QiuHaoran?v=4?s=50" width="50px;" alt="Haoran Qiu"/><br /><sub><b>Haoran Qiu</b></sub>](https://github.com/James-QiuHaoran)
- [<img src="https://avatars.githubusercontent.com/Ekhao?v=4?s=50" width="50px;" alt="Emil Njor"/><br /><sub><b>Emil Njor</b></sub>](https://github.com/Ekhao)
- [<img src="https://avatars.githubusercontent.com/ELSuitorHarvard?v=4?s=50" width="50px;" alt="ELSuitorHarvard"/><br /><sub><b>ELSuitorHarvard</b></sub>](https://github.com/ELSuitorHarvard)
- [<img src="https://avatars.githubusercontent.com/kaiM0ves?v=4?s=50" width="50px;" alt="kaiM0ves"/><br /><sub><b>kaiM0ves</b></sub>](https://github.com/kaiM0ves)
- [<img src="https://avatars.githubusercontent.com/oishib?v=4?s=50" width="50px;" alt="oishib"/><br /><sub><b>oishib</b></sub>](https://github.com/oishib)
- [<img src="https://avatars.githubusercontent.com/jared-ni?v=4?s=50" width="50px;" alt="Jared Ni"/><br /><sub><b>Jared Ni</b></sub>](https://github.com/jared-ni)
- [<img src="https://avatars.githubusercontent.com/AditiR-42?v=4?s=50" width="50px;" alt="Aditi Raju"/><br /><sub><b>Aditi Raju</b></sub>](https://github.com/AditiR-42)
- [<img src="https://avatars.githubusercontent.com/MichaelSchnebly?v=4?s=50" width="50px;" alt="Michael Schnebly"/><br /><sub><b>Michael Schnebly</b></sub>](https://github.com/MichaelSchnebly)
- [<img src="https://avatars.githubusercontent.com/VThuong99?v=4?s=50" width="50px;" alt="Thuong Duong"/><br /><sub><b>Thuong Duong</b></sub>](https://github.com/VThuong99)
- [<img src="https://avatars.githubusercontent.com/leo47007?v=4?s=50" width="50px;" alt="Yu-Shun Hsiao"/><br /><sub><b>Yu-Shun Hsiao</b></sub>](https://github.com/leo47007)
- [<img src="https://avatars.githubusercontent.com/BaeHenryS?v=4?s=50" width="50px;" alt="Henry Bae"/><br /><sub><b>Henry Bae</b></sub>](https://github.com/BaeHenryS)
- [<img src="https://avatars.githubusercontent.com/eimlav?v=4?s=50" width="50px;" alt="Eimhin Laverty"/><br /><sub><b>Eimhin Laverty</b></sub>](https://github.com/eimlav)
- [<img src="https://avatars.githubusercontent.com/jaywonchung?v=4?s=50" width="50px;" alt="Jae-Won Chung"/><br /><sub><b>Jae-Won Chung</b></sub>](https://github.com/jaywonchung)
- [<img src="https://avatars.githubusercontent.com/ShvetankPrakash?v=4?s=50" width="50px;" alt="Shvetank Prakash"/><br /><sub><b>Shvetank Prakash</b></sub>](https://github.com/ShvetankPrakash)
- [<img src="https://avatars.githubusercontent.com/marcozennaro?v=4?s=50" width="50px;" alt="Marco Zennaro"/><br /><sub><b>Marco Zennaro</b></sub>](https://github.com/marcozennaro)
- [<img src="https://avatars.githubusercontent.com/aryatschand?v=4?s=50" width="50px;" alt="Arya Tschand"/><br /><sub><b>Arya Tschand</b></sub>](https://github.com/aryatschand)
- [<img src="https://avatars.githubusercontent.com/arbass22?v=4?s=50" width="50px;" alt="Andrew Bass"/><br /><sub><b>Andrew Bass</b></sub>](https://github.com/arbass22)
- [<img src="https://avatars.githubusercontent.com/pongtr?v=4?s=50" width="50px;" alt="Pong Trairatvorakul"/><br /><sub><b>Pong Trairatvorakul</b></sub>](https://github.com/pongtr)
- [<img src="https://avatars.githubusercontent.com/euranofshin?v=4?s=50" width="50px;" alt="Eura Nofshin"/><br /><sub><b>Eura Nofshin</b></sub>](https://github.com/euranofshin)
- [<img src="https://www.gravatar.com/avatar/0c931fcfd03cd548d44c90602dd773ba?d=identicon&s=100?v=4?s=50" width="50px;" alt="Matthew Stewart"/><br /><sub><b>Matthew Stewart</b></sub>](https://github.com/harvard-edge/cs249r_book/graphs/contributors)
- [<img src="https://www.gravatar.com/avatar/af39c27c6090c50a1921a9b6366e81cc?d=identicon&s=100?v=4?s=50" width="50px;" alt="Emeka Ezike"/><br /><sub><b>Emeka Ezike</b></sub>](https://github.com/harvard-edge/cs249r_book/graphs/contributors)
- [<img src="https://avatars.githubusercontent.com/jianqingdu?v=4?s=50" width="50px;" alt="jianqingdu"/><br /><sub><b>jianqingdu</b></sub>](https://github.com/jianqingdu)

--- _brand/scripts/subscribe-modal.js ---
/**
 * Subscribe Modal Component
 * Elegant popup subscription form for ML Systems Textbook
 */

(function() {
  'use strict';

  // Create modal HTML structure
  function createModalHTML() {
    return `
      <div id="subscribe-modal" class="modal-overlay" style="display: none;">
        <div class="modal-container">
          <button class="modal-close" data-close-modal aria-label="Close">&times;</button>
          <div class="modal-content">
            <div class="modal-header">
              <div class="modal-brand-row">
                <span class="modal-brand-item">üìö MLSysBook</span>
              </div>
              <h2 class="modal-title">Stay in the Loop</h2>
              <p class="modal-subtitle">Get updates on new chapters, hands-on labs, and ML systems resources.</p>
            </div>
            <form id="subscribe-modal-form" class="subscribe-form" action="https://buttondown.email/api/emails/embed-subscribe/mlsysbook" method="post">
              <div class="form-row">
                <div class="form-group">
                  <label for="modal-first-name">First name</label>
                  <input type="text" id="modal-first-name" name="metadata__first_name" required placeholder="Jane">
                </div>
                <div class="form-group">
                  <label for="modal-last-name">Last name</label>
                  <input type="text" id="modal-last-name" name="metadata__last_name" required placeholder="Smith">
                </div>
              </div>
              <div class="form-group">
                <label for="modal-email">Email</label>
                <input type="email" id="modal-email" name="email" required placeholder="jane@university.edu">
              </div>
              <div class="form-group">
                <label>I am a...</label>
                <div class="role-options role-options-three-compact">
                  <label class="role-option">
                    <input type="radio" name="metadata__role" value="educator" required>
                    <span class="role-label">üë©‚Äçüè´ Educator</span>
                  </label>
                  <label class="role-option">
                    <input type="radio" name="metadata__role" value="student">
                    <span class="role-label">üéì Student</span>
                  </label>
                  <label class="role-option">
                    <input type="radio" name="metadata__role" value="industry">
                    <span class="role-label">üíº Industry</span>
                  </label>
                </div>
              </div>
              <div class="form-group">
                <label for="modal-organization">Organization <span class="optional-label">(optional)</span></label>
                <input type="text" id="modal-organization" name="metadata__organization" placeholder="University or company">
              </div>
              <div class="form-group">
                <label for="modal-motivation">What brings you here? <span class="optional-label">(optional)</span></label>
                <textarea id="modal-motivation" name="metadata__motivation" rows="2" placeholder="e.g., teaching a course, learning ML systems, building edge devices..."></textarea>
              </div>
              <input type="hidden" name="tag" value="mlsysbook-site">
              <button type="submit" class="btn btn-primary subscribe-btn">Subscribe</button>
              <p class="form-note">No spam, ever. Unsubscribe anytime.</p>
            </form>
            <div id="modal-subscribe-success" class="subscribe-success" style="display: none;">
              <div class="success-icon">üéâ</div>
              <h3>You're subscribed!</h3>
              <p>Thanks for signing up. We'll keep you updated on new chapters, labs, and resources.</p>
            </div>
          </div>
        </div>
      </div>
    `;
  }

  // Create modal CSS
  function createModalCSS() {
    const style = document.createElement('style');
    style.textContent = `
      /* Modal Overlay and Container */
      .modal-overlay {
        position: fixed;
        top: 0;
        left: 0;
        right: 0;
        bottom: 0;
        background: rgba(15, 23, 42, 0.6);
        backdrop-filter: blur(4px);
        z-index: 10001;
        align-items: center;
        justify-content: center;
        padding: 1rem;
        animation: fadeIn 0.2s ease;
      }

      @keyframes fadeIn {
        from { opacity: 0; }
        to { opacity: 1; }
      }

      @keyframes slideUp {
        from {
          opacity: 0;
          transform: translateY(20px) scale(0.98);
        }
        to {
          opacity: 1;
          transform: translateY(0) scale(1);
        }
      }

      .modal-container {
        background: white;
        border-radius: 16px;
        max-width: 440px;
        width: 100%;
        max-height: 90vh;
        overflow-y: auto;
        position: relative;
        box-shadow: 0 20px 25px -5px rgb(0 0 0 / 0.1), 0 8px 10px -6px rgb(0 0 0 / 0.1), 0 0 0 1px rgba(0,0,0,0.05);
        animation: slideUp 0.3s ease;
        margin: auto;
      }

      .modal-close {
        position: absolute;
        top: 1rem;
        right: 1rem;
        width: 36px;
        height: 36px;
        border: none;
        background: #f8fafc;
        border-radius: 50%;
        font-size: 1.5rem;
        color: #64748b;
        cursor: pointer;
        display: flex;
        align-items: center;
        justify-content: center;
        transition: all 0.2s ease;
        z-index: 10;
        line-height: 1;
      }

      .modal-close:hover {
        background: white;
        color: #0f172a;
        transform: scale(1.05);
      }

      .modal-content {
        padding: 2rem 2.5rem 2.5rem 2.5rem;
      }

      .modal-header {
        text-align: center;
        margin-bottom: 1.5rem;
        display: flex;
        flex-direction: column;
        align-items: center;
      }

      .modal-brand-row {
        display: inline-flex;
        align-items: center;
        justify-content: center;
        gap: 0.5rem;
        margin-bottom: 1rem;
      }

      .modal-brand-item {
        font-size: 0.8rem;
        font-weight: 600;
        color: #374151;
        background: #f3f4f6;
        padding: 0.3rem 0.6rem;
        border-radius: 5px;
        white-space: nowrap;
      }

      .modal-brand-plus {
        font-size: 0.9rem;
        font-weight: 300;
        color: #9ca3af;
      }

      .modal-title {
        font-size: 1.5rem;
        font-weight: 700;
        color: #0f172a;
        margin: 0 0 0.4rem 0;
        line-height: 1.2;
        width: 100%;
      }

      .modal-subtitle {
        font-size: 0.9rem;
        color: #64748b;
        margin: 0;
        line-height: 1.5;
        max-width: 320px;
      }

      /* Form Styles */
      .subscribe-form {
        display: flex;
        flex-direction: column;
        gap: 1.25rem;
      }

      .form-row {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 0.75rem;
      }

      .form-row .form-group {
        min-width: 0;
      }

      .form-row .form-group input {
        width: 100%;
        box-sizing: border-box;
      }

      .form-group {
        display: flex;
        flex-direction: column;
        gap: 0.5rem;
      }

      .form-group label {
        font-size: 0.9rem;
        font-weight: 600;
        color: #0f172a;
      }

      .optional-label {
        font-weight: 400;
        color: #64748b;
      }

      .form-group input[type="text"],
      .form-group input[type="email"],
      .form-group textarea {
        padding: 0.875rem 1rem;
        border: 1px solid #cbd5e1;
        border-radius: 8px;
        font-size: 1rem;
        transition: all 0.2s ease;
        background: #f8fafc;
        font-family: inherit;
      }

      .form-group textarea {
        resize: vertical;
        min-height: 60px;
      }

      .form-group input[type="text"]:focus,
      .form-group input[type="email"]:focus,
      .form-group textarea:focus {
        outline: none;
        border-color: #3b82f6;
        background: white;
        box-shadow: 0 0 0 3px rgba(59, 130, 246, 0.1);
      }

      .form-group input::placeholder,
      .form-group textarea::placeholder {
        color: #94a3b8;
      }

      /* Role Options - compact style */
      .role-options {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 0.75rem;
      }

      .role-options-three-compact {
        grid-template-columns: repeat(3, 1fr);
      }

      .role-option {
        cursor: pointer;
      }

      .role-option input[type="radio"] {
        position: absolute;
        opacity: 0;
        width: 0;
        height: 0;
      }

      .role-label {
        display: flex;
        align-items: center;
        justify-content: center;
        gap: 0.5rem;
        padding: 0.75rem 1rem;
        border: 2px solid #e2e8f0;
        border-radius: 8px;
        font-size: 0.9rem;
        font-weight: 500;
        color: #475569;
        transition: all 0.2s ease;
        background: #f8fafc;
      }

      .role-options-three-compact .role-label {
        padding: 0.625rem 0.5rem;
        font-size: 0.8rem;
        text-align: center;
      }

      .role-option input[type="radio"]:checked + .role-label {
        border-color: #3b82f6;
        background: rgba(59, 130, 246, 0.08);
        color: #3b82f6;
      }

      .role-option:hover .role-label {
        border-color: #cbd5e1;
        background: white;
      }

      /* Button Styles */
      .btn {
        display: inline-flex;
        align-items: center;
        justify-content: center;
        gap: 0.5rem;
        padding: 0.75rem 1.5rem;
        border-radius: 8px;
        text-decoration: none;
        font-weight: 600;
        font-size: 0.95rem;
        transition: all 0.2s ease;
        border: none;
        cursor: pointer;
        font-family: inherit;
      }

      .btn-primary {
        background: #3b82f6;
        color: white;
      }

      .btn-primary:hover {
        background: #1e3a8a;
        transform: translateY(-1px);
        box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1), 0 2px 4px -2px rgb(0 0 0 / 0.1);
      }

      .subscribe-btn {
        width: 100%;
        padding: 1rem;
        font-size: 1rem;
        margin-top: 0.5rem;
      }

      .form-note {
        text-align: center;
        font-size: 0.85rem;
        color: #64748b;
        margin: 0;
      }

      /* Success Message */
      .subscribe-success {
        text-align: center;
        padding: 2rem 1rem;
      }

      .success-icon {
        font-size: 3rem;
        margin-bottom: 1rem;
      }

      .subscribe-success h3 {
        font-size: 1.5rem;
        font-weight: 600;
        color: #0f172a;
        margin-bottom: 0.5rem;
      }

      .subscribe-success p {
        color: #475569;
        font-size: 1rem;
      }

      /* Dark mode support */
      body.quarto-dark .modal-container {
        background: #1e293b;
      }

      body.quarto-dark .modal-close {
        background: #0f172a;
        color: #94a3b8;
      }

      body.quarto-dark .modal-close:hover {
        background: #334155;
        color: #f1f5f9;
      }

      body.quarto-dark .modal-brand-item {
        background: #334155;
        color: #e2e8f0;
      }

      body.quarto-dark .modal-brand-plus {
        color: #64748b;
      }

      body.quarto-dark .modal-title,
      body.quarto-dark .form-group label,
      body.quarto-dark .subscribe-success h3 {
        color: #f1f5f9;
      }

      body.quarto-dark .modal-subtitle,
      body.quarto-dark .subscribe-success p {
        color: #cbd5e1;
      }

      body.quarto-dark .form-group input[type="text"],
      body.quarto-dark .form-group input[type="email"],
      body.quarto-dark .form-group textarea {
        background: #0f172a;
        border-color: #334155;
        color: #f1f5f9;
      }

      body.quarto-dark .role-label {
        background: #0f172a;
        border-color: #334155;
        color: #cbd5e1;
      }

      body.quarto-dark .role-option input[type="radio"]:checked + .role-label {
        border-color: #3b82f6;
        background: rgba(59, 130, 246, 0.15);
        color: #60a5fa;
      }

      /* Responsive */
      @media (max-width: 640px) {
        .modal-content {
          padding: 2rem 1.5rem;
        }

        .form-row {
          grid-template-columns: 1fr;
        }

        .role-options-three-compact {
          grid-template-columns: repeat(3, 1fr);
        }
      }
    `;
    return style;
  }

  // Initialize modal
  function initModal() {
    // Add CSS
    document.head.appendChild(createModalCSS());

    // Add HTML
    const modalDiv = document.createElement('div');
    modalDiv.innerHTML = createModalHTML();
    document.body.appendChild(modalDiv.firstElementChild);

    const modal = document.getElementById('subscribe-modal');
    const form = document.getElementById('subscribe-modal-form');
    const success = document.getElementById('modal-subscribe-success');

    // Open modal function
    window.openModal = function() {
      modal.style.display = 'flex';
      document.body.style.overflow = 'hidden';

      // Focus first input
      setTimeout(() => {
        const firstInput = document.getElementById('modal-first-name');
        if (firstInput) firstInput.focus();
      }, 100);
    };

    // Close modal function
    window.closeModal = function() {
      modal.style.display = 'none';
      document.body.style.overflow = '';

      // Reset form after closing
      setTimeout(() => {
        form.style.display = 'flex';
        form.reset();
        success.style.display = 'none';
      }, 300);
    };

    // Close on overlay click
    modal.addEventListener('click', (e) => {
      if (e.target === modal) {
        closeModal();
      }
    });

    // Close button click
    const closeBtn = modal.querySelector('[data-close-modal]');
    if (closeBtn) {
      closeBtn.addEventListener('click', closeModal);
    }

    // Close on Escape key
    document.addEventListener('keydown', (e) => {
      if (e.key === 'Escape' && modal.style.display === 'flex') {
        closeModal();
      }
    });

    // Handle form submission
    form.addEventListener('submit', function() {
      // Let the form submit to Buttondown
      setTimeout(() => {
        form.style.display = 'none';
        success.style.display = 'block';

        // Close modal after 5 seconds
        setTimeout(closeModal, 5000);
      }, 100);
    });

    // Check if URL has #subscribe hash on page load - auto-open modal
    if (window.location.hash === '#subscribe') {
      // Small delay to ensure page is fully loaded
      setTimeout(() => {
        openModal();
      }, 300);
    }

    // Also listen for hash changes (e.g., user clicks back/forward)
    window.addEventListener('hashchange', function() {
      if (window.location.hash === '#subscribe') {
        openModal();
      }
    });

    // Intercept navbar subscribe link
    setTimeout(() => {
      // Look for subscribe links in navbar
      const subscribeSelectors = [
        'a[href*="buttondown.email/mlsysbook"]',
        'a[href="#subscribe"]',
        'a[href*="subscribe"]',
        '#navbar-subscribe-btn',
        '.subscribe-link'
      ];

      subscribeSelectors.forEach(selector => {
        try {
          const links = document.querySelectorAll(selector);
          links.forEach(link => {
            link.addEventListener('click', function(e) {
              e.preventDefault();
              openModal();
            });
          });
        } catch (err) {
          // Selector not supported, continue
        }
      });
    }, 1000);
  }

  // Initialize when DOM is ready
  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initModal);
  } else {
    initModal();
  }
})();


--- binder/requirements.txt ---
# TinyTorch Binder Environment (root-level wrapper)
# Binder looks for config at repo root, so we redirect to tinytorch/
#
# This file is used by Binder to set up the execution environment
# Keep synchronized with tinytorch/binder/requirements.txt

# Core numerical computing (TinyTorch dependency)
numpy>=1.24.0,<3.0.0

# Terminal UI (for tito CLI and development feedback)
rich>=13.0.0

# Configuration files (for tito CLI)
PyYAML>=6.0

# Jupyter environment
jupyter>=1.1.0
jupyterlab>=4.2.0
ipykernel>=6.29.0
ipywidgets>=8.0.0

# Visualization (for milestone examples and modules)
matplotlib>=3.9.0

# Type checking support
typing-extensions>=4.12.0

# Jupytext (required to convert src/*.py to notebooks)
jupytext>=1.16.0

# Note: tinytorch package itself is installed via postBuild script
# This ensures the latest code from the repository is used


--- requirements.txt ---
# MLSysBook Dependencies
# This is a convenience file that includes all dependencies
#
# To install all dependencies:
#   pip install -r requirements.txt
#
# For development work including Book Binder CLI:
#   pip install -r requirements.txt
#
# Main dependencies are maintained in book/tools/dependencies/requirements.txt
# (MLSysBook content is now under book/ to accommodate TinyTorch at root)

-r book/tools/dependencies/requirements.txt


--- .github/workflows/contributors/README.md ---
# Contributor Management Scripts

This folder contains scripts for managing contributor recognition across the repository.

## Overview

The contributor system tracks contributions to four projects:
- **book/** - ML Systems textbook
- **tinytorch/** - Educational ML framework
- **kits/** - Hardware kits
- **labs/** - Lab exercises

Each project has its own `.all-contributorsrc` file, and the main `README.md` displays all contributors in organized sections.

## Scripts

### `update_contributors.py`

Updates the root `.all-contributorsrc` from GitHub API.

```bash
# Requires GITHUB_TOKEN environment variable
python update_contributors.py
```

**What it does:**
- Queries GitHub API for all repository contributors
- Resolves git emails to GitHub usernames
- Generates gravatar URLs for non-GitHub contributors
- Merges new contributors with existing entries

### `generate_main_readme.py`

Generates the sectioned contributor table in the main `README.md`.

```bash
python generate_main_readme.py [--dry-run]
```

**What it does:**
- Reads all four project `.all-contributorsrc` files
- Generates HTML tables with contributor avatars and badges
- Updates the Contributors section in `README.md`
- Creates sections: Book, TinyTorch, Kits, Labs

### `generate_readme_tables.py`

Updates per-project README files with contributor tables.

```bash
python generate_readme_tables.py [--project PROJECT] [--update]
```

**Options:**
- `--project`: Process only one project (book, tinytorch, kits, labs)
- `--update`: Actually update the README files (without this, just prints)

**What it does:**
- Reads each project's `.all-contributorsrc`
- Generates HTML contributor tables
- Updates the `<!-- ALL-CONTRIBUTORS-LIST -->` section in each project's README

### `scan_contributors.py`

Scans git history to discover contributors (manual/one-time use).

```bash
python scan_contributors.py [--project PROJECT] [--output FORMAT] [--update]
```

**Options:**
- `--project`: Scan only one project
- `--output`: Output format (table, json, rc)
- `--update`: Update `.all-contributorsrc` files directly
- `--dry-run`: Preview changes without writing

**What it does:**
- Analyzes git commit history per project folder
- Categorizes contributions (code, doc, bug, etc.) from commit messages
- Maps git emails to GitHub usernames
- Filters out bots and AI tools

## Workflow Integration

There are two workflows that manage contributors:

### 1. `all-contributors-add.yml` - Comment-Triggered (Recommended)

Automatically adds contributors when you comment on any issue or PR:

```
@all-contributors please add @username for bug, code, doc
```

**How it works:**
1. Parses the comment to extract username and contribution types
2. Detects which project (book, tinytorch, kits, labs) from labels/title
3. Updates the project's `.all-contributorsrc` file
4. Regenerates README tables
5. Commits and pushes directly (no PR needed!)
6. Replies to confirm the addition

**Project Detection:**
- Add `tinytorch` label OR mention "tinytorch" in issue title ‚Üí tinytorch project
- Add `kits` label OR mention "kits" in issue title ‚Üí kits project
- Add `labs` label OR mention "labs" in issue title ‚Üí labs project
- Otherwise ‚Üí book project (default)

### 2. `update-contributors.yml` - Push-Triggered

Runs when `.all-contributorsrc` files are manually edited and pushed:

```
Trigger: Push to dev/main with .all-contributorsrc changes
         OR manual dispatch

Steps:
1. update_contributors.py    ‚Üí Update root config from GitHub API
2. generate_main_readme.py   ‚Üí Rebuild main README sections
3. generate_readme_tables.py ‚Üí Update per-project READMEs
4. Commit and push changes
```

## Adding Contributors

### Method 1: Comment Command (Recommended)

Comment on any issue or PR:
```
@all-contributors please add @username for bug, code, doc
```

The workflow will automatically:
- Look up the user's GitHub profile
- Add them to the correct project's contributor list
- Update all README files
- Reply with confirmation

### Method 2: Manual Edit

1. Edit the appropriate `.all-contributorsrc` file
2. Add entry with: login, name, avatar_url, contributions
3. Push to dev/main to trigger the update workflow

## Contribution Types

We use the standard [All Contributors emoji key](https://allcontributors.org/docs/en/emoji-key).

Common types: `bug`, `code`, `doc`, `design`, `ideas`, `review`, `test`, `tool`, `tutorial`, `maintenance`, `infra`, `research`

## File Structure

```
.github/workflows/
‚îú‚îÄ‚îÄ all-contributors-add.yml     # Comment-triggered workflow (main)
‚îú‚îÄ‚îÄ update-contributors.yml      # Push-triggered workflow
‚îî‚îÄ‚îÄ contributors/
    ‚îú‚îÄ‚îÄ README.md                 # This file
    ‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
    ‚îú‚îÄ‚îÄ update_contributors.py    # GitHub API updater
    ‚îú‚îÄ‚îÄ generate_main_readme.py   # Main README generator
    ‚îú‚îÄ‚îÄ generate_readme_tables.py # Per-project README generator
    ‚îî‚îÄ‚îÄ scan_contributors.py      # Git history scanner

Project configs:
‚îú‚îÄ‚îÄ .all-contributorsrc           # Root config (legacy)
‚îú‚îÄ‚îÄ book/.all-contributorsrc      # Book contributors
‚îú‚îÄ‚îÄ tinytorch/.all-contributorsrc # TinyTorch contributors
‚îú‚îÄ‚îÄ kits/.all-contributorsrc      # Kits contributors
‚îî‚îÄ‚îÄ labs/.all-contributorsrc      # Labs contributors
```


## Links discovered
- [All Contributors emoji key](https://allcontributors.org/docs/en/emoji-key)

--- .github/dev-landing/index.html ---
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MLSysBook Development Preview</title>
    <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>üìö</text></svg>">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%);
            min-height: 100vh;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            color: #e4e4e4;
            padding: 2rem;
        }

        .container {
            max-width: 800px;
            text-align: center;
        }

        .badge {
            display: inline-block;
            background: #e94560;
            color: white;
            padding: 0.5rem 1rem;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 600;
            margin-bottom: 1.5rem;
            text-transform: uppercase;
            letter-spacing: 1px;
        }

        h1 {
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
            background: linear-gradient(90deg, #fff, #e4e4e4);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .subtitle {
            color: #a4a4a4;
            margin-bottom: 3rem;
            font-size: 1.1rem;
        }

        .cards {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 1.5rem;
            margin-bottom: 3rem;
        }

        .card {
            background: rgba(255, 255, 255, 0.05);
            border: 1px solid rgba(255, 255, 255, 0.1);
            border-radius: 16px;
            padding: 2rem;
            text-decoration: none;
            color: inherit;
            transition: all 0.3s ease;
        }

        .card:hover {
            transform: translateY(-5px);
            background: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.2);
        }

        .card-icon {
            font-size: 3rem;
            margin-bottom: 1rem;
        }

        .card h2 {
            font-size: 1.5rem;
            margin-bottom: 0.5rem;
        }

        .card p {
            color: #a4a4a4;
            font-size: 0.95rem;
            margin-bottom: 1rem;
        }

        .card-link {
            color: #e94560;
            font-size: 0.9rem;
            font-weight: 500;
        }

        .card-link.coming-soon {
            color: #888;
        }

        .coming-badge {
            display: inline-block;
            background: rgba(255, 255, 255, 0.15);
            color: #a4a4a4;
            padding: 0.2rem 0.5rem;
            border-radius: 10px;
            font-size: 0.7rem;
            font-weight: 500;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            margin-left: 0.5rem;
            vertical-align: middle;
        }

        .footer {
            color: #666;
            font-size: 0.85rem;
        }

        .footer a {
            color: #888;
            text-decoration: none;
        }

        .footer a:hover {
            color: #e94560;
        }

        .commit-info {
            margin-top: 0.5rem;
            font-family: monospace;
            font-size: 0.8rem;
            color: #555;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="badge">üöß Development Preview</div>

        <h1>MLSysBook</h1>
        <p class="subtitle">Machine Learning Systems ‚Ä¢ Development Environment</p>

        <div class="cards">
            <a href="./book/" class="card">
                <div class="card-icon">üìñ</div>
                <h2>Textbook</h2>
                <p>Machine Learning Systems: Principles and Practices of Engineering Artificially Intelligent Systems</p>
                <span class="card-link">Open Book Preview ‚Üí</span>
            </a>

            <a href="./tinytorch/" class="card">
                <div class="card-icon">üî•</div>
                <h2>Tinyüî•Torch</h2>
                <p>Build ML Systems from Scratch: An interactive course for implementing your own PyTorch-style framework</p>
                <span class="card-link">Open TinyTorch Preview ‚Üí</span>
            </a>

            <a href="./kits/" class="card">
                <div class="card-icon">üì¶</div>
                <h2>Hardware Kits</h2>
                <p>Hands-on hardware labs for deploying ML on edge devices: Arduino, Raspberry Pi, and more</p>
                <span class="card-link">Open Kits Preview ‚Üí</span>
            </a>

            <a href="./labs/" class="card">
                <div class="card-icon">üîÆ</div>
                <h2>Labs <span class="coming-badge">Coming 2026</span></h2>
                <p>Interactive experiments to see ML systems in action: quantization, memory profiling, and deployment tradeoffs</p>
                <span class="card-link coming-soon">View Landing Page ‚Üí</span>
            </a>
        </div>

        <div class="footer">
            <p>
                <a href="https://github.com/harvard-edge/cs249r_book">GitHub</a> ‚Ä¢
                <a href="https://mlsysbook.ai">Live Site</a> ‚Ä¢
                <a href="https://github.com/harvard-edge/cs249r_book/tree/dev">dev branch</a>
            </p>
            <p class="commit-info">
                This is a development preview. For the stable version, visit <a href="https://mlsysbook.ai">mlsysbook.ai</a>
            </p>
        </div>
    </div>
</body>
</html>


## Links discovered
- [<div class="card-icon">üìñ</div> <h2>Textbook</h2> <p>Machine Learning Systems: Principles and Practices of Engineering Artificially Intelligent Systems</p> <span class="card-link">Open Book Preview ‚Üí</span>](https://github.com/harvard-edge/cs249r_book/blob/dev/.github/dev-landing/book.md)
- [<div class="card-icon">üî•</div> <h2>Tinyüî•Torch</h2> <p>Build ML Systems from Scratch: An interactive course for implementing your own PyTorch-style framework</p> <span class="card-link">Open TinyTorch Preview ‚Üí</span>](https://github.com/harvard-edge/cs249r_book/blob/dev/.github/dev-landing/tinytorch.md)
- [<div class="card-icon">üì¶</div> <h2>Hardware Kits</h2> <p>Hands-on hardware labs for deploying ML on edge devices: Arduino, Raspberry Pi, and more</p> <span class="card-link">Open Kits Preview ‚Üí</span>](https://github.com/harvard-edge/cs249r_book/blob/dev/.github/dev-landing/kits.md)
- [<div class="card-icon">üîÆ</div> <h2>Labs <span class="coming-badge">Coming 2026</span></h2> <p>Interactive experiments to see ML systems in action: quantization, memory profiling, and deployment tradeoffs</p> <span class="card-link coming-soon">View Landing Page ‚Üí</span>](https://github.com/harvard-edge/cs249r_book/blob/dev/.github/dev-landing/labs.md)
- [GitHub](https://github.com/harvard-edge/cs249r_book)
- [Live Site](https://mlsysbook.ai)
- [dev branch](https://github.com/harvard-edge/cs249r_book/tree/dev)
- [mlsysbook.ai](https://mlsysbook.ai)

--- .github/workflows/contributors/generate_main_readme.py ---
#!/usr/bin/env python3
"""
Generate the main README.md contributor section from all project configs.

This script reads the .all-contributorsrc files from each project
(book, tinytorch, kits, labs) and generates a sectioned contributor
table for the main README.md.

Usage:
    python generate_main_readme.py [--dry-run]
"""

import json
import re
import sys
from pathlib import Path

# Emoji mapping for contribution types (only types actually in use)
# Synced with generate_readme_tables.py
CONTRIBUTION_EMOJIS = {
    "bug": "ü™≤",             # Bug Hunter
    "code": "üßë‚Äçüíª",            # Code Contributor
    "design": "üé®",          # Design Artist
    "doc": "‚úçÔ∏è",             # Doc Wizard
    "ideas": "üß†",           # Idea Spark
    "review": "üîé",          # Code Reviewer
    "test": "üß™",            # Test Tinkerer
    "tool": "üõ†Ô∏è",           # Tool Builder
}

# Legend for contribution types (shown in README)
# Only includes types currently in use across all projects
CONTRIBUTION_LEGEND = {
    "bug": ("ü™≤", "Bug Hunter"),
    "code": ("üßë‚Äçüíª", "Code Contributor"),
    "doc": ("‚úçÔ∏è", "Doc Wizard"),
    "design": ("üé®", "Design Artist"),
    "ideas": ("üß†", "Idea Spark"),
    "review": ("üîé", "Code Reviewer"),
    "test": ("üß™", "Test Tinkerer"),
    "tool": ("üõ†Ô∏è", "Tool Builder"),
}


def load_config(path: Path) -> dict:
    """Load a .all-contributorsrc file."""
    if not path.exists():
        return {"contributors": []}
    with open(path) as f:
        return json.load(f)


def generate_contributor_cell(contributor: dict, show_badges: bool = True, image_size: int = 50, width_pct: str = "11.11%") -> str:
    """Generate HTML for a single contributor cell."""
    login = contributor.get("login", "")
    name = contributor.get("name", login)
    avatar_url = contributor.get("avatar_url", "")
    profile = contributor.get("profile", f"https://github.com/{login}")
    contributions = contributor.get("contributions", [])

    # Generate badge string
    badges = ""
    if show_badges and contributions:
        badges = " ".join(CONTRIBUTION_EMOJIS.get(c, "") for c in contributions)
        badges = f"<br />{badges}" if badges.strip() else ""

    return f'''      <td align="center" valign="top" width="{width_pct}"><a href="{profile}"><img src="{avatar_url}?v=4?s={image_size}" width="{image_size}px;" alt="{name}"/><br /><sub><b>{name}</b></sub></a>{badges}</td>'''


def generate_contributor_table(contributors: list, show_badges: bool = True, cols: int = 9, image_size: int = 50) -> str:
    """Generate an HTML table for contributors.

    Args:
        contributors: List of contributor dicts
        show_badges: Whether to show contribution badges
        cols: Number of columns per row (default 9 for compact display)
        image_size: Size of avatar images in pixels (default 50 for compact display)
    """
    if not contributors:
        return "<p><em>Coming soon!</em></p>"

    # Sort by contribution count (most contributions first)
    sorted_contributors = sorted(
        contributors,
        key=lambda c: len(c.get("contributions", [])),
        reverse=True
    )

    # Calculate width percentage based on columns
    width_pct = f"{100/cols:.2f}%"

    rows = []
    row_cells = []

    for i, contributor in enumerate(sorted_contributors):
        row_cells.append(generate_contributor_cell(contributor, show_badges, image_size, width_pct))

        # Dynamic columns per row
        if len(row_cells) == cols:
            rows.append("    <tr>\n" + "\n".join(row_cells) + "\n    </tr>")
            row_cells = []

    # Add remaining cells
    if row_cells:
        rows.append("    <tr>\n" + "\n".join(row_cells) + "\n    </tr>")

    return f'''<table>
  <tbody>
{chr(10).join(rows)}
  </tbody>
</table>'''


def generate_legend() -> str:
    """Generate a compact legend for contribution types."""
    items = [f"{emoji} {title}" for emoji, title in CONTRIBUTION_LEGEND.values()]
    return " ¬∑ ".join(items)


def generate_sectioned_contributors(repo_root: Path) -> str:
    """Generate the full sectioned contributor section showing ALL contributors."""
    # Load all configs
    book_config = load_config(repo_root / "book" / ".all-contributorsrc")
    tinytorch_config = load_config(repo_root / "tinytorch" / ".all-contributorsrc")
    kits_config = load_config(repo_root / "kits" / ".all-contributorsrc")
    labs_config = load_config(repo_root / "labs" / ".all-contributorsrc")

    book_contributors = book_config.get("contributors", [])
    tinytorch_contributors = tinytorch_config.get("contributors", [])
    kits_contributors = kits_config.get("contributors", [])
    labs_contributors = labs_config.get("contributors", [])

    # Count contributors
    book_count = len(book_contributors)
    tinytorch_count = len(tinytorch_contributors)
    kits_count = len(kits_contributors)
    labs_count = len(labs_contributors)

    # Generate tables - show ALL contributors
    book_table = generate_contributor_table(book_contributors)
    tinytorch_table = generate_contributor_table(tinytorch_contributors)
    kits_table = generate_contributor_table(kits_contributors)
    labs_table = generate_contributor_table(labs_contributors)

    # Generate legend
    legend = generate_legend()

    return f'''## Contributors

Thanks goes to these wonderful people who have contributed to making this resource better for everyone!

**Legend:** {legend}

### üìñ Textbook Contributors

<!-- BOOK-CONTRIBUTORS-START -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
{book_table}

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->
<!-- BOOK-CONTRIBUTORS-END -->

---

### üî• TinyTorch Contributors

<!-- TINYTORCH-CONTRIBUTORS-START -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
{tinytorch_table}

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->
<!-- TINYTORCH-CONTRIBUTORS-END -->

---

### üõ†Ô∏è Hardware Kits Contributors

<!-- KITS-CONTRIBUTORS-START -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
{kits_table}

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->
<!-- KITS-CONTRIBUTORS-END -->

---

### üß™ Labs Contributors

<!-- LABS-CONTRIBUTORS-START -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
{labs_table}

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->
<!-- LABS-CONTRIBUTORS-END -->'''


def update_readme(repo_root: Path, dry_run: bool = False) -> bool:
    """Update the main README.md with sectioned contributors."""
    readme_path = repo_root / "README.md"

    if not readme_path.exists():
        print(f"ERROR: README.md not found at {readme_path}")
        return False

    content = readme_path.read_text()

    # Generate new contributor section
    new_section = generate_sectioned_contributors(repo_root)

    # Pattern to match the entire Contributors section
    # From "## Contributors" to just before the next "---" followed by a div or end of file
    pattern = r'## Contributors\n.*?(?=\n---\n\n<div align="center">|\Z)'

    if not re.search(pattern, content, re.DOTALL):
        print("ERROR: Could not find Contributors section in README.md")
        return False

    # Replace the section
    new_content = re.sub(pattern, new_section, content, flags=re.DOTALL)

    if dry_run:
        print("=== DRY RUN - Would update README.md with: ===")
        print(new_section[:2000] + "..." if len(new_section) > 2000 else new_section)
        return True

    readme_path.write_text(new_content)
    print(f"Updated {readme_path}")
    return True


def main():
    dry_run = "--dry-run" in sys.argv

    # Find repo root (this script is in .github/workflows/contributors/)
    script_dir = Path(__file__).parent
    repo_root = script_dir.parent.parent.parent

    # Verify we're in the right place
    if not (repo_root / "README.md").exists():
        print(f"ERROR: Cannot find README.md in {repo_root}")
        sys.exit(1)

    success = update_readme(repo_root, dry_run)
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()


## Links discovered
- [<img src="{avatar_url}?v=4?s={image_size}" width="{image_size}px;" alt="{name}"/><br /><sub><b>{name}</b></sub>](https://github.com/harvard-edge/cs249r_book/blob/dev/.github/workflows/contributors/{profile}.md)

--- .github/workflows/contributors/generate_readme_tables.py ---
#!/usr/bin/env python3
"""
Generate All Contributors tables for README files.

This script reads .all-contributorsrc files and generates the HTML tables
that go in the README.md files.

Usage:
    python generate_readme_tables.py [--project PROJECT] [--update]
"""

import json
import re
import argparse
from pathlib import Path

PROJECTS = {
    "book": "book/",
    "kits": "kits/",
    "labs": "labs/",
    "tinytorch": "tinytorch/",
}

# Emoji mapping for contribution types (only types actually in use)
# Synced with generate_main_readme.py
EMOJI_KEY = {
    "bug": "ü™≤",             # Bug Hunter
    "code": "üßë‚Äçüíª",            # Code Contributor
    "design": "üé®",          # Design Artist
    "doc": "‚úçÔ∏è",             # Doc Wizard
    "ideas": "üß†",           # Idea Spark
    "review": "üîé",          # Code Reviewer
    "test": "üß™",            # Test Tinkerer
    "tool": "üõ†Ô∏è",           # Tool Builder
}


def generate_contributor_cell(contributor: dict, image_size: int = 80) -> str:
    """Generate HTML for a single contributor cell."""
    login = contributor['login']
    name = contributor.get('name', login)
    avatar_url = contributor.get('avatar_url', f"https://avatars.githubusercontent.com/{login}")
    profile = contributor.get('profile', f"https://github.com/{login}")
    contributions = contributor.get('contributions', [])
    
    # Generate emoji badges
    badges = " ".join(EMOJI_KEY.get(c, c) for c in contributions)
    
    return f'''<td align="center" valign="top" width="14.28%"><a href="{profile}"><img src="{avatar_url}?v=4?s={image_size}" width="{image_size}px;" alt="{name}"/><br /><sub><b>{name}</b></sub></a><br />{badges}</td>'''


def generate_table(contributors: list[dict], per_line: int = 7, image_size: int = 80) -> str:
    """Generate the full HTML table for contributors."""
    if not contributors:
        return ""
    
    lines = ["<table>", "  <tbody>"]
    
    for i in range(0, len(contributors), per_line):
        row = contributors[i:i + per_line]
        lines.append("    <tr>")
        for contributor in row:
            lines.append("      " + generate_contributor_cell(contributor, image_size))
        lines.append("    </tr>")
    
    lines.append("  </tbody>")
    lines.append("</table>")
    
    return "\n".join(lines)


def update_readme(project_path: str, table_html: str) -> bool:
    """Update the README.md with the new contributors table."""
    readme_path = Path(project_path) / "README.md"
    
    if not readme_path.exists():
        print(f"Warning: {readme_path} does not exist")
        return False
    
    with open(readme_path, 'r') as f:
        content = f.read()
    
    # Pattern to match the ALL-CONTRIBUTORS-LIST section
    pattern = r'(<!-- ALL-CONTRIBUTORS-LIST:START.*?-->).*?(<!-- ALL-CONTRIBUTORS-LIST:END -->)'
    
    if not re.search(pattern, content, re.DOTALL):
        print(f"Warning: No ALL-CONTRIBUTORS-LIST markers found in {readme_path}")
        return False
    
    # Build replacement content
    replacement = f'''\\1
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
{table_html}

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->

\\2'''
    
    new_content = re.sub(pattern, replacement, content, flags=re.DOTALL)
    
    with open(readme_path, 'w') as f:
        f.write(new_content)
    
    return True


def process_project(project_name: str, project_path: str, update: bool = False) -> None:
    """Process a single project."""
    rc_path = Path(project_path) / ".all-contributorsrc"
    
    if not rc_path.exists():
        print(f"Skipping {project_name}: no .all-contributorsrc found")
        return
    
    with open(rc_path, 'r') as f:
        rc_data = json.load(f)
    
    contributors = rc_data.get('contributors', [])
    per_line = rc_data.get('contributorsPerLine', 7)
    image_size = rc_data.get('imageSize', 80)

    if not contributors:
        print(f"{project_name}: No contributors to display")
        return

    # Sort contributors by number of contributions (descending)
    sorted_contributors = sorted(
        contributors,
        key=lambda c: len(c.get('contributions', [])),
        reverse=True
    )

    table_html = generate_table(sorted_contributors, per_line, image_size)
    
    print(f"\n=== {project_name} ({len(contributors)} contributors) ===")
    
    if update:
        if update_readme(project_path, table_html):
            print(f"Updated {project_path}README.md")
        else:
            print(f"Failed to update {project_path}README.md")
    else:
        print(table_html)


def main():
    parser = argparse.ArgumentParser(description="Generate All Contributors tables")
    parser.add_argument("--project", choices=list(PROJECTS.keys()), help="Process specific project")
    parser.add_argument("--update", action="store_true", help="Update README files")
    args = parser.parse_args()

    # Find repo root (this script is in .github/workflows/contributors/)
    script_dir = Path(__file__).parent
    repo_root = script_dir.parent.parent.parent

    if args.project:
        projects = {args.project: PROJECTS[args.project]}
    else:
        projects = PROJECTS

    for name, rel_path in projects.items():
        process_project(name, str(repo_root / rel_path), args.update)


if __name__ == "__main__":
    main()


## Links discovered
- [<img src="{avatar_url}?v=4?s={image_size}" width="{image_size}px;" alt="{name}"/><br /><sub><b>{name}</b></sub>](https://github.com/harvard-edge/cs249r_book/blob/dev/.github/workflows/contributors/{profile}.md)

--- .github/workflows/contributors/requirements.txt ---
absl-py
pandas
PyGithub
requests>=2.31.0


--- .github/workflows/contributors/scan_contributors.py ---
#!/usr/bin/env python3
"""
Scan git history to identify contributors per project.

This script analyzes git commits to:
1. Find unique contributors per project folder
2. Categorize contribution types based on commit messages and files changed
3. Map git emails to GitHub usernames where possible
4. Filter out bots and AI tools
5. Output data for .all-contributorsrc files

Usage:
    python scan_contributors.py [--project PROJECT] [--output json|table|rc]
    
Examples:
    python scan_contributors.py                      # Scan all projects
    python scan_contributors.py --project tinytorch  # Scan only tinytorch
    python scan_contributors.py --output json        # Output as JSON
"""

import subprocess
import json
import re
import argparse
from collections import defaultdict
from pathlib import Path

# Project folders to scan
PROJECTS = {
    "book": "book/",
    "kits": "kits/",
    "labs": "labs/",
    "tinytorch": "tinytorch/",
}

# Patterns to exclude (bots, AI tools, etc.)
EXCLUDE_PATTERNS = [
    r"bot",
    r"github-actions",
    r"dependabot",
    r"claude",
    r"cursor",
    r"copilot",
    r"\[bot\]",
    r"noreply\.github\.com.*bot",
]

# Contribution type detection based on commit message keywords
CONTRIBUTION_PATTERNS = {
    "bug": [
        r"\bfix(es|ed|ing)?\b",
        r"\bbug\b",
        r"\bissue\b",
        r"\berror\b",
        r"\bpatch\b",
        r"\bresolve[sd]?\b",
    ],
    "doc": [
        r"\bdoc(s|umentation)?\b",
        r"\breadme\b",
        r"\bcomment\b",
        r"\btypo\b",
        r"\bspelling\b",
        r"\bgrammar\b",
        r"\bexplain\b",
        r"\bdescription\b",
    ],
    "test": [
        r"\btest(s|ing)?\b",
        r"\bspec\b",
        r"\bcoverage\b",
        r"\bvalidat(e|ion)\b",
    ],
    "code": [
        r"\bfeat(ure)?\b",
        r"\badd(s|ed|ing)?\b",
        r"\bimplement(s|ed|ing|ation)?\b",
        r"\bcreate[sd]?\b",
        r"\bbuild\b",
        r"\brefactor\b",
        r"\bupdate[sd]?\b",
        r"\benhance\b",
        r"\bimprove[sd]?\b",
    ],
    "review": [
        r"\breview(ed|ing)?\b",
        r"\bfeedback\b",
        r"\bsuggestion\b",
    ],
    "design": [
        r"\bdesign\b",
        r"\bdiagram\b",
        r"\bfigure\b",
        r"\bimage\b",
        r"\billustrat(e|ion)\b",
        r"\bvisual\b",
    ],
    "translation": [
        r"\btranslat(e|ion|ed)\b",
        r"\blocali[sz](e|ation)\b",
        r"\bi18n\b",
    ],
    "tool": [
        r"\btool(s|ing)?\b",
        r"\bscript\b",
        r"\bautomation\b",
        r"\bworkflow\b",
        r"\bci\b",
        r"\bcd\b",
    ],
    "ideas": [
        r"\bidea\b",
        r"\bpropos(e|al)\b",
        r"\bsuggest\b",
        r"\brfc\b",
    ],
}

# Known email to GitHub username mappings (extend as needed)
EMAIL_TO_GITHUB = {
    # Core team
    "vj@eecs.harvard.edu": "profvjreddi",
    "zeljko.hrcek@gmail.com": "hzeljko",
    "mjrovai@gmail.com": "Mjrovai",
    "jjj4se@virginia.edu": "jasonjabbour",
    "iuchendu@g.harvard.edu": "uchendui",
    
    # Contributors
    "kkleinbard@avaya.com": "kai4avaya",
    "kai4avaya@gmail.com": "kai4avaya",
    "khoshnevis.naeem@gmail.com": "Naeemkh",
    "matthew_stewart@g.harvard.edu": "mrdragonbear",
    "jeffreyma@g.harvard.edu": "18jeffreyma",
    "douwedb@gmail.com": "V0XNIHILI",
    "shanzeh.batool@gmail.com": "shanzehbatool",
    "jaredping@yahoo.com": "JaredP94",
    "sara.khosravi.ds@gmail.com": "Sara-Khosravi",
    "i.j.shapira@gmail.com": "ishapira1",
    "durand.didier@gmail.com": "didier-durand",
    "gabriel.amazonas.eng@gmail.com": "GabrielAmazonas",
    "cbanbury@g.harvard.edu": "colbybanbury",
    "zishenwan@g.harvard.edu": "zishenwan",
    "mark@markmaz.com": "mmaz",
    "lazio2013@gmail.com": "ma3mool",
    "divya.amirtharaj@gmail.com": "DivyaAmirtharaj",
    "91.srivatsan@gmail.com": "srivatsankrishnan",
    "alexbrodriguez@gmail.com": "alxrod",
    "jlin3@college.harvard.edu": "jaysonzlin",
    "jwnchung@umich.edu": "jaywonchung",
    "jettythek@gmail.com": "jettythek",
    "anfe4949anfe@gmail.com": "andreamurillomtz",
    "oishibanerjee@gmail.com": "oishib",
    "yushun_hsiao@g.harvard.edu": "leo47007",
    "michael.schnebly@gmail.com": "MichaelSchnebly",
    "duongvanthuong9a8@gmail.com": "VThuong99",
    "eliasab@college.harvard.edu": "eliasab16",
}


def is_excluded(name: str, email: str) -> bool:
    """Check if contributor should be excluded (bot, AI, etc.)."""
    combined = f"{name} {email}".lower()
    for pattern in EXCLUDE_PATTERNS:
        if re.search(pattern, combined, re.IGNORECASE):
            return True
    return False


def detect_contribution_types(commit_message: str, files_changed: list[str]) -> set[str]:
    """Detect contribution types from commit message and files changed."""
    types = set()
    message_lower = commit_message.lower()
    
    # Check commit message patterns
    for contrib_type, patterns in CONTRIBUTION_PATTERNS.items():
        for pattern in patterns:
            if re.search(pattern, message_lower, re.IGNORECASE):
                types.add(contrib_type)
                break
    
    # Check file extensions for additional hints
    for file in files_changed:
        file_lower = file.lower()
        if file_lower.endswith(('.md', '.rst', '.txt')):
            types.add("doc")
        elif file_lower.endswith(('test_', '_test.py', 'test.py', '.spec.')):
            types.add("test")
        elif 'test' in file_lower:
            types.add("test")
        elif file_lower.endswith(('.png', '.jpg', '.svg', '.gif')):
            types.add("design")
        elif file_lower.endswith(('.py', '.js', '.ts', '.c', '.cpp', '.h')):
            types.add("code")
    
    # Default to "code" if nothing detected
    if not types:
        types.add("code")
    
    return types


def get_github_username(name: str, email: str) -> str | None:
    """Try to get GitHub username from email or name."""
    email_lower = email.lower()
    
    # Check known mappings
    if email_lower in EMAIL_TO_GITHUB:
        return EMAIL_TO_GITHUB[email_lower]
    
    # Try to extract from noreply email
    # Format: 12345+username@users.noreply.github.com
    noreply_match = re.match(r'(\d+\+)?([^@]+)@users\.noreply\.github\.com', email_lower)
    if noreply_match:
        return noreply_match.group(2)
    
    return None


def get_commits_for_project(project_path: str) -> list[dict]:
    """Get all commits for a project folder."""
    # Format: hash|author_name|author_email|subject
    cmd = [
        "git", "log",
        "--format=%H|%an|%ae|%s",
        "--name-only",
        "--", project_path
    ]
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
    except subprocess.CalledProcessError:
        return []
    
    commits = []
    current_commit = None
    
    for line in result.stdout.split('\n'):
        if '|' in line and line.count('|') >= 3:
            # This is a commit line
            if current_commit:
                commits.append(current_commit)
            
            parts = line.split('|', 3)
            current_commit = {
                'hash': parts[0],
                'name': parts[1],
                'email': parts[2],
                'message': parts[3] if len(parts) > 3 else '',
                'files': []
            }
        elif line.strip() and current_commit:
            # This is a file line
            current_commit['files'].append(line.strip())
    
    if current_commit:
        commits.append(current_commit)
    
    return commits


def analyze_project(project_name: str, project_path: str) -> dict:
    """Analyze a project and return contributor data."""
    commits = get_commits_for_project(project_path)
    
    # Aggregate by contributor
    contributors = defaultdict(lambda: {
        'name': '',
        'email': '',
        'github': None,
        'commits': 0,
        'types': set(),
    })
    
    for commit in commits:
        name = commit['name']
        email = commit['email']
        
        # Skip excluded contributors
        if is_excluded(name, email):
            continue
        
        # Use email as key for deduplication
        key = email.lower()
        
        contributors[key]['name'] = name
        contributors[key]['email'] = email
        contributors[key]['github'] = get_github_username(name, email)
        contributors[key]['commits'] += 1
        
        # Detect contribution types
        types = detect_contribution_types(commit['message'], commit['files'])
        contributors[key]['types'].update(types)
    
    # Convert to list and sort by commits
    result = []
    for email, data in contributors.items():
        result.append({
            'name': data['name'],
            'email': data['email'],
            'github': data['github'],
            'commits': data['commits'],
            'types': sorted(list(data['types'])),
        })
    
    result.sort(key=lambda x: x['commits'], reverse=True)
    return {
        'project': project_name,
        'path': project_path,
        'contributors': result,
        'total_contributors': len(result),
    }


def format_as_table(data: dict) -> str:
    """Format project data as a markdown table."""
    lines = [
        f"\n## {data['project']} ({data['total_contributors']} contributors)\n",
        "| Name | GitHub | Commits | Types |",
        "|------|--------|---------|-------|",
    ]
    
    for c in data['contributors']:
        github = f"@{c['github']}" if c['github'] else "?"
        types = ", ".join(c['types'])
        lines.append(f"| {c['name']} | {github} | {c['commits']} | {types} |")
    
    return "\n".join(lines)


def format_as_allcontributorsrc(data: dict) -> dict:
    """Format project data as .all-contributorsrc contributor entries."""
    # Dedupe by GitHub username
    seen_github = {}
    
    for c in data['contributors']:
        if not c['github']:
            continue  # Skip if no GitHub username
        
        github_lower = c['github'].lower()
        
        if github_lower in seen_github:
            # Merge contribution types
            seen_github[github_lower]['contributions'].update(c['types'])
            # Keep higher commit count name
            if c['commits'] > seen_github[github_lower].get('_commits', 0):
                seen_github[github_lower]['name'] = c['name']
                seen_github[github_lower]['_commits'] = c['commits']
        else:
            seen_github[github_lower] = {
                "login": c['github'],
                "name": c['name'],
                "avatar_url": f"https://avatars.githubusercontent.com/{c['github']}",
                "profile": f"https://github.com/{c['github']}",
                "contributions": set(c['types']),
                "_commits": c['commits']
            }
    
    # Convert sets to sorted lists and remove internal fields
    contributors = []
    for entry in seen_github.values():
        contributors.append({
            "login": entry['login'],
            "name": entry['name'],
            "avatar_url": entry['avatar_url'],
            "profile": entry['profile'],
            "contributions": sorted(list(entry['contributions']))
        })
    
    # Sort by number of contribution types (most active first)
    contributors.sort(key=lambda x: len(x['contributions']), reverse=True)
    
    return {
        "project": data['project'],
        "contributors": contributors
    }


def update_allcontributorsrc(project_name: str, contributors: list[dict], dry_run: bool = True) -> bool:
    """Update the .all-contributorsrc file for a project."""
    rc_path = Path(PROJECTS[project_name]) / ".all-contributorsrc"
    
    if not rc_path.exists():
        print(f"Warning: {rc_path} does not exist", file=__import__('sys').stderr)
        return False
    
    with open(rc_path, 'r') as f:
        rc_data = json.load(f)
    
    # Merge new contributors with existing
    existing_logins = {c['login'].lower() for c in rc_data.get('contributors', [])}
    
    added = []
    for new_contrib in contributors:
        if new_contrib['login'].lower() not in existing_logins:
            rc_data.setdefault('contributors', []).append(new_contrib)
            added.append(new_contrib['login'])
            existing_logins.add(new_contrib['login'].lower())
    
    if dry_run:
        print(f"\n[DRY RUN] Would add {len(added)} contributors to {rc_path}:")
        for login in added:
            print(f"  - @{login}")
        return True
    
    # Write updated file
    with open(rc_path, 'w') as f:
        json.dump(rc_data, f, indent=4)
    
    print(f"Updated {rc_path} with {len(added)} new contributors")
    return True


def main():
    parser = argparse.ArgumentParser(description="Scan git history for contributors")
    parser.add_argument("--project", choices=list(PROJECTS.keys()), help="Scan specific project")
    parser.add_argument("--output", choices=["json", "table", "rc"], default="table", help="Output format")
    parser.add_argument("--min-commits", type=int, default=1, help="Minimum commits to include")
    parser.add_argument("--update", action="store_true", help="Update .all-contributorsrc files")
    parser.add_argument("--dry-run", action="store_true", help="Show what would be updated without writing")
    args = parser.parse_args()
    
    # Determine which projects to scan
    if args.project:
        projects = {args.project: PROJECTS[args.project]}
    else:
        projects = PROJECTS
    
    results = []
    
    for name, path in projects.items():
        print(f"Scanning {name}...", file=__import__('sys').stderr)
        data = analyze_project(name, path)
        
        # Filter by minimum commits
        data['contributors'] = [c for c in data['contributors'] if c['commits'] >= args.min_commits]
        data['total_contributors'] = len(data['contributors'])
        
        results.append(data)
    
    # Update mode
    if args.update or args.dry_run:
        for data in results:
            rc_data = format_as_allcontributorsrc(data)
            update_allcontributorsrc(
                data['project'], 
                rc_data['contributors'],
                dry_run=args.dry_run or not args.update
            )
        return
    
    # Output results
    if args.output == "json":
        print(json.dumps(results, indent=2))
    elif args.output == "rc":
        for data in results:
            rc_data = format_as_allcontributorsrc(data)
            print(f"\n=== {data['project']}/.all-contributorsrc contributors ===")
            print(json.dumps(rc_data['contributors'], indent=2))
    else:  # table
        for data in results:
            print(format_as_table(data))


if __name__ == "__main__":
    main()


## Links discovered
- [r"\btranslat(e|ion|ed)\b",
        r"\blocali[sz](https://github.com/harvard-edge/cs249r_book/blob/dev/.github/workflows/contributors/e|ation.md)

--- .github/workflows/contributors/update_contributors.py ---
#!/usr/bin/env python3
"""
Update contributors from GitHub API.

This script queries the GitHub API to find all contributors to the repository
and updates the root .all-contributorsrc file with their information.

Features:
- Fetches contributors from GitHub commit history
- Resolves email addresses to GitHub usernames
- Generates gravatar URLs for contributors without GitHub avatars
- Excludes bots and specified users
- Merges new contributors with existing ones

Usage:
    python update_contributors.py

Environment variables:
    GITHUB_TOKEN: Required. GitHub personal access token for API access.

Note: This script updates the ROOT .all-contributorsrc file only.
For per-project configs, use scan_contributors.py instead.
"""

import os
import json
import random
import hashlib

import requests
import pandas as pd
from absl import app
from absl import logging

CONTRIBUTORS_FILE = ".all-contributorsrc"

EXCLUDED_USERS = {
    "web-flow",
    "github-actions[bot]",
    "mrdragonbear",
    "jveejay",
    "Matthew Steward",
}

OWNER = "harvard-edge"
REPO = "cs249r_book"
BRANCH = "dev"
RESULTS_PER_PAGE = 1000


def get_user_data_from_username(username):
    headers = {"Authorization": f"token {os.environ['GITHUB_TOKEN']}"}
    res = requests.get(f"https://api.github.com/users/{username}", headers=headers)
    user_full_name = pd.NA
    email_address = pd.NA
    if res.status_code == 200:
        user_data = res.json()
        user_full_name = user_data["name"]
        email_address = user_data["email"]
    else:
        logging.error(f"Could not find user with username: {username}")
    return {
        "username": username,
        "user_full_name": user_full_name,
        "email_address": email_address,
    }


def get_user_data_from_email(email_address):
    headers = {"Authorization": f"token {os.environ['GITHUB_TOKEN']}"}
    res = requests.get(
        f"https://api.github.com/search/users?q={email_address}", headers=headers
    )
    username = pd.NA
    if res.status_code == 200:
        user_data = res.json()
        if user_data["total_count"] > 0:
            username = user_data["items"][0]["login"]
    else:
        logging.error(f"Could not find user with email address: {email_address}")
    return {
        "username": username,
        "user_full_name": pd.NA,
        "email_address": email_address,
    }


def get_co_authors_from_commit_message(commit_message):
    co_author_data = []
    if commit_message:
        lines = commit_message.splitlines()
        for line in lines:
            try:
                if line.startswith("Co-authored-by:"):
                    co_author = line.split(":")[1].strip()
                    user_full_name, email_address = co_author.split("<")
                    user_full_name = user_full_name.strip()
                    email_address = email_address.strip(">")
                    co_author_data.append(
                        {
                            "user_full_name": user_full_name,
                            "email_address": email_address,
                            "username": pd.NA,
                        }
                    )
            except ValueError as e:
                logging.error(
                    f"Error parsing co-author: {line}. Co-author should be of the form: "
                    f"'Co-authored-by: NAME <email>'. "
                    f"Remember to include the angle brackets around the email."
                )
        return pd.DataFrame(co_author_data)


def merge_user_full_names(row, col1, col2):
    """
    Merges two columns containing user full names based on the following criteria:
    - Takes the longest name that is not null and not an email address.

    Parameters:
    - row: A single row from the DataFrame.
    - col1: The first column name containing user full names.
    - col2: The second column name containing user full names.

    Returns:
    - The merged user full name based on the criteria.
    """

    def is_email(string):
        return isinstance(string, str) and "@" in string

    name1 = row[col1]
    name2 = row[col2]

    if (
        pd.notna(name1)
        and not is_email(name1)
        and (pd.isna(name2) or len(name1) >= len(name2))
    ):
        return name1
    elif pd.notna(name2) and not is_email(name2):
        return name2
    else:
        return pd.NA


def merge_email_addresses(row, col1, col2):
    """
    Merges two columns containing email addresses based on the following criteria:
    - Returns the email address that is not null.
    - If both email addresses are not null, it prioritizes the one that does not contain 'noreply.github.com'.

    Parameters:
    - row: A single row from the DataFrame.
    - col1: The first column name containing email addresses.
    - col2: The second column name containing email addresses.

    Returns:
    - The selected email address based on the criteria, or pd.NA if both are null.
    """

    email1 = row[col1]
    email2 = row[col2]

    # Check if either email is not null
    if pd.notna(email1) and "noreply.github.com" not in email1:
        return email1
    elif pd.notna(email2) and "noreply.github.com" not in email2:
        return email2
    elif pd.notna(email1):
        return email1
    elif pd.notna(email2):
        return email2
    else:
        return pd.NA


def main(_):
    token = os.environ["GITHUB_TOKEN"]
    # Get BOOK_QUARTO from environment variable, fallback to default
    book_quarto = os.environ.get("BOOK_QUARTO", "book/quarto")
    headers = {"Authorization": f"token {token}"}
    data = []
    next_page = f"https://api.github.com/repos/{OWNER}/{REPO}/commits?sha={BRANCH}&per_page={RESULTS_PER_PAGE}"
    last_page = None
    while next_page != last_page:
        print(f"Fetching page: {next_page}")
        res = requests.get(next_page, headers=headers)
        if res.status_code != 200:
            logging.error(f"GitHub API request failed with status {res.status_code}: {res.text}")
            raise RuntimeError(f"Failed to fetch commits: {res.status_code}")
        data.extend(res.json())
        next_page = res.links.get("next", {}).get("url", None)
        last_page = res.links.get("last", {}).get("url", None)

    # Parse the commit response data
    commit_data = []
    for node in data:
        commit_message = node.get("commit", {}).get("message", pd.NA)
        commit_info = node.get("commit", None)
        commit_author_info = commit_info.get("author", None)
        commit_commiter_info = commit_info.get("committer", None)
        author_info = node.get("author", None)
        committer_info = node.get("committer", None)
        committer_login = (
            committer_info.get("login", None) if committer_info else None
        )
        user_full_name = pd.NA
        user_login = pd.NA
        user_email_address = pd.NA

        if commit_author_info:
            user_full_name = commit_author_info["name"]
            user_email_address = commit_author_info["email"]
        elif commit_commiter_info:
            user_full_name = commit_commiter_info["name"]

        if author_info:
            user_login = author_info["login"]
        elif committer_login:
            user_login = committer_login

        commit_data.append(
            {
                "commit_message": commit_message,
                "user_full_name": user_full_name,
                "email_address": user_email_address,
                "username": user_login,
            }
        )
    commit_data_df = pd.DataFrame(commit_data)

    # Parse the co-author data from the commit messages
    co_authors_list = [
        get_co_authors_from_commit_message(row["commit_message"])
        for index, row in commit_data_df.iterrows()
    ]
    co_authors_df = pd.concat(co_authors_list, ignore_index=True)

    # All co-authors must have an email address, so look up info and replace
    # with whatever is on GitHub
    for index, row in co_authors_df.iterrows():
        user_data = get_user_data_from_email(row.email_address)
        co_authors_df.loc[index, "username"] = user_data["username"]

    # Remove excluded users
    co_authors_df = co_authors_df[
        ~co_authors_df["username"].isin(EXCLUDED_USERS)
        & ~co_authors_df["user_full_name"].isin(EXCLUDED_USERS)
    ]
    commit_data_df = commit_data_df[
        ~commit_data_df["username"].isin(EXCLUDED_USERS)
        & ~commit_data_df["user_full_name"].isin(EXCLUDED_USERS)
    ]

    # Count contributions in each DataFrame
    co_authors_df["co_author_count"] = co_authors_df.groupby("email_address")[
        "email_address"
    ].transform("count")

    # Create a combined key using username and email_address to handle
    # cases with missing usernames. Users can commit without specifying
    # their username, but they should have an email address.
    commit_data_df["user_key"] = commit_data_df["username"].combine_first(
        commit_data_df["email_address"]
    )

    # Count the number of commits per user (grouped by user_key)
    commit_data_df["commit_count"] = commit_data_df.groupby("user_key")[
        "user_key"
    ].transform("count")

    # Drop the user_key if it's no longer needed
    commit_data_df.drop(columns=["user_key"], inplace=True)

    # Since we have the count, remove duplicates
    commit_data_df = commit_data_df.drop(columns=["commit_message"])
    co_authors_df.drop_duplicates(inplace=True)
    commit_data_df.drop_duplicates(inplace=True)

    # Now try to find all users with GitHub API
    for index, row in commit_data_df.iterrows():
        if not pd.isna(row["username"]):
            user_data = get_user_data_from_username(row["username"])
            if not pd.isna(user_data["username"]):
                commit_data_df.loc[index, "user_full_name"] = user_data[
                    "user_full_name"
                ]
            if not pd.isna(user_data["email_address"]):
                commit_data_df.loc[index, "email_address"] = user_data["email_address"]
        elif not pd.isna(row["email_address"]):
            user_data = get_user_data_from_email(row["email_address"])
            if not pd.isna(user_data["username"]):
                commit_data_df.loc[index, "username"] = user_data["username"]
            if not pd.isna(user_data["user_full_name"]):
                commit_data_df.loc[index, "user_full_name"] = user_data[
                    "user_full_name"
                ]
        else:
            logging.error(
                f"Could not find user data for commit. Username: {row.get('username', 'N/A')}, Email: {row.get('email_address', 'N/A')}"
            )

    co_authors_with_username = co_authors_df[~co_authors_df["username"].isna()]
    co_authors_without_username = co_authors_df[co_authors_df["username"].isna()]

    # First merge: on username
    merged_df = co_authors_with_username.merge(
        commit_data_df,
        how="outer",
        on=["username"],
        suffixes=("_co", "_commit"),
        indicator=True,
    )

    # Calculate total contributions after first merge
    merged_df["total_contributions"] = merged_df["co_author_count"].fillna(
        0
    ) + merged_df["commit_count"].fillna(0)

    # Merge user full name columns
    merged_df["user_full_name"] = merged_df.apply(
        merge_user_full_names,
        col1="user_full_name_commit",
        col2="user_full_name_co",
        axis=1,
    )

    # Merge email address columns
    merged_df["email_address"] = merged_df.apply(
        merge_email_addresses,
        col1="email_address_commit",
        col2="email_address_co",
        axis=1,
    )

    # Drop unnecessary columns
    merged_df = merged_df.drop(
        columns=[
            "_merge",
            "co_author_count",
            "commit_count",
            "user_full_name_co",
            "user_full_name_commit",
            "email_address_co",
            "email_address_commit",
        ]
    )
    merged_df.drop_duplicates(inplace=True)

    # Second merge: co-authors without username on email
    merged_df = co_authors_without_username.merge(
        merged_df,
        how="outer",
        on="email_address",
        suffixes=("_co_no_user", ""),
        indicator=True,
    )

    # Update total contributions after second merge
    merged_df["total_contributions"] = merged_df["total_contributions"].fillna(
        0
    ) + merged_df["co_author_count"].fillna(0)

    # Merge user full name columns
    merged_df["user_full_name"] = merged_df.apply(
        merge_user_full_names,
        col1="user_full_name",
        col2="user_full_name_co_no_user",
        axis=1,
    )

    # Remove unnecessary columns
    merged_df = merged_df.drop(
        columns=["_merge", "co_author_count", "username_co_no_user", "user_full_name_co_no_user"]
    )

    # Get name length to figure out which full name to use
    merged_df = merged_df.assign(name_length=merged_df["user_full_name"].str.len())
    merged_df = merged_df.fillna(pd.NA)
    merged_df = merged_df.sort_values(
        by=["total_contributions", "name_length"], ascending=False
    )

    # Separate rows with and without usernames
    df_with_username = merged_df.dropna(subset=["username"])
    df_without_username = merged_df[merged_df["username"].isna()]

    # Group by username, and take the user_full_name with the most characters for rows with usernames
    df_with_username = df_with_username.groupby("username", as_index=False).first()

    # Remove rows from df_without_username where the user_full_name matches a user_full_name in df_with_username.
    # We do this to avoid duplicate entries for the same user. Without a
    # username, we do not know if two rows are the same user.
    df_without_username = df_without_username[
        ~df_without_username["user_full_name"].isin(df_with_username["user_full_name"])
    ]

    # Combine the grouped rows with usernames and the original rows without usernames
    merged_df = pd.concat([df_with_username, df_without_username], ignore_index=True)

    def generate_gravatar_url(name):
        random.seed(name)
        name_list = list(name)
        random.shuffle(name_list)
        name = "".join(name_list)
        name_hash = hashlib.md5(name.encode("utf-8")).hexdigest()
        return f"https://www.gravatar.com/avatar/{name_hash}?d=identicon&s=100"

    # Update avatar_url
    merged_df["avatar_url"] = merged_df.apply(
        lambda row: (
            generate_gravatar_url(row["user_full_name"])
            if pd.isna(row["username"])
            else f"https://avatars.githubusercontent.com/{row['username']}"
        ),
        axis=1,
    )

    # Update profile URL
    merged_df["profile"] = merged_df.apply(
        lambda row: (
            "https://github.com/harvard-edge/cs249r_book/graphs/contributors"
            if pd.isna(row["username"])
            else f"https://github.com/{row['username']}"
        ),
        axis=1,
    )

    # Sort by number of commits
    merged_df.sort_values(by="total_contributions", ascending=False, inplace=True)

    final_result = dict(
        projectName=REPO,
        projectOwner=OWNER,
        files=[f"{book_quarto}/contents/frontmatter/acknowledgements/acknowledgements.qmd", "README.md"],
        contributors=[
            dict(
                login=(
                    row.username if not pd.isna(row.username) else row.user_full_name
                ),
                name=(
                    row.user_full_name
                    if not pd.isna(row.user_full_name)
                    else row.username
                ),
                avatar_url=row.avatar_url,
                profile=row.profile,
                contributions=[],
            )
            for row in merged_df.itertuples()
        ],
        repoType="github",
        contributorsPerLine=5,
        repoHost="https://github.com",
        commitConvention="angular",
        skipCi=True,
    )

    json_string = json.dumps(final_result, indent=4)
    print(json_string)

    with open(CONTRIBUTORS_FILE, "w") as contrib_file:
        contrib_file.write(json_string)


if __name__ == "__main__":
    try:
        app.run(main)
    except Exception as e:
        logging.error(f"Script failed with error: {e}")
        import traceback
        logging.error(traceback.format_exc())
        raise


--- book/README.md ---
# Machine Learning Systems

*Principles and Practices of Engineering Artificially Intelligent Systems*

[![Build](https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/book-validate-dev.yml?branch=dev&label=Build&logo=githubactions)](https://github.com/harvard-edge/cs249r_book/actions/workflows/book-validate-dev.yml)
[![Website](https://img.shields.io/badge/Read-mlsysbook.ai-blue)](https://mlsysbook.ai)
[![PDF](https://img.shields.io/badge/Download-PDF-red)](https://mlsysbook.ai/pdf)
[![EPUB](https://img.shields.io/badge/Download-EPUB-green)](https://mlsysbook.ai/epub)

**[Read Online](https://mlsysbook.ai)** | **[PDF](https://mlsysbook.ai/pdf)** | **[EPUB](https://mlsysbook.ai/epub)**

---

## What This Is

The ML Systems textbook teaches you how to engineer AI systems that work in the real world. It bridges machine learning theory with systems engineering practice, covering everything from neural network fundamentals to production deployment.

This directory contains the textbook source and build system for contributors.

---

## What You Will Learn

| ML Concepts | Systems Engineering |
|-------------|---------------------|
| Neural networks and deep learning | Memory hierarchies and caching |
| Model architectures (CNNs, Transformers) | Hardware accelerators (GPUs, TPUs, NPUs) |
| Training and optimization | Distributed systems and parallelism |
| Inference and deployment | Power and thermal management |
| Compression and quantization | Latency, throughput, and efficiency |

### The ML ‚Üî Systems Bridge

| You know... | You will learn... |
|-------------|-------------------|
| How to train a model | How training scales across GPU clusters |
| That quantization shrinks models | How INT8 math maps to silicon |
| What a transformer is | Why KV-cache dominates memory |
| Models run on GPUs | How schedulers balance latency vs throughput |
| Edge devices have limits | How to co-design models and hardware |

### Book Structure

| Part | Focus | Chapters |
|------|-------|----------|
| **Foundations** | ML and systems basics | Introduction, ML Primer, DL Primer, AI Acceleration |
| **Workflow** | Production pipeline | Workflows, Data Engineering, Frameworks |
| **Training** | Learning at scale | Training, Distributed Training, Efficient AI |
| **Deployment** | Real-world systems | Inference, On-Device AI, Hardware Benchmarking, Ops |
| **Advanced** | Frontier topics | Privacy, Security, Responsible AI, Sustainable AI, Genertic AI, Frontiers |

---

## What Makes This Book Different

**Systems first**: Start with hardware constraints and work up to algorithms, not the other way around.

**Production focus**: Every concept connects to real deployment scenarios, not just research benchmarks.

**Open and evolving**: Community-driven updates keep content current with a fast-moving field.

**Hands-on companion**: Pair with [TinyTorch](../tinytorch/) to build what you learn from scratch.

---

## Quick Start

### For Readers

```bash
# Read online
open https://mlsysbook.ai

# Download formats
curl -O https://mlsysbook.ai/pdf
curl -O https://mlsysbook.ai/epub
```

### For Contributors

```bash
cd book

# First time setup
./binder setup
./binder doctor

# Daily workflow
./binder clean              # Clean build artifacts
./binder build              # Build HTML book
./binder preview intro      # Preview chapter with live reload

# Build all formats
./binder pdf                # Build PDF
./binder epub               # Build EPUB

# Utilities
./binder help               # Show all commands
./binder list               # List chapters
```

---

## Directory Structure

```
book/
‚îú‚îÄ‚îÄ quarto/              # Book source (Quarto markdown)
‚îÇ   ‚îú‚îÄ‚îÄ contents/        # Chapter content
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core/        # Core chapters
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ labs/        # Hands-on labs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ frontmatter/ # Preface, about, changelog
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ backmatter/  # References, glossary
‚îÇ   ‚îú‚îÄ‚îÄ assets/          # Images, downloads
‚îÇ   ‚îî‚îÄ‚îÄ _quarto.yml      # Quarto configuration
‚îú‚îÄ‚îÄ cli/                 # Binder CLI tool
‚îú‚îÄ‚îÄ docker/              # Development containers
‚îú‚îÄ‚îÄ docs/                # Documentation
‚îú‚îÄ‚îÄ tools/               # Build scripts
‚îî‚îÄ‚îÄ binder               # CLI entry point
```

---

## Documentation

| Audience | Resources |
|----------|-----------|
| **Readers** | [Online Book](https://mlsysbook.ai) „Éª [PDF](https://mlsysbook.ai/pdf) „Éª [EPUB](https://mlsysbook.ai/epub) |
| **Contributors** | [CONTRIBUTING.md](docs/CONTRIBUTING.md) „Éª [BUILD.md](docs/BUILD.md) |
| **Developers** | [DEVELOPMENT.md](docs/DEVELOPMENT.md) „Éª [BINDER.md](docs/BINDER.md) |

---

## Contributing

We welcome contributions! See [docs/CONTRIBUTING.md](docs/CONTRIBUTING.md) for guidelines.

1. **Fork and clone** the repository
2. **Set up** your environment: `./binder setup`
3. **Find an issue** or propose a change
4. **Make your changes** in the `quarto/contents/` directory
5. **Preview** your changes: `./binder preview <chapter>`
6. **Submit a PR** with a clear description

---

## Related

| Component | Description |
|-----------|-------------|
| **[Main README](../README.md)** | Project overview and ecosystem |
| **[TinyTorch](../tinytorch/)** | Build ML frameworks from scratch |
| **[Hardware Kits](../kits/)** | Deploy to Arduino, Raspberry Pi, edge devices |
| **[Website](https://mlsysbook.ai)** | Read the book online |

---

## Contributors

Thanks to these wonderful people who helped improve the book!

**Legend:** ü™≤ Bug Hunter ¬∑ ‚ö° Code Warrior ¬∑ üìö Documentation Hero ¬∑ üé® Design Artist ¬∑ üß† Idea Generator ¬∑ üîé Code Reviewer ¬∑ üß™ Test Engineer ¬∑ üõ†Ô∏è Tool Builder

<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
<table>
  <tbody>
    <tr>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/profvjreddi"><img src="https://avatars.githubusercontent.com/profvjreddi?v=4?s=80" width="80px;" alt="Vijay Janapa Reddi"/><br /><sub><b>Vijay Janapa Reddi</b></sub></a><br />ü™≤ üßë‚Äçüíª üé® ‚úçÔ∏è üß† üîé üß™ üõ†Ô∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/Mjrovai"><img src="https://avatars.githubusercontent.com/Mjrovai?v=4?s=80" width="80px;" alt="Marcelo Rovai"/><br /><sub><b>Marcelo Rovai</b></sub></a><br />üßë‚Äçüíª üé® üß™</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/GabrielAmazonas"><img src="https://avatars.githubusercontent.com/GabrielAmazonas?v=4?s=80" width="80px;" alt="Gabriel Amazonas"/><br /><sub><b>Gabriel Amazonas</b></sub></a><br />ü™≤ ‚úçÔ∏è üß†</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/kai4avaya"><img src="https://avatars.githubusercontent.com/kai4avaya?v=4?s=80" width="80px;" alt="Kai Kleinbard"/><br /><sub><b>Kai Kleinbard</b></sub></a><br />üßë‚Äçüíª üõ†Ô∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/didier-durand"><img src="https://avatars.githubusercontent.com/didier-durand?v=4?s=80" width="80px;" alt="Didier Durand"/><br /><sub><b>Didier Durand</b></sub></a><br />‚úçÔ∏è ü™≤</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/hzeljko"><img src="https://avatars.githubusercontent.com/hzeljko?v=4?s=80" width="80px;" alt="Zeljko Hrcek"/><br /><sub><b>Zeljko Hrcek</b></sub></a><br />üßë‚Äçüíª</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/jasonjabbour"><img src="https://avatars.githubusercontent.com/jasonjabbour?v=4?s=80" width="80px;" alt="Jason Jabbour"/><br /><sub><b>Jason Jabbour</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/uchendui"><img src="https://avatars.githubusercontent.com/uchendui?v=4?s=80" width="80px;" alt="Ikechukwu Uchendu"/><br /><sub><b>Ikechukwu Uchendu</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/Naeemkh"><img src="https://avatars.githubusercontent.com/Naeemkh?v=4?s=80" width="80px;" alt="Naeem Khoshnevis"/><br /><sub><b>Naeem Khoshnevis</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/Sara-Khosravi"><img src="https://avatars.githubusercontent.com/Sara-Khosravi?v=4?s=80" width="80px;" alt="Sara Khosravi"/><br /><sub><b>Sara Khosravi</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/V0XNIHILI"><img src="https://avatars.githubusercontent.com/V0XNIHILI?v=4?s=80" width="80px;" alt="Douwe den Blanken"/><br /><sub><b>Douwe den Blanken</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/18jeffreyma"><img src="https://avatars.githubusercontent.com/18jeffreyma?v=4?s=80" width="80px;" alt="Jeffrey Ma"/><br /><sub><b>Jeffrey Ma</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/shanzehbatool"><img src="https://avatars.githubusercontent.com/shanzehbatool?v=4?s=80" width="80px;" alt="shanzehbatool"/><br /><sub><b>shanzehbatool</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/eliasab16"><img src="https://avatars.githubusercontent.com/eliasab16?v=4?s=80" width="80px;" alt="Elias"/><br /><sub><b>Elias</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/JaredP94"><img src="https://avatars.githubusercontent.com/JaredP94?v=4?s=80" width="80px;" alt="Jared Ping"/><br /><sub><b>Jared Ping</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/ishapira1"><img src="https://avatars.githubusercontent.com/ishapira1?v=4?s=80" width="80px;" alt="Itai Shapira"/><br /><sub><b>Itai Shapira</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/8863743b4f26c1a20e730fcf7ebc3bc0?d=identicon&s=100?v=4?s=80" width="80px;" alt="Maximilian Lam"/><br /><sub><b>Maximilian Lam</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/jaysonzlin"><img src="https://avatars.githubusercontent.com/jaysonzlin?v=4?s=80" width="80px;" alt="Jayson Lin"/><br /><sub><b>Jayson Lin</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/sophiacho1"><img src="https://avatars.githubusercontent.com/sophiacho1?v=4?s=80" width="80px;" alt="Sophia Cho"/><br /><sub><b>Sophia Cho</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/andreamurillomtz"><img src="https://avatars.githubusercontent.com/andreamurillomtz?v=4?s=80" width="80px;" alt="Andrea"/><br /><sub><b>Andrea</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/alxrod"><img src="https://avatars.githubusercontent.com/alxrod?v=4?s=80" width="80px;" alt="Alex Rodriguez"/><br /><sub><b>Alex Rodriguez</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/korneelf1"><img src="https://avatars.githubusercontent.com/korneelf1?v=4?s=80" width="80px;" alt="Korneel Van den Berghe"/><br /><sub><b>Korneel Van den Berghe</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/foundingnimo"><img src="https://avatars.githubusercontent.com/foundingnimo?v=4?s=80" width="80px;" alt="Nimo"/><br /><sub><b>Nimo</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/colbybanbury"><img src="https://avatars.githubusercontent.com/colbybanbury?v=4?s=80" width="80px;" alt="Colby Banbury"/><br /><sub><b>Colby Banbury</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/zishenwan"><img src="https://avatars.githubusercontent.com/zishenwan?v=4?s=80" width="80px;" alt="Zishen Wan"/><br /><sub><b>Zishen Wan</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/mmaz"><img src="https://avatars.githubusercontent.com/mmaz?v=4?s=80" width="80px;" alt="Mark Mazumder"/><br /><sub><b>Mark Mazumder</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/ma3mool"><img src="https://avatars.githubusercontent.com/ma3mool?v=4?s=80" width="80px;" alt="Abdulrahman Mahmoud"/><br /><sub><b>Abdulrahman Mahmoud</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/DivyaAmirtharaj"><img src="https://avatars.githubusercontent.com/DivyaAmirtharaj?v=4?s=80" width="80px;" alt="Divya Amirtharaj"/><br /><sub><b>Divya Amirtharaj</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/srivatsankrishnan"><img src="https://avatars.githubusercontent.com/srivatsankrishnan?v=4?s=80" width="80px;" alt="Srivatsan Krishnan"/><br /><sub><b>Srivatsan Krishnan</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/arnaumarin"><img src="https://avatars.githubusercontent.com/arnaumarin?v=4?s=80" width="80px;" alt="marin-llobet"/><br /><sub><b>marin-llobet</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/aptl26"><img src="https://avatars.githubusercontent.com/aptl26?v=4?s=80" width="80px;" alt="Aghyad Deeb"/><br /><sub><b>Aghyad Deeb</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/James-QiuHaoran"><img src="https://avatars.githubusercontent.com/James-QiuHaoran?v=4?s=80" width="80px;" alt="Haoran Qiu"/><br /><sub><b>Haoran Qiu</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/Ekhao"><img src="https://avatars.githubusercontent.com/Ekhao?v=4?s=80" width="80px;" alt="Emil Njor"/><br /><sub><b>Emil Njor</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/ELSuitorHarvard"><img src="https://avatars.githubusercontent.com/ELSuitorHarvard?v=4?s=80" width="80px;" alt="ELSuitorHarvard"/><br /><sub><b>ELSuitorHarvard</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/kaiM0ves"><img src="https://avatars.githubusercontent.com/kaiM0ves?v=4?s=80" width="80px;" alt="kaiM0ves"/><br /><sub><b>kaiM0ves</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/oishib"><img src="https://avatars.githubusercontent.com/oishib?v=4?s=80" width="80px;" alt="oishib"/><br /><sub><b>oishib</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/jared-ni"><img src="https://avatars.githubusercontent.com/jared-ni?v=4?s=80" width="80px;" alt="Jared Ni"/><br /><sub><b>Jared Ni</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/AditiR-42"><img src="https://avatars.githubusercontent.com/AditiR-42?v=4?s=80" width="80px;" alt="Aditi Raju"/><br /><sub><b>Aditi Raju</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/MichaelSchnebly"><img src="https://avatars.githubusercontent.com/MichaelSchnebly?v=4?s=80" width="80px;" alt="Michael Schnebly"/><br /><sub><b>Michael Schnebly</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/VThuong99"><img src="https://avatars.githubusercontent.com/VThuong99?v=4?s=80" width="80px;" alt="Thuong Duong"/><br /><sub><b>Thuong Duong</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/leo47007"><img src="https://avatars.githubusercontent.com/leo47007?v=4?s=80" width="80px;" alt="Yu-Shun Hsiao"/><br /><sub><b>Yu-Shun Hsiao</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/BaeHenryS"><img src="https://avatars.githubusercontent.com/BaeHenryS?v=4?s=80" width="80px;" alt="Henry Bae"/><br /><sub><b>Henry Bae</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/eimlav"><img src="https://avatars.githubusercontent.com/eimlav?v=4?s=80" width="80px;" alt="Eimhin Laverty"/><br /><sub><b>Eimhin Laverty</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/jaywonchung"><img src="https://avatars.githubusercontent.com/jaywonchung?v=4?s=80" width="80px;" alt="Jae-Won Chung"/><br /><sub><b>Jae-Won Chung</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/ShvetankPrakash"><img src="https://avatars.githubusercontent.com/ShvetankPrakash?v=4?s=80" width="80px;" alt="Shvetank Prakash"/><br /><sub><b>Shvetank Prakash</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/marcozennaro"><img src="https://avatars.githubusercontent.com/marcozennaro?v=4?s=80" width="80px;" alt="Marco Zennaro"/><br /><sub><b>Marco Zennaro</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/aryatschand"><img src="https://avatars.githubusercontent.com/aryatschand?v=4?s=80" width="80px;" alt="Arya Tschand"/><br /><sub><b>Arya Tschand</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/arbass22"><img src="https://avatars.githubusercontent.com/arbass22?v=4?s=80" width="80px;" alt="Andrew Bass"/><br /><sub><b>Andrew Bass</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/pongtr"><img src="https://avatars.githubusercontent.com/pongtr?v=4?s=80" width="80px;" alt="Pong Trairatvorakul"/><br /><sub><b>Pong Trairatvorakul</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/euranofshin"><img src="https://avatars.githubusercontent.com/euranofshin?v=4?s=80" width="80px;" alt="Eura Nofshin"/><br /><sub><b>Eura Nofshin</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/0c931fcfd03cd548d44c90602dd773ba?d=identicon&s=100?v=4?s=80" width="80px;" alt="Matthew Stewart"/><br /><sub><b>Matthew Stewart</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/af39c27c6090c50a1921a9b6366e81cc?d=identicon&s=100?v=4?s=80" width="80px;" alt="Emeka Ezike"/><br /><sub><b>Emeka Ezike</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/jianqingdu"><img src="https://avatars.githubusercontent.com/jianqingdu?v=4?s=80" width="80px;" alt="jianqingdu"/><br /><sub><b>jianqingdu</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/jzhou1318"><img src="https://avatars.githubusercontent.com/jzhou1318?v=4?s=80" width="80px;" alt="Jennifer Zhou"/><br /><sub><b>Jennifer Zhou</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/vitasam"><img src="https://avatars.githubusercontent.com/vitasam?v=4?s=80" width="80px;" alt="The Random DIY"/><br /><sub><b>The Random DIY</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/468ef35acc69f3266efd700992daa369?d=identicon&s=100?v=4?s=80" width="80px;" alt="Fatima Shah"/><br /><sub><b>Fatima Shah</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/BrunoScaglione"><img src="https://avatars.githubusercontent.com/BrunoScaglione?v=4?s=80" width="80px;" alt="Bruno Scaglione"/><br /><sub><b>Bruno Scaglione</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/Allen-Kuang"><img src="https://avatars.githubusercontent.com/Allen-Kuang?v=4?s=80" width="80px;" alt="Allen-Kuang"/><br /><sub><b>Allen-Kuang</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/4ad8cdf19eb3b666ace97d3eedb19278?d=identicon&s=100?v=4?s=80" width="80px;" alt="Tess314"/><br /><sub><b>Tess314</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/taunoe"><img src="https://avatars.githubusercontent.com/taunoe?v=4?s=80" width="80px;" alt="Tauno Erik"/><br /><sub><b>Tauno Erik</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/gnodipac886"><img src="https://avatars.githubusercontent.com/gnodipac886?v=4?s=80" width="80px;" alt="gnodipac886"/><br /><sub><b>gnodipac886</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/serco425"><img src="https://avatars.githubusercontent.com/serco425?v=4?s=80" width="80px;" alt="Sercan Ayg√ºn"/><br /><sub><b>Sercan Ayg√ºn</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/TheHiddenLayer"><img src="https://avatars.githubusercontent.com/TheHiddenLayer?v=4?s=80" width="80px;" alt="TheHiddenLayer"/><br /><sub><b>TheHiddenLayer</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/Gjain234"><img src="https://avatars.githubusercontent.com/Gjain234?v=4?s=80" width="80px;" alt="Gauri Jain"/><br /><sub><b>Gauri Jain</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/FinAminToastCrunch"><img src="https://avatars.githubusercontent.com/FinAminToastCrunch?v=4?s=80" width="80px;" alt="Fin Amin"/><br /><sub><b>Fin Amin</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/alex-oesterling"><img src="https://avatars.githubusercontent.com/alex-oesterling?v=4?s=80" width="80px;" alt="Alex Oesterling"/><br /><sub><b>Alex Oesterling</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/AbenezerKb"><img src="https://avatars.githubusercontent.com/AbenezerKb?v=4?s=80" width="80px;" alt="Abenezer Angamo"/><br /><sub><b>Abenezer Angamo</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/BravoBaldo"><img src="https://avatars.githubusercontent.com/BravoBaldo?v=4?s=80" width="80px;" alt="Baldassarre Cesarano"/><br /><sub><b>Baldassarre Cesarano</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/Jahnic-kb"><img src="https://avatars.githubusercontent.com/Jahnic-kb?v=4?s=80" width="80px;" alt="Jahnic Beck"/><br /><sub><b>Jahnic Beck</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/aethernavshulkraven-allain"><img src="https://avatars.githubusercontent.com/aethernavshulkraven-allain?v=4?s=80" width="80px;" alt="‡§Ö‡§∞‡§®‡§µ ‡§∂‡•Å‡§ï‡•ç‡§≤‡§æ | Arnav Shukla"/><br /><sub><b>‡§Ö‡§∞‡§®‡§µ ‡§∂‡•Å‡§ï‡•ç‡§≤‡§æ | Arnav Shukla</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/RinZ27"><img src="https://avatars.githubusercontent.com/RinZ27?v=4?s=80" width="80px;" alt="Rin"/><br /><sub><b>Rin</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/bilgeacun"><img src="https://avatars.githubusercontent.com/bilgeacun?v=4?s=80" width="80px;" alt="Bilge Acun"/><br /><sub><b>Bilge Acun</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/atcheng2"><img src="https://avatars.githubusercontent.com/atcheng2?v=4?s=80" width="80px;" alt="Andy Cheng"/><br /><sub><b>Andy Cheng</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/arighosh05"><img src="https://avatars.githubusercontent.com/arighosh05?v=4?s=80" width="80px;" alt="Aritra Ghosh"/><br /><sub><b>Aritra Ghosh</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/abigailswallow"><img src="https://avatars.githubusercontent.com/abigailswallow?v=4?s=80" width="80px;" alt="abigailswallow"/><br /><sub><b>abigailswallow</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/YangZhou1997"><img src="https://avatars.githubusercontent.com/YangZhou1997?v=4?s=80" width="80px;" alt="Yang Zhou"/><br /><sub><b>Yang Zhou</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/XaicuL"><img src="https://avatars.githubusercontent.com/XaicuL?v=4?s=80" width="80px;" alt="JEON HYUNJUN(Luciano)"/><br /><sub><b>JEON HYUNJUN(Luciano)</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/emmanuel2406"><img src="https://avatars.githubusercontent.com/emmanuel2406?v=4?s=80" width="80px;" alt="Emmanuel Rassou"/><br /><sub><b>Emmanuel Rassou</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/jasonlyik"><img src="https://avatars.githubusercontent.com/jasonlyik?v=4?s=80" width="80px;" alt="Jason Yik"/><br /><sub><b>Jason Yik</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/jessicaquaye"><img src="https://avatars.githubusercontent.com/jessicaquaye?v=4?s=80" width="80px;" alt="Jessica Quaye"/><br /><sub><b>Jessica Quaye</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/cursoragent"><img src="https://avatars.githubusercontent.com/cursoragent?v=4?s=80" width="80px;" alt="Cursor Agent"/><br /><sub><b>Cursor Agent</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/happyappledog"><img src="https://avatars.githubusercontent.com/happyappledog?v=4?s=80" width="80px;" alt="happyappledog"/><br /><sub><b>happyappledog</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/snuggs"><img src="https://avatars.githubusercontent.com/snuggs?v=4?s=80" width="80px;" alt="Snuggs"/><br /><sub><b>Snuggs</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/swilcock0"><img src="https://avatars.githubusercontent.com/swilcock0?v=4?s=80" width="80px;" alt="Sam Wilcock"/><br /><sub><b>Sam Wilcock</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/sjohri20"><img src="https://avatars.githubusercontent.com/sjohri20?v=4?s=80" width="80px;" alt="Shreya Johri"/><br /><sub><b>Shreya Johri</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/skmur"><img src="https://avatars.githubusercontent.com/skmur?v=4?s=80" width="80px;" alt="Sonia Murthy"/><br /><sub><b>Sonia Murthy</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/fc4f3460cdfb9365ab59bdeafb06413e?d=identicon&s=100?v=4?s=80" width="80px;" alt="Costin-Andrei Oncescu"/><br /><sub><b>Costin-Andrei Oncescu</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/0d6b8616427d8b19d425c9808692e347?d=identicon&s=100?v=4?s=80" width="80px;" alt="formlsysbookissue"/><br /><sub><b>formlsysbookissue</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/7cd8d5dfd83071f23979019d97655dc5?d=identicon&s=100?v=4?s=80" width="80px;" alt="Annie Laurie Cook"/><br /><sub><b>Annie Laurie Cook</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/5aa037840c0ca11ee42784ed4843c655?d=identicon&s=100?v=4?s=80" width="80px;" alt="Parampreet Singh"/><br /><sub><b>Parampreet Singh</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/b15b6e0e9adf58099905c1a0fd474cb9?d=identicon&s=100?v=4?s=80" width="80px;" alt="Vijay Edupuganti"/><br /><sub><b>Vijay Edupuganti</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/f88052cca4f401d9b0f43aed0a53434a?d=identicon&s=100?v=4?s=80" width="80px;" alt="Jothi Ramaswamy"/><br /><sub><b>Jothi Ramaswamy</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/35a8d9ffd03f05e79a2c6ce6206a56f2?d=identicon&s=100?v=4?s=80" width="80px;" alt="Batur Arslan"/><br /><sub><b>Batur Arslan</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/bd53d146aa888548c8db4da02bf81e7a?d=identicon&s=100?v=4?s=80" width="80px;" alt="Curren Iyer"/><br /><sub><b>Curren Iyer</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/8d8410338458e08bd5e4b96f58e1c217?d=identicon&s=100?v=4?s=80" width="80px;" alt="Edward Jin"/><br /><sub><b>Edward Jin</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/28c6123d2c9f75578d3ccdedb0df3d11?d=identicon&s=100?v=4?s=80" width="80px;" alt="Tess Watt"/><br /><sub><b>Tess Watt</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/ef139181fe00190f21730f6912532e9e?d=identicon&s=100?v=4?s=80" width="80px;" alt="bluebaer7"/><br /><sub><b>bluebaer7</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/f5d58ba6aa9b00189d4c018d370e8f43?d=identicon&s=100?v=4?s=80" width="80px;" alt="yanjingl"/><br /><sub><b>yanjingl</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/a5a47df988ab1720dd706062e523ca32?d=identicon&s=100?v=4?s=80" width="80px;" alt="a-saraf"/><br /><sub><b>a-saraf</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/c2dc311aa8122d5f5f061e1db14682b1?d=identicon&s=100?v=4?s=80" width="80px;" alt="songhan"/><br /><sub><b>songhan</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/4814aad67982ab07a69006a1ce9d2a72?d=identicon&s=100?v=4?s=80" width="80px;" alt="jvijay"/><br /><sub><b>jvijay</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"><img src="https://www.gravatar.com/avatar/43b1feff77c8a95fd581774fb8ec891f?d=identicon&s=100?v=4?s=80" width="80px;" alt="Zishen"/><br /><sub><b>Zishen</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/BunningsWarehouseOfficial"><img src="https://avatars.githubusercontent.com/u/49220945?v=4?v=4?s=80" width="80px;" alt="Kristian Rado≈°"/><br /><sub><b>Kristian Rado≈°</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/minhdang26403"><img src="https://avatars.githubusercontent.com/u/86156224?v=4?v=4?s=80" width="80px;" alt="Dang Truong"/><br /><sub><b>Dang Truong</b></sub></a><br />üßë‚Äçüíª</td>
    </tr>
  </tbody>
</table>

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->

<!-- ALL-CONTRIBUTORS-LIST:END -->

**Recognize a contributor:** Comment on any issue or PR:
```
@all-contributors please add @username for doc, review, translation, or design
```

---

## License

Book content is licensed under **Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International** (CC BY-NC-SA 4.0).

See [LICENSE.md](../LICENSE.md) for details.


## Links discovered
- [![Build](https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/book-validate-dev.yml?branch=dev&label=Build&logo=githubactions)
- [![Website](https://img.shields.io/badge/Read-mlsysbook.ai-blue)
- [![PDF](https://img.shields.io/badge/Download-PDF-red)
- [![EPUB](https://img.shields.io/badge/Download-EPUB-green)
- [Read Online](https://mlsysbook.ai)
- [PDF](https://mlsysbook.ai/pdf)
- [EPUB](https://mlsysbook.ai/epub)
- [TinyTorch](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch.md)
- [Online Book](https://mlsysbook.ai)
- [CONTRIBUTING.md](https://github.com/harvard-edge/cs249r_book/blob/dev/book/docs/CONTRIBUTING.md)
- [BUILD.md](https://github.com/harvard-edge/cs249r_book/blob/dev/book/docs/BUILD.md)
- [DEVELOPMENT.md](https://github.com/harvard-edge/cs249r_book/blob/dev/book/docs/DEVELOPMENT.md)
- [BINDER.md](https://github.com/harvard-edge/cs249r_book/blob/dev/book/docs/BINDER.md)
- [docs/CONTRIBUTING.md](https://github.com/harvard-edge/cs249r_book/blob/dev/book/docs/CONTRIBUTING.md)
- [Main README](https://github.com/harvard-edge/cs249r_book/blob/dev/README.md)
- [Hardware Kits](https://github.com/harvard-edge/cs249r_book/blob/dev/kits.md)
- [Website](https://mlsysbook.ai)
- [LICENSE.md](https://github.com/harvard-edge/cs249r_book/blob/dev/LICENSE.md)
- [<img src="https://avatars.githubusercontent.com/profvjreddi?v=4?s=80" width="80px;" alt="Vijay Janapa Reddi"/><br /><sub><b>Vijay Janapa Reddi</b></sub>](https://github.com/profvjreddi)
- [<img src="https://avatars.githubusercontent.com/Mjrovai?v=4?s=80" width="80px;" alt="Marcelo Rovai"/><br /><sub><b>Marcelo Rovai</b></sub>](https://github.com/Mjrovai)
- [<img src="https://avatars.githubusercontent.com/GabrielAmazonas?v=4?s=80" width="80px;" alt="Gabriel Amazonas"/><br /><sub><b>Gabriel Amazonas</b></sub>](https://github.com/GabrielAmazonas)
- [<img src="https://avatars.githubusercontent.com/kai4avaya?v=4?s=80" width="80px;" alt="Kai Kleinbard"/><br /><sub><b>Kai Kleinbard</b></sub>](https://github.com/kai4avaya)
- [<img src="https://avatars.githubusercontent.com/didier-durand?v=4?s=80" width="80px;" alt="Didier Durand"/><br /><sub><b>Didier Durand</b></sub>](https://github.com/didier-durand)
- [<img src="https://avatars.githubusercontent.com/hzeljko?v=4?s=80" width="80px;" alt="Zeljko Hrcek"/><br /><sub><b>Zeljko Hrcek</b></sub>](https://github.com/hzeljko)
- [<img src="https://avatars.githubusercontent.com/jasonjabbour?v=4?s=80" width="80px;" alt="Jason Jabbour"/><br /><sub><b>Jason Jabbour</b></sub>](https://github.com/jasonjabbour)
- [<img src="https://avatars.githubusercontent.com/uchendui?v=4?s=80" width="80px;" alt="Ikechukwu Uchendu"/><br /><sub><b>Ikechukwu Uchendu</b></sub>](https://github.com/uchendui)
- [<img src="https://avatars.githubusercontent.com/Naeemkh?v=4?s=80" width="80px;" alt="Naeem Khoshnevis"/><br /><sub><b>Naeem Khoshnevis</b></sub>](https://github.com/Naeemkh)
- [<img src="https://avatars.githubusercontent.com/Sara-Khosravi?v=4?s=80" width="80px;" alt="Sara Khosravi"/><br /><sub><b>Sara Khosravi</b></sub>](https://github.com/Sara-Khosravi)
- [<img src="https://avatars.githubusercontent.com/V0XNIHILI?v=4?s=80" width="80px;" alt="Douwe den Blanken"/><br /><sub><b>Douwe den Blanken</b></sub>](https://github.com/V0XNIHILI)
- [<img src="https://avatars.githubusercontent.com/18jeffreyma?v=4?s=80" width="80px;" alt="Jeffrey Ma"/><br /><sub><b>Jeffrey Ma</b></sub>](https://github.com/18jeffreyma)
- [<img src="https://avatars.githubusercontent.com/shanzehbatool?v=4?s=80" width="80px;" alt="shanzehbatool"/><br /><sub><b>shanzehbatool</b></sub>](https://github.com/shanzehbatool)
- [<img src="https://avatars.githubusercontent.com/eliasab16?v=4?s=80" width="80px;" alt="Elias"/><br /><sub><b>Elias</b></sub>](https://github.com/eliasab16)
- [<img src="https://avatars.githubusercontent.com/JaredP94?v=4?s=80" width="80px;" alt="Jared Ping"/><br /><sub><b>Jared Ping</b></sub>](https://github.com/JaredP94)
- [<img src="https://avatars.githubusercontent.com/ishapira1?v=4?s=80" width="80px;" alt="Itai Shapira"/><br /><sub><b>Itai Shapira</b></sub>](https://github.com/ishapira1)
- [<img src="https://www.gravatar.com/avatar/8863743b4f26c1a20e730fcf7ebc3bc0?d=identicon&s=100?v=4?s=80" width="80px;" alt="Maximilian Lam"/><br /><sub><b>Maximilian Lam</b></sub>](https://github.com/harvard-edge/cs249r_book/graphs/contributors)
- [<img src="https://avatars.githubusercontent.com/jaysonzlin?v=4?s=80" width="80px;" alt="Jayson Lin"/><br /><sub><b>Jayson Lin</b></sub>](https://github.com/jaysonzlin)
- [<img src="https://avatars.githubusercontent.com/sophiacho1?v=4?s=80" width="80px;" alt="Sophia Cho"/><br /><sub><b>Sophia Cho</b></sub>](https://github.com/sophiacho1)
- [<img src="https://avatars.githubusercontent.com/andreamurillomtz?v=4?s=80" width="80px;" alt="Andrea"/><br /><sub><b>Andrea</b></sub>](https://github.com/andreamurillomtz)
- [<img src="https://avatars.githubusercontent.com/alxrod?v=4?s=80" width="80px;" alt="Alex Rodriguez"/><br /><sub><b>Alex Rodriguez</b></sub>](https://github.com/alxrod)
- [<img src="https://avatars.githubusercontent.com/korneelf1?v=4?s=80" width="80px;" alt="Korneel Van den Berghe"/><br /><sub><b>Korneel Van den Berghe</b></sub>](https://github.com/korneelf1)
- [<img src="https://avatars.githubusercontent.com/foundingnimo?v=4?s=80" width="80px;" alt="Nimo"/><br /><sub><b>Nimo</b></sub>](https://github.com/foundingnimo)
- [<img src="https://avatars.githubusercontent.com/colbybanbury?v=4?s=80" width="80px;" alt="Colby Banbury"/><br /><sub><b>Colby Banbury</b></sub>](https://github.com/colbybanbury)
- [<img src="https://avatars.githubusercontent.com/zishenwan?v=4?s=80" width="80px;" alt="Zishen Wan"/><br /><sub><b>Zishen Wan</b></sub>](https://github.com/zishenwan)
- [<img src="https://avatars.githubusercontent.com/mmaz?v=4?s=80" width="80px;" alt="Mark Mazumder"/><br /><sub><b>Mark Mazumder</b></sub>](https://github.com/mmaz)
- [<img src="https://avatars.githubusercontent.com/ma3mool?v=4?s=80" width="80px;" alt="Abdulrahman Mahmoud"/><br /><sub><b>Abdulrahman Mahmoud</b></sub>](https://github.com/ma3mool)
- [<img src="https://avatars.githubusercontent.com/DivyaAmirtharaj?v=4?s=80" width="80px;" alt="Divya Amirtharaj"/><br /><sub><b>Divya Amirtharaj</b></sub>](https://github.com/DivyaAmirtharaj)
- [<img src="https://avatars.githubusercontent.com/srivatsankrishnan?v=4?s=80" width="80px;" alt="Srivatsan Krishnan"/><br /><sub><b>Srivatsan Krishnan</b></sub>](https://github.com/srivatsankrishnan)
- [<img src="https://avatars.githubusercontent.com/arnaumarin?v=4?s=80" width="80px;" alt="marin-llobet"/><br /><sub><b>marin-llobet</b></sub>](https://github.com/arnaumarin)
- [<img src="https://avatars.githubusercontent.com/aptl26?v=4?s=80" width="80px;" alt="Aghyad Deeb"/><br /><sub><b>Aghyad Deeb</b></sub>](https://github.com/aptl26)
- [<img src="https://avatars.githubusercontent.com/James-QiuHaoran?v=4?s=80" width="80px;" alt="Haoran Qiu"/><br /><sub><b>Haoran Qiu</b></sub>](https://github.com/James-QiuHaoran)
- [<img src="https://avatars.githubusercontent.com/Ekhao?v=4?s=80" width="80px;" alt="Emil Njor"/><br /><sub><b>Emil Njor</b></sub>](https://github.com/Ekhao)
- [<img src="https://avatars.githubusercontent.com/ELSuitorHarvard?v=4?s=80" width="80px;" alt="ELSuitorHarvard"/><br /><sub><b>ELSuitorHarvard</b></sub>](https://github.com/ELSuitorHarvard)
- [<img src="https://avatars.githubusercontent.com/kaiM0ves?v=4?s=80" width="80px;" alt="kaiM0ves"/><br /><sub><b>kaiM0ves</b></sub>](https://github.com/kaiM0ves)
- [<img src="https://avatars.githubusercontent.com/oishib?v=4?s=80" width="80px;" alt="oishib"/><br /><sub><b>oishib</b></sub>](https://github.com/oishib)
- [<img src="https://avatars.githubusercontent.com/jared-ni?v=4?s=80" width="80px;" alt="Jared Ni"/><br /><sub><b>Jared Ni</b></sub>](https://github.com/jared-ni)
- [<img src="https://avatars.githubusercontent.com/AditiR-42?v=4?s=80" width="80px;" alt="Aditi Raju"/><br /><sub><b>Aditi Raju</b></sub>](https://github.com/AditiR-42)
- [<img src="https://avatars.githubusercontent.com/MichaelSchnebly?v=4?s=80" width="80px;" alt="Michael Schnebly"/><br /><sub><b>Michael Schnebly</b></sub>](https://github.com/MichaelSchnebly)
- [<img src="https://avatars.githubusercontent.com/VThuong99?v=4?s=80" width="80px;" alt="Thuong Duong"/><br /><sub><b>Thuong Duong</b></sub>](https://github.com/VThuong99)
- [<img src="https://avatars.githubusercontent.com/leo47007?v=4?s=80" width="80px;" alt="Yu-Shun Hsiao"/><br /><sub><b>Yu-Shun Hsiao</b></sub>](https://github.com/leo47007)
- [<img src="https://avatars.githubusercontent.com/BaeHenryS?v=4?s=80" width="80px;" alt="Henry Bae"/><br /><sub><b>Henry Bae</b></sub>](https://github.com/BaeHenryS)
- [<img src="https://avatars.githubusercontent.com/eimlav?v=4?s=80" width="80px;" alt="Eimhin Laverty"/><br /><sub><b>Eimhin Laverty</b></sub>](https://github.com/eimlav)
- [<img src="https://avatars.githubusercontent.com/jaywonchung?v=4?s=80" width="80px;" alt="Jae-Won Chung"/><br /><sub><b>Jae-Won Chung</b></sub>](https://github.com/jaywonchung)
- [<img src="https://avatars.githubusercontent.com/ShvetankPrakash?v=4?s=80" width="80px;" alt="Shvetank Prakash"/><br /><sub><b>Shvetank Prakash</b></sub>](https://github.com/ShvetankPrakash)
- [<img src="https://avatars.githubusercontent.com/marcozennaro?v=4?s=80" width="80px;" alt="Marco Zennaro"/><br /><sub><b>Marco Zennaro</b></sub>](https://github.com/marcozennaro)
- [<img src="https://avatars.githubusercontent.com/aryatschand?v=4?s=80" width="80px;" alt="Arya Tschand"/><br /><sub><b>Arya Tschand</b></sub>](https://github.com/aryatschand)
- [<img src="https://avatars.githubusercontent.com/arbass22?v=4?s=80" width="80px;" alt="Andrew Bass"/><br /><sub><b>Andrew Bass</b></sub>](https://github.com/arbass22)
- [<img src="https://avatars.githubusercontent.com/pongtr?v=4?s=80" width="80px;" alt="Pong Trairatvorakul"/><br /><sub><b>Pong Trairatvorakul</b></sub>](https://github.com/pongtr)
- [<img src="https://avatars.githubusercontent.com/euranofshin?v=4?s=80" width="80px;" alt="Eura Nofshin"/><br /><sub><b>Eura Nofshin</b></sub>](https://github.com/euranofshin)
- [<img src="https://www.gravatar.com/avatar/0c931fcfd03cd548d44c90602dd773ba?d=identicon&s=100?v=4?s=80" width="80px;" alt="Matthew Stewart"/><br /><sub><b>Matthew Stewart</b></sub>](https://github.com/harvard-edge/cs249r_book/graphs/contributors)
- [<img src="https://www.gravatar.com/avatar/af39c27c6090c50a1921a9b6366e81cc?d=identicon&s=100?v=4?s=80" width="80px;" alt="Emeka Ezike"/><br /><sub><b>Emeka Ezike</b></sub>](https://github.com/harvard-edge/cs249r_book/graphs/contributors)
- [<img src="https://avatars.githubusercontent.com/jianqingdu?v=4?s=80" width="80px;" alt="jianqingdu"/><br /><sub><b>jianqingdu</b></sub>](https://github.com/jianqingdu)
- [<img src="https://avatars.githubusercontent.com/jzhou1318?v=4?s=80" width="80px;" alt="Jennifer Zhou"/><br /><sub><b>Jennifer Zhou</b></sub>](https://github.com/jzhou1318)
- [<img src="https://avatars.githubusercontent.com/vitasam?v=4?s=80" width="80px;" alt="The Random DIY"/><br /><sub><b>The Random DIY</b></sub>](https://github.com/vitasam)
- [<img src="https://www.gravatar.com/avatar/468ef35acc69f3266efd700992daa369?d=identicon&s=100?v=4?s=80" width="80px;" alt="Fatima Shah"/><br /><sub><b>Fatima Shah</b></sub>](https://github.com/harvard-edge/cs249r_book/graphs/contributors)
- [<img src="https://avatars.githubusercontent.com/BrunoScaglione?v=4?s=80" width="80px;" alt="Bruno Scaglione"/><br /><sub><b>Bruno Scaglione</b></sub>](https://github.com/BrunoScaglione)
- [<img src="https://avatars.githubusercontent.com/Allen-Kuang?v=4?s=80" width="80px;" alt="Allen-Kuang"/><br /><sub><b>Allen-Kuang</b></sub>](https://github.com/Allen-Kuang)
- [<img src="https://www.gravatar.com/avatar/4ad8cdf19eb3b666ace97d3eedb19278?d=identicon&s=100?v=4?s=80" width="80px;" alt="Tess314"/><br /><sub><b>Tess314</b></sub>](https://github.com/harvard-edge/cs249r_book/graphs/contributors)
- [<img src="https://avatars.githubusercontent.com/taunoe?v=4?s=80" width="80px;" alt="Tauno Erik"/><br /><sub><b>Tauno Erik</b></sub>](https://github.com/taunoe)
- [<img src="https://avatars.githubusercontent.com/gnodipac886?v=4?s=80" width="80px;" alt="gnodipac886"/><br /><sub><b>gnodipac886</b></sub>](https://github.com/gnodipac886)
- [<img src="https://avatars.githubusercontent.com/serco425?v=4?s=80" width="80px;" alt="Sercan Ayg√ºn"/><br /><sub><b>Sercan Ayg√ºn</b></sub>](https://github.com/serco425)
- [<img src="https://avatars.githubusercontent.com/TheHiddenLayer?v=4?s=80" width="80px;" alt="TheHiddenLayer"/><br /><sub><b>TheHiddenLayer</b></sub>](https://github.com/TheHiddenLayer)
- [<img src="https://avatars.githubusercontent.com/Gjain234?v=4?s=80" width="80px;" alt="Gauri Jain"/><br /><sub><b>Gauri Jain</b></sub>](https://github.com/Gjain234)
- [<img src="https://avatars.githubusercontent.com/FinAminToastCrunch?v=4?s=80" width="80px;" alt="Fin Amin"/><br /><sub><b>Fin Amin</b></sub>](https://github.com/FinAminToastCrunch)
- [<img src="https://avatars.githubusercontent.com/alex-oesterling?v=4?s=80" width="80px;" alt="Alex Oesterling"/><br /><sub><b>Alex Oesterling</b></sub>](https://github.com/alex-oesterling)
- [<img src="https://avatars.githubusercontent.com/AbenezerKb?v=4?s=80" width="80px;" alt="Abenezer Angamo"/><br /><sub><b>Abenezer Angamo</b></sub>](https://github.com/AbenezerKb)
- [<img src="https://avatars.githubusercontent.com/BravoBaldo?v=4?s=80" width="80px;" alt="Baldassarre Cesarano"/><br /><sub><b>Baldassarre Cesarano</b></sub>](https://github.com/BravoBaldo)
- [<img src="https://avatars.githubusercontent.com/Jahnic-kb?v=4?s=80" width="80px;" alt="Jahnic Beck"/><br /><sub><b>Jahnic Beck</b></sub>](https://github.com/Jahnic-kb)
- [<img src="https://avatars.githubusercontent.com/aethernavshulkraven-allain?v=4?s=80" width="80px;" alt="‡§Ö‡§∞‡§®‡§µ ‡§∂‡•Å‡§ï‡•ç‡§≤‡§æ | Arnav Shukla"/><br /><sub><b>‡§Ö‡§∞‡§®‡§µ ‡§∂‡•Å‡§ï‡•ç‡§≤‡§æ | Arnav Shukla</b></sub>](https://github.com/aethernavshulkraven-allain)
- [<img src="https://avatars.githubusercontent.com/RinZ27?v=4?s=80" width="80px;" alt="Rin"/><br /><sub><b>Rin</b></sub>](https://github.com/RinZ27)
- [<img src="https://avatars.githubusercontent.com/bilgeacun?v=4?s=80" width="80px;" alt="Bilge Acun"/><br /><sub><b>Bilge Acun</b></sub>](https://github.com/bilgeacun)
- [<img src="https://avatars.githubusercontent.com/atcheng2?v=4?s=80" width="80px;" alt="Andy Cheng"/><br /><sub><b>Andy Cheng</b></sub>](https://github.com/atcheng2)
- [<img src="https://avatars.githubusercontent.com/arighosh05?v=4?s=80" width="80px;" alt="Aritra Ghosh"/><br /><sub><b>Aritra Ghosh</b></sub>](https://github.com/arighosh05)
- [<img src="https://avatars.githubusercontent.com/abigailswallow?v=4?s=80" width="80px;" alt="abigailswallow"/><br /><sub><b>abigailswallow</b></sub>](https://github.com/abigailswallow)
- [<img src="https://avatars.githubusercontent.com/YangZhou1997?v=4?s=80" width="80px;" alt="Yang Zhou"/><br /><sub><b>Yang Zhou</b></sub>](https://github.com/YangZhou1997)
- [<img src="https://avatars.githubusercontent.com/XaicuL?v=4?s=80" width="80px;" alt="JEON HYUNJUN(Luciano)"/><br /><sub><b>JEON HYUNJUN(Luciano)</b></sub>](https://github.com/XaicuL)
- [<img src="https://avatars.githubusercontent.com/emmanuel2406?v=4?s=80" width="80px;" alt="Emmanuel Rassou"/><br /><sub><b>Emmanuel Rassou</b></sub>](https://github.com/emmanuel2406)
- [<img src="https://avatars.githubusercontent.com/jasonlyik?v=4?s=80" width="80px;" alt="Jason Yik"/><br /><sub><b>Jason Yik</b></sub>](https://github.com/jasonlyik)
- [<img src="https://avatars.githubusercontent.com/jessicaquaye?v=4?s=80" width="80px;" alt="Jessica Quaye"/><br /><sub><b>Jessica Quaye</b></sub>](https://github.com/jessicaquaye)
- [<img src="https://avatars.githubusercontent.com/cursoragent?v=4?s=80" width="80px;" alt="Cursor Agent"/><br /><sub><b>Cursor Agent</b></sub>](https://github.com/cursoragent)
- [<img src="https://avatars.githubusercontent.com/happyappledog?v=4?s=80" width="80px;" alt="happyappledog"/><br /><sub><b>happyappledog</b></sub>](https://github.com/happyappledog)

--- book/cli/README.md ---
# MLSysBook CLI v2.0 - Modular Architecture

This directory contains the refactored, modular CLI for the MLSysBook project. The CLI has been broken down from a monolithic 4000+ line script into maintainable, testable modules.

## Architecture

```
cli/
‚îú‚îÄ‚îÄ __init__.py           # Package initialization
‚îú‚îÄ‚îÄ main.py              # Main CLI application class
‚îú‚îÄ‚îÄ core/                # Core functionality
‚îÇ   ‚îú‚îÄ‚îÄ config.py        # Configuration management
‚îÇ   ‚îî‚îÄ‚îÄ discovery.py     # Chapter/file discovery
‚îú‚îÄ‚îÄ commands/            # Command implementations
‚îÇ   ‚îî‚îÄ‚îÄ build.py         # Build operations
‚îú‚îÄ‚îÄ formats/             # Format-specific handlers (future)
‚îî‚îÄ‚îÄ utils/               # Shared utilities (future)
```

## Key Components

### ConfigManager (`core/config.py`)
- Manages Quarto configuration files for HTML, PDF, and EPUB
- Handles symlink creation and switching between formats
- Provides output directory detection from config files

### ChapterDiscovery (`core/discovery.py`)
- Discovers and validates chapter files
- Supports fuzzy matching for chapter names
- Provides chapter listing and dependency detection

### BuildCommand (`commands/build.py`)
- Handles build operations for all formats
- Supports both full book and individual chapter builds
- Includes progress indication and error handling

### MLSysBookCLI (`main.py`)
- Main application class that orchestrates all components
- Provides command routing and help system
- Beautiful Rich-based UI with organized command tables

## Usage

The modular CLI has replaced the original binder and is available as `./binder` in the project root:

```bash
# Show help
./binder help

# Build commands
./binder build                    # Build full book (HTML)
./binder build intro,ml_systems   # Build specific chapters (HTML)
./binder pdf intro                # Build chapter as PDF
./binder epub                     # Build full book as EPUB

# Preview commands
./binder preview                  # Start live dev server for full book
./binder preview intro            # Start live dev server for chapter

# Management
./binder list                     # List all chapters
./binder status                   # Show current status
./binder doctor                   # Run comprehensive health check
```

## Benefits of Modular Architecture

1. **Maintainability**: Each component has a single responsibility
2. **Testability**: Individual modules can be unit tested
3. **Debuggability**: Issues can be isolated to specific modules
4. **Extensibility**: New formats and commands are easy to add
5. **Code Reuse**: Shared functionality is properly modularized
6. **Collaboration**: Multiple developers can work on different components

## Migration Complete

The modular CLI has successfully replaced the original monolithic binder script:

- **`./binder`** - Modular CLI (4000+ lines ‚Üí organized modules)

All functionality has been preserved and enhanced:
- All existing commands work the same way
- Same configuration files and output directories
- Same build processes and quality
- Enhanced error handling and progress indication
- New preview and doctor commands added

## Future Enhancements

The modular architecture enables easy addition of:
- Format-specific handlers in `formats/`
- Additional commands in `commands/`
- Shared utilities in `utils/`
- Plugin system for custom extensions
- Comprehensive unit test suite


--- book/socratiQ/README.md ---
# SocratiQ AI Learning Companion

** NOTE: SocratiQ will be open sourced soon

Welcome to **SocratiQ** (pronounced "Socratic"), an AI learning assistant seamlessly integrated throughout this resource. Inspired by the Socratic method of teaching‚Äîemphasizing thoughtful questions and answers to stimulate critical thinking‚ÄîSocratiQ is part of our experiment with what we call **Generative Learning**. By combining interactive quizzes, personalized assistance, and real-time feedback, SocratiQ is meant to reinforce your understanding and help you create new connections.

## About This Repository

**Important Note:** This repository contains the SocratiQ integration for this specific book/resource.

- **Full Widget Code:** The complete SocratiQ widget codebase will be open-sourced in its own dedicated GitHub repository in the future.
- **Book-Specific Version:** The source code version specifically used for this book will be placed in this directory in the future.
- **Stay Tuned:** Please check back for updates as we continue to develop and release the codebase.

## What is SocratiQ?

SocratiQ is an AI-powered learning companion designed to provide an interactive, personalized learning experience. Unlike traditional textbook study, SocratiQ offers:

- **Interactive Quizzes:** AI-generated quizzes tailored to reinforce key concepts
- **Personalized Assistance:** Context-aware explanations and help with challenging concepts
- **Real-time Feedback:** Immediate responses and detailed explanations
- **Progress Tracking:** Comprehensive dashboard with statistics, streaks, and achievement badges
- **Adaptive Learning:** Content tailored to your academic level

## Features

### Quick Access
- **Keyboard Shortcut:** Press `Cmd/Ctrl + /` to open SocratiQ anytime
- **URL Parameters:** Control SocratiQ via URL:
  - Activate: `?socratiq=true`
  - Deactivate: `?socratiq=false`

### Learning Features

#### 1. **AI-Generated Quizzes**
- Automatically generated at the end of major subsections
- Typically 3-5 multiple-choice questions per quiz
- Takes 1-2 minutes to complete
- Immediate feedback with detailed explanations

#### 2. **Concept Assistance**
- Select any text from the textbook and ask for explanations
- Reference sections, subsections, and keywords using `@` symbol
- Get suggestions for related content from the textbook
- Adjust difficulty level of AI responses

#### 3. **Progress Tracking**
- Performance dashboard with quiz statistics
- Learning streaks tracking
- Achievement badges system:
  - üéØ **First Steps:** Complete your first quiz
  - üî¢ **On a Streak:** Maintain a streak of perfect scores
  - üèÜ **Quiz Medalist:** Complete 10 quizzes
  - üèÜüèÜ **Quiz Champion:** Complete 20 quizzes
  - üèÜüèÜüèÜ **Quiz Legend:** Complete 30 quizzes
  - üèÜüèÜüèÜüèÜ x n **Quiz AGI Super Human:** Complete 40+ quizzes
- PDF report generation with unique hash validation

#### 4. **Personalization**
- Set your academic level in Settings
- Customize AI response difficulty
- Save and load conversations

## Technical Requirements

To use SocratiQ effectively, you'll need:

- Chrome or Safari browser
- JavaScript enabled
- Stable internet connection

## Data Storage

**Important:** All progress data is stored locally in your browser. Clearing your browser history or cache will erase your entire learning history, including quiz scores, streaks, and achievement badges.

## Common Issues and Troubleshooting

- **SocratiQ isn't responding:** Refresh the page
- **Quizzes don't load:** Check your internet connection
- **Progress isn't saving:** Ensure cookies are enabled

For persistent issues, please contact: vj[@]eecs.harvard.edu

## Providing Feedback

Your feedback helps us improve SocratiQ. You can:

- Report technical issues
- Suggest improvements to quiz questions
- Share thoughts about AI responses
- Submit a GitHub issue

## Research and Resources

- **Research Paper:** [SocratiQ: A Generative AI-Powered Learning Companion for Personalized Education and Broader Accessibility](https://arxiv.org/abs/2502.00341)
- **AI-Generated Podcast:** Listen to our podcast about SocratiQ

## Warning

**About AI Responses:** While SocratiQ uses advanced AI to generate quizzes and provide assistance, like all AI systems, it may occasionally provide imperfect or incomplete answers. However, we've designed and tested it to ensure it's effective for supporting your learning journey. If you're unsure about any response, refer to the textbook content or consult your instructor.

## Status

SocratiQ is still a work in progress, and we welcome your feedback. This is an experimental feature exploring dynamic and personalized learning experiences through generative AI.

---

**Note:** This is an experimental feature. We are experimenting with the idea of creating a dynamic and personalized learning experience by harnessing the power of generative AI. We hope that this approach will transform how you interact with and absorb complex concepts.


## Links discovered
- [SocratiQ: A Generative AI-Powered Learning Companion for Personalized Education and Broader Accessibility](https://arxiv.org/abs/2502.00341)

--- book/docker/linux/README.md ---
# Quarto Build Container

This directory contains the Docker container configuration for the MLSysBook build system.

## Purpose

The container pre-installs all dependencies to eliminate the 30-45 minute setup time for Linux builds, reducing build times from 45 minutes to 5-10 minutes.

## Structure

```
docker/quarto-build/
‚îú‚îÄ‚îÄ Dockerfile          # Container definition
‚îú‚îÄ‚îÄ README.md          # This file
‚îî‚îÄ‚îÄ .dockerignore      # Files to exclude from build
```

## Container Contents

- **Base**: Ubuntu 22.04
- **TeX Live**: Full distribution (texlive-full)
- **R**: R-base with all required packages
- **Python**: Python 3.13 with all requirements
- **Quarto**: Version 1.7.31
- **Tools**: Inkscape, Ghostscript, fonts
- **Dependencies**: All from `tools/dependencies/`

## Build Process

The container is built and tested via GitHub Actions:

```bash
# Trigger container build
gh workflow run build-container.yml
```

## Usage

The container is used in the containerized build workflow:

```yaml
container:
  image: ghcr.io/harvard-edge/cs249r_book/quarto-build:latest
  options: --user root
```

## Testing

The container build includes 17 comprehensive tests:

1. Quarto functionality
2. Python packages (all from requirements.txt)
3. R packages (all from install_packages.R)
4. TeX Live and LaTeX engines
5. Inkscape SVG to PDF conversion
6. Ghostscript PDF compression
7. Fonts and graphics libraries
8. Quarto render test
9. TikZ compilation test
10. System resources check
11. Network connectivity
12. Book structure compatibility
13. Quarto configuration files
14. Dependencies files accessibility
15. Quarto check (same as workflow)
16. Actual build process simulation
17. Memory and disk space verification

## Registry

- **Registry**: GitHub Container Registry (ghcr.io)
- **Image**: `ghcr.io/harvard-edge/cs249r_book/quarto-build`
- **Tags**: `latest`, `main`, `dev`, branch-specific tags
- **Size**: ~2-3GB (includes TeX Live, R, Python packages)

## Performance

The container reduces build times significantly:
- **Traditional Linux build**: 45 minutes (including dependency installation)
- **Containerized build**: 5-10 minutes (dependencies pre-installed)
- **Container size**: ~2-3GB (optimized with multi-layer cleanup)
- **Build phases**: 11 optimized phases with progress tracking

## Recent Improvements (2025)

- Fixed dependency path issues after repository restructuring
- Improved error handling and progress tracking
- Optimized TeX Live package installation
- Enhanced cleanup procedures for smaller image size
- Added comprehensive testing (17 test scenarios)
- Fixed PATH environment variables for all tools

## Build Phases

1. **System Dependencies** - Core Ubuntu packages and libraries
2. **Inkscape Installation** - SVG to PDF conversion capability
3. **Quarto Installation** - Latest Quarto CLI (v1.7.31)
4. **TeX Live Installation** - Complete LaTeX distribution
5. **Ghostscript Installation** - PDF processing capabilities
6. **R Installation** - R base and development packages
7. **Python Installation** - Python 3 with pip
8. **Python Packages** - All production requirements
9. **R Packages** - All required R libraries
10. **R Package Verification** - Validation of successful installation
11. **Comprehensive Cleanup** - Size optimization and cache clearing

- **Traditional build**: 45 minutes
- **Containerized build**: 5-10 minutes
- **Improvement**: 80-90% time reduction


--- book/docker/windows/README.md ---
# Windows Quarto Build Container

This directory contains the Windows Server 2022 container configuration for building the MLSysBook with Quarto.

## üê≥ Container Features

- **Base Image**: Windows Server 2022 LTSC
- **PowerShell**: 7.4.1 (ZIP install, container-safe)
- **Quarto**: 1.7.31 (ZIP install)
- **Python**: 3.13.1 + production dependencies
- **TeX Live**: 2025 snapshot with required packages
- **R**: 4.3.2 + R Markdown packages
- **Graphics**: Ghostscript + Inkscape (via Chocolatey)

## üîß Key Fixes Applied

### 1. PowerShell 7 Path Issues
- **Problem**: Using `pwsh` shorthand can fail in containers
- **Fix**: Use full path `C:\Program Files\PowerShell\7\pwsh.exe`

### 2. TeX Live Installation
- **Problem**: `Start-Process` without `-NoNewWindow` can hang
- **Fix**: Added `-NoNewWindow` flag for container compatibility
- **Problem**: Comments in `tl_packages` file
- **Fix**: Filter out comment lines when installing packages

### 3. TikZ Test Document
- **Problem**: Complex here-string with backticks
- **Fix**: Simplified to standard multi-line string

### 4. Package Installation
- **Problem**: Silent failures in package installation
- **Fix**: Added verbose output and better error handling

## üöÄ Building the Container

### Prerequisites
- Windows Docker Desktop or Windows Server with Docker
- At least 8GB RAM available for Docker
- 20GB+ free disk space

## Local Build
To build the Windows container locally, run the following command from the repository root:
```powershell
docker build -f docker/windows/Dockerfile -t mlsysbook-windows .
```

### Testing
To test the Dockerfile before building, you can use the provided PowerShell script:
```powershell
./docker/windows/test_dockerfile.ps1
```

## Workflow
The container is built and pushed to the GitHub Container Registry via the [`.github/workflows/build-windows-container.yml`](../../.github/workflows/build-windows-container.yml) workflow.
This workflow is triggered manually or on a weekly schedule.

## Notes
- Building the Windows container can take a significant amount of time (often over 2 hours).
- The image is large due to the comprehensive set of pre-installed dependencies.

## üìã Build Phases

1. **Base Setup**: Directories, environment variables
2. **PowerShell 7**: ZIP installation (container-safe)
3. **Chocolatey**: Package manager installation
4. **Dependencies**: Copy required files
5. **Quarto**: ZIP installation with PATH setup
6. **Python**: 3.13.1 + production requirements
7. **Graphics**: Ghostscript + Inkscape
8. **TeX Live**: 2025 snapshot + packages
9. **R**: 4.3.2 + R Markdown packages
10. **Cleanup**: Remove temporary files

## üîç Verification Steps

The container includes comprehensive verification:

- **PowerShell 7**: Version check
- **Quarto**: Version and command availability
- **Python**: Version and pip functionality
- **TeX Live**: Package verification with `kpsewhich`
- **Fonts**: Helvetica font files verification
- **TikZ**: Smoke test with PDF generation
- **R**: Package installation verification

## ‚ö†Ô∏è Common Issues & Solutions

### 1. Build Timeouts
- **Cause**: Large downloads (TeX Live, Python packages)
- **Solution**: Increased timeout values in Dockerfile

### 2. PATH Issues
- **Cause**: Windows PATH not properly updated
- **Solution**: Explicit PATH manipulation with regex escaping

### 3. Package Installation Failures
- **Cause**: Network issues or missing dependencies
- **Solution**: Added verbose output and error checking

### 4. Memory Issues
- **Cause**: TeX Live installation requires significant memory
- **Solution**: Use `scheme-infraonly` for minimal installation

## üß™ Testing

### Run Container
```powershell
docker run -it mlsysbook-windows pwsh
```

### Test Quarto
```powershell
quarto --version
quarto check
```

### Test Python
```powershell
python --version
python -c "import nltk; print('NLTK available')"
```

### Test R
```powershell
R --version
Rscript -e "library(rmarkdown); print('R Markdown available')"
```

### Test TeX Live
```powershell
lualatex --version
kpsewhich pgf.sty
```

## üìä Performance Notes

- **Build Time**: ~45-60 minutes on typical hardware
- **Image Size**: ~8-12GB (includes TeX Live, R, Python)
- **Memory Usage**: 4-6GB during build, 2-3GB runtime
- **Disk Space**: 15-20GB for build cache

## üîß Troubleshooting

### Build Fails on TeX Live
```powershell
# Check available memory
docker system df
docker system prune -f
```

### PowerShell Issues
```powershell
# Verify PowerShell 7 installation
docker run mlsysbook-windows pwsh -Command "Get-Host"
```

### Package Installation Issues
```powershell
# Check Chocolatey installation
docker run mlsysbook-windows choco --version
```

## üìù Maintenance

### Updating Dependencies
1. Update version numbers in Dockerfile
2. Test with validation script
3. Rebuild and verify all components

### Adding New Packages
1. Add to appropriate phase in Dockerfile
2. Update verification steps
3. Test thoroughly

### Security Updates
- Regularly update base image
- Monitor for CVE reports
- Update package versions as needed


## Links discovered
- [`.github/workflows/build-windows-container.yml`](https://github.com/harvard-edge/cs249r_book/blob/dev/book/.github/workflows/build-windows-container.yml)

--- book/quarto/_extensions/README.md ---
# MLSysBook Extensions

This directory contains Quarto extensions used by the MLSysBook project.

## ‚ö†Ô∏è CRITICAL WARNING - READ BEFORE MAKING CHANGES

**Some extensions in this directory are HEAVILY CUSTOMIZED and should NEVER be reinstalled using `quarto add`.**

### üîç Quick Safety Check

Before installing or updating any extension, **ALWAYS** check for:
1. `.CUSTOM_LOCK` files in extension directories
2. Version numbers ending in `-mlsysbook-custom`
3. Warnings in extension `_extension.yml` files

### üìã Extension Inventory

| Extension | Status | Safe to Update? |
|-----------|--------|-----------------|
| `margin-video` | üö´ **100% Custom** | Never (MLSysBook only) |
| `ute/custom-numbered-blocks` | üö´ **Heavy Customization** | Never |
| `nmfs-opensci/titlepage` | üö´ **Customized** | Never |
| `pandoc-ext/diagram` | ‚úÖ Standard | Yes |
| `quarto-ext/lightbox` | ‚úÖ Standard | Yes |

### üö® Emergency Recovery

If you accidentally overwrote a custom extension:
```bash
# Check what changed
git status

# Restore from git if needed
git checkout HEAD -- book/_extensions/EXTENSION_NAME/

# Run protection check
python tools/scripts/check_custom_extensions.py
```

### üìö Full Documentation

For complete details, see: [`CUSTOM_EXTENSIONS.md`](./CUSTOM_EXTENSIONS.md)

---
**When in doubt, DON'T reinstall - ask the team first!**


## Links discovered
- [`CUSTOM_EXTENSIONS.md`](https://github.com/harvard-edge/cs249r_book/blob/dev/book/quarto/_extensions/CUSTOM_EXTENSIONS.md)

--- book/quarto/scripts/README.md ---
# Post-Render Scripts

This directory contains scripts that run after Quarto builds to fix various issues.

## fix_cross_references.py

### Problem It Solves

When using **selective rendering** to speed up builds (only building index + introduction instead of all 20+ chapters), Quarto cannot resolve cross-references to chapters that weren't built. This results in broken references appearing as `?@sec-chapter-name` in the HTML output.

This particularly affects:
- **Glossary**: Contains 800+ cross-references to all chapters (hence the original script name)
- **Introduction**: References many other chapters for context
- **Any chapter** that references other chapters

### How It Works

1. **Runs automatically** as a post-render hook after Quarto finishes building
2. **Scans ALL HTML files** in the `_build/html/` directory
3. **Finds unresolved references** that appear as `<strong>?@sec-xxx</strong>`
4. **Converts them to proper links**: `<strong><a href="../path/to/chapter.html#sec-xxx">Chapter Title</a></strong>`

### Configuration

The script is configured as a post-render hook in the Quarto configuration files:

```yaml
# In config/_quarto-html.yml
project:
  post-render:
    - scripts/clean_svgs.py
    - scripts/fix_cross_references.py  # Fixes cross-references
```

### Maintenance

When adding new chapters to the book:

1. Add the chapter's section ID to `CHAPTER_MAPPING` dictionary
2. Add the chapter's display title to `CHAPTER_TITLES` dictionary
3. Ensure the section ID matches what's in the `.qmd` file (e.g., `{#sec-new-chapter}`)

Example:
```python
CHAPTER_MAPPING = {
    # ... existing chapters ...
    "sec-new-chapter": "../core/new_chapter/new_chapter.html#sec-new-chapter",
}

CHAPTER_TITLES = {
    # ... existing chapters ...
    "sec-new-chapter": "New Chapter Title",
}
```

### Manual Testing

```bash
# Test on all HTML files
python3 scripts/fix_cross_references.py

# Test on specific file
python3 scripts/fix_cross_references.py _build/html/contents/backmatter/glossary/glossary.html
```

### Output

The script provides clear output showing what it fixed:

```
üîó [Cross-Reference Fix] Scanning 3 HTML files...
‚úÖ Fixed 850 cross-references in 2 files:
   üìÑ contents/backmatter/glossary/glossary.html: 810 refs
   üìÑ contents/core/introduction/introduction.html: 40 refs
```

## clean_svgs.py

Cleans up SVG files generated during the build process.

---

## Why These Scripts Exist

These post-render scripts enable **fast iterative development** by allowing selective chapter builds while maintaining a fully functional website with working cross-references. Without them, developers would need to build all 20+ chapters every time (taking minutes) just to test changes to a single chapter.


--- book/tools/dependencies/README.md ---
# Python Dependencies

This directory contains Python dependencies organized in a clean, modular structure.

## Structure

```
requirements/
‚îú‚îÄ‚îÄ base.txt          # Core dependencies (pandas, requests, etc.)
‚îú‚îÄ‚îÄ production.txt    # Production-specific (includes base.txt)
‚îî‚îÄ‚îÄ development.txt   # Full ML stack (includes production.txt)
```

## Files

### `requirements/base.txt` (Core Dependencies)
- Essential packages needed by all environments
- Jupyter, pandas, requests, PyYAML, etc.
- Lightweight foundation (~200MB)

### `requirements/production.txt` (Container/Build)
- Includes `base.txt` + production-specific packages
- OpenAI API, Groq, pre-commit, Ghostscript, etc.
- Excludes heavy ML libraries
- Optimized for containers (~500MB)

### `requirements/development.txt` (Full ML Stack)
- Includes `production.txt` + ML dependencies
- PyTorch, sentence-transformers, FAISS, scikit-learn
- Required for cross-reference generation
- Complete development environment (~4GB)

## Convenience Files

### `requirements.txt` (Default - Full)
```bash
pip install -r requirements.txt
```
References `requirements/development.txt` for complete functionality

### `requirements-build.txt` (Container Optimized)
```bash
pip install -r requirements-build.txt
```
References `requirements/production.txt` for lightweight builds

### `requirements-dev.txt` (Development Alias)
```bash
pip install -r requirements-dev.txt
```
Alias for `requirements/development.txt`

## Usage Patterns

### Container Builds (GitHub Actions)
```yaml
RUN pip install -r tools/dependencies/requirements-build.txt
```
**Result**: Fast builds, small containers (~500MB Python deps)

### Local Development
```bash
# For full development (recommended)
pip install -r tools/dependencies/requirements.txt

# For container-optimized builds only
pip install -r tools/dependencies/requirements-build.txt
```

### Cross-Reference Generation
```bash
# Use full dependencies for ML tools
pip install -r tools/dependencies/requirements.txt
python tools/scripts/cross_refs/cross_refs.py
```

## Migration Notes

- **`requirements.txt`**: Full dependencies (unchanged for backward compatibility)
- **`requirements-build.txt`**: New minimal set for containers (~500MB vs ~4GB)
- **Containers should switch** to `requirements-build.txt` for 3-4GB savings
- **Local development**: Continue using `requirements.txt`


--- book/tools/scripts/README.md ---
# MLSysBook Scripts Directory

This directory contains all automation scripts, tools, and utilities for the Machine Learning Systems book project. The scripts are organized into logical categories for easy discovery and maintenance.

## üìÅ Directory Structure

```
tools/scripts/
‚îú‚îÄ‚îÄ build/           # Build and development scripts
‚îú‚îÄ‚îÄ content/         # Content management and editing tools
‚îú‚îÄ‚îÄ maintenance/     # System maintenance and updates
‚îú‚îÄ‚îÄ testing/         # Test scripts and validation
‚îú‚îÄ‚îÄ utilities/       # General utility scripts
‚îú‚îÄ‚îÄ docs/            # Documentation for scripts
‚îú‚îÄ‚îÄ genai/           # AI and generation tools
‚îú‚îÄ‚îÄ cross_refs/      # Cross-reference management
‚îú‚îÄ‚îÄ publish/  # Publishing and deployment
‚îî‚îÄ‚îÄ ai_menu/         # AI menu and interface tools
```

## üî® Build Scripts (`build/`)

Scripts for building, cleaning, and development workflows:

- **`clean.sh`** - Comprehensive cleanup script (build artifacts, caches, temp files)
- **`standardize_sources.sh`** - Standardize source file formatting
- **`generate_stats.py`** - Generate statistics about the Quarto project

### Usage Examples
```bash
# Clean all build artifacts
./build/clean.sh

# Deep clean including caches and virtual environments
./build/clean.sh --deep

# Generate project statistics
python build/generate_stats.py
```

## üìù Content Management (`content/`)

Tools for managing, editing, and validating book content:

- **`improve_figure_captions.py`** - Enhance figure captions using AI
- **`manage_section_ids.py`** - Manage section IDs and cross-references
- **`find_unreferenced_labels.py`** - Find unused labels and references
- **`find_duplicate_labels.py`** - Detect duplicate labels
- **`extract_headers.py`** - Extract headers from content files
- **`find_acronyms.py`** - Find and manage acronyms
- **`find_fig_references.py`** - Analyze figure references
- **`fix_bibliography.py`** - Fix bibliography formatting
- **`sync_bibliographies.py`** - Synchronize bibliography files
- **`clean_callout_titles.py`** - Clean callout title formatting
- **`collapse_blank_lines.py`** - Remove excessive blank lines

### Usage Examples
```bash
# Improve figure captions
python content/improve_figure_captions.py

# Find unreferenced labels
python content/find_unreferenced_labels.py

# Manage section IDs
python content/manage_section_ids.py
```

## üîß Maintenance Scripts (`maintenance/`)

System maintenance, updates, and changelog management:

- **`generate_release_content.py`** - Generate changelog entries and release notes
- **`fix_changelog.py`** - Fix changelog formatting issues
- **`update_texlive_packages.py`** - Update LaTeX package dependencies
- **`cleanup_old_runs.sh`** - Clean up old build runs

### Usage Examples
```bash
# Generate changelog entry or release notes
python maintenance/generate_release_content.py

# Update LaTeX packages
python maintenance/update_texlive_packages.py
```

## üß™ Testing Scripts (`testing/`)

Test scripts and validation tools:

- **`run_tests.py`** - Run comprehensive test suite
- **`test_section_ids.py`** - Test section ID management

### Usage Examples
```bash
# Run all tests
python testing/run_tests.py

# Test section ID system
python testing/test_section_ids.py
```

## üõ†Ô∏è Utilities (`utilities/`)

General-purpose utility scripts:

- **`check_ascii.py`** - Check for non-ASCII characters
- **`check_images.py`** - Validate image files and references
- **`check_sources.py`** - Comprehensive source file validation
- **`fix_titles.py`** - Fix title formatting
- **`count_footnotes.sh`** - Count footnotes
- **`analyze_footnotes.sh`** - Detailed footnote analysis

### Usage Examples
```bash
# Check for non-ASCII characters
python utilities/check_ascii.py

# Validate images
python utilities/check_images.py

# Check source files
python utilities/check_sources.py
```

## üìñ Documentation (`docs/`)

Documentation for scripts and systems:

- **`README.md`** - General scripts documentation
- **`SECTION_ID_SYSTEM.md`** - Section ID management system guide
- **`FIGURE_CAPTIONS.md`** - Figure caption enhancement guide

## ü§ñ Specialized Tools

### AI and Generation (`genai/`)
Tools for AI-powered content generation and enhancement.

### Cross-References (`cross_refs/`)
Advanced cross-reference management and validation tools.

### Publishing (`publish/`)
Scripts for publishing and deployment workflows. **Note**: The main publishing workflow is now handled by `./binder publish`.

### AI Menu (`ai_menu/`)
AI-powered menu and interface tools.

## üöÄ Quick Start

### First Time Setup
```bash
# Make all scripts executable
find tools/scripts -name "*.sh" -exec chmod +x {} \;

# Install Python dependencies (if needed)
pip install -r tools/dependencies/requirements.txt
```

### Common Workflows

#### Before Working on Content
```bash
# Clean workspace
./build/clean.sh

# Check project health
python utilities/check_sources.py
```

#### Content Editing Session
```bash
# Improve figures
python content/improve_figure_captions.py

# Find issues
python content/find_unreferenced_labels.py
python content/find_duplicate_labels.py

# Clean up formatting
python content/collapse_blank_lines.py
```

#### Before Publishing
```bash
# Full validation
python testing/run_tests.py
python utilities/check_images.py
python utilities/ascii_checker.py

# Update changelog
python maintenance/update_changelog.py

# Final cleanup
./build/clean.sh

# Publish (using binder)
./binder publish
```

## üìã Script Categories Summary

| Category | Purpose | Count | Key Scripts |
|----------|---------|-------|-------------|
| **build** | Development & building | 3 | `clean.sh`, `generate_stats.py` |
| **content** | Content management | 11 | `manage_section_ids.py`, `improve_figure_captions.py` |
| **maintenance** | System maintenance | 4 | `generate_release_content.py`, `update_texlive_packages.py` |
| **testing** | Testing & validation | 2 | `run_tests.py`, `test_section_ids.py` |
| **utilities** | General utilities | 6 | `check_sources.py`, `check_ascii.py` |
| **docs** | Documentation | 3 | Various `.md` files |

## üîç Finding the Right Script

### By Purpose
- **Need to clean up?** ‚Üí `build/clean.sh`
- **Content has issues?** ‚Üí `utilities/check_sources.py`
- **Figures need improvement?** ‚Üí `content/improve_figure_captions.py`
- **Want project stats?** ‚Üí `build/generate_stats.py`
- **Need to test changes?** ‚Üí `testing/run_tests.py`

### By File Type
- **`.sh` scripts** - Shell scripts (mostly in `build/` and `utilities/`)
- **`.py` scripts** - Python scripts (distributed across categories)
- **`.md` files** - Documentation (in `docs/`)

## ü§ù Contributing New Scripts

When adding new scripts:

1. **Choose the right category** based on the script's primary purpose
2. **Follow naming conventions** - descriptive, lowercase with underscores
3. **Add documentation** - Include usage examples and descriptions
4. **Update this README** - Add the script to the appropriate section
5. **Make executable** - `chmod +x` for shell scripts
6. **Test thoroughly** - Ensure scripts work in different environments

## üìû Support

For issues with specific scripts:
1. Check the script's docstring or comments
2. Look for documentation in the `docs/` directory
3. Run scripts with `--help` flag if available
4. Review this README for context and examples


--- book/tools/scripts/README_TABLE_FORMATTER.md ---
# Table Formatter

Automatically formats markdown grid tables in the MLSysBook with proper bolding and alignment.

## Features

### 1. Smart Bolding
- **Always bolds headers** (first row)
- **Intelligently bolds first column** based on table type:
  - ‚úÖ Comparison tables (Criterion, Aspect, Feature, etc.)
  - ‚úÖ Trade-off tables (detected by caption keywords)
  - ‚úÖ Definition tables (descriptive multi-word entries)
  - ‚ùå Data tables (Year, ID, numeric values)
  - ‚ùå Simple identifier columns (#, Index, etc.)

### 2. Automatic Width Calculation
- Calculates optimal column widths based on content
- Accounts for Unicode characters (arrows, symbols)
- Accounts for bold markers in width calculations
- Adjusts alignment bars to match content

### 3. Handles Edge Cases
- Empty cells (doesn't add bold markers to empty strings)
- Multi-row cells (spanning rows)
- Already bolded content (doesn't double-bold)
- Tables with mixed content types

## Usage

### Check a single file
```bash
python tools/scripts/format_tables.py --check quarto/contents/core/optimizations/optimizations.qmd
```

### Fix a single file
```bash
python tools/scripts/format_tables.py --fix quarto/contents/core/optimizations/optimizations.qmd
```

### Check all files
```bash
python tools/scripts/format_tables.py --check-all
```

### Fix all files
```bash
python tools/scripts/format_tables.py --fix-all
```

### Verbose mode (see detection decisions)
```bash
python tools/scripts/format_tables.py --check <file> --verbose
```

## Detection Heuristics

The script uses multiple strategies to determine if the first column should be bolded:

### Caption Analysis
Keywords in table caption trigger first column bolding:
- "comparison", "trade-off", "overview", "summary"
- "criteria", "characteristics", "features", "differences"

### Header Analysis
First column header names that trigger bolding:
- "criterion", "aspect", "feature", "technique"
- "method", "approach", "strategy", "type"
- "challenge", "principle", "component"

First column header names that prevent bolding:
- "id", "#", "number", "index"
- "year", "date", "time", "rank"

### Content Analysis
- **Numeric content** (>70%): DON'T bold (data table)
- **Descriptive phrases** (>50% multi-word): Bold (comparison table)
- **Questions** (contains "?"): Bold (FAQ style)

## Examples

### Comparison Table (First Column Bolded)
```markdown
+-----------+----------+----------+
| **Criterion** | **Method A** | **Method B** |
+:=========+:========:+:========:+
| **Accuracy**  |   High   |  Medium  |
+-----------+----------+----------+
```

### Data Table (First Column NOT Bolded)
```markdown
+------+---------+--------+
| **Year** | **Revenue** | **Growth** |
+:====:+:=======:+:======:+
| 2020 |  100M   |  10%   |
+------+---------+--------+
```

## Pre-commit Integration

To add this to pre-commit hooks, add to `.pre-commit-config.yaml`:

```yaml
- repo: local
  hooks:
    - id: format-tables
      name: "Format markdown tables"
      entry: python tools/scripts/format_tables.py --check-all
      language: python
      pass_filenames: false
      files: ^quarto/contents/.*\.qmd$
```

## Testing

Run the test suite:
```bash
python tools/scripts/test_format_tables.py
```

Tests cover:
- Display width calculation (including Unicode)
- Bold text handling
- Row parsing
- Alignment extraction
- Border and separator building
- Cell formatting
- Empty cells
- Already formatted tables

## Design Rationale

### Why Bold Headers?
- Universal textbook standard
- Improves scannability
- Clear visual hierarchy

### Why Selective First Column Bolding?
- **Comparison tables**: First column is categorical labels (should stand out)
- **Data tables**: First column is just data (shouldn't dominate visually)
- **Context matters**: Detection uses caption, headers, and content to decide

### Why Auto-adjust Widths?
- Prevents misalignment when adding bold markers
- Ensures professional appearance
- Accommodates Unicode characters correctly

## Limitations

- Only processes grid-style tables (not pipe tables)
- Assumes well-formed table structure
- Detection heuristics may need tuning for edge cases
- Does not handle nested tables or complex merged cells

## Future Enhancements

Potential improvements:
- Add command-line override for detection (force bold/no-bold)
- Support for pipe-style tables
- More sophisticated content type detection
- Integration with Quarto table generation
- Table style templates


--- kits/README.md ---
# Hardware Kits

*Hands-on Embedded ML Labs for Real Devices*

[![Build](https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/kits-publish-dev.yml?branch=dev&label=Build&logo=githubactions)](https://github.com/harvard-edge/cs249r_book/actions/workflows/kits-publish-dev.yml)
[![Website](https://img.shields.io/badge/Read-mlsysbook.ai/kits-blue)](https://mlsysbook.ai/kits)
[![PDF](https://img.shields.io/badge/Download-PDF-red)](https://mlsysbook.ai/kits/assets/downloads/Hardware-Kits.pdf)

**[Read Online](https://mlsysbook.ai/kits)** | **[PDF](https://mlsysbook.ai/kits/assets/downloads/Hardware-Kits.pdf)**

---

## What This Is

The Hardware Kits teach you how to deploy ML models to real embedded devices. You will face actual hardware constraints: limited memory, power budgets, and latency requirements that do not exist in cloud environments.

This is where AI systems meet the physical world.

---

## What You Will Learn

| Concept | What You Do |
|---------|-------------|
| **Image Classification** | Deploy CNN models to classify images in real-time on microcontrollers |
| **Object Detection** | Run YOLO-style detection on camera-equipped boards |
| **Keyword Spotting** | Build always-on wake word detection with audio DSP |
| **Motion Classification** | Use IMU sensors for gesture and activity recognition |
| **Model Optimization** | Quantize and compress models to fit in KB of RAM |
| **Power Management** | Balance accuracy vs battery life for edge deployment |

### Hardware Platforms

| Platform | Description | Best For |
|----------|-------------|----------|
| **Arduino Nicla Vision** | Compact AI camera board with STM32H7 | Vision projects, ultra-low power |
| **Seeed XIAO ESP32S3** | Tiny ESP32-S3 with camera support | WiFi-connected vision |
| **Grove Vision AI V2** | No-code AI vision module | Rapid prototyping |
| **Raspberry Pi** | Full Linux SBC for edge AI | Complex pipelines, prototyping |

---

## Quick Start

### For Learners

1. Pick a platform from the [labs](https://mlsysbook.ai/kits)
2. Follow the setup guide for your hardware
3. Complete the labs in order: Setup ‚Üí Image Classification ‚Üí Object Detection ‚Üí Keyword Spotting

### For Contributors

```bash
cd kits

# Build HTML site
ln -sf config/_quarto-html.yml _quarto.yml
quarto render

# Build PDF
ln -sf config/_quarto-pdf.yml _quarto.yml
quarto render --to titlepage-pdf

# Preview with live reload
quarto preview
```

---

## Labs Overview

Each platform includes progressive labs:

| Lab | What You Build | Skills |
|-----|----------------|--------|
| **Setup** | Hardware setup and environment configuration | Toolchain, flashing, debugging |
| **Image Classification** | CNN-based image recognition | Model deployment, inference |
| **Object Detection** | Real-time object detection | YOLO, bounding boxes |
| **Keyword Spotting** | Audio wake word detection | DSP, MFCC features |
| **Motion Classification** | IMU-based gesture recognition | Sensor fusion, time series |

---

## Directory Structure

```
kits/
‚îú‚îÄ‚îÄ contents/                # Lab content
‚îÇ   ‚îú‚îÄ‚îÄ arduino/             # Arduino Nicla Vision labs
‚îÇ   ‚îú‚îÄ‚îÄ seeed/               # Seeed XIAO & Grove Vision labs
‚îÇ   ‚îú‚îÄ‚îÄ raspi/               # Raspberry Pi labs
‚îÇ   ‚îî‚îÄ‚îÄ shared/              # Shared resources (DSP, features)
‚îú‚îÄ‚îÄ assets/                  # Images, styles, scripts
‚îú‚îÄ‚îÄ config/                  # Quarto configurations
‚îÇ   ‚îú‚îÄ‚îÄ _quarto-html.yml     # Website config
‚îÇ   ‚îî‚îÄ‚îÄ _quarto-pdf.yml      # PDF config
‚îú‚îÄ‚îÄ tex/                     # LaTeX includes for PDF
‚îú‚îÄ‚îÄ filters/                 # Lua filters
‚îî‚îÄ‚îÄ index.qmd                # Landing page
```

---

## Documentation

| Audience | Resources |
|----------|-----------|
| **Learners** | [Online Labs](https://mlsysbook.ai/kits) „Éª [PDF](https://mlsysbook.ai/kits/assets/downloads/Hardware-Kits.pdf) |
| **Contributors** | See build instructions above |

---

## Contributing

We welcome contributions to the hardware labs! To contribute:

1. Fork and clone the repository
2. Add or improve lab content in `contents/`
3. Test your changes with `quarto preview`
4. Submit a PR with a clear description

---

## Related

| Component | Description |
|-----------|-------------|
| **[Main README](../README.md)** | Project overview and ecosystem |
| **[Textbook](../book/)** | ML Systems concepts and theory |
| **[TinyTorch](../tinytorch/)** | Build ML frameworks from scratch |
| **[Website](https://mlsysbook.ai/kits)** | Read labs online |

---

## Contributors

Thanks to these wonderful people who helped improve the hardware kits!

**Legend:** ü™≤ Bug Hunter ¬∑ ‚ö° Code Warrior ¬∑ üìö Documentation Hero ¬∑ üé® Design Artist ¬∑ üß† Idea Generator ¬∑ üîé Code Reviewer ¬∑ üß™ Test Engineer ¬∑ üõ†Ô∏è Tool Builder

<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
<table>
  <tbody>
    <tr>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/profvjreddi"><img src="https://avatars.githubusercontent.com/profvjreddi?v=4?s=80" width="80px;" alt="Vijay Janapa Reddi"/><br /><sub><b>Vijay Janapa Reddi</b></sub></a><br />ü™≤ üßë‚Äçüíª üé® ‚úçÔ∏è üß™ üõ†Ô∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/Mjrovai"><img src="https://avatars.githubusercontent.com/Mjrovai?v=4?s=80" width="80px;" alt="Marcelo Rovai"/><br /><sub><b>Marcelo Rovai</b></sub></a><br />‚úçÔ∏è üßë‚Äçüíª üé® tutorial</td>
    </tr>
  </tbody>
</table>

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->

<!-- ALL-CONTRIBUTORS-LIST:END -->

**Recognize a contributor:** Comment on any issue or PR:
```
@all-contributors please add @username for tool, test, video, or doc
```

---

## License

Content is licensed under **Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International** (CC BY-NC-SA 4.0).

See [LICENSE.md](../LICENSE.md) for details.


## Links discovered
- [![Build](https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/kits-publish-dev.yml?branch=dev&label=Build&logo=githubactions)
- [![Website](https://img.shields.io/badge/Read-mlsysbook.ai/kits-blue)
- [![PDF](https://img.shields.io/badge/Download-PDF-red)
- [Read Online](https://mlsysbook.ai/kits)
- [PDF](https://mlsysbook.ai/kits/assets/downloads/Hardware-Kits.pdf)
- [labs](https://mlsysbook.ai/kits)
- [Online Labs](https://mlsysbook.ai/kits)
- [Main README](https://github.com/harvard-edge/cs249r_book/blob/dev/README.md)
- [Textbook](https://github.com/harvard-edge/cs249r_book/blob/dev/book.md)
- [TinyTorch](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch.md)
- [Website](https://mlsysbook.ai/kits)
- [LICENSE.md](https://github.com/harvard-edge/cs249r_book/blob/dev/LICENSE.md)
- [<img src="https://avatars.githubusercontent.com/profvjreddi?v=4?s=80" width="80px;" alt="Vijay Janapa Reddi"/><br /><sub><b>Vijay Janapa Reddi</b></sub>](https://github.com/profvjreddi)
- [<img src="https://avatars.githubusercontent.com/Mjrovai?v=4?s=80" width="80px;" alt="Marcelo Rovai"/><br /><sub><b>Marcelo Rovai</b></sub>](https://github.com/Mjrovai)

--- kits/assets/scripts/subscribe-modal.js ---
/**
 * Subscribe Modal Component
 * Elegant popup subscription form for ML Systems Textbook
 */

(function() {
  'use strict';

  // Create modal HTML structure
  function createModalHTML() {
    return `
      <div id="subscribe-modal" class="modal-overlay" style="display: none;">
        <div class="modal-container">
          <button class="modal-close" data-close-modal aria-label="Close">&times;</button>
          <div class="modal-content">
            <div class="modal-header">
              <div class="modal-brand-row">
                <span class="modal-brand-item">üìö MLSysBook</span>
              </div>
              <h2 class="modal-title">Stay in the Loop</h2>
              <p class="modal-subtitle">Get updates on new chapters, hands-on labs, and ML systems resources.</p>
            </div>
            <form id="subscribe-modal-form" class="subscribe-form" action="https://buttondown.email/api/emails/embed-subscribe/mlsysbook" method="post">
              <div class="form-row">
                <div class="form-group">
                  <label for="modal-first-name">First name</label>
                  <input type="text" id="modal-first-name" name="metadata__first_name" required placeholder="Jane">
                </div>
                <div class="form-group">
                  <label for="modal-last-name">Last name</label>
                  <input type="text" id="modal-last-name" name="metadata__last_name" required placeholder="Smith">
                </div>
              </div>
              <div class="form-group">
                <label for="modal-email">Email</label>
                <input type="email" id="modal-email" name="email" required placeholder="jane@university.edu">
              </div>
              <div class="form-group">
                <label>I am a...</label>
                <div class="role-options role-options-three-compact">
                  <label class="role-option">
                    <input type="radio" name="metadata__role" value="educator" required>
                    <span class="role-label">üë©‚Äçüè´ Educator</span>
                  </label>
                  <label class="role-option">
                    <input type="radio" name="metadata__role" value="student">
                    <span class="role-label">üéì Student</span>
                  </label>
                  <label class="role-option">
                    <input type="radio" name="metadata__role" value="industry">
                    <span class="role-label">üíº Industry</span>
                  </label>
                </div>
              </div>
              <div class="form-group">
                <label for="modal-organization">Organization <span class="optional-label">(optional)</span></label>
                <input type="text" id="modal-organization" name="metadata__organization" placeholder="University or company">
              </div>
              <div class="form-group">
                <label for="modal-motivation">What brings you here? <span class="optional-label">(optional)</span></label>
                <textarea id="modal-motivation" name="metadata__motivation" rows="2" placeholder="e.g., teaching a course, learning ML systems, building edge devices..."></textarea>
              </div>
              <input type="hidden" name="tag" value="mlsysbook-site">
              <button type="submit" class="btn btn-primary subscribe-btn">Subscribe</button>
              <p class="form-note">No spam, ever. Unsubscribe anytime.</p>
            </form>
            <div id="modal-subscribe-success" class="subscribe-success" style="display: none;">
              <div class="success-icon">üéâ</div>
              <h3>You're subscribed!</h3>
              <p>Thanks for signing up. We'll keep you updated on new chapters, labs, and resources.</p>
            </div>
          </div>
        </div>
      </div>
    `;
  }

  // Create modal CSS
  function createModalCSS() {
    const style = document.createElement('style');
    style.textContent = `
      /* Modal Overlay and Container */
      .modal-overlay {
        position: fixed;
        top: 0;
        left: 0;
        right: 0;
        bottom: 0;
        background: rgba(15, 23, 42, 0.6);
        backdrop-filter: blur(4px);
        z-index: 10001;
        align-items: center;
        justify-content: center;
        padding: 1rem;
        animation: fadeIn 0.2s ease;
      }

      @keyframes fadeIn {
        from { opacity: 0; }
        to { opacity: 1; }
      }

      @keyframes slideUp {
        from {
          opacity: 0;
          transform: translateY(20px) scale(0.98);
        }
        to {
          opacity: 1;
          transform: translateY(0) scale(1);
        }
      }

      .modal-container {
        background: white;
        border-radius: 16px;
        max-width: 440px;
        width: 100%;
        max-height: 90vh;
        overflow-y: auto;
        position: relative;
        box-shadow: 0 20px 25px -5px rgb(0 0 0 / 0.1), 0 8px 10px -6px rgb(0 0 0 / 0.1), 0 0 0 1px rgba(0,0,0,0.05);
        animation: slideUp 0.3s ease;
        margin: auto;
      }

      .modal-close {
        position: absolute;
        top: 1rem;
        right: 1rem;
        width: 36px;
        height: 36px;
        border: none;
        background: #f8fafc;
        border-radius: 50%;
        font-size: 1.5rem;
        color: #64748b;
        cursor: pointer;
        display: flex;
        align-items: center;
        justify-content: center;
        transition: all 0.2s ease;
        z-index: 10;
        line-height: 1;
      }

      .modal-close:hover {
        background: white;
        color: #0f172a;
        transform: scale(1.05);
      }

      .modal-content {
        padding: 2rem 2.5rem 2.5rem 2.5rem;
      }

      .modal-header {
        text-align: center;
        margin-bottom: 1.5rem;
        display: flex;
        flex-direction: column;
        align-items: center;
      }

      .modal-brand-row {
        display: inline-flex;
        align-items: center;
        justify-content: center;
        gap: 0.5rem;
        margin-bottom: 1rem;
      }

      .modal-brand-item {
        font-size: 0.8rem;
        font-weight: 600;
        color: #374151;
        background: #f3f4f6;
        padding: 0.3rem 0.6rem;
        border-radius: 5px;
        white-space: nowrap;
      }

      .modal-brand-plus {
        font-size: 0.9rem;
        font-weight: 300;
        color: #9ca3af;
      }

      .modal-title {
        font-size: 1.5rem;
        font-weight: 700;
        color: #0f172a;
        margin: 0 0 0.4rem 0;
        line-height: 1.2;
        width: 100%;
      }

      .modal-subtitle {
        font-size: 0.9rem;
        color: #64748b;
        margin: 0;
        line-height: 1.5;
        max-width: 320px;
      }

      /* Form Styles */
      .subscribe-form {
        display: flex;
        flex-direction: column;
        gap: 1.25rem;
      }

      .form-row {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 0.75rem;
      }

      .form-row .form-group {
        min-width: 0;
      }

      .form-row .form-group input {
        width: 100%;
        box-sizing: border-box;
      }

      .form-group {
        display: flex;
        flex-direction: column;
        gap: 0.5rem;
      }

      .form-group label {
        font-size: 0.9rem;
        font-weight: 600;
        color: #0f172a;
      }

      .optional-label {
        font-weight: 400;
        color: #64748b;
      }

      .form-group input[type="text"],
      .form-group input[type="email"],
      .form-group textarea {
        padding: 0.875rem 1rem;
        border: 1px solid #cbd5e1;
        border-radius: 8px;
        font-size: 1rem;
        transition: all 0.2s ease;
        background: #f8fafc;
        font-family: inherit;
      }

      .form-group textarea {
        resize: vertical;
        min-height: 60px;
      }

      .form-group input[type="text"]:focus,
      .form-group input[type="email"]:focus,
      .form-group textarea:focus {
        outline: none;
        border-color: #3b82f6;
        background: white;
        box-shadow: 0 0 0 3px rgba(59, 130, 246, 0.1);
      }

      .form-group input::placeholder,
      .form-group textarea::placeholder {
        color: #94a3b8;
      }

      /* Role Options - compact style */
      .role-options {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 0.75rem;
      }

      .role-options-three-compact {
        grid-template-columns: repeat(3, 1fr);
      }

      .role-option {
        cursor: pointer;
      }

      .role-option input[type="radio"] {
        position: absolute;
        opacity: 0;
        width: 0;
        height: 0;
      }

      .role-label {
        display: flex;
        align-items: center;
        justify-content: center;
        gap: 0.5rem;
        padding: 0.75rem 1rem;
        border: 2px solid #e2e8f0;
        border-radius: 8px;
        font-size: 0.9rem;
        font-weight: 500;
        color: #475569;
        transition: all 0.2s ease;
        background: #f8fafc;
      }

      .role-options-three-compact .role-label {
        padding: 0.625rem 0.5rem;
        font-size: 0.8rem;
        text-align: center;
      }

      .role-option input[type="radio"]:checked + .role-label {
        border-color: #3b82f6;
        background: rgba(59, 130, 246, 0.08);
        color: #3b82f6;
      }

      .role-option:hover .role-label {
        border-color: #cbd5e1;
        background: white;
      }

      /* Button Styles */
      .btn {
        display: inline-flex;
        align-items: center;
        justify-content: center;
        gap: 0.5rem;
        padding: 0.75rem 1.5rem;
        border-radius: 8px;
        text-decoration: none;
        font-weight: 600;
        font-size: 0.95rem;
        transition: all 0.2s ease;
        border: none;
        cursor: pointer;
        font-family: inherit;
      }

      .btn-primary {
        background: #3b82f6;
        color: white;
      }

      .btn-primary:hover {
        background: #1e3a8a;
        transform: translateY(-1px);
        box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1), 0 2px 4px -2px rgb(0 0 0 / 0.1);
      }

      .subscribe-btn {
        width: 100%;
        padding: 1rem;
        font-size: 1rem;
        margin-top: 0.5rem;
      }

      .form-note {
        text-align: center;
        font-size: 0.85rem;
        color: #64748b;
        margin: 0;
      }

      /* Success Message */
      .subscribe-success {
        text-align: center;
        padding: 2rem 1rem;
      }

      .success-icon {
        font-size: 3rem;
        margin-bottom: 1rem;
      }

      .subscribe-success h3 {
        font-size: 1.5rem;
        font-weight: 600;
        color: #0f172a;
        margin-bottom: 0.5rem;
      }

      .subscribe-success p {
        color: #475569;
        font-size: 1rem;
      }

      /* Dark mode support */
      body.quarto-dark .modal-container {
        background: #1e293b;
      }

      body.quarto-dark .modal-close {
        background: #0f172a;
        color: #94a3b8;
      }

      body.quarto-dark .modal-close:hover {
        background: #334155;
        color: #f1f5f9;
      }

      body.quarto-dark .modal-brand-item {
        background: #334155;
        color: #e2e8f0;
      }

      body.quarto-dark .modal-brand-plus {
        color: #64748b;
      }

      body.quarto-dark .modal-title,
      body.quarto-dark .form-group label,
      body.quarto-dark .subscribe-success h3 {
        color: #f1f5f9;
      }

      body.quarto-dark .modal-subtitle,
      body.quarto-dark .subscribe-success p {
        color: #cbd5e1;
      }

      body.quarto-dark .form-group input[type="text"],
      body.quarto-dark .form-group input[type="email"],
      body.quarto-dark .form-group textarea {
        background: #0f172a;
        border-color: #334155;
        color: #f1f5f9;
      }

      body.quarto-dark .role-label {
        background: #0f172a;
        border-color: #334155;
        color: #cbd5e1;
      }

      body.quarto-dark .role-option input[type="radio"]:checked + .role-label {
        border-color: #3b82f6;
        background: rgba(59, 130, 246, 0.15);
        color: #60a5fa;
      }

      /* Responsive */
      @media (max-width: 640px) {
        .modal-content {
          padding: 2rem 1.5rem;
        }

        .form-row {
          grid-template-columns: 1fr;
        }

        .role-options-three-compact {
          grid-template-columns: repeat(3, 1fr);
        }
      }
    `;
    return style;
  }

  // Initialize modal
  function initModal() {
    // Add CSS
    document.head.appendChild(createModalCSS());

    // Add HTML
    const modalDiv = document.createElement('div');
    modalDiv.innerHTML = createModalHTML();
    document.body.appendChild(modalDiv.firstElementChild);

    const modal = document.getElementById('subscribe-modal');
    const form = document.getElementById('subscribe-modal-form');
    const success = document.getElementById('modal-subscribe-success');

    // Open modal function
    window.openModal = function() {
      modal.style.display = 'flex';
      document.body.style.overflow = 'hidden';

      // Focus first input
      setTimeout(() => {
        const firstInput = document.getElementById('modal-first-name');
        if (firstInput) firstInput.focus();
      }, 100);
    };

    // Close modal function
    window.closeModal = function() {
      modal.style.display = 'none';
      document.body.style.overflow = '';

      // Reset form after closing
      setTimeout(() => {
        form.style.display = 'flex';
        form.reset();
        success.style.display = 'none';
      }, 300);
    };

    // Close on overlay click
    modal.addEventListener('click', (e) => {
      if (e.target === modal) {
        closeModal();
      }
    });

    // Close button click
    const closeBtn = modal.querySelector('[data-close-modal]');
    if (closeBtn) {
      closeBtn.addEventListener('click', closeModal);
    }

    // Close on Escape key
    document.addEventListener('keydown', (e) => {
      if (e.key === 'Escape' && modal.style.display === 'flex') {
        closeModal();
      }
    });

    // Handle form submission
    form.addEventListener('submit', function() {
      // Let the form submit to Buttondown
      setTimeout(() => {
        form.style.display = 'none';
        success.style.display = 'block';

        // Close modal after 5 seconds
        setTimeout(closeModal, 5000);
      }, 100);
    });

    // Check if URL has #subscribe hash on page load - auto-open modal
    if (window.location.hash === '#subscribe') {
      // Small delay to ensure page is fully loaded
      setTimeout(() => {
        openModal();
      }, 300);
    }

    // Also listen for hash changes (e.g., user clicks back/forward)
    window.addEventListener('hashchange', function() {
      if (window.location.hash === '#subscribe') {
        openModal();
      }
    });

    // Intercept navbar subscribe link
    setTimeout(() => {
      // Look for subscribe links in navbar
      const subscribeSelectors = [
        'a[href*="buttondown.email/mlsysbook"]',
        'a[href="#subscribe"]',
        'a[href*="subscribe"]',
        '#navbar-subscribe-btn',
        '.subscribe-link'
      ];

      subscribeSelectors.forEach(selector => {
        try {
          const links = document.querySelectorAll(selector);
          links.forEach(link => {
            link.addEventListener('click', function(e) {
              e.preventDefault();
              openModal();
            });
          });
        } catch (err) {
          // Selector not supported, continue
        }
      });
    }, 1000);
  }

  // Initialize when DOM is ready
  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initModal);
  } else {
    initModal();
  }
})();


--- kits/_extensions/mlsysbook-ext/titlepage/mathjax.html ---
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: {autoNumber: "AMS"} },
  tex2jax: {inlineMath: [ ['$','$'], ["\\(","\\)"] ]}
});
</script>

<!-- This is what works with Quarto -->
<script>
  MathJax = {
    tex: {
      tags: 'ams'  // should be 'ams', 'none', or 'all'
    }
  };
</script>


--- labs/README.md ---
# Labs

**Understanding the Interplay Between Algorithms and Systems**

> **Status:** Coming Summer 2026

---

## What Are Labs?

Labs are hands-on interactive notebooks that bridge the gap between **reading about ML systems** (the textbook) and **building them from scratch** (TinyTorch).

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 ‚îÇ     ‚îÇ                 ‚îÇ     ‚îÇ                 ‚îÇ
‚îÇ    Textbook     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ      Labs       ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    TinyTorch    ‚îÇ
‚îÇ                 ‚îÇ     ‚îÇ                 ‚îÇ     ‚îÇ                 ‚îÇ
‚îÇ  Concepts &     ‚îÇ     ‚îÇ  Experiment &   ‚îÇ     ‚îÇ  Build from     ‚îÇ
‚îÇ  Theory         ‚îÇ     ‚îÇ  Explore        ‚îÇ     ‚îÇ  Scratch        ‚îÇ
‚îÇ                 ‚îÇ     ‚îÇ                 ‚îÇ     ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      READ                    EXPLORE                  BUILD
```

## The Learning Journey

| Phase | Resource | What You Do |
|-------|----------|-------------|
| **Understand** | [Textbook](https://mlsysbook.ai) | Learn concepts, theory, and system design principles |
| **Experiment** | Labs | Explore tradeoffs, tweak parameters, see how decisions ripple through systems |
| **Build** | [TinyTorch](https://mlsysbook.ai/tinytorch) | Implement everything from scratch, own every line of code |

## Why Labs?

ML systems are where algorithms meet hardware. A model that works perfectly in theory can fail in practice due to memory limits, latency constraints, or numerical precision. Labs help you develop intuition for these algorithm-system interactions.

- **See the tradeoffs** ‚Äî How does batch size affect memory? How does quantization affect accuracy?
- **Explore interactively** ‚Äî Adjust parameters and watch how changes ripple through the system
- **Build intuition** ‚Äî Understand *why* systems behave the way they do, not just *what* they do
- **Zero setup** ‚Äî Run directly in your browser via Google Colab

## Example Topics (Planned)

- **Memory vs. Compute Tradeoffs** ‚Äî Watch how batch size affects memory footprint and training speed
- **Quantization Effects** ‚Äî See accuracy degradation as you reduce precision from FP32 ‚Üí INT8 ‚Üí INT4
- **Attention Visualization** ‚Äî Explore what transformer attention heads actually learn
- **Optimization Landscapes** ‚Äî Navigate loss surfaces with different optimizers
- **Pruning Strategies** ‚Äî Compare structured vs. unstructured pruning on real models

## Stay Updated

Labs are under active development. To be notified when they launch:

- [Subscribe to updates](https://buttondown.email/mlsysbook)
- [Star the repo](https://github.com/harvard-edge/cs249r_book)
- [Join discussions](https://github.com/harvard-edge/cs249r_book/discussions)

---

## Related Resources

| Resource | Description |
|----------|-------------|
| [Textbook](https://mlsysbook.ai) | ML Systems principles and practices |
| [TinyTorch](https://mlsysbook.ai/tinytorch) | Build your own ML framework from scratch |
| [Discussions](https://github.com/harvard-edge/cs249r_book/discussions) | Ask questions, share feedback |

---

## Contributors

Thanks to these wonderful people who helped build the labs!

**Legend:** ü™≤ Bug Hunter ¬∑ ‚ö° Code Warrior ¬∑ üìö Documentation Hero ¬∑ üé® Design Artist ¬∑ üß† Idea Generator ¬∑ üîé Code Reviewer ¬∑ üß™ Test Engineer ¬∑ üõ†Ô∏è Tool Builder

<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
<table>
  <tbody>
    <tr>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/profvjreddi"><img src="https://avatars.githubusercontent.com/profvjreddi?v=4?s=80" width="80px;" alt="Vijay Janapa Reddi"/><br /><sub><b>Vijay Janapa Reddi</b></sub></a><br />üßë‚Äçüíª üé® ‚úçÔ∏è</td>
    </tr>
  </tbody>
</table>

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->

<!-- ALL-CONTRIBUTORS-LIST:END -->

**Recognize a contributor:** Comment on any issue or PR:
```
@all-contributors please add @username for code, tutorial, test, or doc
```

---

<div align="center">

**Read. Explore. Build.** *(Labs coming soon)*

</div>


## Links discovered
- [Textbook](https://mlsysbook.ai)
- [TinyTorch](https://mlsysbook.ai/tinytorch)
- [Subscribe to updates](https://buttondown.email/mlsysbook)
- [Star the repo](https://github.com/harvard-edge/cs249r_book)
- [Join discussions](https://github.com/harvard-edge/cs249r_book/discussions)
- [Discussions](https://github.com/harvard-edge/cs249r_book/discussions)
- [<img src="https://avatars.githubusercontent.com/profvjreddi?v=4?s=80" width="80px;" alt="Vijay Janapa Reddi"/><br /><sub><b>Vijay Janapa Reddi</b></sub>](https://github.com/profvjreddi)

--- labs/assets/scripts/subscribe-modal.js ---
/**
 * Subscribe Modal Component
 * Elegant popup subscription form for ML Systems Textbook
 */

(function() {
  'use strict';

  // Create modal HTML structure
  function createModalHTML() {
    return `
      <div id="subscribe-modal" class="modal-overlay" style="display: none;">
        <div class="modal-container">
          <button class="modal-close" data-close-modal aria-label="Close">&times;</button>
          <div class="modal-content">
            <div class="modal-header">
              <div class="modal-brand-row">
                <span class="modal-brand-item">üìö MLSysBook</span>
              </div>
              <h2 class="modal-title">Stay in the Loop</h2>
              <p class="modal-subtitle">Get updates on new chapters, hands-on labs, and ML systems resources.</p>
            </div>
            <form id="subscribe-modal-form" class="subscribe-form" action="https://buttondown.email/api/emails/embed-subscribe/mlsysbook" method="post">
              <div class="form-row">
                <div class="form-group">
                  <label for="modal-first-name">First name</label>
                  <input type="text" id="modal-first-name" name="metadata__first_name" required placeholder="Jane">
                </div>
                <div class="form-group">
                  <label for="modal-last-name">Last name</label>
                  <input type="text" id="modal-last-name" name="metadata__last_name" required placeholder="Smith">
                </div>
              </div>
              <div class="form-group">
                <label for="modal-email">Email</label>
                <input type="email" id="modal-email" name="email" required placeholder="jane@university.edu">
              </div>
              <div class="form-group">
                <label>I am a...</label>
                <div class="role-options role-options-three-compact">
                  <label class="role-option">
                    <input type="radio" name="metadata__role" value="educator" required>
                    <span class="role-label">üë©‚Äçüè´ Educator</span>
                  </label>
                  <label class="role-option">
                    <input type="radio" name="metadata__role" value="student">
                    <span class="role-label">üéì Student</span>
                  </label>
                  <label class="role-option">
                    <input type="radio" name="metadata__role" value="industry">
                    <span class="role-label">üíº Industry</span>
                  </label>
                </div>
              </div>
              <div class="form-group">
                <label for="modal-organization">Organization <span class="optional-label">(optional)</span></label>
                <input type="text" id="modal-organization" name="metadata__organization" placeholder="University or company">
              </div>
              <div class="form-group">
                <label for="modal-motivation">What brings you here? <span class="optional-label">(optional)</span></label>
                <textarea id="modal-motivation" name="metadata__motivation" rows="2" placeholder="e.g., teaching a course, learning ML systems, building edge devices..."></textarea>
              </div>
              <input type="hidden" name="tag" value="mlsysbook-site">
              <button type="submit" class="btn btn-primary subscribe-btn">Subscribe</button>
              <p class="form-note">No spam, ever. Unsubscribe anytime.</p>
            </form>
            <div id="modal-subscribe-success" class="subscribe-success" style="display: none;">
              <div class="success-icon">üéâ</div>
              <h3>You're subscribed!</h3>
              <p>Thanks for signing up. We'll keep you updated on new chapters, labs, and resources.</p>
            </div>
          </div>
        </div>
      </div>
    `;
  }

  // Create modal CSS
  function createModalCSS() {
    const style = document.createElement('style');
    style.textContent = `
      /* Modal Overlay and Container */
      .modal-overlay {
        position: fixed;
        top: 0;
        left: 0;
        right: 0;
        bottom: 0;
        background: rgba(15, 23, 42, 0.6);
        backdrop-filter: blur(4px);
        z-index: 10001;
        align-items: center;
        justify-content: center;
        padding: 1rem;
        animation: fadeIn 0.2s ease;
      }

      @keyframes fadeIn {
        from { opacity: 0; }
        to { opacity: 1; }
      }

      @keyframes slideUp {
        from {
          opacity: 0;
          transform: translateY(20px) scale(0.98);
        }
        to {
          opacity: 1;
          transform: translateY(0) scale(1);
        }
      }

      .modal-container {
        background: white;
        border-radius: 16px;
        max-width: 440px;
        width: 100%;
        max-height: 90vh;
        overflow-y: auto;
        position: relative;
        box-shadow: 0 20px 25px -5px rgb(0 0 0 / 0.1), 0 8px 10px -6px rgb(0 0 0 / 0.1), 0 0 0 1px rgba(0,0,0,0.05);
        animation: slideUp 0.3s ease;
        margin: auto;
      }

      .modal-close {
        position: absolute;
        top: 1rem;
        right: 1rem;
        width: 36px;
        height: 36px;
        border: none;
        background: #f8fafc;
        border-radius: 50%;
        font-size: 1.5rem;
        color: #64748b;
        cursor: pointer;
        display: flex;
        align-items: center;
        justify-content: center;
        transition: all 0.2s ease;
        z-index: 10;
        line-height: 1;
      }

      .modal-close:hover {
        background: white;
        color: #0f172a;
        transform: scale(1.05);
      }

      .modal-content {
        padding: 2rem 2.5rem 2.5rem 2.5rem;
      }

      .modal-header {
        text-align: center;
        margin-bottom: 1.5rem;
        display: flex;
        flex-direction: column;
        align-items: center;
      }

      .modal-brand-row {
        display: inline-flex;
        align-items: center;
        justify-content: center;
        gap: 0.5rem;
        margin-bottom: 1rem;
      }

      .modal-brand-item {
        font-size: 0.8rem;
        font-weight: 600;
        color: #374151;
        background: #f3f4f6;
        padding: 0.3rem 0.6rem;
        border-radius: 5px;
        white-space: nowrap;
      }

      .modal-brand-plus {
        font-size: 0.9rem;
        font-weight: 300;
        color: #9ca3af;
      }

      .modal-title {
        font-size: 1.5rem;
        font-weight: 700;
        color: #0f172a;
        margin: 0 0 0.4rem 0;
        line-height: 1.2;
        width: 100%;
      }

      .modal-subtitle {
        font-size: 0.9rem;
        color: #64748b;
        margin: 0;
        line-height: 1.5;
        max-width: 320px;
      }

      /* Form Styles */
      .subscribe-form {
        display: flex;
        flex-direction: column;
        gap: 1.25rem;
      }

      .form-row {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 0.75rem;
      }

      .form-row .form-group {
        min-width: 0;
      }

      .form-row .form-group input {
        width: 100%;
        box-sizing: border-box;
      }

      .form-group {
        display: flex;
        flex-direction: column;
        gap: 0.5rem;
      }

      .form-group label {
        font-size: 0.9rem;
        font-weight: 600;
        color: #0f172a;
      }

      .optional-label {
        font-weight: 400;
        color: #64748b;
      }

      .form-group input[type="text"],
      .form-group input[type="email"],
      .form-group textarea {
        padding: 0.875rem 1rem;
        border: 1px solid #cbd5e1;
        border-radius: 8px;
        font-size: 1rem;
        transition: all 0.2s ease;
        background: #f8fafc;
        font-family: inherit;
      }

      .form-group textarea {
        resize: vertical;
        min-height: 60px;
      }

      .form-group input[type="text"]:focus,
      .form-group input[type="email"]:focus,
      .form-group textarea:focus {
        outline: none;
        border-color: #3b82f6;
        background: white;
        box-shadow: 0 0 0 3px rgba(59, 130, 246, 0.1);
      }

      .form-group input::placeholder,
      .form-group textarea::placeholder {
        color: #94a3b8;
      }

      /* Role Options - compact style */
      .role-options {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 0.75rem;
      }

      .role-options-three-compact {
        grid-template-columns: repeat(3, 1fr);
      }

      .role-option {
        cursor: pointer;
      }

      .role-option input[type="radio"] {
        position: absolute;
        opacity: 0;
        width: 0;
        height: 0;
      }

      .role-label {
        display: flex;
        align-items: center;
        justify-content: center;
        gap: 0.5rem;
        padding: 0.75rem 1rem;
        border: 2px solid #e2e8f0;
        border-radius: 8px;
        font-size: 0.9rem;
        font-weight: 500;
        color: #475569;
        transition: all 0.2s ease;
        background: #f8fafc;
      }

      .role-options-three-compact .role-label {
        padding: 0.625rem 0.5rem;
        font-size: 0.8rem;
        text-align: center;
      }

      .role-option input[type="radio"]:checked + .role-label {
        border-color: #3b82f6;
        background: rgba(59, 130, 246, 0.08);
        color: #3b82f6;
      }

      .role-option:hover .role-label {
        border-color: #cbd5e1;
        background: white;
      }

      /* Button Styles */
      .btn {
        display: inline-flex;
        align-items: center;
        justify-content: center;
        gap: 0.5rem;
        padding: 0.75rem 1.5rem;
        border-radius: 8px;
        text-decoration: none;
        font-weight: 600;
        font-size: 0.95rem;
        transition: all 0.2s ease;
        border: none;
        cursor: pointer;
        font-family: inherit;
      }

      .btn-primary {
        background: #3b82f6;
        color: white;
      }

      .btn-primary:hover {
        background: #1e3a8a;
        transform: translateY(-1px);
        box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1), 0 2px 4px -2px rgb(0 0 0 / 0.1);
      }

      .subscribe-btn {
        width: 100%;
        padding: 1rem;
        font-size: 1rem;
        margin-top: 0.5rem;
      }

      .form-note {
        text-align: center;
        font-size: 0.85rem;
        color: #64748b;
        margin: 0;
      }

      /* Success Message */
      .subscribe-success {
        text-align: center;
        padding: 2rem 1rem;
      }

      .success-icon {
        font-size: 3rem;
        margin-bottom: 1rem;
      }

      .subscribe-success h3 {
        font-size: 1.5rem;
        font-weight: 600;
        color: #0f172a;
        margin-bottom: 0.5rem;
      }

      .subscribe-success p {
        color: #475569;
        font-size: 1rem;
      }

      /* Dark mode support */
      body.quarto-dark .modal-container {
        background: #1e293b;
      }

      body.quarto-dark .modal-close {
        background: #0f172a;
        color: #94a3b8;
      }

      body.quarto-dark .modal-close:hover {
        background: #334155;
        color: #f1f5f9;
      }

      body.quarto-dark .modal-brand-item {
        background: #334155;
        color: #e2e8f0;
      }

      body.quarto-dark .modal-brand-plus {
        color: #64748b;
      }

      body.quarto-dark .modal-title,
      body.quarto-dark .form-group label,
      body.quarto-dark .subscribe-success h3 {
        color: #f1f5f9;
      }

      body.quarto-dark .modal-subtitle,
      body.quarto-dark .subscribe-success p {
        color: #cbd5e1;
      }

      body.quarto-dark .form-group input[type="text"],
      body.quarto-dark .form-group input[type="email"],
      body.quarto-dark .form-group textarea {
        background: #0f172a;
        border-color: #334155;
        color: #f1f5f9;
      }

      body.quarto-dark .role-label {
        background: #0f172a;
        border-color: #334155;
        color: #cbd5e1;
      }

      body.quarto-dark .role-option input[type="radio"]:checked + .role-label {
        border-color: #3b82f6;
        background: rgba(59, 130, 246, 0.15);
        color: #60a5fa;
      }

      /* Responsive */
      @media (max-width: 640px) {
        .modal-content {
          padding: 2rem 1.5rem;
        }

        .form-row {
          grid-template-columns: 1fr;
        }

        .role-options-three-compact {
          grid-template-columns: repeat(3, 1fr);
        }
      }
    `;
    return style;
  }

  // Initialize modal
  function initModal() {
    // Add CSS
    document.head.appendChild(createModalCSS());

    // Add HTML
    const modalDiv = document.createElement('div');
    modalDiv.innerHTML = createModalHTML();
    document.body.appendChild(modalDiv.firstElementChild);

    const modal = document.getElementById('subscribe-modal');
    const form = document.getElementById('subscribe-modal-form');
    const success = document.getElementById('modal-subscribe-success');

    // Open modal function
    window.openModal = function() {
      modal.style.display = 'flex';
      document.body.style.overflow = 'hidden';

      // Focus first input
      setTimeout(() => {
        const firstInput = document.getElementById('modal-first-name');
        if (firstInput) firstInput.focus();
      }, 100);
    };

    // Close modal function
    window.closeModal = function() {
      modal.style.display = 'none';
      document.body.style.overflow = '';

      // Reset form after closing
      setTimeout(() => {
        form.style.display = 'flex';
        form.reset();
        success.style.display = 'none';
      }, 300);
    };

    // Close on overlay click
    modal.addEventListener('click', (e) => {
      if (e.target === modal) {
        closeModal();
      }
    });

    // Close button click
    const closeBtn = modal.querySelector('[data-close-modal]');
    if (closeBtn) {
      closeBtn.addEventListener('click', closeModal);
    }

    // Close on Escape key
    document.addEventListener('keydown', (e) => {
      if (e.key === 'Escape' && modal.style.display === 'flex') {
        closeModal();
      }
    });

    // Handle form submission
    form.addEventListener('submit', function() {
      // Let the form submit to Buttondown
      setTimeout(() => {
        form.style.display = 'none';
        success.style.display = 'block';

        // Close modal after 5 seconds
        setTimeout(closeModal, 5000);
      }, 100);
    });

    // Check if URL has #subscribe hash on page load - auto-open modal
    if (window.location.hash === '#subscribe') {
      // Small delay to ensure page is fully loaded
      setTimeout(() => {
        openModal();
      }, 300);
    }

    // Also listen for hash changes (e.g., user clicks back/forward)
    window.addEventListener('hashchange', function() {
      if (window.location.hash === '#subscribe') {
        openModal();
      }
    });

    // Intercept navbar subscribe link
    setTimeout(() => {
      // Look for subscribe links in navbar
      const subscribeSelectors = [
        'a[href*="buttondown.email/mlsysbook"]',
        'a[href="#subscribe"]',
        'a[href*="subscribe"]',
        '#navbar-subscribe-btn',
        '.subscribe-link'
      ];

      subscribeSelectors.forEach(selector => {
        try {
          const links = document.querySelectorAll(selector);
          links.forEach(link => {
            link.addEventListener('click', function(e) {
              e.preventDefault();
              openModal();
            });
          });
        } catch (err) {
          // Selector not supported, continue
        }
      });
    }, 1000);
  }

  // Initialize when DOM is ready
  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initModal);
  } else {
    initModal();
  }
})();


--- README/README_ja.md ---
# Ê©üÊ¢∞Â≠¶Áøí„Ç∑„Çπ„ÉÜ„É†
*‰∫∫Â∑•Áü•ËÉΩ„Ç∑„Çπ„ÉÜ„É†Â∑•Â≠¶„ÅÆÂéüÂâá„Å®ÂÆüË∑µ*

<p align="center">
  <a href="../README.md">English</a> ‚Ä¢
  <a href="README_zh.md">‰∏≠Êñá</a> ‚Ä¢
  <a href="README_ja.md">Êó•Êú¨Ë™û</a> ‚Ä¢
  <a href="README_ko.md">ÌïúÍµ≠Ïñ¥</a>
</p>

<div align="center">

<p align="center">

  [![Book](https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/book-validate-dev.yml?branch=dev&label=Book&logo=githubactions&cacheSeconds=300)](https://github.com/harvard-edge/cs249r_book/actions/workflows/book-validate-dev.yml)
  [![TinyTorch](https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/tinytorch-validate-dev.yml?branch=dev&label=TinyTorch&logo=python&cacheSeconds=300)](https://github.com/harvard-edge/cs249r_book/actions/workflows/tinytorch-validate-dev.yml)
  ![Updated](https://img.shields.io/github/last-commit/harvard-edge/cs249r_book/dev?label=Updated&logo=git&cacheSeconds=300)
  [![License](https://img.shields.io/badge/License-CC--BY--NC--ND%204.0-blue.svg)](https://github.com/harvard-edge/cs249r_book/blob/dev/LICENSE.md)
  [![Cite](https://img.shields.io/badge/Cite-IEEE%202024-blue?logo=ieee)](#-citation--license)
  [![Fund Us](https://img.shields.io/badge/Fund%20Us-Open%20Collective-blue.svg?logo=open-collective)](https://opencollective.com/mlsysbook)

</p>

<p align="center">

  <!-- Reader Navigation -->
  **[üìñ „Ç™„É≥„É©„Ç§„É≥„ÅßË™≠„ÇÄ](https://mlsysbook.ai)** ‚Ä¢
  **[Tinyüî•Torch](https://mlsysbook.ai/tinytorch)** ‚Ä¢
  **[üìÑ PDF „ÉÄ„Ç¶„É≥„É≠„Éº„Éâ](https://mlsysbook.ai/assets/downloads/Machine-Learning-Systems.pdf)** ‚Ä¢
  **[üìì EPUB „ÉÄ„Ç¶„É≥„É≠„Éº„Éâ](https://mlsysbook.ai/epub)** ‚Ä¢
  **[üåê „Ç®„Ç≥„Ç∑„Çπ„ÉÜ„É†„ÇíÊé¢Ê§ú](https://mlsysbook.org)**

</p>

üìö **2026Âπ¥„Å´MIT Press„Åã„Çâ„Éè„Éº„Éâ„Ç´„Éê„ÉºÁâà„ÅåÂá∫Áâà‰∫àÂÆö**

</div>

---

## „Éü„ÉÉ„Ç∑„Éß„É≥

**‰∏ñÁïå„ÅØAI„Ç∑„Çπ„ÉÜ„É†„ÇíÊÄ•„ÅÑ„Åß‰Ωú„Å£„Å¶„ÅÑ„Åæ„Åô„Åå„ÄÅ„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞„ÅåË∂≥„Çä„Åæ„Åõ„Çì„ÄÇ**

„Åù„Çå„ÅåÁßÅ„Åü„Å°„ÅåË®Ä„ÅÜAI„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞„Åß„Åô„ÄÇ

**AI„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞„ÅØ„ÄÅÂÆü‰∏ñÁïå„ÅßÂäπÁéáÁöÑ„Åß‰ø°È†ºÊÄß„Åå„ÅÇ„Çä„ÄÅÂÆâÂÖ®„ÅßÈ†ë‰∏à„Å™Áü•ËÉΩ„Ç∑„Çπ„ÉÜ„É†„ÇíÊßãÁØâ„Åô„ÇãÂ≠¶Âïè„Åß„Åô„ÄÇÂçò„Å´„É¢„Éá„É´„Çí‰Ωú„Çã„Å†„Åë„Åß„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ**

**ÁßÅ„Åü„Å°„ÅÆ„Éü„ÉÉ„Ç∑„Éß„É≥:** „ÇΩ„Éï„Éà„Ç¶„Çß„Ç¢„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞„Å®„Ç≥„É≥„Éî„É•„Éº„Çø„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞„Å´Âä†„Åà„Å¶„ÄÅAI„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞„ÇíÂü∫Á§éÂ≠¶Âïè„Å®„Åó„Å¶‰ΩçÁΩÆ‰ªò„Åë„ÄÅ„Ç®„É≥„Éâ„ÉÑ„Éº„Ç®„É≥„Éâ„ÅÆÁü•ËÉΩ„Ç∑„Çπ„ÉÜ„É†„ÇíË®≠Ë®à„ÉªÊßãÁØâ„ÉªË©ï‰æ°„Åô„ÇãÊñπÊ≥ï„ÇíÊïô„Åà„Çã„Åì„Å®„Åß„Åô„ÄÇAI„ÅÆÈï∑ÊúüÁöÑ„Å™ÂΩ±Èüø„ÅØ„ÄÅ„Ç¢„Ç§„Éá„Ç¢„ÇíÂÆüÈöõ„Å´Âãï„Åè‰ø°È†º„Åß„Åç„Çã„Ç∑„Çπ„ÉÜ„É†„Å´Â§â„Åà„Çã„Ç®„É≥„Ç∏„Éã„Ç¢„Å´„Çà„Å£„Å¶ÂΩ¢‰Ωú„Çâ„Çå„Åæ„Åô„ÄÇ

---

## „Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„Å´Âê´„Åæ„Çå„Çã„ÇÇ„ÅÆ

„Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅØ„ÄÅAI„Ç∑„Çπ„ÉÜ„É†Â∑•Â≠¶„ÅÆ„Åü„ÇÅ„ÅÆ„Ç™„Éº„Éó„É≥„É©„Éº„Éã„É≥„Ç∞„Çπ„Çø„ÉÉ„ÇØ„Åß„Åô„ÄÇ

„ÉÜ„Ç≠„Çπ„Éà„Éñ„ÉÉ„ÇØ„ÅÆ„ÇΩ„Éº„Çπ„ÄÅTinyTorch„ÄÅ„Éè„Éº„Éâ„Ç¶„Çß„Ç¢„Ç≠„ÉÉ„Éà„ÄÅ„Åù„Åó„Å¶ÂéüÂâá„Å®ÂÆüË°åÂèØËÉΩ„Å™„Ç≥„Éº„Éâ„ÉªÂÆü„Éá„Éê„Ç§„Çπ„ÇíÁµê„Å≥„Å§„Åë„ÇãÂÖ±Âêå„É©„ÉúÔºàco‚ÄëlabsÔºâ„ÇíÂê´„Åø„Åæ„Åô„ÄÇ

---

## Âßã„ÇÅÊñπ

ÁõÆÁöÑ„Å´Âêà„Çè„Åõ„Å¶„Éë„Çπ„ÇíÈÅ∏„Çì„Åß„Åè„Å†„Åï„ÅÑ„ÄÇ

**READ** [„ÉÜ„Ç≠„Çπ„Éà„Éñ„ÉÉ„ÇØ](https://mlsysbook.ai)„Åã„ÇâÂßã„ÇÅ„Åæ„Åô„ÄÇ„Åæ„Åö„ÅØ[Chapter 1](https://www.mlsysbook.ai/contents/core/introduction/introduction.html)„Å®[Benchmarking chapter](https://mlsysbook.ai/contents/core/benchmarking/benchmarking.html)„ÇíË¶ã„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

**BUILD** [Getting Started guide](https://mlsysbook.ai/tinytorch/getting-started.html)„Å´Âæì„Å£„Å¶TinyTorch„ÇíÂßã„ÇÅ„Åæ„Åô„ÄÇModule 01„Åã„ÇâÂßã„ÇÅ„Å¶CNN„ÄÅTransformer„ÄÅMLPerf„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Å∏ÈÄ≤„Åø„Åæ„Åô„ÄÇ

**DEPLOY** [„Éè„Éº„Éâ„Ç¶„Çß„Ç¢„Ç≠„ÉÉ„Éà](https://mlsysbook.ai/kits)„ÇíÈÅ∏„Å≥„ÄÅArduino„ÇÑRaspberry Pi„Å™„Å©„ÅÆ„Ç®„ÉÉ„Ç∏„Éá„Éê„Ç§„Çπ„ÅßÂÆüÈ®ì„Åó„Åæ„Åô„ÄÇ

**CONNECT** [Discussions](https://github.com/harvard-edge/cs249r_book/discussions)„ÅßÊå®Êã∂„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„Åß„Åç„Çã„Å†„ÅëÊó©„ÅèËøî‰ø°„Åó„Åæ„Åô„ÄÇ

---

## Â≠¶Áøí„Çπ„Çø„ÉÉ„ÇØ

‰ª•‰∏ã„ÅÆÂõ≥„ÅØ„ÄÅ„ÉÜ„Ç≠„Çπ„Éà„Éñ„ÉÉ„ÇØ„Åå„Éè„É≥„Ç∫„Ç™„É≥„ÇÑ„Éá„Éó„É≠„Ç§„Å®„Å©„ÅÆ„Çà„ÅÜ„Å´Áµê„Å≥„Å§„Åè„Åã„ÇíÁ§∫„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„ÉÜ„Ç≠„Çπ„Éà„Éñ„ÉÉ„ÇØ„ÇíË™≠„Çì„Åß„ÄÅÂ•Ω„Åç„Å™„Éë„Çπ„ÇíÈÅ∏„Çì„Åß„Åè„Å†„Åï„ÅÑ:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                               ‚îÇ
‚îÇ                           MACHINE LEARNING SYSTEMS                            ‚îÇ
‚îÇ                              Read the Textbook                                ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îÇ                    Theory ‚Ä¢ Concepts ‚Ä¢ Best Practices                         ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                        ‚îÇ
                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                          ‚îÇ             ‚îÇ             ‚îÇ
                          ‚ñº             ‚ñº             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                            HANDS‚ÄëON ACTIVITIES                                ‚îÇ
‚îÇ                           (pick one or all)                                   ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ     ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ    SOFTWARE     ‚îÇ      ‚îÇ    TINYTORCH    ‚îÇ      ‚îÇ    HARDWARE     ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ    CO‚ÄëLABS      ‚îÇ      ‚îÇ    FRAMEWORK    ‚îÇ      ‚îÇ      LABS       ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ EXPLORE         ‚îÇ      ‚îÇ BUILD           ‚îÇ      ‚îÇ DEPLOY          ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ Run controlled  ‚îÇ      ‚îÇ Understand      ‚îÇ      ‚îÇ Engineer under  ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ experiments on  ‚îÇ      ‚îÇ frameworks by   ‚îÇ      ‚îÇ real constraints‚îÇ     ‚îÇ
‚îÇ     ‚îÇ latency, memory,‚îÇ      ‚îÇ implementing    ‚îÇ      ‚îÇ memory, power,  ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ energy, cost    ‚îÇ      ‚îÇ them            ‚îÇ      ‚îÇ timing, safety  ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ
‚îÇ     ‚îÇ (coming 2026)   ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ Arduino, Pi     ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îÇ           EXPLORE                  BUILD                   DEPLOY             ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                        ‚îÇ
                                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                               ‚îÇ
‚îÇ                                  AI OLYMPICS                                  ‚îÇ
‚îÇ                                 Prove Mastery                                 ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îÇ       Compete across all tracks ‚Ä¢ University teams ‚Ä¢ Public leaderboards      ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îÇ                                (coming 2026)                                  ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

|   | Component | What You Do | Link |
|---|-----------|-------------|------|
| **READ** | [üìñ „ÉÜ„Ç≠„Çπ„Éà„Éñ„ÉÉ„ÇØ](https://mlsysbook.ai) | ML„Ç∑„Çπ„ÉÜ„É†„ÅÆÊ¶ÇÂøµ„ÇíÁêÜËß£ | [book/](book/README.md) |
| **EXPLORE** | üîÆ Software Co‚ÄëLabs | „É¨„Ç§„ÉÜ„É≥„Ç∑„Éª„É°„É¢„É™„Éª„Ç®„Éç„É´„ÇÆ„Éº„Éª„Ç≥„Çπ„Éà„ÅÆÂÆüÈ®ì | *Coming 2026* |
| **BUILD** | [üî• TinyTorch](https://mlsysbook.ai/tinytorch) | „Éï„É¨„Éº„É†„ÉØ„Éº„ÇØÂÆüË£Ö„Çí‰ΩìÈ®ì | [tinytorch/](tinytorch/README.md) |
| **DEPLOY** | [üîß Hardware Kits](https://mlsysbook.ai/kits) | „É°„É¢„É™„ÉªÈõªÂäõ„ÉªÊôÇÈñì„ÉªÂÆâÂÖ®ÊÄß„ÅÆÂà∂Á¥Ñ‰∏ã„Åß„Éè„Éº„Éâ„Ç¶„Çß„Ç¢„Çí„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞ | [kits/](kits/README.md) |
| **PROVE** | üèÜ AI Olympics | „Åô„Åπ„Å¶„ÅÆ„Éà„É©„ÉÉ„ÇØ„ÅßÁ´∂‰∫â„Éª„Éô„É≥„ÉÅ„Éû„Éº„ÇØ | *Coming 2026* |

**ÂêÑ„Éë„Çπ„ÅåÊïô„Åà„Çã„Åì„Å®:**
- **EXPLORE**„ÅØ *„Å™„Åú* ‚Äî „Éà„É¨„Éº„Éâ„Ç™„Éï„ÇíÁêÜËß£„ÄÇ„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„ÉªÁ≤æÂ∫¶„Éª„É¢„Éá„É´ÊßãÈÄ†„ÇíÂ§â„Åà„Çã„Å®„É¨„Ç§„ÉÜ„É≥„Ç∑„Éª„É°„É¢„É™„ÉªÁ≤æÂ∫¶„Åå„Å©„ÅÜÂ§â„Çè„Çã„Åã„ÇíÁ¢∫Ë™ç„ÄÇ
- **BUILD**„ÅØ *„Å©„ÅÜ„ÇÑ„Å£„Å¶* ‚Äî ÂÜÖÈÉ®ÊßãÈÄ†„ÇíÁêÜËß£„ÄÇautograd„ÄÅoptimizer„ÄÅattention „ÇíËá™ÂàÜ„ÅßÂÆüË£Ö„Åó„Å¶ TensorFlow„ÉªPyTorch „ÅåÂÆüÈöõ„Å´„Å©„ÅÜÂãï„Åè„Åã‰ΩìÊÑü„ÄÇ
- **DEPLOY**„ÅØ *„Å©„Åì„Åß* ‚Äî Âà∂Á¥ÑÊù°‰ª∂„ÇíÁêÜËß£„ÄÇÂÆüÈöõ„ÅÆ„É°„É¢„É™‰∏äÈôê„ÉªÈõªÂäõ‰∫àÁÆó„Éª„É¨„Ç§„ÉÜ„É≥„Ç∑Ë¶Å‰ª∂„ÇíÊåÅ„Å§„Éè„Éº„Éâ„Ç¶„Çß„Ç¢„ÅßÂÆüÈ®ì„ÄÇ

---

## Â≠¶„Åπ„Çã„Åì„Å®

„Åì„ÅÆÊïôÁßëÊõ∏„ÅØ„ÄÅÊ©üÊ¢∞Â≠¶Áøí„Å®„Ç∑„Çπ„ÉÜ„É†Â∑•Â≠¶„ÅÆ‰∫§Â∑ÆÁÇπ„ÇíËÄÉ„Åà„ÇãÊñπÊ≥ï„ÇíÊïô„Åà„Åæ„Åô„ÄÇÂêÑÁ´†„ÅØ„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅÆÊ¶ÇÂøµ„Å®„Åù„Çå„ÇíÂÆüÈöõ„Å´Âãï„Åã„Åô„Ç§„É≥„Éï„É©„ÇíÁµê„Å≥„Å§„Åë„Åæ„Åô„ÄÇ

### ML ‚Üî Systems Bridge

| ML Concept | Systems Concept | What You Learn |
|------------|-----------------|----------------|
| Model parameters | Memory constraints | Èôê„Çâ„Çå„Åü„É™„ÇΩ„Éº„Çπ„Éá„Éê„Ç§„Çπ„Å´Â§ßË¶èÊ®°„É¢„Éá„É´„Çí„Å©„ÅÜÂêà„Çè„Åõ„Çã„Åã |
| Inference latency | Hardware acceleration | GPU„ÄÅTPU„ÄÅ„Ç¢„ÇØ„Çª„É©„É¨„Éº„Çø„Åå„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„Çí„Å©„ÅÜÂÆüË°å„Åô„Çã„Åã |
| Training convergence | Compute efficiency | Ê∑∑ÂêàÁ≤æÂ∫¶„ÉªÊúÄÈÅ©ÂåñÊâãÊ≥ï„Åß„Ç≥„Çπ„Éà„ÇíÂâäÊ∏õ„Åô„ÇãÊñπÊ≥ï |
| Model accuracy | Quantization and pruning | ÊÄßËÉΩ„Çí‰øù„Å°„Å§„Å§„É¢„Éá„É´„ÇíÂúßÁ∏Æ„Åô„ÇãÊñπÊ≥ï |
| Data requirements | Pipeline infrastructure | ÂäπÁéáÁöÑ„Å™„Éá„Éº„Çø„É≠„Éº„Éâ„ÉªÂâçÂá¶ÁêÜ„Éë„Ç§„Éó„É©„Ç§„É≥„ÅÆÊßãÁØâÊñπÊ≥ï |
| Model deployment | MLOps practices | „Éó„É≠„ÉÄ„ÇØ„Ç∑„Éß„É≥„Åß„É¢„Éá„É´„ÇíÁõ£Ë¶ñ„Éª„Éê„Éº„Ç∏„Éß„É≥ÁÆ°ÁêÜ„ÉªÊõ¥Êñ∞„Åô„ÇãÊñπÊ≥ï |
| Privacy constraints | On‚Äëdevice learning | „Éá„Éº„Çø„Çí„ÇØ„É©„Ç¶„Éâ„Å´ÈÄÅ„Çâ„Åö„Å´Â≠¶Áøí„ÉªÈÅ©Âøú„Åô„ÇãÊñπÊ≥ï |

### Êú¨„ÅÆÊßãÊàê

| Part | Focus | Chapters |
|------|-------|----------|
| **I. Foundations** | Âü∫Á§éÊ¶ÇÂøµ | Introduction, ML Systems, DL Primer, Architectures |
| **II. Design** | „Éì„É´„Éá„Ç£„É≥„Ç∞„Éñ„É≠„ÉÉ„ÇØ | Workflow, Data Engineering, Frameworks, Training |
| **III. Performance** | È´òÈÄüÂåñ | Efficient AI, Optimizations, HW Acceleration, Benchmarking |
| **IV. Deployment** | ÂÆüË£Ö | MLOps, On‚Äëdevice Learning, Privacy, Robustness |
| **V. Trust** | Ê≠£„Åó„Åè‰Ωú„Çã | Responsible AI, Sustainable AI, AI for Good |
| **VI. Frontiers** | Ê¨°„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó | Emerging trends and future directions |

---

## ÁâπÂæ¥

„Åì„ÅÆÊú¨„ÅØ„ÄåÁîü„Åç„Å¶„ÅÑ„Çã„ÄçÊïôÁßëÊõ∏„Åß„Åô„ÄÇÂàÜÈáé„ÅåÊàêÈï∑„Åô„Çå„Å∞Á∂ôÁ∂öÁöÑ„Å´Êõ¥Êñ∞„Åó„ÄÅ„Ç≥„Éü„É•„Éã„ÉÜ„Ç£„ÅÆÊÑèË¶ã„ÇíÂèñ„ÇäÂÖ•„Çå„Åæ„Åô„ÄÇ

AI„ÅØÁ®≤Â¶ª„ÅÆ„Çà„ÅÜ„Å´ÈÄü„ÅèÂ§â„Çè„Çä„Åæ„Åô„Åå„ÄÅ„Åù„Çå„ÇíÂãï„Åã„Åô„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞„Éñ„É≠„ÉÉ„ÇØ„ÅØË¶ãÂá∫„Åó„Åª„Å©ÈÄü„Åè„ÅØÂ§â„Çè„Çä„Åæ„Åõ„Çì„ÄÇ„Åì„ÅÆ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅØ„Åù„ÅÆÂÆâÂÆö„Åó„ÅüÂü∫Áõ§„ÅÆ‰∏ä„Å´ÁØâ„Åã„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ

„É¨„Ç¥„ÇíÊÄù„ÅÑÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÊñ∞„Åó„ÅÑ„Çª„ÉÉ„Éà„ÅåÊ¨°„ÄÖÂá∫„Åæ„Åô„Åå„ÄÅ„Éñ„É≠„ÉÉ„ÇØËá™‰Ωì„ÅØÂ§â„Çè„Çä„Åæ„Åõ„Çì„ÄÇ„Éñ„É≠„ÉÉ„ÇØ„ÅÆÁµÑ„ÅøÂêà„Çè„ÅõÊñπ„ÇíÂ≠¶„Åπ„Å∞‰Ωï„Åß„ÇÇ‰Ωú„Çå„Åæ„Åô„ÄÇ„Åì„Åì„Åß„ÅÆ "AI„Éñ„É≠„ÉÉ„ÇØ" „ÅØ„ÄÅAI„ÇíÂãï„Åã„ÅôÂ†ÖÂõ∫„Å™„Ç∑„Çπ„ÉÜ„É†ÂéüÂâá„Åß„Åô„ÄÇ

Ë™≠„Çì„Å†„Çä„ÄÅÂÆüÈ®ì„Åó„Åü„Çä„ÄÅ„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ„Åó„Åü„Çä„Åô„Çã„Åì„Å®„Åß„ÄÅÊ¨°„ÅÆÂ≠¶ÁøíËÄÖ„Å∏„ÅÆ„Ç¢„ÇØ„Çª„ÇπÊÄß„ÇíÈ´ò„ÇÅ„ÇãÊâãÂä©„Åë„Çí„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

### Research to Teaching Loop

Á†îÁ©∂„Å®ÊïôËÇ≤„ÇíÂêå„Åò„É´„Éº„Éó„Åß‰Ωø„ÅÑ„Åæ„Åô: „Ç∑„Çπ„ÉÜ„É†Ë™≤È°åÂÆöÁæ© ‚Üí ÂèÇËÄÉÂÆüË£ÖÊßãÁØâ ‚Üí „Éô„É≥„ÉÅ„Éû„Éº„ÇØ ‚Üí „Ç´„É™„Ç≠„É•„É©„É†„Éª„ÉÑ„Éº„É´Âåñ ‚Üí ‰ªñËÄÖ„ÅåÂÜçÁèæ„ÉªÊã°Âºµ„Åß„Åç„Çã„Çà„ÅÜ„Å´„ÄÇ

| Loop Step | Research Artifacts | Teaching Artifacts |
|-----------|-------------------|-------------------|
| **Measure** | Benchmarks, suites, metrics | Benchmarking chapter, assignments |
| **Build** | Reference systems, compilers, runtimes | TinyTorch modules, co‚Äëlabs |
| **Deploy** | Hardware targets, constraints, reliability | Hardware labs, kits |

---

## ÊîØÊè¥„ÅÆ„ÅäÈ°ò„ÅÑ

ÁßÅ„Åü„Å°„ÅØ **2030Âπ¥„Åæ„Åß„Å´100‰∏á‰∫∫„ÅÆÂ≠¶ÁøíËÄÖ** „ÇíÁõÆÊåá„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇAI„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞„ÇíÂ≠§Á´ã„Åó„ÅüÊÖ£‰æã„Åß„ÅØ„Å™„Åè„ÄÅÂÖ±Êúâ„Åß„Åç„ÇãÂ≠¶Âïè„Å´„Åô„Çã„Åü„ÇÅ„Åß„Åô„ÄÇ„Çπ„Çø„Éº„ÄÅ„Ç∑„Çß„Ç¢„ÄÅË≤¢ÁåÆ„ÅØ„Åô„Åπ„Å¶„Åì„ÅÆÂãï„Åç„ÇíÂä†ÈÄü„Åï„Åõ„Åæ„Åô„ÄÇ

### „Å™„ÅúGitHub Stars„ÅåÈáçË¶Å„Åã?

<div align="center">

*Ê∏¨ÂÆö„Åï„Çå„Åü„ÇÇ„ÅÆ„ÅØÊîπÂñÑ„Åï„Çå„Çã„ÄÇ*

ÂêÑ„Çπ„Çø„Éº„ÅØ„ÄÅAI„Ç∑„Çπ„ÉÜ„É†„ÇíÂé≥ÂØÜ„Åã„Å§ÂÆü‰∏ñÁïå„ÅÆÂà∂Á¥Ñ„ÇíËÄÉÊÖÆ„Åó„Å¶„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞„Åô„Åπ„Åç„Å†„Å®‰ø°„Åò„ÇãÂ≠¶ÁøíËÄÖ„ÉªÊïôËÇ≤ËÄÖ„ÉªÊîØÊè¥ËÄÖ„Åß„Åô„ÄÇ

[![Stars](https://img.shields.io/github/stars/harvard-edge/cs249r_book?style=for-the-badge&logo=github&color=gold)](https://github.com/harvard-edge/cs249r_book/stargazers)

[![Star History Chart](https://api.star-history.com/svg?repos=harvard-edge/cs249r_book&type=Date)](https://star-history.com/#harvard-edge/cs249r_book&Date)

1 Â≠¶ÁøíËÄÖ ‚Üí 10 Â≠¶ÁøíËÄÖ ‚Üí 100 Â≠¶ÁøíËÄÖ ‚Üí 1,000 Â≠¶ÁøíËÄÖ ‚Üí **10,000 Â≠¶ÁøíËÄÖ** ‚Üí 100,000 Â≠¶ÁøíËÄÖ ‚Üí **1M Â≠¶ÁøíËÄÖ**

</div>

Stars„ÅØÁõÆÊ®ô„Åß„ÅØ„Å™„Åè„Ç∑„Ç∞„Éä„É´„Åß„Åô„ÄÇ

ÂèØË¶ñÁöÑ„Å™„Ç≥„Éü„É•„Éã„ÉÜ„Ç£„ÅØ„ÄÅÂ§ßÂ≠¶„ÉªË≤°Âõ£„ÉªÁî£Ê•≠„Éë„Éº„Éà„Éä„Éº„Åå„Åì„ÅÆË≥áÊñô„ÇíÊé°Áî®„Éª„Éè„Éº„Éâ„Ç¶„Çß„Ç¢„ÇíÂØÑ‰ªò„Éª„ÉØ„Éº„ÇØ„Ç∑„Éß„ÉÉ„Éó„ÇíÊîØÊè¥„Åó„ÇÑ„Åô„Åè„Åó„ÄÅ„Åù„ÅÆÁµêÊûú„ÅØÊ¨°‰∏ñ‰ª£„ÅÆÊïôÂÆ§„Éª„Ç≥„Éõ„Éº„Éà„ÉªÂ≠¶ÁøíËÄÖ„Å∏„ÅÆ„Éè„Éº„Éâ„É´„Çí‰∏ã„Åí„Åæ„Åô„ÄÇ

ÊîØÊè¥Èáë„ÅØ [Open Collective](https://opencollective.com/mlsysbook) „Å´ÊµÅ„Çå„ÄÅTinyML4D „ÉØ„Éº„ÇØ„Ç∑„Éß„ÉÉ„Éó„ÉªÊÅµ„Åæ„Çå„Å™„ÅÑÊïôÂÆ§Âêë„Åë„Éè„Éº„Éâ„Ç¶„Çß„Ç¢„Ç≠„ÉÉ„Éà„ÉªÁÑ°Êñô„Éª„Ç™„Éº„Éó„É≥„É™„ÇΩ„Éº„Çπ„ÅÆÁ∂≠ÊåÅ„Å´‰Ωø„Çè„Çå„Åæ„Åô„ÄÇ

„ÉØ„É≥„ÇØ„É™„ÉÉ„ÇØ„ÅßÊ¨°„ÅÆÊïôÂÆ§„ÉªÊ¨°„ÅÆË≤¢ÁåÆËÄÖ„ÉªÊ¨°„ÅÆAI„Ç®„É≥„Ç∏„Éã„Ç¢‰∏ñ‰ª£„ÇíÈñã„Åè„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ

### „Éü„ÉÉ„Ç∑„Éß„É≥„Å∏„ÅÆÂØÑ‰ªò

<div align="center">

All contributions go to [Open Collective](https://opencollective.com/mlsysbook), a transparent fund that supports educational outreach.

[![Open Collective](https://img.shields.io/badge/üíù%20Support%20AI%20Education-Open%20Collective-blue.svg?style=for-the-badge)](https://opencollective.com/mlsysbook)

</div>

---

## „Ç≥„Éü„É•„Éã„ÉÜ„Ç£„Å®„É™„ÇΩ„Éº„Çπ

| Resource | Description |
|---|---|
| [üìñ **„ÉÜ„Ç≠„Çπ„Éà„Éñ„ÉÉ„ÇØ**](https://mlsysbook.ai) | „Ç§„É≥„Çø„É©„ÇØ„ÉÜ„Ç£„Éñ„Å™„Ç™„É≥„É©„Ç§„É≥ÊïôÁßëÊõ∏ |
| [üî• **TinyTorch**](https://mlsysbook.ai/tinytorch) | ML„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„ÇíÊúÄÂàù„Åã„ÇâÂÆüË£Ö |
| [üîß **Hardware Kits**](https://mlsysbook.ai/kits) | Arduino„ÄÅRaspberry Pi„ÄÅ„Ç®„ÉÉ„Ç∏„Éá„Éê„Ç§„Çπ„Å∏„Éá„Éó„É≠„Ç§ |
| [üåê **Ecosystem**](https://mlsysbook.org) | „É™„ÇΩ„Éº„Çπ„Éª„ÉØ„Éº„ÇØ„Ç∑„Éß„ÉÉ„Éó„Éª„Ç≥„Éü„É•„Éã„ÉÜ„Ç£ |
| [üí¨ **Discussions**](https://github.com/harvard-edge/cs249r_book/discussions) | Ë≥™Âïè„Éª„Ç¢„Ç§„Éá„Ç¢ |

---

## „Ç≥„É≥„Éà„É™„Éì„É•„Éº„Ç∑„Éß„É≥

ÁßÅ„Åü„Å°„ÅØÊïôÁßëÊõ∏„ÉªTinyTorch„Éª„Éè„Éº„Éâ„Ç¶„Çß„Ç¢„Ç≠„ÉÉ„Éà„Å∏„ÅÆË≤¢ÁåÆ„ÇíÊ≠ìËøé„Åó„Åæ„ÅôÔºÅ

| I want to‚Ä¶ | Go here |
|--------------|---------|
| Ë™§Â≠ó‰øÆÊ≠£„ÉªÁ´†ÊîπÂñÑ | [book/docs/CONTRIBUTING.md](book/docs/CONTRIBUTING.md) |
| TinyTorch „É¢„Ç∏„É•„Éº„É´ËøΩÂä†„Éª„Éê„Ç∞‰øÆÊ≠£ | [tinytorch/CONTRIBUTING.md](tinytorch/CONTRIBUTING.md) |
| „Éè„Éº„Éâ„Ç¶„Çß„Ç¢ÂÆüÈ®ìÊîπÂñÑ | [kits/README.md](kits/README.md) |
| Issue Â†±Âëä | [GitHub Issues](https://github.com/harvard-edge/cs249r_book/issues) |
| Ë≥™Âïè | [GitHub Discussions](https://github.com/harvard-edge/cs249r_book/discussions) |

---

## ÂºïÁî®„Å®„É©„Ç§„Çª„É≥„Çπ

### ÂºïÁî®
```bibtex
@inproceedings{reddi2024mlsysbook,
  title        = {MLSysBook.AI: Principles and Practices of Machine Learning Systems Engineering},
  author       = {Reddi, Vijay Janapa},
  booktitle    = {2024 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ ISSS)},
  pages        = {41--42},
  year         = {2024},
  organization = {IEEE},
  url          = {https://mlsysbook.org}
}
```

### „É©„Ç§„Çª„É≥„Çπ

„Åì„ÅÆ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅØ‰∫åÈáç„É©„Ç§„Çª„É≥„ÇπÊßãÈÄ†„Çí‰ΩøÁî®„Åó„Åæ„Åô:

| Component | License | What It Means |
|-----------|---------|---------------|
| **Book content** | [CC BY‚ÄëNC‚ÄëND 4.0](LICENSE.md) | Âá∫ÂÖ∏Ë°®Á§∫„ÉªÈùûÂñ∂Âà©„ÉªÊîπÂ§âÁ¶ÅÊ≠¢„ÅÆÊù°‰ª∂„ÅßËá™Áî±ÈÖçÂ∏É |
| **TinyTorch code** | [Apache 2.0](tinytorch/LICENSE) | Ëá™Áî±‰ΩøÁî®„Éª‰øÆÊ≠£„ÉªÈÖçÂ∏É„ÉªÁâπË®±‰øùË≠∑Âê´„ÇÄ |

„ÉÜ„Ç≠„Çπ„Éà„Éñ„ÉÉ„ÇØ„ÅÆÂÜÖÂÆπÔºàÁ´†„ÉªÂõ≥„ÉªËß£Ë™¨Ôºâ„ÅØÊïôËÇ≤Ë≥áÊñô„Åß„ÅÇ„Çä„ÄÅÂá∫ÂÖ∏Ë°®Á§∫„Å®ÈùûÂñ∂Âà©Âà©Áî®„ÇíÂâçÊèê„Å´Ëá™Áî±„Å´ÂÖ±Êúâ„Åß„Åç„Åæ„Åô„ÄÇ„ÇΩ„Éï„Éà„Ç¶„Çß„Ç¢„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„ÅØË™∞„Åß„ÇÇ‰ΩøÁî®„Éª‰øÆÊ≠£„ÉªÁµ±Âêà„Åß„Åç„Çã„Çà„ÅÜË®≠Ë®à„Åï„Çå„Åü„ÉÑ„Éº„É´„Åß„Åô„ÄÇ

---

## Ë≤¢ÁåÆËÄÖ

‰ª•‰∏ã„ÅÆÁ¥†Êô¥„Çâ„Åó„ÅÑÊñπ„ÄÖ„Åå„Åì„ÅÆ„É™„ÇΩ„Éº„Çπ„Çí„Çà„ÇäËâØ„Åè„Åô„Çã„Åü„ÇÅ„Å´Ë≤¢ÁåÆ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„Åæ„Åó„Åü:

<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->
<!-- ... (contributors omitted for brevity) -->
<!-- ALL-CONTRIBUTORS-LIST:END -->

---

<div align="center">

**[‚≠ê GitHub„Åß„Çπ„Çø„Éº„Çí‰ªò„Åë„Çã](https://github.com/harvard-edge/cs249r_book#support-this-work) ‚Ä¢ [‚úâÔ∏è Ë≥ºË™≠„Åô„Çã](https://buttondown.email/mlsysbook) ‚Ä¢ [üí¨ „Éá„Ç£„Çπ„Ç´„ÉÉ„Ç∑„Éß„É≥„Å´ÂèÇÂä†](https://github.com/harvard-edge/cs249r_book/discussions) ‚Ä¢ [üåê mlsysbook.ai „ÇíË®™Âïè](https://mlsysbook.ai)**

*MLSysBook „Ç≥„Éü„É•„Éã„ÉÜ„Ç£„ÅÆÁåÆË∫´„Å´„Çà„Å£„Å¶‰Ωú„Çâ„Çå„Åæ„Åó„Åü„ÄÇ*

</div>


## Links discovered
- [![Book](https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/book-validate-dev.yml?branch=dev&label=Book&logo=githubactions&cacheSeconds=300)
- [![TinyTorch](https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/tinytorch-validate-dev.yml?branch=dev&label=TinyTorch&logo=python&cacheSeconds=300)
- [Updated](https://img.shields.io/github/last-commit/harvard-edge/cs249r_book/dev?label=Updated&logo=git&cacheSeconds=300)
- [![License](https://img.shields.io/badge/License-CC--BY--NC--ND%204.0-blue.svg)
- [![Cite](https://img.shields.io/badge/Cite-IEEE%202024-blue?logo=ieee)
- [![Fund Us](https://img.shields.io/badge/Fund%20Us-Open%20Collective-blue.svg?logo=open-collective)
- [üìñ „Ç™„É≥„É©„Ç§„É≥„ÅßË™≠„ÇÄ](https://mlsysbook.ai)
- [Tinyüî•Torch](https://mlsysbook.ai/tinytorch)
- [üìÑ PDF „ÉÄ„Ç¶„É≥„É≠„Éº„Éâ](https://mlsysbook.ai/assets/downloads/Machine-Learning-Systems.pdf)
- [üìì EPUB „ÉÄ„Ç¶„É≥„É≠„Éº„Éâ](https://mlsysbook.ai/epub)
- [üåê „Ç®„Ç≥„Ç∑„Çπ„ÉÜ„É†„ÇíÊé¢Ê§ú](https://mlsysbook.org)
- [„ÉÜ„Ç≠„Çπ„Éà„Éñ„ÉÉ„ÇØ](https://mlsysbook.ai)
- [Chapter 1](https://www.mlsysbook.ai/contents/core/introduction/introduction.html)
- [Benchmarking chapter](https://mlsysbook.ai/contents/core/benchmarking/benchmarking.html)
- [Getting Started guide](https://mlsysbook.ai/tinytorch/getting-started.html)
- [„Éè„Éº„Éâ„Ç¶„Çß„Ç¢„Ç≠„ÉÉ„Éà](https://mlsysbook.ai/kits)
- [Discussions](https://github.com/harvard-edge/cs249r_book/discussions)
- [üìñ „ÉÜ„Ç≠„Çπ„Éà„Éñ„ÉÉ„ÇØ](https://mlsysbook.ai)
- [book/](https://github.com/harvard-edge/cs249r_book/blob/dev/README/book/README.md)
- [üî• TinyTorch](https://mlsysbook.ai/tinytorch)
- [tinytorch/](https://github.com/harvard-edge/cs249r_book/blob/dev/README/tinytorch/README.md)
- [üîß Hardware Kits](https://mlsysbook.ai/kits)
- [kits/](https://github.com/harvard-edge/cs249r_book/blob/dev/README/kits/README.md)
- [![Stars](https://img.shields.io/github/stars/harvard-edge/cs249r_book?style=for-the-badge&logo=github&color=gold)
- [![Star History Chart](https://api.star-history.com/svg?repos=harvard-edge/cs249r_book&type=Date)
- [Open Collective](https://opencollective.com/mlsysbook)
- [![Open Collective](https://img.shields.io/badge/üíù%20Support%20AI%20Education-Open%20Collective-blue.svg?style=for-the-badge)
- [üìñ **„ÉÜ„Ç≠„Çπ„Éà„Éñ„ÉÉ„ÇØ**](https://mlsysbook.ai)
- [üî• **TinyTorch**](https://mlsysbook.ai/tinytorch)
- [üîß **Hardware Kits**](https://mlsysbook.ai/kits)
- [üåê **Ecosystem**](https://mlsysbook.org)
- [üí¨ **Discussions**](https://github.com/harvard-edge/cs249r_book/discussions)
- [book/docs/CONTRIBUTING.md](https://github.com/harvard-edge/cs249r_book/blob/dev/README/book/docs/CONTRIBUTING.md)
- [tinytorch/CONTRIBUTING.md](https://github.com/harvard-edge/cs249r_book/blob/dev/README/tinytorch/CONTRIBUTING.md)
- [kits/README.md](https://github.com/harvard-edge/cs249r_book/blob/dev/README/kits/README.md)
- [GitHub Issues](https://github.com/harvard-edge/cs249r_book/issues)
- [GitHub Discussions](https://github.com/harvard-edge/cs249r_book/discussions)
- [CC BY‚ÄëNC‚ÄëND 4.0](https://github.com/harvard-edge/cs249r_book/blob/dev/README/LICENSE.md)
- [Apache 2.0](https://github.com/harvard-edge/cs249r_book/blob/dev/README/tinytorch/LICENSE.md)
- [‚≠ê GitHub„Åß„Çπ„Çø„Éº„Çí‰ªò„Åë„Çã](https://github.com/harvard-edge/cs249r_book#support-this-work)
- [‚úâÔ∏è Ë≥ºË™≠„Åô„Çã](https://buttondown.email/mlsysbook)
- [üí¨ „Éá„Ç£„Çπ„Ç´„ÉÉ„Ç∑„Éß„É≥„Å´ÂèÇÂä†](https://github.com/harvard-edge/cs249r_book/discussions)
- [üåê mlsysbook.ai „ÇíË®™Âïè](https://mlsysbook.ai)
- [English](https://github.com/harvard-edge/cs249r_book/blob/dev/README.md)
- [‰∏≠Êñá](https://github.com/harvard-edge/cs249r_book/blob/dev/README/README_zh.md)
- [Êó•Êú¨Ë™û](https://github.com/harvard-edge/cs249r_book/blob/dev/README/README_ja.md)
- [ÌïúÍµ≠Ïñ¥](https://github.com/harvard-edge/cs249r_book/blob/dev/README/README_ko.md)

--- README/README_ko.md ---
# Î®∏Ïã†Îü¨Îãù ÏãúÏä§ÌÖú
*Ïù∏Í≥µÏßÄÎä• ÏãúÏä§ÌÖú ÏóîÏßÄÎãàÏñ¥ÎßÅÏùò ÏõêÎ¶¨ÏôÄ Ïã§Ï≤ú*

<p align="center">
  <a href="../README.md">English</a> ‚Ä¢
  <a href="README_zh.md">‰∏≠Êñá</a> ‚Ä¢
  <a href="README_ja.md">Êó•Êú¨Ë™û</a> ‚Ä¢
  <a href="README_ko.md">ÌïúÍµ≠Ïñ¥</a>
</p>

<div align="center">

<p align="center">

  [![Book](https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/book-validate-dev.yml?branch=dev&label=Book&logo=githubactions&cacheSeconds=300)](https://github.com/harvard-edge/cs249r_book/actions/workflows/book-validate-dev.yml)
  [![TinyTorch](https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/tinytorch-validate-dev.yml?branch=dev&label=TinyTorch&logo=python&cacheSeconds=300)](https://github.com/harvard-edge/cs249r_book/actions/workflows/tinytorch-validate-dev.yml)
  ![Updated](https://img.shields.io/github/last-commit/harvard-edge/cs249r_book/dev?label=Updated&logo=git&cacheSeconds=300)
  [![License](https://img.shields.io/badge/License-CC--BY--NC--ND%204.0-blue.svg)](https://github.com/harvard-edge/cs249r_book/blob/dev/LICENSE.md)
  [![Cite](https://img.shields.io/badge/Cite-IEEE%202024-blue?logo=ieee)](#-citation--license)
  [![Fund Us](https://img.shields.io/badge/Fund%20Us-Open%20Collective-blue.svg?logo=open-collective)](https://opencollective.com/mlsysbook)

</p>

<p align="center">

  <!-- Reader Navigation -->
  **[üìñ Ïò®ÎùºÏù∏ ÏùΩÍ∏∞](https://mlsysbook.ai)** ‚Ä¢
  **[Tinyüî•Torch](https://mlsysbook.ai/tinytorch)** ‚Ä¢
  **[üìÑ PDF Îã§Ïö¥Î°úÎìú](https://mlsysbook.ai/assets/downloads/Machine-Learning-Systems.pdf)** ‚Ä¢
  **[üìì EPUB Îã§Ïö¥Î°úÎìú](https://mlsysbook.ai/epub)** ‚Ä¢
  **[üåê ÏÉùÌÉúÍ≥Ñ ÌÉêÌóò](https://mlsysbook.org)**

</p>

üìö **2026ÎÖÑ MIT PressÏóêÏÑú ÌïòÎìúÏª§Î≤Ñ Ï∂úÌåê ÏòàÏ†ï**

</div>

---

## ÎØ∏ÏÖò

**ÏÑ∏ÏÉÅÏùÄ AI ÏãúÏä§ÌÖúÏùÑ Í∏âÌûà ÎßåÎì§Í≥† ÏûàÏäµÎãàÎã§. ÌïòÏßÄÎßå ÏóîÏßÄÎãàÏñ¥ÎßÅÏùÄ Î∂ÄÏ°±Ìï©ÎãàÎã§.**

Í∑∏ Í≤©Ï∞®Í∞Ä Î∞îÎ°ú Ïö∞Î¶¨Í∞Ä ÎßêÌïòÎäî AI ÏóîÏßÄÎãàÏñ¥ÎßÅÏûÖÎãàÎã§.

**AI ÏóîÏßÄÎãàÏñ¥ÎßÅÏùÄ Ïã§Ï†ú ÏÑ∏Í≥ÑÏóêÏÑú Ìö®Ïú®Ï†ÅÏù¥Í≥†, Ïã†Î¢∞Ìï† Ïàò ÏûàÏúºÎ©∞, ÏïàÏ†ÑÌïòÍ≥†, Í≤¨Í≥†Ìïú ÏßÄÎä•Ìòï ÏãúÏä§ÌÖúÏùÑ Íµ¨Ï∂ïÌïòÎäî ÌïôÎ¨∏ÏûÖÎãàÎã§. Îã®ÏàúÌûà Î™®Îç∏Îßå ÎßåÎìúÎäî Í≤ÉÏù¥ ÏïÑÎãàÎùºÏöî.**

**Ïö∞Î¶¨Ïùò ÎØ∏ÏÖò:** ÏÜåÌîÑÌä∏Ïõ®Ïñ¥ ÏóîÏßÄÎãàÏñ¥ÎßÅÍ≥º Ïª¥Ìì®ÌÑ∞ ÏóîÏßÄÎãàÏñ¥ÎßÅÏóê Ïù¥Ïñ¥ AI ÏóîÏßÄÎãàÏñ¥ÎßÅÏùÑ Í∏∞Î≥∏ ÌïôÎ¨∏ÏúºÎ°ú ÏûêÎ¶¨Îß§ÍπÄÌïòÎèÑÎ°ù, ÏóîÎìú‚ÄëÌà¨‚ÄëÏóîÎìú ÏßÄÎä•Ìòï ÏãúÏä§ÌÖúÏùÑ ÏÑ§Í≥Ñ¬∑Íµ¨Ï∂ï¬∑ÌèâÍ∞ÄÌïòÎäî Î∞©Î≤ïÏùÑ Í∞ÄÎ•¥ÏπòÎäî Í≤ÉÏûÖÎãàÎã§. AIÏùò Ïû•Í∏∞Ï†Å ÏòÅÌñ•ÏùÄ ÏïÑÏù¥ÎîîÏñ¥Î•º Ïã§Ï†ú ÏûëÎèôÌïòÍ≥† Ïã†Î¢∞Ìï† Ïàò ÏûàÎäî ÏãúÏä§ÌÖúÏúºÎ°ú Î∞îÍøÄ Ïàò ÏûàÎäî ÏóîÏßÄÎãàÏñ¥Ïóê ÏùòÌï¥ ÌòïÏÑ±Îê† Í≤ÉÏûÖÎãàÎã§.

---

## Ïù¥ Ï†ÄÏû•ÏÜåÏóê Ìè¨Ìï®Îêú ÎÇ¥Ïö©

Ïù¥ Ï†ÄÏû•ÏÜåÎäî AI ÏãúÏä§ÌÖú ÏóîÏßÄÎãàÏñ¥ÎßÅÏùÑ ÏúÑÌïú Ïò§Ìîà ÌïôÏäµ Ïä§ÌÉùÏûÖÎãàÎã§.

ÌÖçÏä§Ìä∏Î∂Å ÏÜåÏä§, TinyTorch, ÌïòÎìúÏõ®Ïñ¥ ÌÇ§Ìä∏, Í∑∏Î¶¨Í≥† ÏõêÎ¶¨ÏôÄ Ïã§Ìñâ Í∞ÄÎä•Ìïú ÏΩîÎìú¬∑Ïã§Ï†ú Ïû•ÏπòÎ•º Ïó∞Í≤∞ÌïòÎäî Ìñ•ÌõÑ Í≥µÎèô Ïã§Ìóò(co‚Äëlabs) Îì±ÏùÑ Ìè¨Ìï®Ìï©ÎãàÎã§.

---

## ÏãúÏûëÌïòÍ∏∞

Î™©ÌëúÏóê Îî∞Îùº Í≤ΩÎ°úÎ•º ÏÑ†ÌÉùÌïòÏÑ∏Ïöî.

**READ** [ÌÖçÏä§Ìä∏Î∂Å](https://mlsysbook.ai)Î∂ÄÌÑ∞ ÏãúÏûëÌïòÏÑ∏Ïöî. [Chapter 1](https://www.mlsysbook.ai/contents/core/introduction/introduction.html)Í≥º [Benchmarking chapter](https://mlsysbook.ai/contents/core/benchmarking/benchmarking.html)ÏùÑ ÏÇ¥Ìé¥Î≥¥ÏÑ∏Ïöî.

**BUILD** [Getting Started guide](https://mlsysbook.ai/tinytorch/getting-started.html)Î•º Îî∞Îùº TinyTorchÎ•º ÏãúÏûëÌïòÏÑ∏Ïöî. Module 01Î∂ÄÌÑ∞ ÏãúÏûëÌï¥ CNN, Transformer, MLPerf Î≤§ÏπòÎßàÌÅ¨ÍπåÏßÄ ÏßÑÌñâÌï©ÎãàÎã§.

**DEPLOY** [ÌïòÎìúÏõ®Ïñ¥ ÌÇ§Ìä∏](https://mlsysbook.ai/kits)Î•º ÏÑ†ÌÉùÌï¥ Arduino, Raspberry Pi Îì± Ïó£ÏßÄ ÎîîÎ∞îÏù¥Ïä§ÏóêÏÑú Ïã§ÌóòÏùÑ ÏßÑÌñâÌïòÏÑ∏Ïöî.

**CONNECT** [Discussions](https://github.com/harvard-edge/cs249r_book/discussions)ÏóêÏÑú Ïù∏ÏÇ¨Ìï¥ Ï£ºÏÑ∏Ïöî. Í∞ÄÎä•Ìïú Ìïú Îπ†Î•¥Í≤å ÎãµÎ≥ÄÎìúÎ¶¨Í≤†ÏäµÎãàÎã§.

---

## ÌïôÏäµ Ïä§ÌÉù

ÏïÑÎûò Í∑∏Î¶ºÏùÄ ÌÖçÏä§Ìä∏Î∂ÅÏù¥ Ïã§Ïäµ Î∞è Î∞∞Ìè¨ÏôÄ Ïñ¥ÎñªÍ≤å Ïó∞Í≤∞ÎêòÎäîÏßÄÎ•º Î≥¥Ïó¨Ï§çÎãàÎã§. ÌÖçÏä§Ìä∏Î∂ÅÏùÑ ÏùΩÍ≥†, ÏõêÌïòÎäî Í≤ΩÎ°úÎ•º ÏÑ†ÌÉùÌïòÏÑ∏Ïöî:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                               ‚îÇ
‚îÇ                           MACHINE LEARNING SYSTEMS                            ‚îÇ
‚îÇ                              Read the Textbook                                ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îÇ                    Theory ‚Ä¢ Concepts ‚Ä¢ Best Practices                         ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                        ‚îÇ
                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                          ‚îÇ             ‚îÇ             ‚îÇ
                          ‚ñº             ‚ñº             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                            HANDS‚ÄëON ACTIVITIES                                ‚îÇ
‚îÇ                           (pick one or all)                                   ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ     ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ    SOFTWARE     ‚îÇ      ‚îÇ    TINYTORCH    ‚îÇ      ‚îÇ    HARDWARE     ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ    CO‚ÄëLABS      ‚îÇ      ‚îÇ    FRAMEWORK    ‚îÇ      ‚îÇ      LABS       ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ EXPLORE         ‚îÇ      ‚îÇ BUILD           ‚îÇ      ‚îÇ DEPLOY          ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ Run controlled  ‚îÇ      ‚îÇ Understand      ‚îÇ      ‚îÇ Engineer under  ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ experiments on  ‚îÇ      ‚îÇ frameworks by   ‚îÇ      ‚îÇ real constraints‚îÇ     ‚îÇ
‚îÇ     ‚îÇ latency, memory,‚îÇ      ‚îÇ implementing    ‚îÇ      ‚îÇ memory, power,  ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ energy, cost    ‚îÇ      ‚îÇ them            ‚îÇ      ‚îÇ timing, safety  ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ
‚îÇ     ‚îÇ (coming 2026)   ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ Arduino, Pi     ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îÇ           EXPLORE                  BUILD                   DEPLOY             ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                        ‚îÇ
                                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                               ‚îÇ
‚îÇ                                  AI OLYMPICS                                  ‚îÇ
‚îÇ                                 Prove Mastery                                 ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îÇ       Compete across all tracks ‚Ä¢ University teams ‚Ä¢ Public leaderboards      ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îÇ                                (coming 2026)                                  ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

|   | Component | What You Do | Link |
|---|-----------|-------------|------|
| **READ** | [üìñ ÌÖçÏä§Ìä∏Î∂Å](https://mlsysbook.ai) | ML ÏãúÏä§ÌÖú Í∞úÎÖê Ïù¥Ìï¥ | [book/](book/README.md) |
| **EXPLORE** | üîÆ Software Co‚ÄëLabs | Î†àÏù¥ÌÑ¥Ïãú¬∑Î©îÎ™®Î¶¨¬∑ÏóêÎÑàÏßÄ¬∑ÎπÑÏö© Ïã§Ìóò | *Coming 2026* |
| **BUILD** | [üî• TinyTorch](https://mlsysbook.ai/tinytorch) | ÌîÑÎ†àÏûÑÏõåÌÅ¨ Íµ¨ÌòÑÏùÑ ÏßÅÏ†ë Í≤ΩÌóò | [tinytorch/](tinytorch/README.md) |
| **DEPLOY** | [üîß Hardware Kits](https://mlsysbook.ai/kits) | Î©îÎ™®Î¶¨¬∑Ï†ÑÎ†•¬∑ÏãúÍ∞Ñ¬∑ÏïàÏ†Ñ Ï†úÏïΩ ÌïòÎìúÏõ®Ïñ¥ ÏóîÏßÄÎãàÏñ¥ÎßÅ | [kits/](kits/README.md) |
| **PROVE** | üèÜ AI Olympics | Î™®Îì† Ìä∏ÎûôÏóêÏÑú Í≤ΩÏüÅ¬∑Î≤§ÏπòÎßàÌÅ¨ | *Coming 2026* |

**Í∞Å Í≤ΩÎ°úÍ∞Ä Í∞ÄÎ•¥ÏπòÎäî ÎÇ¥Ïö©:**
- **EXPLORE**Îäî *Ïôú* ‚Äî Ìä∏Î†àÏù¥ÎìúÏò§ÌîÑ Ïù¥Ìï¥. Î∞∞Ïπò ÌÅ¨Í∏∞¬∑Ï†ïÎ∞ÄÎèÑ¬∑Î™®Îç∏ Íµ¨Ï°∞Î•º Î∞îÍæ∏Î©¥ Î†àÏù¥ÌÑ¥Ïãú¬∑Î©îÎ™®Î¶¨¬∑Ï†ïÌôïÎèÑÍ∞Ä Ïñ¥ÎñªÍ≤å Î≥ÄÌïòÎäîÏßÄ ÌôïÏù∏.
- **BUILD**Îäî *Ïñ¥ÎñªÍ≤å* ‚Äî ÎÇ¥Î∂Ä Íµ¨Ï°∞ Ïù¥Ìï¥. autograd, optimizer, attentionÏùÑ ÏßÅÏ†ë Íµ¨ÌòÑÌï¥ TensorFlow¬∑PyTorchÍ∞Ä Ïã§Ï†úÎ°ú Ïñ¥ÎñªÍ≤å ÎèôÏûëÌïòÎäîÏßÄ Ï≤¥Ìóò.
- **DEPLOY**Îäî *Ïñ¥ÎîîÏÑú* ‚Äî Ï†úÏïΩ Ï°∞Í±¥ Ïù¥Ìï¥. Ïã§Ï†ú Î©îÎ™®Î¶¨ ÌïúÍ≥Ñ¬∑Ï†ÑÎ†• ÏòàÏÇ∞¬∑Î†àÏù¥ÌÑ¥Ïãú ÏöîÍµ¨ÏÇ¨Ìï≠ÏùÑ Í∞ñÎäî ÌïòÎìúÏõ®Ïñ¥ÏóêÏÑú Ïã§Ìóò.

---

## Î∞∞Ïö∏ ÎÇ¥Ïö©

Ïù¥ ÍµêÏû¨Îäî Î®∏Ïã†Îü¨ÎãùÍ≥º ÏãúÏä§ÌÖú ÏóîÏßÄÎãàÏñ¥ÎßÅÏùò ÍµêÏ∞®Ï†êÏùÑ ÏÉùÍ∞ÅÌïòÎèÑÎ°ù Í∞ÄÎ•¥Ïπ©ÎãàÎã§. Í∞Å Ïû•ÏùÄ ÏïåÍ≥†Î¶¨Ï¶ò Í∞úÎÖêÍ≥º Ïù¥Î•º Ïã§Ï†úÎ°ú ÎèôÏûëÌïòÍ≤å ÌïòÎäî Ïù∏ÌîÑÎùºÎ•º Ïó∞Í≤∞Ìï©ÎãàÎã§.

### ML ‚Üî Systems Bridge

| ML Concept | Systems Concept | What You Learn |
|------------|-----------------|----------------|
| Model parameters | Memory constraints | Ï†úÌïúÎêú ÏûêÏõê ÎîîÎ∞îÏù¥Ïä§Ïóê ÌÅ∞ Î™®Îç∏ÏùÑ Ïñ¥ÎñªÍ≤å ÎßûÏ∂úÏßÄ |
| Inference latency | Hardware acceleration | GPU, TPU, Í∞ÄÏÜçÍ∏∞Í∞Ä Ïã†Í≤ΩÎßùÏùÑ Ïñ¥ÎñªÍ≤å Ïã§ÌñâÌïòÎäîÏßÄ |
| Training convergence | Compute efficiency | ÌòºÌï© Ï†ïÎ∞ÄÎèÑ¬∑ÏµúÏ†ÅÌôî Í∏∞Î≤ïÏúºÎ°ú ÎπÑÏö©ÏùÑ Ïñ¥ÎñªÍ≤å Ï§ÑÏù¥ÎäîÏßÄ |
| Model accuracy | Quantization and pruning | ÏÑ±Îä•ÏùÑ Ïú†ÏßÄÌïòÎ©¥ÏÑú Î™®Îç∏ÏùÑ ÏïïÏ∂ïÌïòÎäî Î∞©Î≤ï |
| Data requirements | Pipeline infrastructure | Ìö®Ïú®Ï†ÅÏù∏ Îç∞Ïù¥ÌÑ∞ Î°úÎî©¬∑Ï†ÑÏ≤òÎ¶¨ ÌååÏù¥ÌîÑÎùºÏù∏ Íµ¨Ï∂ï Î∞©Î≤ï |
| Model deployment | MLOps practices | ÌîÑÎ°úÎçïÏÖòÏóêÏÑú Î™®Îç∏ÏùÑ Î™®ÎãàÌÑ∞ÎßÅ¬∑Î≤ÑÏ†Ñ Í¥ÄÎ¶¨¬∑ÏóÖÎç∞Ïù¥Ìä∏ÌïòÎäî Î∞©Î≤ï |
| Privacy constraints | On‚Äëdevice learning | Îç∞Ïù¥ÌÑ∞Î•º ÌÅ¥ÎùºÏö∞ÎìúÏóê Î≥¥ÎÇ¥ÏßÄ ÏïäÍ≥† ÌïôÏäµ¬∑Ï†ÅÏùëÌïòÎäî Î∞©Î≤ï |

### Ï±Ö Íµ¨Ï°∞

| Part | Focus | Chapters |
|------|-------|----------|
| **I. Foundations** | ÌïµÏã¨ Í∞úÎÖê | Introduction, ML Systems, DL Primer, Architectures |
| **II. Design** | ÎπåÎî© Î∏îÎ°ù | Workflow, Data Engineering, Frameworks, Training |
| **III. Performance** | Îπ†Î•¥Í≤å ÎßåÎì§Í∏∞ | Efficient AI, Optimizations, HW Acceleration, Benchmarking |
| **IV. Deployment** | Ïã§Ï†ú Ï†ÅÏö© | MLOps, On‚Äëdevice Learning, Privacy, Robustness |
| **V. Trust** | Ïò¨Î∞îÎ•¥Í≤å ÎßåÎì§Í∏∞ | Responsible AI, Sustainable AI, AI for Good |
| **VI. Frontiers** | Îã§Ïùå Îã®Í≥Ñ | Emerging trends and future directions |

---

## Ï∞®Î≥ÑÏ†ê

Ïù¥ Ï±ÖÏùÄ ÏÇ¥ÏïÑÏûàÎäî ÍµêÏû¨ÏûÖÎãàÎã§. Î∂ÑÏïºÍ∞Ä ÏÑ±Ïû•Ìï®Ïóê Îî∞Îùº ÏßÄÏÜçÏ†ÅÏúºÎ°ú ÏóÖÎç∞Ïù¥Ìä∏ÌïòÍ≥†, Ïª§ÎÆ§ÎãàÌã∞ ÏûÖÎ†•ÏùÑ Î∞òÏòÅÌï©ÎãàÎã§.

AIÎäî Î≤àÍ∞úÏ≤òÎüº Îπ†Î•¥Í≤å Î≥ÄÌïòÏßÄÎßå, Ïù¥Î•º ÏûëÎèôÌïòÍ≤å ÌïòÎäî ÏóîÏßÄÎãàÏñ¥ÎßÅ Î∏îÎ°ùÏùÄ Ìó§ÎìúÎùºÏù∏ÎßåÌÅº Îπ†Î•¥Í≤å Î≥ÄÌïòÏßÄ ÏïäÏäµÎãàÎã§. Ïù¥ ÌîÑÎ°úÏ†ùÌä∏Îäî Í∑∏ ÏïàÏ†ïÏ†ÅÏù∏ Í∏∞Î∞ò ÏúÑÏóê ÏÑ∏ÏõåÏ°åÏäµÎãàÎã§.

Î†àÍ≥†Î•º Îñ†Ïò¨Î†§ Î≥¥ÏÑ∏Ïöî. ÏÉàÎ°úÏö¥ ÏÑ∏Ìä∏Í∞Ä Í≥ÑÏÜç ÎÇòÏò§ÏßÄÎßå, Î∏îÎ°ù ÏûêÏ≤¥Îäî Î≥ÄÌïòÏßÄ ÏïäÏ£†. Î∏îÎ°ù ÎßûÏ∂îÎäî Î≤ïÏùÑ Î∞∞Ïö∞Î©¥ Î¨¥ÏóáÏù¥Îì† ÎßåÎì§ Ïàò ÏûàÏäµÎãàÎã§. Ïó¨Í∏∞ÏÑú "AI Î∏îÎ°ù"ÏùÄ AIÍ∞Ä ÏûëÎèôÌïòÎèÑÎ°ù ÌïòÎäî Í≤¨Í≥†Ìïú ÏãúÏä§ÌÖú ÏõêÏπôÏûÖÎãàÎã§.

ÏùΩÍ∏∞, Ïã§Ìóò, ÌîºÎìúÎ∞±ÏùÑ ÌÜµÌï¥ Îã§Ïùå ÌïôÏäµÏûêÎ•º ÏúÑÌïú Ï†ëÍ∑ºÏÑ±ÏùÑ ÎÜíÏù¥Îäî Îç∞ Ìï®ÍªòÌï¥ Ï£ºÏÑ∏Ïöî.

### Research to Teaching Loop

Ïó∞Íµ¨ÏôÄ ÍµêÏú°ÏùÑ Í∞ôÏùÄ Î£®ÌîÑÎ°ú ÏÇ¨Ïö©Ìï©ÎãàÎã§: ÏãúÏä§ÌÖú Î¨∏Ï†ú Ï†ïÏùò ‚Üí Î†àÌçºÎü∞Ïä§ Íµ¨ÌòÑ Íµ¨Ï∂ï ‚Üí Î≤§ÏπòÎßàÌÅ¨ ‚Üí Ïª§Î¶¨ÌÅòÎüº¬∑Ìà¥ÎßÅÏúºÎ°ú Ï†ÑÌôò ‚Üí Îã§Î•∏ ÏÇ¨ÎûåÎì§Ïù¥ Ïû¨ÌòÑ¬∑ÌôïÏû• Í∞ÄÎä•ÌïòÍ≤å.

| Loop Step | Research Artifacts | Teaching Artifacts |
|-----------|-------------------|-------------------|
| **Measure** | Benchmarks, suites, metrics | Benchmarking chapter, assignments |
| **Build** | Reference systems, compilers, runtimes | TinyTorch modules, co‚Äëlabs |
| **Deploy** | Hardware targets, constraints, reliability | Hardware labs, kits |

---

## Ïù¥ ÏûëÏóÖÏùÑ ÏßÄÏõêÌï¥ Ï£ºÏÑ∏Ïöî

Ïö∞Î¶¨Îäî **2030ÎÖÑÍπåÏßÄ 1Î∞±Îßå Î™ÖÏùò ÌïôÏäµÏûê**Î•º Î™©ÌëúÎ°ú Ìï©ÎãàÎã§. AI ÏóîÏßÄÎãàÏñ¥ÎßÅÏùÑ Í≥†Î¶ΩÎêú Í¥ÄÌñâÏù¥ ÏïÑÎãå Í≥µÏú† Í∞ÄÎä•Ìïú ÌïôÎ¨∏ÏúºÎ°ú ÎßåÎì§Í∏∞ ÏúÑÌï¥Ïöî. Î≥Ñ, Í≥µÏú†, Í∏∞Ïó¨Îäî Î™®Îëê Ïù¥ ÏõÄÏßÅÏûÑÏùÑ Ï¥âÏßÑÌï©ÎãàÎã§.

### Ïôú GitHub StarsÍ∞Ä Ï§ëÏöîÌïúÍ∞Ä?

<div align="center">

*Ï∏°Ï†ïÎêú Í≤ÉÏù¥ Í∞úÏÑ†Îê©ÎãàÎã§.*

Í∞Å Ïä§ÌÉÄÎäî AI ÏãúÏä§ÌÖúÏùÑ ÏóÑÍ≤©ÌïòÍ≥† Ïã§Ï†ú Ï†úÏïΩÏùÑ Í≥†Î†§Ìï¥ ÏóîÏßÄÎãàÏñ¥ÎßÅÌï¥Ïïº ÌïúÎã§Í≥† ÎØøÎäî ÌïôÏäµÏûê¬∑ÍµêÏú°Ïûê¬∑ÏßÄÏõêÏûêÏûÖÎãàÎã§.

[![Stars](https://img.shields.io/github/stars/harvard-edge/cs249r_book?style=for-the-badge&logo=github&color=gold)](https://github.com/harvard-edge/cs249r_book/stargazers)

[![Star History Chart](https://api.star-history.com/svg?repos=harvard-edge/cs249r_book&type=Date)](https://star-history.com/#harvard-edge/cs249r_book&Date)

1 ÌïôÏäµÏûê ‚Üí 10 ÌïôÏäµÏûê ‚Üí 100 ÌïôÏäµÏûê ‚Üí 1,000 ÌïôÏäµÏûê ‚Üí **10,000 ÌïôÏäµÏûê** ‚Üí 100,000 ÌïôÏäµÏûê ‚Üí **1M ÌïôÏäµÏûê**

</div>

StarsÎäî Î™©ÌëúÍ∞Ä ÏïÑÎãàÎùº Ïã†Ìò∏ÏûÖÎãàÎã§.

Í∞ÄÏãúÏ†ÅÏù∏ Ïª§ÎÆ§ÎãàÌã∞Îäî ÎåÄÌïô¬∑Ïû¨Îã®¬∑ÏÇ∞ÏóÖ ÌååÌä∏ÎÑàÍ∞Ä Ïù¥ ÏûêÎ£åÎ•º Ï±ÑÌÉù¬∑ÌïòÎìúÏõ®Ïñ¥Î•º Í∏∞Î∂Ä¬∑ÏõåÌÅ¨ÏàçÏùÑ ÏßÄÏõêÌïòÍ∏∞ ÏâΩÍ≤å ÎßåÎì§Í≥†, Í∑∏ Í≤∞Í≥ºÎäî Ï∞®ÏÑ∏ÎåÄ ÍµêÏã§¬∑ÏΩîÌò∏Ìä∏¬∑ÌïôÏäµÏûêÎ•º ÏúÑÌïú Ïû•Î≤ΩÏùÑ ÎÇÆÏ∂•ÎãàÎã§.

ÏßÄÏõêÍ∏àÏùÄ [Open Collective](https://opencollective.com/mlsysbook)Î°ú ÌùêÎ•¥Í≥†, TinyML4D ÏõåÌÅ¨Ïàç¬∑ÏÜåÏô∏Îêú ÍµêÏã§Ïö© ÌïòÎìúÏõ®Ïñ¥ ÌÇ§Ìä∏¬∑Î¨¥Î£å¬∑Ïò§Ìîà Î¶¨ÏÜåÏä§ Ïú†ÏßÄÏóê Ïì∞ÏûÖÎãàÎã§.

Ìïú Î≤à ÌÅ¥Î¶≠ÏúºÎ°ú Îã§Ïùå ÍµêÏã§¬∑Îã§Ïùå Í∏∞Ïó¨Ïûê¬∑Îã§Ïùå AI ÏóîÏßÄÎãàÏñ¥ ÏÑ∏ÎåÄÎ•º Ïó¥ Ïàò ÏûàÏäµÎãàÎã§.

### ÏÇ¨Î™ÖÏùÑ ÏúÑÌïú Í∏∞Î∂Ä

<div align="center">

All contributions go to [Open Collective](https://opencollective.com/mlsysbook), a transparent fund that supports educational outreach.

[![Open Collective](https://img.shields.io/badge/üíù%20Support%20AI%20Education-Open%20Collective-blue.svg?style=for-the-badge)](https://opencollective.com/mlsysbook)

</div>

---

## Ïª§ÎÆ§ÎãàÌã∞ÏôÄ Î¶¨ÏÜåÏä§

| Resource | Description |
|---|---|
| [üìñ **ÌÖçÏä§Ìä∏Î∂Å**](https://mlsysbook.ai) | Ïù∏ÌÑ∞ÎûôÌã∞Î∏å Ïò®ÎùºÏù∏ ÍµêÏû¨ |
| [üî• **TinyTorch**](https://mlsysbook.ai/tinytorch) | ML ÌîÑÎ†àÏûÑÏõåÌÅ¨Î•º Ï≤òÏùåÎ∂ÄÌÑ∞ Íµ¨ÌòÑ |
| [üîß **Hardware Kits**](https://mlsysbook.ai/kits) | Arduino, Raspberry Pi, Ïó£ÏßÄ ÎîîÎ∞îÏù¥Ïä§Ïóê Î∞∞Ìè¨ |
| [üåê **Ecosystem**](https://mlsysbook.org) | Î¶¨ÏÜåÏä§¬∑ÏõåÌÅ¨Ïàç¬∑Ïª§ÎÆ§ÎãàÌã∞ |
| [üí¨ **Discussions**](https://github.com/harvard-edge/cs249r_book/discussions) | ÏßàÎ¨∏¬∑ÏïÑÏù¥ÎîîÏñ¥ |

---

## Í∏∞Ïó¨ÌïòÍ∏∞

Ïö∞Î¶¨Îäî ÍµêÏû¨¬∑TinyTorch¬∑ÌïòÎìúÏõ®Ïñ¥ ÌÇ§Ìä∏Ïóê ÎåÄÌïú Í∏∞Ïó¨Î•º ÌôòÏòÅÌï©ÎãàÎã§!

| I want to‚Ä¶ | Go here |
|--------------|---------|
| Ïò§ÌÉÄ ÏàòÏ†ï¬∑Ï±ïÌÑ∞ Í∞úÏÑ† | [book/docs/CONTRIBUTING.md](book/docs/CONTRIBUTING.md) |
| TinyTorch Î™®Îìà Ï∂îÍ∞Ä¬∑Î≤ÑÍ∑∏ ÏàòÏ†ï | [tinytorch/CONTRIBUTING.md](tinytorch/CONTRIBUTING.md) |
| ÌïòÎìúÏõ®Ïñ¥ Ïã§Ìóò Í∞úÏÑ† | [kits/README.md](kits/README.md) |
| Ïù¥Ïäà Î≥¥Í≥† | [GitHub Issues](https://github.com/harvard-edge/cs249r_book/issues) |
| ÏßàÎ¨∏ÌïòÍ∏∞ | [GitHub Discussions](https://github.com/harvard-edge/cs249r_book/discussions) |

---

## Ïù∏Ïö© Î∞è ÎùºÏù¥ÏÑ†Ïä§

### Ïù∏Ïö©
```bibtex
@inproceedings{reddi2024mlsysbook,
  title        = {MLSysBook.AI: Principles and Practices of Machine Learning Systems Engineering},
  author       = {Reddi, Vijay Janapa},
  booktitle    = {2024 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ ISSS)},
  pages        = {41--42},
  year         = {2024},
  organization = {IEEE},
  url          = {https://mlsysbook.org}
}
```

### ÎùºÏù¥ÏÑ†Ïä§

Ïù¥ ÌîÑÎ°úÏ†ùÌä∏Îäî Ïù¥Ï§ë ÎùºÏù¥ÏÑ†Ïä§ Íµ¨Ï°∞Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§:

| Component | License | What It Means |
|-----------|---------|---------------|
| **Book content** | [CC BY‚ÄëNC‚ÄëND 4.0](LICENSE.md) | Ï∂úÏ≤ò ÌëúÏãú¬∑ÎπÑÏÉÅÏóÖ¬∑Î≥ÄÍ≤Ω Í∏àÏßÄ Ï°∞Í±¥ÏúºÎ°ú ÏûêÏú† Î∞∞Ìè¨ |
| **TinyTorch code** | [Apache 2.0](tinytorch/LICENSE) | ÏûêÏú† ÏÇ¨Ïö©¬∑ÏàòÏ†ï¬∑Î∞∞Ìè¨¬∑ÌäπÌóà Î≥¥Ìò∏ Ìè¨Ìï® |

ÌÖçÏä§Ìä∏Î∂Å ÎÇ¥Ïö©(Ï±ïÌÑ∞¬∑Í∑∏Î¶º¬∑ÏÑ§Î™Ö)ÏùÄ ÍµêÏú° ÏûêÎ£åÏù¥Î©∞, Ï∂úÏ≤ò ÌëúÏãúÏôÄ ÎπÑÏÉÅÏóÖÏ†Å ÏÇ¨Ïö©ÏùÑ Ï†ÑÏ†úÎ°ú ÏûêÏú†Î°≠Í≤å Í≥µÏú†Îê©ÎãàÎã§. ÏÜåÌîÑÌä∏Ïõ®Ïñ¥ ÌîÑÎ†àÏûÑÏõåÌÅ¨Îäî ÎàÑÍµ¨ÎÇò ÏÇ¨Ïö©¬∑ÏàòÏ†ï¬∑ÌÜµÌï©Ìï† Ïàò ÏûàÎèÑÎ°ù ÏÑ§Í≥ÑÎêú ÎèÑÍµ¨ÏûÖÎãàÎã§.

---

## Í∏∞Ïó¨ÏûêÎì§

Îã§Ïùå ÌõåÎ•≠Ìïú Î∂ÑÎì§Ïù¥ Ïù¥ Î¶¨ÏÜåÏä§Î•º Îçî ÎÇòÏùÄ Í≤ÉÏúºÎ°ú ÎßåÎì§Í∏∞ ÏúÑÌï¥ Í∏∞Ïó¨Ìï¥ Ï£ºÏÖ®ÏäµÎãàÎã§:

<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->
<!-- ... (contributors table omitted for brevity) -->
<!-- ALL-CONTRIBUTORS-LIST:END -->

---

<div align="center">

**[‚≠ê GitHubÏóê Î≥Ñ Îã¨Í∏∞](https://github.com/harvard-edge/cs249r_book#support-this-work) ‚Ä¢ [‚úâÔ∏è Íµ¨ÎèÖÌïòÍ∏∞](https://buttondown.email/mlsysbook) ‚Ä¢ [üí¨ ÌÜ†Î°† Ï∞∏Ïó¨](https://github.com/harvard-edge/cs249r_book/discussions) ‚Ä¢ [üåê mlsysbook.ai Î∞©Î¨∏](https://mlsysbook.ai)**

*MLSysBook Ïª§ÎÆ§ÎãàÌã∞Ïùò ÌóåÏã†ÏúºÎ°ú ÎßåÎì§Ïñ¥Ï°åÏäµÎãàÎã§.*

</div>


## Links discovered
- [![Book](https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/book-validate-dev.yml?branch=dev&label=Book&logo=githubactions&cacheSeconds=300)
- [![TinyTorch](https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/tinytorch-validate-dev.yml?branch=dev&label=TinyTorch&logo=python&cacheSeconds=300)
- [Updated](https://img.shields.io/github/last-commit/harvard-edge/cs249r_book/dev?label=Updated&logo=git&cacheSeconds=300)
- [![License](https://img.shields.io/badge/License-CC--BY--NC--ND%204.0-blue.svg)
- [![Cite](https://img.shields.io/badge/Cite-IEEE%202024-blue?logo=ieee)
- [![Fund Us](https://img.shields.io/badge/Fund%20Us-Open%20Collective-blue.svg?logo=open-collective)
- [üìñ Ïò®ÎùºÏù∏ ÏùΩÍ∏∞](https://mlsysbook.ai)
- [Tinyüî•Torch](https://mlsysbook.ai/tinytorch)
- [üìÑ PDF Îã§Ïö¥Î°úÎìú](https://mlsysbook.ai/assets/downloads/Machine-Learning-Systems.pdf)
- [üìì EPUB Îã§Ïö¥Î°úÎìú](https://mlsysbook.ai/epub)
- [üåê ÏÉùÌÉúÍ≥Ñ ÌÉêÌóò](https://mlsysbook.org)
- [ÌÖçÏä§Ìä∏Î∂Å](https://mlsysbook.ai)
- [Chapter 1](https://www.mlsysbook.ai/contents/core/introduction/introduction.html)
- [Benchmarking chapter](https://mlsysbook.ai/contents/core/benchmarking/benchmarking.html)
- [Getting Started guide](https://mlsysbook.ai/tinytorch/getting-started.html)
- [ÌïòÎìúÏõ®Ïñ¥ ÌÇ§Ìä∏](https://mlsysbook.ai/kits)
- [Discussions](https://github.com/harvard-edge/cs249r_book/discussions)
- [üìñ ÌÖçÏä§Ìä∏Î∂Å](https://mlsysbook.ai)
- [book/](https://github.com/harvard-edge/cs249r_book/blob/dev/README/book/README.md)
- [üî• TinyTorch](https://mlsysbook.ai/tinytorch)
- [tinytorch/](https://github.com/harvard-edge/cs249r_book/blob/dev/README/tinytorch/README.md)
- [üîß Hardware Kits](https://mlsysbook.ai/kits)
- [kits/](https://github.com/harvard-edge/cs249r_book/blob/dev/README/kits/README.md)
- [![Stars](https://img.shields.io/github/stars/harvard-edge/cs249r_book?style=for-the-badge&logo=github&color=gold)
- [![Star History Chart](https://api.star-history.com/svg?repos=harvard-edge/cs249r_book&type=Date)
- [Open Collective](https://opencollective.com/mlsysbook)
- [![Open Collective](https://img.shields.io/badge/üíù%20Support%20AI%20Education-Open%20Collective-blue.svg?style=for-the-badge)
- [üìñ **ÌÖçÏä§Ìä∏Î∂Å**](https://mlsysbook.ai)
- [üî• **TinyTorch**](https://mlsysbook.ai/tinytorch)
- [üîß **Hardware Kits**](https://mlsysbook.ai/kits)
- [üåê **Ecosystem**](https://mlsysbook.org)
- [üí¨ **Discussions**](https://github.com/harvard-edge/cs249r_book/discussions)
- [book/docs/CONTRIBUTING.md](https://github.com/harvard-edge/cs249r_book/blob/dev/README/book/docs/CONTRIBUTING.md)
- [tinytorch/CONTRIBUTING.md](https://github.com/harvard-edge/cs249r_book/blob/dev/README/tinytorch/CONTRIBUTING.md)
- [kits/README.md](https://github.com/harvard-edge/cs249r_book/blob/dev/README/kits/README.md)
- [GitHub Issues](https://github.com/harvard-edge/cs249r_book/issues)
- [GitHub Discussions](https://github.com/harvard-edge/cs249r_book/discussions)
- [CC BY‚ÄëNC‚ÄëND 4.0](https://github.com/harvard-edge/cs249r_book/blob/dev/README/LICENSE.md)
- [Apache 2.0](https://github.com/harvard-edge/cs249r_book/blob/dev/README/tinytorch/LICENSE.md)
- [‚≠ê GitHubÏóê Î≥Ñ Îã¨Í∏∞](https://github.com/harvard-edge/cs249r_book#support-this-work)
- [‚úâÔ∏è Íµ¨ÎèÖÌïòÍ∏∞](https://buttondown.email/mlsysbook)
- [üí¨ ÌÜ†Î°† Ï∞∏Ïó¨](https://github.com/harvard-edge/cs249r_book/discussions)
- [üåê mlsysbook.ai Î∞©Î¨∏](https://mlsysbook.ai)
- [English](https://github.com/harvard-edge/cs249r_book/blob/dev/README.md)
- [‰∏≠Êñá](https://github.com/harvard-edge/cs249r_book/blob/dev/README/README_zh.md)
- [Êó•Êú¨Ë™û](https://github.com/harvard-edge/cs249r_book/blob/dev/README/README_ja.md)
- [ÌïúÍµ≠Ïñ¥](https://github.com/harvard-edge/cs249r_book/blob/dev/README/README_ko.md)

--- README/README_zh.md ---
# Êú∫Âô®Â≠¶‰π†Á≥ªÁªü
*‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÂ∑•Á®ãÁöÑÂéüÁêÜ‰∏éÂÆûË∑µ*

<p align="center">
  <a href="../README.md">English</a> ‚Ä¢
  <a href="README_zh.md">‰∏≠Êñá</a> ‚Ä¢
  <a href="README_ja.md">Êó•Êú¨Ë™û</a> ‚Ä¢
  <a href="README_ko.md">ÌïúÍµ≠Ïñ¥</a>
</p>

<div align="center">

<p align="center">

  [![Book](https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/book-validate-dev.yml?branch=dev&label=Book&logo=githubactions&cacheSeconds=300)](https://github.com/harvard-edge/cs249r_book/actions/workflows/book-validate-dev.yml)
  [![TinyTorch](https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/tinytorch-validate-dev.yml?branch=dev&label=TinyTorch&logo=python&cacheSeconds=300)](https://github.com/harvard-edge/cs249r_book/actions/workflows/tinytorch-validate-dev.yml)
  ![Updated](https://img.shields.io/github/last-commit/harvard-edge/cs249r_book/dev?label=Updated&logo=git&cacheSeconds=300)
  [![License](https://img.shields.io/badge/License-CC--BY--NC--ND%204.0-blue.svg)](https://github.com/harvard-edge/cs249r_book/blob/dev/LICENSE.md)
  [![Cite](https://img.shields.io/badge/Cite-IEEE%202024-blue?logo=ieee)](#-citation--license)
  [![Fund Us](https://img.shields.io/badge/Fund%20Us-Open%20Collective-blue.svg?logo=open-collective)](https://opencollective.com/mlsysbook)

</p>

<p align="center">

  <!-- Reader Navigation -->
  **[üìñ Âú®Á∫øÈòÖËØª](https://mlsysbook.ai)** ‚Ä¢
  **[Tinyüî•Torch](https://mlsysbook.ai/tinytorch)** ‚Ä¢
  **[üìÑ ‰∏ãËΩΩ PDF](https://mlsysbook.ai/assets/downloads/Machine-Learning-Systems.pdf)** ‚Ä¢
  **[üìì ‰∏ãËΩΩ EPUB](https://mlsysbook.ai/epub)** ‚Ä¢
  **[üåê Êé¢Á¥¢ÁîüÊÄÅÁ≥ªÁªü](https://mlsysbook.org)**

</p>

üìö **2026 Âπ¥ MIT Press Â∞ÜÂá∫ÁâàÁ∫∏Ë¥®Áâà**

</div>

---

## ‰ΩøÂëΩ

**‰∏ñÁïåÊ≠£ÊÄ•ÈÄüÊûÑÂª∫ AI Á≥ªÁªüÔºåÂç¥Áº∫‰πèÁ≥ªÁªüÊÄßÁöÑÂ∑•Á®ãÊñπÊ≥ï„ÄÇ**

ËøôÊ≠£ÊòØÊàë‰ª¨ÊâÄËØ¥ÁöÑ AI Â∑•Á®ã„ÄÇ

**AI Â∑•Á®ãÊòØ‰∏ÄÈó®Â≠¶ÁßëÔºåËá¥Âäõ‰∫éÂú®ÁúüÂÆû‰∏ñÁïå‰∏≠ÊûÑÂª∫È´òÊïà„ÄÅÂèØÈù†„ÄÅÂÆâÂÖ®‰∏îÁ®≥ÂÅ•ÁöÑÊô∫ËÉΩÁ≥ªÁªüÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØÂ≠§Á´ãÁöÑÊ®°Âûã„ÄÇ**

**Êàë‰ª¨ÁöÑ‰ΩøÂëΩÔºö** Â∞Ü AI Â∑•Á®ãÁ°ÆÁ´ã‰∏∫Âü∫Á°ÄÂ≠¶ÁßëÔºå‰∏éËΩØ‰ª∂Â∑•Á®ãÂíåËÆ°ÁÆóÊú∫Â∑•Á®ãÂπ∂ÂàóÔºåÈÄöËøáÊïôÂ≠¶ËÆ©‰∫∫‰ª¨ÊéåÊè°Á´ØÂà∞Á´ØÊô∫ËÉΩÁ≥ªÁªüÁöÑËÆæËÆ°„ÄÅÊûÑÂª∫‰∏éËØÑ‰º∞ÊñπÊ≥ï„ÄÇAI ÁöÑÈïøÊúüÂΩ±ÂìçÂ∞ÜÁî±ËÉΩÂ§üÂ∞ÜÊÉ≥Ê≥ïËΩ¨Âåñ‰∏∫ÂèØËøêË°å„ÄÅÂèØ‰ø°ËµñÁ≥ªÁªüÁöÑÂ∑•Á®ãÂ∏àÂ°ëÈÄ†„ÄÇ

---

## Êú¨‰ªìÂ∫ìÂåÖÂê´ÁöÑÂÜÖÂÆπ

Êú¨‰ªìÂ∫ìÊòØ AI Á≥ªÁªüÂ∑•Á®ãÁöÑÂºÄÊîæÂ≠¶‰π†Ê†à„ÄÇ

ÂÆÉÂåÖÊã¨ÊïôÊùêÊ∫êÁ†Å„ÄÅTinyTorch„ÄÅÁ°¨‰ª∂Â•ó‰ª∂‰ª•ÂèäÂç≥Â∞ÜÊé®Âá∫ÁöÑÂ∞ÜÂéüÁêÜ‰∏éÂèØËøêË°å‰ª£Á†Å„ÄÅÁúüÂÆûËÆæÂ§áÁõ∏ËøûÊé•ÁöÑÂçè‰ΩúÂÆûÈ™åÔºàco‚ÄëlabsÔºâ„ÄÇ

---

## ÂÖ•Èó®ÊåáÂçó

Ê†πÊçÆ‰Ω†ÁöÑÁõÆÊ†áÈÄâÊã©Ë∑ØÂæÑ„ÄÇ

**READ** ‰ªé[ÊïôÊùê](https://mlsysbook.ai)ÂºÄÂßã„ÄÇÂÖàÈòÖËØª[Á¨¨ 1 Á´†](https://www.mlsysbook.ai/contents/core/introduction/introduction.html)Âíå[Benchmarking Á´†ËäÇ](https://mlsysbook.ai/contents/core/benchmarking/benchmarking.html)„ÄÇ

**BUILD** ÊåâÁÖß[ÂÖ•Èó®ÊåáÂçó](https://mlsysbook.ai/tinytorch/getting-started.html)ÂêØÂä® TinyTorch„ÄÇ‰ªé Module 01 ÂºÄÂßãÔºåÈÄêÊ≠•Â≠¶‰π† CNN„ÄÅTransformer ‰ª•Âèä MLPerf Âü∫ÂáÜ„ÄÇ

**DEPLOY** ÈÄâÊã©[Á°¨‰ª∂Â•ó‰ª∂](https://mlsysbook.ai/kits)ÔºåÂú® Arduino„ÄÅRaspberry Pi Á≠âËæπÁºòËÆæÂ§á‰∏äËøõË°åÂÆûÈ™å„ÄÇ

**CONNECT** Âú®[Discussions](https://github.com/harvard-edge/cs249r_book/discussions)‰∏≠ÊâìÂ£∞ÊãõÂëºÔºåÊàë‰ª¨‰ºöÂ∞ΩÂø´ÂõûÂ§ç„ÄÇ

---

## Â≠¶‰π†Ê†à

‰∏ãÈù¢ÁöÑÁ§∫ÊÑèÂõæÂ±ïÁ§∫‰∫ÜÊïôÊùêÂ¶Ç‰Ωï‰∏éÂä®ÊâãÂÆûË∑µÂíåÈÉ®ÁΩ≤Áõ∏ËøûÊé•„ÄÇÈòÖËØªÊïôÊùêÂêéÔºåÊåëÈÄâ‰Ω†ÊÑüÂÖ¥Ë∂£ÁöÑË∑ØÂæÑÔºö

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                               ‚îÇ
‚îÇ                           MACHINE LEARNING SYSTEMS                            ‚îÇ
‚îÇ                              Read the Textbook                                ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îÇ                    Theory ‚Ä¢ Concepts ‚Ä¢ Best Practices                         ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                        ‚îÇ
                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                          ‚îÇ             ‚îÇ             ‚îÇ
                          ‚ñº             ‚ñº             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                            HANDS‚ÄëON ACTIVITIES                                ‚îÇ
‚îÇ                           (pick one or all)                                   ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ     ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ    SOFTWARE     ‚îÇ      ‚îÇ    TINYTORCH    ‚îÇ      ‚îÇ    HARDWARE     ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ    CO‚ÄëLABS      ‚îÇ      ‚îÇ    FRAMEWORK    ‚îÇ      ‚îÇ      LABS       ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ EXPLORE         ‚îÇ      ‚îÇ BUILD           ‚îÇ      ‚îÇ DEPLOY          ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ Run controlled  ‚îÇ      ‚îÇ Understand      ‚îÇ      ‚îÇ Engineer under  ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ experiments on  ‚îÇ      ‚îÇ frameworks by   ‚îÇ      ‚îÇ real constraints‚îÇ     ‚îÇ
‚îÇ     ‚îÇ latency, memory,‚îÇ      ‚îÇ implementing    ‚îÇ      ‚îÇ memory, power,  ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ energy, cost    ‚îÇ      ‚îÇ them            ‚îÇ      ‚îÇ timing, safety  ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ
‚îÇ     ‚îÇ (coming 2026)   ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ Arduino, Pi     ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îÇ           EXPLORE                  BUILD                   DEPLOY             ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                        ‚îÇ
                                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                               ‚îÇ
‚îÇ                                  AI OLYMPICS                                  ‚îÇ
‚îÇ                                 Prove Mastery                                 ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îÇ       Compete across all tracks ‚Ä¢ University teams ‚Ä¢ Public leaderboards      ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îÇ                                (coming 2026)                                  ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

|   | Component | What You Do | Link |
|---|-----------|-------------|------|
| **READ** | [üìñ ÊïôÊùê](https://mlsysbook.ai) | ÁêÜËß£Êú∫Âô®Â≠¶‰π†Á≥ªÁªüÊ¶ÇÂøµ | [book/](book/README.md) |
| **EXPLORE** | üîÆ Software Co‚ÄëLabs | ËøõË°åÂª∂Ëøü„ÄÅÂÜÖÂ≠ò„ÄÅËÉΩËÄó„ÄÅÊàêÊú¨ÂÆûÈ™å | *Coming 2026* |
| **BUILD** | [üî• TinyTorch](https://mlsysbook.ai/tinytorch) | ‰∫≤ÊâãÂÆûÁé∞Ê°ÜÊû∂ | [tinytorch/](tinytorch/README.md) |
| **DEPLOY** | [üîß Hardware Kits](https://mlsysbook.ai/kits) | Âú®ÂèóÂÜÖÂ≠ò„ÄÅÂäüËÄó„ÄÅÊó∂Âª∂„ÄÅÂÆâÂÖ®Á∫¶ÊùüÁöÑÁ°¨‰ª∂‰∏äÂ∑•Á®ãÂÆûÁé∞ | [kits/](kits/README.md) |
| **PROVE** | üèÜ AI Olympics | ÂèÇ‰∏éÊâÄÊúâËµõÈÅìÁöÑÁ´ûÊäÄ‰∏éÂü∫ÂáÜÊµãËØï | *Coming 2026* |

**ÊØèÊù°Ë∑ØÂæÑÁöÑÂ≠¶‰π†ÂÜÖÂÆπÔºö**
- **EXPLORE** Ëß£Èáä *‰∏∫‰ªÄ‰πà* ‚Äî‚Äî ‰∫ÜËß£ÊùÉË°°„ÄÇÊîπÂèò batch size„ÄÅÁ≤æÂ∫¶„ÄÅÊ®°ÂûãÁªìÊûÑÔºåËßÇÂØüÂª∂Ëøü„ÄÅÂÜÖÂ≠òÂíåÂáÜÁ°ÆÁéáÁöÑÂèòÂåñ„ÄÇ
- **BUILD** Ëß£Èáä *ÊÄé‰πàÂÅö* ‚Äî‚Äî ÁêÜËß£ÂÜÖÈÉ®ÂÆûÁé∞„ÄÇËá™Ë°åÂÆûÁé∞ autograd„ÄÅ‰ºòÂåñÂô®„ÄÅÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊÑüÂèó TensorFlow ‰∏é PyTorch ÁöÑÂ∑•‰ΩúÂéüÁêÜ„ÄÇ
- **DEPLOY** Ëß£Èáä *Âú®Âì™Èáå* ‚Äî‚Äî ‰∫ÜËß£Á∫¶Êùü„ÄÇÁúüÂÆûÁ°¨‰ª∂ÁöÑÂÜÖÂ≠ò‰∏äÈôê„ÄÅÂäüËÄóÈ¢ÑÁÆóÂíåÊó∂Âª∂Ë¶ÅÊ±Ç‰∏ãËøõË°åÂÆûÈ™å„ÄÇ

---

## ‰Ω†Â∞ÜÂ≠¶Âà∞ÁöÑÂÜÖÂÆπ

Êú¨ÊïôÊùêÊïô‰ºö‰Ω†Âú®Êú∫Âô®Â≠¶‰π†‰∏éÁ≥ªÁªüÂ∑•Á®ãÁöÑ‰∫§ÂèâÁÇπÊÄùËÄÉ„ÄÇÊØè‰∏ÄÁ´†ÈÉΩÂ∞ÜÁÆóÊ≥ïÊ¶ÇÂøµ‰∏éÊîØÊíëÂÖ∂ËøêË°åÁöÑÂü∫Á°ÄËÆæÊñΩÁõ∏ËøûÊé•„ÄÇ

### ML ‚Üî Systems Bridge

| ML Concept | Systems Concept | What You Learn |
|------------|-----------------|----------------|
| Model parameters | Memory constraints | Â¶Ç‰ΩïÂú®ËµÑÊ∫êÂèóÈôêÁöÑËÆæÂ§á‰∏äÂÆπÁ∫≥Â§ßÂûãÊ®°Âûã |
| Inference latency | Hardware acceleration | GPU„ÄÅTPU„ÄÅÂä†ÈÄüÂô®Â¶Ç‰ΩïÊâßË°åÁ•ûÁªèÁΩëÁªú |
| Training convergence | Compute efficiency | Ê∑∑ÂêàÁ≤æÂ∫¶‰∏é‰ºòÂåñÊäÄÊúØÂ¶Ç‰ΩïÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨ |
| Model accuracy | Quantization and pruning | Âú®‰øùÊåÅÊÄßËÉΩÁöÑÂâçÊèê‰∏ãÂéãÁº©Ê®°ÂûãÁöÑÊñπÊ≥ï |
| Data requirements | Pipeline infrastructure | Â¶Ç‰ΩïÊûÑÂª∫È´òÊïàÁöÑÊï∞ÊçÆÂä†ËΩΩ‰∏éÈ¢ÑÂ§ÑÁêÜÊµÅÊ∞¥Á∫ø |
| Model deployment | MLOps practices | Âú®Áîü‰∫ßÁéØÂ¢É‰∏≠ÁõëÊéß„ÄÅÁâàÊú¨ÁÆ°ÁêÜ‰∏éÊõ¥Êñ∞Ê®°ÂûãÁöÑÊñπÂºè |
| Privacy constraints | On‚Äëdevice learning | Â¶Ç‰ΩïÂú®‰∏çÂ∞ÜÊï∞ÊçÆ‰∏ä‰º†‰∫ëÁ´ØÁöÑÊÉÖÂÜµ‰∏ãËøõË°åÂ≠¶‰π†‰∏éÈÄÇÂ∫î |

### ‰π¶ÁöÑÁªìÊûÑ

| Part | Focus | Chapters |
|------|-------|----------|
| **I. Foundations** | Âü∫Á°ÄÊ¶ÇÂøµ | Introduction, ML Systems, DL Primer, Architectures |
| **II. Design** | ÊûÑÂª∫Ê®°Âùó | Workflow, Data Engineering, Frameworks, Training |
| **III. Performance** | Âä†ÈÄüÊÄßËÉΩ | Efficient AI, Optimizations, HW Acceleration, Benchmarking |
| **IV. Deployment** | ÂÆûÈôÖÈÉ®ÁΩ≤ | MLOps, On‚Äëdevice Learning, Privacy, Robustness |
| **V. Trust** | ÂèØ‰ø°ÂèØÈù† | Responsible AI, Sustainable AI, AI for Good |
| **VI. Frontiers** | ÂâçÊ≤øÂ±ïÊúõ | Emerging trends and future directions |

---

## ‰∏é‰ºó‰∏çÂêå‰πãÂ§Ñ

Êú¨‰π¶ÊòØ‰∏ÄÊú¨Ê¥ªÁöÑÊïôÊùê„ÄÇÈöèÁùÄÈ¢ÜÂüüÁöÑÂèëÂ±ïÔºåÊàë‰ª¨‰ºöÊåÅÁª≠Êõ¥Êñ∞ÔºåÂπ∂Âê∏Êî∂Á§æÂå∫ÁöÑÂèçÈ¶à„ÄÇ

AI ÁöÑÂèëÂ±ïÈÄüÂ∫¶Â¶ÇÈó™ÁîµËà¨Ôºå‰ΩÜÊîØÊíëÂÖ∂ËøêË°åÁöÑÂ∑•Á®ãÊ®°ÂùóÂπ∂‰∏ç‰ºöÂÉèÂ§¥Êù°Êñ∞ÈóªÈÇ£Ê†∑Âø´ÈÄüÊõ¥Ëø≠„ÄÇÊú¨È°πÁõÆÂõ¥ÁªïËøô‰∫õÁ®≥Âõ∫ÁöÑÂü∫Á°ÄÊûÑÂª∫„ÄÇ

ÊääÂÆÉÊÉ≥Ë±°Êàê‰πêÈ´ò„ÄÇÊñ∞Â•óË£ÖÂ±ÇÂá∫‰∏çÁ©∑Ôºå‰ΩÜÁßØÊú®Êú¨Ë∫´‰øùÊåÅ‰∏çÂèò„ÄÇÂè™Ë¶ÅÂ≠¶‰ºöÂ¶Ç‰ΩïÊãºÊé•ÁßØÊú®ÔºåÂ∞±ËÉΩÊûÑÂª∫‰ªª‰Ωï‰∏úË•ø„ÄÇËøôÈáåÁöÑ "AI ÁßØÊú®" Â∞±ÊòØËÆ© AI Ê≠£Â∏∏Â∑•‰ΩúÁöÑÂùöÂÆûÁ≥ªÁªüÂéüÁêÜ„ÄÇ

Êó†ËÆ∫ÊòØÈòÖËØªÁ´†ËäÇ„ÄÅÂä®ÊâãÂÆûÈ™åËøòÊòØÊèê‰æõÂèçÈ¶àÔºå‰Ω†ÈÉΩÂú®Â∏ÆÂä©ËÆ©Ëøô‰∫õÁêÜÂøµÂØπ‰∏ã‰∏Ä‰ª£Â≠¶‰π†ËÄÖÊõ¥Âä†ÊòìÂæó„ÄÇ

### Research to Teaching Loop

Êàë‰ª¨‰ΩøÁî®Áõ∏ÂêåÁöÑÂæ™ÁéØËøõË°åÁ†îÁ©∂‰∏éÊïôÂ≠¶ÔºöÂÆö‰πâÁ≥ªÁªüÈóÆÈ¢ò ‚Üí ÊûÑÂª∫ÂèÇËÄÉÂÆûÁé∞ ‚Üí Âü∫ÂáÜÊµãËØï ‚Üí Â∞ÜÂÖ∂ËΩ¨Âåñ‰∏∫ËØæÁ®ã‰∏éÂ∑•ÂÖ∑ ‚Üí ËÆ©‰ªñ‰∫∫ËÉΩÂ§üÂ§çÁé∞‰∏éÊâ©Â±ï„ÄÇ

| Loop Step | Research Artifacts | Teaching Artifacts |
|-----------|-------------------|-------------------|
| **Measure** | Benchmarks, suites, metrics | Benchmarking chapter, assignments |
| **Build** | Reference systems, compilers, runtimes | TinyTorch modules, co‚Äëlabs |
| **Deploy** | Hardware targets, constraints, reliability | Hardware labs, kits |

---

## ÊîØÊåÅÊàë‰ª¨ÁöÑÂ∑•‰Ωú

Êàë‰ª¨ÁõÆÊ†áÊòØÂú® **2030 Âπ¥‰πãÂâçÂüπÂÖª 100 ‰∏áÂ≠¶‰π†ËÄÖ**ÔºåËÆ© AI Â∑•Á®ãÊàê‰∏∫ÂÖ±‰∫´ÁöÑ„ÄÅÂèØÊïôÂ≠¶ÁöÑÂ≠¶ÁßëÔºåËÄå‰∏çÊòØÈõ∂Êï£ÁöÑÂÆûË∑µÈõÜÂêà„ÄÇÊØè‰∏ÄÊ¨°ÊòüÊ†á„ÄÅÂàÜ‰∫´‰∏éË¥°ÁåÆÈÉΩÂú®Êé®Âä®Ëøô‰∏ÄÁõÆÊ†áÂâçËøõ„ÄÇ

### ‰∏∫‰ªÄ‰πà GitHub Stars ÂæàÈáçË¶ÅÔºü

<div align="center">

*ÊúâÂ∫¶ÈáèÊâç‰ºöÊîπËøõ„ÄÇ*

ÊØè‰∏Ä‰∏™ÊòüÊ†á‰ª£Ë°®‰∏Ä‰ΩçÁõ∏‰ø° AI Á≥ªÁªüÂ∫îÂú®‰∏•Ê†º‰∏îÁúüÂÆûÁ∫¶Êùü‰∏ãËøõË°åÂ∑•Á®ãÂåñÁöÑÂ≠¶‰π†ËÄÖ„ÄÅÊïôËÇ≤ËÄÖÊàñÊîØÊåÅËÄÖ„ÄÇ

[![Stars](https://img.shields.io/github/stars/harvard-edge/cs249r_book?style=for-the-badge&logo=github&color=gold)](https://github.com/harvard-edge/cs249r_book/stargazers)

[![Star History Chart](https://api.star-history.com/svg?repos=harvard-edge/cs249r_book&type=Date)](https://star-history.com/#harvard-edge/cs249r_book&Date)

1 ‰ΩçÂ≠¶‰π†ËÄÖ ‚Üí 10 ‰Ωç ‚Üí 100 ‰Ωç ‚Üí 1,000 ‰Ωç ‚Üí **10,000 ‰Ωç** ‚Üí 100,000 ‰Ωç ‚Üí **1M ‰Ωç**

</div>

ÊòüÊ†á‰∏çÊòØÁªàÁÇπÔºåËÄåÊòØ‰ø°Âè∑„ÄÇ

‰∏Ä‰∏™ÂèØËßÅÁöÑÁ§æÂå∫ËÆ©Â§ßÂ≠¶„ÄÅÂü∫Èáë‰ºöÂíåË°å‰∏öÂêà‰Ωú‰ºô‰º¥Êõ¥ÂÆπÊòìÈááÁî®Êú¨ËµÑÊ∫ê„ÄÅÊçêËµ†Á°¨‰ª∂„ÄÅËµÑÂä©Á†îËÆ®‰ºö„ÄÇÊ≠§‰∏æÈôç‰Ωé‰∫Ü‰∏ã‰∏ÄÊâÄÂ≠¶Ê†°„ÄÅ‰∏ã‰∏ÄÈó¥ÊïôÂÆ§‰ª•Âèä‰∏ã‰∏ÄÊâπÂ≠¶‰π†ËÄÖÁöÑÈó®Êßõ„ÄÇ

ÊçêÂä©Â∞ÜÊµÅÂêë [Open Collective](https://opencollective.com/mlsysbook)ÔºåÁî®‰∫é TinyML4D Á†îËÆ®‰ºö„ÄÅ‰∏∫ËµÑÊ∫êÂåÆ‰πèÁöÑËØæÂ†ÇÊèê‰æõÁ°¨‰ª∂Â•ó‰ª∂‰ª•ÂèäÁª¥ÊåÅÂÖçË¥π„ÄÅÂºÄÊîæËµÑÊ∫êÁöÑÂü∫Á°ÄËÆæÊñΩ„ÄÇ

‰∏ÄÊ¨°ÁÇπÂáªÂç≥ÂèØÊâìÂºÄ‰∏ã‰∏ÄÈó¥ÊïôÂÆ§„ÄÅ‰∏ã‰∏Ä‰ΩçË¥°ÁåÆËÄÖ‰ª•Âèä‰∏ã‰∏Ä‰ª£ AI Â∑•Á®ãÂ∏à„ÄÇ

### ‰∏∫‰ΩøÂëΩÊçêÊ¨æ

<div align="center">

All contributions go to [Open Collective](https://opencollective.com/mlsysbook), a transparent fund that supports educational outreach.

[![Open Collective](https://img.shields.io/badge/üíù%20Support%20AI%20Education-Open%20Collective-blue.svg?style=for-the-badge)](https://opencollective.com/mlsysbook)

</div>

---

## Á§æÂå∫‰∏éËµÑÊ∫ê

| Resource | Description |
|---|---|
| [üìñ **ÊïôÊùê**](https://mlsysbook.ai) | ‰∫§‰∫íÂºèÂú®Á∫øÊïôÊùê |
| [üî• **TinyTorch**](https://mlsysbook.ai/tinytorch) | ‰ªéÈõ∂ÂÆûÁé∞Êú∫Âô®Â≠¶‰π†Ê°ÜÊû∂ |
| [üîß **Hardware Kits**](https://mlsysbook.ai/kits) | ÈÉ®ÁΩ≤Ëá≥ Arduino„ÄÅRaspberry Pi„ÄÅËæπÁºòËÆæÂ§á |
| [üåê **Ecosystem**](https://mlsysbook.org) | ËµÑÊ∫ê„ÄÅÁ†îËÆ®‰ºö„ÄÅÁ§æÂå∫ |
| [üí¨ **Discussions**](https://github.com/harvard-edge/cs249r_book/discussions) | ÊèêÈóÆ‰∏éÊÉ≥Ê≥ï |

---

## Ë¥°ÁåÆÊåáÂçó

Êàë‰ª¨Ê¨¢ËøéÂØπÊïôÊùê„ÄÅTinyTorch ‰∏éÁ°¨‰ª∂Â•ó‰ª∂ÁöÑË¥°ÁåÆÔºÅ

| ÊàëÊÉ≥‚Ä¶ | ÂâçÂæÄ |
|--------------|---------|
| ‰øÆÊ≠£ÈîôÂà´Â≠óÊàñÊîπËøõÁ´†ËäÇ | [book/docs/CONTRIBUTING.md](book/docs/CONTRIBUTING.md) |
| Ê∑ªÂä† TinyTorch Ê®°ÂùóÊàñ‰øÆÂ§ç bug | [tinytorch/CONTRIBUTING.md](tinytorch/CONTRIBUTING.md) |
| ÊîπËøõÁ°¨‰ª∂ÂÆûÈ™å | [kits/README.md](kits/README.md) |
| Êä•ÂëäÈóÆÈ¢ò | [GitHub Issues](https://github.com/harvard-edge/cs249r_book/issues) |
| ÊèêÈóÆ | [GitHub Discussions](https://github.com/harvard-edge/cs249r_book/discussions) |

---

## ÂºïÁî®‰∏éËÆ∏ÂèØËØÅ

### ÂºïÁî®
```bibtex
@inproceedings{reddi2024mlsysbook,
  title        = {MLSysBook.AI: Principles and Practices of Machine Learning Systems Engineering},
  author       = {Reddi, Vijay Janapa},
  booktitle    = {2024 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ ISSS)},
  pages        = {41--42},
  year         = {2024},
  organization = {IEEE},
  url          = {https://mlsysbook.org}
}
```

### ËÆ∏ÂèØËØÅ

Êú¨È°πÁõÆÈááÁî®ÂèåËÆ∏ÂèØËØÅÁªìÊûÑÔºö

| Component | License | What It Means |
|-----------|---------|---------------|
| **Book content** | [CC BY‚ÄëNC‚ÄëND 4.0](LICENSE.md) | Âú®ÁΩ≤Âêç„ÄÅÈùûÂïÜ‰∏ö„ÄÅÁ¶ÅÊ≠¢ÊºîÁªéÁöÑÂâçÊèê‰∏ãËá™Áî±ÂàÜÂèë |
| **TinyTorch code** | [Apache 2.0](tinytorch/LICENSE) | Ëá™Áî±‰ΩøÁî®„ÄÅ‰øÆÊîπ„ÄÅÂàÜÂèëÂπ∂ÈôÑÂ∏¶‰∏ìÂà©‰øùÊä§ |

ÊïôÊùêÂÜÖÂÆπÔºàÁ´†ËäÇ„ÄÅÂõæË°®„ÄÅËß£ÈáäÔºâÂ±û‰∫éÊïôËÇ≤ËµÑÊñôÔºåÂ∫îÂú®ÁΩ≤Âêç‰∏îÈùûÂïÜ‰∏ö‰ΩøÁî®ÁöÑÂâçÊèê‰∏ãËá™Áî±ÂÖ±‰∫´„ÄÇËΩØ‰ª∂Ê°ÜÊû∂ÂàôÊòØ‰æõ‰ªª‰Ωï‰∫∫‰ΩøÁî®„ÄÅ‰øÆÊîπ„ÄÅÈõÜÊàêÁöÑÂ∑•ÂÖ∑„ÄÇ

---

## Ë¥°ÁåÆËÄÖ

‰ª•‰∏ã‰ºòÁßÄÁöÑË¥°ÁåÆËÄÖËÆ©Êú¨ËµÑÊ∫êÊõ¥Âä†ÂÆåÂñÑÔºö

<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->
<!-- ... (contributors omitted for brevity) -->
<!-- ALL-CONTRIBUTORS-LIST:END -->

---

<div align="center">

**[‚≠ê Âú® GitHub ‰∏äÁªôÊàë‰ª¨Âä†Êòü](https://github.com/harvard-edge/cs249r_book#support-this-work) ‚Ä¢ [‚úâÔ∏è ËÆ¢ÈòÖÊõ¥Êñ∞](https://buttondown.email/mlsysbook) ‚Ä¢ [üí¨ ÂèÇ‰∏éËÆ®ËÆ∫](https://github.com/harvard-edge/cs249r_book/discussions) ‚Ä¢ [üåê ËÆøÈóÆ mlsysbook.ai](https://mlsysbook.ai)**

*Êú¨ÊïôÊùêÁî± MLSysBook Á§æÂå∫ÂÄæÊÉÖÊâìÈÄ†„ÄÇ*

</div>


## Links discovered
- [![Book](https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/book-validate-dev.yml?branch=dev&label=Book&logo=githubactions&cacheSeconds=300)
- [![TinyTorch](https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/tinytorch-validate-dev.yml?branch=dev&label=TinyTorch&logo=python&cacheSeconds=300)
- [Updated](https://img.shields.io/github/last-commit/harvard-edge/cs249r_book/dev?label=Updated&logo=git&cacheSeconds=300)
- [![License](https://img.shields.io/badge/License-CC--BY--NC--ND%204.0-blue.svg)
- [![Cite](https://img.shields.io/badge/Cite-IEEE%202024-blue?logo=ieee)
- [![Fund Us](https://img.shields.io/badge/Fund%20Us-Open%20Collective-blue.svg?logo=open-collective)
- [üìñ Âú®Á∫øÈòÖËØª](https://mlsysbook.ai)
- [Tinyüî•Torch](https://mlsysbook.ai/tinytorch)
- [üìÑ ‰∏ãËΩΩ PDF](https://mlsysbook.ai/assets/downloads/Machine-Learning-Systems.pdf)
- [üìì ‰∏ãËΩΩ EPUB](https://mlsysbook.ai/epub)
- [üåê Êé¢Á¥¢ÁîüÊÄÅÁ≥ªÁªü](https://mlsysbook.org)
- [ÊïôÊùê](https://mlsysbook.ai)
- [Á¨¨ 1 Á´†](https://www.mlsysbook.ai/contents/core/introduction/introduction.html)
- [Benchmarking Á´†ËäÇ](https://mlsysbook.ai/contents/core/benchmarking/benchmarking.html)
- [ÂÖ•Èó®ÊåáÂçó](https://mlsysbook.ai/tinytorch/getting-started.html)
- [Á°¨‰ª∂Â•ó‰ª∂](https://mlsysbook.ai/kits)
- [Discussions](https://github.com/harvard-edge/cs249r_book/discussions)
- [üìñ ÊïôÊùê](https://mlsysbook.ai)
- [book/](https://github.com/harvard-edge/cs249r_book/blob/dev/README/book/README.md)
- [üî• TinyTorch](https://mlsysbook.ai/tinytorch)
- [tinytorch/](https://github.com/harvard-edge/cs249r_book/blob/dev/README/tinytorch/README.md)
- [üîß Hardware Kits](https://mlsysbook.ai/kits)
- [kits/](https://github.com/harvard-edge/cs249r_book/blob/dev/README/kits/README.md)
- [![Stars](https://img.shields.io/github/stars/harvard-edge/cs249r_book?style=for-the-badge&logo=github&color=gold)
- [![Star History Chart](https://api.star-history.com/svg?repos=harvard-edge/cs249r_book&type=Date)
- [Open Collective](https://opencollective.com/mlsysbook)
- [![Open Collective](https://img.shields.io/badge/üíù%20Support%20AI%20Education-Open%20Collective-blue.svg?style=for-the-badge)
- [üìñ **ÊïôÊùê**](https://mlsysbook.ai)
- [üî• **TinyTorch**](https://mlsysbook.ai/tinytorch)
- [üîß **Hardware Kits**](https://mlsysbook.ai/kits)
- [üåê **Ecosystem**](https://mlsysbook.org)
- [üí¨ **Discussions**](https://github.com/harvard-edge/cs249r_book/discussions)
- [book/docs/CONTRIBUTING.md](https://github.com/harvard-edge/cs249r_book/blob/dev/README/book/docs/CONTRIBUTING.md)
- [tinytorch/CONTRIBUTING.md](https://github.com/harvard-edge/cs249r_book/blob/dev/README/tinytorch/CONTRIBUTING.md)
- [kits/README.md](https://github.com/harvard-edge/cs249r_book/blob/dev/README/kits/README.md)
- [GitHub Issues](https://github.com/harvard-edge/cs249r_book/issues)
- [GitHub Discussions](https://github.com/harvard-edge/cs249r_book/discussions)
- [CC BY‚ÄëNC‚ÄëND 4.0](https://github.com/harvard-edge/cs249r_book/blob/dev/README/LICENSE.md)
- [Apache 2.0](https://github.com/harvard-edge/cs249r_book/blob/dev/README/tinytorch/LICENSE.md)
- [‚≠ê Âú® GitHub ‰∏äÁªôÊàë‰ª¨Âä†Êòü](https://github.com/harvard-edge/cs249r_book#support-this-work)
- [‚úâÔ∏è ËÆ¢ÈòÖÊõ¥Êñ∞](https://buttondown.email/mlsysbook)
- [üí¨ ÂèÇ‰∏éËÆ®ËÆ∫](https://github.com/harvard-edge/cs249r_book/discussions)
- [üåê ËÆøÈóÆ mlsysbook.ai](https://mlsysbook.ai)
- [English](https://github.com/harvard-edge/cs249r_book/blob/dev/README.md)
- [‰∏≠Êñá](https://github.com/harvard-edge/cs249r_book/blob/dev/README/README_zh.md)
- [Êó•Êú¨Ë™û](https://github.com/harvard-edge/cs249r_book/blob/dev/README/README_ja.md)
- [ÌïúÍµ≠Ïñ¥](https://github.com/harvard-edge/cs249r_book/blob/dev/README/README_ko.md)

--- tinytorch/README.md ---
<div align="center">

# Tinyüî•Torch

### Build Your Own ML Framework From Scratch

[![Version](https://img.shields.io/github/v/tag/harvard-edge/cs249r_book?filter=tinytorch-v*&label=version&color=D4740C&logo=fireship&logoColor=white)](https://github.com/harvard-edge/cs249r_book/releases?q=tinytorch)
[![Status](https://img.shields.io/badge/status-preview-orange?logo=github)](https://github.com/harvard-edge/cs249r_book/discussions/1076)
[![Docs](https://img.shields.io/badge/docs-mlsysbook.ai-blue?logo=readthedocs)](https://mlsysbook.ai/tinytorch)
[![Python](https://img.shields.io/badge/python-3.8+-3776ab?logo=python&logoColor=white)](https://python.org)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Harvard](https://img.shields.io/badge/Harvard-CS249r-A51C30?logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCI+PHBhdGggZmlsbD0id2hpdGUiIGQ9Ik0xMiAyTDIgN2wxMCA1IDEwLTV6TTIgMTdsMTAgNSAxMC01TTIgMTJsMTAgNSAxMC01Ii8+PC9zdmc+)](https://mlsysbook.ai)

**Most ML courses teach you to *use* frameworks. TinyTorch teaches you to *build* them.**

[The Vision](#why-tinytorch) ¬∑ [20 Modules](#-20-progressive-modules) ¬∑ [Share Feedback](https://github.com/harvard-edge/cs249r_book/discussions/1076)

</div>

---

> üöß **Preview Release** ‚Äî TinyTorch is functional but evolving. We're sharing early to shape the direction with community input rather than building in isolation.
>
> üìÖ **Classroom Ready**: Summer/Fall 2026 ¬∑ **Right Now**: [We want your feedback](#-help-shape-tinytorch)

---

## Why TinyTorch?

Everyone wants to be an astronaut üßë‚ÄçüöÄ. Very few want to be the rocket scientist üöÄ.

In machine learning, we see the same pattern. Everyone wants to train models, run inference, deploy AI. Very few want to understand how the frameworks actually work. Even fewer want to build one.

**The world is full of users. We do not have enough builders.**

### The Solution: AI Bricks üß±

TinyTorch teaches you the **AI bricks**‚Äîthe stable engineering foundations you can use to build any AI system.

- **Small enough to learn from**: bite-sized code that runs even on a Raspberry Pi
- **Big enough to matter**: showing the real architecture of how frameworks are built

A Harvard University course that transforms you from framework user to systems engineer, giving you the deep understanding needed to optimize, debug, and innovate at the foundation of AI.

---

## What You'll Build

A **complete ML framework** capable of:

üéØ **North Star Achievement**: Train CNNs for image classification
- Real computer vision on standard benchmark datasets
- Built entirely from scratch using only NumPy
- Competitive performance with modern frameworks

**Additional Capabilities**:
- GPT-style language models with attention mechanisms
- Modern optimizers (Adam, SGD) with learning rate scheduling
- Performance profiling, optimization, and competitive benchmarking

**No dependencies on PyTorch or TensorFlow - everything is YOUR code!**

---

## üõ† Help Shape TinyTorch

We're sharing TinyTorch early because we'd rather shape the direction with community input than build in isolation. Before diving into code, we want to hear from you:

**If you're a student:**
‚Üí What hands-on labs or projects would help you learn ML systems?

**If you teach:**
‚Üí What would make TinyTorch easy to bring into a course?

**If you're a practitioner:**
‚Üí What real-world systems tasks should we simulate?

**For everyone:**
‚Üí What natural extensions belong in this "AI bricks" model?

üì£ **[Share your thoughts in the discussion ‚Üí](https://github.com/harvard-edge/cs249r_book/discussions/1076)**

---

## Current Status

| Ready | In Progress | Coming Soon |
|-------|-------------|-------------|
| ‚úÖ All 20 modules implemented | üîß Documentation polish | üìÖ NBGrader integration |
| ‚úÖ Complete test suite (600+ tests) | üîß Edge case handling | üìÖ Community leaderboard |
| ‚úÖ `tito` CLI for workflows | üîß Instructor resources | üìÖ Binder/Colab support |
| ‚úÖ Historical milestone scripts | | |

**Want to explore the code?** [Browse the repository structure](#repository-structure) to see how modules are organized.

**Adventurous early adopter?** Local installation works, but expect rough edges. See the [setup guide](site/getting-started.md).

---

## 20 Progressive Modules

Build your framework through four progressive parts:

| Part | Modules | What You Build |
|------|---------|----------------|
| **I. Foundations** | 01-08 | Tensors, activations, layers, losses, dataloader, autograd, optimizers, training |
| **II. Vision** | 09 | Conv2d, CNNs for image classification |
| **III. Language** | 10-13 | Tokenization, embeddings, attention, transformers |
| **IV. Optimization** | 14-20 | Profiling, quantization, compression, acceleration, benchmarking, capstone |

Each module asks: **"Can I build this capability from scratch?"**

üìñ **[Full curriculum and module details ‚Üí](https://mlsysbook.ai/tinytorch)**

---

## Historical Milestones

As you progress, unlock recreations of landmark ML achievements:

| Year | Milestone | Your Achievement |
|------|-----------|------------------|
| 1958 | Perceptron | Binary classification with gradient descent |
| 1969 | XOR Crisis | Multi-layer networks solve non-linear problems |
| 1986 | Backpropagation | Multi-layer network training |
| 1998 | CNN Revolution | **Image classification with convolutions** |
| 2017 | Transformer Era | Language generation with self-attention |
| 2018+ | MLPerf | Production-ready optimization |

**These aren't toy demos** - they're historically significant ML achievements rebuilt with YOUR framework!

---

## Learning Philosophy

```python
# Traditional Course:
import torch
model.fit(X, y)  # Magic happens

# TinyTorch:
# You implement every component
# You measure memory usage
# You optimize performance
# You understand the systems
```

**Why Build Your Own Framework?**
- **Deep Understanding** - Know exactly what `loss.backward()` does
- **Systems Thinking** - Understand memory, compute, and scaling
- **Debugging Skills** - Fix problems at any level of the stack
- **Production Ready** - Learn patterns used in real ML systems

---

## Documentation

| Audience | Resources |
|----------|-----------|
| **Students** | [Course Website](https://mlsysbook.ai/tinytorch) „Éª [Getting Started](site/getting-started.md) |
| **Instructors** | [Instructor Guide](INSTRUCTOR.md) |
| **Contributors** | [Contributing Guide](CONTRIBUTING.md) |

---

## Repository Structure

```
TinyTorch/
‚îú‚îÄ‚îÄ src/                        # üíª Python source files (developers/contributors edit here)
‚îÇ   ‚îú‚îÄ‚îÄ 01_tensor/              # Module 01: Tensor operations from scratch
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 01_tensor.py        # Python source (version controlled)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ABOUT.md            # Conceptual overview & learning objectives
‚îÇ   ‚îú‚îÄ‚îÄ 02_activations/         # Module 02: ReLU, Softmax activations
‚îÇ   ‚îú‚îÄ‚îÄ 03_layers/              # Module 03: Linear layers, Module system
‚îÇ   ‚îú‚îÄ‚îÄ 04_losses/              # Module 04: MSE, CrossEntropy losses
‚îÇ   ‚îú‚îÄ‚îÄ 05_dataloader/          # Module 05: Efficient data pipelines
‚îÇ   ‚îú‚îÄ‚îÄ 06_autograd/            # Module 06: Automatic differentiation
‚îÇ   ‚îú‚îÄ‚îÄ 07_optimizers/          # Module 07: SGD, Adam optimizers
‚îÇ   ‚îú‚îÄ‚îÄ 08_training/            # Module 08: Complete training loops
‚îÇ   ‚îú‚îÄ‚îÄ 09_convolutions/        # Module 09: Conv2d, MaxPool2d, CNNs
‚îÇ   ‚îú‚îÄ‚îÄ 10_tokenization/        # Module 10: Text processing
‚îÇ   ‚îú‚îÄ‚îÄ 11_embeddings/          # Module 11: Token & positional embeddings
‚îÇ   ‚îú‚îÄ‚îÄ 12_attention/           # Module 12: Multi-head attention
‚îÇ   ‚îú‚îÄ‚îÄ 13_transformers/        # Module 13: Complete transformer blocks
‚îÇ   ‚îú‚îÄ‚îÄ 14_profiling/           # Module 14: Performance analysis
‚îÇ   ‚îú‚îÄ‚îÄ 15_quantization/        # Module 15: Model compression (precision reduction)
‚îÇ   ‚îú‚îÄ‚îÄ 16_compression/         # Module 16: Pruning & distillation
‚îÇ   ‚îú‚îÄ‚îÄ 17_acceleration/        # Module 17: Hardware optimization
‚îÇ   ‚îú‚îÄ‚îÄ 18_memoization/         # Module 18: KV-cache/memoization
‚îÇ   ‚îú‚îÄ‚îÄ 19_benchmarking/        # Module 19: Performance measurement
‚îÇ   ‚îî‚îÄ‚îÄ 20_capstone/            # Module 20: Complete ML systems
‚îÇ
‚îú‚îÄ‚îÄ modules/                    # üìì Generated notebooks (learners work here)
‚îÇ   ‚îú‚îÄ‚îÄ 01_tensor/              # Auto-generated from src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tensor.ipynb         # Jupyter notebook for learning
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md           # Practical implementation guide
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tensor.py           # Your implementation
‚îÇ   ‚îî‚îÄ‚îÄ ...                     # (20 module directories)
‚îÇ
‚îú‚îÄ‚îÄ site/                       # üåê Course website & documentation (Jupyter Book)
‚îÇ   ‚îú‚îÄ‚îÄ intro.md                # Landing page
‚îÇ   ‚îú‚îÄ‚îÄ _toc.yml                # Site navigation (links to modules)
‚îÇ   ‚îú‚îÄ‚îÄ _config.yml             # HTML website configuration
‚îÇ   ‚îú‚îÄ‚îÄ chapters/               # Course content chapters
‚îÇ   ‚îî‚îÄ‚îÄ modules/                # Module documentation
‚îÇ
‚îú‚îÄ‚îÄ milestones/                 # üèÜ Historical ML evolution - prove what you built!
‚îÇ   ‚îú‚îÄ‚îÄ 01_1958_perceptron/     # Rosenblatt's first trainable network
‚îÇ   ‚îú‚îÄ‚îÄ 02_1969_xor/            # Minsky's challenge & multi-layer solution
‚îÇ   ‚îú‚îÄ‚îÄ 03_1986_mlp/            # Backpropagation & MNIST digits
‚îÇ   ‚îú‚îÄ‚îÄ 04_1998_cnn/            # LeCun's CNNs & CIFAR-10
‚îÇ   ‚îú‚îÄ‚îÄ 05_2017_transformer/    # Attention mechanisms & language
‚îÇ   ‚îî‚îÄ‚îÄ 06_2018_mlperf/         # Modern optimization & profiling
‚îÇ
‚îú‚îÄ‚îÄ tito/                       # üéõÔ∏è CLI tool for streamlined workflows
‚îÇ   ‚îú‚îÄ‚îÄ main.py                 # Entry point
‚îÇ   ‚îú‚îÄ‚îÄ commands/               # 23 command modules
‚îÇ   ‚îî‚îÄ‚îÄ core/                   # Core utilities
‚îÇ
‚îú‚îÄ‚îÄ tinytorch/                  # üì¶ Generated package (import from here)
‚îÇ   ‚îú‚îÄ‚îÄ core/                   # Core ML components
‚îÇ   ‚îî‚îÄ‚îÄ ...                     # Your built framework!
‚îÇ
‚îî‚îÄ‚îÄ tests/                      # ‚úÖ Comprehensive test suite (600+ tests)
```

**Key workflow**: `src/*.py` ‚Üí `modules/*.ipynb` ‚Üí `tinytorch/*.py`

---

## Join the Community

TinyTorch is part of the [ML Systems Book](https://mlsysbook.ai) ecosystem. We're building an open community of learners and educators passionate about ML systems.

**Ways to get involved:**
- ‚≠ê Star this repo to show support
- üí¨ Join [Discussions](https://github.com/harvard-edge/cs249r_book/discussions) to ask questions
- üêõ Report issues or suggest improvements
- ü§ù Contribute modules, fixes, or documentation

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

---

## Related Projects

"TinyTorch" is a popular name for educational ML frameworks. We acknowledge excellent projects with similar names:

- [tinygrad](https://github.com/tinygrad/tinygrad) - George Hotz's minimalist framework
- [micrograd](https://github.com/karpathy/micrograd) - Andrej Karpathy's tiny autograd
- [MiniTorch](https://minitorch.github.io/) - Cornell's educational framework

**Our TinyTorch** distinguishes itself through its 20-module curriculum, NBGrader integration, ML systems focus, and connection to the [ML Systems Book](https://mlsysbook.ai) ecosystem.

---

## Contributors

Thanks to these wonderful people who helped improve TinyTorch!

**Legend:** ü™≤ Bug Hunter ¬∑ ‚ö° Code Warrior ¬∑ üìö Documentation Hero ¬∑ üé® Design Artist ¬∑ üß† Idea Generator ¬∑ üîé Code Reviewer ¬∑ üß™ Test Engineer ¬∑ üõ†Ô∏è Tool Builder

<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
<table>
  <tbody>
    <tr>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/profvjreddi"><img src="https://avatars.githubusercontent.com/profvjreddi?v=4?s=80" width="80px;" alt="Vijay Janapa Reddi"/><br /><sub><b>Vijay Janapa Reddi</b></sub></a><br />ü™≤ üßë‚Äçüíª üé® ‚úçÔ∏è üß† üîé üß™ üõ†Ô∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/kai4avaya"><img src="https://avatars.githubusercontent.com/kai4avaya?v=4?s=80" width="80px;" alt="kai"/><br /><sub><b>kai</b></sub></a><br />ü™≤ üßë‚Äçüíª üé® ‚úçÔ∏è üß™</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/minhdang26403"><img src="https://avatars.githubusercontent.com/minhdang26403?v=4?s=80" width="80px;" alt="Dang Truong"/><br /><sub><b>Dang Truong</b></sub></a><br />ü™≤ üßë‚Äçüíª ‚úçÔ∏è üß™</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/didier-durand"><img src="https://avatars.githubusercontent.com/didier-durand?v=4?s=80" width="80px;" alt="Didier Durand"/><br /><sub><b>Didier Durand</b></sub></a><br />ü™≤ üßë‚Äçüíª ‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/karthikdani"><img src="https://avatars.githubusercontent.com/karthikdani?v=4?s=80" width="80px;" alt="Karthik Dani"/><br /><sub><b>Karthik Dani</b></sub></a><br />ü™≤ üßë‚Äçüíª</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/avikde"><img src="https://avatars.githubusercontent.com/avikde?v=4?s=80" width="80px;" alt="Avik De"/><br /><sub><b>Avik De</b></sub></a><br />ü™≤ üß™</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/Takosaga"><img src="https://avatars.githubusercontent.com/Takosaga?v=4?s=80" width="80px;" alt="Takosaga"/><br /><sub><b>Takosaga</b></sub></a><br />ü™≤ ‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/rnjema"><img src="https://avatars.githubusercontent.com/rnjema?v=4?s=80" width="80px;" alt="rnjema"/><br /><sub><b>rnjema</b></sub></a><br />üßë‚Äçüíª üõ†Ô∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/joeswagson"><img src="https://avatars.githubusercontent.com/joeswagson?v=4?s=80" width="80px;" alt="joeswagson"/><br /><sub><b>joeswagson</b></sub></a><br />üßë‚Äçüíª üõ†Ô∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/AndreaMattiaGaravagno"><img src="https://avatars.githubusercontent.com/u/22458187?v=4?v=4?s=80" width="80px;" alt="AndreaMattiaGaravagno"/><br /><sub><b>AndreaMattiaGaravagno</b></sub></a><br />üßë‚Äçüíª ‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/AmirAlasady"><img src="https://avatars.githubusercontent.com/AmirAlasady?v=4?s=80" width="80px;" alt="Amir Alasady"/><br /><sub><b>Amir Alasady</b></sub></a><br />ü™≤</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/jettythek"><img src="https://avatars.githubusercontent.com/jettythek?v=4?s=80" width="80px;" alt="jettythek"/><br /><sub><b>jettythek</b></sub></a><br />üßë‚Äçüíª</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/wz1114841863"><img src="https://avatars.githubusercontent.com/wz1114841863?v=4?s=80" width="80px;" alt="wzz"/><br /><sub><b>wzz</b></sub></a><br />ü™≤</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/ngbolin"><img src="https://avatars.githubusercontent.com/u/9389997?v=4?v=4?s=80" width="80px;" alt="Ng Bo Lin"/><br /><sub><b>Ng Bo Lin</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
    <tr>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/keo-dara"><img src="https://avatars.githubusercontent.com/u/175544368?v=4?v=4?s=80" width="80px;" alt="keo-dara"/><br /><sub><b>keo-dara</b></sub></a><br />ü™≤</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/Kobra299"><img src="https://avatars.githubusercontent.com/u/4283156?v=4?v=4?s=80" width="80px;" alt="Wayne Norman"/><br /><sub><b>Wayne Norman</b></sub></a><br />ü™≤</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/lalalostcode"><img src="https://avatars.githubusercontent.com/u/149884766?v=4?v=4?s=80" width="80px;" alt="Ilham Rafiqin"/><br /><sub><b>Ilham Rafiqin</b></sub></a><br />ü™≤</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/oscarf189"><img src="https://avatars.githubusercontent.com/u/28113740?v=4?v=4?s=80" width="80px;" alt="Oscar Flores"/><br /><sub><b>Oscar Flores</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/harishb00a"><img src="https://avatars.githubusercontent.com/harishb00a?v=4?s=80" width="80px;" alt="harishb00a"/><br /><sub><b>harishb00a</b></sub></a><br />‚úçÔ∏è</td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/sotoblanco"><img src="https://avatars.githubusercontent.com/u/46135649?v=4?v=4?s=80" width="80px;" alt="Pastor Soto"/><br /><sub><b>Pastor Soto</b></sub></a><br />‚úçÔ∏è</td>
    </tr>
  </tbody>
</table>

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->

<!-- ALL-CONTRIBUTORS-LIST:END -->

**Recognize a contributor:** Comment on any issue or PR:
```
@all-contributors please add @username for bug, code, doc, or ideas
```

---

## Acknowledgments

Created by [Prof. Vijay Janapa Reddi](https://vijay.seas.harvard.edu) at Harvard University.

---

## License

MIT License - see [LICENSE](LICENSE) for details.

---

<div align="center">

**[üìñ Full Documentation](https://mlsysbook.ai/tinytorch)** „Éª **[üí¨ Discussions](https://github.com/harvard-edge/cs249r_book/discussions)** „Éª **[üåê ML Systems Book](https://mlsysbook.ai)**

**Start Small. Go Deep. Build ML Systems.**

</div>


## Links discovered
- [![Version](https://img.shields.io/github/v/tag/harvard-edge/cs249r_book?filter=tinytorch-v*&label=version&color=D4740C&logo=fireship&logoColor=white)
- [![Status](https://img.shields.io/badge/status-preview-orange?logo=github)
- [![Docs](https://img.shields.io/badge/docs-mlsysbook.ai-blue?logo=readthedocs)
- [![Python](https://img.shields.io/badge/python-3.8+-3776ab?logo=python&logoColor=white)
- [![License](https://img.shields.io/badge/license-MIT-green.svg)
- [![Harvard](https://img.shields.io/badge/Harvard-CS249r-A51C30?logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCI+PHBhdGggZmlsbD0id2hpdGUiIGQ9Ik0xMiAyTDIgN2wxMCA1IDEwLTV6TTIgMTdsMTAgNSAxMC01TTIgMTJsMTAgNSAxMC01Ii8+PC9zdmc+)
- [Share Feedback](https://github.com/harvard-edge/cs249r_book/discussions/1076)
- [Share your thoughts in the discussion ‚Üí](https://github.com/harvard-edge/cs249r_book/discussions/1076)
- [setup guide](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/site/getting-started.md)
- [Full curriculum and module details ‚Üí](https://mlsysbook.ai/tinytorch)
- [Course Website](https://mlsysbook.ai/tinytorch)
- [Getting Started](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/site/getting-started.md)
- [Instructor Guide](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/INSTRUCTOR.md)
- [Contributing Guide](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/CONTRIBUTING.md)
- [ML Systems Book](https://mlsysbook.ai)
- [Discussions](https://github.com/harvard-edge/cs249r_book/discussions)
- [CONTRIBUTING.md](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/CONTRIBUTING.md)
- [tinygrad](https://github.com/tinygrad/tinygrad)
- [micrograd](https://github.com/karpathy/micrograd)
- [MiniTorch](https://minitorch.github.io/)
- [Prof. Vijay Janapa Reddi](https://vijay.seas.harvard.edu)
- [LICENSE](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/LICENSE.md)
- [üìñ Full Documentation](https://mlsysbook.ai/tinytorch)
- [üí¨ Discussions](https://github.com/harvard-edge/cs249r_book/discussions)
- [üåê ML Systems Book](https://mlsysbook.ai)
- [<img src="https://avatars.githubusercontent.com/profvjreddi?v=4?s=80" width="80px;" alt="Vijay Janapa Reddi"/><br /><sub><b>Vijay Janapa Reddi</b></sub>](https://github.com/profvjreddi)
- [<img src="https://avatars.githubusercontent.com/kai4avaya?v=4?s=80" width="80px;" alt="kai"/><br /><sub><b>kai</b></sub>](https://github.com/kai4avaya)
- [<img src="https://avatars.githubusercontent.com/minhdang26403?v=4?s=80" width="80px;" alt="Dang Truong"/><br /><sub><b>Dang Truong</b></sub>](https://github.com/minhdang26403)
- [<img src="https://avatars.githubusercontent.com/didier-durand?v=4?s=80" width="80px;" alt="Didier Durand"/><br /><sub><b>Didier Durand</b></sub>](https://github.com/didier-durand)
- [<img src="https://avatars.githubusercontent.com/karthikdani?v=4?s=80" width="80px;" alt="Karthik Dani"/><br /><sub><b>Karthik Dani</b></sub>](https://github.com/karthikdani)
- [<img src="https://avatars.githubusercontent.com/avikde?v=4?s=80" width="80px;" alt="Avik De"/><br /><sub><b>Avik De</b></sub>](https://github.com/avikde)
- [<img src="https://avatars.githubusercontent.com/Takosaga?v=4?s=80" width="80px;" alt="Takosaga"/><br /><sub><b>Takosaga</b></sub>](https://github.com/Takosaga)
- [<img src="https://avatars.githubusercontent.com/rnjema?v=4?s=80" width="80px;" alt="rnjema"/><br /><sub><b>rnjema</b></sub>](https://github.com/rnjema)
- [<img src="https://avatars.githubusercontent.com/joeswagson?v=4?s=80" width="80px;" alt="joeswagson"/><br /><sub><b>joeswagson</b></sub>](https://github.com/joeswagson)
- [<img src="https://avatars.githubusercontent.com/u/22458187?v=4?v=4?s=80" width="80px;" alt="AndreaMattiaGaravagno"/><br /><sub><b>AndreaMattiaGaravagno</b></sub>](https://github.com/AndreaMattiaGaravagno)
- [<img src="https://avatars.githubusercontent.com/AmirAlasady?v=4?s=80" width="80px;" alt="Amir Alasady"/><br /><sub><b>Amir Alasady</b></sub>](https://github.com/AmirAlasady)
- [<img src="https://avatars.githubusercontent.com/jettythek?v=4?s=80" width="80px;" alt="jettythek"/><br /><sub><b>jettythek</b></sub>](https://github.com/jettythek)
- [<img src="https://avatars.githubusercontent.com/wz1114841863?v=4?s=80" width="80px;" alt="wzz"/><br /><sub><b>wzz</b></sub>](https://github.com/wz1114841863)
- [<img src="https://avatars.githubusercontent.com/u/9389997?v=4?v=4?s=80" width="80px;" alt="Ng Bo Lin"/><br /><sub><b>Ng Bo Lin</b></sub>](https://github.com/ngbolin)
- [<img src="https://avatars.githubusercontent.com/u/175544368?v=4?v=4?s=80" width="80px;" alt="keo-dara"/><br /><sub><b>keo-dara</b></sub>](https://github.com/keo-dara)
- [<img src="https://avatars.githubusercontent.com/u/4283156?v=4?v=4?s=80" width="80px;" alt="Wayne Norman"/><br /><sub><b>Wayne Norman</b></sub>](https://github.com/Kobra299)
- [<img src="https://avatars.githubusercontent.com/u/149884766?v=4?v=4?s=80" width="80px;" alt="Ilham Rafiqin"/><br /><sub><b>Ilham Rafiqin</b></sub>](https://github.com/lalalostcode)
- [<img src="https://avatars.githubusercontent.com/u/28113740?v=4?v=4?s=80" width="80px;" alt="Oscar Flores"/><br /><sub><b>Oscar Flores</b></sub>](https://github.com/oscarf189)
- [<img src="https://avatars.githubusercontent.com/harishb00a?v=4?s=80" width="80px;" alt="harishb00a"/><br /><sub><b>harishb00a</b></sub>](https://github.com/harishb00a)
- [<img src="https://avatars.githubusercontent.com/u/46135649?v=4?v=4?s=80" width="80px;" alt="Pastor Soto"/><br /><sub><b>Pastor Soto</b></sub>](https://github.com/sotoblanco)

--- tinytorch/binder/README.md ---
# Binder Environment Setup

This directory contains configuration files for running TinyTorch in cloud environments via [Binder](https://mybinder.org) and [Google Colab](https://colab.research.google.com).

## Files

- **`requirements.txt`**: Python dependencies for the Binder environment
- **`postBuild`**: Script that runs after environment setup to install TinyTorch

## How It Works

### Binder

When users click the "Launch Binder" button on any notebook page in the TinyTorch documentation:

1. Binder reads `binder/requirements.txt` to install Python dependencies
2. Binder runs `binder/postBuild` which:
   - Installs the TinyTorch package (`pip install -e .`)
   - Generates student notebooks from `src/*.py` files using Jupytext
   - Populates `modules/` with ready-to-use Jupyter notebooks
3. Users get a fully configured JupyterLab environment with TinyTorch and all notebooks ready to use

**Note**: The `modules/` directory is gitignored because notebooks are generated from the source `.py` files. This ensures students always get notebooks that match the current code.

**Binder URL Format:**
```
https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main
```

### Google Colab

Colab launch buttons automatically:
1. Clone the repository
2. Install dependencies from `binder/requirements.txt`
3. Run setup commands (users may need to manually run `pip install -e .`)

**Colab URL Format:**
```
https://colab.research.google.com/github/harvard-edge/cs249r_book/blob/main/tinytorch/path/to/notebook.ipynb
```

## Testing

To test your Binder setup:

1. **Test Binder Build:**
   ```bash
   # Visit: https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main
   # Or use the badge:
   [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main)
   ```

2. **Verify Installation:**
   Once Binder launches, test in a notebook:
   ```python
   import tinytorch
   print(tinytorch.__version__)
   ```

3. **Check Available Resources:**
   ```python
   import os
   print("Modules:", os.listdir("modules"))
   print("Assignments:", os.listdir("assignments"))
   print("Milestones:", os.listdir("milestones"))
   ```

## Troubleshooting

### Binder Build Fails

- Check `binder/requirements.txt` for syntax errors
- Verify `binder/postBuild` has execute permissions (`chmod +x binder/postBuild`)
- Review Binder build logs at: https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?urlpath=lab/tree/logs%2Fbuild.log

### Colab Import Errors

- Ensure `binder/requirements.txt` includes all dependencies
- Users may need to run: `!pip install -e .` in a Colab cell
- Check that the repository is public (Colab can't access private repos)

### Package Not Found

- Verify `postBuild` script runs `pip install -e .` correctly
- Check that `pyproject.toml` is in the repository root
- Ensure all dependencies in `requirements.txt` are compatible

## Deployment Environments

As documented in the TinyTorch paper, three deployment environments are supported:

1. **JupyterHub** (institutional server)
   - 8-core/32GB supports ~50 students
   - Best for classroom use

2. **Google Colab** (zero installation)
   - Best for MOOCs and self-paced learning
   - No setup required from students

3. **Local Installation** (`pip install tinytorch`)
   - Best for self-paced learning and development
   - Full control over environment

## Keeping Dependencies Updated

When updating dependencies:

1. Update `requirements.txt` (root) - for local development
2. Update `binder/requirements.txt` - for Binder/Colab
3. Update `docs/requirements.txt` - for documentation builds
4. Keep versions synchronized where possible

## References

- [Binder Documentation](https://mybinder.readthedocs.io/)
- [Jupyter Book Launch Buttons](https://jupyterbook.org/en/stable/interactive/launchbuttons.html)
- [Google Colab GitHub Integration](https://colab.research.google.com/github/)


## Links discovered
- [Binder](https://mybinder.org)
- [Google Colab](https://colab.research.google.com)
- [![Binder](https://mybinder.org/badge_logo.svg)
- [Binder Documentation](https://mybinder.readthedocs.io/)
- [Jupyter Book Launch Buttons](https://jupyterbook.org/en/stable/interactive/launchbuttons.html)
- [Google Colab GitHub Integration](https://colab.research.google.com/github/)

--- tinytorch/datasets/README.md ---
# TinyTorch Datasets

This directory contains datasets for TinyTorch milestone examples.

## Directory Structure

```
datasets/
‚îú‚îÄ‚îÄ tinydigits/     ‚Üê 8√ó8 handwritten digits (ships with repo, ~310KB)
‚îú‚îÄ‚îÄ tinytalks/      ‚Üê Q&A dataset for transformers (ships with repo, ~40KB)
‚îî‚îÄ‚îÄ README.md       ‚Üê This file
```

## Shipped Datasets (No Download Required)

### TinyDigits
- **Used by:** Milestones 03 & 04 (MLP and CNN examples)
- **Contents:** 1,000 training + 200 test samples
- **Format:** 8√ó8 grayscale images, pickled
- **Size:** ~310 KB
- **Purpose:** Fast iteration on real image classification

### TinyTalks
- **Used by:** Milestone 05 (Transformer/GPT examples)
- **Contents:** 350 Q&A pairs across 5 difficulty levels
- **Format:** Plain text (Q: ... A: ... format)
- **Size:** ~40 KB
- **Purpose:** Character-level conversational AI training

## Downloaded Datasets (On-Demand)

The milestones automatically download larger datasets when needed:

### MNIST
- **Used by:** `milestones/03_1986_mlp/02_rumelhart_mnist.py`
- **Downloads to:** `milestones/datasets/mnist/`
- **Contents:** 60K training + 10K test samples
- **Format:** 28√ó28 grayscale images
- **Size:** ~10 MB compressed
- **Auto-downloaded by:** `milestones/data_manager.py`

### CIFAR-10
- **Used by:** `milestones/04_1998_cnn/02_lecun_cifar10.py`
- **Downloads to:** `milestones/datasets/cifar-10/`
- **Contents:** 50K training + 10K test samples
- **Format:** 32√ó32 RGB images
- **Size:** ~170 MB compressed
- **Auto-downloaded by:** `milestones/data_manager.py`

## Design Philosophy

**Shipped datasets** follow Karpathy's "~1K samples" philosophy:
- Small enough to ship with repo
- Large enough for meaningful learning
- Fast training (seconds to minutes)
- Instant gratification for students

**Downloaded datasets** are full benchmarks:
- Standard ML benchmarks (MNIST, CIFAR-10)
- Larger, slower, more realistic
- Auto-downloaded only when needed
- Used for scaling demonstrations

## Total Repository Size

- **Shipped data:** ~350 KB (tinydigits + tinytalks)
- **USB-friendly:** Entire repo fits on any device
- **Offline-capable:** Core milestones work without internet
- **Git-friendly:** No large binary files in version control


--- tinytorch/milestones/README.md ---
# TinyTorch Milestones

Milestones are capstone experiences that bring together everything you've built in the TinyTorch modules. Each milestone recreates a pivotal moment in ML history using YOUR implementations.

## How Milestones Work

After completing a set of modules, you unlock the ability to run a milestone. Each milestone:

1. **Uses YOUR code** - Every tensor operation, gradient computation, and layer runs on code YOU wrote
2. **Recreates history** - Experience the same breakthroughs researchers achieved decades ago
3. **Proves understanding** - If it works, you truly understand how these systems function

## Available Milestones

| ID | Name | Year | Required Modules | What You'll Do |
|----|------|------|------------------|----------------|
| 01 | Perceptron | 1958 | 01-03 | Build Rosenblatt's first neural network (forward pass) |
| 02 | XOR Crisis | 1969 | 01-03 | Experience the XOR limitation that triggered AI Winter |
| 03 | MLP Revival | 1986 | 01-08 | Train MLPs to solve XOR + recognize digits |
| 04 | CNN Revolution | 1998 | 01-09 | Build LeNet for image recognition |
| 05 | Transformer Era | 2017 | 01-08, 11-13 | Build attention and generate text |
| 06 | MLPerf Benchmarks | 2018 | 01-08, 14-19 | Optimize and benchmark your neural networks |

## Running Milestones

```bash
# List available milestones and your progress
tito milestone list

# Run a specific milestone (all parts)
tito milestone run 03

# Run a specific part of a multi-part milestone
tito milestone run 03 --part 1  # Part 1: XOR Solved
tito milestone run 03 --part 2  # Part 2: TinyDigits

# Get detailed info about a milestone
tito milestone info 05
```

## Directory Structure

```
milestones/
‚îú‚îÄ‚îÄ 01_1958_perceptron/     # Milestone 01: Rosenblatt's Perceptron
‚îú‚îÄ‚îÄ 02_1969_xor/            # Milestone 02: XOR Problem
‚îú‚îÄ‚îÄ 03_1986_mlp/            # Milestone 03: Backpropagation MLP
‚îú‚îÄ‚îÄ 04_1998_cnn/            # Milestone 04: LeNet CNN
‚îú‚îÄ‚îÄ 05_2017_transformer/    # Milestone 05: Attention Mechanism
‚îú‚îÄ‚îÄ 06_2018_mlperf/         # Milestone 06: Optimization Olympics
‚îú‚îÄ‚îÄ extras/                 # Additional demos and variants (see extras/README.md)
‚îî‚îÄ‚îÄ data_manager.py         # Shared dataset management utility
```

## The Journey

```
Module 01-03          Module 04-06           Module 08-09
    ‚îÇ                     ‚îÇ                      ‚îÇ
    ‚ñº                     ‚ñº                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ MS 01   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ MS 02   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ MS 03   ‚îÇ
‚îÇ 1957    ‚îÇ         ‚îÇ 1969    ‚îÇ            ‚îÇ 1986    ‚îÇ
‚îÇ Forward ‚îÇ         ‚îÇ XOR     ‚îÇ            ‚îÇ Backprop‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                ‚îÇ
                    Module 11-13                ‚îÇ  Module 09
                        ‚îÇ                       ‚ñº      ‚îÇ
                        ‚ñº                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ MS 04   ‚îÇ‚óÑ‚îÄ‚îò
                  ‚îÇ MS 05    ‚îÇ            ‚îÇ 1998    ‚îÇ
                  ‚îÇ 2017     ‚îÇ            ‚îÇ CNN     ‚îÇ
                  ‚îÇ Attention‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚îÇ  Module 14-19
                        ‚ñº
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ MS 06   ‚îÇ
                  ‚îÇ 2018    ‚îÇ
                  ‚îÇ Optimize‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Success Criteria

Each milestone has specific success criteria. Passing means your implementation is correct:

- **Milestone 01**: Forward pass produces reasonable outputs
- **Milestone 02**: Demonstrates XOR is unsolvable with single layer (75% max accuracy)
- **Milestone 03**: Part 1 solves XOR (100% accuracy), Part 2 achieves 85%+ on TinyDigits
- **Milestone 04**: TinyDigits achieves 90%+ accuracy with CNN
- **Milestone 05**: Pass all three attention challenges (95%+ accuracy)
- **Milestone 06**: Part 1 completes optimization pipeline, Part 2 shows KV cache speedup

## Troubleshooting

If a milestone fails:

1. Check that all required modules are completed: `tito module status`
2. Run the module tests: `tito test <module_number>`
3. Look at the specific error message for debugging hints
4. Review the milestone's docstring for implementation requirements


--- tinytorch/paper/README.md ---
# TinyTorch Research Paper

Complete LaTeX source for the TinyTorch research paper.

---

## Files

- **[paper.tex](paper.tex)** - Main paper (~12-15 pages, two-column format)
- **[references.bib](references.bib)** - Bibliography (22 references)
- **[compile_paper.sh](compile_paper.sh)** - Build script (requires LaTeX installation)

---

## Quick Start: Get PDF

### Option 1: Overleaf (Recommended)

1. Go to [Overleaf.com](https://www.overleaf.com)
2. Create free account
3. Upload `paper.tex` and `references.bib`
4. Click "Recompile"
5. Download PDF

### Option 2: Local Compilation

```bash
./compile_paper.sh
```

Requires LaTeX installation (MacTeX or BasicTeX).

---

## Paper Details

- **Format**: Two-column LaTeX (conference-standard)
- **Length**: ~12-15 pages
- **Sections**: 7 complete sections
- **Tables**: 3 (framework comparison, learning objectives, performance benchmarks)
- **Code listings**: 5 (syntax-highlighted Python examples)
- **References**: 22 citations

---

## Key Contributions

1. **Progressive disclosure via monkey-patching** - Novel pedagogical pattern
2. **Systems-first curriculum design** - Memory/FLOPs from Module 01
3. **Historical milestone validation** - 70 years of ML as learning modules
4. **Constructionist framework building** - Students build complete ML system

Framed as design contribution with empirical validation planned for Fall 2025.

---

## Submission Venues

- **ArXiv** - Immediate (establish priority)
- **SIGCSE 2026** - August deadline (may need 6-page condensed version)
- **ICER 2026** - After classroom data (full empirical study)

---

Ready for submission! Upload to Overleaf to get your PDF.


## Links discovered
- [paper.tex](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/paper/paper.tex)
- [references.bib](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/paper/references.bib)
- [compile_paper.sh](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/paper/compile_paper.sh)
- [Overleaf.com](https://www.overleaf.com)

--- tinytorch/site/README.md ---
# Tiny\raisebox{-0.1em}{\includegraphics[height=1em]{../_static/logos/fire-emoji.png}}Torch Documentation Site

This directory contains the TinyTorch course website and documentation.

##  Building the Site

All builds are managed through the Makefile:

```bash
cd tinytorch/site

# Build HTML website
make html

# Build PDF (requires LuaLaTeX)
make pdf

# Clean build artifacts
make clean

# Install dependencies
make install
```

## üìö User Documentation

- **`STUDENT_QUICKSTART.md`** - Getting started guide for students
- **`instructor-guide.md`** - Setup and grading guide for instructors
- **`quickstart-guide.md`** - Quick start guide for all users

## üîß Development Documentation

### Development Standards
- **`development/module-rules.md`** - Module development standards and patterns
- **`development/DEVELOPER_SETUP.md`** - Developer environment setup
- **`development/MODULE_ABOUT_TEMPLATE.md`** - Template for module documentation

### NBGrader Integration
- **`nbgrader/NBGrader_Quick_Reference.md`** - Daily use commands and workflow
- **`nbgrader/NBGrader_Text_Response_Technical_Implementation.md`** - Technical implementation details


**Start here**:
- **Students**: Read `STUDENT_QUICKSTART.md`
- **Instructors**: Read `instructor-guide.md`
- **Developers**: Read `development/module-rules.md`


--- tinytorch/tests/README.md ---
# TinyTorch Test Suite

Comprehensive testing organized by purpose and scope.

## Test Organization

### üì¶ Module Tests (`XX_modulename/`)
**Purpose**: Test individual module functionality
**Scope**: Single module, isolated behavior
**Example**: `01_tensor/test_progressive_integration.py`

These tests validate that each module works correctly in isolation.

### üîó Integration Tests (`integration/`)
**Purpose**: Test cross-module interactions
**Scope**: Multiple modules working together
**Files**:
- `test_gradient_flow.py` - **CRITICAL**: Validates gradients flow through entire training stack
- `test_end_to_end_training.py` - Full training loops (TODO)
- `test_module_compatibility.py` - Module interfaces (TODO)

**Why this matters**:
- Catches bugs that unit tests miss
- Validates the "seams" between modules
- Ensures training actually works end-to-end

### üêõ Debugging Tests (`debugging/`)
**Purpose**: Catch common student pitfalls
**Scope**: Pedagogical - teaches debugging
**Files**:
- `test_gradient_vanishing.py` - Detect/diagnose vanishing gradients (TODO)
- `test_gradient_explosion.py` - Detect/diagnose exploding gradients (TODO)
- `test_common_mistakes.py` - "Did you forget backward()?" style tests (TODO)

**Philosophy**: When these tests fail, the error message should teach the student what went wrong and how to fix it.

### ‚ö° Autograd Edge Cases (`06_autograd/`)
**Purpose**: Stress-test autograd system
**Scope**: Autograd internals and edge cases
**Files**:
- `test_broadcasting.py` - Broadcasting gradient bugs (TODO)
- `test_computation_graph.py` - Graph construction edge cases (TODO)
- `test_backward_edge_cases.py` - Numerical stability, etc. (TODO)

## Running Tests

### Standard Mode
```bash
pytest tests/ -v                    # All tests
pytest tests/integration/ -v        # Integration tests only
pytest tests/01_tensor/ -v          # Specific module
```

### üéì Educational Mode (Recommended for Students)
```bash
pytest tests/ --tinytorch           # Rich output with WHAT/WHY context
pytest tests/01_tensor/ --tinytorch # Single module with education
```

**Educational mode shows:**
- Module groupings before running
- What each test does (WHAT)
- Why it matters (WHY)
- Learning tips on failure (STUDENT LEARNING)
- Clear pass/fail indicators with Rich formatting

### Run without pytest
```bash
python tests/integration/test_gradient_flow.py
```

## Test Philosophy

1. **Integration tests catch real bugs**: The gradient flow test caught the exact bugs that prevented training
2. **Descriptive names**: Test names should explain what they test
3. **Good error messages**: When tests fail, students should understand why
4. **Pedagogical value**: Tests teach correct usage patterns

## Educational Test Docstrings

All `*_core.py` test files use a structured docstring format:

```python
def test_tensor_addition(self):
    """
    WHAT: Element-wise tensor addition.

    WHY: Addition is used everywhere in neural networks:
    - Adding bias to layer output: y = Wx + b
    - Residual connections: output = layer(x) + x

    STUDENT LEARNING: Operations return new Tensors (functional style).
    """
```

This format enables the `--tinytorch` flag to show educational context when tests run.

## Adding New Tests

When adding a test, ask:
- **Is it testing one module?** ‚Üí Put in `XX_modulename/`
- **Is it testing modules working together?** ‚Üí Put in `integration/`
- **Is it teaching debugging?** ‚Üí Put in `debugging/`
- **Is it an autograd edge case?** ‚Üí Put in `06_autograd/`

## Most Important Tests

üî• **Must pass before merging**:
- `integration/test_gradient_flow.py` - If this fails, training is broken

üìö **Module validation**:
- Each module's inline tests (in `modules/`)
- Module-specific tests in `tests/XX_modulename/`

## Test Coverage Goals

- ‚úÖ All tensor operations have gradient tests
- ‚úÖ All layers compute gradients correctly
- ‚úÖ All activations integrate with autograd
- ‚úÖ All loss functions compute gradients
- ‚úÖ All optimizers update parameters
- ‚è≥ End-to-end training converges (TODO)
- ‚è≥ Common pitfalls are detected (TODO)


--- tinytorch/tools/README.md ---
# Development Tools

This directory contains tools for TinyTorch maintainers and contributors.

## Structure

- **`dev/`** - Development environment setup and utilities
- **`build/`** - Build scripts for generating notebooks and metadata
- **`maintenance/`** - Maintenance and cleanup scripts

## For Students

Students don't need anything in this directory. Use the main setup scripts in the project root.

## For Developers

See `docs/development/DEVELOPER_SETUP.md` for complete developer documentation.


--- tinytorch/datasets/tinydigits/README.md ---
# TinyDigits Dataset

A curated subset of the sklearn digits dataset for rapid ML prototyping and educational demonstrations.

Following Karpathy's "~1000 samples" philosophy for educational datasets.

## Contents

- **Training**: 1000 samples (100 per digit, 0-9)
- **Test**: 200 samples (20 per digit, balanced)
- **Format**: 8√ó8 grayscale images, float32 normalized [0, 1]
- **Size**: ~310 KB total (vs 10 MB MNIST, 50√ó smaller)

## Files

```
datasets/tinydigits/
‚îú‚îÄ‚îÄ train.pkl  # {'images': (1000, 8, 8), 'labels': (1000,)}
‚îî‚îÄ‚îÄ test.pkl   # {'images': (200, 8, 8), 'labels': (200,)}
```

## Usage

```python
import pickle

# Load training data
with open('datasets/tinydigits/train.pkl', 'rb') as f:
    data = pickle.load(f)
    train_images = data['images']  # (1000, 8, 8)
    train_labels = data['labels']  # (1000,)

# Load test data
with open('datasets/tinydigits/test.pkl', 'rb') as f:
    data = pickle.load(f)
    test_images = data['images']   # (200, 8, 8)
    test_labels = data['labels']   # (200,)
```

## Purpose

**Educational Infrastructure**: Designed for teaching ML systems with real data at edge-device scale.

Following Andrej Karpathy's philosophy: "~1000 samples is the sweet spot for educational datasets."

- **Decent accuracy**: Achieves ~80% test accuracy on MLPs (vs <20% with 150 samples)
- **Fast training**: <10 sec on CPU, instant feedback loop
- **Balanced classes**: Perfect 100 samples per digit (0-9)
- **Offline-capable**: Ships with repo, no downloads needed
- **USB-friendly**: 310 KB fits on any device, even RasPi0
- **Real learning curve**: Model improves visibly across epochs

## Curation Process

Created from the sklearn digits dataset (8√ó8 downsampled MNIST):

1. **Balanced Sampling**: 100 training samples per digit class (1000 total)
2. **Test Split**: 20 samples per digit (200 total) from remaining examples
3. **Random Seeding**: Reproducible selection (seed=42)
4. **Normalization**: Pixels normalized to [0, 1] range
5. **Shuffled**: Training and test sets randomly shuffled for fair evaluation

The sklearn digits dataset itself is derived from the UCI ML hand-written digits datasets.

## Why TinyDigits vs Full MNIST?

| Metric | MNIST | TinyDigits | Benefit |
|--------|-------|------------|---------|
| Samples | 60,000 | 1,000 | 60√ó fewer samples |
| File size | 10 MB | 310 KB | 32√ó smaller |
| Train time | 5-10 min | <10 sec | 30-60√ó faster |
| Test accuracy (MLP) | ~92% | ~80% | Close enough for learning |
| Download | Network required | Ships with repo | Always available |
| Resolution | 28√ó28 (784 pixels) | 8√ó8 (64 pixels) | Faster forward pass |
| Edge deployment | Challenging | Perfect | Works on RasPi0 |

## Educational Progression

TinyDigits serves as the first step in a scaffolded learning path:

1. **TinyDigits (8√ó8)** ‚Üê Start here: Learn MLP/CNN basics with instant feedback
2. **Full MNIST (28√ó28)** ‚Üê Graduate to: Standard benchmark, longer training
3. **CIFAR-10 (32√ó32 RGB)** ‚Üê Advanced: Color images, real-world complexity

## Citation

TinyDigits is curated from the sklearn digits dataset for educational use in TinyTorch.

**Original Source**:
- sklearn.datasets.load_digits()
- Derived from UCI ML hand-written digits datasets
- License: BSD 3-Clause (sklearn)

**TinyTorch Curation**:
```bibtex
@misc{tinydigits2025,
  title={TinyDigits: Curated Educational Dataset for ML Systems Learning},
  author={TinyTorch Project},
  year={2025},
  note={Balanced subset of sklearn digits optimized for edge deployment}
}
```

## Generation

To regenerate this dataset from the original sklearn data:

```bash
python3 datasets/tinydigits/create_tinydigits.py
```

This ensures reproducibility and allows customization for specific educational needs.

## License

See [LICENSE](LICENSE) for details. TinyDigits inherits the BSD 3-Clause license from sklearn.


## Links discovered
- [LICENSE](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/datasets/tinydigits/LICENSE.md)

--- tinytorch/datasets/tinytalks/README.md ---
# TinyTalks: A Conversational Q&A Dataset for Educational Transformers

**A carefully curated question-answering dataset designed for learning transformer architectures**

[![License: CC BY 4.0](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/)
[![Size: ~50KB](https://img.shields.io/badge/Size-~50KB-blue.svg)]()
[![Version: 1.0.0](https://img.shields.io/badge/Version-1.0.0-green.svg)]()

---

## üìñ Overview

**TinyTalks** is a lightweight, pedagogically-designed conversational dataset for training transformer models in educational settings. Unlike large-scale datasets that require hours of training, TinyTalks enables students to see their first transformer learn meaningful patterns in **under 5 minutes**.

### Why TinyTalks?

‚úÖ **Fast Training** - Trains in 3-5 minutes on a laptop
‚úÖ **Verifiable Learning** - Clear success metrics (correct vs. incorrect answers)
‚úÖ **Progressive Difficulty** - 5 levels from greetings to reasoning
‚úÖ **Educational Focus** - Designed for "aha!" moments, not benchmarks
‚úÖ **Zero Dependencies** - Ships with TinyTorch, no downloads needed
‚úÖ **Reproducible** - Deterministic generation, versioned releases

---

## üìä Dataset Statistics

| Property | Value |
|----------|-------|
| **Total Q&A Pairs** | 350 |
| **File Size** | ~40 KB |
| **Vocabulary Size** | ~1,500 unique tokens (character-level) |
| **Avg Question Length** | 8 words |
| **Avg Answer Length** | 10 words |
| **Training Split** | 245 pairs (70%) |
| **Validation Split** | 53 pairs (15%) |
| **Test Split** | 52 pairs (15%) |

---

## üéØ Content Structure

TinyTalks is organized into **5 progressive difficulty levels**:

### **Level 1: Greetings & Identity (50 pairs)**
Basic conversational patterns and self-identification.

```
Q: Hello!
A: Hi there! How can I help you today?

Q: What is your name?
A: I am TinyBot, a simple AI assistant.
```

**Learning Goal:** Model learns conversation structure and identity.

---

### **Level 2: Simple Facts (100 pairs)**
Factual knowledge about the world (colors, animals, objects).

```
Q: What color is the sky?
A: The sky is blue during the day.

Q: How many legs does a dog have?
A: A dog has four legs.
```

**Learning Goal:** Model learns to retrieve and state facts.

---

### **Level 3: Basic Math (50 pairs)**
Simple arithmetic operations (single-digit addition, subtraction).

```
Q: What is 2 plus 3?
A: 2 plus 3 equals 5.

Q: What is 7 minus 4?
A: 7 minus 4 equals 3.
```

**Learning Goal:** Model learns basic numerical reasoning.

---

### **Level 4: Common Sense Reasoning (100 pairs)**
Purpose, usage, and logical relationships.

```
Q: What do you use an umbrella for?
A: You use an umbrella to stay dry in the rain.

Q: Where do fish live?
A: Fish live in water.
```

**Learning Goal:** Model learns causal and functional relationships.

---

### **Level 5: Multi-turn Context (50 pairs)**
Simple context tracking across exchanges.

```
Q: I like pizza.
A: Pizza is delicious! What toppings do you like?

Q: I like pepperoni.
A: Pepperoni is a popular choice!
```

**Learning Goal:** Model begins to track conversational context.

---

## üöÄ Quick Start

### Loading the Dataset

```python
# Load full dataset
with open('datasets/tinytalks/tinytalks_v1.txt', 'r') as f:
    text = f.read()

# Or use pre-split versions
with open('datasets/tinytalks/splits/train.txt', 'r') as f:
    train_text = f.read()
```

### Training a Transformer

```python
# See milestones/05_2017_transformer/tinybot_demo.py for full example
from tinytorch.models.transformer import GPT
from tinytorch.text.tokenization import CharTokenizer

# Initialize model
tokenizer = CharTokenizer()
tokenizer.fit(train_text)

model = GPT(
    vocab_size=len(tokenizer),
    embed_dim=128,
    num_layers=4,
    num_heads=4,
    max_seq_len=64
)

# Train for 5 minutes ‚Üí See meaningful results!
```

### Expected Performance

After training for **10-20 epochs** (~3-5 minutes):
- ‚úÖ Correctly answers Level 1-2 questions (~80% accuracy)
- ‚úÖ Maintains grammatical structure
- ‚úÖ Generates coherent (if not always correct) responses
- ‚ö†Ô∏è Level 3-5 show partial understanding

This demonstrates the transformer has **learned patterns**, not just memorized.

---

## üìê Dataset Format

**Simple, human-readable text format:**

```
Q: [Question text]
A: [Answer text]

Q: [Next question]
A: [Next answer]
```

**Rationale:**
- Character-level tokenization (no special tokenizers needed)
- Easy to inspect and validate
- Works with any text processing pipeline
- Human-readable for debugging

**Delimiter:** Empty line separates Q&A pairs.

---

## üî¨ Dataset Creation Methodology

### Generation Process

1. **Manual Curation** - All Q&A pairs hand-written by TinyTorch maintainers
2. **Diversity Sampling** - Systematic coverage of topics within each level
3. **Quality Control** - Each pair reviewed for grammar, factual accuracy, appropriateness
4. **Balance Verification** - Ensured even distribution across levels
5. **Reproducibility** - Generation script (`scripts/generate_tinytalks.py`) produces identical output

### Quality Assurance

- ‚úÖ Grammar check (automated + manual review)
- ‚úÖ Factual accuracy verification
- ‚úÖ No offensive or biased content
- ‚úÖ No personally identifiable information
- ‚úÖ Balanced topic distribution
- ‚úÖ Appropriate for all ages

### Validation Script

```bash
python datasets/tinytalks/scripts/validate_dataset.py
```

Checks:
- Format consistency
- No duplicate pairs
- Balanced splits
- Character encoding (UTF-8)
- Line endings (Unix)

---

## üìä Dataset Statistics

Run `scripts/stats.py` to generate:

```bash
python datasets/tinytalks/scripts/stats.py
```

Output:
- Total pairs per level
- Vocabulary statistics
- Length distributions
- Split sizes
- Character frequency

---

## üéì Educational Use Cases

### Primary Use: Module 13 (Transformers)

TinyTalks is designed as the **canonical dataset** for TinyTorch's Transformer milestone:

- **milestones/05_2017_transformer/tinybot_demo.py** - Main training demo
- Students see their first transformer learn in < 5 minutes
- Clear success metric: Can it answer questions?
- "Wow, I built this!" moment

### Secondary Uses

1. **Tokenization** (Module 10) - Character vs. BPE comparison
2. **Embeddings** (Module 11) - Visualize learned embeddings
3. **Attention** (Module 12) - Inspect attention patterns on Q&A
4. **Debugging** - Small enough to trace gradients manually
5. **Experimentation** - Test architecture changes quickly

---

## ‚öñÔ∏è License

**Creative Commons Attribution 4.0 International (CC BY 4.0)**

You are free to:
- ‚úÖ Share ‚Äî copy and redistribute in any format
- ‚úÖ Adapt ‚Äî remix, transform, and build upon the material
- ‚úÖ Commercial use allowed

Under these terms:
- **Attribution** ‚Äî Cite TinyTalks (see below)
- **No additional restrictions**

See [LICENSE](LICENSE) for full text.

---

## üìö Citation

If you use TinyTalks in your work, please cite:

```bibtex
@dataset{tinytalks2025,
  title={TinyTalks: A Conversational Q\&A Dataset for Educational Transformers},
  author={TinyTorch Contributors},
  year={2025},
  publisher={GitHub},
  url={https://github.com/VJ/TinyTorch/tree/main/datasets/tinytalks},
  version={1.0.0}
}
```

**Text citation:**
TinyTorch Contributors. (2025). TinyTalks: A Conversational Q&A Dataset for Educational Transformers (Version 1.0.0). https://github.com/VJ/TinyTorch

---

## üîÑ Versioning

**Version 1.0.0** (Current)
- Initial release: 350 Q&A pairs across 5 levels
- Character-level format
- 70/15/15 train/val/test split

**Planned:**
- v1.1 - Add 100 more Level 4-5 pairs for better reasoning
- v2.0 - Multi-language support (Spanish, French)
- v3.0 - Expanded to 1,000 pairs with more complex reasoning

See [CHANGELOG.md](CHANGELOG.md) for detailed history.

---

## ü§ù Contributing

We welcome contributions! Ways to help:

1. **Report Issues** - Found a factual error or typo? Open an issue.
2. **Suggest Q&A Pairs** - Submit ideas for new questions via PR.
3. **Translations** - Help translate TinyTalks to other languages.
4. **Validation** - Test on different models and report results.

**Guidelines:**
- Follow existing format and style
- Ensure factual accuracy
- Keep language simple and clear
- No offensive or biased content
- Appropriate for all ages (G-rated)

See [CONTRIBUTING.md](../../CONTRIBUTING.md) for details.

---

## üìû Contact & Support

- **Issues:** [GitHub Issues](https://github.com/VJ/TinyTorch/issues)
- **Discussions:** [GitHub Discussions](https://github.com/VJ/TinyTorch/discussions)
- **Email:** tinytorch@example.com (for sensitive issues)

---

## üôè Acknowledgments

**Inspired by:**
- bAbI Dataset (Facebook AI Research) - Reasoning tasks
- SQuAD - Question answering format
- TinyStories - Simplicity philosophy
- TinyTorch Community - Feedback and testing

**Created for:**
- Students learning transformer architectures
- Educators teaching NLP
- Researchers prototyping small models
- Developers testing implementations

---

## üìñ Additional Documentation

- **[DATASHEET.md](DATASHEET.md)** - Comprehensive dataset metadata (Gebru et al. format)
- **[examples/demo_usage.py](examples/demo_usage.py)** - Complete usage examples
- **[scripts/README.md](scripts/README.md)** - Scripts documentation

---

## üåü Why "TinyTalks"?

The name embodies our philosophy:

- **Tiny** - Small enough to train in minutes, not hours
- **Talks** - Conversational, accessible, human-like
- **Educational** - Designed for learning, not leaderboards

Just like TinyTorch makes deep learning accessible, TinyTalks makes conversational AI **immediate and tangible**.

---

*Built with ‚ù§Ô∏è by the TinyTorch community*

*"The best way to understand transformers is to see them learn."*


## Links discovered
- [![License: CC BY 4.0](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)
- [![Size: ~50KB](https://img.shields.io/badge/Size-~50KB-blue.svg)
- [![Version: 1.0.0](https://img.shields.io/badge/Version-1.0.0-green.svg)
- [LICENSE](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/datasets/tinytalks/LICENSE.md)
- [CHANGELOG.md](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/datasets/tinytalks/CHANGELOG.md)
- [CONTRIBUTING.md](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/CONTRIBUTING.md)
- [GitHub Issues](https://github.com/VJ/TinyTorch/issues)
- [GitHub Discussions](https://github.com/VJ/TinyTorch/discussions)
- [DATASHEET.md](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/datasets/tinytalks/DATASHEET.md)
- [examples/demo_usage.py](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/datasets/tinytalks/examples/demo_usage.py)
- [scripts/README.md](https://github.com/harvard-edge/cs249r_book/blob/dev/tinytorch/datasets/tinytalks/scripts/README.md)
