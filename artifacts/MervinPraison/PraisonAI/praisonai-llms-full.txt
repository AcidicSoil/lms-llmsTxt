# llms-full (private-aware)
> Built from GitHub files and website pages. Large files may be truncated.

--- examples/python/mcp/mcp-transports-overview.py ---
"""
MCP Transports Overview Example

Demonstrates all MCP transport types and auto-detection.
The MCP class automatically selects the appropriate transport
based on the URL pattern.

Transport Types:
- stdio: Command strings (local subprocess)
- Streamable HTTP: http:// or https:// URLs
- WebSocket: ws:// or wss:// URLs
- SSE (Legacy): URLs ending in /sse

Protocol: MCP 2025-11-25
"""

# Example code (requires running MCP servers):
#
# from praisonaiagents import Agent, MCP
#
# # 1. stdio Transport - Local subprocess
# agent_stdio = Agent(
#     name="Stdio Assistant",
#     tools=MCP("npx @modelcontextprotocol/server-memory")
# )
#
# # 2. Streamable HTTP Transport
# agent_http = Agent(
#     name="HTTP Assistant",
#     tools=MCP("https://api.example.com/mcp")
# )
#
# # 3. WebSocket Transport
# agent_ws = Agent(
#     name="WebSocket Assistant",
#     tools=MCP("wss://api.example.com/mcp")
# )
#
# # 4. SSE Transport (Legacy)
# agent_sse = Agent(
#     name="SSE Assistant",
#     tools=MCP("http://localhost:8080/sse")
# )

if __name__ == "__main__":
    from praisonaiagents.mcp.mcp_transport import get_transport_type
    
    print("MCP Transports Overview")
    print("=" * 50)
    print()
    print("Transport Auto-Detection:")
    print("-" * 50)
    
    examples = [
        ("npx @mcp/server-memory", "stdio"),
        ("python3 server.py", "stdio"),
        ("https://api.example.com/mcp", "http_stream"),
        ("http://localhost:8080/mcp", "http_stream"),
        ("ws://localhost:8080/mcp", "websocket"),
        ("wss://api.example.com/mcp", "websocket"),
        ("http://localhost:8080/sse", "sse"),
        ("https://api.example.com/sse", "sse"),
    ]
    
    for url, expected in examples:
        detected = get_transport_type(url)
        status = "âœ“" if detected == expected else "âœ—"
        print(f"  {status} '{url}'")
        print(f"      â†’ {detected}")
    
    print()
    print("Transport Features:")
    print("-" * 50)
    print("  stdio:")
    print("    - Local subprocess communication")
    print("    - Newline-delimited JSON-RPC")
    print("    - Environment variable support")
    print()
    print("  Streamable HTTP:")
    print("    - Session management (Mcp-Session-Id)")
    print("    - Protocol versioning (Mcp-Protocol-Version)")
    print("    - SSE streaming with resumability")
    print()
    print("  WebSocket:")
    print("    - Bidirectional real-time")
    print("    - Auto-reconnect with exponential backoff")
    print("    - Authentication token support")
    print()
    print("  SSE (Legacy):")
    print("    - Backward compatibility")
    print("    - Protocol version 2024-11-05")


--- src/praisonai-ts/examples/quickstart/hello-agent.ts ---
/**
 * Hello Agent - The simplest possible PraisonAI example (3 lines)
 * 
 * Run: npx ts-node examples/quickstart/hello-agent.ts
 */

import { Agent } from '../../src';

const agent = new Agent({ instructions: "You are a helpful assistant" });
agent.chat("Hello! Tell me a fun fact about AI.").then(console.log);


--- src/praisonai-ts/examples/quickstart/with-persistence.ts ---
/**
 * Agent with Persistence - Auto-save messages to database (4 lines)
 * 
 * Run: npx ts-node examples/quickstart/with-persistence.ts
 */

import { Agent, db } from '../../src';

const agent = new Agent({
  instructions: "You are a helpful assistant with memory",
  db: db("memory:"),  // Use in-memory db for demo (use "sqlite:./data.db" for file)
  sessionId: "my-session"
});

agent.chat("Remember that my favorite color is blue.").then(console.log);


--- src/praisonai-ts/examples/quickstart/with-tools.ts ---
/**
 * Agent with Tools - Pass plain functions as tools (5 lines)
 * 
 * Run: npx ts-node examples/quickstart/with-tools.ts
 */

import { Agent } from '../../src';

// Define a simple tool as a plain function
const getWeather = (city: string) => `Weather in ${city}: 22Â°C, Sunny`;

const agent = new Agent({
  instructions: "You are a weather assistant. Use the getWeather tool to answer questions.",
  tools: [getWeather]
});

agent.chat("What's the weather like in Paris?").then(console.log);


--- src/praisonai/praisonai/setup/post_install.py ---
import os
import sys
import subprocess
import logging

def install_playwright():
    """Install playwright browsers."""
    try:
        subprocess.run([sys.executable, "-m", "playwright", "install"], check=True)
        logging.info("Playwright browsers installed successfully")
    except subprocess.CalledProcessError as e:
        logging.error(f"Failed to install playwright browsers: {e}")
    except Exception as e:
        logging.error(f"Unexpected error installing playwright browsers: {e}")

def main():
    """Post-installation script."""
    # Check if this is a chat or code installation
    if any(extra in sys.argv for extra in ['chat', 'code']):
        install_playwright()

if __name__ == "__main__":
    main()

--- examples/js/embeddings/embed-docs.ts ---
/**
 * Document Embedding Example
 * 
 * Demonstrates chunking and embedding documents for retrieval
 * 
 * Run: npx ts-node examples/js/embeddings/embed-docs.ts
 */

import { embedMany, cosineSimilarity } from '../../../src/praisonai-ts/dist/llm/embeddings';

// Sample documents
const documents = [
  {
    id: 'doc1',
    title: 'Introduction to AI',
    content: 'Artificial Intelligence (AI) is the simulation of human intelligence by machines. It includes machine learning, natural language processing, and computer vision.'
  },
  {
    id: 'doc2',
    title: 'Machine Learning Basics',
    content: 'Machine learning is a subset of AI that enables systems to learn from data. It includes supervised learning, unsupervised learning, and reinforcement learning.'
  },
  {
    id: 'doc3',
    title: 'Deep Learning',
    content: 'Deep learning uses neural networks with multiple layers. It excels at tasks like image recognition, speech processing, and natural language understanding.'
  },
  {
    id: 'doc4',
    title: 'Natural Language Processing',
    content: 'NLP enables computers to understand human language. Applications include chatbots, translation, sentiment analysis, and text summarization.'
  },
  {
    id: 'doc5',
    title: 'Computer Vision',
    content: 'Computer vision allows machines to interpret visual information. It powers applications like facial recognition, autonomous vehicles, and medical imaging.'
  }
];

interface EmbeddedDocument {
  id: string;
  title: string;
  content: string;
  embedding: number[];
}

async function main() {
  console.log('=== Document Embedding Example ===\n');

  // Step 1: Embed all documents
  console.log('1. Embedding documents...');
  const contents = documents.map(d => d.content);
  const { embeddings } = await embedMany(contents);
  
  const embeddedDocs: EmbeddedDocument[] = documents.map((doc, i) => ({
    ...doc,
    embedding: embeddings[i]
  }));
  
  console.log(`   Embedded ${embeddedDocs.length} documents`);
  console.log(`   Embedding dimensions: ${embeddings[0].length}`);

  // Step 2: Search function
  async function search(query: string, topK: number = 3): Promise<Array<{ doc: EmbeddedDocument; score: number }>> {
    const { embedding: queryEmbedding } = await (await import('../../../src/praisonai-ts/dist/llm/embeddings')).embed(query);
    
    const results = embeddedDocs.map(doc => ({
      doc,
      score: cosineSimilarity(queryEmbedding, doc.embedding)
    }));
    
    return results
      .sort((a, b) => b.score - a.score)
      .slice(0, topK);
  }

  // Step 3: Test queries
  console.log('\n2. Testing search queries...\n');

  const queries = [
    'How do machines learn from data?',
    'What is image recognition?',
    'Tell me about chatbots and translation'
  ];

  for (const query of queries) {
    console.log(`Query: "${query}"`);
    const results = await search(query);
    
    console.log('Top results:');
    results.forEach((r, i) => {
      console.log(`  ${i + 1}. [${(r.score * 100).toFixed(1)}%] ${r.doc.title}`);
    });
    console.log('');
  }

  // Step 4: Show document similarity matrix
  console.log('3. Document Similarity Matrix');
  console.log('');
  
  // Header
  process.stdout.write('         ');
  documents.forEach((_, i) => process.stdout.write(`Doc${i + 1}  `));
  console.log('');
  
  // Matrix
  for (let i = 0; i < embeddedDocs.length; i++) {
    process.stdout.write(`Doc${i + 1}     `);
    for (let j = 0; j < embeddedDocs.length; j++) {
      const sim = cosineSimilarity(embeddedDocs[i].embedding, embeddedDocs[j].embedding);
      process.stdout.write(`${(sim * 100).toFixed(0).padStart(3)}%  `);
    }
    console.log('');
  }

  console.log('\n=== Complete ===');
}

main().catch(console.error);


--- src/praisonai-agents/DOCS_PARITY.md ---
# Documentation Parity Tracker (Python)

> **Categories:** 60 | **Documented:** 57 | **Parity:** 95.0%

This report compares **Python SDK feature categories** against **Python documentation** (docs/concepts, docs/features, etc.).

## Summary

| Metric | Count |
|--------|-------|
| Feature Categories | 60 |
| **Documented Categories** | **57** |
| **Undocumented Categories** | **3** |
| **Parity** | **95.0%** |

## Documented Categories

| Category | Features | Docs | Lines |
|----------|----------|------|-------|
| âœ… AGUI | 1 | 2 | 420 |
| âœ… Agent | 19 | 30 | 8367 |
| âœ… Agent-to-Agent (A2A) | 1 | 3 | 625 |
| âœ… Approval | 1 | 2 | 889 |
| âœ… Audio | 2 | 10 | 649 |
| âœ… Auto Generation | 3 | 6 | 1386 |
| âœ… Autonomy | 3 | 4 | 1146 |
| âœ… Bots | 5 | 2 | 573 |
| âœ… Chunking | 2 | 2 | 285 |
| âœ… Citations | 2 | 1 | 202 |
| âœ… Code Execution | 3 | 13 | 3395 |
| âœ… Conditions | 1 | 3 | 1086 |
| âœ… Configuration | 2 | 7 | 1753 |
| âœ… Context Management | 13 | 29 | 7576 |
| âœ… Database | 1 | 31 | 3037 |
| âœ… Display | 6 | 2 | 1175 |
| âœ… Embeddings | 6 | 23 | 1966 |
| âœ… Evaluation | 1 | 6 | 2538 |
| âœ… Events | 1 | 2 | 699 |
| âœ… Execution | 3 | 2 | 515 |
| âœ… Files | 2 | 5 | 863 |
| âœ… Flow | 1 | 1 | 284 |
| âœ… Gateway | 6 | 1 | 256 |
| âœ… Guardrails | 4 | 4 | 2279 |
| âœ… Handoffs | 10 | 4 | 1674 |
| âœ… Hooks | 2 | 6 | 1606 |
| âœ… Image | 1 | 11 | 1096 |
| âœ… Knowledge | 3 | 12 | 3262 |
| âœ… LLM | 2 | 2 | 1049 |
| âœ… Loops | 4 | 1 | 250 |
| âœ… MCP | 2 | 47 | 9754 |
| âœ… Memory | 6 | 12 | 4474 |
| âœ… Output | 3 | 4 | 868 |
| âœ… Parallel Execution | 2 | 2 | 347 |
| âœ… Planning | 6 | 5 | 1506 |
| âœ… Plugins | 8 | 3 | 1494 |
| âœ… Prompts | 2 | 4 | 796 |
| âœ… Providers | 2 | 50 | 5396 |
| âœ… Query | 1 | 2 | 616 |
| âœ… RAG | 5 | 14 | 2910 |
| âœ… Realtime | 2 | 5 | 584 |
| âœ… Reflection | 3 | 2 | 469 |
| âœ… Retrieval | 2 | 5 | 941 |
| âœ… Routing | 1 | 2 | 597 |
| âœ… Sandbox | 5 | 3 | 1002 |
| âœ… Security | 1 | 1 | 875 |
| âœ… Sessions | 2 | 8 | 2128 |
| âœ… Skills | 5 | 5 | 2002 |
| âœ… Tasks | 2 | 5 | 2590 |
| âœ… Telemetry | 1 | 2 | 541 |
| âœ… Templates | 1 | 7 | 1411 |
| âœ… Tools | 4 | 97 | 17827 |
| âœ… Tracing | 3 | 2 | 139 |
| âœ… Video | 2 | 5 | 460 |
| âœ… Vision | 2 | 1 | 283 |
| âœ… Web | 4 | 6 | 1267 |
| âœ… Workflows | 4 | 9 | 3579 |

## Stub Documentation (Need Content)

These categories have documentation files but < 50 lines (stubs):

| Category | Features | Docs | Lines |
|----------|----------|------|-------|
| âš ï¸ OCR | 2 | 1 | 49 |

## Undocumented Categories (Need Documentation)

| Category | Features |
|----------|----------|
| âŒ Failover | 2 |
| âŒ Optimizer | 1 |

## Documentation Without Features

These docs exist but don't match any implemented feature category:

- â„¹ï¸ AI SDK (1 docs, 112 lines)
- â„¹ï¸ Agent-to-User (A2U) (3 docs, 770 lines)
- â„¹ï¸ CLI (91 docs, 21386 lines)
- â„¹ï¸ Callbacks (1 docs, 985 lines)
- â„¹ï¸ Chat (3 docs, 546 lines)
- â„¹ï¸ Deep Research (2 docs, 437 lines)
- â„¹ï¸ Deployment (29 docs, 5931 lines)
- â„¹ï¸ Jobs (1 docs, 358 lines)
- â„¹ï¸ Middleware (1 docs, 60 lines)
- â„¹ï¸ Observability (21 docs, 1580 lines)
- â„¹ï¸ Process (1 docs, 506 lines)
- â„¹ï¸ Scheduler (2 docs, 858 lines)
- â„¹ï¸ Storage (4 docs, 722 lines)
- â„¹ï¸ Streaming (3 docs, 485 lines)
- â„¹ï¸ Token Management (3 docs, 1020 lines)
- â„¹ï¸ Vector Store (11 docs, 733 lines)

---

*Generated by `praisonai._dev.parity.docs_generator`*


--- src/praisonai-rust/DOCS_PARITY.md ---
# Documentation Parity Tracker (Rust)

> **Categories:** 68 | **Documented:** 68 | **Parity:** 100.0%

This report compares **Rust SDK feature categories** against **Rust documentation** (docs/rust/).

## Summary

| Metric | Count |
|--------|-------|
| Feature Categories | 68 |
| **Documented Categories** | **68** |
| **Undocumented Categories** | **0** |
| **Parity** | **100.0%** |

## Documented Categories

| Category | Features | Docs | Lines |
|----------|----------|------|-------|
| âœ… AGUI | 6 | 1 | 108 |
| âœ… Agent | 39 | 5 | 901 |
| âœ… Agent-to-Agent (A2A) | 6 | 1 | 131 |
| âœ… Approval | 2 | 1 | 102 |
| âœ… Audio | 3 | 1 | 100 |
| âœ… Auto Generation | 5 | 1 | 102 |
| âœ… Autonomy | 4 | 1 | 108 |
| âœ… Bots | 10 | 1 | 271 |
| âœ… Budget | 2 | 1 | 90 |
| âœ… Callbacks | 1 | 1 | 96 |
| âœ… Chunking | 3 | 1 | 114 |
| âœ… Citations | 2 | 1 | 123 |
| âœ… Code Execution | 5 | 1 | 147 |
| âœ… Conditions | 1 | 1 | 243 |
| âœ… Configuration | 2 | 1 | 301 |
| âœ… Context Management | 21 | 1 | 284 |
| âœ… Criteria | 4 | 1 | 108 |
| âœ… Database | 2 | 1 | 80 |
| âœ… Display | 12 | 1 | 282 |
| âœ… Documents | 1 | 1 | 113 |
| âœ… Embeddings | 8 | 2 | 323 |
| âœ… Evaluation | 5 | 1 | 311 |
| âœ… Events | 3 | 1 | 117 |
| âœ… Execution | 3 | 1 | 93 |
| âœ… Failover | 3 | 1 | 279 |
| âœ… Files | 3 | 1 | 101 |
| âœ… Flow | 2 | 1 | 99 |
| âœ… Gateway | 8 | 1 | 293 |
| âœ… Guardrails | 10 | 1 | 322 |
| âœ… Handoffs | 13 | 1 | 311 |
| âœ… Hooks | 12 | 1 | 297 |
| âœ… Image | 4 | 1 | 139 |
| âœ… Knowledge | 12 | 1 | 297 |
| âœ… LLM | 10 | 1 | 237 |
| âœ… Loops | 7 | 1 | 111 |
| âœ… MCP | 11 | 1 | 359 |
| âœ… Memory | 11 | 1 | 267 |
| âœ… OCR | 5 | 1 | 130 |
| âœ… Optimizer | 1 | 1 | 62 |
| âœ… Output | 3 | 1 | 99 |
| âœ… Parallel Execution | 4 | 1 | 98 |
| âœ… Planning | 10 | 1 | 305 |
| âœ… Plugins | 11 | 1 | 337 |
| âœ… Process | 2 | 1 | 83 |
| âœ… Prompts | 4 | 1 | 117 |
| âœ… Providers | 3 | 1 | 140 |
| âœ… Query | 5 | 1 | 109 |
| âœ… RAG | 6 | 1 | 328 |
| âœ… Realtime | 3 | 1 | 130 |
| âœ… Reflection | 4 | 1 | 89 |
| âœ… Retrieval | 4 | 1 | 136 |
| âœ… Routing | 1 | 1 | 129 |
| âœ… Sandbox | 7 | 2 | 56 |
| âœ… Security | 2 | 1 | 127 |
| âœ… Sessions | 7 | 1 | 279 |
| âœ… Skills | 5 | 1 | 224 |
| âœ… Streaming | 6 | 1 | 332 |
| âœ… Tasks | 8 | 1 | 269 |
| âœ… Telemetry | 4 | 1 | 285 |
| âœ… Templates | 1 | 1 | 95 |
| âœ… Token Management | 2 | 1 | 119 |
| âœ… Tools | 16 | 1 | 281 |
| âœ… Tracing | 8 | 1 | 280 |
| âœ… Vector Store | 2 | 1 | 128 |
| âœ… Video | 5 | 1 | 133 |
| âœ… Vision | 3 | 1 | 89 |
| âœ… Web | 5 | 1 | 106 |
| âœ… Workflows | 5 | 1 | 295 |

## Documentation Without Features

These docs exist but don't match any implemented feature category:

- â„¹ï¸ CLI (1 docs, 168 lines)

---

*Generated by `praisonai._dev.parity.docs_generator`*


--- src/praisonai-ts/DOCS_PARITY.md ---
# Documentation Parity Tracker (TypeScript/JavaScript)

> **Categories:** 74 | **Documented:** 74 | **Parity:** 100.0%

This report compares **TypeScript/JavaScript SDK feature categories** against **TypeScript/JavaScript documentation** (docs/js/).

## Summary

| Metric | Count |
|--------|-------|
| Feature Categories | 74 |
| **Documented Categories** | **74** |
| **Undocumented Categories** | **0** |
| **Parity** | **100.0%** |

## Documented Categories

| Category | Features | Docs | Lines |
|----------|----------|------|-------|
| âœ… AGUI | 1 | 1 | 183 |
| âœ… AI SDK | 38 | 8 | 980 |
| âœ… Agent | 66 | 7 | 1926 |
| âœ… Agent-to-Agent (A2A) | 15 | 1 | 330 |
| âœ… Approval | 9 | 1 | 366 |
| âœ… Audio | 8 | 1 | 338 |
| âœ… Auto Generation | 14 | 3 | 495 |
| âœ… Autonomy | 5 | 1 | 339 |
| âœ… Bots | 5 | 1 | 278 |
| âœ… CLI | 2 | 1 | 112 |
| âœ… Caching | 2 | 1 | 208 |
| âœ… Chunking | 2 | 2 | 204 |
| âœ… Citations | 3 | 1 | 206 |
| âœ… Code Execution | 6 | 4 | 376 |
| âœ… Conditions | 1 | 1 | 357 |
| âœ… Configuration | 3 | 1 | 251 |
| âœ… Context Management | 17 | 2 | 255 |
| âœ… Criteria | 1 | 1 | 82 |
| âœ… Database | 14 | 2 | 367 |
| âœ… Display | 20 | 1 | 290 |
| âœ… Embeddings | 6 | 3 | 733 |
| âœ… Evaluation | 8 | 4 | 647 |
| âœ… Events | 4 | 1 | 221 |
| âœ… Execution | 4 | 1 | 89 |
| âœ… Failover | 2 | 1 | 89 |
| âœ… Files | 10 | 1 | 116 |
| âœ… Flow | 4 | 1 | 120 |
| âœ… Gateway | 6 | 1 | 386 |
| âœ… Guardrails | 6 | 3 | 622 |
| âœ… Handoffs | 12 | 1 | 178 |
| âœ… Hooks | 8 | 1 | 172 |
| âœ… Image | 4 | 2 | 203 |
| âœ… Jobs | 8 | 1 | 87 |
| âœ… Knowledge | 3 | 2 | 227 |
| âœ… LLM | 6 | 2 | 203 |
| âœ… Loops | 11 | 1 | 176 |
| âœ… MCP | 17 | 4 | 564 |
| âœ… Memory | 17 | 4 | 620 |
| âœ… Middleware | 2 | 1 | 94 |
| âœ… OCR | 2 | 1 | 80 |
| âœ… Observability | 6 | 28 | 1658 |
| âœ… Optimizer | 1 | 1 | 74 |
| âœ… Output | 3 | 1 | 150 |
| âœ… Parallel Execution | 3 | 2 | 215 |
| âœ… Planning | 11 | 2 | 434 |
| âœ… Plugins | 9 | 2 | 635 |
| âœ… Prompts | 4 | 2 | 163 |
| âœ… Providers | 24 | 116 | 5534 |
| âœ… PubSub | 1 | 1 | 86 |
| âœ… Query | 3 | 2 | 167 |
| âœ… RAG | 6 | 2 | 309 |
| âœ… Realtime | 2 | 1 | 102 |
| âœ… Reflection | 3 | 1 | 111 |
| âœ… Retrieval | 5 | 1 | 112 |
| âœ… Routing | 1 | 1 | 210 |
| âœ… Sandbox | 8 | 2 | 56 |
| âœ… Scheduler | 2 | 2 | 323 |
| âœ… Security | 2 | 1 | 111 |
| âœ… Sessions | 3 | 2 | 355 |
| âœ… Skills | 7 | 2 | 307 |
| âœ… Streaming | 2 | 2 | 352 |
| âœ… Tasks | 4 | 1 | 153 |
| âœ… Teams | 1 | 1 | 206 |
| âœ… Telemetry | 6 | 2 | 184 |
| âœ… Templates | 1 | 2 | 516 |
| âœ… Token Management | 1 | 1 | 87 |
| âœ… Tools | 34 | 15 | 2821 |
| âœ… Tracing | 8 | 3 | 421 |
| âœ… Vector Store | 6 | 1 | 231 |
| âœ… Video | 2 | 1 | 132 |
| âœ… Vision | 2 | 1 | 147 |
| âœ… Voice | 1 | 2 | 398 |
| âœ… Web | 4 | 1 | 121 |
| âœ… Workflows | 7 | 3 | 543 |

## Documentation Without Features

These docs exist but don't match any implemented feature category:

- â„¹ï¸ Callbacks (1 docs, 163 lines)
- â„¹ï¸ Deep Research (2 docs, 191 lines)

---

*Generated by `praisonai._dev.parity.docs_generator`*


--- src/praisonai/scripts/generate_docs_parity.py ---
#!/usr/bin/env python3
"""
Generate documentation parity tracker files.

Usage:
    python generate_docs_parity.py                  # Generate all
    python generate_docs_parity.py -t ts            # TypeScript only
    python generate_docs_parity.py -t rust          # Rust only
    python generate_docs_parity.py --check          # Check mode for CI
"""

import sys
from pathlib import Path

# Add praisonai package to path
pkg_root = Path(__file__).resolve().parent.parent / "praisonai"
if pkg_root.exists():
    sys.path.insert(0, str(pkg_root.parent))

from praisonai._dev.parity.docs_generator import main

if __name__ == '__main__':
    main()


--- examples/README.md ---
# PraisonAI Examples

This folder contains examples for PraisonAI. For detailed documentation, visit [docs.praison.ai](https://docs.praison.ai).

## Structure

```
examples/
â”œâ”€â”€ python/           # Python examples
â”‚   â”œâ”€â”€ agents/       # Agent examples (single, multi, router, etc.)
â”‚   â”œâ”€â”€ workflows/    # Workflow patterns (routing, parallel, loop)
â”‚   â”œâ”€â”€ tools/        # Custom tools examples
â”‚   â”œâ”€â”€ mcp/          # MCP protocol examples
â”‚   â”œâ”€â”€ memory/       # Memory and sessions
â”‚   â”œâ”€â”€ code/         # Code editing and external CLI tools
â”‚   â””â”€â”€ ...
â”œâ”€â”€ serve/            # Server and endpoints examples
â”œâ”€â”€ yaml/             # YAML workflow examples
â””â”€â”€ cookbooks/        # Complete use-case examples
```

## Quick Links

| Category | Examples | Docs |
|----------|----------|------|
| **Consolidated Params** | [consolidated_params/](consolidated_params/) | [ðŸ“–](https://docs.praison.ai/api/consolidated-params) |
| **Agents** | [python/agents/](python/agents/) | [ðŸ“–](https://docs.praison.ai/concepts/agents) |
| **Workflows** | [python/workflows/](python/workflows/) | [ðŸ“–](https://docs.praison.ai/features/workflows) |
| **Model Router** | [python/agents/router-agent-cost-optimization.py](python/agents/router-agent-cost-optimization.py) | [ðŸ“–](https://docs.praison.ai/features/model-router) |
| **MCP** | [python/mcp/](python/mcp/) | [ðŸ“–](https://docs.praison.ai/mcp) |
| **Memory** | [python/memory/](python/memory/) | [ðŸ“–](https://docs.praison.ai/concepts/memory) |
| **Tools** | [python/tools/](python/tools/) | [ðŸ“–](https://docs.praison.ai/tools) |
| **Code** | [python/code/](python/code/) | [ðŸ“–](https://docs.praison.ai/code) |
| **YAML** | [yaml/](yaml/) | [ðŸ“–](https://docs.praison.ai/features/yaml-workflows) |
| **Doctor** | [doctor/](doctor/) | [ðŸ“–](https://docs.praison.ai/cli/doctor) |
| **Serve** | [serve/](serve/) | [ðŸ“–](https://docs.praison.ai/cli/serve) |
| **Endpoints** | [serve/](serve/) | [ðŸ“–](https://docs.praison.ai/cli/endpoints) |

## Consolidated Params Examples

Agent-centric API with unified parameter resolution. Precedence: Instance > Config > Array > Dict > String > Bool > Default

| Example | Description |
|---------|-------------|
| [basic_agent.py](consolidated_params/basic_agent.py) | Minimal agent with memory |
| [basic_agents.py](consolidated_params/basic_agents.py) | Multi-agent with memory+planning |
| [basic_workflow.py](consolidated_params/basic_workflow.py) | Workflow with consolidated params |
| [basic_memory.py](consolidated_params/basic_memory.py) | Memory presets (file, redis, postgres) |
| [basic_guardrails.py](consolidated_params/basic_guardrails.py) | Guardrails with callable or config |
| [basic_workflow_agentlike.py](consolidated_params/basic_workflow_agentlike.py) | Workflow with agent-like params |
| [basic_step_override.py](consolidated_params/basic_step_override.py) | Step-level override of workflow defaults |
| [advanced_workflow_full_features.py](consolidated_params/advanced_workflow_full_features.py) | All consolidated params |

## Serve Examples

| Example | Description | CLI Command |
|---------|-------------|-------------|
| [unified_server.py](serve/unified_server.py) | All providers in one server | `praisonai serve unified` |
| [agent_as_api_single.py](serve/agent_as_api_single.py) | Single agent HTTP API | `praisonai serve agents` |
| [agents_as_api_router.py](serve/agents_as_api_router.py) | Multi-agent router API | `praisonai serve agents` |
| [a2a_server_client.py](serve/a2a_server_client.py) | A2A protocol server | `praisonai serve a2a` |
| [a2u_events_stream.py](serve/a2u_events_stream.py) | A2U event stream | `praisonai serve a2u` |
| [mcp_http_server.py](serve/mcp_http_server.py) | MCP HTTP server | `praisonai serve mcp` |
| [tools_as_mcp_server.py](serve/tools_as_mcp_server.py) | Tools as MCP server | `praisonai serve tools` |
| [agent_launch_modes.py](serve/agent_launch_modes.py) | Agent.launch() API | Python only |
| [endpoints_unified_client.py](serve/endpoints_unified_client.py) | Unified client | `praisonai endpoints` |

## Running Examples

```bash
# Install PraisonAI
pip install praisonai

# Set API key
export OPENAI_API_KEY=your_key_here

# Run an example
python examples/python/agents/single-agent.py
```

## CLI Commands

See the main [README.md](../README.md#-cli--no-code-interface) for all CLI commands.


## Links discovered
- [docs.praison.ai](https://docs.praison.ai)
- [consolidated_params/](https://github.com/MervinPraison/PraisonAI/blob/main/examples/consolidated_params.md)
- [ðŸ“–](https://docs.praison.ai/api/consolidated-params)
- [python/agents/](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/agents.md)
- [ðŸ“–](https://docs.praison.ai/concepts/agents)
- [python/workflows/](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/workflows.md)
- [ðŸ“–](https://docs.praison.ai/features/workflows)
- [python/agents/router-agent-cost-optimization.py](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/agents/router-agent-cost-optimization.py)
- [ðŸ“–](https://docs.praison.ai/features/model-router)
- [python/mcp/](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/mcp.md)
- [ðŸ“–](https://docs.praison.ai/mcp)
- [python/memory/](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/memory.md)
- [ðŸ“–](https://docs.praison.ai/concepts/memory)
- [python/tools/](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/tools.md)
- [ðŸ“–](https://docs.praison.ai/tools)
- [python/code/](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/code.md)
- [ðŸ“–](https://docs.praison.ai/code)
- [yaml/](https://github.com/MervinPraison/PraisonAI/blob/main/examples/yaml.md)
- [ðŸ“–](https://docs.praison.ai/features/yaml-workflows)
- [doctor/](https://github.com/MervinPraison/PraisonAI/blob/main/examples/doctor.md)
- [ðŸ“–](https://docs.praison.ai/cli/doctor)
- [serve/](https://github.com/MervinPraison/PraisonAI/blob/main/examples/serve.md)
- [ðŸ“–](https://docs.praison.ai/cli/serve)
- [ðŸ“–](https://docs.praison.ai/cli/endpoints)
- [basic_agent.py](https://github.com/MervinPraison/PraisonAI/blob/main/examples/consolidated_params/basic_agent.py)
- [basic_agents.py](https://github.com/MervinPraison/PraisonAI/blob/main/examples/consolidated_params/basic_agents.py)
- [basic_workflow.py](https://github.com/MervinPraison/PraisonAI/blob/main/examples/consolidated_params/basic_workflow.py)
- [basic_memory.py](https://github.com/MervinPraison/PraisonAI/blob/main/examples/consolidated_params/basic_memory.py)
- [basic_guardrails.py](https://github.com/MervinPraison/PraisonAI/blob/main/examples/consolidated_params/basic_guardrails.py)
- [basic_workflow_agentlike.py](https://github.com/MervinPraison/PraisonAI/blob/main/examples/consolidated_params/basic_workflow_agentlike.py)
- [basic_step_override.py](https://github.com/MervinPraison/PraisonAI/blob/main/examples/consolidated_params/basic_step_override.py)
- [advanced_workflow_full_features.py](https://github.com/MervinPraison/PraisonAI/blob/main/examples/consolidated_params/advanced_workflow_full_features.py)
- [unified_server.py](https://github.com/MervinPraison/PraisonAI/blob/main/examples/serve/unified_server.py)
- [agent_as_api_single.py](https://github.com/MervinPraison/PraisonAI/blob/main/examples/serve/agent_as_api_single.py)
- [agents_as_api_router.py](https://github.com/MervinPraison/PraisonAI/blob/main/examples/serve/agents_as_api_router.py)
- [a2a_server_client.py](https://github.com/MervinPraison/PraisonAI/blob/main/examples/serve/a2a_server_client.py)
- [a2u_events_stream.py](https://github.com/MervinPraison/PraisonAI/blob/main/examples/serve/a2u_events_stream.py)
- [mcp_http_server.py](https://github.com/MervinPraison/PraisonAI/blob/main/examples/serve/mcp_http_server.py)
- [tools_as_mcp_server.py](https://github.com/MervinPraison/PraisonAI/blob/main/examples/serve/tools_as_mcp_server.py)
- [agent_launch_modes.py](https://github.com/MervinPraison/PraisonAI/blob/main/examples/serve/agent_launch_modes.py)
- [endpoints_unified_client.py](https://github.com/MervinPraison/PraisonAI/blob/main/examples/serve/endpoints_unified_client.py)
- [README.md](https://github.com/MervinPraison/PraisonAI/blob/main/README.md#-cli--no-code-interface)

--- examples/acp/README.md ---
# ACP (Agent Client Protocol) Examples

This directory contains examples for using PraisonAI with the Agent Client Protocol (ACP).

## What is ACP?

The Agent Client Protocol (ACP) is a standardized JSON-RPC 2.0 protocol that allows code editors and IDEs to communicate with AI coding agents. It enables seamless integration between PraisonAI and editors like:

- **Zed**
- **JetBrains IDEs** (IntelliJ, PyCharm, WebStorm, etc.)
- **VSCode** (via extensions)
- **Toad**

## Quick Start

### CLI Usage

```bash
# Start ACP server with defaults
praisonai acp

# Enable debug logging
praisonai acp --debug

# Allow file writes
praisonai acp --allow-write

# Use a specific model
praisonai acp -m gpt-4o

# Resume last session
praisonai acp --resume --last
```

### Python API

```python
from praisonai.acp import serve

serve(
    workspace=".",
    debug=True,
    allow_write=True,
)
```

## Editor Configuration

### Zed

Add to `~/.config/zed/settings.json`:

```json
{
  "agent_servers": {
    "PraisonAI": {
      "command": "praisonai",
      "args": ["acp"],
      "env": {}
    }
  }
}
```

### JetBrains

Add to `~/.jetbrains/acp.json`:

```json
{
  "agent_servers": {
    "PraisonAI": {
      "command": "praisonai",
      "args": ["acp"],
      "env": {}
    }
  }
}
```

### Toad

```bash
toad acp "praisonai acp"
```

## Examples

- `basic_acp_server.py` - Simple ACP server with default settings
- `custom_agent_acp.py` - ACP server with a custom agent

## CLI Options

| Option | Description |
|--------|-------------|
| `-w, --workspace` | Workspace root directory |
| `-a, --agent` | Agent name or config file |
| `-m, --model` | LLM model to use |
| `-r, --resume` | Resume session by ID |
| `--last` | Resume the last session |
| `--approve` | Approval mode: manual, auto, scoped |
| `--read-only` | Read-only mode (default) |
| `--allow-write` | Allow file writes |
| `--allow-shell` | Allow shell commands |
| `--debug` | Enable debug logging |

## Installation

```bash
pip install praisonai[acp]
```

## Protocol Details

ACP uses JSON-RPC 2.0 over stdio:
- **stdin**: Receives JSON-RPC requests from the client
- **stdout**: Sends JSON-RPC responses to the client
- **stderr**: Debug/log output (never pollutes stdout)

### Supported Methods

**Agent Methods (client â†’ agent):**
- `initialize` - Negotiate protocol version and capabilities
- `authenticate` - Optional authentication
- `session/new` - Create a new session
- `session/load` - Load an existing session
- `session/prompt` - Send user message
- `session/cancel` - Cancel ongoing operations
- `session/set_mode` - Change operating mode

**Client Methods (agent â†’ client):**
- `session/update` - Send progress updates
- `session/request_permission` - Request user approval
- `fs/read_text_file` - Read file contents
- `fs/write_text_file` - Write file contents
- `terminal/create` - Create terminal
- `terminal/output` - Get terminal output


--- examples/consolidated_params/README.md ---
# Consolidated Params Examples

Examples demonstrating the **Agent-Centric API** with consolidated parameters.

## Precedence Ladder

```
Instance > Config > Array > Dict > String > Bool > Default
```

## Quick Reference

| Feature | Presets | Example |
|---------|---------|---------|
| `memory` | file, sqlite, redis, postgres, mem0, mongodb | `memory="sqlite"` |
| `output` | minimal, normal, verbose, debug, silent | `output="verbose"` |
| `execution` | fast, balanced, thorough, unlimited | `execution="balanced"` |
| `planning` | reasoning, read_only, auto | `planning="reasoning"` |
| `reflection` | minimal, standard, thorough | `reflection="standard"` |
| `guardrails` | strict, permissive, safety | `guardrails="strict"` |
| `web` | duckduckgo, tavily, google, bing, serper | `web="duckduckgo"` |
| `context` | sliding_window, summarize, truncate | `context="sliding_window"` |
| `autonomy` | suggest, auto_edit, full_auto | `autonomy="suggest"` |
| `caching` | enabled, disabled, prompt | `caching="prompt"` |

## Examples

### Basic Examples (Single Feature)

| File | Feature | Description |
|------|---------|-------------|
| `basic_agent.py` | Agent | Minimal agent with memory |
| `basic_memory.py` | memory | SQLite memory preset |
| `basic_output.py` | output | Verbose output preset |
| `basic_execution.py` | execution | Execution control |
| `basic_guardrails.py` | guardrails | Callable validator |
| `basic_reflection.py` | reflection | Self-reflection preset |
| `basic_web.py` | web | Web search preset |
| `basic_planning.py` | planning | Planning preset |
| `basic_autonomy.py` | autonomy | Autonomy preset |
| `basic_caching.py` | caching | Caching preset |
| `basic_knowledge.py` | knowledge | RAG with sources |
| `basic_context.py` | context | Context management |
| `basic_hooks.py` | hooks | Lifecycle callbacks |

### Multi-Agent Examples

| File | Description |
|------|-------------|
| `basic_agents.py` | Multi-agent with memory and planning |

### Workflow Examples

| File | Description |
|------|-------------|
| `basic_workflow.py` | Simple workflow with output preset |
| `basic_workflow_agentlike.py` | Workflow with agent-like params |
| `basic_step_override.py` | Step-level param overrides |
| `advanced_workflow_full_features.py` | All consolidated params |

### Advanced Examples

| File | Description |
|------|-------------|
| `advanced_output_execution.py` | Combined output + execution |

## Usage Forms

Each consolidated param supports multiple input forms:

```python
# Bool - enable with defaults
memory=True

# String preset
memory="sqlite"

# Dict config
memory={"backend": "sqlite", "user_id": "user123"}

# Config instance
memory=MemoryConfig(backend="sqlite", user_id="user123")

# Array with overrides (for some params)
guardrails=["strict", {"max_retries": 10}]
```

## Running Examples

```bash
# Set API key
export OPENAI_API_KEY=your_key_here

# Run any example
python basic_agent.py
```


--- examples/doctor/README.md ---
# PraisonAI Doctor Examples

Examples demonstrating the PraisonAI Doctor health check and diagnostics system.

## CLI Examples

| Command | Description |
|---------|-------------|
| `praisonai doctor` | Run all fast health checks |
| `praisonai doctor --version` | Show doctor version |
| `praisonai doctor --list-checks` | List all available checks |
| `praisonai doctor env` | Check environment configuration |
| `praisonai doctor config` | Validate configuration files |
| `praisonai doctor tools` | Check tool availability |
| `praisonai doctor db` | Check database drivers |
| `praisonai doctor mcp` | Check MCP configuration |
| `praisonai doctor obs` | Check observability providers |
| `praisonai doctor skills` | Check agent skills |
| `praisonai doctor memory` | Check memory storage |
| `praisonai doctor permissions` | Check filesystem permissions |
| `praisonai doctor network` | Check network connectivity |
| `praisonai doctor performance` | Check import times |
| `praisonai doctor ci` | CI mode with JSON output |
| `praisonai doctor selftest` | Test agent functionality |

## Global Flags

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |
| `--format text\|json` | Output format |
| `--output PATH` | Write report to file |
| `--deep` | Enable deeper probes |
| `--timeout SEC` | Per-check timeout |
| `--strict` | Treat warnings as failures |
| `--quiet` | Minimal output |
| `--no-color` | Disable colors |
| `--only IDS` | Only run these checks |
| `--skip IDS` | Skip these checks |

## Python Examples

| File | Description |
|------|-------------|
| [basic_doctor.py](basic_doctor.py) | Programmatic health checks |
| [ci_integration.py](ci_integration.py) | CI/CD pipeline integration |

## Quick Start

```bash
# Run basic health checks
praisonai doctor

# Run with JSON output
praisonai doctor --json

# Run specific checks
praisonai doctor --only python_version,openai_api_key

# CI mode
praisonai doctor ci

# Save report to file
praisonai doctor --output report.json
```


## Links discovered
- [basic_doctor.py](https://github.com/MervinPraison/PraisonAI/blob/main/examples/doctor/basic_doctor.py)
- [ci_integration.py](https://github.com/MervinPraison/PraisonAI/blob/main/examples/doctor/ci_integration.py)

--- examples/mcp/README.md ---
# MCP Server Examples

Examples demonstrating the MCP Server v2 features per MCP Protocol Version 2025-11-25.

## Features Demonstrated

### 1. Pagination (`pagination_example.py`)

Demonstrates pagination for `tools/list`, `resources/list`, and `prompts/list`:

- Opaque cursor encoding (base64url)
- Server-determined page size (default: 50, max: 100)
- `nextCursor` for fetching more results
- Cursor validation with JSON-RPC errors

```bash
python pagination_example.py
```

### 2. Tool Annotations (`tool_annotations_example.py`)

Demonstrates MCP 2025-11-25 tool annotation hints:

- `readOnlyHint`: Tool only reads data
- `destructiveHint`: Tool may have destructive effects
- `idempotentHint`: Safe to call multiple times
- `openWorldHint`: Interacts with external world

```bash
python tool_annotations_example.py
```

### 3. CLI Tools (`cli_tools_example.sh`)

Demonstrates the new CLI commands:

```bash
# List tools with pagination
praisonai mcp list-tools --limit 10
praisonai mcp list-tools --cursor <cursor> --json

# Search tools
praisonai mcp tools search "query"
praisonai mcp tools search --category memory
praisonai mcp tools search --read-only
praisonai mcp tools search --json

# Get tool info
praisonai mcp tools info <tool-name>
praisonai mcp tools info <tool-name> --json

# Get tool schema
praisonai mcp tools schema <tool-name>
```

## API Reference

### Pagination

```python
from praisonai.mcp_server.registry import MCPToolRegistry

registry = MCPToolRegistry()
# ... register tools ...

# Get first page
tools, next_cursor = registry.list_paginated(page_size=50)

# Get next page
if next_cursor:
    more_tools, next_cursor = registry.list_paginated(cursor=next_cursor)
```

### Tool Search

```python
# Search by query
tools, next_cursor, total = registry.search(query="memory")

# Filter by category
tools, _, _ = registry.search(category="file")

# Filter by read-only hint
tools, _, _ = registry.search(read_only=True)

# Combined filters with pagination
tools, next_cursor, total = registry.search(
    query="data",
    category="storage",
    read_only=True,
    page_size=10,
)
```

### Tool Annotations

```python
from praisonai.mcp_server.registry import MCPToolDefinition

# Read-only tool
tool = MCPToolDefinition(
    name="data.read",
    description="Read data",
    handler=read_handler,
    input_schema={"type": "object"},
    read_only_hint=True,
    destructive_hint=False,
)

# Destructive tool
tool = MCPToolDefinition(
    name="file.delete",
    description="Delete file",
    handler=delete_handler,
    input_schema={"type": "object"},
    destructive_hint=True,
    idempotent_hint=False,
)
```

## MCP Protocol Compliance

These examples comply with MCP Protocol Version 2025-11-25:

- Pagination uses opaque cursors (base64url encoded)
- Server determines page size (client cannot override)
- Invalid cursors return JSON-RPC error code -32602
- Tool annotations follow the spec defaults:
  - `readOnlyHint`: false
  - `destructiveHint`: true
  - `idempotentHint`: false
  - `openWorldHint`: true


--- examples/mcp_server/README.md ---
# PraisonAI MCP Server Examples

This directory contains examples for running PraisonAI as an MCP (Model Context Protocol) server.

## Overview

PraisonAI can expose its capabilities via MCP protocol, allowing integration with:
- Claude Desktop
- Cursor
- Windsurf
- VSCode MCP clients
- Any MCP-compatible client

## Protocol Version

These examples use MCP Protocol Version **2025-11-25**.

## Examples

### 1. STDIO Server (stdio_server.py)

Run PraisonAI as an MCP server using STDIO transport (for Claude Desktop integration).

```bash
# Run directly
python stdio_server.py

# Or via CLI
praisonai mcp serve --transport stdio
```

### 2. HTTP Stream Server (http_stream_server.py)

Run PraisonAI as an MCP server using HTTP Stream transport.

```bash
# Run directly
python http_stream_server.py

# Or via CLI
praisonai mcp serve --transport http-stream --port 8080
```

### 3. Custom Tools Server (custom_tools_server.py)

Register custom tools and expose them via MCP.

```bash
python custom_tools_server.py
```

### 4. Client Example (mcp_client_example.py)

Example of connecting to a PraisonAI MCP server as a client.

```bash
python mcp_client_example.py
```

## CLI Commands

```bash
# Start STDIO server
praisonai mcp serve --transport stdio

# Start HTTP Stream server
praisonai mcp serve --transport http-stream --port 8080

# Start with authentication
praisonai mcp serve --transport http-stream --api-key YOUR_KEY

# List available tools
praisonai mcp list-tools

# List available resources
praisonai mcp list-resources

# List available prompts
praisonai mcp list-prompts

# Generate client configuration
praisonai mcp config-generate --client claude-desktop

# Check server health
praisonai mcp doctor
```

## Client Configuration

### Claude Desktop

Add to `~/.config/claude/claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "praisonai": {
      "command": "praisonai",
      "args": ["mcp", "serve", "--transport", "stdio"]
    }
  }
}
```

### Cursor

Add to Cursor settings:

```json
{
  "mcpServers": {
    "praisonai": {
      "command": "praisonai",
      "args": ["mcp", "serve", "--transport", "stdio"]
    }
  }
}
```

## Environment Variables

Set the following environment variables for full functionality:

```bash
export OPENAI_API_KEY=your_key
export ANTHROPIC_API_KEY=your_key  # Optional
export GOOGLE_API_KEY=your_key     # Optional
```

## Available Tools

Run `praisonai mcp list-tools` to see all available tools, including:

- `praisonai.chat.completion` - Chat with LLM
- `praisonai.agent.chat` - Chat with an agent
- `praisonai.agent.run` - Run a task with an agent
- `praisonai.workflow.run` - Run a workflow
- `praisonai.images.generate` - Generate images
- `praisonai.audio.transcribe` - Transcribe audio
- `praisonai.embed.create` - Create embeddings
- `praisonai.memory.*` - Memory operations
- `praisonai.knowledge.*` - Knowledge base operations
- And many more...

## Available Resources

- `praisonai://memory/sessions` - List memory sessions
- `praisonai://workflows` - List available workflows
- `praisonai://tools` - List available tools
- `praisonai://agents` - List agent configurations
- `praisonai://knowledge/sources` - List knowledge sources
- `praisonai://config` - Get current configuration
- `praisonai://mcp/status` - Get MCP server status

## Available Prompts

- `deep-research` - Generate deep research prompts
- `code-review` - Generate code review prompts
- `workflow-auto` - Generate workflow auto-generation prompts
- `guardrail-check` - Generate guardrail check prompts
- `context-engineering` - Generate context engineering prompts
- `eval-criteria` - Generate evaluation criteria prompts
- `agent-instructions` - Generate agent instructions prompts


--- examples/package_manager/README.md ---
# Package Manager Example

This example demonstrates how to use the PraisonAI package manager for installing and managing Python packages with security defaults.

## Features Demonstrated

- Installing packages with version constraints
- Listing installed packages
- Searching for packages on PyPI
- Managing package index configuration
- Security features (dependency confusion prevention)

## Quick Start

```bash
# Run the example
python package_manager_example.py
```

## CLI Commands

### Install Packages

```bash
# Install a package
praisonai install requests

# Install with version constraint
praisonai install "requests>=2.28"

# Install multiple packages
praisonai install requests httpx aiohttp

# Upgrade existing package
praisonai install requests --upgrade

# JSON output
praisonai install requests --json
```

### Uninstall Packages

```bash
# Uninstall a package
praisonai uninstall requests

# Uninstall without confirmation
praisonai uninstall requests --yes
```

### List Packages

```bash
# List all installed packages
praisonai package list

# JSON output
praisonai package list --json
```

### Search Packages

```bash
# Search PyPI
praisonai package search langchain

# JSON output
praisonai package search langchain --json
```

### Index Configuration

```bash
# Show current index settings
praisonai package index show --json

# Set custom index
praisonai package index set https://pypi.mycompany.com/simple

# Reset to PyPI default
praisonai package index set https://pypi.org/simple
```

## Security Features

By default, only the primary index (PyPI) is used. Extra indexes are blocked to prevent dependency confusion attacks.

```bash
# This will FAIL (extra index not allowed by default)
praisonai install mypackage --extra-index-url https://other.index.com/simple

# Explicitly allow extra index (shows security warning)
praisonai install mypackage \
  --extra-index-url https://other.index.com/simple \
  --allow-extra-index
```

## Environment Variables

- `PRAISONAI_PACKAGE_INDEX_URL` - Override primary index URL
- `PIP_INDEX_URL` - Fallback to pip's index URL


--- examples/policy/README.md ---
# Policy Packs Example

This example demonstrates how to use PraisonAI policy packs for managing tool permissions, data policies, and execution modes.

## Features Demonstrated

- Creating and loading policy packs
- Tool permission checking (allow/deny)
- Policy modes (dev/prod)
- Merging policies
- Data and PII policies

## Quick Start

```bash
python policy_example.py
```

## CLI Commands

```bash
# Show default policy
praisonai recipe policy show

# Show policy from file
praisonai recipe policy show my-policy.yaml

# Create policy template
praisonai recipe policy init -o my-policy.yaml

# Validate policy file
praisonai recipe policy validate my-policy.yaml
```

## Policy File Format

```yaml
name: my-org-policy
version: "1.0"

tools:
  allow:
    - web.search
    - db.query
  deny:
    - shell.exec
    - file.write

network:
  allow_domains:
    - api.openai.com
  deny_domains:
    - localhost

pii:
  mode: redact  # allow, deny, redact
  fields:
    - email
    - phone

data:
  retention_days: 30
  export_allowed: true

modes:
  dev:
    allow_interactive_prompts: true
    strict_tool_enforcement: false
  prod:
    allow_interactive_prompts: false
    strict_tool_enforcement: true
    require_auth: true
```

## Using Policy with Recipe Run

```bash
# Run with policy file
praisonai recipe run my-recipe --policy my-policy.yaml --mode prod
```


--- examples/rag/README.md ---
# RAG Examples

This directory contains comprehensive examples demonstrating RAG (Retrieval Augmented Generation) with PraisonAI Agents.

## Core RAG Concepts

| Example | Description | Key Concepts |
|---------|-------------|--------------|
| `basic_retrieval.py` | Fundamental retrieve-then-generate pattern | Knowledge base, context injection |
| `auto_retrieval.py` | Intelligent retrieval decisions with AutoRagAgent | Conditional RAG, retrieval policies |
| `chunking_strategies.py` | Document splitting approaches | Paragraph, section, fixed-size chunking |
| `hybrid_search.py` | Dense + sparse retrieval combination | Semantic search, keyword matching |
| `reranking.py` | Two-stage retrieval with reranking | Precision optimization, cross-encoders |

## Advanced RAG Patterns

| Example | Description | Key Concepts |
|---------|-------------|--------------|
| `multi_document.py` | Synthesizing across multiple sources | Cross-referencing, source diversity |
| `knowledge_graph.py` | Relationship-aware retrieval | Entity connections, multi-hop reasoning |
| `external_sources.py` | Web, API, and database augmentation | Real-time data, hybrid sources |
| `structured_output.py` | Guided and constrained generation | JSON output, Pydantic schemas |
| `retrieval_policies.py` | Agent-decided retrieval strategies | Adaptive top-k, priority filtering |
| `citations.py` | Source attribution and verification | Inline citations, verification |

## Application Examples

### PDF Q&A (`rag_pdf_qa.py`)

Basic RAG example showing how to:
- Use Agent with knowledge parameter
- Create explicit RAG pipeline with citations
- Stream responses

```bash
python rag_pdf_qa.py
```

### Multi-Agent RAG (`rag_multi_agent.py`)

Advanced example showing:
- Shared knowledge base across multiple agents
- Collaborative RAG workflow
- Sequential task processing

```bash
python rag_multi_agent.py
```

### RAG Evaluation (`rag_evaluation.py`)

Evaluation example showing:
- Golden query testing
- Retrieval accuracy metrics
- Answer relevance checking

```bash
python rag_evaluation.py
```

## Quick Start

Run any example:

```bash
# Basic RAG
python basic_retrieval.py

# Auto RAG with intelligent decisions
python auto_retrieval.py

# Hybrid search (dense + sparse)
python hybrid_search.py

# RAG with citations
python citations.py
```

## Installation

```bash
pip install "praisonaiagents[knowledge]"
```

## CLI Usage

```bash
# Index documents
praisonai rag index ./documents --collection my_docs

# Query
praisonai rag query "What is the main finding?" --collection my_docs

# Interactive chat
praisonai rag chat --collection my_docs

# Evaluate
praisonai rag eval golden_queries.json --collection my_docs
```

## Configuration

Create `rag_config.yaml`:

```yaml
knowledge:
  collection: my_docs
  chunking:
    strategy: recursive
    chunk_size: 512

rag:
  top_k: 5
  min_score: 0.3
  include_citations: true
```

Use with:
```bash
praisonai rag query "Question" --config rag_config.yaml
```


--- examples/registry/README.md ---
# Recipe Registry Example

This example demonstrates how to use the PraisonAI recipe registry for publishing and pulling recipes via both local filesystem and HTTP server.

## Features Demonstrated

- Creating a local registry
- Starting an HTTP registry server
- Publishing recipes with token authentication
- Listing and searching recipes
- Pulling recipes from HTTP registry

## Quick Start

```bash
# Run the HTTP registry example
python http_registry_example.py
```

## HTTP Registry Server

```bash
# Start HTTP registry server
praisonai registry serve --port 7777

# Start with authentication
praisonai registry serve --port 7777 --token mysecret

# Check server status
praisonai registry status --registry http://localhost:7777
```

## CLI Commands with HTTP Registry

```bash
# Publish to HTTP registry
praisonai recipe publish ./my-recipe --registry http://localhost:7777 --json

# Publish with token
praisonai recipe publish ./my-recipe --registry http://localhost:7777 --token mysecret

# Pull from HTTP registry
praisonai recipe pull my-recipe@1.0.0 --registry http://localhost:7777 -o ./recipes

# List recipes from HTTP registry
praisonai recipe list --registry http://localhost:7777 --json

# Search HTTP registry
praisonai recipe search "hello" --registry http://localhost:7777
```

## Local Registry Commands

```bash
# Publish to local registry (default)
praisonai recipe publish ./my-recipe --json

# Pull from local registry
praisonai recipe pull my-recipe@1.0.0 -o ./recipes

# Use custom local path
praisonai recipe publish ./my-recipe --registry /path/to/registry
```

## Environment Variables

- `PRAISONAI_REGISTRY_TOKEN` - Default token for HTTP registry authentication

## Default Registry Location

By default, the local registry is stored at `~/.praison/registry`.


--- api.md ---
# PraisonAI API Reference

This file is auto-generated. Do not edit manually.
Regenerate with: `praisonai docs api-md --write`

# Shared Types

Types:
```python
from praisonaiagents import ContextPolicy, GuardrailResult, HandoffConfig, HandoffCycleError, HandoffDepthError, HandoffError, HandoffInputData, HandoffResult, HandoffTimeoutError, ReflectionOutput, StepResult, TaskOutput, ToolResult, ToolValidationError, WorkflowContext
```

Methods:

* <code title="class GuardrailResult">GuardrailResult.<a href="./src/praisonai-agents/praisonaiagents/guardrails/guardrail_result.py">from_tuple</a>(result: Tuple[bool, Any]) -> 'GuardrailResult'</code>
* <code title="class HandoffConfig">HandoffConfig.<a href="./src/praisonai-agents/praisonaiagents/agent/handoff.py">from_dict</a>(data: Dict[str, Any]) -> 'HandoffConfig'</code>
* <code title="class HandoffConfig">HandoffConfig.<a href="./src/praisonai-agents/praisonaiagents/agent/handoff.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class TaskOutput">TaskOutput.<a href="./src/praisonai-agents/praisonaiagents/main.py">json</a>() -> Optional[str]</code>
* <code title="class TaskOutput">TaskOutput.<a href="./src/praisonai-agents/praisonaiagents/main.py">to_dict</a>() -> dict</code>
* <code title="class ToolResult">ToolResult.<a href="./src/praisonai-agents/praisonaiagents/tools/base.py">to_dict</a>() -> Dict[str, Any]</code>

# Agents

Types:
```python
from praisonaiagents import Agent, AutoAgents, AutoRagAgent, ContextAgent, DeepResearchAgent, ImageAgent, PlanningAgent, PromptExpanderAgent, QueryRewriterAgent, create_context_agent
```

Methods:

* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">achat</a>(prompt: str, temperature = 1.0, tools = None, output_json = None, output_pydantic = None, reasoning_steps = False, task_name = None, task_description = None, task_id = None, attachments = None)</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">aexecute</a>(task, context = None)</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">agent_id</a>()</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">analyze_prompt</a>(prompt: str) -> set</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">arun</a>(prompt: str, **kwargs)</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">astart</a>(prompt: str, **kwargs)</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">auto_memory</a>()</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">auto_memory</a>(value)</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">background</a>()</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">background</a>(value)</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">chat</a>(prompt, temperature = 1.0, tools = None, output_json = None, output_pydantic = None, reasoning_steps = False, stream = None, task_name = None, task_description = None, task_id = None, config = None, force_retrieval = False, skip_retrieval = False, attachments = None, tool_choice = None)</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">chat_with_context</a>(message: str, context: 'ContextPack', **kwargs) -> str</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">checkpoints</a>()</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">checkpoints</a>(value)</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">clean_json_output</a>(output: str) -> str</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">clear_history</a>()</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">console</a>()</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">context_manager</a>()</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">context_manager</a>(value)</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">delete_history</a>(index: int) -> bool</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">delete_history_matching</a>(pattern: str) -> int</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">display_name</a>() -> str</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">ephemeral</a>()</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">execute</a>(task, context = None)</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">execute_tool</a>(function_name, arguments)</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">execute_tool_async</a>(function_name: str, arguments: Dict[str, Any]) -> Any</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">from_template</a>(uri: str, config: Optional[Dict[str, Any]] = None, offline: bool = False, **kwargs) -> 'Agent'</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">generate_task</a>() -> 'Task'</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">get_available_tools</a>() -> List[Any]</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">get_history_size</a>() -> int</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">get_learn_context</a>() -> str</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">get_memory_context</a>(query: Optional[str] = None) -> str</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">get_recommended_stage</a>(prompt: str) -> str</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">get_rules_context</a>(file_path: Optional[str] = None, include_manual: Optional[List[str]] = None) -> str</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">get_skills_prompt</a>() -> str</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">handoff_to</a>(target_agent: 'Agent', prompt: str, context: Optional[Dict[str, Any]] = None, config: Optional['HandoffConfig'] = None) -> 'HandoffResult'</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">handoff_to_async</a>(target_agent: 'Agent', prompt: str, context: Optional[Dict[str, Any]] = None, config: Optional['HandoffConfig'] = None) -> 'HandoffResult'</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">iter_stream</a>(prompt: str, **kwargs)</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">launch</a>(path: str = '/', port: int = 8000, host: str = '0.0.0.0', debug: bool = False, protocol: str = 'http')</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">llm_model</a>()</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">output_style</a>()</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">output_style</a>(value)</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">policy</a>()</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">policy</a>(value)</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">prune_history</a>(keep_last: int = 5) -> int</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">query</a>(question: str, **kwargs) -> 'RAGResult'</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">rag</a>()</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">rag_query</a>(question: str, **kwargs) -> 'RAGResult'</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">retrieval_config</a>()</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">retrieve</a>(query: str, **kwargs) -> 'ContextPack'</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">rules_manager</a>()</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">run</a>(prompt: str, **kwargs)</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">run_autonomous</a>(prompt: str, max_iterations: Optional[int] = None, timeout_seconds: Optional[float] = None, completion_promise: Optional[str] = None, clear_context: bool = False)</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">run_autonomous_async</a>(prompt: str, max_iterations: Optional[int] = None, timeout_seconds: Optional[float] = None, completion_promise: Optional[str] = None, clear_context: bool = False)</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">run_until</a>(prompt: str, criteria: str, threshold: float = 8.0, max_iterations: int = 5, mode: str = 'optimize', on_iteration: Optional[Callable[[Any], None]] = None, verbose: bool = False) -> 'EvaluationLoopResult'</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">run_until_async</a>(prompt: str, criteria: str, threshold: float = 8.0, max_iterations: int = 5, mode: str = 'optimize', on_iteration: Optional[Callable[[Any], None]] = None, verbose: bool = False) -> 'EvaluationLoopResult'</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">session_id</a>() -> Optional[str]</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">skill_manager</a>()</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">start</a>(prompt: str = None, **kwargs)</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">store_memory</a>(content: str, memory_type: str = 'short_term', **kwargs)</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">stream_emitter</a>()</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">stream_emitter</a>(value)</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">switch_model</a>(new_model: str) -> None</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">thinking_budget</a>()</code>
* <code title="class Agent">Agent.<a href="./src/praisonai-agents/praisonaiagents/agent/agent.py">thinking_budget</a>(value)</code>
* <code title="class AutoAgents">AutoAgents.<a href="./src/praisonai-agents/praisonaiagents/agents/autoagents.py">astart</a>()</code>
* <code title="class AutoAgents">AutoAgents.<a href="./src/praisonai-agents/praisonaiagents/agents/autoagents.py">start</a>()</code>
* <code title="class AutoRagAgent">AutoRagAgent.<a href="./src/praisonai-agents/praisonaiagents/agents/auto_rag_agent.py">achat</a>(message: str, **kwargs) -> str</code>
* <code title="class AutoRagAgent">AutoRagAgent.<a href="./src/praisonai-agents/praisonaiagents/agents/auto_rag_agent.py">chat</a>(message: str, **kwargs) -> str</code>
* <code title="class AutoRagAgent">AutoRagAgent.<a href="./src/praisonai-agents/praisonaiagents/agents/auto_rag_agent.py">name</a>() -> str</code>
* <code title="class AutoRagAgent">AutoRagAgent.<a href="./src/praisonai-agents/praisonaiagents/agents/auto_rag_agent.py">rag</a>() -> Optional['RAG']</code>
* <code title="class ContextAgent">ContextAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">aanalyze_codebase</a>(project_path: str) -> Dict[str, Any]</code>
* <code title="class ContextAgent">ContextAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">acreate_implementation_blueprint</a>(feature_request: str, context_analysis: Optional[Dict[str, Any]] = None) -> Dict[str, Any]</code>
* <code title="class ContextAgent">ContextAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">agenerate_prp</a>(feature_request: str, context_analysis: Optional[Dict[str, Any]] = None) -> str</code>
* <code title="class ContextAgent">ContextAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">analyze_codebase</a>(project_path: str) -> Dict[str, Any]</code>
* <code title="class ContextAgent">ContextAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">analyze_codebase_with_gitingest</a>(project_path: str) -> Dict[str, Any]</code>
* <code title="class ContextAgent">ContextAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">analyze_integration_points</a>(project_path: str) -> Dict[str, Any]</code>
* <code title="class ContextAgent">ContextAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">analyze_test_patterns</a>(project_path: str) -> Dict[str, Any]</code>
* <code title="class ContextAgent">ContextAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">build_implementation_blueprint</a>(feature_request: str, context_analysis: Dict[str, Any] = None) -> Dict[str, Any]</code>
* <code title="class ContextAgent">ContextAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">compile_context_documentation</a>(project_path: str) -> Dict[str, Any]</code>
* <code title="class ContextAgent">ContextAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">create_implementation_blueprint</a>(feature_request: str, context_analysis: Optional[Dict[str, Any]] = None) -> Dict[str, Any]</code>
* <code title="class ContextAgent">ContextAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">create_quality_gates</a>(requirements: List[str]) -> Dict[str, Any]</code>
* <code title="class ContextAgent">ContextAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">create_validation_framework</a>(project_path: str) -> Dict[str, Any]</code>
* <code title="class ContextAgent">ContextAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">execute_prp</a>(prp_file_path: str) -> Dict[str, Any]</code>
* <code title="class ContextAgent">ContextAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">extract_implementation_patterns</a>(project_path: str, ast_analysis: Dict[str, Any] = None) -> Dict[str, Any]</code>
* <code title="class ContextAgent">ContextAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">generate_comprehensive_prp</a>(feature_request: str, context_analysis: Dict[str, Any] = None) -> str</code>
* <code title="class ContextAgent">ContextAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">generate_feature_prp</a>(feature_request: str) -> str</code>
* <code title="class ContextAgent">ContextAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">generate_prp</a>(feature_request: str, context_analysis: Optional[Dict[str, Any]] = None) -> str</code>
* <code title="class ContextAgent">ContextAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">get_agent_interaction_summary</a>() -> Dict[str, Any]</code>
* <code title="class ContextAgent">ContextAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">log_debug</a>(message: str, **kwargs)</code>
* <code title="class ContextAgent">ContextAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">perform_ast_analysis</a>(project_path: str) -> Dict[str, Any]</code>
* <code title="class ContextAgent">ContextAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">save_comprehensive_session_report</a>()</code>
* <code title="class ContextAgent">ContextAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">save_markdown_output</a>(content: str, filename: str, section_title: str = 'Output')</code>
* <code title="class ContextAgent">ContextAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">setup_logging</a>()</code>
* <code title="class ContextAgent">ContextAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">setup_output_directories</a>()</code>
* <code title="class ContextAgent">ContextAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">start</a>(input_text: str) -> str</code>
* <code title="class DeepResearchAgent">DeepResearchAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/deep_research_agent.py">aresearch</a>(query: str, instructions: Optional[str] = None, model: Optional[str] = None, summary_mode: Optional[Literal['auto', 'detailed', 'concise']] = None, web_search: Optional[bool] = None, code_interpreter: Optional[bool] = None, mcp_servers: Optional[List[Dict[str, Any]]] = None, file_ids: Optional[List[str]] = None, file_search: Optional[bool] = None, file_search_stores: Optional[List[str]] = None) -> DeepResearchResponse</code>
* <code title="class DeepResearchAgent">DeepResearchAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/deep_research_agent.py">async_openai_client</a>()</code>
* <code title="class DeepResearchAgent">DeepResearchAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/deep_research_agent.py">clarify</a>(query: str, model: Optional[str] = None) -> str</code>
* <code title="class DeepResearchAgent">DeepResearchAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/deep_research_agent.py">follow_up</a>(query: str, previous_interaction_id: str, model: Optional[str] = None) -> DeepResearchResponse</code>
* <code title="class DeepResearchAgent">DeepResearchAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/deep_research_agent.py">gemini_client</a>()</code>
* <code title="class DeepResearchAgent">DeepResearchAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/deep_research_agent.py">openai_client</a>()</code>
* <code title="class DeepResearchAgent">DeepResearchAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/deep_research_agent.py">research</a>(query: str, instructions: Optional[str] = None, model: Optional[str] = None, summary_mode: Optional[Literal['auto', 'detailed', 'concise']] = None, web_search: Optional[bool] = None, code_interpreter: Optional[bool] = None, mcp_servers: Optional[List[Dict[str, Any]]] = None, file_ids: Optional[List[str]] = None, file_search: Optional[bool] = None, file_search_stores: Optional[List[str]] = None, stream: bool = True) -> DeepResearchResponse</code>
* <code title="class DeepResearchAgent">DeepResearchAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/deep_research_agent.py">rewrite_query</a>(query: str, model: Optional[str] = None) -> str</code>
* <code title="class ImageAgent">ImageAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/image_agent.py">achat</a>(prompt: str, temperature: float = 0.2, tools: Optional[List[Any]] = None, output_json: Optional[str] = None, output_pydantic: Optional[Any] = None, reasoning_steps: bool = False, **kwargs) -> Union[str, Dict[str, Any]]</code>
* <code title="class ImageAgent">ImageAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/image_agent.py">aedit</a>(image: str, prompt: str, mask: Optional[str] = None, n: int = 1, size: Optional[str] = None, **kwargs) -> Dict[str, Any]</code>
* <code title="class ImageAgent">ImageAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/image_agent.py">agenerate</a>(prompt: str, **kwargs) -> Dict[str, Any]</code>
* <code title="class ImageAgent">ImageAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/image_agent.py">agenerate_image</a>(prompt: str, **kwargs) -> Dict[str, Any]</code>
* <code title="class ImageAgent">ImageAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/image_agent.py">avariation</a>(image: str, n: int = 1, size: Optional[str] = None, **kwargs) -> Dict[str, Any]</code>
* <code title="class ImageAgent">ImageAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/image_agent.py">chat</a>(prompt: str, **kwargs) -> Dict[str, Any]</code>
* <code title="class ImageAgent">ImageAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/image_agent.py">edit</a>(image: str, prompt: str, mask: Optional[str] = None, n: int = 1, size: Optional[str] = None, **kwargs) -> Dict[str, Any]</code>
* <code title="class ImageAgent">ImageAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/image_agent.py">generate</a>(prompt: str, **kwargs) -> Dict[str, Any]</code>
* <code title="class ImageAgent">ImageAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/image_agent.py">generate_image</a>(prompt: str, **kwargs) -> Dict[str, Any]</code>
* <code title="class ImageAgent">ImageAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/image_agent.py">litellm</a>()</code>
* <code title="class ImageAgent">ImageAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/image_agent.py">variation</a>(image: str, n: int = 1, size: Optional[str] = None, **kwargs) -> Dict[str, Any]</code>
* <code title="class PlanningAgent">PlanningAgent.<a href="./src/praisonai-agents/praisonaiagents/planning/planner.py">analyze_context</a>(context: str) -> str</code>
* <code title="class PlanningAgent">PlanningAgent.<a href="./src/praisonai-agents/praisonaiagents/planning/planner.py">analyze_context_sync</a>(context: str) -> str</code>
* <code title="class PlanningAgent">PlanningAgent.<a href="./src/praisonai-agents/praisonaiagents/planning/planner.py">create_plan</a>(request: str, agents: List['Agent'], tasks: Optional[List['Task']] = None, context: Optional[str] = None) -> Plan</code>
* <code title="class PlanningAgent">PlanningAgent.<a href="./src/praisonai-agents/praisonaiagents/planning/planner.py">create_plan_sync</a>(request: str, agents: List['Agent'], tasks: Optional[List['Task']] = None, context: Optional[str] = None) -> Plan</code>
* <code title="class PlanningAgent">PlanningAgent.<a href="./src/praisonai-agents/praisonaiagents/planning/planner.py">is_tool_allowed</a>(tool_name: str) -> bool</code>
* <code title="class PlanningAgent">PlanningAgent.<a href="./src/praisonai-agents/praisonaiagents/planning/planner.py">refine_plan</a>(plan: Plan, feedback: str) -> Plan</code>
* <code title="class PlanningAgent">PlanningAgent.<a href="./src/praisonai-agents/praisonaiagents/planning/planner.py">refine_plan_sync</a>(plan: Plan, feedback: str) -> Plan</code>
* <code title="class PromptExpanderAgent">PromptExpanderAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/prompt_expander_agent.py">agent</a>()</code>
* <code title="class PromptExpanderAgent">PromptExpanderAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/prompt_expander_agent.py">expand</a>(prompt: str, strategy: ExpandStrategy = ..., context: Optional[str] = None) -> ExpandResult</code>
* <code title="class PromptExpanderAgent">PromptExpanderAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/prompt_expander_agent.py">expand_basic</a>(prompt: str, context: Optional[str] = None) -> ExpandResult</code>
* <code title="class PromptExpanderAgent">PromptExpanderAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/prompt_expander_agent.py">expand_creative</a>(prompt: str, context: Optional[str] = None) -> ExpandResult</code>
* <code title="class PromptExpanderAgent">PromptExpanderAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/prompt_expander_agent.py">expand_detailed</a>(prompt: str, context: Optional[str] = None) -> ExpandResult</code>
* <code title="class PromptExpanderAgent">PromptExpanderAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/prompt_expander_agent.py">expand_structured</a>(prompt: str, context: Optional[str] = None) -> ExpandResult</code>
* <code title="class QueryRewriterAgent">QueryRewriterAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/query_rewriter_agent.py">add_abbreviation</a>(abbrev: str, expansion: str) -> None</code>
* <code title="class QueryRewriterAgent">QueryRewriterAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/query_rewriter_agent.py">add_abbreviations</a>(abbreviations: Dict[str, str]) -> None</code>
* <code title="class QueryRewriterAgent">QueryRewriterAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/query_rewriter_agent.py">agent</a>()</code>
* <code title="class QueryRewriterAgent">QueryRewriterAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/query_rewriter_agent.py">rewrite</a>(query: str, strategy: RewriteStrategy = ..., chat_history: Optional[List[Dict[str, str]]] = None, context: Optional[str] = None, num_queries: int = None) -> RewriteResult</code>
* <code title="class QueryRewriterAgent">QueryRewriterAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/query_rewriter_agent.py">rewrite_basic</a>(query: str) -> RewriteResult</code>
* <code title="class QueryRewriterAgent">QueryRewriterAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/query_rewriter_agent.py">rewrite_contextual</a>(query: str, chat_history: List[Dict[str, str]]) -> RewriteResult</code>
* <code title="class QueryRewriterAgent">QueryRewriterAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/query_rewriter_agent.py">rewrite_hyde</a>(query: str) -> RewriteResult</code>
* <code title="class QueryRewriterAgent">QueryRewriterAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/query_rewriter_agent.py">rewrite_multi_query</a>(query: str, num_queries: int = None) -> RewriteResult</code>
* <code title="class QueryRewriterAgent">QueryRewriterAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/query_rewriter_agent.py">rewrite_step_back</a>(query: str) -> RewriteResult</code>
* <code title="class QueryRewriterAgent">QueryRewriterAgent.<a href="./src/praisonai-agents/praisonaiagents/agent/query_rewriter_agent.py">rewrite_sub_queries</a>(query: str) -> RewriteResult</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/agent/context_agent.py">create_context_agent</a>(llm: Optional[Union[str, Any]] = None, **kwargs) -> ContextAgent</code>

# Tools

Types:
```python
from praisonaiagents import BaseTool, FunctionTool, ToolRegistry, Tools, get_registry, get_tool, register_tool, tool, validate_tool
```

Methods:

* <code title="class BaseTool">BaseTool.<a href="./src/praisonai-agents/praisonaiagents/tools/base.py">__call__</a>(**kwargs) -> Any</code>
* <code title="class BaseTool">BaseTool.<a href="./src/praisonai-agents/praisonaiagents/tools/base.py">get_schema</a>() -> Dict[str, Any]</code>
* <code title="class BaseTool">BaseTool.<a href="./src/praisonai-agents/praisonaiagents/tools/base.py">run</a>(**kwargs) -> Any</code>
* <code title="class BaseTool">BaseTool.<a href="./src/praisonai-agents/praisonaiagents/tools/base.py">safe_run</a>(**kwargs) -> ToolResult</code>
* <code title="class BaseTool">BaseTool.<a href="./src/praisonai-agents/praisonaiagents/tools/base.py">validate</a>() -> bool</code>
* <code title="class BaseTool">BaseTool.<a href="./src/praisonai-agents/praisonaiagents/tools/base.py">validate_class</a>() -> bool</code>
* <code title="class FunctionTool">FunctionTool.<a href="./src/praisonai-agents/praisonaiagents/tools/decorator.py">__call__</a>(*args, **kwargs) -> Any</code>
* <code title="class FunctionTool">FunctionTool.<a href="./src/praisonai-agents/praisonaiagents/tools/decorator.py">injected_params</a>() -> Dict[str, Any]</code>
* <code title="class FunctionTool">FunctionTool.<a href="./src/praisonai-agents/praisonaiagents/tools/decorator.py">run</a>(**kwargs) -> Any</code>
* <code title="class ToolRegistry">ToolRegistry.<a href="./src/praisonai-agents/praisonaiagents/tools/registry.py">clear</a>() -> None</code>
* <code title="class ToolRegistry">ToolRegistry.<a href="./src/praisonai-agents/praisonaiagents/tools/registry.py">discover_plugins</a>() -> int</code>
* <code title="class ToolRegistry">ToolRegistry.<a href="./src/praisonai-agents/praisonaiagents/tools/registry.py">discover_single_file_plugins</a>() -> int</code>
* <code title="class ToolRegistry">ToolRegistry.<a href="./src/praisonai-agents/praisonaiagents/tools/registry.py">get</a>(name: str) -> Optional[Union[BaseTool, Callable]]</code>
* <code title="class ToolRegistry">ToolRegistry.<a href="./src/praisonai-agents/praisonaiagents/tools/registry.py">get_all</a>() -> Dict[str, Union[BaseTool, Callable]]</code>
* <code title="class ToolRegistry">ToolRegistry.<a href="./src/praisonai-agents/praisonaiagents/tools/registry.py">list_base_tools</a>() -> List[BaseTool]</code>
* <code title="class ToolRegistry">ToolRegistry.<a href="./src/praisonai-agents/praisonaiagents/tools/registry.py">list_tools</a>() -> List[str]</code>
* <code title="class ToolRegistry">ToolRegistry.<a href="./src/praisonai-agents/praisonaiagents/tools/registry.py">register</a>(tool: Union[BaseTool, Callable], name: Optional[str] = None, overwrite: bool = False) -> None</code>
* <code title="class ToolRegistry">ToolRegistry.<a href="./src/praisonai-agents/praisonaiagents/tools/registry.py">unregister</a>(name: str) -> bool</code>
* <code title="class Tools">Tools.<a href="./src/praisonai-agents/praisonaiagents/tools/tools.py">internet_search</a>(*args, **kwargs)</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/tools/registry.py">get_registry</a>() -> ToolRegistry</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/tools/registry.py">get_tool</a>(name: str) -> Optional[Union[BaseTool, Callable]]</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/tools/registry.py">register_tool</a>(tool: Union[BaseTool, Callable], name: Optional[str] = None) -> None</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/tools/decorator.py">tool</a>(func: Optional[Callable] = None) -> Union[FunctionTool, Callable[[Callable], FunctionTool]]</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/tools/base.py">validate_tool</a>(tool: Any) -> bool</code>

# Workflows

Types:
```python
from praisonaiagents import Loop, Parallel, Pipeline, Repeat, Route, Task, Workflow, loop, parallel, repeat, route
```

Methods:

* <code title="class Task">Task.<a href="./src/praisonai-agents/praisonaiagents/task/task.py">depends_on</a>()</code>
* <code title="class Task">Task.<a href="./src/praisonai-agents/praisonaiagents/task/task.py">depends_on</a>(value)</code>
* <code title="class Task">Task.<a href="./src/praisonai-agents/praisonaiagents/task/task.py">evaluate_when</a>(context: Dict[str, Any]) -> bool</code>
* <code title="class Task">Task.<a href="./src/praisonai-agents/praisonaiagents/task/task.py">execute_callback</a>(task_output: TaskOutput) -> None</code>
* <code title="class Task">Task.<a href="./src/praisonai-agents/praisonaiagents/task/task.py">execute_callback_sync</a>(task_output: TaskOutput) -> None</code>
* <code title="class Task">Task.<a href="./src/praisonai-agents/praisonaiagents/task/task.py">get_next_task</a>(context: Dict[str, Any]) -> Optional[str]</code>
* <code title="class Task">Task.<a href="./src/praisonai-agents/praisonaiagents/task/task.py">initialize_memory</a>()</code>
* <code title="class Task">Task.<a href="./src/praisonai-agents/praisonaiagents/task/task.py">store_in_memory</a>(content: str, agent_name: str = None, task_id: str = None)</code>
* <code title="class Task">Task.<a href="./src/praisonai-agents/praisonaiagents/task/task.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/workflows/workflows.py">loop</a>(step: Any = None, steps: Optional[List[Any]] = None, over: Optional[str] = None, from_csv: Optional[str] = None, from_file: Optional[str] = None, var_name: str = 'item', parallel: bool = False, max_workers: Optional[int] = None, output_variable: Optional[str] = None) -> Loop</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/workflows/workflows.py">parallel</a>(steps: List) -> Parallel</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/workflows/workflows.py">repeat</a>(step: Any, until: Optional[Callable[[WorkflowContext], bool]] = None, max_iterations: int = 10) -> Repeat</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/workflows/workflows.py">route</a>(routes: Dict[str, List], default: Optional[List] = None) -> Route</code>

# DB

Types:
```python
from praisonaiagents import db
```

# Memory

Types:
```python
from praisonaiagents import Memory, MemoryBackend, MemoryConfig
```

Methods:

* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">build_context_for_task</a>(task_descr: str, user_id: Optional[str] = None, additional: str = '', max_items: int = 3, include_in_output: Optional[bool] = None) -> str</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">calculate_quality_metrics</a>(output: str, expected_output: str, llm: Optional[str] = None, custom_prompt: Optional[str] = None) -> Dict[str, float]</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">compute_quality_score</a>(completeness: float, relevance: float, clarity: float, accuracy: float, weights: Dict[str, float] = None) -> float</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">delete_long_term</a>(memory_id: str) -> bool</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">delete_memories</a>(memory_ids: List[str]) -> int</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">delete_memories_matching</a>(query: str, memory_type: Optional[str] = None, limit: int = 10) -> int</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">delete_memory</a>(memory_id: str, memory_type: Optional[str] = None) -> bool</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">delete_short_term</a>(memory_id: str) -> bool</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">finalize_task_output</a>(content: str, agent_name: str, quality_score: float, threshold: float = 0.7, metrics: Dict[str, Any] = None, task_id: str = None)</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">get_all_memories</a>() -> List[Dict[str, Any]]</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">get_learn_context</a>() -> str</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">learn</a>()</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">reset_all</a>()</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">reset_entity_only</a>()</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">reset_long_term</a>()</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">reset_short_term</a>()</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">reset_user_memory</a>()</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">search</a>(query: str, user_id: Optional[str] = None, agent_id: Optional[str] = None, run_id: Optional[str] = None, limit: int = 5, rerank: bool = False, **kwargs) -> List[Dict[str, Any]]</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">search_entity</a>(query: str, limit: int = 5) -> List[Dict[str, Any]]</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">search_long_term</a>(query: str, limit: int = 5, relevance_cutoff: float = 0.0, min_quality: float = 0.0, rerank: bool = False, **kwargs) -> List[Dict[str, Any]]</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">search_short_term</a>(query: str, limit: int = 5, min_quality: float = 0.0, relevance_cutoff: float = 0.0, rerank: bool = False, **kwargs) -> List[Dict[str, Any]]</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">search_user_memory</a>(user_id: str, query: str, limit: int = 5, rerank: bool = False, **kwargs) -> List[Dict[str, Any]]</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">search_with_quality</a>(query: str, min_quality: float = 0.0, memory_type: Literal['short', 'long'] = 'long', limit: int = 5) -> List[Dict[str, Any]]</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">store_entity</a>(name: str, type_: str, desc: str, relations: str)</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">store_long_term</a>(text: str, metadata: Dict[str, Any] = None, completeness: float = None, relevance: float = None, clarity: float = None, accuracy: float = None, weights: Dict[str, float] = None, evaluator_quality: float = None)</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">store_quality</a>(text: str, quality_score: float, task_id: Optional[str] = None, iteration: Optional[int] = None, metrics: Optional[Dict[str, float]] = None, memory_type: Literal['short', 'long'] = 'long') -> None</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">store_short_term</a>(text: str, metadata: Dict[str, Any] = None, completeness: float = None, relevance: float = None, clarity: float = None, accuracy: float = None, weights: Dict[str, float] = None, evaluator_quality: float = None)</code>
* <code title="class Memory">Memory.<a href="./src/praisonai-agents/praisonaiagents/memory/memory.py">store_user_memory</a>(user_id: str, text: str, extra: Dict[str, Any] = None)</code>
* <code title="class MemoryConfig">MemoryConfig.<a href="./src/praisonai-agents/praisonaiagents/config/feature_configs.py">to_dict</a>() -> Dict[str, Any]</code>

# Knowledge

Types:
```python
from praisonaiagents import Chunking, ChunkingStrategy, Knowledge, KnowledgeConfig
```

Methods:

* <code title="class Chunking">Chunking.<a href="./src/praisonai-agents/praisonaiagents/knowledge/chunking.py">SUPPORTED_CHUNKERS</a>() -> Dict[str, Any]</code>
* <code title="class Chunking">Chunking.<a href="./src/praisonai-agents/praisonaiagents/knowledge/chunking.py">__call__</a>(text: Union[str, List[str]], **kwargs) -> Union[List[Any], List[List[Any]]]</code>
* <code title="class Chunking">Chunking.<a href="./src/praisonai-agents/praisonaiagents/knowledge/chunking.py">chunk</a>(text: Union[str, List[str]], **kwargs) -> Union[List[Any], List[List[Any]]]</code>
* <code title="class Chunking">Chunking.<a href="./src/praisonai-agents/praisonaiagents/knowledge/chunking.py">chunker</a>()</code>
* <code title="class Chunking">Chunking.<a href="./src/praisonai-agents/praisonaiagents/knowledge/chunking.py">embedding_model</a>()</code>
* <code title="class Knowledge">Knowledge.<a href="./src/praisonai-agents/praisonaiagents/knowledge/knowledge.py">add</a>(file_path, user_id = None, agent_id = None, run_id = None, metadata = None)</code>
* <code title="class Knowledge">Knowledge.<a href="./src/praisonai-agents/praisonaiagents/knowledge/knowledge.py">chunker</a>()</code>
* <code title="class Knowledge">Knowledge.<a href="./src/praisonai-agents/praisonaiagents/knowledge/knowledge.py">config</a>()</code>
* <code title="class Knowledge">Knowledge.<a href="./src/praisonai-agents/praisonaiagents/knowledge/knowledge.py">delete</a>(memory_id)</code>
* <code title="class Knowledge">Knowledge.<a href="./src/praisonai-agents/praisonaiagents/knowledge/knowledge.py">delete_all</a>(user_id = None, agent_id = None, run_id = None)</code>
* <code title="class Knowledge">Knowledge.<a href="./src/praisonai-agents/praisonaiagents/knowledge/knowledge.py">get</a>(memory_id)</code>
* <code title="class Knowledge">Knowledge.<a href="./src/praisonai-agents/praisonaiagents/knowledge/knowledge.py">get_all</a>(user_id = None, agent_id = None, run_id = None)</code>
* <code title="class Knowledge">Knowledge.<a href="./src/praisonai-agents/praisonaiagents/knowledge/knowledge.py">get_corpus_stats</a>()</code>
* <code title="class Knowledge">Knowledge.<a href="./src/praisonai-agents/praisonaiagents/knowledge/knowledge.py">history</a>(memory_id)</code>
* <code title="class Knowledge">Knowledge.<a href="./src/praisonai-agents/praisonaiagents/knowledge/knowledge.py">index</a>(path: str, incremental: bool = True, force: bool = False, include_glob: list = None, exclude_glob: list = None, user_id: str = None, agent_id: str = None, run_id: str = None)</code>
* <code title="class Knowledge">Knowledge.<a href="./src/praisonai-agents/praisonaiagents/knowledge/knowledge.py">markdown</a>()</code>
* <code title="class Knowledge">Knowledge.<a href="./src/praisonai-agents/praisonaiagents/knowledge/knowledge.py">memory</a>()</code>
* <code title="class Knowledge">Knowledge.<a href="./src/praisonai-agents/praisonaiagents/knowledge/knowledge.py">normalize_content</a>(content)</code>
* <code title="class Knowledge">Knowledge.<a href="./src/praisonai-agents/praisonaiagents/knowledge/knowledge.py">reset</a>()</code>
* <code title="class Knowledge">Knowledge.<a href="./src/praisonai-agents/praisonaiagents/knowledge/knowledge.py">search</a>(query, user_id = None, agent_id = None, run_id = None, rerank = None, **kwargs)</code>
* <code title="class Knowledge">Knowledge.<a href="./src/praisonai-agents/praisonaiagents/knowledge/knowledge.py">store</a>(content, user_id = None, agent_id = None, run_id = None, metadata = None)</code>
* <code title="class Knowledge">Knowledge.<a href="./src/praisonai-agents/praisonaiagents/knowledge/knowledge.py">update</a>(memory_id, data)</code>
* <code title="class KnowledgeConfig">KnowledgeConfig.<a href="./src/praisonai-agents/praisonaiagents/config/feature_configs.py">to_dict</a>() -> Dict[str, Any]</code>

# RAG

Types:
```python
from praisonaiagents import CitationsMode, ContextPack, RAG, RAGCitation, RAGConfig, RAGResult, RetrievalConfig, RetrievalPolicy
```

Methods:

* <code title="class ContextPack">ContextPack.<a href="./src/praisonai-agents/praisonaiagents/rag/models.py">format_for_prompt</a>(include_sources: bool = True) -> str</code>
* <code title="class ContextPack">ContextPack.<a href="./src/praisonai-agents/praisonaiagents/rag/models.py">from_dict</a>(data: Dict[str, Any]) -> 'ContextPack'</code>
* <code title="class ContextPack">ContextPack.<a href="./src/praisonai-agents/praisonaiagents/rag/models.py">has_citations</a>() -> bool</code>
* <code title="class ContextPack">ContextPack.<a href="./src/praisonai-agents/praisonaiagents/rag/models.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class RAG">RAG.<a href="./src/praisonai-agents/praisonaiagents/rag/pipeline.py">aquery</a>(question: str, user_id: Optional[str] = None, agent_id: Optional[str] = None, run_id: Optional[str] = None, **kwargs) -> RAGResult</code>
* <code title="class RAG">RAG.<a href="./src/praisonai-agents/praisonaiagents/rag/pipeline.py">aretrieve</a>(query: str, **kwargs) -> ContextPack</code>
* <code title="class RAG">RAG.<a href="./src/praisonai-agents/praisonaiagents/rag/pipeline.py">astream</a>(question: str, user_id: Optional[str] = None, agent_id: Optional[str] = None, run_id: Optional[str] = None, **kwargs) -> AsyncIterator[str]</code>
* <code title="class RAG">RAG.<a href="./src/praisonai-agents/praisonaiagents/rag/pipeline.py">get_citations</a>(question: str, user_id: Optional[str] = None, agent_id: Optional[str] = None, run_id: Optional[str] = None, **kwargs) -> List[Citation]</code>
* <code title="class RAG">RAG.<a href="./src/praisonai-agents/praisonaiagents/rag/pipeline.py">llm</a>()</code>
* <code title="class RAG">RAG.<a href="./src/praisonai-agents/praisonaiagents/rag/pipeline.py">query</a>(question: str, user_id: Optional[str] = None, agent_id: Optional[str] = None, run_id: Optional[str] = None, **kwargs) -> RAGResult</code>
* <code title="class RAG">RAG.<a href="./src/praisonai-agents/praisonaiagents/rag/pipeline.py">retrieve</a>(query: str, **kwargs) -> ContextPack</code>
* <code title="class RAG">RAG.<a href="./src/praisonai-agents/praisonaiagents/rag/pipeline.py">stream</a>(question: str, user_id: Optional[str] = None, agent_id: Optional[str] = None, run_id: Optional[str] = None, **kwargs) -> Iterator[str]</code>
* <code title="class RAGCitation">RAGCitation.<a href="./src/praisonai-agents/praisonaiagents/rag/models.py">from_dict</a>(data: Dict[str, Any]) -> 'Citation'</code>
* <code title="class RAGCitation">RAGCitation.<a href="./src/praisonai-agents/praisonaiagents/rag/models.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class RAGConfig">RAGConfig.<a href="./src/praisonai-agents/praisonaiagents/rag/models.py">from_dict</a>(data: Dict[str, Any]) -> 'RAGConfig'</code>
* <code title="class RAGConfig">RAGConfig.<a href="./src/praisonai-agents/praisonaiagents/rag/models.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class RAGResult">RAGResult.<a href="./src/praisonai-agents/praisonaiagents/rag/models.py">format_answer_with_citations</a>() -> str</code>
* <code title="class RAGResult">RAGResult.<a href="./src/praisonai-agents/praisonaiagents/rag/models.py">from_dict</a>(data: Dict[str, Any]) -> 'RAGResult'</code>
* <code title="class RAGResult">RAGResult.<a href="./src/praisonai-agents/praisonaiagents/rag/models.py">has_citations</a>() -> bool</code>
* <code title="class RAGResult">RAGResult.<a href="./src/praisonai-agents/praisonaiagents/rag/models.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class RetrievalConfig">RetrievalConfig.<a href="./src/praisonai-agents/praisonaiagents/rag/retrieval_config.py">from_dict</a>(data: Dict[str, Any]) -> 'RetrievalConfig'</code>
* <code title="class RetrievalConfig">RetrievalConfig.<a href="./src/praisonai-agents/praisonaiagents/rag/retrieval_config.py">get_strategy</a>(corpus_stats = None)</code>
* <code title="class RetrievalConfig">RetrievalConfig.<a href="./src/praisonai-agents/praisonaiagents/rag/retrieval_config.py">get_token_budget</a>(model_name: Optional[str] = None)</code>
* <code title="class RetrievalConfig">RetrievalConfig.<a href="./src/praisonai-agents/praisonaiagents/rag/retrieval_config.py">should_retrieve</a>(query: str, force: bool = False, skip: bool = False) -> bool</code>
* <code title="class RetrievalConfig">RetrievalConfig.<a href="./src/praisonai-agents/praisonaiagents/rag/retrieval_config.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class RetrievalConfig">RetrievalConfig.<a href="./src/praisonai-agents/praisonaiagents/rag/retrieval_config.py">to_knowledge_config</a>() -> Dict[str, Any]</code>
* <code title="class RetrievalConfig">RetrievalConfig.<a href="./src/praisonai-agents/praisonaiagents/rag/retrieval_config.py">to_rag_config</a>() -> Dict[str, Any]</code>

# Handoff

Types:
```python
from praisonaiagents import Handoff, RECOMMENDED_PROMPT_PREFIX, handoff, handoff_filters, prompt_with_handoff_instructions
```

Methods:

* <code title="class Handoff">Handoff.<a href="./src/praisonai-agents/praisonaiagents/agent/handoff.py">default_tool_description</a>() -> str</code>
* <code title="class Handoff">Handoff.<a href="./src/praisonai-agents/praisonaiagents/agent/handoff.py">default_tool_name</a>() -> str</code>
* <code title="class Handoff">Handoff.<a href="./src/praisonai-agents/praisonaiagents/agent/handoff.py">execute_async</a>(source_agent: 'Agent', prompt: str, context: Optional[Dict[str, Any]] = None) -> HandoffResult</code>
* <code title="class Handoff">Handoff.<a href="./src/praisonai-agents/praisonaiagents/agent/handoff.py">execute_programmatic</a>(source_agent: 'Agent', prompt: str, context: Optional[Dict[str, Any]] = None) -> HandoffResult</code>
* <code title="class Handoff">Handoff.<a href="./src/praisonai-agents/praisonaiagents/agent/handoff.py">to_tool_function</a>(source_agent: 'Agent') -> Callable</code>
* <code title="class Handoff">Handoff.<a href="./src/praisonai-agents/praisonaiagents/agent/handoff.py">tool_description</a>() -> str</code>
* <code title="class Handoff">Handoff.<a href="./src/praisonai-agents/praisonaiagents/agent/handoff.py">tool_name</a>() -> str</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/agent/handoff.py">handoff</a>(agent: 'Agent', tool_name_override: Optional[str] = None, tool_description_override: Optional[str] = None, on_handoff: Optional[Callable] = None, input_type: Optional[type] = None, input_filter: Optional[Callable[[HandoffInputData], HandoffInputData]] = None, config: Optional[HandoffConfig] = None, context_policy: Optional[str] = None, timeout_seconds: Optional[float] = None, max_concurrent: Optional[int] = None, detect_cycles: Optional[bool] = None, max_depth: Optional[int] = None) -> Handoff</code>
* <code title="class handoff_filters">handoff_filters.<a href="./src/praisonai-agents/praisonaiagents/agent/handoff.py">keep_last_n_messages</a>(n: int) -> Callable[[HandoffInputData], HandoffInputData]</code>
* <code title="class handoff_filters">handoff_filters.<a href="./src/praisonai-agents/praisonaiagents/agent/handoff.py">remove_all_tools</a>(data: HandoffInputData) -> HandoffInputData</code>
* <code title="class handoff_filters">handoff_filters.<a href="./src/praisonai-agents/praisonaiagents/agent/handoff.py">remove_system_messages</a>(data: HandoffInputData) -> HandoffInputData</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/agent/handoff.py">prompt_with_handoff_instructions</a>(base_prompt: str, agent: 'Agent') -> str</code>

# Guardrails

Types:
```python
from praisonaiagents import GuardrailAction, GuardrailConfig, LLMGuardrail
```

Methods:

* <code title="class GuardrailConfig">GuardrailConfig.<a href="./src/praisonai-agents/praisonaiagents/config/feature_configs.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class LLMGuardrail">LLMGuardrail.<a href="./src/praisonai-agents/praisonaiagents/guardrails/llm_guardrail.py">__call__</a>(task_output: TaskOutput) -> Tuple[bool, Union[str, TaskOutput]]</code>

# Planning

Types:
```python
from praisonaiagents import ApprovalCallback, Plan, PlanStep, PlanStorage, PlanningConfig, READ_ONLY_TOOLS, RESTRICTED_TOOLS, TodoItem, TodoList
```

Methods:

* <code title="class ApprovalCallback">ApprovalCallback.<a href="./src/praisonai-agents/praisonaiagents/planning/approval.py">__call__</a>(plan: 'Plan') -> bool</code>
* <code title="class ApprovalCallback">ApprovalCallback.<a href="./src/praisonai-agents/praisonaiagents/planning/approval.py">always_approve</a>(plan: 'Plan') -> bool</code>
* <code title="class ApprovalCallback">ApprovalCallback.<a href="./src/praisonai-agents/praisonaiagents/planning/approval.py">always_reject</a>(plan: 'Plan') -> bool</code>
* <code title="class ApprovalCallback">ApprovalCallback.<a href="./src/praisonai-agents/praisonaiagents/planning/approval.py">approve_if_no_dangerous_tools</a>(plan: 'Plan') -> bool</code>
* <code title="class ApprovalCallback">ApprovalCallback.<a href="./src/praisonai-agents/praisonaiagents/planning/approval.py">approve_if_small</a>(plan: 'Plan', max_steps: int = 5) -> bool</code>
* <code title="class ApprovalCallback">ApprovalCallback.<a href="./src/praisonai-agents/praisonaiagents/planning/approval.py">async_call</a>(plan: 'Plan') -> bool</code>
* <code title="class ApprovalCallback">ApprovalCallback.<a href="./src/praisonai-agents/praisonaiagents/planning/approval.py">console_approval</a>(plan: 'Plan') -> bool</code>
* <code title="class Plan">Plan.<a href="./src/praisonai-agents/praisonaiagents/planning/plan.py">add_step</a>(step: PlanStep) -> None</code>
* <code title="class Plan">Plan.<a href="./src/praisonai-agents/praisonaiagents/planning/plan.py">approve</a>() -> None</code>
* <code title="class Plan">Plan.<a href="./src/praisonai-agents/praisonaiagents/planning/plan.py">cancel</a>() -> None</code>
* <code title="class Plan">Plan.<a href="./src/praisonai-agents/praisonaiagents/planning/plan.py">complete</a>() -> None</code>
* <code title="class Plan">Plan.<a href="./src/praisonai-agents/praisonaiagents/planning/plan.py">completed_step_ids</a>() -> List[str]</code>
* <code title="class Plan">Plan.<a href="./src/praisonai-agents/praisonaiagents/planning/plan.py">from_dict</a>(data: Dict[str, Any]) -> 'Plan'</code>
* <code title="class Plan">Plan.<a href="./src/praisonai-agents/praisonaiagents/planning/plan.py">from_markdown</a>(markdown: str) -> 'Plan'</code>
* <code title="class Plan">Plan.<a href="./src/praisonai-agents/praisonaiagents/planning/plan.py">get_next_steps</a>() -> List[PlanStep]</code>
* <code title="class Plan">Plan.<a href="./src/praisonai-agents/praisonaiagents/planning/plan.py">get_step</a>(step_id: str) -> Optional[PlanStep]</code>
* <code title="class Plan">Plan.<a href="./src/praisonai-agents/praisonaiagents/planning/plan.py">is_complete</a>() -> bool</code>
* <code title="class Plan">Plan.<a href="./src/praisonai-agents/praisonaiagents/planning/plan.py">progress</a>() -> float</code>
* <code title="class Plan">Plan.<a href="./src/praisonai-agents/praisonaiagents/planning/plan.py">remove_step</a>(step_id: str) -> bool</code>
* <code title="class Plan">Plan.<a href="./src/praisonai-agents/praisonaiagents/planning/plan.py">start_execution</a>() -> None</code>
* <code title="class Plan">Plan.<a href="./src/praisonai-agents/praisonaiagents/planning/plan.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class Plan">Plan.<a href="./src/praisonai-agents/praisonaiagents/planning/plan.py">to_markdown</a>() -> str</code>
* <code title="class Plan">Plan.<a href="./src/praisonai-agents/praisonaiagents/planning/plan.py">update_step_status</a>(step_id: str, status: str) -> bool</code>
* <code title="class PlanStep">PlanStep.<a href="./src/praisonai-agents/praisonaiagents/planning/plan.py">from_dict</a>(data: Dict[str, Any]) -> 'PlanStep'</code>
* <code title="class PlanStep">PlanStep.<a href="./src/praisonai-agents/praisonaiagents/planning/plan.py">is_ready</a>(completed_steps: List[str]) -> bool</code>
* <code title="class PlanStep">PlanStep.<a href="./src/praisonai-agents/praisonaiagents/planning/plan.py">mark_complete</a>() -> None</code>
* <code title="class PlanStep">PlanStep.<a href="./src/praisonai-agents/praisonaiagents/planning/plan.py">mark_in_progress</a>() -> None</code>
* <code title="class PlanStep">PlanStep.<a href="./src/praisonai-agents/praisonaiagents/planning/plan.py">mark_skipped</a>() -> None</code>
* <code title="class PlanStep">PlanStep.<a href="./src/praisonai-agents/praisonaiagents/planning/plan.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class PlanStorage">PlanStorage.<a href="./src/praisonai-agents/praisonaiagents/planning/storage.py">cleanup_old_plans</a>(keep_count: int = 10) -> int</code>
* <code title="class PlanStorage">PlanStorage.<a href="./src/praisonai-agents/praisonaiagents/planning/storage.py">delete_plan</a>(plan_id: str) -> bool</code>
* <code title="class PlanStorage">PlanStorage.<a href="./src/praisonai-agents/praisonaiagents/planning/storage.py">delete_todo</a>(name: str) -> bool</code>
* <code title="class PlanStorage">PlanStorage.<a href="./src/praisonai-agents/praisonaiagents/planning/storage.py">get_latest_plan</a>() -> Optional[Plan]</code>
* <code title="class PlanStorage">PlanStorage.<a href="./src/praisonai-agents/praisonaiagents/planning/storage.py">list_plans</a>() -> List[Dict[str, Any]]</code>
* <code title="class PlanStorage">PlanStorage.<a href="./src/praisonai-agents/praisonaiagents/planning/storage.py">list_todos</a>() -> List[str]</code>
* <code title="class PlanStorage">PlanStorage.<a href="./src/praisonai-agents/praisonaiagents/planning/storage.py">load_plan</a>(plan_id: str) -> Optional[Plan]</code>
* <code title="class PlanStorage">PlanStorage.<a href="./src/praisonai-agents/praisonaiagents/planning/storage.py">load_plan_from_file</a>(path: str) -> Optional[Plan]</code>
* <code title="class PlanStorage">PlanStorage.<a href="./src/praisonai-agents/praisonaiagents/planning/storage.py">load_todo</a>(name: str = 'current') -> Optional[TodoList]</code>
* <code title="class PlanStorage">PlanStorage.<a href="./src/praisonai-agents/praisonaiagents/planning/storage.py">save_plan</a>(plan: Plan, filename: Optional[str] = None) -> str</code>
* <code title="class PlanStorage">PlanStorage.<a href="./src/praisonai-agents/praisonaiagents/planning/storage.py">save_todo</a>(todo: TodoList, name: str = 'current') -> str</code>
* <code title="class PlanningConfig">PlanningConfig.<a href="./src/praisonai-agents/praisonaiagents/config/feature_configs.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class TodoItem">TodoItem.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">complete</a>() -> None</code>
* <code title="class TodoItem">TodoItem.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">from_dict</a>(data: Dict[str, Any]) -> 'TodoItem'</code>
* <code title="class TodoItem">TodoItem.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">is_ready</a>(completed_ids: List[str]) -> bool</code>
* <code title="class TodoItem">TodoItem.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">reset</a>() -> None</code>
* <code title="class TodoItem">TodoItem.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">start</a>() -> None</code>
* <code title="class TodoItem">TodoItem.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class TodoList">TodoList.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">add</a>(item: Union[TodoItem, str]) -> TodoItem</code>
* <code title="class TodoList">TodoList.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">complete</a>(item_id: str) -> bool</code>
* <code title="class TodoList">TodoList.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">completed</a>() -> List[TodoItem]</code>
* <code title="class TodoList">TodoList.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">completed_ids</a>() -> List[str]</code>
* <code title="class TodoList">TodoList.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">from_dict</a>(data: Dict[str, Any]) -> 'TodoList'</code>
* <code title="class TodoList">TodoList.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">from_json</a>(json_str: str) -> 'TodoList'</code>
* <code title="class TodoList">TodoList.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">from_markdown</a>(markdown: str) -> 'TodoList'</code>
* <code title="class TodoList">TodoList.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">from_plan</a>(plan: 'Plan') -> 'TodoList'</code>
* <code title="class TodoList">TodoList.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">get</a>(item_id: str) -> Optional[TodoItem]</code>
* <code title="class TodoList">TodoList.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">get_ready_items</a>() -> List[TodoItem]</code>
* <code title="class TodoList">TodoList.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">in_progress</a>() -> List[TodoItem]</code>
* <code title="class TodoList">TodoList.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">is_complete</a>() -> bool</code>
* <code title="class TodoList">TodoList.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">pending</a>() -> List[TodoItem]</code>
* <code title="class TodoList">TodoList.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">progress</a>() -> float</code>
* <code title="class TodoList">TodoList.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">remove</a>(item_id: str) -> bool</code>
* <code title="class TodoList">TodoList.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">start</a>(item_id: str) -> bool</code>
* <code title="class TodoList">TodoList.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">sync_with_plan</a>(plan: 'Plan') -> None</code>
* <code title="class TodoList">TodoList.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class TodoList">TodoList.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">to_json</a>() -> str</code>
* <code title="class TodoList">TodoList.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">to_markdown</a>() -> str</code>
* <code title="class TodoList">TodoList.<a href="./src/praisonai-agents/praisonaiagents/planning/todo.py">update_from_plan</a>(plan: 'Plan') -> None</code>

# Skills

Types:
```python
from praisonaiagents import SkillLoader, SkillManager, SkillMetadata, SkillProperties, SkillsConfig
```

Methods:

* <code title="class SkillLoader">SkillLoader.<a href="./src/praisonai-agents/praisonaiagents/skills/loader.py">activate</a>(skill: LoadedSkill) -> bool</code>
* <code title="class SkillLoader">SkillLoader.<a href="./src/praisonai-agents/praisonaiagents/skills/loader.py">load</a>(skill_path: str, activate: bool = False) -> Optional[LoadedSkill]</code>
* <code title="class SkillLoader">SkillLoader.<a href="./src/praisonai-agents/praisonaiagents/skills/loader.py">load_all_resources</a>(skill: LoadedSkill) -> None</code>
* <code title="class SkillLoader">SkillLoader.<a href="./src/praisonai-agents/praisonaiagents/skills/loader.py">load_assets</a>(skill: LoadedSkill) -> dict</code>
* <code title="class SkillLoader">SkillLoader.<a href="./src/praisonai-agents/praisonaiagents/skills/loader.py">load_metadata</a>(skill_path: str) -> Optional[LoadedSkill]</code>
* <code title="class SkillLoader">SkillLoader.<a href="./src/praisonai-agents/praisonaiagents/skills/loader.py">load_references</a>(skill: LoadedSkill) -> dict</code>
* <code title="class SkillLoader">SkillLoader.<a href="./src/praisonai-agents/praisonaiagents/skills/loader.py">load_scripts</a>(skill: LoadedSkill) -> dict</code>
* <code title="class SkillManager">SkillManager.<a href="./src/praisonai-agents/praisonaiagents/skills/manager.py">activate</a>(skill: LoadedSkill) -> bool</code>
* <code title="class SkillManager">SkillManager.<a href="./src/praisonai-agents/praisonaiagents/skills/manager.py">activate_by_name</a>(name: str) -> bool</code>
* <code title="class SkillManager">SkillManager.<a href="./src/praisonai-agents/praisonaiagents/skills/manager.py">add_skill</a>(skill_path: str) -> Optional[LoadedSkill]</code>
* <code title="class SkillManager">SkillManager.<a href="./src/praisonai-agents/praisonaiagents/skills/manager.py">clear</a>() -> None</code>
* <code title="class SkillManager">SkillManager.<a href="./src/praisonai-agents/praisonaiagents/skills/manager.py">discover</a>(skill_dirs: Optional[List[str]] = None, include_defaults: bool = True) -> int</code>
* <code title="class SkillManager">SkillManager.<a href="./src/praisonai-agents/praisonaiagents/skills/manager.py">get_available_skills</a>() -> List[SkillMetadata]</code>
* <code title="class SkillManager">SkillManager.<a href="./src/praisonai-agents/praisonaiagents/skills/manager.py">get_instructions</a>(name: str) -> Optional[str]</code>
* <code title="class SkillManager">SkillManager.<a href="./src/praisonai-agents/praisonaiagents/skills/manager.py">get_skill</a>(name: str) -> Optional[LoadedSkill]</code>
* <code title="class SkillManager">SkillManager.<a href="./src/praisonai-agents/praisonaiagents/skills/manager.py">load_resources</a>(name: str) -> bool</code>
* <code title="class SkillManager">SkillManager.<a href="./src/praisonai-agents/praisonaiagents/skills/manager.py">skill_names</a>() -> List[str]</code>
* <code title="class SkillManager">SkillManager.<a href="./src/praisonai-agents/praisonaiagents/skills/manager.py">skills</a>() -> List[LoadedSkill]</code>
* <code title="class SkillManager">SkillManager.<a href="./src/praisonai-agents/praisonaiagents/skills/manager.py">to_prompt</a>() -> str</code>
* <code title="class SkillMetadata">SkillMetadata.<a href="./src/praisonai-agents/praisonaiagents/skills/models.py">from_properties</a>(props: SkillProperties) -> 'SkillMetadata'</code>
* <code title="class SkillProperties">SkillProperties.<a href="./src/praisonai-agents/praisonaiagents/skills/models.py">to_dict</a>() -> dict</code>
* <code title="class SkillsConfig">SkillsConfig.<a href="./src/praisonai-agents/praisonaiagents/config/feature_configs.py">to_dict</a>() -> Dict[str, Any]</code>

# Session

Types:
```python
from praisonaiagents import Session
```

Methods:

* <code title="class Session">Session.<a href="./src/praisonai-agents/praisonaiagents/session/api.py">Agent</a>(name: str, role: str = 'Assistant', instructions: Optional[str] = None, tools: Optional[List[Any]] = None, memory: bool = True, knowledge: Optional[List[str]] = None, **kwargs) -> 'Agent'</code>
* <code title="class Session">Session.<a href="./src/praisonai-agents/praisonaiagents/session/api.py">add_knowledge</a>(source: str) -> None</code>
* <code title="class Session">Session.<a href="./src/praisonai-agents/praisonaiagents/session/api.py">add_memory</a>(text: str, memory_type: str = 'long', **metadata) -> None</code>
* <code title="class Session">Session.<a href="./src/praisonai-agents/praisonaiagents/session/api.py">chat</a>(message: str, **kwargs) -> str</code>
* <code title="class Session">Session.<a href="./src/praisonai-agents/praisonaiagents/session/api.py">clear_memory</a>(memory_type: str = 'all') -> None</code>
* <code title="class Session">Session.<a href="./src/praisonai-agents/praisonaiagents/session/api.py">create_agent</a>(*args, **kwargs) -> 'Agent'</code>
* <code title="class Session">Session.<a href="./src/praisonai-agents/praisonaiagents/session/api.py">get_context</a>(query: str, max_items: int = 3) -> str</code>
* <code title="class Session">Session.<a href="./src/praisonai-agents/praisonaiagents/session/api.py">get_state</a>(key: str, default: Any = None) -> Any</code>
* <code title="class Session">Session.<a href="./src/praisonai-agents/praisonaiagents/session/api.py">increment_state</a>(key: str, increment: int = 1, default: int = 0) -> None</code>
* <code title="class Session">Session.<a href="./src/praisonai-agents/praisonaiagents/session/api.py">knowledge</a>() -> 'Knowledge'</code>
* <code title="class Session">Session.<a href="./src/praisonai-agents/praisonaiagents/session/api.py">memory</a>() -> 'Memory'</code>
* <code title="class Session">Session.<a href="./src/praisonai-agents/praisonaiagents/session/api.py">restore_state</a>() -> Dict[str, Any]</code>
* <code title="class Session">Session.<a href="./src/praisonai-agents/praisonaiagents/session/api.py">save_state</a>(state_data: Dict[str, Any]) -> None</code>
* <code title="class Session">Session.<a href="./src/praisonai-agents/praisonaiagents/session/api.py">search_knowledge</a>(query: str, limit: int = 5) -> List[Dict[str, Any]]</code>
* <code title="class Session">Session.<a href="./src/praisonai-agents/praisonaiagents/session/api.py">search_memory</a>(query: str, memory_type: str = 'long', limit: int = 5) -> List[Dict[str, Any]]</code>
* <code title="class Session">Session.<a href="./src/praisonai-agents/praisonaiagents/session/api.py">send_message</a>(message: str, **kwargs) -> str</code>
* <code title="class Session">Session.<a href="./src/praisonai-agents/praisonaiagents/session/api.py">set_state</a>(key: str, value: Any) -> None</code>

# MCP

Types:
```python
from praisonaiagents import MCP
```

Methods:

* <code title="class MCP">MCP.<a href="./src/praisonai-agents/praisonaiagents/mcp/mcp.py">get_tools</a>() -> List[Callable]</code>
* <code title="class MCP">MCP.<a href="./src/praisonai-agents/praisonaiagents/mcp/mcp.py">shutdown</a>()</code>
* <code title="class MCP">MCP.<a href="./src/praisonai-agents/praisonaiagents/mcp/mcp.py">to_openai_tool</a>()</code>

# Telemetry

Types:
```python
from praisonaiagents import MinimalTelemetry, TelemetryCollector, cleanup_telemetry_resources, disable_performance_mode, disable_telemetry, enable_performance_mode, enable_telemetry, get_telemetry
```

Methods:

* <code title="class MinimalTelemetry">MinimalTelemetry.<a href="./src/praisonai-agents/praisonaiagents/telemetry/telemetry.py">flush</a>()</code>
* <code title="class MinimalTelemetry">MinimalTelemetry.<a href="./src/praisonai-agents/praisonaiagents/telemetry/telemetry.py">get_metrics</a>() -> Dict[str, Any]</code>
* <code title="class MinimalTelemetry">MinimalTelemetry.<a href="./src/praisonai-agents/praisonaiagents/telemetry/telemetry.py">shutdown</a>()</code>
* <code title="class MinimalTelemetry">MinimalTelemetry.<a href="./src/praisonai-agents/praisonaiagents/telemetry/telemetry.py">track_agent_execution</a>(agent_name: str = None, success: bool = True, async_mode: bool = False)</code>
* <code title="class MinimalTelemetry">MinimalTelemetry.<a href="./src/praisonai-agents/praisonaiagents/telemetry/telemetry.py">track_error</a>(error_type: str = None)</code>
* <code title="class MinimalTelemetry">MinimalTelemetry.<a href="./src/praisonai-agents/praisonaiagents/telemetry/telemetry.py">track_feature_usage</a>(feature_name: str)</code>
* <code title="class MinimalTelemetry">MinimalTelemetry.<a href="./src/praisonai-agents/praisonaiagents/telemetry/telemetry.py">track_task_completion</a>(task_name: str = None, success: bool = True)</code>
* <code title="class MinimalTelemetry">MinimalTelemetry.<a href="./src/praisonai-agents/praisonaiagents/telemetry/telemetry.py">track_tool_usage</a>(tool_name: str, success: bool = True, execution_time: float = None)</code>
* <code title="class TelemetryCollector">TelemetryCollector.<a href="./src/praisonai-agents/praisonaiagents/telemetry/telemetry.py">get_metrics</a>() -> Dict[str, Any]</code>
* <code title="class TelemetryCollector">TelemetryCollector.<a href="./src/praisonai-agents/praisonaiagents/telemetry/telemetry.py">record_cost</a>(cost: float, model: str = None)</code>
* <code title="class TelemetryCollector">TelemetryCollector.<a href="./src/praisonai-agents/praisonaiagents/telemetry/telemetry.py">record_tokens</a>(prompt_tokens: int, completion_tokens: int, model: str = None)</code>
* <code title="class TelemetryCollector">TelemetryCollector.<a href="./src/praisonai-agents/praisonaiagents/telemetry/telemetry.py">start</a>()</code>
* <code title="class TelemetryCollector">TelemetryCollector.<a href="./src/praisonai-agents/praisonaiagents/telemetry/telemetry.py">stop</a>()</code>
* <code title="class TelemetryCollector">TelemetryCollector.<a href="./src/praisonai-agents/praisonaiagents/telemetry/telemetry.py">trace_agent_execution</a>(agent_name: str, **attributes)</code>
* <code title="class TelemetryCollector">TelemetryCollector.<a href="./src/praisonai-agents/praisonaiagents/telemetry/telemetry.py">trace_llm_call</a>(model: str = None, **attributes)</code>
* <code title="class TelemetryCollector">TelemetryCollector.<a href="./src/praisonai-agents/praisonaiagents/telemetry/telemetry.py">trace_task_execution</a>(task_name: str, agent_name: str = None, **attributes)</code>
* <code title="class TelemetryCollector">TelemetryCollector.<a href="./src/praisonai-agents/praisonaiagents/telemetry/telemetry.py">trace_tool_call</a>(tool_name: str, **attributes)</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/telemetry/__init__.py">cleanup_telemetry_resources</a>()</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/telemetry/__init__.py">disable_performance_mode</a>()</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/telemetry/__init__.py">disable_telemetry</a>()</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/telemetry/__init__.py">enable_performance_mode</a>()</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/telemetry/__init__.py">enable_telemetry</a>()</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/telemetry/__init__.py">get_telemetry</a>() -> 'MinimalTelemetry'</code>

# Observability

Types:
```python
from praisonaiagents import FlowDisplay, obs, track_workflow
```

Methods:

* <code title="class FlowDisplay">FlowDisplay.<a href="./src/praisonai-agents/praisonaiagents/flow_display.py">display</a>()</code>
* <code title="class FlowDisplay">FlowDisplay.<a href="./src/praisonai-agents/praisonaiagents/flow_display.py">start</a>()</code>
* <code title="class FlowDisplay">FlowDisplay.<a href="./src/praisonai-agents/praisonaiagents/flow_display.py">stop</a>()</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/flow_display.py">track_workflow</a>()</code>

# Context

Types:
```python
from praisonaiagents import ContextManager
```

Methods:

* <code title="class ContextManager">ContextManager.<a href="./src/praisonai-agents/praisonaiagents/context/manager.py">capture_llm_boundary</a>(messages: List[Dict[str, Any]], tools: List[Dict[str, Any]]) -> SnapshotHookData</code>
* <code title="class ContextManager">ContextManager.<a href="./src/praisonai-agents/praisonaiagents/context/manager.py">emergency_truncate</a>(messages: List[Dict[str, Any]], target_tokens: int) -> List[Dict[str, Any]]</code>
* <code title="class ContextManager">ContextManager.<a href="./src/praisonai-agents/praisonaiagents/context/manager.py">estimate_tokens</a>(text: str, validate: bool = False) -> Tuple[int, Optional[EstimationMetrics]]</code>
* <code title="class ContextManager">ContextManager.<a href="./src/praisonai-agents/praisonaiagents/context/manager.py">get_history</a>() -> List[Dict[str, Any]]</code>
* <code title="class ContextManager">ContextManager.<a href="./src/praisonai-agents/praisonaiagents/context/manager.py">get_last_snapshot_hook</a>() -> Optional[SnapshotHookData]</code>
* <code title="class ContextManager">ContextManager.<a href="./src/praisonai-agents/praisonaiagents/context/manager.py">get_resolved_config</a>() -> Dict[str, Any]</code>
* <code title="class ContextManager">ContextManager.<a href="./src/praisonai-agents/praisonaiagents/context/manager.py">get_stats</a>() -> Dict[str, Any]</code>
* <code title="class ContextManager">ContextManager.<a href="./src/praisonai-agents/praisonaiagents/context/manager.py">get_tool_budget</a>(tool_name: str) -> int</code>
* <code title="class ContextManager">ContextManager.<a href="./src/praisonai-agents/praisonaiagents/context/manager.py">process</a>(messages: List[Dict[str, Any]], system_prompt: str = '', tools: Optional[List[Dict[str, Any]]] = None, trigger: Literal['turn', 'tool_call', 'manual', 'overflow'] = 'turn') -> Dict[str, Any]</code>
* <code title="class ContextManager">ContextManager.<a href="./src/praisonai-agents/praisonaiagents/context/manager.py">register_snapshot_callback</a>(callback: Callable[[SnapshotHookData], None]) -> None</code>
* <code title="class ContextManager">ContextManager.<a href="./src/praisonai-agents/praisonaiagents/context/manager.py">reset</a>() -> None</code>
* <code title="class ContextManager">ContextManager.<a href="./src/praisonai-agents/praisonaiagents/context/manager.py">set_tool_budget</a>(tool_name: str, max_tokens: int, protected: bool = False) -> None</code>
* <code title="class ContextManager">ContextManager.<a href="./src/praisonai-agents/praisonaiagents/context/manager.py">truncate_tool_output</a>(tool_name: str, output: str) -> str</code>

# Config

Types:
```python
from praisonaiagents import AutonomyLevel, CachingConfig, ExecutionConfig, ExecutionPreset, GuardrailConfig, HooksConfig, KnowledgeConfig, MemoryConfig, MultiAgentExecutionConfig, MultiAgentHooksConfig, MultiAgentMemoryConfig, MultiAgentOutputConfig, MultiAgentPlanningConfig, OutputConfig, OutputPreset, PlanningConfig, ReflectionConfig, SkillsConfig, TemplateConfig, WebConfig, WebSearchProvider
```

Methods:

* <code title="class CachingConfig">CachingConfig.<a href="./src/praisonai-agents/praisonaiagents/config/feature_configs.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class ExecutionConfig">ExecutionConfig.<a href="./src/praisonai-agents/praisonaiagents/config/feature_configs.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class GuardrailConfig">GuardrailConfig.<a href="./src/praisonai-agents/praisonaiagents/config/feature_configs.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class HooksConfig">HooksConfig.<a href="./src/praisonai-agents/praisonaiagents/config/feature_configs.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class KnowledgeConfig">KnowledgeConfig.<a href="./src/praisonai-agents/praisonaiagents/config/feature_configs.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class MemoryConfig">MemoryConfig.<a href="./src/praisonai-agents/praisonaiagents/config/feature_configs.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class MultiAgentExecutionConfig">MultiAgentExecutionConfig.<a href="./src/praisonai-agents/praisonaiagents/config/feature_configs.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class MultiAgentHooksConfig">MultiAgentHooksConfig.<a href="./src/praisonai-agents/praisonaiagents/config/feature_configs.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class MultiAgentMemoryConfig">MultiAgentMemoryConfig.<a href="./src/praisonai-agents/praisonaiagents/config/feature_configs.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class MultiAgentOutputConfig">MultiAgentOutputConfig.<a href="./src/praisonai-agents/praisonaiagents/config/feature_configs.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class MultiAgentPlanningConfig">MultiAgentPlanningConfig.<a href="./src/praisonai-agents/praisonaiagents/config/feature_configs.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class OutputConfig">OutputConfig.<a href="./src/praisonai-agents/praisonaiagents/config/feature_configs.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class PlanningConfig">PlanningConfig.<a href="./src/praisonai-agents/praisonaiagents/config/feature_configs.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class ReflectionConfig">ReflectionConfig.<a href="./src/praisonai-agents/praisonaiagents/config/feature_configs.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class SkillsConfig">SkillsConfig.<a href="./src/praisonai-agents/praisonaiagents/config/feature_configs.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class TemplateConfig">TemplateConfig.<a href="./src/praisonai-agents/praisonaiagents/config/feature_configs.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class WebConfig">WebConfig.<a href="./src/praisonai-agents/praisonaiagents/config/feature_configs.py">to_dict</a>() -> Dict[str, Any]</code>

# Display

Types:
```python
from praisonaiagents import async_display_callbacks, clean_triple_backticks, display_error, display_generating, display_instruction, display_interaction, display_self_reflection, display_tool_call, error_logs, register_display_callback, sync_display_callbacks
```

Methods:

* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/main.py">clean_triple_backticks</a>(text: str) -> str</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/main.py">display_error</a>(message: str, console = None)</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/main.py">display_generating</a>(content: str = '', start_time: Optional[float] = None)</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/main.py">display_instruction</a>(message: str, console = None, agent_name: str = None, agent_role: str = None, agent_tools: List[str] = None)</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/main.py">display_interaction</a>(message, response, markdown = True, generation_time = None, console = None, agent_name = None, agent_role = None, agent_tools = None, task_name = None, task_description = None, task_id = None, metrics = None)</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/main.py">display_self_reflection</a>(message: str, console = None)</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/main.py">display_tool_call</a>(message: str, console = None, tool_name: str = None, tool_input: dict = None, tool_output: str = None, elapsed_time: float = None, success: bool = True)</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/main.py">register_display_callback</a>(display_type: str, callback_fn, is_async: bool = False)</code>

# Utilities

Types:
```python
from praisonaiagents import ArrayMode, is_policy_string, parse_policy_string, resolve, resolve_autonomy, resolve_caching, resolve_context, resolve_execution, resolve_guardrail_policies, resolve_guardrails, resolve_hooks, resolve_knowledge, resolve_memory, resolve_output, resolve_planning, resolve_reflection, resolve_routing, resolve_skills, resolve_web
```

Methods:

* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/config/parse_utils.py">is_policy_string</a>(value: str) -> bool</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/config/parse_utils.py">parse_policy_string</a>(value: str) -> tuple</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/config/param_resolver.py">resolve</a>(value: Any, param_name: str, config_class: Optional[Type] = None, presets: Optional[Dict[str, Any]] = None, default: Any = None, instance_check: Optional[Callable[[Any], bool]] = None, url_schemes: Optional[Dict[str, str]] = None, array_mode: Optional[str] = None, string_mode: Optional[str] = None) -> Any</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/config/param_resolver.py">resolve_autonomy</a>(value: Any, config_class: Type) -> Any</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/config/param_resolver.py">resolve_caching</a>(value: Any, config_class: Type) -> Any</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/config/param_resolver.py">resolve_context</a>(value: Any, config_class: Type) -> Any</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/config/param_resolver.py">resolve_execution</a>(value: Any, config_class: Type) -> Any</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/config/param_resolver.py">resolve_guardrail_policies</a>(policies: list, config_class: Type) -> Any</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/config/param_resolver.py">resolve_guardrails</a>(value: Any, config_class: Type) -> Any</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/config/param_resolver.py">resolve_hooks</a>(value: Any, config_class: Optional[Type] = None) -> Any</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/config/param_resolver.py">resolve_knowledge</a>(value: Any, config_class: Type) -> Any</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/config/param_resolver.py">resolve_memory</a>(value: Any, config_class: Type) -> Any</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/config/param_resolver.py">resolve_output</a>(value: Any, config_class: Type) -> Any</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/config/param_resolver.py">resolve_planning</a>(value: Any, config_class: Type) -> Any</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/config/param_resolver.py">resolve_reflection</a>(value: Any, config_class: Type) -> Any</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/config/param_resolver.py">resolve_routing</a>(value: Any, config_class: Type) -> Any</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/config/param_resolver.py">resolve_skills</a>(value: Any, config_class: Type) -> Any</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/config/param_resolver.py">resolve_web</a>(value: Any, config_class: Type) -> Any</code>

# Other

Types:
```python
from praisonaiagents import AgentAppConfig, AgentAppProtocol, AgentFlow, AgentManager, AgentOSConfig, AgentOSProtocol, AgentTeam, EmbeddingResult, aembedding, aembeddings, embedding, embeddings, get_dimensions
```

Methods:

* <code title="class AgentFlow">AgentFlow.<a href="./src/praisonai-agents/praisonaiagents/workflows/workflows.py">arun</a>(input: str = '', llm: Optional[str] = None, verbose: bool = False) -> Dict[str, Any]</code>
* <code title="class AgentFlow">AgentFlow.<a href="./src/praisonai-agents/praisonaiagents/workflows/workflows.py">astart</a>(input: str = '', llm: Optional[str] = None, verbose: bool = False) -> Dict[str, Any]</code>
* <code title="class AgentFlow">AgentFlow.<a href="./src/praisonai-agents/praisonaiagents/workflows/workflows.py">from_template</a>(uri: str, config: Optional[Dict[str, Any]] = None, offline: bool = False, **kwargs) -> 'Workflow'</code>
* <code title="class AgentFlow">AgentFlow.<a href="./src/praisonai-agents/praisonaiagents/workflows/workflows.py">get_history</a>() -> List[Dict[str, Any]]</code>
* <code title="class AgentFlow">AgentFlow.<a href="./src/praisonai-agents/praisonaiagents/workflows/workflows.py">memory_config</a>() -> Optional[Dict[str, Any]]</code>
* <code title="class AgentFlow">AgentFlow.<a href="./src/praisonai-agents/praisonaiagents/workflows/workflows.py">on_step_complete</a>() -> Optional[Callable]</code>
* <code title="class AgentFlow">AgentFlow.<a href="./src/praisonai-agents/praisonaiagents/workflows/workflows.py">on_step_error</a>() -> Optional[Callable]</code>
* <code title="class AgentFlow">AgentFlow.<a href="./src/praisonai-agents/praisonaiagents/workflows/workflows.py">on_step_start</a>() -> Optional[Callable]</code>
* <code title="class AgentFlow">AgentFlow.<a href="./src/praisonai-agents/praisonaiagents/workflows/workflows.py">on_workflow_complete</a>() -> Optional[Callable]</code>
* <code title="class AgentFlow">AgentFlow.<a href="./src/praisonai-agents/praisonaiagents/workflows/workflows.py">on_workflow_start</a>() -> Optional[Callable]</code>
* <code title="class AgentFlow">AgentFlow.<a href="./src/praisonai-agents/praisonaiagents/workflows/workflows.py">planning_llm</a>() -> Optional[str]</code>
* <code title="class AgentFlow">AgentFlow.<a href="./src/praisonai-agents/praisonaiagents/workflows/workflows.py">reasoning</a>() -> bool</code>
* <code title="class AgentFlow">AgentFlow.<a href="./src/praisonai-agents/praisonaiagents/workflows/workflows.py">run</a>(input: str = '', llm: Optional[str] = None, verbose: bool = False, stream: bool = None) -> Dict[str, Any]</code>
* <code title="class AgentFlow">AgentFlow.<a href="./src/praisonai-agents/praisonaiagents/workflows/workflows.py">start</a>(input: str = '', **kwargs) -> Dict[str, Any]</code>
* <code title="class AgentFlow">AgentFlow.<a href="./src/praisonai-agents/praisonaiagents/workflows/workflows.py">stream</a>() -> bool</code>
* <code title="class AgentFlow">AgentFlow.<a href="./src/praisonai-agents/praisonaiagents/workflows/workflows.py">to_dict</a>() -> Dict[str, Any]</code>
* <code title="class AgentFlow">AgentFlow.<a href="./src/praisonai-agents/praisonaiagents/workflows/workflows.py">verbose</a>() -> bool</code>
* <code title="class AgentFlow">AgentFlow.<a href="./src/praisonai-agents/praisonaiagents/workflows/workflows.py">verbose</a>(value: bool)</code>
* <code title="class AgentOSProtocol">AgentOSProtocol.<a href="./src/praisonai-agents/praisonaiagents/app/protocols.py">get_app</a>() -> Any</code>
* <code title="class AgentOSProtocol">AgentOSProtocol.<a href="./src/praisonai-agents/praisonaiagents/app/protocols.py">serve</a>(host: Optional[str] = None, port: Optional[int] = None, reload: bool = False, **kwargs: Any) -> None</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">add_task</a>(task)</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">aexecute_task</a>(task_id)</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">append_to_state</a>(key: str, value: Any, max_length: Optional[int] = None) -> List[Any]</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">arun_all_tasks</a>()</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">arun_task</a>(task_id)</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">astart</a>(content = None, return_dict = False, **kwargs)</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">clean_json_output</a>(output: str) -> str</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">clear_state</a>() -> None</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">context_manager</a>()</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">current_plan</a>()</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">default_completion_checker</a>(task, agent_output)</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">delete_state</a>(key: str) -> bool</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">display_token_usage</a>()</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">execute_task</a>(task_id)</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">get_agent_details</a>(agent_name)</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">get_all_state</a>() -> Dict[str, Any]</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">get_all_tasks_status</a>()</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">get_detailed_token_report</a>() -> Dict[str, Any]</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">get_plan_markdown</a>() -> str</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">get_state</a>(key: str, default: Any = None) -> Any</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">get_task_details</a>(task_id)</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">get_task_result</a>(task_id)</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">get_task_status</a>(task_id)</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">get_todo_markdown</a>() -> str</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">get_token_usage_summary</a>() -> Dict[str, Any]</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">has_state</a>(key: str) -> bool</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">increment_state</a>(key: str, amount: float = 1, default: float = 0) -> float</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">launch</a>(path: str = '/agents', port: int = 8000, host: str = '0.0.0.0', debug: bool = False, protocol: str = 'http')</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">restore_session_state</a>(session_id: str) -> bool</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">run</a>(content = None, return_dict = False, **kwargs)</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">run_all_tasks</a>()</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">run_task</a>(task_id)</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">save_output_to_file</a>(task, task_output)</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">save_session_state</a>(session_id: str, include_memory: bool = True) -> None</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">set_state</a>(key: str, value: Any) -> None</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">start</a>(content = None, return_dict = False, output = None, **kwargs)</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">todo_list</a>()</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">update_plan_step_status</a>(step_id: str, status: str) -> bool</code>
* <code title="class AgentTeam">AgentTeam.<a href="./src/praisonai-agents/praisonaiagents/agents/agents.py">update_state</a>(updates: Dict) -> None</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/embedding/embed.py">aembedding</a>(input: Union[str, List[str]], model: str = 'text-embedding-3-small', dimensions: Optional[int] = None, encoding_format: str = 'float', timeout: float = 600.0, api_key: Optional[str] = None, api_base: Optional[str] = None, metadata: Optional[Dict[str, Any]] = None, **kwargs) -> EmbeddingResult</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/embedding/embed.py">aembeddings</a>(input: Union[str, List[str]], model: str = 'text-embedding-3-small', dimensions: Optional[int] = None, encoding_format: str = 'float', timeout: float = 600.0, api_key: Optional[str] = None, api_base: Optional[str] = None, metadata: Optional[Dict[str, Any]] = None, **kwargs) -> EmbeddingResult</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/embedding/embed.py">embedding</a>(input: Union[str, List[str]], model: str = 'text-embedding-3-small', dimensions: Optional[int] = None, encoding_format: str = 'float', timeout: float = 600.0, api_key: Optional[str] = None, api_base: Optional[str] = None, metadata: Optional[Dict[str, Any]] = None, **kwargs) -> EmbeddingResult</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/embedding/embed.py">embeddings</a>(input: Union[str, List[str]], model: str = 'text-embedding-3-small', dimensions: Optional[int] = None, encoding_format: str = 'float', timeout: float = 600.0, api_key: Optional[str] = None, api_base: Optional[str] = None, metadata: Optional[Dict[str, Any]] = None, **kwargs) -> EmbeddingResult</code>
* <code title="function">praisonaiagents.<a href="./src/praisonai-agents/praisonaiagents/embedding/dimensions.py">get_dimensions</a>(model_name: str) -> int</code>

# Wrapper (praisonai)

Types:
```python
from praisonai import AgentApp, AgentOS, CloudProvider, Deploy, DeployConfig, DeployType, PraisonAI, __version__
```

# CLI

Methods:

* <code title="cli">praisonai <a href="./src/praisonai/praisonai/cli/main.py">--help</a></code>
* <code title="cli">praisonai acp acp-main <a href="./src/praisonai/praisonai/cli/commands/acp.py">--help</a></code>
* <code title="cli">praisonai agents create <a href="./src/praisonai/praisonai/cli/commands/agents.py">--help</a></code>
* <code title="cli">praisonai agents info <a href="./src/praisonai/praisonai/cli/commands/agents.py">--help</a></code>
* <code title="cli">praisonai agents list <a href="./src/praisonai/praisonai/cli/commands/agents.py">--help</a></code>
* <code title="cli">praisonai app app <a href="./src/praisonai/praisonai/cli/commands/app.py">--help</a></code>
* <code title="cli">praisonai audit agent-centric <a href="./src/praisonai/praisonai/cli/commands/audit.py">--help</a></code>
* <code title="cli">praisonai batch batch-run <a href="./src/praisonai/praisonai/cli/commands/batch.py">--help</a></code>
* <code title="cli">praisonai batch list <a href="./src/praisonai/praisonai/cli/commands/batch.py">--help</a></code>
* <code title="cli">praisonai batch report <a href="./src/praisonai/praisonai/cli/commands/batch.py">--help</a></code>
* <code title="cli">praisonai batch stats <a href="./src/praisonai/praisonai/cli/commands/batch.py">--help</a></code>
* <code title="cli">praisonai benchmark agent <a href="./src/praisonai/praisonai/cli/commands/benchmark.py">--help</a></code>
* <code title="cli">praisonai benchmark benchmark-callback <a href="./src/praisonai/praisonai/cli/commands/benchmark.py">--help</a></code>
* <code title="cli">praisonai benchmark cli <a href="./src/praisonai/praisonai/cli/commands/benchmark.py">--help</a></code>
* <code title="cli">praisonai benchmark compare <a href="./src/praisonai/praisonai/cli/commands/benchmark.py">--help</a></code>
* <code title="cli">praisonai benchmark litellm <a href="./src/praisonai/praisonai/cli/commands/benchmark.py">--help</a></code>
* <code title="cli">praisonai benchmark profile <a href="./src/praisonai/praisonai/cli/commands/benchmark.py">--help</a></code>
* <code title="cli">praisonai benchmark sdk <a href="./src/praisonai/praisonai/cli/commands/benchmark.py">--help</a></code>
* <code title="cli">praisonai benchmark workflow <a href="./src/praisonai/praisonai/cli/commands/benchmark.py">--help</a></code>
* <code title="cli">praisonai bot bot-callback <a href="./src/praisonai/praisonai/cli/commands/bot.py">--help</a></code>
* <code title="cli">praisonai bot discord <a href="./src/praisonai/praisonai/cli/commands/bot.py">--help</a></code>
* <code title="cli">praisonai bot slack <a href="./src/praisonai/praisonai/cli/commands/bot.py">--help</a></code>
* <code title="cli">praisonai bot telegram <a href="./src/praisonai/praisonai/cli/commands/bot.py">--help</a></code>
* <code title="cli">praisonai browser browser-callback <a href="./src/praisonai/praisonai/cli/commands/browser.py">--help</a></code>
* <code title="cli">praisonai browser click <a href="./src/praisonai/praisonai/cli/commands/browser.py">--help</a></code>
* <code title="cli">praisonai browser navigate <a href="./src/praisonai/praisonai/cli/commands/browser.py">--help</a></code>
* <code title="cli">praisonai browser open <a href="./src/praisonai/praisonai/cli/commands/browser.py">--help</a></code>
* <code title="cli">praisonai browser profiles <a href="./src/praisonai/praisonai/cli/commands/browser.py">--help</a></code>
* <code title="cli">praisonai browser screenshot <a href="./src/praisonai/praisonai/cli/commands/browser.py">--help</a></code>
* <code title="cli">praisonai browser snapshot <a href="./src/praisonai/praisonai/cli/commands/browser.py">--help</a></code>
* <code title="cli">praisonai browser status <a href="./src/praisonai/praisonai/cli/commands/browser.py">--help</a></code>
* <code title="cli">praisonai browser type <a href="./src/praisonai/praisonai/cli/commands/browser.py">--help</a></code>
* <code title="cli">praisonai call call-main <a href="./src/praisonai/praisonai/cli/commands/call.py">--help</a></code>
* <code title="cli">praisonai chat <a href="./src/praisonai/praisonai/cli/main.py">--help</a></code>
* <code title="cli">praisonai chat chat-main <a href="./src/praisonai/praisonai/cli/commands/chat.py">--help</a></code>
* <code title="cli">praisonai code <a href="./src/praisonai/praisonai/cli/main.py">--help</a></code>
* <code title="cli">praisonai code code-main <a href="./src/praisonai/praisonai/cli/commands/code.py">--help</a></code>
* <code title="cli">praisonai commit commit-main <a href="./src/praisonai/praisonai/cli/commands/commit.py">--help</a></code>
* <code title="cli">praisonai completion bash <a href="./src/praisonai/praisonai/cli/commands/completion.py">--help</a></code>
* <code title="cli">praisonai completion completion-callback <a href="./src/praisonai/praisonai/cli/commands/completion.py">--help</a></code>
* <code title="cli">praisonai completion fish <a href="./src/praisonai/praisonai/cli/commands/completion.py">--help</a></code>
* <code title="cli">praisonai completion zsh <a href="./src/praisonai/praisonai/cli/commands/completion.py">--help</a></code>
* <code title="cli">praisonai config get <a href="./src/praisonai/praisonai/cli/commands/config.py">--help</a></code>
* <code title="cli">praisonai config list <a href="./src/praisonai/praisonai/cli/commands/config.py">--help</a></code>
* <code title="cli">praisonai config path <a href="./src/praisonai/praisonai/cli/commands/config.py">--help</a></code>
* <code title="cli">praisonai config reset <a href="./src/praisonai/praisonai/cli/commands/config.py">--help</a></code>
* <code title="cli">praisonai config set <a href="./src/praisonai/praisonai/cli/commands/config.py">--help</a></code>
* <code title="cli">praisonai context add <a href="./src/praisonai/praisonai/cli/commands/context.py">--help</a></code>
* <code title="cli">praisonai context clear <a href="./src/praisonai/praisonai/cli/commands/context.py">--help</a></code>
* <code title="cli">praisonai context compact <a href="./src/praisonai/praisonai/cli/commands/context.py">--help</a></code>
* <code title="cli">praisonai context export <a href="./src/praisonai/praisonai/cli/commands/context.py">--help</a></code>
* <code title="cli">praisonai context grep <a href="./src/praisonai/praisonai/cli/commands/context.py">--help</a></code>
* <code title="cli">praisonai context list <a href="./src/praisonai/praisonai/cli/commands/context.py">--help</a></code>
* <code title="cli">praisonai context show <a href="./src/praisonai/praisonai/cli/commands/context.py">--help</a></code>
* <code title="cli">praisonai context stats <a href="./src/praisonai/praisonai/cli/commands/context.py">--help</a></code>
* <code title="cli">praisonai context tail <a href="./src/praisonai/praisonai/cli/commands/context.py">--help</a></code>
* <code title="cli">praisonai debug acp <a href="./src/praisonai/praisonai/cli/commands/debug.py">--help</a></code>
* <code title="cli">praisonai debug debug-callback <a href="./src/praisonai/praisonai/cli/commands/debug.py">--help</a></code>
* <code title="cli">praisonai debug interactive <a href="./src/praisonai/praisonai/cli/commands/debug.py">--help</a></code>
* <code title="cli">praisonai debug lsp <a href="./src/praisonai/praisonai/cli/commands/debug.py">--help</a></code>
* <code title="cli">praisonai debug trace <a href="./src/praisonai/praisonai/cli/commands/debug.py">--help</a></code>
* <code title="cli">praisonai deploy aws <a href="./src/praisonai/praisonai/cli/commands/deploy.py">--help</a></code>
* <code title="cli">praisonai deploy azure <a href="./src/praisonai/praisonai/cli/commands/deploy.py">--help</a></code>
* <code title="cli">praisonai deploy docker <a href="./src/praisonai/praisonai/cli/commands/deploy.py">--help</a></code>
* <code title="cli">praisonai deploy gcp <a href="./src/praisonai/praisonai/cli/commands/deploy.py">--help</a></code>
* <code title="cli">praisonai diag diag-callback <a href="./src/praisonai/praisonai/cli/commands/diag.py">--help</a></code>
* <code title="cli">praisonai diag export <a href="./src/praisonai/praisonai/cli/commands/diag.py">--help</a></code>
* <code title="cli">praisonai docs api-md <a href="./src/praisonai/praisonai/cli/commands/docs.py">--help</a></code>
* <code title="cli">praisonai docs generate <a href="./src/praisonai/praisonai/cli/commands/docs.py">--help</a></code>
* <code title="cli">praisonai docs list <a href="./src/praisonai/praisonai/cli/commands/docs.py">--help</a></code>
* <code title="cli">praisonai docs report <a href="./src/praisonai/praisonai/cli/commands/docs.py">--help</a></code>
* <code title="cli">praisonai docs run <a href="./src/praisonai/praisonai/cli/commands/docs.py">--help</a></code>
* <code title="cli">praisonai docs run-all <a href="./src/praisonai/praisonai/cli/commands/docs.py">--help</a></code>
* <code title="cli">praisonai docs serve <a href="./src/praisonai/praisonai/cli/commands/docs.py">--help</a></code>
* <code title="cli">praisonai docs stats <a href="./src/praisonai/praisonai/cli/commands/docs.py">--help</a></code>
* <code title="cli">praisonai doctor cleanup <a href="./src/praisonai/praisonai/cli/commands/doctor.py">--help</a></code>
* <code title="cli">praisonai doctor config <a href="./src/praisonai/praisonai/cli/commands/doctor.py">--help</a></code>
* <code title="cli">praisonai doctor db <a href="./src/praisonai/praisonai/cli/commands/doctor.py">--help</a></code>
* <code title="cli">praisonai doctor doctor-callback <a href="./src/praisonai/praisonai/cli/commands/doctor.py">--help</a></code>
* <code title="cli">praisonai doctor env <a href="./src/praisonai/praisonai/cli/commands/doctor.py">--help</a></code>
* <code title="cli">praisonai doctor mcp <a href="./src/praisonai/praisonai/cli/commands/doctor.py">--help</a></code>
* <code title="cli">praisonai doctor network <a href="./src/praisonai/praisonai/cli/commands/doctor.py">--help</a></code>
* <code title="cli">praisonai doctor performance <a href="./src/praisonai/praisonai/cli/commands/doctor.py">--help</a></code>
* <code title="cli">praisonai doctor selftest <a href="./src/praisonai/praisonai/cli/commands/doctor.py">--help</a></code>
* <code title="cli">praisonai doctor tools <a href="./src/praisonai/praisonai/cli/commands/doctor.py">--help</a></code>
* <code title="cli">praisonai doctor troubleshoot <a href="./src/praisonai/praisonai/cli/commands/doctor.py">--help</a></code>
* <code title="cli">praisonai endpoints list <a href="./src/praisonai/praisonai/cli/commands/endpoints.py">--help</a></code>
* <code title="cli">praisonai endpoints test <a href="./src/praisonai/praisonai/cli/commands/endpoints.py">--help</a></code>
* <code title="cli">praisonai environment check <a href="./src/praisonai/praisonai/cli/commands/environment.py">--help</a></code>
* <code title="cli">praisonai environment doctor <a href="./src/praisonai/praisonai/cli/commands/environment.py">--help</a></code>
* <code title="cli">praisonai environment view <a href="./src/praisonai/praisonai/cli/commands/environment.py">--help</a></code>
* <code title="cli">praisonai eval accuracy <a href="./src/praisonai/praisonai/cli/commands/eval.py">--help</a></code>
* <code title="cli">praisonai eval judge <a href="./src/praisonai/praisonai/cli/commands/eval.py">--help</a></code>
* <code title="cli">praisonai eval list-judges <a href="./src/praisonai/praisonai/cli/commands/eval.py">--help</a></code>
* <code title="cli">praisonai eval performance <a href="./src/praisonai/praisonai/cli/commands/eval.py">--help</a></code>
* <code title="cli">praisonai examples info <a href="./src/praisonai/praisonai/cli/commands/examples.py">--help</a></code>
* <code title="cli">praisonai examples list <a href="./src/praisonai/praisonai/cli/commands/examples.py">--help</a></code>
* <code title="cli">praisonai examples report <a href="./src/praisonai/praisonai/cli/commands/examples.py">--help</a></code>
* <code title="cli">praisonai examples run <a href="./src/praisonai/praisonai/cli/commands/examples.py">--help</a></code>
* <code title="cli">praisonai examples run-all <a href="./src/praisonai/praisonai/cli/commands/examples.py">--help</a></code>
* <code title="cli">praisonai examples stats <a href="./src/praisonai/praisonai/cli/commands/examples.py">--help</a></code>
* <code title="cli">praisonai hooks add <a href="./src/praisonai/praisonai/cli/commands/hooks.py">--help</a></code>
* <code title="cli">praisonai hooks list <a href="./src/praisonai/praisonai/cli/commands/hooks.py">--help</a></code>
* <code title="cli">praisonai hooks remove <a href="./src/praisonai/praisonai/cli/commands/hooks.py">--help</a></code>
* <code title="cli">praisonai knowledge add <a href="./src/praisonai/praisonai/cli/commands/knowledge.py">--help</a></code>
* <code title="cli">praisonai knowledge index <a href="./src/praisonai/praisonai/cli/commands/knowledge.py">--help</a></code>
* <code title="cli">praisonai knowledge list <a href="./src/praisonai/praisonai/cli/commands/knowledge.py">--help</a></code>
* <code title="cli">praisonai knowledge search <a href="./src/praisonai/praisonai/cli/commands/knowledge.py">--help</a></code>
* <code title="cli">praisonai loop help <a href="./src/praisonai/praisonai/cli/commands/loop.py">--help</a></code>
* <code title="cli">praisonai loop loop-main <a href="./src/praisonai/praisonai/cli/commands/loop.py">--help</a></code>
* <code title="cli">praisonai lsp logs <a href="./src/praisonai/praisonai/cli/commands/lsp.py">--help</a></code>
* <code title="cli">praisonai lsp lsp-callback <a href="./src/praisonai/praisonai/cli/commands/lsp.py">--help</a></code>
* <code title="cli">praisonai lsp start <a href="./src/praisonai/praisonai/cli/commands/lsp.py">--help</a></code>
* <code title="cli">praisonai lsp status <a href="./src/praisonai/praisonai/cli/commands/lsp.py">--help</a></code>
* <code title="cli">praisonai lsp stop <a href="./src/praisonai/praisonai/cli/commands/lsp.py">--help</a></code>
* <code title="cli">praisonai mcp add <a href="./src/praisonai/praisonai/cli/commands/mcp.py">--help</a></code>
* <code title="cli">praisonai mcp auth <a href="./src/praisonai/praisonai/cli/commands/mcp.py">--help</a></code>
* <code title="cli">praisonai mcp describe <a href="./src/praisonai/praisonai/cli/commands/mcp.py">--help</a></code>
* <code title="cli">praisonai mcp list <a href="./src/praisonai/praisonai/cli/commands/mcp.py">--help</a></code>
* <code title="cli">praisonai mcp logout <a href="./src/praisonai/praisonai/cli/commands/mcp.py">--help</a></code>
* <code title="cli">praisonai mcp mcp-callback <a href="./src/praisonai/praisonai/cli/commands/mcp.py">--help</a></code>
* <code title="cli">praisonai mcp remove <a href="./src/praisonai/praisonai/cli/commands/mcp.py">--help</a></code>
* <code title="cli">praisonai mcp run <a href="./src/praisonai/praisonai/cli/commands/mcp.py">--help</a></code>
* <code title="cli">praisonai mcp status <a href="./src/praisonai/praisonai/cli/commands/mcp.py">--help</a></code>
* <code title="cli">praisonai mcp sync <a href="./src/praisonai/praisonai/cli/commands/mcp.py">--help</a></code>
* <code title="cli">praisonai mcp test <a href="./src/praisonai/praisonai/cli/commands/mcp.py">--help</a></code>
* <code title="cli">praisonai mcp tools <a href="./src/praisonai/praisonai/cli/commands/mcp.py">--help</a></code>
* <code title="cli">praisonai memory add <a href="./src/praisonai/praisonai/cli/commands/memory.py">--help</a></code>
* <code title="cli">praisonai memory clear <a href="./src/praisonai/praisonai/cli/commands/memory.py">--help</a></code>
* <code title="cli">praisonai memory search <a href="./src/praisonai/praisonai/cli/commands/memory.py">--help</a></code>
* <code title="cli">praisonai memory show <a href="./src/praisonai/praisonai/cli/commands/memory.py">--help</a></code>
* <code title="cli">praisonai memory status <a href="./src/praisonai/praisonai/cli/commands/memory.py">--help</a></code>
* <code title="cli">praisonai package install <a href="./src/praisonai/praisonai/cli/commands/package.py">--help</a></code>
* <code title="cli">praisonai package list <a href="./src/praisonai/praisonai/cli/commands/package.py">--help</a></code>
* <code title="cli">praisonai package uninstall <a href="./src/praisonai/praisonai/cli/commands/package.py">--help</a></code>
* <code title="cli">praisonai plugins create <a href="./src/praisonai/praisonai/cli/commands/plugins.py">--help</a></code>
* <code title="cli">praisonai plugins disable <a href="./src/praisonai/praisonai/cli/commands/plugins.py">--help</a></code>
* <code title="cli">praisonai plugins discover <a href="./src/praisonai/praisonai/cli/commands/plugins.py">--help</a></code>
* <code title="cli">praisonai plugins doctor <a href="./src/praisonai/praisonai/cli/commands/plugins.py">--help</a></code>
* <code title="cli">praisonai plugins enable <a href="./src/praisonai/praisonai/cli/commands/plugins.py">--help</a></code>
* <code title="cli">praisonai plugins info <a href="./src/praisonai/praisonai/cli/commands/plugins.py">--help</a></code>
* <code title="cli">praisonai plugins install <a href="./src/praisonai/praisonai/cli/commands/plugins.py">--help</a></code>
* <code title="cli">praisonai plugins list <a href="./src/praisonai/praisonai/cli/commands/plugins.py">--help</a></code>
* <code title="cli">praisonai plugins plugins-callback <a href="./src/praisonai/praisonai/cli/commands/plugins.py">--help</a></code>
* <code title="cli">praisonai plugins remove <a href="./src/praisonai/praisonai/cli/commands/plugins.py">--help</a></code>
* <code title="cli">praisonai profile imports <a href="./src/praisonai/praisonai/cli/commands/profile.py">--help</a></code>
* <code title="cli">praisonai profile optimize <a href="./src/praisonai/praisonai/cli/commands/profile.py">--help</a></code>
* <code title="cli">praisonai profile profile-callback <a href="./src/praisonai/praisonai/cli/commands/profile.py">--help</a></code>
* <code title="cli">praisonai profile query <a href="./src/praisonai/praisonai/cli/commands/profile.py">--help</a></code>
* <code title="cli">praisonai profile snapshot <a href="./src/praisonai/praisonai/cli/commands/profile.py">--help</a></code>
* <code title="cli">praisonai profile startup <a href="./src/praisonai/praisonai/cli/commands/profile.py">--help</a></code>
* <code title="cli">praisonai profile suite <a href="./src/praisonai/praisonai/cli/commands/profile.py">--help</a></code>
* <code title="cli">praisonai rag chat <a href="./src/praisonai/praisonai/cli/commands/rag.py">--help</a></code>
* <code title="cli">praisonai rag eval <a href="./src/praisonai/praisonai/cli/commands/rag.py">--help</a></code>
* <code title="cli">praisonai rag index <a href="./src/praisonai/praisonai/cli/commands/rag.py">--help</a></code>
* <code title="cli">praisonai rag query <a href="./src/praisonai/praisonai/cli/commands/rag.py">--help</a></code>
* <code title="cli">praisonai rag serve <a href="./src/praisonai/praisonai/cli/commands/rag.py">--help</a></code>
* <code title="cli">praisonai realtime realtime-main <a href="./src/praisonai/praisonai/cli/commands/realtime.py">--help</a></code>
* <code title="cli">praisonai recipe apply <a href="./src/praisonai/praisonai/cli/commands/recipe.py">--help</a></code>
* <code title="cli">praisonai recipe create <a href="./src/praisonai/praisonai/cli/commands/recipe.py">--help</a></code>
* <code title="cli">praisonai recipe info <a href="./src/praisonai/praisonai/cli/commands/recipe.py">--help</a></code>
* <code title="cli">praisonai recipe install <a href="./src/praisonai/praisonai/cli/commands/recipe.py">--help</a></code>
* <code title="cli">praisonai recipe judge <a href="./src/praisonai/praisonai/cli/commands/recipe.py">--help</a></code>
* <code title="cli">praisonai recipe list <a href="./src/praisonai/praisonai/cli/commands/recipe.py">--help</a></code>
* <code title="cli">praisonai recipe optimize <a href="./src/praisonai/praisonai/cli/commands/recipe.py">--help</a></code>
* <code title="cli">praisonai recipe run <a href="./src/praisonai/praisonai/cli/commands/recipe.py">--help</a></code>
* <code title="cli">praisonai recipe serve <a href="./src/praisonai/praisonai/cli/commands/recipe.py">--help</a></code>
* <code title="cli">praisonai registry list <a href="./src/praisonai/praisonai/cli/commands/registry.py">--help</a></code>
* <code title="cli">praisonai registry serve <a href="./src/praisonai/praisonai/cli/commands/registry.py">--help</a></code>
* <code title="cli">praisonai replay cleanup <a href="./src/praisonai/praisonai/cli/commands/replay.py">--help</a></code>
* <code title="cli">praisonai replay context <a href="./src/praisonai/praisonai/cli/commands/replay.py">--help</a></code>
* <code title="cli">praisonai replay dashboard <a href="./src/praisonai/praisonai/cli/commands/replay.py">--help</a></code>
* <code title="cli">praisonai replay delete <a href="./src/praisonai/praisonai/cli/commands/replay.py">--help</a></code>
* <code title="cli">praisonai replay flow <a href="./src/praisonai/praisonai/cli/commands/replay.py">--help</a></code>
* <code title="cli">praisonai replay list <a href="./src/praisonai/praisonai/cli/commands/replay.py">--help</a></code>
* <code title="cli">praisonai replay show <a href="./src/praisonai/praisonai/cli/commands/replay.py">--help</a></code>
* <code title="cli">praisonai research research-main <a href="./src/praisonai/praisonai/cli/commands/research.py">--help</a></code>
* <code title="cli">praisonai retrieval index <a href="./src/praisonai/praisonai/cli/commands/retrieval.py">--help</a></code>
* <code title="cli">praisonai retrieval query <a href="./src/praisonai/praisonai/cli/commands/retrieval.py">--help</a></code>
* <code title="cli">praisonai retrieval search <a href="./src/praisonai/praisonai/cli/commands/retrieval.py">--help</a></code>
* <code title="cli">praisonai rules add <a href="./src/praisonai/praisonai/cli/commands/rules.py">--help</a></code>
* <code title="cli">praisonai rules clear <a href="./src/praisonai/praisonai/cli/commands/rules.py">--help</a></code>
* <code title="cli">praisonai rules list <a href="./src/praisonai/praisonai/cli/commands/rules.py">--help</a></code>
* <code title="cli">praisonai run <a href="./src/praisonai/praisonai/cli/main.py">--help</a></code>
* <code title="cli">praisonai run run-main <a href="./src/praisonai/praisonai/cli/commands/run.py">--help</a></code>
* <code title="cli">praisonai sandbox explain <a href="./src/praisonai/praisonai/cli/commands/sandbox.py">--help</a></code>
* <code title="cli">praisonai sandbox list <a href="./src/praisonai/praisonai/cli/commands/sandbox.py">--help</a></code>
* <code title="cli">praisonai sandbox recreate <a href="./src/praisonai/praisonai/cli/commands/sandbox.py">--help</a></code>
* <code title="cli">praisonai sandbox sandbox-callback <a href="./src/praisonai/praisonai/cli/commands/sandbox.py">--help</a></code>
* <code title="cli">praisonai sandbox status <a href="./src/praisonai/praisonai/cli/commands/sandbox.py">--help</a></code>
* <code title="cli">praisonai schedule delete <a href="./src/praisonai/praisonai/cli/commands/schedule.py">--help</a></code>
* <code title="cli">praisonai schedule describe <a href="./src/praisonai/praisonai/cli/commands/schedule.py">--help</a></code>
* <code title="cli">praisonai schedule list <a href="./src/praisonai/praisonai/cli/commands/schedule.py">--help</a></code>
* <code title="cli">praisonai schedule logs <a href="./src/praisonai/praisonai/cli/commands/schedule.py">--help</a></code>
* <code title="cli">praisonai schedule restart <a href="./src/praisonai/praisonai/cli/commands/schedule.py">--help</a></code>
* <code title="cli">praisonai schedule schedule-callback <a href="./src/praisonai/praisonai/cli/commands/schedule.py">--help</a></code>
* <code title="cli">praisonai schedule start <a href="./src/praisonai/praisonai/cli/commands/schedule.py">--help</a></code>
* <code title="cli">praisonai schedule stats <a href="./src/praisonai/praisonai/cli/commands/schedule.py">--help</a></code>
* <code title="cli">praisonai schedule stop <a href="./src/praisonai/praisonai/cli/commands/schedule.py">--help</a></code>
* <code title="cli">praisonai serve a2a <a href="./src/praisonai/praisonai/cli/commands/serve.py">--help</a></code>
* <code title="cli">praisonai serve a2u <a href="./src/praisonai/praisonai/cli/commands/serve.py">--help</a></code>
* <code title="cli">praisonai serve acp <a href="./src/praisonai/praisonai/cli/commands/serve.py">--help</a></code>
* <code title="cli">praisonai serve agents <a href="./src/praisonai/praisonai/cli/commands/serve.py">--help</a></code>
* <code title="cli">praisonai serve docs <a href="./src/praisonai/praisonai/cli/commands/serve.py">--help</a></code>
* <code title="cli">praisonai serve gateway <a href="./src/praisonai/praisonai/cli/commands/serve.py">--help</a></code>
* <code title="cli">praisonai serve lsp <a href="./src/praisonai/praisonai/cli/commands/serve.py">--help</a></code>
* <code title="cli">praisonai serve mcp <a href="./src/praisonai/praisonai/cli/commands/serve.py">--help</a></code>
* <code title="cli">praisonai serve rag <a href="./src/praisonai/praisonai/cli/commands/serve.py">--help</a></code>
* <code title="cli">praisonai serve recipe <a href="./src/praisonai/praisonai/cli/commands/serve.py">--help</a></code>
* <code title="cli">praisonai serve registry <a href="./src/praisonai/praisonai/cli/commands/serve.py">--help</a></code>
* <code title="cli">praisonai serve scheduler <a href="./src/praisonai/praisonai/cli/commands/serve.py">--help</a></code>
* <code title="cli">praisonai serve serve-callback <a href="./src/praisonai/praisonai/cli/commands/serve.py">--help</a></code>
* <code title="cli">praisonai serve start <a href="./src/praisonai/praisonai/cli/commands/serve.py">--help</a></code>
* <code title="cli">praisonai serve status <a href="./src/praisonai/praisonai/cli/commands/serve.py">--help</a></code>
* <code title="cli">praisonai serve stop <a href="./src/praisonai/praisonai/cli/commands/serve.py">--help</a></code>
* <code title="cli">praisonai serve ui <a href="./src/praisonai/praisonai/cli/commands/serve.py">--help</a></code>
* <code title="cli">praisonai serve unified <a href="./src/praisonai/praisonai/cli/commands/serve.py">--help</a></code>
* <code title="cli">praisonai session delete <a href="./src/praisonai/praisonai/cli/commands/session.py">--help</a></code>
* <code title="cli">praisonai session export <a href="./src/praisonai/praisonai/cli/commands/session.py">--help</a></code>
* <code title="cli">praisonai session import <a href="./src/praisonai/praisonai/cli/commands/session.py">--help</a></code>
* <code title="cli">praisonai session list <a href="./src/praisonai/praisonai/cli/commands/session.py">--help</a></code>
* <code title="cli">praisonai session resume <a href="./src/praisonai/praisonai/cli/commands/session.py">--help</a></code>
* <code title="cli">praisonai session show <a href="./src/praisonai/praisonai/cli/commands/session.py">--help</a></code>
* <code title="cli">praisonai skills check <a href="./src/praisonai/praisonai/cli/commands/skills.py">--help</a></code>
* <code title="cli">praisonai skills create <a href="./src/praisonai/praisonai/cli/commands/skills.py">--help</a></code>
* <code title="cli">praisonai skills eligible <a href="./src/praisonai/praisonai/cli/commands/skills.py">--help</a></code>
* <code title="cli">praisonai skills info <a href="./src/praisonai/praisonai/cli/commands/skills.py">--help</a></code>
* <code title="cli">praisonai skills install <a href="./src/praisonai/praisonai/cli/commands/skills.py">--help</a></code>
* <code title="cli">praisonai skills list <a href="./src/praisonai/praisonai/cli/commands/skills.py">--help</a></code>
* <code title="cli">praisonai skills search <a href="./src/praisonai/praisonai/cli/commands/skills.py">--help</a></code>
* <code title="cli">praisonai skills validate <a href="./src/praisonai/praisonai/cli/commands/skills.py">--help</a></code>
* <code title="cli">praisonai standardise check <a href="./src/praisonai/praisonai/cli/commands/standardise.py">--help</a></code>
* <code title="cli">praisonai standardise fix <a href="./src/praisonai/praisonai/cli/commands/standardise.py">--help</a></code>
* <code title="cli">praisonai standardise init <a href="./src/praisonai/praisonai/cli/commands/standardise.py">--help</a></code>
* <code title="cli">praisonai standardise report <a href="./src/praisonai/praisonai/cli/commands/standardise.py">--help</a></code>
* <code title="cli">praisonai templates create <a href="./src/praisonai/praisonai/cli/commands/templates.py">--help</a></code>
* <code title="cli">praisonai templates list <a href="./src/praisonai/praisonai/cli/commands/templates.py">--help</a></code>
* <code title="cli">praisonai test info <a href="./src/praisonai/praisonai/cli/commands/test.py">--help</a></code>
* <code title="cli">praisonai test interactive <a href="./src/praisonai/praisonai/cli/commands/test.py">--help</a></code>
* <code title="cli">praisonai test run <a href="./src/praisonai/praisonai/cli/commands/test.py">--help</a></code>
* <code title="cli">praisonai todo add <a href="./src/praisonai/praisonai/cli/commands/todo.py">--help</a></code>
* <code title="cli">praisonai todo done <a href="./src/praisonai/praisonai/cli/commands/todo.py">--help</a></code>
* <code title="cli">praisonai todo list <a href="./src/praisonai/praisonai/cli/commands/todo.py">--help</a></code>
* <code title="cli">praisonai tools info <a href="./src/praisonai/praisonai/cli/commands/tools.py">--help</a></code>
* <code title="cli">praisonai tools list <a href="./src/praisonai/praisonai/cli/commands/tools.py">--help</a></code>
* <code title="cli">praisonai tools test <a href="./src/praisonai/praisonai/cli/commands/tools.py">--help</a></code>
* <code title="cli">praisonai tools validate <a href="./src/praisonai/praisonai/cli/commands/tools.py">--help</a></code>
* <code title="cli">praisonai traces disable <a href="./src/praisonai/praisonai/cli/commands/traces.py">--help</a></code>
* <code title="cli">praisonai traces enable <a href="./src/praisonai/praisonai/cli/commands/traces.py">--help</a></code>
* <code title="cli">praisonai traces list <a href="./src/praisonai/praisonai/cli/commands/traces.py">--help</a></code>
* <code title="cli">praisonai traces status <a href="./src/praisonai/praisonai/cli/commands/traces.py">--help</a></code>
* <code title="cli">praisonai train agents <a href="./src/praisonai/praisonai/cli/commands/train.py">--help</a></code>
* <code title="cli">praisonai train apply <a href="./src/praisonai/praisonai/cli/commands/train.py">--help</a></code>
* <code title="cli">praisonai train list <a href="./src/praisonai/praisonai/cli/commands/train.py">--help</a></code>
* <code title="cli">praisonai train llm <a href="./src/praisonai/praisonai/cli/commands/train.py">--help</a></code>
* <code title="cli">praisonai train show <a href="./src/praisonai/praisonai/cli/commands/train.py">--help</a></code>
* <code title="cli">praisonai train train-callback <a href="./src/praisonai/praisonai/cli/commands/train.py">--help</a></code>
* <code title="cli">praisonai ui chat <a href="./src/praisonai/praisonai/cli/commands/ui.py">--help</a></code>
* <code title="cli">praisonai ui code <a href="./src/praisonai/praisonai/cli/commands/ui.py">--help</a></code>
* <code title="cli">praisonai ui gradio <a href="./src/praisonai/praisonai/cli/commands/ui.py">--help</a></code>
* <code title="cli">praisonai ui realtime <a href="./src/praisonai/praisonai/cli/commands/ui.py">--help</a></code>
* <code title="cli">praisonai ui ui-main <a href="./src/praisonai/praisonai/cli/commands/ui.py">--help</a></code>
* <code title="cli">praisonai version check <a href="./src/praisonai/praisonai/cli/commands/version.py">--help</a></code>
* <code title="cli">praisonai version show <a href="./src/praisonai/praisonai/cli/commands/version.py">--help</a></code>
* <code title="cli">praisonai version version-callback <a href="./src/praisonai/praisonai/cli/commands/version.py">--help</a></code>
* <code title="cli">praisonai workflow create <a href="./src/praisonai/praisonai/cli/commands/workflow.py">--help</a></code>
* <code title="cli">praisonai workflow list <a href="./src/praisonai/praisonai/cli/commands/workflow.py">--help</a></code>
* <code title="cli">praisonai workflow run <a href="./src/praisonai/praisonai/cli/commands/workflow.py">--help</a></code>

# TypeScript

Types/Exports:
```ts
export { Agent, AgentTeam, Agents, PraisonAIAgents, Router } from "./agent";
export type { AgentTeamConfig, PraisonAIAgentsConfig, SimpleAgentConfig, SimpleRouteConfig, SimpleRouterConfig } from "./agent";
export { AudioAgent, createAudioAgent } from "./agent/audio";
export type { AudioAgentConfig, AudioProvider, AudioSpeakOptions, AudioSpeakResult, AudioTranscribeOptions, AudioTranscribeResult } from "./agent/audio";
export { ContextAgent, createContextAgent } from "./agent/context";
export { ContextPolicy, Handoff, HandoffCycleError, HandoffDepthError, HandoffError, HandoffTimeoutError, RECOMMENDED_PROMPT_PREFIX, handoff, handoffFilters, promptWithHandoffInstructions } from "./agent/handoff";
export { ImageAgent, createImageAgent } from "./agent/image";
export { PromptExpanderAgent, createPromptExpanderAgent } from "./agent/prompt-expander";
export { QueryRewriterAgent, createQueryRewriterAgent } from "./agent/query-rewriter";
export { DeepResearchAgent, createDeepResearchAgent } from "./agent/research";
export { RouterAgent, createRouter, routeConditions } from "./agent/router";
export { // Agent loop
  createAgentLoop, // DevTools
  enableDevTools, // MCP
  createMCP, // Middleware (renamed to avoid conflicts)
  createCachingMiddleware, // Models
  createModel, // Multimodal
  createImagePart, // Next.js
  createRouteHandler, // OAuth for MCP
  type OAuthClientProvider, // Server adapters
  createHttpHandler, // Speech & Transcription
  generateSpeech, // Telemetry (AI SDK v6 parity)
  configureTelemetry, // Tool Approval (AI SDK v6 parity)
  ApprovalManager, // Tools
  defineTool, // UI Message (AI SDK v6 parity)
  convertToModelMessages, AIAgentStep, AIEmbedManyResult, AIEmbedOptions, AIEmbedResult, AIFilePart, AIGenerateImageOptions, AIGenerateImageResult, AIGenerateObjectOptions, AIGenerateObjectResult, AIGenerateTextOptions, AIGenerateTextResult, AIImagePart, AIMiddleware, AIMiddlewareConfig, AIModelMessage, AISpan, AISpanKind, AISpanOptions, AISpanStatus, AIStreamObjectOptions, AIStreamObjectResult, AIStreamTextOptions, AIStreamTextResult, AITelemetryEvent, AITelemetrySettings, AITextPart, AIToolDefinition, AITracer, AgentLoop, DANGEROUS_PATTERNS, MCPClientType, MODEL_ALIASES, SPEECH_MODELS, TRANSCRIPTION_MODELS, ToolApprovalDeniedError, ToolApprovalTimeoutError, aiEmbed, aiEmbedMany, aiGenerateImage, aiGenerateObject, aiGenerateText, aiStreamObject, aiStreamText, applyMiddleware, autoEnableDevTools, base64ToUint8Array, clearAICache, clearEvents, closeAllMCPClients, closeMCPClient, convertToUIMessages, createAILoggingMiddleware, createAISpan, createApprovalResponse, createDangerousPatternChecker, createDevToolsMiddleware, createExpressHandler, createFastifyHandler, createFilePart, createHonoHandler, createMultimodalMessage, createNestHandler, createPagesHandler, createPdfPart, createSystemMessage, createTelemetryMiddleware, createTelemetrySettings, createTextMessage, createTextPart, createToolSet, disableAITelemetry, disableDevTools, enableAITelemetry, functionToTool, getAICacheStats, getApprovalManager, getDevToolsState, getDevToolsUrl, getEvents, getMCPClient, getModel, getTelemetrySettings, getToolsNeedingApproval, getTracer, hasModelAlias, hasPendingApprovals, initOpenTelemetry, isDangerous, isDataUrl, isDevToolsEnabled, isTelemetryEnabled, isUrl, listModelAliases, mcpToolsToAITools, parseModel, pipeUIMessageStreamToResponse, recordEvent, resolveModelAlias, safeValidateUIMessages, setApprovalManager, stopAfterSteps, stopWhen, stopWhenNoToolCalls, toMessageContent, toUIMessageStreamResponse, transcribe, uint8ArrayToBase64, validateUIMessages, withApproval, withSpan, wrapModel } from "./ai";
export { AutoAgents, AutoTaskConfig, createAutoAgents } from "./auto";
export { BaseCache, FileCache, MemoryCache, createFileCache, createMemoryCache } from "./cache";
export { CLI_SPEC_VERSION, executeCommand, parseArgs } from "./cli";
export { // Autonomy Mode
  AutonomyManager, // Background Jobs
  JobQueue, // Checkpoints
  CheckpointManager, // Cost Tracker
  CostTracker, // External Agents
  BaseExternalAgent, // Fast Context (Python parity with praisonaiagents/context/fast)
  FastContext, // Flow Display
  FlowDisplay, // Git Integration
  GitManager, // Interactive TUI
  InteractiveTUI, // N8N Integration
  N8NIntegration, // Python parity additions
  type LineRange, // Repo Map
  RepoMap, // Sandbox Executor
  SandboxExecutor, // Scheduler
  Scheduler, // Slash Commands
  SlashCommandHandler, AiderAgent, ClaudeCodeAgent, CodexCliAgent, CommandValidator, CostTokenUsage, DEFAULT_BLOCKED_COMMANDS, DEFAULT_BLOCKED_PATHS, DEFAULT_IGNORE_PATTERNS, DiffViewer, FileCheckpointStorage, FileJobStorage, GeminiCliAgent, GenericExternalAgent, HistoryManager, MODEL_PRICING, MODE_POLICIES, MemoryCheckpointStorage, MemoryJobStorage, StatusDisplay, addLineRangeToFileMatch, cliApprovalPrompt, createAutonomyManager, createCheckpointManager, createCostTracker, createDiffViewer, createExternalAgent, createFastContext, createFileCheckpointStorage, createFileJobStorage, createFileMatch, createFlowDisplay, createGitManager, createHistoryManager, createInteractiveTUI, createJobQueue, createLineRange, createN8NIntegration, createRepoMap, createSandboxExecutor, createScheduler, createSlashCommandHandler, createStatusDisplay, cronExpressions, estimateTokens, executeSlashCommand, externalAgentAsTool, formatCost, getExternalAgentRegistry, getLineCount, getQuickContext, getRepoTree, getTotalLines, isSlashCommand, mergeRanges, parseSlashCommand, rangesOverlap, registerCommand, renderWorkflow, sandboxExec, triggerN8NWebhook } from "./cli/features";
export { // Classes
  DictCondition, // Functions
  evaluateCondition, // Types
  type ConditionProtocol, ExpressionCondition, FunctionCondition, andConditions, createCondition, notCondition, orConditions } from "./conditions";
export { // Enums
  MemoryBackend, // Errors
  ConfigValidationError, // Parse utilities
  detect_url_scheme, // Presets
  MEMORY_PRESETS, // Resolver functions
  resolve, AUTONOMY_PRESETS, ArrayMode, CACHING_PRESETS, CONTEXT_PRESETS, ChunkingStrategy, EXECUTION_PRESETS, ExecutionPreset, FeatureMemoryConfig, GUARDRAIL_PRESETS, GuardrailAction, KNOWLEDGE_PRESETS, MEMORY_URL_SCHEMES, MULTI_AGENT_EXECUTION_PRESETS, MULTI_AGENT_OUTPUT_PRESETS, OUTPUT_PRESETS, OutputPreset, PLANNING_PRESETS, REFLECTION_PRESETS, WEB_PRESETS, WebSearchProvider, apply_config_defaults, clean_triple_backticks, get_config, get_config_path, get_default, get_defaults_config, get_plugins_config, is_path_like, is_policy_string, parse_policy_string, resolve_autonomy, resolve_caching, resolve_context, resolve_execution, resolve_guardrails, resolve_hooks, resolve_knowledge, resolve_memory, resolve_output, resolve_planning, resolve_reflection, resolve_routing, resolve_skills, resolve_web, suggest_similar, validate_config } from "./config";
export { createDbAdapter, db, getDefaultDbAdapter, setDefaultDbAdapter } from "./db";
export type { DbAdapter, DbConfig, DbMessage, DbRun, DbTrace } from "./db";
export { MemoryPostgresAdapter, NeonPostgresAdapter, PostgresSessionStorage, createMemoryPostgres, createNeonPostgres, createPostgresSessionStorage } from "./db/postgres";
export { MemoryRedisAdapter, UpstashRedisAdapter, createMemoryRedis, createUpstashRedis } from "./db/redis";
export { SQLiteAdapter, createSQLiteAdapter } from "./db/sqlite";
export { // Types
  type DisplayCallback, DisplayFlow, DisplayFlowConfig, asyncDisplayCallbacks, clearDisplayCallbacks, clearErrorLogs, displayError, displayGenerating, displayInstruction, displayInteraction, displaySelfReflection, displayToolCall, errorLogs, logError, registerDisplay, syncDisplayCallbacks } from "./display";
export { // Functions
  embed, // Types
  type EmbeddingResult, aembed, aembedding, aembeddings, cosineSimilarity, embedding, embeddings, euclideanDistance, getDimensions, normalizeEmbedding, setEmbeddingConfig } from "./embeddings";
export { // LLM-as-Judge
  Judge, AccuracyJudge, CriteriaJudge, EvalResults, EvalSuite, Evaluator, RecipeJudge, accuracyEval, addJudge, addOptimizationRule, containsKeywordsCriterion, createDefaultEvaluator, createEvalResults, createEvaluator, getJudge, getOptimizationRule, lengthCriterion, listJudges, listOptimizationRules, noHarmfulContentCriterion, parseJudgeResponse, performanceEval, relevanceCriterion, reliabilityEval, removeJudge, removeOptimizationRule } from "./eval";
export { AgentEventBus, AgentEvents, EventEmitterPubSub, PubSub, createEventBus, createPubSub } from "./events";
export { // Bot types
  type BotConfig, // Classes
  FailoverManager, // Enums
  SandboxStatus, // Gateway types
  type GatewayConfig, // Other types
  type ProviderStatus, AutonomyLevel, RagRetrievalPolicy } from "./gateway";
export { LLMGuardrail, createLLMGuardrail } from "./guardrails/llm-guardrail";
export { DisplayTypes, HooksManager, WorkflowHooksExecutor, clearAllCallbacks, clearApprovalCallback, createHooksManager, createLoggingOperationHooks, createLoggingWorkflowHooks, createTimingWorkflowHooks, createValidationOperationHooks, createWorkflowHooks, executeCallback, executeSyncCallback, getRegisteredDisplayTypes, hasApprovalCallback, registerApprovalCallback, registerDisplayCallback, requestApproval, unregisterDisplayCallback } from "./hooks";
export { // Computer Use
  createComputerUse, ComputerUseClient, createCLIApprovalPrompt, createComputerUseAgent } from "./integrations/computer-use";
export { BaseObservabilityProvider, ConsoleObservabilityProvider, LangfuseObservabilityProvider, MemoryObservabilityProvider, ObservabilityTraceContext, createConsoleObservability, createLangfuseObservability, createMemoryObservability } from "./integrations/observability";
export { // Natural Language Postgres
  createNLPostgres, NLPostgresClient, NLPostgresConfig, createPostgresTool } from "./integrations/postgres";
export { // Slack
  createSlackBot, SlackBot, parseSlackMessage, verifySlackSignature } from "./integrations/slack";
export { BaseVectorStore, ChromaVectorStore, MemoryVectorStore, PineconeVectorStore, QdrantVectorStore, VectorQueryResult, WeaviateVectorStore, createChromaStore, createMemoryVectorStore, createPineconeStore, createQdrantStore, createWeaviateStore } from "./integrations/vector";
export { BaseVoiceProvider, ElevenLabsVoiceProvider, OpenAIVoiceProvider, createElevenLabsVoice, createOpenAIVoice } from "./integrations/voice";
export { GraphRAG, GraphStore, createGraphRAG } from "./knowledge/graph-rag";
export { BaseReranker, CohereReranker, CrossEncoderReranker, LLMReranker, createCohereReranker, createCrossEncoderReranker, createLLMReranker } from "./knowledge/reranker";
export { // Provider classes
  OpenAIProvider, // Provider factory and utilities
  createProvider, // Provider registry (extensibility API)
  ProviderRegistry, // Types
  type LLMProvider, AnthropicProvider, BaseProvider, GoogleProvider, ProviderMessage, ProviderToolDefinition, createProviderRegistry, getAvailableProviders, getDefaultProvider, getDefaultRegistry, hasProvider, isProviderAvailable, listProviders, parseModelString, registerBuiltinProviders, registerProvider, unregisterProvider } from "./llm/providers";
export { ADAPTERS, AISDK_PROVIDERS, COMMUNITY_PROVIDERS, PROVIDER_ALIASES } from "./llm/providers/ai-sdk/types";
export { MCPClient, MCPSecurity, MCPServer, MCPSessionManager, createApiKeyPolicy, createMCPClient, createMCPSecurity, createMCPServer, createMCPSession, createRateLimitPolicy, getMCPTools } from "./mcp";
export { AutoMemory, AutoMemoryKnowledgeBase, AutoMemoryVectorStore, DEFAULT_POLICIES, createAutoMemory, createLLMSummarizer } from "./memory/auto-memory";
export { DocsManager, createDocsManager } from "./memory/docs-manager";
export { FileMemory, createFileMemory } from "./memory/file-memory";
export { MemoryHooks, createEncryptionHooks, createLoggingHooks, createMemoryHooks, createValidationHooks } from "./memory/hooks";
export { Memory, createMemory } from "./memory/memory";
export type { MemoryConfig, MemoryEntry } from "./memory/memory";
export { RulesManager, createRulesManager, createSafetyRules } from "./memory/rules-manager";
export { // Adapters
  NoopObservabilityAdapter, // Constants
  OBSERVABILITY_TOOLS, // Global adapter management
  setObservabilityAdapter, // Types
  type SpanKind, ConsoleObservabilityAdapter, MemoryObservabilityAdapter, clearAdapterCache, createConsoleAdapter, createMemoryAdapter, createObservabilityAdapter, getObservabilityAdapter, getObservabilityToolInfo, hasObservabilityToolEnvVar, listObservabilityTools, noopAdapter, resetObservabilityAdapter, trace } from "./observability";
export { AgentApp, AgentAppConfig, AgentAppProtocol, AgentOS, AgentOSConfig, AgentOSProtocol, DEFAULT_AGENTOS_CONFIG, mergeConfig } from "./os";
export type { AgentAppOptions, AgentOSOptions } from "./os";
export { // P0: Call Types (new)
  type MCPCall, // P0: Specialized Agent Configs (new)
  type AudioConfig, // P0: Specialized Agents (new)
  CodeAgent, // P1: Workflow Patterns (new)
  Chunking, // P2: Context Types (new - only items not already exported)
  ContextManager, // P3: Display callbacks (snake_case for Python parity)
  register_display_callback, // P3: Plugin functions
  get_plugin_manager, // P3: Trace & condition functions
  evaluate_condition, EmbeddingAgent, If, Knowledge, MCP, OCRAgent, Parallel, RealtimeAgent, Route, Session, VideoAgent, VisionAgent, async_display_callbacks, cleanup_telemetry_resources, create_context_agent, disable_performance_mode, disable_telemetry, discover_and_load_plugins, discover_plugins, display_error, display_generating, display_instruction, display_interaction, display_self_reflection, display_tool_call, enable_performance_mode, enable_telemetry, ensure_plugin_dir, error_logs, get_default_plugin_dirs, get_dimensions, get_plugin_template, get_telemetry, handoff_filters, load_plugin, parse_plugin_header, parse_plugin_header_from_file, prompt_with_handoff_instructions, resolve_guardrail_policies, sync_display_callbacks, trace_context, track_workflow, when } from "./parity";
export { // Core classes
  Plan, // Python parity additions
  ApprovalCallback, PlanStep, PlanStorage, PlanningAgent, READ_ONLY_TOOLS, RESEARCH_TOOLS, RESTRICTED_TOOLS, TaskAgent, TodoItem, TodoList, createApprovalCallback, createPlan, createPlanStorage, createPlanningAgent, createTaskAgent, createTodoList } from "./planning";
export { // Classes
  Plugin, // Enums
  PluginHook, // Functions
  getPluginManager, // Interfaces
  type PluginMetadata, FunctionPlugin, PluginManager, PluginParseError, PluginType, disablePlugins, discoverAndLoadPlugins, discoverPlugins, enablePlugins, ensurePluginDir, getDefaultPluginDirs, getPluginTemplate, isPluginEnabled, listPlugins, loadPlugin, parsePluginHeader, parsePluginHeaderFromFile } from "./plugins";
export { // A2A Protocol
  A2ATaskState, // AGUI Protocol
  AGUI, // AgentManager alias type
  type AgentManager, // Global singletons
  config, // Guardrail policies
  type GuardrailPolicy, // Tools class
  type ToolDefinition, A2A, A2ARole, AutoRagAgent, AutoRetrievalPolicy, DEFAULT_AUTO_KEYWORDS, GUARDRAIL_POLICY_PRESETS, Tools, memory, obs, resolveGuardrailPolicies, workflows } from "./protocols";
export { CitationsMode, DEFAULT_RAG_TEMPLATE, RAG, RAGCitation, RAGContextPack, RetrievalPolicy, RetrievalStrategy, createCitation, createContextPack, createRAG, createRAGConfig, createRAGResult, createRetrievalConfig, createSimpleRetrievalConfig, createSmartRetrievalConfig, formatAnswerWithCitations, formatCitation, formatContextPackForPrompt, hasCitations } from "./rag";
export { // Python parity additions
  SkillLoader, SkillManager, createSkillLoader, createSkillManager, createSkillProperties, parseSkillFile } from "./skills";
export { AgentTask, AgentTaskConfig, BaseTask, createTaskOutput } from "./task";
export { // Python parity additions
  MinimalTelemetry, AgentTelemetry, PerformanceMonitor, TelemetryCollector, TelemetryIntegration, cleanupTelemetry, cleanupTelemetryResources, createAgentTelemetry, createConsoleSink, createHTTPSink, createPerformanceMonitor, createTelemetryIntegration, disablePerformanceMode, disableTelemetry, enablePerformanceMode, enableTelemetry, getMinimalTelemetry, getTelemetry } from "./telemetry";
export { // Subagent Tool (agent-as-tool pattern)
  SubagentTool, BaseTool, FunctionTool, ToolRegistry, ToolResult, ToolValidationError, createDelegator, createSubagentTool, createSubagentTools, createTool, getRegistry, getTool, registerTool, tool, validateTool } from "./tools";
export { airweaveSearch, bedrockBrowserClick, bedrockBrowserFill, bedrockBrowserNavigate, bedrockCodeInterpreter, codeExecution, codeMode, createCustomTool, exaSearch, firecrawlCrawl, firecrawlScrape, parallelSearch, perplexitySearch, registerCustomTool, registerLocalTool, registerNpmTool, superagentGuard, superagentRedact, superagentVerify, tavilyCrawl, tavilyExtract, tavilySearch, valyuBioSearch, valyuCompanyResearch, valyuEconomicsSearch, valyuFinanceSearch, valyuPaperSearch, valyuPatentSearch, valyuSecSearch, valyuWebSearch } from "./tools/builtins";
export { MissingDependencyError, MissingEnvVarError, ToolsRegistry, composeMiddleware, createLoggingMiddleware, createRateLimitMiddleware, createRedactionMiddleware, createRetryMiddleware, createTimeoutMiddleware, createToolsRegistry, createTracingMiddleware, createValidationMiddleware, getToolsRegistry, resetToolsRegistry } from "./tools/registry";
export type { InstallHints, PraisonTool, RedactionHooks, RegisteredTool, ToolCapabilities, ToolExecutionContext, ToolExecutionResult, ToolFactory, ToolHooks, ToolInstallStatus, ToolLimits, ToolLogger, ToolMetadata, ToolMiddleware, ToolParameterProperty, ToolParameterSchema } from "./tools/registry";
export { registerBuiltinTools, tools } from "./tools/tools";
export { // Classes
  TraceSink, // Enums
  ContextEventType, // Functions
  createContextEvent, // Types
  type ContextEvent, ContextListSink, ContextNoOpSink, ContextTraceEmitter, ContextTraceSink, EventType, MessageType, TraceCtx, traceContext, trackWorkflow } from "./trace";
export { // New: Python-parity Loop and Repeat classes
  Loop, // Task class
  Task, AgentFlow, Pipeline, Repeat, Workflow, loop, loopPattern, parallel, repeat, repeatPattern, route } from "./workflows";
export type { LoopConfig, LoopResult, RepeatConfig, RepeatContext, RepeatResult, StepContextConfig, StepExecutionConfig, StepOutputConfig, StepResult, StepRoutingConfig, TaskConfig, WorkflowContext } from "./workflows";
export { createWorkflowFromYAML, loadWorkflowFromFile, parseYAMLWorkflow, validateWorkflowDefinition } from "./workflows/yaml-parser";
```

# Optional Plugins

External tools are available via `praisonai-tools` package:

```bash
pip install praisonai-tools
```

See [PraisonAI-tools](https://github.com/MervinPraison/PraisonAI-tools) for available tools.


## Links discovered
- [PraisonAI-tools](https://github.com/MervinPraison/PraisonAI-tools)
- [from_tuple](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/guardrails/guardrail_result.py)
- [from_dict](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/handoff.py)
- [to_dict](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/handoff.py)
- [json](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/main.py)
- [to_dict](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/main.py)
- [to_dict](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/tools/base.py)
- [achat](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [aexecute](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [agent_id](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [analyze_prompt](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [arun](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [astart](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [auto_memory](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [background](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [chat](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [chat_with_context](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [checkpoints](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [clean_json_output](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [clear_history](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [console](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [context_manager](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [delete_history](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [delete_history_matching](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [display_name](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [ephemeral](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [execute](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [execute_tool](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [execute_tool_async](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [from_template](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [generate_task](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [get_available_tools](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [get_history_size](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [get_learn_context](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [get_memory_context](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [get_recommended_stage](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [get_rules_context](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [get_skills_prompt](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [handoff_to](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [handoff_to_async](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [iter_stream](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [launch](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [llm_model](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [output_style](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [policy](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [prune_history](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [query](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [rag](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [rag_query](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [retrieval_config](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [retrieve](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [rules_manager](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [run](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [run_autonomous](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [run_autonomous_async](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [run_until](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [run_until_async](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [session_id](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [skill_manager](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [start](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [store_memory](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [stream_emitter](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [switch_model](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [thinking_budget](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/agent.py)
- [astart](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agents/autoagents.py)
- [start](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agents/autoagents.py)
- [achat](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agents/auto_rag_agent.py)
- [chat](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agents/auto_rag_agent.py)
- [name](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agents/auto_rag_agent.py)
- [rag](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agents/auto_rag_agent.py)
- [aanalyze_codebase](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/context_agent.py)
- [acreate_implementation_blueprint](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/context_agent.py)
- [agenerate_prp](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/context_agent.py)
- [analyze_codebase](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/context_agent.py)
- [analyze_codebase_with_gitingest](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/context_agent.py)
- [analyze_integration_points](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/context_agent.py)
- [analyze_test_patterns](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/context_agent.py)
- [build_implementation_blueprint](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/context_agent.py)
- [compile_context_documentation](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/context_agent.py)
- [create_implementation_blueprint](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/context_agent.py)
- [create_quality_gates](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/context_agent.py)
- [create_validation_framework](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/context_agent.py)
- [execute_prp](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/context_agent.py)
- [extract_implementation_patterns](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/context_agent.py)
- [generate_comprehensive_prp](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/context_agent.py)
- [generate_feature_prp](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/context_agent.py)
- [generate_prp](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/context_agent.py)
- [get_agent_interaction_summary](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/context_agent.py)
- [log_debug](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/context_agent.py)
- [perform_ast_analysis](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/context_agent.py)
- [save_comprehensive_session_report](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/context_agent.py)
- [save_markdown_output](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/context_agent.py)
- [setup_logging](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/context_agent.py)
- [setup_output_directories](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/context_agent.py)
- [start](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/context_agent.py)
- [aresearch](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/deep_research_agent.py)
- [async_openai_client](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/deep_research_agent.py)
- [clarify](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/deep_research_agent.py)
- [follow_up](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/deep_research_agent.py)
- [gemini_client](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/praisonaiagents/agent/deep_research_agent.py)

--- src/praisonai/api.py ---
from flask import Flask
import markdown

app = Flask(__name__)

def basic():
    from praisonai import PraisonAI
    praisonai = PraisonAI(agent_file="agents.yaml")
    return praisonai.run()

@app.route('/')
def home():
    output = basic()
    html_output = markdown.markdown(output)
    return f'<html><body>{html_output}</body></html>'

if __name__ == "__main__":
    app.run(debug=True)


--- src/praisonai/api_server.py ---
"""
Auto-generated API server for PraisonAI agents.
"""
from flask import Flask, request, jsonify
from flask_cors import CORS
from praisonai import PraisonAI
import os

app = Flask(__name__)

# CORS configuration
CORS(app)

# Authentication
AUTH_ENABLED = False
AUTH_TOKEN = None

def check_auth():
    """Check authentication if enabled."""
    if not AUTH_ENABLED:
        return True
    
    token = request.headers.get('Authorization', '').replace('Bearer ', '')
    return token == AUTH_TOKEN

@app.route('/health', methods=['GET'])
def health():
    """Health check endpoint."""
    return jsonify({"status": "ok", "service": "praisonai-api"})

@app.route('/chat', methods=['POST'])
def chat():
    """Chat endpoint for agent interaction."""
    if not check_auth():
        return jsonify({"error": "Unauthorized"}), 401
    
    data = request.get_json()
    if not data or 'message' not in data:
        return jsonify({"error": "Message required"}), 400
    
    try:
        praisonai = PraisonAI(agent_file="agents.yaml")
        result = praisonai.run()
        
        return jsonify({
            "response": result,
            "status": "success"
        })
    except Exception as e:
        return jsonify({
            "error": str(e),
            "status": "error"
        }), 500

@app.route('/agents', methods=['GET'])
def list_agents():
    """List available agents."""
    if not check_auth():
        return jsonify({"error": "Unauthorized"}), 401
    
    return jsonify({
        "agents": ["default"],
        "agent_file": "agents.yaml"
    })

if __name__ == '__main__':
    app.run(
        host='0.0.0.0',
        port=8080,
        debug=False
    )


--- src/praisonai-agents/multi-agents-api.py ---
from praisonaiagents import Agent, AgentTeam, Tools

research_agent = Agent(name="Research", instructions="You are a research agent to search internet about AI 2024", tools=[Tools.internet_search])
summarise_agent = Agent(name="Summarise", instructions="You are a summarize agent to summarise in points")
agents = AgentTeam(agents=[research_agent, summarise_agent])
agents.launch(path="/agents", port=3030)

--- src/praisonai-agents/simple-api-mcp.py ---
from praisonaiagents import Agent, MCP

search_agent = Agent(
    instructions="""You are a Tweet.""",
    llm="openai/gpt-5-nano",
    tools=MCP("http://localhost:8080/sse")
)
search_agent.launch(path="/tweet", port=3030)

--- src/praisonai/scripts/generate_api_md.py ---
#!/usr/bin/env python3
"""
Generate api.md for PraisonAI.

This script generates a comprehensive API reference document
covering all public exports from praisonaiagents, praisonai, CLI, and TypeScript.

Usage:
    ./scripts/generate_api_md.py           # Generate api.md
    ./scripts/generate_api_md.py --check   # Check if api.md is up to date
    ./scripts/generate_api_md.py --stdout  # Print to stdout
"""

import sys
from pathlib import Path

# Add the praisonai package to path
# Script is in src/praisonai/scripts/, so go up 3 levels to repo root
repo_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(repo_root / "src" / "praisonai"))

from praisonai._dev.api_md import generate_api_md

if __name__ == '__main__':
    # Parse simple args
    check = '--check' in sys.argv
    stdout = '--stdout' in sys.argv
    
    exit_code = generate_api_md(
        repo_root=repo_root,
        check=check,
        stdout=stdout
    )
    sys.exit(exit_code)


--- src/praisonai-agents/tests/multi-agent-api.py ---
from praisonaiagents import Agent

weather_agent = Agent(
    instructions="""You are a weather agent that can provide weather information for a given city.""",
    llm="gpt-4o-mini"
)

stock_agent = Agent(
    instructions="""You are a stock market agent that can provide information about stock prices and market trends.""",
    llm="gpt-4o-mini"
)

travel_agent = Agent(
    instructions="""You are a travel agent that can provide recommendations for destinations, hotels, and activities.""",
    llm="gpt-4o-mini"
)

weather_agent.launch(path="/weather", port=3030)
stock_agent.launch(path="/stock", port=3030)
travel_agent.launch(path="/travel", port=3030) 

--- src/praisonai-agents/tests/multi-agents-api.py ---
from praisonaiagents import Agent, AgentTeam, Tools

research_agent = Agent(name="Research", instructions="You are a research agent to search internet about AI 2024", tools=[Tools.internet_search])
summarise_agent = Agent(name="Summarise", instructions="You are a summarize agent to summarise in points")
agents = AgentTeam(agents=[research_agent, summarise_agent])
agents.launch(path="/agents", port=3030)

--- src/praisonai-agents/tests/multi-agents-group-api.py ---
from praisonaiagents import Agent, AgentTeam, Tools

research_agent = Agent(name="Research", instructions="You are a research agent to search internet about AI 2024", tools=[Tools.internet_search])
summarise_agent = Agent(name="Summarise", instructions="You are a summarize agent to summarise in points")
agents = AgentTeam(agents=[research_agent, summarise_agent])
agents2 = AgentTeam(agents=[research_agent])
agents.launch(path="/agents", port=3030)
agents2.launch(path="/agents2", port=3030)

--- src/praisonai-agents/tests/simple-api.py ---
from praisonaiagents import Agent

agent = Agent(instructions="""You are a helpful assistant.""", llm="gpt-4o-mini")
agent.launch(path="/ask", port=3030)

--- src/praisonai-ts/tests/development/concepts/multi-agent.ts ---
import { Agent, PraisonAIAgents } from '../../src/agent';

async function main() {
    // Create multiple agents with different roles
    const researchAgent = new Agent({
        name: "ResearchAgent",
        instructions: "Research and provide detailed information about renewable energy sources.",
        verbose: true
    });

    const summaryAgent = new Agent({
        name: "SummaryAgent",
        instructions: "Create a concise summary of the research findings about renewable energy sources. Use {previous_result} as input.",
        verbose: true
    });

    const recommendationAgent = new Agent({
        name: "RecommendationAgent",
        instructions: "Based on the summary in {previous_result}, provide specific recommendations for implementing renewable energy solutions.",
        verbose: true
    });

    // Run the agents in sequence
    const praisonAI = new PraisonAIAgents({
        agents: [researchAgent, summaryAgent, recommendationAgent],
        tasks: [
            "Research and analyze current renewable energy technologies and their implementation.",
            "Summarize the key findings from the research.",
            "Provide actionable recommendations based on the summary."
        ],
        verbose: true,
        process: 'sequential'  // Agents will run in sequence, passing results to each other
    });

    try {
        console.log('Starting multi-agent example...');
        const results = await praisonAI.start();
        console.log('\nFinal Results:');
        console.log('Research Results:', results[0]);
        console.log('\nSummary Results:', results[1]);
        console.log('\nRecommendation Results:', results[2]);
    } catch (error) {
        console.error('Error:', error);
    }
}

// Run the example
if (require.main === module) {
    main();
}


--- src/praisonai-ts/tests/development/concepts/single-agent.ts ---
import { Agent, PraisonAIAgents } from '../../src/agent';

async function main() {
    // Create a simple agent (no task specified)
    const agent = new Agent({
        name: "BiologyExpert",
        instructions: "Explain the process of photosynthesis in detail.",
        verbose: true
    });

    // Run the agent
    const praisonAI = new PraisonAIAgents({
        agents: [agent],
        tasks: ["Explain the process of photosynthesis in detail."],
        verbose: true
    });

    try {
        console.log('Starting single agent example...');
        const results = await praisonAI.start();
        console.log('\nFinal Results:', results);
    } catch (error) {
        console.error('Error:', error);
    }
}

// Run the example
if (require.main === module) {
    main();
}


--- src/praisonai-ts/tests/development/concepts/task-based-agent.ts ---
import { Agent, Task, PraisonAIAgents } from '../../src/agent';

async function main() {
    // Create agents first
    const dietAgent = new Agent({
        name: "DietAgent",
        role: "Nutrition Expert",
        goal: "Create healthy and delicious recipes",
        backstory: "You are a certified nutritionist with years of experience in creating balanced meal plans.",
        verbose: true,  // Enable streaming output
        instructions: `You are a professional chef and nutritionist. Create 5 healthy food recipes that are both nutritious and delicious.
Each recipe should include:
1. Recipe name
2. List of ingredients with quantities
3. Step-by-step cooking instructions
4. Nutritional information
5. Health benefits

Format your response in markdown.`
    });

    const blogAgent = new Agent({
        name: "BlogAgent",
        role: "Food Blogger",
        goal: "Write engaging blog posts about food and recipes",
        backstory: "You are a successful food blogger known for your ability to make recipes sound delicious and approachable.",
        verbose: true,  // Enable streaming output
        instructions: `You are a food and health blogger. Write an engaging blog post about the provided recipes.
The blog post should:
1. Have an engaging title
2. Include an introduction about healthy eating
3. Discuss each recipe and its unique health benefits
4. Include tips for meal planning and preparation
5. End with a conclusion encouraging healthy eating habits

Use the following recipes as input:
{recipes}

Format your response in markdown.`
    });

    // Then create tasks and assign agents to them
    const recipeTask = new Task({
        name: "Create Recipes",
        description: "Create 5 healthy food recipes that are both nutritious and delicious",
        expected_output: "A list of 5 detailed recipes with ingredients and instructions",
        agent: dietAgent
    });

    const blogTask = new Task({
        name: "Write Blog Post",
        description: "Write an engaging blog post about the provided recipes",
        expected_output: "A well-structured blog post discussing the recipes and their health benefits",
        dependencies: [recipeTask],
        agent: blogAgent
    });

    // Run the tasks
    const praisonAI = new PraisonAIAgents({
        agents: [dietAgent, blogAgent],
        tasks: [recipeTask, blogTask],
        verbose: true,
        process: 'hierarchical'
    });

    try {
        console.log('Starting task-based agent example...');
        const results = await praisonAI.start();
        console.log('\nFinal Results:');
        console.log('Recipe Task Results:', results[0]);
        console.log('\nBlog Task Results:', results[1]);
    } catch (error) {
        console.error('Error:', error);
    }
}

// Run the example
if (require.main === module) {
    main();
}


--- src/praisonai-agents/benchmarks/BENCHMARK_RESULTS.md ---
# PraisonAI Agents - Benchmark Results

**Generated:** 2025-12-18 14:40:22
**Iterations:** 100
**Test:** Agent instantiation (without tools)

## Results

| Framework | Avg Time (Î¼s) | Relative |
|-----------|---------------|----------|
| **PraisonAI** | **3.77** | **1.00x (fastest)** |
| OpenAI Agents SDK | 5.26 | 1.39x |
| Agno | 5.64 | 1.49x |
| PraisonAI (LiteLLM) | 7.56 | 2.00x |
| PydanticAI | 226.94 | 60.16x |
| LangGraph | 4,558.71 | 1,209x |
| CrewAI | 15,607.92 | 4,138x |

## Package Versions

| Package | Version |
|---------|--------|
| PraisonAI | 0.1.5 |
| Agno | 2.3.14 |
| PydanticAI | 1.35.0 |
| OpenAI Agents SDK | 0.6.3 |
| LangGraph | 1.0.5 |
| CrewAI | 1.6.1 |

## How to Reproduce

```bash
cd praisonai-agents
python benchmarks/simple_benchmark.py
```


--- src/praisonai/benchmarks/BENCHMARK_RESULTS.md ---
# PraisonAI CLI - Benchmark Results

**Generated:** 2025-12-19 16:04:40

## Import Times

| Module | Time (ms) | Target | Status |
|--------|-----------|--------|--------|
| praisonai | 4.11 | <10 | âœ… |
| message_queue | 6.87 | <50 | âœ… |
| at_mentions | 0.58 | <50 | âœ… |
| profiler | 0.90 | <10 | âœ… |

## Instantiation Times

| Class | Time (Î¼s) | Target | Status |
|-------|-----------|--------|--------|
| MessageQueue | 0.26 | <100 | âœ… |
| StateManager | 0.30 | <100 | âœ… |
| FileSearchService | 8.42 | <100 | âœ… |

## Lazy Loading

| Operation | Time (ms) |
|-----------|----------|
| PraisonAI lazy load | 3886.47 |

## How to Reproduce

```bash
cd praisonai
python benchmarks/simple_benchmark.py
```


--- src/praisonai/scripts/bump_and_release.py ---
#!/usr/bin/env python3
"""
Combined bump version and release script for PraisonAI package.

Usage:
    # Basic: bump praisonai to 3.8.2 with agents 0.11.8
    python scripts/bump_and_release.py --agents 0.11.8 3.8.2
    
    # Wait for agents version to propagate to PyPI (recommended after publishing agents)
    python scripts/bump_and_release.py --agents 0.11.8 --wait 3.8.2
    
    # Auto-detect latest agents version from PyPI
    python scripts/bump_and_release.py --auto 3.8.2

This script:
1. Validates pre-flight conditions (clean git state, versions)
2. Optionally waits for praisonaiagents to be available on PyPI
3. Bumps version in all required files
4. Copies root README.md to package dir
5. Runs uv lock & uv build
6. Commits changes
7. Creates git tag
8. Pushes to GitHub (with rebase if needed)
9. Creates GitHub release (latest)
"""

import re
import sys
import time
import json
import argparse
import subprocess
import shutil
import urllib.request
from pathlib import Path
from typing import Optional


def get_project_root() -> Path:
    """Get the project root directory (praisonai-package)."""
    return Path(__file__).parent.parent.parent.parent


def get_praisonai_dir() -> Path:
    """Get the praisonai package directory."""
    return get_project_root() / "src/praisonai"


def run(cmd: list[str], cwd: Optional[Path] = None, check: bool = True, silent: bool = False) -> subprocess.CompletedProcess:
    """Run a command and print it."""
    if not silent:
        print(f"$ {' '.join(cmd)}")
    result = subprocess.run(cmd, cwd=cwd, capture_output=True, text=True)
    if not silent:
        if result.stdout:
            print(result.stdout)
        if result.stderr:
            print(result.stderr, file=sys.stderr)
    if check and result.returncode != 0:
        print(f"âŒ Command failed with exit code {result.returncode}")
        sys.exit(1)
    return result


def get_pypi_version(package: str) -> Optional[str]:
    """Get the latest version of a package from PyPI."""
    try:
        url = f"https://pypi.org/pypi/{package}/json"
        with urllib.request.urlopen(url, timeout=10) as response:
            data = json.loads(response.read().decode())
            return data["info"]["version"]
    except Exception as e:
        print(f"  âš ï¸  Failed to fetch PyPI version: {e}")
        return None


def wait_for_pypi_version(package: str, version: str, max_wait: int = 300, interval: int = 30) -> bool:
    """Wait for a specific version to be available on PyPI."""
    print(f"\nâ³ Waiting for {package}=={version} to be available on PyPI...")
    start = time.time()
    
    while time.time() - start < max_wait:
        current = get_pypi_version(package)
        if current == version:
            print(f"  âœ… {package}=={version} is now available on PyPI")
            return True
        
        elapsed = int(time.time() - start)
        remaining = max_wait - elapsed
        print(f"  â³ Current: {current}, waiting for: {version} ({remaining}s remaining...)")
        time.sleep(interval)
    
    print(f"  âŒ Timeout waiting for {package}=={version} on PyPI")
    return False


def check_git_status() -> bool:
    """Check if there are uncommitted changes."""
    root = get_project_root()
    result = run(["git", "status", "--porcelain"], cwd=root, check=False, silent=True)
    if result.stdout.strip():
        return True  # Has uncommitted changes
    return False


def update_file(filepath: Path, patterns: list[tuple[str, str]], root: Path) -> bool:
    """Update a file with the given regex patterns and replacements."""
    if not filepath.exists():
        print(f"  âš ï¸  File not found: {filepath}")
        return False
    
    content = filepath.read_text()
    original = content
    
    for pattern, replacement in patterns:
        content = re.sub(pattern, replacement, content)
    
    if content != original:
        filepath.write_text(content)
        print(f"  âœ… Updated: {filepath.relative_to(root)}")
        return True
    else:
        print(f"  â­ï¸  No changes: {filepath.relative_to(root)}")
        return False


def bump_version(new_version: str, agents_version: Optional[str] = None):
    """Bump version in all required files."""
    root = get_project_root()
    praisonai_dir = get_praisonai_dir()
    
    print(f"\nðŸš€ Bumping PraisonAI version to {new_version}\n")
    
    # 1. Update version.py (single source of truth)
    print("ðŸ“¦ Python Package:")
    update_file(
        praisonai_dir / "praisonai/version.py",
        [(r'__version__ = "[^"]+"', f'__version__ = "{new_version}"')],
        root
    )
    
    # 2. Update deploy.py (Dockerfile template)
    print("\nðŸ³ Deploy Script:")
    update_file(
        praisonai_dir / "praisonai/deploy.py",
        [(r'praisonai==[0-9.]+', f'praisonai=={new_version}')],
        root
    )
    
    # 3. Update Dockerfiles
    print("\nðŸ³ Dockerfiles:")
    dockerfiles = [
        "docker/Dockerfile",
        "docker/Dockerfile.chat",
        "docker/Dockerfile.dev",
        "docker/Dockerfile.ui",
    ]
    for dockerfile in dockerfiles:
        update_file(
            root / dockerfile,
            [(r'"praisonai>=[0-9.]+"', f'"praisonai>={new_version}"')],
            root
        )
    
    # 4. Update Homebrew formula
    print("\nðŸº Homebrew Formula:")
    update_file(
        praisonai_dir / "praisonai.rb",
        [(r'v[0-9]+\.[0-9]+\.[0-9]+', f'v{new_version}')],
        root
    )
    
    # 5. Update praisonaiagents dependency if specified
    if agents_version:
        print(f"\nðŸ“¦ Updating praisonaiagents dependency to {agents_version}:")
        update_file(
            praisonai_dir / "pyproject.toml",
            [(r'praisonaiagents>=[0-9.]+', f'praisonaiagents>={agents_version}')],
            root
        )
    
    print("\nâœ¨ Version bump complete!")


def validate_dependencies(max_retries: int = 3, retry_interval: int = 60) -> bool:
    """Validate that uv lock will succeed, with retry logic for PyPI propagation."""
    praisonai_dir = get_praisonai_dir()
    
    for attempt in range(max_retries):
        print(f"\nðŸ” Validating dependencies (attempt {attempt + 1}/{max_retries})...")
        
        # Clear cache before validation
        run(["uv", "cache", "clean"], cwd=praisonai_dir, silent=True)
        
        result = run(["uv", "lock", "--dry-run"], cwd=praisonai_dir, check=False)
        if result.returncode == 0:
            print("  âœ… Dependencies validated successfully")
            return True
        
        if attempt < max_retries - 1:
            print(f"\nâ³ Waiting {retry_interval}s for PyPI propagation before retry...")
            time.sleep(retry_interval)
    
    print("\nâŒ Dependency validation failed after all retries.")
    return False


def release(version: str):
    """Run the release process."""
    root = get_project_root()
    praisonai_dir = get_praisonai_dir()
    tag = f"v{version}"
    
    print(f"\nðŸš€ Releasing PraisonAI {tag}\n")
    
    # 1. Copy root README.md to package dir for PyPI
    print("ðŸ“„ Copying README.md...")
    root_readme = root / "README.md"
    pkg_readme = praisonai_dir / "README.md"
    if root_readme.exists():
        shutil.copy(root_readme, pkg_readme)
        print(f"  âœ… Copied {root_readme} -> {pkg_readme}")
    
    # 2. Clear uv cache and run uv lock
    print("\nðŸ§¹ Clearing uv cache...")
    run(["uv", "cache", "clean"], cwd=praisonai_dir)
    
    print("\nðŸ“¦ Running uv lock...")
    run(["uv", "lock"], cwd=praisonai_dir)
    
    # 3. uv build
    print("\nðŸ”¨ Running uv build...")
    run(["uv", "build"], cwd=praisonai_dir)
    
    # 4. Git add and commit
    print("\nðŸ“ Committing changes...")
    run(["git", "add", "-A"], cwd=root)
    run(["git", "commit", "-m", f"Release {tag}"], cwd=root, check=False)
    
    # 5. Create git tag
    print(f"\nðŸ·ï¸  Creating tag {tag}...")
    run(["git", "tag", "-f", tag], cwd=root)
    
    # 6. Pull rebase and push to GitHub
    print("\nâ¬†ï¸  Pushing to GitHub...")
    # First fetch and rebase to handle any remote changes (e.g., auto-generated api.md)
    result = run(["git", "pull", "--rebase", "origin", "main"], cwd=root, check=False)
    if result.returncode != 0:
        print("  âš ï¸  Rebase failed, trying to continue...")
    
    # Recreate tag after rebase (commit hash may have changed)
    run(["git", "tag", "-f", tag], cwd=root)
    
    # Push changes
    run(["git", "push"], cwd=root)
    run(["git", "push", "--tags", "-f"], cwd=root)
    
    # 7. Create GitHub release
    print(f"\nðŸŽ‰ Creating GitHub release {tag}...")
    run([
        "gh", "release", "create", tag,
        "--title", f"PraisonAI {tag}",
        "--notes", f"Release {tag}",
        "--latest"
    ], cwd=root)
    
    print(f"\nâœ… Released PraisonAI {tag}")
    print("\nNext step:")
    print("  cd src/praisonai && uv publish")


def main():
    parser = argparse.ArgumentParser(
        description="Bump version and release PraisonAI package",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Bump to 3.8.2 with agents 0.11.8
  python scripts/bump_and_release.py --agents 0.11.8 3.8.2
  
  # Wait for agents to propagate to PyPI first (recommended)
  python scripts/bump_and_release.py --agents 0.11.8 --wait 3.8.2
  
  # Auto-detect latest agents version from PyPI
  python scripts/bump_and_release.py --auto 3.8.2
"""
    )
    parser.add_argument(
        "version",
        help="New version number (e.g., 3.8.2)"
    )
    parser.add_argument(
        "--agents", "-a",
        help="Update praisonaiagents dependency version (e.g., 0.11.8)",
        default=None
    )
    parser.add_argument(
        "--auto",
        action="store_true",
        help="Auto-detect latest praisonaiagents version from PyPI"
    )
    parser.add_argument(
        "--wait", "-w",
        action="store_true",
        help="Wait for the specified agents version to be available on PyPI"
    )
    parser.add_argument(
        "--max-wait",
        type=int,
        default=300,
        help="Maximum seconds to wait for PyPI propagation (default: 300)"
    )
    parser.add_argument(
        "--retries", "-r",
        type=int,
        default=3,
        help="Number of retries for dependency validation (default: 3)"
    )
    parser.add_argument(
        "--force", "-f",
        action="store_true",
        help="Skip pre-flight checks (use with caution)"
    )
    
    args = parser.parse_args()
    
    # Validate version format
    if not re.match(r'^\d+\.\d+\.\d+$', args.version):
        print(f"âŒ Invalid version format: {args.version}")
        print("   Expected format: X.Y.Z (e.g., 3.8.2)")
        sys.exit(1)
    
    # Handle --auto flag
    if args.auto:
        print("\nðŸ” Auto-detecting latest praisonaiagents version from PyPI...")
        agents_version = get_pypi_version("praisonaiagents")
        if not agents_version:
            print("âŒ Failed to auto-detect praisonaiagents version")
            sys.exit(1)
        print(f"  âœ… Latest version: {agents_version}")
        args.agents = agents_version
    
    # Validate agents version format if provided
    if args.agents and not re.match(r'^\d+\.\d+\.\d+$', args.agents):
        print(f"âŒ Invalid agents version format: {args.agents}")
        print("   Expected format: X.Y.Z (e.g., 0.11.8)")
        sys.exit(1)
    
    # Pre-flight checks
    if not args.force:
        print("\nðŸ” Pre-flight checks...")
        
        # Check for uncommitted changes (warning only)
        if check_git_status():
            print("  âš ï¸  Warning: You have uncommitted changes")
        else:
            print("  âœ… Git working directory is clean")
    
    # Wait for PyPI propagation if requested
    if args.wait and args.agents:
        if not wait_for_pypi_version("praisonaiagents", args.agents, max_wait=args.max_wait):
            print("\nðŸ’¡ Tip: Check if the package was published successfully")
            print("ðŸ’¡ Tip: You can retry without --wait if the package is confirmed published")
            sys.exit(1)
    
    # Run bump version
    bump_version(args.version, args.agents)
    
    # Validate dependencies after version bump (with retries)
    if not validate_dependencies(max_retries=args.retries):
        print("\nðŸ’¡ Tip: Revert changes with 'git checkout .' if needed")
        print("ðŸ’¡ Tip: The package may need more time to propagate to PyPI")
        sys.exit(1)
    
    # Run release
    release(args.version)


if __name__ == "__main__":
    main()


--- src/praisonai-agents/benchmarks/deep_profiling.py ---
#!/usr/bin/env python3
"""
PraisonAI Agents - Deep Profiling Benchmark

Profiles each step of agent initialization to identify performance bottlenecks.

Key Optimizations Applied:
1. Lazy RulesManager - filesystem operations deferred until first access
2. Lazy os.getcwd() - fast_context_path resolved only when needed
3. Lazy console initialization - Rich Console created on demand

Advanced Profiling Features:
- Line-by-line profiling of __init__ method
- Call graph analysis
- Memory allocation tracking
- Function call counting
- Attribute access profiling

Usage:
    python benchmarks/deep_profiling.py
    python benchmarks/deep_profiling.py --deep  # More detailed profiling
"""

import time
import tracemalloc
import cProfile
import pstats
import io
from typing import Literal, Callable, Dict, List
from dataclasses import dataclass

# ============================================================================
# Timing Utilities
# ============================================================================

@dataclass
class StepTiming:
    name: str
    duration_us: float  # microseconds
    memory_kb: float    # kilobytes


class StepProfiler:
    """Profile individual steps during initialization"""
    
    def __init__(self):
        self.steps: List[StepTiming] = []
        self._start_time = None
        self._start_memory = None
    
    def start_step(self, name: str):
        tracemalloc.start()
        self._start_time = time.perf_counter()
        self._step_name = name
    
    def end_step(self):
        end_time = time.perf_counter()
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        
        duration_us = (end_time - self._start_time) * 1_000_000
        memory_kb = peak / 1024
        
        self.steps.append(StepTiming(
            name=self._step_name,
            duration_us=duration_us,
            memory_kb=memory_kb
        ))
    
    def print_report(self, title: str):
        print(f"\n{'='*70}")
        print(f"STEP-BY-STEP PROFILING: {title}")
        print(f"{'='*70}")
        
        total_time = sum(s.duration_us for s in self.steps)
        total_memory = sum(s.memory_kb for s in self.steps)
        
        print(f"\n{'Step':<40} {'Time (Î¼s)':<15} {'Memory (KB)':<15} {'% Time':<10}")
        print("-" * 80)
        
        for step in sorted(self.steps, key=lambda x: x.duration_us, reverse=True):
            pct = (step.duration_us / total_time * 100) if total_time > 0 else 0
            print(f"{step.name:<40} {step.duration_us:<15.2f} {step.memory_kb:<15.2f} {pct:<10.1f}%")
        
        print("-" * 80)
        print(f"{'TOTAL':<40} {total_time:<15.2f} {total_memory:<15.2f} {'100.0':<10}%")


def time_function(func: Callable, iterations: int = 100) -> Dict:
    """Time a function with detailed statistics"""
    times = []
    
    # Warmup
    for _ in range(10):
        func()
    
    # Measure
    for _ in range(iterations):
        start = time.perf_counter()
        func()
        end = time.perf_counter()
        times.append((end - start) * 1_000_000)  # Convert to microseconds
    
    return {
        'avg': sum(times) / len(times),
        'min': min(times),
        'max': max(times),
        'median': sorted(times)[len(times)//2]
    }


# ============================================================================
# Profile PraisonAI Agent Initialization - Step by Step
# ============================================================================

def profile_praisonai_detailed():
    """Profile each step of PraisonAI Agent initialization"""
    print("\n" + "="*70)
    print("DETAILED PROFILING: PraisonAI Agent")
    print("="*70)
    
    profiler = StepProfiler()
    
    # Step 1: Import
    profiler.start_step("1. Import praisonaiagents")
    from praisonaiagents import Agent as PraisonAgent
    profiler.end_step()
    
    # Step 2: Define tool
    profiler.start_step("2. Define tool function")
    def get_weather(city: Literal["nyc", "sf"]):
        """Use this to get weather information."""
        if city == "nyc":
            return "It might be cloudy in nyc"
        elif city == "sf":
            return "It's always sunny in sf"
    tools = [get_weather]
    profiler.end_step()
    
    # Step 3: Full instantiation
    profiler.start_step("3. Full Agent instantiation")
    agent = PraisonAgent(
        name="Test Agent",
        instructions="Be concise, reply with one sentence.",
        llm="gpt-4o-mini",
        tools=tools,
        output="silent"
    )
    profiler.end_step()
    
    profiler.print_report("PraisonAI Agent")
    
    # Now profile the __init__ internals using cProfile
    print("\n" + "="*70)
    print("cProfile ANALYSIS: PraisonAI Agent.__init__")
    print("="*70)
    
    pr = cProfile.Profile()
    pr.enable()
    
    for _ in range(100):
        agent = PraisonAgent(
            name="Test Agent",
            instructions="Be concise, reply with one sentence.",
            llm="gpt-4o-mini",
            tools=tools,
            output="silent"
        )
    
    pr.disable()
    
    # Print top 30 functions by cumulative time
    s = io.StringIO()
    ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')
    ps.print_stats(30)
    print(s.getvalue())
    
    return agent


# ============================================================================
# Profile Agno Agent Initialization - Step by Step
# ============================================================================

def profile_agno_detailed():
    """Profile each step of Agno Agent initialization"""
    print("\n" + "="*70)
    print("DETAILED PROFILING: Agno Agent")
    print("="*70)
    
    profiler = StepProfiler()
    
    # Step 1: Import
    profiler.start_step("1. Import agno")
    from agno.agent import Agent as AgnoAgent
    from agno.models.openai import OpenAIChat
    profiler.end_step()
    
    # Step 2: Define tool
    profiler.start_step("2. Define tool function")
    def get_weather(city: Literal["nyc", "sf"]):
        """Use this to get weather information."""
        if city == "nyc":
            return "It might be cloudy in nyc"
        elif city == "sf":
            return "It's always sunny in sf"
    tools = [get_weather]
    profiler.end_step()
    
    # Step 3: Full instantiation
    profiler.start_step("3. Full Agent instantiation")
    agent = AgnoAgent(model=OpenAIChat(id="gpt-4o-mini"), tools=tools)
    profiler.end_step()
    
    profiler.print_report("Agno Agent")
    
    # Now profile the __init__ internals using cProfile
    print("\n" + "="*70)
    print("cProfile ANALYSIS: Agno Agent.__init__")
    print("="*70)
    
    pr = cProfile.Profile()
    pr.enable()
    
    for _ in range(100):
        agent = AgnoAgent(model=OpenAIChat(id="gpt-4o-mini"), tools=tools)
    
    pr.disable()
    
    # Print top 30 functions by cumulative time
    s = io.StringIO()
    ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')
    ps.print_stats(30)
    print(s.getvalue())
    
    return agent


# ============================================================================
# Profile Individual Components of PraisonAI
# ============================================================================

def profile_praisonai_components():
    """Profile individual components of PraisonAI Agent initialization"""
    print("\n" + "="*70)
    print("COMPONENT-LEVEL PROFILING: PraisonAI Agent")
    print("="*70)
    
    from praisonaiagents import Agent as PraisonAgent
    
    def get_weather(city: Literal["nyc", "sf"]):
        if city == "nyc":
            return "It might be cloudy in nyc"
        elif city == "sf":
            return "It's always sunny in sf"
    tools = [get_weather]
    
    # Test different configurations
    configs = [
        ("Minimal (no tools, no llm)", {"name": "Test", "output": "silent"}),
        ("With name only", {"name": "Test Agent", "output": "silent"}),
        ("With instructions", {"name": "Test", "instructions": "Be concise", "output": "silent"}),
        ("With llm (gpt-4o-mini)", {"name": "Test", "llm": "gpt-4o-mini", "output": "silent"}),
        ("With llm (openai/gpt-4o-mini)", {"name": "Test", "llm": "openai/gpt-4o-mini", "output": "silent"}),
        ("With tools", {"name": "Test", "tools": tools, "output": "silent"}),
        ("Full config (OpenAI SDK)", {"name": "Test", "instructions": "Be concise", "llm": "gpt-4o-mini", "tools": tools, "output": "silent"}),
        ("Full config (LiteLLM)", {"name": "Test", "instructions": "Be concise", "llm": "openai/gpt-4o-mini", "tools": tools, "output": "silent"}),
    ]
    
    print(f"\n{'Configuration':<40} {'Avg Time (Î¼s)':<15} {'Min (Î¼s)':<15} {'Max (Î¼s)':<15}")
    print("-" * 85)
    
    for name, config in configs:
        def create_agent():
            return PraisonAgent(**config)
        
        stats = time_function(create_agent, iterations=100)
        print(f"{name:<40} {stats['avg']:<15.2f} {stats['min']:<15.2f} {stats['max']:<15.2f}")


# ============================================================================
# Compare Specific Operations
# ============================================================================

def compare_specific_operations():
    """Compare specific operations between PraisonAI and Agno"""
    print("\n" + "="*70)
    print("OPERATION COMPARISON: PraisonAI vs Agno")
    print("="*70)
    
    from praisonaiagents import Agent as PraisonAgent
    from agno.agent import Agent as AgnoAgent
    from agno.models.openai import OpenAIChat
    
    def get_weather(city: Literal["nyc", "sf"]):
        if city == "nyc":
            return "It might be cloudy in nyc"
        elif city == "sf":
            return "It's always sunny in sf"
    tools = [get_weather]
    
    operations = []
    
    # Operation 1: Bare minimum instantiation
    def praisonai_minimal():
        return PraisonAgent(name="Test", output="silent")
    
    def agno_minimal():
        return AgnoAgent(model=OpenAIChat(id="gpt-4o-mini"))
    
    praison_stats = time_function(praisonai_minimal, 100)
    agno_stats = time_function(agno_minimal, 100)
    operations.append(("Minimal instantiation", praison_stats['avg'], agno_stats['avg']))
    
    # Operation 2: With tools
    def praisonai_with_tools():
        return PraisonAgent(name="Test", tools=tools, output="silent")
    
    def agno_with_tools():
        return AgnoAgent(model=OpenAIChat(id="gpt-4o-mini"), tools=tools)
    
    praison_stats = time_function(praisonai_with_tools, 100)
    agno_stats = time_function(agno_with_tools, 100)
    operations.append(("With tools", praison_stats['avg'], agno_stats['avg']))
    
    # Operation 3: Full config
    def praisonai_full():
        return PraisonAgent(
            name="Test Agent",
            instructions="Be concise",
            llm="gpt-4o-mini",
            tools=tools,
            output="silent"
        )
    
    def agno_full():
        return AgnoAgent(
            model=OpenAIChat(id="gpt-4o-mini"),
            tools=tools,
            instructions=["Be concise"]
        )
    
    praison_stats = time_function(praisonai_full, 100)
    agno_stats = time_function(agno_full, 100)
    operations.append(("Full config", praison_stats['avg'], agno_stats['avg']))
    
    print(f"\n{'Operation':<30} {'PraisonAI (Î¼s)':<18} {'Agno (Î¼s)':<18} {'Ratio':<10}")
    print("-" * 76)
    
    for op_name, praison_time, agno_time in operations:
        ratio = praison_time / agno_time if agno_time > 0 else 0
        print(f"{op_name:<30} {praison_time:<18.2f} {agno_time:<18.2f} {ratio:<10.1f}x")


# ============================================================================
# Advanced: Attribute Access Profiling
# ============================================================================

def profile_attribute_access():
    """Profile attribute access patterns in Agent.__init__"""
    print("\n" + "="*70)
    print("ATTRIBUTE ACCESS PROFILING")
    print("="*70)
    
    from praisonaiagents import Agent as PraisonAgent
    
    # Count attribute accesses during init
    attr_counts = {}
    original_setattr = object.__setattr__
    
    def counting_setattr(obj, name, value):
        if isinstance(obj, PraisonAgent):
            attr_counts[name] = attr_counts.get(name, 0) + 1
        return original_setattr(obj, name, value)
    
    # Temporarily patch
    import builtins
    old_setattr = builtins.setattr if hasattr(builtins, 'setattr') else None
    
    # Create agent and count
    agent = PraisonAgent(name="Test", output="silent")
    
    # Sort by count
    sorted_attrs = sorted(attr_counts.items(), key=lambda x: x[1], reverse=True)
    
    print(f"\n{'Attribute':<40} {'Set Count':<15}")
    print("-" * 55)
    for attr, count in sorted_attrs[:30]:
        print(f"{attr:<40} {count:<15}")
    
    print(f"\nTotal attributes set: {len(attr_counts)}")
    print(f"Total setattr calls: {sum(attr_counts.values())}")


# ============================================================================
# Advanced: Function Call Analysis
# ============================================================================

def analyze_init_function_calls():
    """Analyze which functions are called during Agent.__init__"""
    print("\n" + "="*70)
    print("FUNCTION CALL ANALYSIS: PraisonAI Agent.__init__")
    print("="*70)
    
    import sys
    from praisonaiagents import Agent as PraisonAgent
    
    call_counts = {}
    
    def trace_calls(frame, event, arg):
        if event == 'call':
            code = frame.f_code
            func_name = code.co_name
            filename = code.co_filename
            
            # Only track praisonaiagents calls
            if 'praisonaiagents' in filename:
                key = f"{func_name} ({filename.split('/')[-1]}:{code.co_firstlineno})"
                call_counts[key] = call_counts.get(key, 0) + 1
        return trace_calls
    
    # Enable tracing
    sys.settrace(trace_calls)
    
    try:
        agent = PraisonAgent(name="Test", output="silent")
    finally:
        sys.settrace(None)
    
    # Sort by count
    sorted_calls = sorted(call_counts.items(), key=lambda x: x[1], reverse=True)
    
    print(f"\n{'Function':<60} {'Calls':<10}")
    print("-" * 70)
    for func, count in sorted_calls[:40]:
        print(f"{func:<60} {count:<10}")
    
    print(f"\nTotal unique functions called: {len(call_counts)}")
    print(f"Total function calls: {sum(call_counts.values())}")


# ============================================================================
# Advanced: Import Time Analysis
# ============================================================================

def analyze_import_overhead():
    """Analyze import overhead for key modules"""
    print("\n" + "="*70)
    print("IMPORT OVERHEAD ANALYSIS")
    print("="*70)
    
    import sys
    import importlib
    
    modules_to_test = [
        'praisonaiagents',
        'praisonaiagents.agent.agent',
        'praisonaiagents.config.param_resolver',
        'praisonaiagents.config.presets',
        'praisonaiagents.config.feature_configs',
    ]
    
    print(f"\n{'Module':<50} {'Import Time (ms)':<20}")
    print("-" * 70)
    
    for module_name in modules_to_test:
        # Clear from cache
        to_remove = [k for k in sys.modules.keys() if k.startswith('praisonaiagents')]
        for k in to_remove:
            del sys.modules[k]
        
        start = time.perf_counter()
        try:
            importlib.import_module(module_name)
            elapsed = (time.perf_counter() - start) * 1000
            print(f"{module_name:<50} {elapsed:<20.2f}")
        except ImportError as e:
            print(f"{module_name:<50} FAILED: {e}")


# ============================================================================
# Advanced: Agno vs PraisonAI Architecture Comparison
# ============================================================================

def compare_architecture():
    """Compare architectural differences between Agno and PraisonAI"""
    print("\n" + "="*70)
    print("ARCHITECTURE COMPARISON: Agno vs PraisonAI")
    print("="*70)
    
    from praisonaiagents import Agent as PraisonAgent
    from agno.agent import Agent as AgnoAgent
    from agno.models.openai import OpenAIChat
    
    # Create instances
    praison_agent = PraisonAgent(name="Test", output="silent")
    agno_agent = AgnoAgent(model=OpenAIChat(id="gpt-4o-mini"))
    
    print("\n### Class Structure ###")
    print(f"\nPraisonAI Agent:")
    print(f"  - Uses @dataclass: No (regular class)")
    print(f"  - __slots__: {'Yes' if hasattr(PraisonAgent, '__slots__') else 'No'}")
    print(f"  - Instance dict size: {len(praison_agent.__dict__)} attributes")
    
    print(f"\nAgno Agent:")
    print(f"  - Uses @dataclass: Yes (init=False)")
    print(f"  - __slots__: {'Yes' if hasattr(AgnoAgent, '__slots__') else 'No'}")
    print(f"  - Instance dict size: {len(agno_agent.__dict__)} attributes")
    
    print("\n### Key Differences ###")
    print("\n1. Agno uses @dataclass(init=False) - class attributes pre-declared")
    print("2. Agno __init__ is mostly simple attribute assignments")
    print("3. PraisonAI __init__ does extensive parameter resolution")
    print("4. PraisonAI imports config modules inside __init__")
    print("5. PraisonAI calls resolve() 10+ times for feature params")
    
    # Count resolve calls in PraisonAI
    print("\n### Parameter Resolution Overhead ###")
    print("\nPraisonAI resolve() calls in __init__:")
    print("  - output: resolve()")
    print("  - execution: resolve()")
    print("  - templates: resolve()")
    print("  - caching: resolve()")
    print("  - hooks: resolve()")
    print("  - skills: resolve()")
    print("  - memory: resolve()")
    print("  - knowledge: resolve()")
    print("  - planning: resolve()")
    print("  - reflection: resolve()")
    print("  - guardrails: resolve()")
    print("  - web: resolve()")
    print("  Total: 12 resolve() calls per Agent creation")


# ============================================================================
# Main
# ============================================================================

if __name__ == "__main__":
    import os
    import argparse
    
    parser = argparse.ArgumentParser(description='Deep Profiling Benchmark')
    parser.add_argument('--deep', action='store_true', help='Run deep profiling (slower)')
    parser.add_argument('--save', action='store_true', help='Save results to file')
    args = parser.parse_args()
    
    os.environ.setdefault("OPENAI_API_KEY", "sk-test")
    
    print("="*70)
    print("PraisonAI Agents - Deep Profiling Benchmark")
    print("="*70)
    print("\nThis analysis identifies performance bottlenecks in agent initialization.\n")
    
    # Run all profiling
    profile_praisonai_detailed()
    profile_agno_detailed()
    profile_praisonai_components()
    compare_specific_operations()
    
    # Advanced profiling (optional)
    if args.deep:
        print("\n" + "="*70)
        print("ADVANCED PROFILING (--deep mode)")
        print("="*70)
        analyze_init_function_calls()
        analyze_import_overhead()
    
    # Always run architecture comparison
    compare_architecture()
    
    print("\n" + "="*70)
    print("PROFILING COMPLETE")
    print("="*70)


--- src/praisonai-agents/benchmarks/detailed_profiling_benchmark.py ---
#!/usr/bin/env python3
"""
PraisonAI Agents - Detailed Profiling Benchmark

Provides function-level profiling for PraisonAI agent execution.
Shows exactly which functions are called, from which files, and how long each takes.

Usage:
    export OPENAI_API_KEY=your_key
    python benchmarks/detailed_profiling_benchmark.py
    python benchmarks/detailed_profiling_benchmark.py --framework praisonai
    python benchmarks/detailed_profiling_benchmark.py --framework agno
"""

import time
import os
import sys
import argparse
import cProfile
import pstats
import io
from functools import wraps
from collections import defaultdict
from datetime import datetime

# Defaults
DEFAULT_MODEL = "gpt-4o-mini"
DEFAULT_PROMPT = "What is 2+2? Reply with just the number."


class DetailedProfiler:
    """Detailed function-level profiler with file path tracking."""
    
    def __init__(self):
        self.call_times = defaultdict(list)
        self.call_counts = defaultdict(int)
        self.profiler = None
        
    def start(self):
        """Start cProfile profiler."""
        self.profiler = cProfile.Profile()
        self.profiler.enable()
        
    def stop(self):
        """Stop profiler and return stats."""
        if self.profiler:
            self.profiler.disable()
            return self.profiler
        return None
    
    def get_stats_string(self, sort_by='cumtime', limit=50):
        """Get formatted stats string."""
        if not self.profiler:
            return "No profiler data"
        
        s = io.StringIO()
        ps = pstats.Stats(self.profiler, stream=s)
        ps.sort_stats(sort_by)
        ps.print_stats(limit)
        return s.getvalue()
    
    def get_detailed_breakdown(self, filter_pattern=None, limit=100):
        """Get detailed breakdown with file paths."""
        if not self.profiler:
            return []
        
        stats = pstats.Stats(self.profiler)
        
        results = []
        for (filename, line, func), (cc, nc, tt, ct, callers) in stats.stats.items():
            # Filter by pattern if provided
            if filter_pattern and filter_pattern not in filename and filter_pattern not in func:
                continue
            
            results.append({
                'file': filename,
                'line': line,
                'function': func,
                'calls': nc,
                'total_time': tt,
                'cumulative_time': ct,
                'time_per_call': tt / nc if nc > 0 else 0,
            })
        
        # Sort by cumulative time
        results.sort(key=lambda x: x['cumulative_time'], reverse=True)
        return results[:limit]


def profile_praisonai_detailed(model, prompt):
    """Profile PraisonAI agent execution in detail."""
    print("\n" + "=" * 80)
    print("DETAILED PROFILING: PraisonAI")
    print("=" * 80)
    
    profiler = DetailedProfiler()
    
    # Profile import
    print("\n--- Import Phase ---")
    import_start = time.perf_counter()
    profiler.start()
    from praisonaiagents import Agent
    profiler.stop()
    import_time = time.perf_counter() - import_start
    print(f"Import time: {import_time*1000:.2f}ms")
    
    # Show import profile
    print("\nTop 20 functions during import (by cumulative time):")
    print("-" * 80)
    breakdown = profiler.get_detailed_breakdown(filter_pattern='praisonai', limit=20)
    print(f"{'Function':<40} {'File':<50} {'Calls':>8} {'CumTime':>10}")
    print("-" * 80)
    for item in breakdown:
        func = item['function'][:38]
        file = item['file'].split('/')[-1][:48] if '/' in item['file'] else item['file'][:48]
        print(f"{func:<40} {file:<50} {item['calls']:>8} {item['cumulative_time']*1000:>9.2f}ms")
    
    # Profile instantiation
    print("\n--- Instantiation Phase ---")
    profiler2 = DetailedProfiler()
    inst_start = time.perf_counter()
    profiler2.start()
    agent = Agent(
        name="Calculator",
        instructions="You are a helpful assistant. Be very brief.",
        llm=model
    )
    profiler2.stop()
    inst_time = time.perf_counter() - inst_start
    print(f"Instantiation time: {inst_time*1000:.2f}ms")
    
    print("\nTop 30 functions during instantiation (by cumulative time):")
    print("-" * 80)
    breakdown = profiler2.get_detailed_breakdown(filter_pattern='praisonai', limit=30)
    print(f"{'Function':<40} {'File':<50} {'Calls':>8} {'CumTime':>10}")
    print("-" * 80)
    for item in breakdown:
        func = item['function'][:38]
        file = item['file'].split('/')[-1][:48] if '/' in item['file'] else item['file'][:48]
        print(f"{func:<40} {file:<50} {item['calls']:>8} {item['cumulative_time']*1000:>9.2f}ms")
    
    # Profile execution
    print("\n--- Execution Phase (agent.start) ---")
    profiler3 = DetailedProfiler()
    exec_start = time.perf_counter()
    profiler3.start()
    response = agent.start(prompt, output="silent")
    profiler3.stop()
    exec_time = time.perf_counter() - exec_start
    print(f"Execution time: {exec_time*1000:.2f}ms ({exec_time:.2f}s)")
    print(f"Response: {str(response)[:100]}")
    
    print("\nTop 50 functions during execution (by cumulative time):")
    print("-" * 80)
    breakdown = profiler3.get_detailed_breakdown(limit=50)
    print(f"{'Function':<40} {'File':<50} {'Calls':>8} {'CumTime':>10}")
    print("-" * 80)
    for item in breakdown:
        func = item['function'][:38]
        file = item['file'].split('/')[-1][:48] if '/' in item['file'] else item['file'][:48]
        print(f"{func:<40} {file:<50} {item['calls']:>8} {item['cumulative_time']*1000:>9.2f}ms")
    
    # PraisonAI-specific functions
    print("\n--- PraisonAI-Specific Functions ---")
    print("-" * 80)
    praisonai_funcs = profiler3.get_detailed_breakdown(filter_pattern='praisonai', limit=50)
    print(f"{'Function':<40} {'File':<50} {'Calls':>8} {'CumTime':>10}")
    print("-" * 80)
    for item in praisonai_funcs:
        func = item['function'][:38]
        file = item['file'].split('/')[-1][:48] if '/' in item['file'] else item['file'][:48]
        print(f"{func:<40} {file:<50} {item['calls']:>8} {item['cumulative_time']*1000:>9.2f}ms")
    
    # Summary
    print("\n" + "=" * 80)
    print("SUMMARY: PraisonAI")
    print("=" * 80)
    print(f"Import time:        {import_time*1000:>10.2f}ms")
    print(f"Instantiation time: {inst_time*1000:>10.2f}ms")
    print(f"Execution time:     {exec_time*1000:>10.2f}ms")
    print(f"Total time:         {(import_time+inst_time+exec_time)*1000:>10.2f}ms")
    
    return {
        'import_time': import_time,
        'instantiation_time': inst_time,
        'execution_time': exec_time,
        'total_time': import_time + inst_time + exec_time,
    }


def profile_agno_detailed(model, prompt):
    """Profile Agno agent execution in detail."""
    print("\n" + "=" * 80)
    print("DETAILED PROFILING: Agno")
    print("=" * 80)
    
    profiler = DetailedProfiler()
    
    # Profile import
    print("\n--- Import Phase ---")
    import_start = time.perf_counter()
    profiler.start()
    from agno.agent import Agent
    from agno.models.openai import OpenAIChat
    profiler.stop()
    import_time = time.perf_counter() - import_start
    print(f"Import time: {import_time*1000:.2f}ms")
    
    print("\nTop 20 functions during import (by cumulative time):")
    print("-" * 80)
    breakdown = profiler.get_detailed_breakdown(filter_pattern='agno', limit=20)
    print(f"{'Function':<40} {'File':<50} {'Calls':>8} {'CumTime':>10}")
    print("-" * 80)
    for item in breakdown:
        func = item['function'][:38]
        file = item['file'].split('/')[-1][:48] if '/' in item['file'] else item['file'][:48]
        print(f"{func:<40} {file:<50} {item['calls']:>8} {item['cumulative_time']*1000:>9.2f}ms")
    
    # Profile instantiation
    print("\n--- Instantiation Phase ---")
    profiler2 = DetailedProfiler()
    inst_start = time.perf_counter()
    profiler2.start()
    agent = Agent(
        model=OpenAIChat(id=model),
        instructions=["You are a helpful assistant. Be very brief."],
    )
    profiler2.stop()
    inst_time = time.perf_counter() - inst_start
    print(f"Instantiation time: {inst_time*1000:.2f}ms")
    
    print("\nTop 30 functions during instantiation (by cumulative time):")
    print("-" * 80)
    breakdown = profiler2.get_detailed_breakdown(filter_pattern='agno', limit=30)
    print(f"{'Function':<40} {'File':<50} {'Calls':>8} {'CumTime':>10}")
    print("-" * 80)
    for item in breakdown:
        func = item['function'][:38]
        file = item['file'].split('/')[-1][:48] if '/' in item['file'] else item['file'][:48]
        print(f"{func:<40} {file:<50} {item['calls']:>8} {item['cumulative_time']*1000:>9.2f}ms")
    
    # Profile execution
    print("\n--- Execution Phase (agent.run) ---")
    profiler3 = DetailedProfiler()
    exec_start = time.perf_counter()
    profiler3.start()
    response = agent.run(prompt)
    profiler3.stop()
    exec_time = time.perf_counter() - exec_start
    content = str(response.content)[:100] if hasattr(response, 'content') else str(response)[:100]
    print(f"Execution time: {exec_time*1000:.2f}ms ({exec_time:.2f}s)")
    print(f"Response: {content}")
    
    print("\nTop 50 functions during execution (by cumulative time):")
    print("-" * 80)
    breakdown = profiler3.get_detailed_breakdown(limit=50)
    print(f"{'Function':<40} {'File':<50} {'Calls':>8} {'CumTime':>10}")
    print("-" * 80)
    for item in breakdown:
        func = item['function'][:38]
        file = item['file'].split('/')[-1][:48] if '/' in item['file'] else item['file'][:48]
        print(f"{func:<40} {file:<50} {item['calls']:>8} {item['cumulative_time']*1000:>9.2f}ms")
    
    # Agno-specific functions
    print("\n--- Agno-Specific Functions ---")
    print("-" * 80)
    agno_funcs = profiler3.get_detailed_breakdown(filter_pattern='agno', limit=50)
    print(f"{'Function':<40} {'File':<50} {'Calls':>8} {'CumTime':>10}")
    print("-" * 80)
    for item in agno_funcs:
        func = item['function'][:38]
        file = item['file'].split('/')[-1][:48] if '/' in item['file'] else item['file'][:48]
        print(f"{func:<40} {file:<50} {item['calls']:>8} {item['cumulative_time']*1000:>9.2f}ms")
    
    # Summary
    print("\n" + "=" * 80)
    print("SUMMARY: Agno")
    print("=" * 80)
    print(f"Import time:        {import_time*1000:>10.2f}ms")
    print(f"Instantiation time: {inst_time*1000:>10.2f}ms")
    print(f"Execution time:     {exec_time*1000:>10.2f}ms")
    print(f"Total time:         {(import_time+inst_time+exec_time)*1000:>10.2f}ms")
    
    return {
        'import_time': import_time,
        'instantiation_time': inst_time,
        'execution_time': exec_time,
        'total_time': import_time + inst_time + exec_time,
    }


def profile_crewai_detailed(model, prompt):
    """Profile CrewAI agent execution in detail."""
    print("\n" + "=" * 80)
    print("DETAILED PROFILING: CrewAI")
    print("=" * 80)
    
    profiler = DetailedProfiler()
    
    # Profile import
    print("\n--- Import Phase ---")
    import_start = time.perf_counter()
    profiler.start()
    from crewai import Agent, Task, Crew
    profiler.stop()
    import_time = time.perf_counter() - import_start
    print(f"Import time: {import_time*1000:.2f}ms")
    
    # Profile instantiation
    print("\n--- Instantiation Phase ---")
    profiler2 = DetailedProfiler()
    inst_start = time.perf_counter()
    profiler2.start()
    agent = Agent(
        role="Calculator",
        goal="Answer math questions",
        backstory="You are a helpful assistant.",
        llm=model,
        verbose=False
    )
    task = Task(
        description=prompt,
        expected_output="A number",
        agent=agent
    )
    crew = Crew(
        agents=[agent],
        tasks=[task],
        verbose=False
    )
    profiler2.stop()
    inst_time = time.perf_counter() - inst_start
    print(f"Instantiation time: {inst_time*1000:.2f}ms")
    
    # Profile execution
    print("\n--- Execution Phase (crew.kickoff) ---")
    profiler3 = DetailedProfiler()
    exec_start = time.perf_counter()
    profiler3.start()
    response = crew.kickoff()
    profiler3.stop()
    exec_time = time.perf_counter() - exec_start
    print(f"Execution time: {exec_time*1000:.2f}ms ({exec_time:.2f}s)")
    print(f"Response: {str(response)[:100]}")
    
    print("\nTop 50 functions during execution (by cumulative time):")
    print("-" * 80)
    breakdown = profiler3.get_detailed_breakdown(limit=50)
    print(f"{'Function':<40} {'File':<50} {'Calls':>8} {'CumTime':>10}")
    print("-" * 80)
    for item in breakdown:
        func = item['function'][:38]
        file = item['file'].split('/')[-1][:48] if '/' in item['file'] else item['file'][:48]
        print(f"{func:<40} {file:<50} {item['calls']:>8} {item['cumulative_time']*1000:>9.2f}ms")
    
    # Summary
    print("\n" + "=" * 80)
    print("SUMMARY: CrewAI")
    print("=" * 80)
    print(f"Import time:        {import_time*1000:>10.2f}ms")
    print(f"Instantiation time: {inst_time*1000:>10.2f}ms")
    print(f"Execution time:     {exec_time*1000:>10.2f}ms")
    print(f"Total time:         {(import_time+inst_time+exec_time)*1000:>10.2f}ms")
    
    return {
        'import_time': import_time,
        'instantiation_time': inst_time,
        'execution_time': exec_time,
        'total_time': import_time + inst_time + exec_time,
    }


def compare_results(results):
    """Compare results across frameworks."""
    print("\n" + "=" * 80)
    print("COMPARISON SUMMARY")
    print("=" * 80)
    
    print(f"\n{'Framework':<20} {'Import':<12} {'Instantiate':<12} {'Execute':<12} {'Total':<12}")
    print("-" * 68)
    
    # Sort by execution time
    sorted_results = sorted(results.items(), key=lambda x: x[1]['execution_time'])
    fastest_exec = sorted_results[0][1]['execution_time']
    
    for name, times in sorted_results:
        import_ms = times['import_time'] * 1000
        inst_ms = times['instantiation_time'] * 1000
        exec_ms = times['execution_time'] * 1000
        total_ms = times['total_time'] * 1000
        ratio = times['execution_time'] / fastest_exec
        
        print(f"{name:<20} {import_ms:>10.1f}ms {inst_ms:>10.1f}ms {exec_ms:>10.1f}ms {total_ms:>10.1f}ms ({ratio:.2f}x)")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='PraisonAI Agents - Detailed Profiling Benchmark')
    parser.add_argument('--framework', '-f', type=str, default='all', 
                        choices=['all', 'praisonai', 'agno', 'crewai'],
                        help='Framework to profile (default: all)')
    parser.add_argument('--model', '-m', type=str, default=DEFAULT_MODEL, 
                        help=f'Model to use (default: {DEFAULT_MODEL})')
    parser.add_argument('--prompt', '-p', type=str, default=DEFAULT_PROMPT, 
                        help=f'Prompt to use')
    args = parser.parse_args()
    
    # Check API key
    if not os.environ.get('OPENAI_API_KEY'):
        print("ERROR: Set OPENAI_API_KEY environment variable")
        exit(1)
    
    print("=" * 80)
    print("PraisonAI Agents - Detailed Profiling Benchmark")
    print("=" * 80)
    print(f"\nModel: {args.model}")
    print(f"Prompt: \"{args.prompt}\"")
    print(f"Framework: {args.framework}")
    print(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    results = {}
    
    if args.framework in ['all', 'praisonai']:
        try:
            results['PraisonAI'] = profile_praisonai_detailed(args.model, args.prompt)
        except Exception as e:
            print(f"PraisonAI error: {e}")
            import traceback
            traceback.print_exc()
    
    if args.framework in ['all', 'agno']:
        try:
            results['Agno'] = profile_agno_detailed(args.model, args.prompt)
        except Exception as e:
            print(f"Agno error: {e}")
            import traceback
            traceback.print_exc()
    
    if args.framework in ['all', 'crewai']:
        try:
            results['CrewAI'] = profile_crewai_detailed(args.model, args.prompt)
        except Exception as e:
            print(f"CrewAI error: {e}")
            import traceback
            traceback.print_exc()
    
    if len(results) > 1:
        compare_results(results)
    
    print("\n" + "=" * 80)
    print("Profiling complete!")
    print("=" * 80)


--- src/praisonai-agents/benchmarks/execution_benchmark.py ---
#!/usr/bin/env python3
"""
PraisonAI Agents - Real Execution Benchmark

Benchmarks ACTUAL agent execution with LLM API calls.
Tests single agent running a simple task across frameworks.

Usage:
    export OPENAI_API_KEY=your_key
    python benchmarks/execution_benchmark.py
    python benchmarks/execution_benchmark.py --no-save
    python benchmarks/execution_benchmark.py --iterations 5
    python benchmarks/execution_benchmark.py --model gpt-4o --iterations 10
"""

import time
import os
import argparse
from importlib.metadata import version as get_version

# Defaults (can be overridden via CLI)
DEFAULT_MODEL = "gpt-4o-mini"
DEFAULT_ITERATIONS = 3
DEFAULT_PROMPT = "What is 2+2? Reply with just the number."


def get_package_versions():
    """Get version numbers for benchmarked packages."""
    packages = {
        'PraisonAI': 'praisonaiagents',
        'Agno': 'agno',
        'CrewAI': 'crewai',
    }
    versions = {}
    for name, pkg in packages.items():
        try:
            versions[name] = get_version(pkg)
        except:
            versions[name] = 'not installed'
    return versions


def benchmark_praisonai(model, iterations, prompt):
    """Benchmark PraisonAI agent execution using agent.start()"""
    from praisonaiagents import Agent
    
    print("\n--- PraisonAI ---")
    print("Method: agent.start()")
    
    times = []
    for i in range(iterations):
        agent = Agent(
            name="Calculator",
            instructions="You are a helpful assistant. Be very brief.",
            llm=model
        )
        
        start = time.perf_counter()
        response = agent.start(prompt, output="silent")
        elapsed = time.perf_counter() - start
        
        times.append(elapsed)
        print(f"  Run {i+1}: {elapsed:.2f}s - Response: {str(response)[:50]}")
    
    avg = sum(times) / len(times)
    print(f"  Average: {avg:.2f}s")
    return avg


def benchmark_praisonai_litellm(model, iterations, prompt):
    """Benchmark PraisonAI with LiteLLM backend using agent.start()"""
    from praisonaiagents import Agent
    
    print("\n--- PraisonAI (LiteLLM) ---")
    print("Method: agent.start()")
    
    times = []
    for i in range(iterations):
        agent = Agent(
            name="Calculator",
            instructions="You are a helpful assistant. Be very brief.",
            llm=f"openai/{model}"
        )
        
        start = time.perf_counter()
        response = agent.start(prompt, output="silent")
        elapsed = time.perf_counter() - start
        
        times.append(elapsed)
        print(f"  Run {i+1}: {elapsed:.2f}s - Response: {str(response)[:50]}")
    
    avg = sum(times) / len(times)
    print(f"  Average: {avg:.2f}s")
    return avg


def benchmark_agno(model, iterations, prompt):
    """Benchmark Agno agent execution using agent.run()"""
    from agno.agent import Agent
    from agno.models.openai import OpenAIChat
    
    print("\n--- Agno ---")
    print("Method: agent.run()")
    
    times = []
    for i in range(iterations):
        agent = Agent(
            model=OpenAIChat(id=model),
            instructions=["You are a helpful assistant. Be very brief."],
        )
        
        start = time.perf_counter()
        response = agent.run(prompt)
        elapsed = time.perf_counter() - start
        
        times.append(elapsed)
        # Extract content from response
        content = str(response.content)[:50] if hasattr(response, 'content') else str(response)[:50]
        print(f"  Run {i+1}: {elapsed:.2f}s - Response: {content}")
    
    avg = sum(times) / len(times)
    print(f"  Average: {avg:.2f}s")
    return avg


def benchmark_crewai(model, iterations, prompt):
    """Benchmark CrewAI agent execution using crew.kickoff()"""
    from crewai import Agent, Task, Crew
    
    print("\n--- CrewAI ---")
    print("Method: crew.kickoff()")
    
    times = []
    for i in range(iterations):
        agent = Agent(
            role="Calculator",
            goal="Answer math questions",
            backstory="You are a helpful assistant.",
            llm=model,
            verbose=False
        )
        
        task = Task(
            description=prompt,
            expected_output="A number",
            agent=agent
        )
        
        crew = Crew(
            agents=[agent],
            tasks=[task],
            verbose=False
        )
        
        start = time.perf_counter()
        response = crew.kickoff()
        elapsed = time.perf_counter() - start
        
        times.append(elapsed)
        print(f"  Run {i+1}: {elapsed:.2f}s - Response: {str(response)[:50]}")
    
    avg = sum(times) / len(times)
    print(f"  Average: {avg:.2f}s")
    return avg


def save_results(results: dict, model: str, iterations: int, prompt: str):
    """Save results to markdown file."""
    import os
    from datetime import datetime
    
    filepath = os.path.join(os.path.dirname(__file__), 'EXECUTION_BENCHMARK_RESULTS.md')
    versions = get_package_versions()
    
    # Sort by time
    sorted_results = sorted(results.items(), key=lambda x: x[1])
    fastest = sorted_results[0][1]
    
    with open(filepath, 'w') as f:
        f.write('# PraisonAI Agents - Real Execution Benchmark\n\n')
        f.write(f'**Generated:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}\n')
        f.write(f'**Model:** {model}\n')
        f.write(f'**Iterations:** {iterations}\n')
        f.write(f'**Prompt:** "{prompt}"\n\n')
        f.write('## Results\n\n')
        f.write('| Framework | Method | Avg Time | Relative |\n')
        f.write('|-----------|--------|----------|----------|\n')
        
        methods = {
            'PraisonAI': 'agent.start()',
            'PraisonAI (LiteLLM)': 'agent.start()',
            'Agno': 'agent.run()',
            'CrewAI': 'crew.kickoff()',
        }
        
        for name, avg in sorted_results:
            ratio = avg / fastest
            method = methods.get(name, 'unknown')
            if ratio == 1.0:
                f.write(f'| **{name}** | `{method}` | **{avg:.2f}s** | **1.00x (fastest)** |\n')
            else:
                f.write(f'| {name} | `{method}` | {avg:.2f}s | {ratio:.2f}x |\n')
        
        f.write('\n## Package Versions\n\n')
        f.write('| Package | Version |\n')
        f.write('|---------|--------|\n')
        for pkg, ver in versions.items():
            f.write(f'| {pkg} | {ver} |\n')
        
        f.write('\n## How to Reproduce\n\n')
        f.write('```bash\n')
        f.write('export OPENAI_API_KEY=your_key\n')
        f.write('cd praisonai-agents\n')
        f.write('python benchmarks/execution_benchmark.py\n')
        f.write('```\n')
    
    print(f'\nResults saved to: {filepath}')


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='PraisonAI Agents - Real Execution Benchmark')
    parser.add_argument('--save', action='store_true', help='Save results to file')
    parser.add_argument('--model', '-m', type=str, default=DEFAULT_MODEL, help=f'Model to use (default: {DEFAULT_MODEL})')
    parser.add_argument('--iterations', '-i', type=int, default=DEFAULT_ITERATIONS, help=f'Number of iterations (default: {DEFAULT_ITERATIONS})')
    parser.add_argument('--prompt', '-p', type=str, default=DEFAULT_PROMPT, help=f'Prompt to use (default: "{DEFAULT_PROMPT}")')
    args = parser.parse_args()
    
    # Check API key
    if not os.environ.get('OPENAI_API_KEY'):
        print("ERROR: Set OPENAI_API_KEY environment variable")
        exit(1)
    
    model = args.model
    iterations = args.iterations
    prompt = args.prompt
    
    print('=' * 60)
    print('PraisonAI Agents - Real Execution Benchmark')
    print('=' * 60)
    print(f'\nModel: {model}')
    print(f'Iterations: {iterations}')
    print(f'Prompt: "{prompt}"')
    
    versions = get_package_versions()
    print('\nPackage versions:')
    for pkg, ver in versions.items():
        print(f'  {pkg}: {ver}')
    
    results = {}
    
    # Run benchmarks
    try:
        results['PraisonAI'] = benchmark_praisonai(model, iterations, prompt)
    except Exception as e:
        print(f"PraisonAI error: {e}")
    
    try:
        results['PraisonAI (LiteLLM)'] = benchmark_praisonai_litellm(model, iterations, prompt)
    except Exception as e:
        print(f"PraisonAI (LiteLLM) error: {e}")
    
    try:
        results['Agno'] = benchmark_agno(model, iterations, prompt)
    except Exception as e:
        print(f"Agno error: {e}")
    
    try:
        results['CrewAI'] = benchmark_crewai(model, iterations, prompt)
    except Exception as e:
        print(f"CrewAI error: {e}")
    
    # Summary
    print('\n' + '=' * 60)
    print('SUMMARY')
    print('=' * 60)
    
    if results:
        sorted_results = sorted(results.items(), key=lambda x: x[1])
        fastest = sorted_results[0][1]
        
        print(f"\n{'Framework':<25} {'Avg Time':<12} {'Relative':<10}")
        print('-' * 47)
        
        for name, avg in sorted_results:
            ratio = avg / fastest
            print(f'{name:<25} {avg:<12.2f}s {ratio:.2f}x')
    
    print('\n' + '=' * 60)
    
    # Save results (if --save)
    if results and args.save:
        save_results(results, model, iterations, prompt)
    elif not args.save:
        print('\nResults not saved (use --save flag to save results to file)')


--- src/praisonai-agents/benchmarks/EXECUTION_BENCHMARK_RESULTS.md ---
# PraisonAI Agents - Real Execution Benchmark

**Generated:** 2026-01-13 06:07:28
**Model:** gpt-4o-mini
**Iterations:** 3
**Prompt:** "What is 2+2? Reply with just the number."

## Results

| Framework | Method | Avg Time | Relative |
|-----------|--------|----------|----------|
| **PraisonAI (LiteLLM)** | `agent.start()` | **0.55s** | **1.00x (fastest)** |
| PraisonAI | `agent.start()` | 0.64s | 1.15x |
| CrewAI | `crew.kickoff()` | 0.95s | 1.71x |
| Agno | `agent.run()` | 1.05s | 1.90x |

## Package Versions

| Package | Version |
|---------|--------|
| PraisonAI | 0.11.7 |
| Agno | 2.3.25 |
| CrewAI | 1.8.0 |

## How to Reproduce

```bash
export OPENAI_API_KEY=your_key
cd praisonai-agents
python benchmarks/execution_benchmark.py
```


--- src/praisonai-agents/benchmarks/import_time.py ---
#!/usr/bin/env python3
"""
Import time benchmark for praisonaiagents.

This benchmark measures the import time of the praisonaiagents package
and verifies that heavy dependencies are not loaded at import time.

Usage:
    python benchmarks/import_time.py
    
CI Gate:
    - PASS: import time < 200ms
    - WARN: import time 200-300ms
    - FAIL: import time > 300ms
"""
import sys
import time
import statistics
import argparse


def clear_modules():
    """Clear all praisonai and litellm related modules from cache."""
    to_remove = [m for m in sys.modules.keys() 
                 if 'praison' in m or 'litellm' in m]
    for mod in to_remove:
        del sys.modules[mod]


def measure_import_time(runs: int = 5) -> dict:
    """
    Measure import time over multiple runs.
    
    Returns:
        dict with min, max, median, mean times in milliseconds
    """
    times = []
    
    for _ in range(runs):
        clear_modules()
        
        start = time.perf_counter()
        import praisonaiagents  # noqa: F401
        end = time.perf_counter()
        
        times.append((end - start) * 1000)
        
        # Clean up for next run
        if 'praisonaiagents' in sys.modules:
            del sys.modules['praisonaiagents']
    
    times.sort()
    
    return {
        'times': times,
        'min': min(times),
        'max': max(times),
        'median': statistics.median(times),
        'mean': statistics.mean(times),
        'stdev': statistics.stdev(times) if len(times) > 1 else 0,
    }


def check_lazy_imports() -> dict:
    """
    Check that heavy dependencies are NOT loaded after importing praisonaiagents.
    
    Returns:
        dict with module names and whether they are loaded
    """
    clear_modules()
    
    import praisonaiagents  # noqa: F401
    
    heavy_modules = [
        'litellm',
        'chromadb', 
        'mem0',
        'requests',
    ]
    
    results = {}
    for mod in heavy_modules:
        results[mod] = mod in sys.modules
    
    return results


def main():
    """Run the benchmark and print results."""
    print("=" * 60)
    print("PraisonAI Agents Import Time Benchmark")
    print("=" * 60)
    
    # Measure import time
    print("\n[1/2] Measuring import time (5 runs)...")
    results = measure_import_time(runs=5)
    
    print(f"\nImport times (ms): {[round(t, 1) for t in results['times']]}")
    print(f"  Min:    {results['min']:.1f} ms")
    print(f"  Max:    {results['max']:.1f} ms")
    print(f"  Median: {results['median']:.1f} ms")
    print(f"  Mean:   {results['mean']:.1f} ms")
    print(f"  StdDev: {results['stdev']:.1f} ms")
    
    # Check lazy imports
    print("\n[2/2] Checking lazy imports...")
    lazy_results = check_lazy_imports()
    
    print("\nHeavy modules loaded at import time:")
    all_lazy = True
    for mod, loaded in lazy_results.items():
        status = "LOADED (BAD)" if loaded else "NOT LOADED (GOOD)"
        print(f"  {mod}: {status}")
        if loaded:
            all_lazy = False
    
    # Determine pass/fail
    print("\n" + "=" * 60)
    print("RESULTS")
    print("=" * 60)
    
    median_time = results['median']
    
    if median_time < 200:
        time_status = "PASS"
        time_msg = f"Import time {median_time:.0f}ms < 200ms target"
    elif median_time < 300:
        time_status = "WARN"
        time_msg = f"Import time {median_time:.0f}ms between 200-300ms"
    else:
        time_status = "FAIL"
        time_msg = f"Import time {median_time:.0f}ms > 300ms limit"
    
    lazy_status = "PASS" if all_lazy else "FAIL"
    lazy_msg = "All heavy deps lazy loaded" if all_lazy else "Some heavy deps loaded eagerly"
    
    print(f"\nImport Time: [{time_status}] {time_msg}")
    print(f"Lazy Imports: [{lazy_status}] {lazy_msg}")
    
    overall = "PASS" if time_status != "FAIL" and lazy_status == "PASS" else "FAIL"
    print(f"\nOverall: [{overall}]")
    
    # Exit code for CI
    return 0 if overall == "PASS" else 1


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='PraisonAI Agents Import Time Benchmark')
    parser.add_argument('--save', action='store_true', help='Save results to file')
    args = parser.parse_args()
    
    if not args.save:
        print("\nNote: Results are not saved to file by default. Use --save to save them.")
        
    sys.exit(main())


--- src/praisonai-agents/benchmarks/memory_usage.py ---
#!/usr/bin/env python3
"""
Memory usage benchmark for praisonaiagents.

This benchmark measures the memory footprint of importing praisonaiagents
and verifies it stays within acceptable limits.

Usage:
    python benchmarks/memory_usage.py
    
CI Gate:
    - PASS: memory < 30MB
    - WARN: memory 30-45MB
    - FAIL: memory > 45MB
"""
import sys
import tracemalloc
import argparse


def clear_modules():
    """Clear all praisonai and litellm related modules from cache."""
    to_remove = [m for m in sys.modules.keys() 
                 if 'praison' in m or 'litellm' in m]
    for mod in to_remove:
        del sys.modules[mod]


def measure_memory() -> dict:
    """
    Measure memory usage after importing praisonaiagents.
    
    Returns:
        dict with current and peak memory in MB
    """
    clear_modules()
    
    tracemalloc.start()
    
    import praisonaiagents  # noqa: F401
    
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()
    
    return {
        'current_bytes': current,
        'peak_bytes': peak,
        'current_mb': current / 1024 / 1024,
        'peak_mb': peak / 1024 / 1024,
    }


def main():
    """Run the benchmark and print results."""
    print("=" * 60)
    print("PraisonAI Agents Memory Usage Benchmark")
    print("=" * 60)
    
    print("\nMeasuring memory usage...")
    results = measure_memory()
    
    print(f"\nMemory Usage:")
    print(f"  Current: {results['current_mb']:.1f} MB")
    print(f"  Peak:    {results['peak_mb']:.1f} MB")
    
    # Determine pass/fail based on peak memory
    peak_mb = results['peak_mb']
    
    print("\n" + "=" * 60)
    print("RESULTS")
    print("=" * 60)
    
    if peak_mb < 30:
        status = "PASS"
        msg = f"Memory {peak_mb:.1f}MB < 30MB target"
    elif peak_mb < 45:
        status = "WARN"
        msg = f"Memory {peak_mb:.1f}MB between 30-45MB"
    else:
        status = "FAIL"
        msg = f"Memory {peak_mb:.1f}MB > 45MB limit"
    
    print(f"\nMemory: [{status}] {msg}")
    
    # Exit code for CI
    return 0 if status != "FAIL" else 1


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='PraisonAI Agents Memory Usage Benchmark')
    parser.add_argument('--save', action='store_true', help='Save results to file')
    args = parser.parse_args()
    
    if not args.save:
        print("\nNote: Results are not saved to file by default. Use --save to save them.")
        
    sys.exit(main())


--- src/praisonai-agents/benchmarks/param_impact_benchmark.py ---
#!/usr/bin/env python3
"""
PraisonAI Agents - Parameter Impact Benchmark

Profiles each Agent parameter to identify which features have the most
performance impact. Results are ordered by cost (time + memory).

This benchmark helps identify optimization opportunities by measuring
the incremental cost of each feature.

Usage:
    python benchmarks/param_impact_benchmark.py
    python benchmarks/param_impact_benchmark.py --no-save
    python benchmarks/param_impact_benchmark.py --iterations 50
"""

import argparse
import gc
import time
import tracemalloc
import statistics
from typing import Literal, List, Dict, Any, Callable
from dataclasses import dataclass


@dataclass
class ParamResult:
    """Result for a single parameter test."""
    param_name: str
    param_value: Any
    avg_time_us: float  # microseconds
    min_time_us: float
    max_time_us: float
    std_dev_us: float
    avg_memory_kb: float  # kilobytes
    delta_time_us: float = 0.0  # difference from baseline
    delta_memory_kb: float = 0.0


def measure_instantiation(
    create_fn: Callable,
    iterations: int = 100,
    warmup: int = 10
) -> Dict[str, float]:
    """Measure instantiation time and memory for a function."""
    # Warmup
    for _ in range(warmup):
        create_fn()
    
    # Measure time
    times = []
    for _ in range(iterations):
        gc.collect()
        start = time.perf_counter()
        create_fn()
        elapsed = (time.perf_counter() - start) * 1_000_000  # microseconds
        times.append(elapsed)
    
    # Measure memory (separate pass for accuracy)
    memory_usages = []
    for _ in range(min(iterations, 20)):  # Fewer iterations for memory
        gc.collect()
        tracemalloc.start()
        create_fn()
        _, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        memory_usages.append(peak / 1024)  # KB
    
    return {
        'avg_time': statistics.mean(times),
        'min_time': min(times),
        'max_time': max(times),
        'std_dev': statistics.stdev(times) if len(times) > 1 else 0,
        'avg_memory': statistics.mean(memory_usages) if memory_usages else 0,
    }


def sample_tool(city: Literal['nyc', 'sf']):
    """Sample tool for benchmark testing."""
    return 'cloudy' if city == 'nyc' else 'sunny'


def run_benchmark(iterations: int = 100) -> List[ParamResult]:
    """Run parameter impact benchmark."""
    from praisonaiagents import Agent
    
    results = []
    
    print('=' * 70)
    print('PraisonAI Agents - Parameter Impact Benchmark')
    print('=' * 70)
    print(f'\nIterations: {iterations}')
    print('Measuring incremental cost of each Agent parameter\n')
    
    # Baseline: Minimal agent
    print("Testing baseline (minimal agent)...")
    baseline_stats = measure_instantiation(
        lambda: Agent(name='Test', output='silent'),
        iterations
    )
    baseline_result = ParamResult(
        param_name='BASELINE',
        param_value='Agent(name="Test", output="silent")',
        avg_time_us=baseline_stats['avg_time'],
        min_time_us=baseline_stats['min_time'],
        max_time_us=baseline_stats['max_time'],
        std_dev_us=baseline_stats['std_dev'],
        avg_memory_kb=baseline_stats['avg_memory'],
    )
    results.append(baseline_result)
    print(f"  Baseline: {baseline_stats['avg_time']:.2f} Î¼s, {baseline_stats['avg_memory']:.2f} KB")
    
    # Define parameters to test
    # Format: (param_name, param_value, description)
    params_to_test = [
        # Core identity
        ('instructions', 'Be helpful and concise.', 'instructions="..."'),
        ('role', 'Assistant', 'role="Assistant"'),
        ('goal', 'Help users', 'goal="Help users"'),
        ('backstory', 'I am an AI assistant.', 'backstory="..."'),
        
        # LLM configuration
        ('llm', 'gpt-4o-mini', 'llm="gpt-4o-mini"'),
        ('llm', 'openai/gpt-4o-mini', 'llm="openai/gpt-4o-mini" (LiteLLM)'),
        
        # Tools
        ('tools', [sample_tool], 'tools=[sample_tool]'),
        
        # Features (bool flags)
        ('context', True, 'context=True'),
        ('context', False, 'context=False'),
        ('planning', True, 'planning=True'),
        ('reflection', True, 'reflection=True'),
        ('memory', True, 'memory=True'),
        
        # Output modes
        ('output', 'verbose', 'output="verbose"'),
        ('output', 'actions', 'output="actions"'),
        ('output', 'stream', 'output="stream"'),
        
        # Execution config
        ('execution', 'fast', 'execution="fast"'),
        ('execution', 'thorough', 'execution="thorough"'),
        
        # Caching
        ('caching', True, 'caching=True'),
        ('caching', False, 'caching=False'),
    ]
    
    # Test each parameter
    for param_name, param_value, description in params_to_test:
        print(f"Testing {description}...")
        
        try:
            # Build kwargs
            kwargs = {'name': 'Test', 'output': 'silent'}
            kwargs[param_name] = param_value
            
            # Special case: if testing output param, don't override with silent
            if param_name == 'output':
                kwargs = {'name': 'Test'}
                kwargs[param_name] = param_value
            
            stats = measure_instantiation(
                lambda k=kwargs: Agent(**k),
                iterations
            )
            
            result = ParamResult(
                param_name=param_name,
                param_value=description,
                avg_time_us=stats['avg_time'],
                min_time_us=stats['min_time'],
                max_time_us=stats['max_time'],
                std_dev_us=stats['std_dev'],
                avg_memory_kb=stats['avg_memory'],
                delta_time_us=stats['avg_time'] - baseline_stats['avg_time'],
                delta_memory_kb=stats['avg_memory'] - baseline_stats['avg_memory'],
            )
            results.append(result)
            
            delta_sign = '+' if result.delta_time_us >= 0 else ''
            print(f"  {stats['avg_time']:.2f} Î¼s ({delta_sign}{result.delta_time_us:.2f} Î¼s from baseline)")
            
        except Exception as e:
            print(f"  SKIPPED: {e}")
    
    return results


def print_results(results: List[ParamResult]):
    """Print results sorted by performance impact."""
    print('\n' + '=' * 70)
    print('RESULTS - Sorted by Time Impact (highest cost first)')
    print('=' * 70)
    
    # Sort by delta time (excluding baseline)
    sorted_results = sorted(
        [r for r in results if r.param_name != 'BASELINE'],
        key=lambda x: x.delta_time_us,
        reverse=True
    )
    
    # Get baseline
    baseline = next((r for r in results if r.param_name == 'BASELINE'), None)
    
    if baseline:
        print(f"\nBaseline: {baseline.avg_time_us:.2f} Î¼s, {baseline.avg_memory_kb:.2f} KB")
    
    print(f"\n{'Parameter':<35} {'Avg (Î¼s)':<12} {'Delta (Î¼s)':<12} {'Memory (KB)':<12} {'Impact':<10}")
    print('-' * 85)
    
    for r in sorted_results:
        # Calculate impact level
        if r.delta_time_us > 1000:
            impact = 'ðŸ”´ HIGH'
        elif r.delta_time_us > 100:
            impact = 'ðŸŸ¡ MEDIUM'
        elif r.delta_time_us > 10:
            impact = 'ðŸŸ¢ LOW'
        else:
            impact = 'âšª MINIMAL'
        
        delta_str = f"+{r.delta_time_us:.2f}" if r.delta_time_us >= 0 else f"{r.delta_time_us:.2f}"
        print(f"{r.param_value:<35} {r.avg_time_us:<12.2f} {delta_str:<12} {r.avg_memory_kb:<12.2f} {impact:<10}")
    
    # Summary
    print('\n' + '=' * 70)
    print('SUMMARY')
    print('=' * 70)
    
    high_impact = [r for r in sorted_results if r.delta_time_us > 1000]
    medium_impact = [r for r in sorted_results if 100 < r.delta_time_us <= 1000]
    low_impact = [r for r in sorted_results if 10 < r.delta_time_us <= 100]
    
    print(f"\nHigh impact (>1ms): {len(high_impact)} parameters")
    for r in high_impact:
        print(f"  - {r.param_value}: +{r.delta_time_us:.2f} Î¼s")
    
    print(f"\nMedium impact (100Î¼s-1ms): {len(medium_impact)} parameters")
    for r in medium_impact:
        print(f"  - {r.param_value}: +{r.delta_time_us:.2f} Î¼s")
    
    print(f"\nLow impact (10-100Î¼s): {len(low_impact)} parameters")
    
    print('\n' + '=' * 70)


def save_results(results: List[ParamResult], filename: str = 'PARAM_IMPACT_RESULTS.md'):
    """Save results to markdown file."""
    import os
    from datetime import datetime
    
    filepath = os.path.join(os.path.dirname(__file__), filename)
    
    # Sort by delta time
    sorted_results = sorted(
        [r for r in results if r.param_name != 'BASELINE'],
        key=lambda x: x.delta_time_us,
        reverse=True
    )
    baseline = next((r for r in results if r.param_name == 'BASELINE'), None)
    
    with open(filepath, 'w') as f:
        f.write('# PraisonAI Agents - Parameter Impact Benchmark Results\n\n')
        f.write(f'**Generated:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}\n\n')
        
        if baseline:
            f.write(f'**Baseline:** {baseline.avg_time_us:.2f} Î¼s, {baseline.avg_memory_kb:.2f} KB\n\n')
        
        f.write('## Results (Sorted by Impact)\n\n')
        f.write('| Parameter | Avg Time (Î¼s) | Delta (Î¼s) | Memory (KB) | Impact |\n')
        f.write('|-----------|---------------|------------|-------------|--------|\n')
        
        for r in sorted_results:
            if r.delta_time_us > 1000:
                impact = 'ðŸ”´ HIGH'
            elif r.delta_time_us > 100:
                impact = 'ðŸŸ¡ MEDIUM'
            elif r.delta_time_us > 10:
                impact = 'ðŸŸ¢ LOW'
            else:
                impact = 'âšª MINIMAL'
            
            delta_str = f"+{r.delta_time_us:.2f}" if r.delta_time_us >= 0 else f"{r.delta_time_us:.2f}"
            f.write(f'| {r.param_value} | {r.avg_time_us:.2f} | {delta_str} | {r.avg_memory_kb:.2f} | {impact} |\n')
        
        f.write('\n## Recommendations\n\n')
        
        high_impact = [r for r in sorted_results if r.delta_time_us > 1000]
        if high_impact:
            f.write('### High Impact Parameters (>1ms)\n\n')
            f.write('These parameters add significant overhead and should be used carefully:\n\n')
            for r in high_impact:
                f.write(f'- **{r.param_value}**: +{r.delta_time_us:.2f} Î¼s\n')
            f.write('\n')
        
        f.write('### Best Practices\n\n')
        f.write('1. Use `output="silent"` for production workloads\n')
        f.write('2. Only enable `context=True` when needed for long conversations\n')
        f.write('3. Use `llm="gpt-4o-mini"` (OpenAI SDK) for fastest instantiation\n')
        f.write('4. Avoid `memory=True` unless persistent memory is required\n')
        
        f.write('\n## How to Reproduce\n\n')
        f.write('```bash\n')
        f.write('cd praisonai-agents\n')
        f.write('python benchmarks/param_impact_benchmark.py\n')
        f.write('```\n')
    
    print(f'\nResults saved to: {filepath}')
    return filepath


def main():
    parser = argparse.ArgumentParser(description='PraisonAI Parameter Impact Benchmark')
    parser.add_argument('--iterations', type=int, default=100, help='Number of iterations per test')
    parser.add_argument('--save', action='store_true', help='Save results to file')
    args = parser.parse_args()
    
    results = run_benchmark(iterations=args.iterations)
    print_results(results)
    
    if args.save:
        save_results(results)
    else:
        print('\nResults not saved (use --save flag to save results to file)')


if __name__ == '__main__':
    main()


--- .github/ISSUE_TEMPLATE/bug_report.md ---
---
name: Bug report
about: Create a report to help us improve
title: '[BUG] '
labels: bug
assignees: ''
---

## Environment
- Provider (select one):
  - [ ] Anthropic
  - [ ] OpenAI
  - [ ] Google Vertex AI
  - [ ] AWS Bedrock
  - [ ] Other: <!-- specify -->
- PraisonAI version: <!-- if known -->
- Operating System: <!-- e.g. macOS 14.3, Windows 11, Ubuntu 22.04 -->

## Full Code
<!-- Full code that reproduces the bug -->

## Steps to Reproduce
1. <!-- First step -->
2. <!-- Second step -->
3. <!-- And so on... -->

## Expected Behavior
<!-- What you expected to happen -->

## Actual Behavior
<!-- What actually happened -->

## Additional Context
<!-- Add any other context about the problem here, such as screenshots, logs, etc. --> 

--- README.md ---
<p align="center">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="docs/logo/dark.png" />
    <source media="(prefers-color-scheme: light)" srcset="docs/logo/light.png" />
    <img alt="PraisonAI Logo" src="docs/logo/light.png" />
  </picture>
</p>

<!-- mcp-name: io.github.MervinPraison/praisonai -->

<p align="center">
<a href="https://github.com/MervinPraison/PraisonAI"><img src="https://static.pepy.tech/badge/PraisonAI" alt="Total Downloads" /></a>
<a href="https://github.com/MervinPraison/PraisonAI"><img src="https://img.shields.io/github/v/release/MervinPraison/PraisonAI" alt="Latest Stable Version" /></a>
<a href="https://github.com/MervinPraison/PraisonAI"><img src="https://img.shields.io/badge/License-MIT-yellow.svg" alt="License" /></a>
<a href="https://registry.modelcontextprotocol.io/servers/io.github.MervinPraison/praisonai"><img src="https://img.shields.io/badge/MCP-Registry-blue" alt="MCP Registry" /></a>
</p>

<div align="center">

# Praison AI

<a href="https://trendshift.io/repositories/9130" target="_blank"><img src="https://trendshift.io/api/badge/repositories/9130" alt="MervinPraison%2FPraisonAI | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>

</div>

PraisonAI is a production-ready Multi-AI Agents framework with self-reflection, designed to create AI Agents to automate and solve problems ranging from simple tasks to complex challenges. It provides a low-code solution to streamline the building and management of multi-agent LLM systems, emphasising simplicity, customisation, and effective human-agent collaboration.

<div align="center">
  <a href="https://docs.praison.ai">
    <p align="center">
      <img src="https://img.shields.io/badge/ðŸ“š_Documentation-Visit_docs.praison.ai-blue?style=for-the-badge&logo=bookstack&logoColor=white" alt="Documentation" />
    </p>
  </a>
</div>

---

> **Quick Paths:**
> - ðŸ†• **New here?** â†’ [Quick Start](#-quick-start) *(1 minute to first agent)*
> - ðŸ“¦ **Installing?** â†’ [Installation](#-installation)
> - ðŸ **Python SDK?** â†’ [Python Examples](#-using-python-code)
> - ðŸŽ¯ **CLI user?** â†’ [CLI Quick Reference](#cli-quick-reference)
> - ðŸ”§ **Need config?** â†’ [Configuration](#-configuration--integration)
> - ðŸ¤ **Contributing?** â†’ [Development](#-development)

---

## ðŸ“‘ Table of Contents

<details open>
<summary><strong>Getting Started</strong></summary>

- [ðŸš€ Quick Start](#-quick-start)
- [ðŸ“¦ Installation](#-installation)
- [âš¡ Performance](#-performance)

</details>

<details>
<summary><strong>Python SDK</strong></summary>

- [ðŸ“˜ Python Examples](#-using-python-code)
  - [1. Single Agent](#1-single-agent) | [2. Multi Agents](#2-multi-agents) | [3. Planning Mode](#3-agent-with-planning-mode)
  - [4. Deep Research](#4-deep-research-agent) | [5. Query Rewriter](#5-query-rewriter-agent) | [6. Agent Memory](#6-agent-memory-zero-dependencies)
  - [7. Rules & Instructions](#7-rules--instructions) | [8. Auto-Generated Memories](#8-auto-generated-memories) | [9. Agentic Workflows](#9-agentic-workflows)
  - [10. Hooks](#10-hooks) | [11. Shadow Git Checkpoints](#11-shadow-git-checkpoints) | [12. Background Tasks](#12-background-tasks)
  - [13. Policy Engine](#13-policy-engine) | [14. Thinking Budgets](#14-thinking-budgets) | [15. Output Styles](#15-output-styles)
  - [16. Context Compaction](#16-context-compaction) | [17. Field Names Reference](#17-field-names-reference-a-i-g-s) | [18. Extended agents.yaml](#18-extended-agentsyaml-with-workflow-patterns)
  - [19. MCP Protocol](#19-mcp-model-context-protocol) | [20. A2A Protocol](#20-a2a-agent2agent-protocol)
- [ðŸ› ï¸ Custom Tools](#ï¸-custom-tools)

</details>

<details>
<summary><strong>JavaScript SDK</strong></summary>

- [ðŸ’» JavaScript Examples](#-using-javascript-code)

</details>

<details>
<summary><strong>CLI Reference</strong></summary>

- [ðŸŽ¯ CLI Overview](#-cli--no-code-interface) | [CLI Quick Reference](#cli-quick-reference)
- [Auto Mode](#auto-mode) | [Interactive Mode](#interactive-mode-cli) | [Deep Research CLI](#deep-research-cli) | [Planning Mode CLI](#planning-mode-cli)
- [Memory CLI](#memory-cli) | [Workflow CLI](#workflow-cli) | [Knowledge CLI](#knowledge-cli) | [Session CLI](#session-cli)
- [Tools CLI](#tools-cli) | [MCP Config CLI](#mcp-config-cli) | [External Agents CLI](#external-agents-cli) | [CLI Features Summary](#cli-features)

</details>

<details>
<summary><strong>Configuration & Features</strong></summary>

- [âœ¨ Key Features](#-key-features) | [ðŸŒ Supported Providers](#-supported-providers)
- [ðŸ”§ Configuration & Integration](#-configuration--integration) | [Ollama](#ollama-integration) | [Groq](#groq-integration) | [100+ Models](#100-models-support)
- [ðŸ“‹ Agents Playbook](#-agents-playbook)
- [ðŸ”¬ Advanced Features](#-advanced-features)

</details>

<details>
<summary><strong>Architecture & Patterns</strong></summary>

- [ðŸ“Š Process Types & Patterns](#-process-types--patterns)
- [Sequential](#sequential-process) | [Hierarchical](#hierarchical-process) | [Workflow](#workflow-process) | [Agentic Patterns](#agentic-patterns)

</details>

<details>
<summary><strong>Data & Persistence</strong></summary>

- [ðŸ’¾ Persistence (Databases)](#-persistence-databases)
- [ðŸ“š Knowledge & Retrieval (RAG)](#-knowledge--retrieval-rag)
- [ðŸ”§ Tools Table](#-tools-table)

</details>

<details>
<summary><strong>Learning & Community</strong></summary>

- [ðŸŽ“ Video Tutorials](#-video-tutorials) | [â­ Star History](#-star-history)
- [ðŸ‘¥ Contributing](#-contributing) | [ðŸ”§ Development](#-development) | [â“ FAQ & Troubleshooting](#-faq--troubleshooting)

</details>

---

## âš¡ Performance

PraisonAI Agents is the **fastest AI agent framework** for agent instantiation.

| Framework | Avg Time (Î¼s) | Relative |
|-----------|---------------|----------|
| **PraisonAI** | **3.77** | **1.00x (fastest)** |
| OpenAI Agents SDK | 5.26 | 1.39x |
| Agno | 5.64 | 1.49x |
| PraisonAI (LiteLLM) | 7.56 | 2.00x |
| PydanticAI | 226.94 | 60.16x |
| LangGraph | 4,558.71 | 1,209x |

<details>
<summary>Run benchmarks yourself</summary>

```bash
cd praisonai-agents
python benchmarks/simple_benchmark.py
```

</details>

---

## ðŸš€ Quick Start

Get started with PraisonAI in under 1 minute:

```bash
# Install
pip install praisonaiagents

# Set API key
export OPENAI_API_KEY=your_key_here

# Create a simple agent
python -c "from praisonaiagents import Agent; Agent(instructions='You are a helpful AI assistant').start('Write a haiku about AI')"
```

> **Next Steps:** [Single Agent Example](#1-single-agent) | [Multi Agents](#2-multi-agents) | [CLI Auto Mode](#auto-mode)

---

## ðŸ“¦ Installation

### Python SDK

Lightweight package dedicated for coding:

```bash
pip install praisonaiagents
```

For the full framework with CLI support:

```bash
pip install praisonai
```

### JavaScript SDK

```bash
npm install praisonai
```

### Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `OPENAI_API_KEY` | Yes* | OpenAI API key |
| `ANTHROPIC_API_KEY` | No | Anthropic Claude API key |
| `GOOGLE_API_KEY` | No | Google Gemini API key |
| `GROQ_API_KEY` | No | Groq API key |
| `OPENAI_BASE_URL` | No | Custom API endpoint (for Ollama, Groq, etc.) |

> *At least one LLM provider API key is required.

```bash
# Set your API key
export OPENAI_API_KEY=your_key_here

# For Ollama (local models)
export OPENAI_BASE_URL=http://localhost:11434/v1

# For Groq
export OPENAI_API_KEY=your_groq_key
export OPENAI_BASE_URL=https://api.groq.com/openai/v1
```

---

## âœ¨ Key Features

<details open>
<summary><strong>ðŸ¤– Core Agents</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Single Agent | [Example](examples/python/agents/single-agent.py) | [ðŸ“–](https://docs.praison.ai/agents/single) |
| Multi Agents | [Example](examples/python/general/mini_agents_example.py) | [ðŸ“–](https://docs.praison.ai/concepts/agents) |
| Auto Agents | [Example](examples/python/general/auto_agents_example.py) | [ðŸ“–](https://docs.praison.ai/features/autoagents) |
| Self Reflection AI Agents | [Example](examples/python/concepts/self-reflection-details.py) | [ðŸ“–](https://docs.praison.ai/features/selfreflection) |
| Reasoning AI Agents | [Example](examples/python/concepts/reasoning-extraction.py) | [ðŸ“–](https://docs.praison.ai/features/reasoning) |
| Multi Modal AI Agents | [Example](examples/python/general/multimodal.py) | [ðŸ“–](https://docs.praison.ai/features/multimodal) |

</details>

<details>
<summary><strong>ðŸ”„ Workflows</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Simple Workflow | [Example](examples/python/workflows/simple_workflow.py) | [ðŸ“–](https://docs.praison.ai/features/workflows) |
| Workflow with Agents | [Example](examples/python/workflows/workflow_with_agents.py) | [ðŸ“–](https://docs.praison.ai/features/workflows) |
| Agentic Routing (`route()`) | [Example](examples/python/workflows/workflow_routing.py) | [ðŸ“–](https://docs.praison.ai/features/routing) |
| Parallel Execution (`parallel()`) | [Example](examples/python/workflows/workflow_parallel.py) | [ðŸ“–](https://docs.praison.ai/features/parallelisation) |
| Loop over List/CSV (`loop()`) | [Example](examples/python/workflows/workflow_loop_csv.py) | [ðŸ“–](https://docs.praison.ai/features/repetitive) |
| Evaluator-Optimizer (`repeat()`) | [Example](examples/python/workflows/workflow_repeat.py) | [ðŸ“–](https://docs.praison.ai/features/evaluator-optimiser) |
| Conditional Steps | [Example](examples/python/workflows/workflow_conditional.py) | [ðŸ“–](https://docs.praison.ai/features/workflows) |
| Workflow Branching | [Example](examples/python/workflows/workflow_branching.py) | [ðŸ“–](https://docs.praison.ai/features/workflows) |
| Workflow Early Stop | [Example](examples/python/workflows/workflow_early_stop.py) | [ðŸ“–](https://docs.praison.ai/features/workflows) |
| Workflow Checkpoints | [Example](examples/python/workflows/workflow_checkpoints.py) | [ðŸ“–](https://docs.praison.ai/features/workflows) |

</details>

<details>
<summary><strong>ðŸ’» Code & Development</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Code Interpreter Agents | [Example](examples/python/agents/code-agent.py) | [ðŸ“–](https://docs.praison.ai/features/codeagent) |
| AI Code Editing Tools | [Example](examples/python/code/code_editing_example.py) | [ðŸ“–](https://docs.praison.ai/code/editing) |
| External Agents (All) | [Example](examples/python/code/external_agents_example.py) | [ðŸ“–](https://docs.praison.ai/code/external-agents) |
| Claude Code CLI | [Example](examples/python/code/claude_code_example.py) | [ðŸ“–](https://docs.praison.ai/code/claude-code) |
| Gemini CLI | [Example](examples/python/code/gemini_cli_example.py) | [ðŸ“–](https://docs.praison.ai/code/gemini-cli) |
| Codex CLI | [Example](examples/python/code/codex_cli_example.py) | [ðŸ“–](https://docs.praison.ai/code/codex-cli) |
| Cursor CLI | [Example](examples/python/code/cursor_cli_example.py) | [ðŸ“–](https://docs.praison.ai/code/cursor-cli) |

</details>

<details>
<summary><strong>ðŸ§  Memory & Knowledge</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Memory (Short & Long Term) | [Example](examples/python/general/memory_example.py) | [ðŸ“–](https://docs.praison.ai/concepts/memory) |
| File-Based Memory | [Example](examples/python/general/memory_example.py) | [ðŸ“–](https://docs.praison.ai/concepts/memory) |
| Claude Memory Tool | [Example](#claude-memory-tool-cli) | [ðŸ“–](https://docs.praison.ai/features/claude-memory-tool) |
| Add Custom Knowledge | [Example](examples/python/concepts/knowledge-agents.py) | [ðŸ“–](https://docs.praison.ai/features/knowledge) |
| RAG Agents | [Example](examples/python/concepts/rag-agents.py) | [ðŸ“–](https://docs.praison.ai/features/rag) |
| Chat with PDF Agents | [Example](examples/python/concepts/chat-with-pdf.py) | [ðŸ“–](https://docs.praison.ai/features/chat-with-pdf) |
| Data Readers (PDF, DOCX, etc.) | [CLI](#knowledge-cli) | [ðŸ“–](https://docs.praison.ai/api/praisonai/knowledge-readers-api) |
| Vector Store Selection | [CLI](#knowledge-cli) | [ðŸ“–](https://docs.praison.ai/api/praisonai/knowledge-vector-store-api) |
| Retrieval Strategies | [CLI](#knowledge-cli) | [ðŸ“–](https://docs.praison.ai/api/praisonai/knowledge-retrieval-api) |
| Rerankers | [CLI](#knowledge-cli) | [ðŸ“–](https://docs.praison.ai/api/praisonai/knowledge-reranker-api) |
| Index Types (Vector/Keyword/Hybrid) | [CLI](#knowledge-cli) | [ðŸ“–](https://docs.praison.ai/api/praisonai/knowledge-index-api) |
| Query Engines (Sub-Question, etc.) | [CLI](#knowledge-cli) | [ðŸ“–](https://docs.praison.ai/api/praisonai/knowledge-query-engine-api) |

</details>

<details>
<summary><strong>ðŸ”¬ Research & Intelligence</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Deep Research Agents | [Example](examples/python/agents/research-agent.py) | [ðŸ“–](https://docs.praison.ai/agents/deep-research) |
| Query Rewriter Agent | [Example](#5-query-rewriter-agent) | [ðŸ“–](https://docs.praison.ai/agents/query-rewriter) |
| Native Web Search | [Example](examples/python/agents/websearch-agent.py) | [ðŸ“–](https://docs.praison.ai/agents/websearch) |
| Built-in Search Tools | [Example](examples/python/agents/websearch-agent.py) | [ðŸ“–](https://docs.praison.ai/tools/tavily) |
| Unified Web Search | [Example](src/praisonai-agents/examples/web_search_example.py) | [ðŸ“–](https://docs.praison.ai/tools/web-search) |
| Web Fetch (Anthropic) | [Example](#web-search-web-fetch--prompt-caching) | [ðŸ“–](https://docs.praison.ai/features/model-capabilities) |

</details>

<details>
<summary><strong>ðŸ“‹ Planning & Execution</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Planning Mode | [Example](examples/python/agents/planning-agent.py) | [ðŸ“–](https://docs.praison.ai/features/planning-mode) |
| Planning Tools | [Example](#3-agent-with-planning-mode) | [ðŸ“–](https://docs.praison.ai/features/planning-mode) |
| Planning Reasoning | [Example](#3-agent-with-planning-mode) | [ðŸ“–](https://docs.praison.ai/features/planning-mode) |
| Prompt Chaining | [Example](examples/python/general/prompt_chaining.py) | [ðŸ“–](https://docs.praison.ai/features/promptchaining) |
| Evaluator Optimiser | [Example](examples/python/general/evaluator-optimiser.py) | [ðŸ“–](https://docs.praison.ai/features/evaluator-optimiser) |
| Orchestrator Workers | [Example](examples/python/general/orchestrator-workers.py) | [ðŸ“–](https://docs.praison.ai/features/orchestrator-worker) |

</details>

<details>
<summary><strong>ðŸ‘¥ Specialized Agents</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Data Analyst Agent | [Example](examples/python/agents/data-analyst-agent.py) | [ðŸ“–](https://docs.praison.ai/agents/data-analyst) |
| Finance Agent | [Example](examples/python/agents/finance-agent.py) | [ðŸ“–](https://docs.praison.ai/agents/finance) |
| Shopping Agent | [Example](examples/python/agents/shopping-agent.py) | [ðŸ“–](https://docs.praison.ai/agents/shopping) |
| Recommendation Agent | [Example](examples/python/agents/recommendation-agent.py) | [ðŸ“–](https://docs.praison.ai/agents/recommendation) |
| Wikipedia Agent | [Example](examples/python/agents/wikipedia-agent.py) | [ðŸ“–](https://docs.praison.ai/agents/wikipedia) |
| Programming Agent | [Example](examples/python/agents/programming-agent.py) | [ðŸ“–](https://docs.praison.ai/agents/programming) |
| Math Agents | [Example](examples/python/agents/math-agent.py) | [ðŸ“–](https://docs.praison.ai/features/mathagent) |
| Markdown Agent | [Example](examples/python/agents/markdown-agent.py) | [ðŸ“–](https://docs.praison.ai/agents/markdown) |
| Prompt Expander Agent | [Example](#prompt-expansion) | [ðŸ“–](https://docs.praison.ai/agents/prompt-expander) |

</details>

<details>
<summary><strong>ðŸŽ¨ Media & Multimodal</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Image Generation Agent | [Example](examples/python/image/image-agent.py) | [ðŸ“–](https://docs.praison.ai/features/image-generation) |
| Image to Text Agent | [Example](examples/python/agents/image-to-text-agent.py) | [ðŸ“–](https://docs.praison.ai/agents/image-to-text) |
| Video Agent | [Example](examples/python/agents/video-agent.py) | [ðŸ“–](https://docs.praison.ai/agents/video) |
| Camera Integration | [Example](examples/python/camera/) | [ðŸ“–](https://docs.praison.ai/features/camera-integration) |

</details>

<details>
<summary><strong>ðŸ”Œ Protocols & Integration</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| MCP Transports | [Example](examples/python/mcp/mcp-transports-overview.py) | [ðŸ“–](https://docs.praison.ai/mcp/transports) |
| WebSocket MCP | [Example](examples/python/mcp/websocket-mcp.py) | [ðŸ“–](https://docs.praison.ai/mcp/sse-transport) |
| MCP Security | [Example](examples/python/mcp/mcp-security.py) | [ðŸ“–](https://docs.praison.ai/mcp/transports) |
| MCP Resumability | [Example](examples/python/mcp/mcp-resumability.py) | [ðŸ“–](https://docs.praison.ai/mcp/sse-transport) |
| MCP Config Management | [Example](#mcp-config-cli) | [ðŸ“–](https://docs.praison.ai/docs/cli/mcp) |
| LangChain Integrated Agents | [Example](examples/python/general/langchain_example.py) | [ðŸ“–](https://docs.praison.ai/features/langchain) |

</details>

<details>
<summary><strong>ðŸ›¡ï¸ Safety & Control</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Guardrails | [Example](examples/python/guardrails/comprehensive-guardrails-example.py) | [ðŸ“–](https://docs.praison.ai/features/guardrails) |
| Human Approval | [Example](examples/python/general/human_approval_example.py) | [ðŸ“–](https://docs.praison.ai/features/approval) |
| Rules & Instructions | [Example](#7-rules--instructions) | [ðŸ“–](https://docs.praison.ai/features/rules) |

</details>

<details>
<summary><strong>âš™ï¸ Advanced Features</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Async & Parallel Processing | [Example](examples/python/general/async_example.py) | [ðŸ“–](https://docs.praison.ai/features/async) |
| Parallelisation | [Example](examples/python/general/parallelisation.py) | [ðŸ“–](https://docs.praison.ai/features/parallelisation) |
| Repetitive Agents | [Example](examples/python/concepts/repetitive-agents.py) | [ðŸ“–](https://docs.praison.ai/features/repetitive) |
| Agent Handoffs | [Example](examples/python/handoff/handoff_basic.py) | [ðŸ“–](https://docs.praison.ai/features/handoffs) |
| Stateful Agents | [Example](examples/python/stateful/workflow-state-example.py) | [ðŸ“–](https://docs.praison.ai/features/stateful-agents) |
| Autonomous Workflow | [Example](examples/python/general/autonomous-agent.py) | [ðŸ“–](https://docs.praison.ai/features/autonomous-workflow) |
| Structured Output Agents | [Example](examples/python/general/structured_agents_example.py) | [ðŸ“–](https://docs.praison.ai/features/structured) |
| Model Router | [Example](examples/python/agents/router-agent-cost-optimization.py) | [ðŸ“–](https://docs.praison.ai/features/model-router) |
| Prompt Caching | [Example](#web-search-web-fetch--prompt-caching) | [ðŸ“–](https://docs.praison.ai/features/model-capabilities) |
| Fast Context | [Example](examples/context/00_agent_fast_context_basic.py) | [ðŸ“–](https://docs.praison.ai/features/fast-context) |

</details>

<details>
<summary><strong>ðŸ› ï¸ Tools & Configuration</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| 100+ Custom Tools | [Example](examples/python/general/tools_example.py) | [ðŸ“–](https://docs.praison.ai/tools/tools) |
| YAML Configuration | [Example](examples/cookbooks/yaml/secondary_market_research_agents.yaml) | [ðŸ“–](https://docs.praison.ai/developers/agents-playbook) |
| 100+ LLM Support | [Example](examples/python/providers/openai/openai_gpt4_example.py) | [ðŸ“–](https://docs.praison.ai/models) |
| Callback Agents | [Example](examples/python/general/advanced-callback-systems.py) | [ðŸ“–](https://docs.praison.ai/features/callbacks) |
| Hooks | [Example](#10-hooks) | [ðŸ“–](https://docs.praison.ai/features/hooks) |
| Middleware System | [Example](examples/middleware/basic_middleware.py) | [ðŸ“–](https://docs.praison.ai/features/middleware) |
| Configurable Model | [Example](examples/middleware/configurable_model.py) | [ðŸ“–](https://docs.praison.ai/features/configurable-model) |
| Rate Limiter | [Example](examples/middleware/rate_limiter.py) | [ðŸ“–](https://docs.praison.ai/features/rate-limiter) |
| Injected Tool State | [Example](examples/middleware/injected_state.py) | [ðŸ“–](https://docs.praison.ai/features/injected-state) |
| Shadow Git Checkpoints | [Example](#11-shadow-git-checkpoints) | [ðŸ“–](https://docs.praison.ai/features/checkpoints) |
| Background Tasks | [Example](examples/background/basic_background.py) | [ðŸ“–](https://docs.praison.ai/features/background-tasks) |
| Policy Engine | [Example](examples/policy/basic_policy.py) | [ðŸ“–](https://docs.praison.ai/features/policy-engine) |
| Thinking Budgets | [Example](examples/thinking/basic_thinking.py) | [ðŸ“–](https://docs.praison.ai/features/thinking-budgets) |
| Output Styles | [Example](examples/output/basic_output.py) | [ðŸ“–](https://docs.praison.ai/features/output-styles) |
| Context Compaction | [Example](examples/compaction/basic_compaction.py) | [ðŸ“–](https://docs.praison.ai/features/context-compaction) |

</details>

<details>
<summary><strong>ðŸ“Š Monitoring & Management</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Sessions Management | [Example](examples/python/sessions/comprehensive-session-management.py) | [ðŸ“–](https://docs.praison.ai/features/sessions) |
| Auto-Save Sessions | [Example](#session-management-python) | [ðŸ“–](https://docs.praison.ai/docs/cli/session) |
| History in Context | [Example](#session-management-python) | [ðŸ“–](https://docs.praison.ai/docs/cli/session) |
| Telemetry | [Example](examples/python/telemetry/production-telemetry-example.py) | [ðŸ“–](https://docs.praison.ai/features/telemetry) |
| Project Docs (.praison/docs/) | [Example](#docs-cli) | [ðŸ“–](https://docs.praison.ai/docs/cli/docs) |
| AI Commit Messages | [Example](#ai-commit-cli) | [ðŸ“–](https://docs.praison.ai/docs/cli/commit) |
| @Mentions in Prompts | [Example](#mentions-in-prompts) | [ðŸ“–](https://docs.praison.ai/docs/cli/mentions) |

</details>

<details>
<summary><strong>ðŸ–¥ï¸ CLI Features</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Slash Commands | [Example](examples/python/cli/slash_commands_example.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/slash-commands) |
| Autonomy Modes | [Example](examples/python/cli/autonomy_modes_example.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/autonomy-modes) |
| Cost Tracking | [Example](examples/python/cli/cost_tracking_example.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/cost-tracking) |
| Repository Map | [Example](examples/python/cli/repo_map_example.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/repo-map) |
| Interactive TUI | [Example](examples/python/cli/interactive_tui_example.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/interactive-tui) |
| Git Integration | [Example](examples/python/cli/git_integration_example.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/git-integration) |
| Sandbox Execution | [Example](examples/python/cli/sandbox_execution_example.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/sandbox-execution) |
| CLI Compare | [Example](examples/compare/cli_compare_basic.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/compare) |
| Profile/Benchmark | [Example](#profile-benchmark) | [ðŸ“–](https://docs.praison.ai/docs/cli/profile) |
| Auto Mode | [Example](#auto-mode) | [ðŸ“–](https://docs.praison.ai/docs/cli/auto) |
| Init | [Example](#init) | [ðŸ“–](https://docs.praison.ai/docs/cli/init) |
| File Input | [Example](#file-input) | [ðŸ“–](https://docs.praison.ai/docs/cli/file-input) |
| Final Agent | [Example](#final-agent) | [ðŸ“–](https://docs.praison.ai/docs/cli/final-agent) |
| Max Tokens | [Example](#max-tokens) | [ðŸ“–](https://docs.praison.ai/docs/cli/max-tokens) |

</details>

<details>
<summary><strong>ðŸ§ª Evaluation</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Accuracy Evaluation | [Example](examples/eval/accuracy_example.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/eval) |
| Performance Evaluation | [Example](examples/eval/performance_example.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/eval) |
| Reliability Evaluation | [Example](examples/eval/reliability_example.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/eval) |
| Criteria Evaluation | [Example](examples/eval/criteria_example.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/eval) |

</details>

<details>
<summary><strong>ðŸŽ¯ Agent Skills</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Skills Management | [Example](examples/skills/basic_skill_usage.py) | [ðŸ“–](https://docs.praison.ai/features/skills) |
| Custom Skills | [Example](examples/skills/custom_skill_example.py) | [ðŸ“–](https://docs.praison.ai/features/skills) |

</details>

<details>
<summary><strong>â° 24/7 Scheduling</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Agent Scheduler | [Example](examples/python/scheduled_agents/news_checker_live.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/scheduler) |

</details>

---

## ðŸŒ Supported Providers

PraisonAI supports 100+ LLM providers through seamless integration:

<details>
<summary><strong>View all 24 providers</strong></summary>

| Provider | Example |
|----------|:-------:|
| OpenAI | [Example](examples/python/providers/openai/openai_gpt4_example.py) |
| Anthropic | [Example](examples/python/providers/anthropic/anthropic_claude_example.py) |
| Google Gemini | [Example](examples/python/providers/google/google_gemini_example.py) |
| Ollama | [Example](examples/python/providers/ollama/ollama-agents.py) |
| Groq | [Example](examples/python/providers/groq/kimi_with_groq_example.py) |
| DeepSeek | [Example](examples/python/providers/deepseek/deepseek_example.py) |
| xAI Grok | [Example](examples/python/providers/xai/xai_grok_example.py) |
| Mistral | [Example](examples/python/providers/mistral/mistral_example.py) |
| Cohere | [Example](examples/python/providers/cohere/cohere_example.py) |
| Perplexity | [Example](examples/python/providers/perplexity/perplexity_example.py) |
| Fireworks | [Example](examples/python/providers/fireworks/fireworks_example.py) |
| Together AI | [Example](examples/python/providers/together/together_ai_example.py) |
| OpenRouter | [Example](examples/python/providers/openrouter/openrouter_example.py) |
| HuggingFace | [Example](examples/python/providers/huggingface/huggingface_example.py) |
| Azure OpenAI | [Example](examples/python/providers/azure/azure_openai_example.py) |
| AWS Bedrock | [Example](examples/python/providers/aws/aws_bedrock_example.py) |
| Google Vertex | [Example](examples/python/providers/vertex/vertex_example.py) |
| Databricks | [Example](examples/python/providers/databricks/databricks_example.py) |
| Cloudflare | [Example](examples/python/providers/cloudflare/cloudflare_example.py) |
| AI21 | [Example](examples/python/providers/ai21/ai21_example.py) |
| Replicate | [Example](examples/python/providers/replicate/replicate_example.py) |
| SageMaker | [Example](examples/python/providers/sagemaker/sagemaker_example.py) |
| Moonshot | [Example](examples/python/providers/moonshot/moonshot_example.py) |
| vLLM | [Example](examples/python/providers/vllm/vllm_example.py) |

</details>

---

## ðŸ“˜ Using Python Code

### 1. Single Agent

Create app.py file and add the code below:
```python
from praisonaiagents import Agent
agent = Agent(instructions="Your are a helpful AI assistant")
agent.start("Write a movie script about a robot in Mars")
```

Run:
```bash
python app.py
```

### 2. Multi Agents

Create app.py file and add the code below:
```python
from praisonaiagents import Agent, Agents

research_agent = Agent(instructions="Research about AI")
summarise_agent = Agent(instructions="Summarise research agent's findings")
agents = Agents(agents=[research_agent, summarise_agent])
agents.start()
```

Run:
```bash
python app.py
```

### 3. Agent with Planning Mode

Enable planning for any agent - the agent creates a plan, then executes step by step:

```python
from praisonaiagents import Agent

def search_web(query: str) -> str:
    return f"Search results for: {query}"

agent = Agent(
    name="AI Assistant",
    instructions="Research and write about topics",
    planning=True,              # Enable planning mode
    planning_tools=[search_web], # Tools for planning research
    planning_reasoning=True      # Chain-of-thought reasoning
)

result = agent.start("Research AI trends in 2025 and write a summary")
```

**What happens:**
1. ðŸ“‹ Agent creates a multi-step plan
2. ðŸš€ Executes each step sequentially
3. ðŸ“Š Shows progress with context passing
4. âœ… Returns final result

### 4. Deep Research Agent

Automated research with real-time streaming, web search, and citations using OpenAI or Gemini Deep Research APIs.

```python
from praisonaiagents import DeepResearchAgent

# OpenAI Deep Research
agent = DeepResearchAgent(
    model="o4-mini-deep-research",  # or "o3-deep-research"
    verbose=True
)

result = agent.research("What are the latest AI trends in 2025?")
print(result.report)
print(f"Citations: {len(result.citations)}")
```

```python
# Gemini Deep Research
from praisonaiagents import DeepResearchAgent

agent = DeepResearchAgent(
    model="deep-research-pro",  # Auto-detected as Gemini
    verbose=True
)

result = agent.research("Research quantum computing advances")
print(result.report)
```

**Features:**
- ðŸ” Multi-provider support (OpenAI, Gemini, LiteLLM)
- ðŸ“¡ Real-time streaming with reasoning summaries
- ðŸ“š Structured citations with URLs
- ðŸ› ï¸ Built-in tools: web search, code interpreter, MCP, file search
- ðŸ”„ Automatic provider detection from model name

### 5. Query Rewriter Agent

Transform user queries to improve RAG retrieval quality using multiple strategies.

```python
from praisonaiagents import QueryRewriterAgent, RewriteStrategy

agent = QueryRewriterAgent(model="gpt-4o-mini")

# Basic - expands abbreviations, adds context
result = agent.rewrite("AI trends")
print(result.primary_query)  # "What are the current trends in Artificial Intelligence?"

# HyDE - generates hypothetical document for semantic matching
result = agent.rewrite("What is quantum computing?", strategy=RewriteStrategy.HYDE)

# Step-back - generates broader context question
result = agent.rewrite("GPT-4 vs Claude 3?", strategy=RewriteStrategy.STEP_BACK)

# Sub-queries - decomposes complex questions
result = agent.rewrite("RAG setup and best embedding models?", strategy=RewriteStrategy.SUB_QUERIES)

# Contextual - resolves references using chat history
result = agent.rewrite("What about cost?", chat_history=[...])
```

**Strategies:**
- **BASIC**: Expand abbreviations, fix typos, add context
- **HYDE**: Generate hypothetical document for semantic matching
- **STEP_BACK**: Generate higher-level concept questions
- **SUB_QUERIES**: Decompose multi-part questions
- **MULTI_QUERY**: Generate multiple paraphrased versions
- **CONTEXTUAL**: Resolve references using conversation history
- **AUTO**: Automatically detect best strategy

### 6. Agent Memory (Zero Dependencies)

Enable persistent memory for agents - works out of the box without any extra packages.

```python
from praisonaiagents import Agent
from praisonaiagents.memory import FileMemory

# Enable memory with a single parameter
agent = Agent(
    name="Personal Assistant",
    instructions="You are a helpful assistant that remembers user preferences.",
    memory=True,  # Enables file-based memory (no extra deps!)
    user_id="user123"  # Isolate memory per user
)

# Memory is automatically injected into conversations
result = agent.start("My name is John and I prefer Python")
# Agent will remember this for future conversations
```

**Memory Types:**
- **Short-term**: Rolling buffer of recent context (auto-expires)
- **Long-term**: Persistent important facts (sorted by importance)
- **Entity**: People, places, organizations with attributes
- **Episodic**: Date-based interaction history

**Advanced Features:**
```python
from praisonaiagents.memory import FileMemory

memory = FileMemory(user_id="user123")

# Session Save/Resume
memory.save_session("project_session", conversation_history=[...])
memory.resume_session("project_session")

# Context Compression
memory.compress(llm_func=lambda p: agent.chat(p), max_items=10)

# Checkpointing
memory.create_checkpoint("before_refactor", include_files=["main.py"])
memory.restore_checkpoint("before_refactor", restore_files=True)

# Slash Commands
memory.handle_command("/memory show")
memory.handle_command("/memory save my_session")
```

**Storage Options:**
| Option | Dependencies | Description |
|--------|-------------|-------------|
| `memory=True` | None | File-based JSON storage (default) |
| `memory="file"` | None | Explicit file-based storage |
| `memory="sqlite"` | Built-in | SQLite with indexing |
| `memory="chromadb"` | chromadb | Vector/semantic search |

### 7. Rules & Instructions

PraisonAI auto-discovers instruction files from your project root and git root:

| File | Description | Priority |
|------|-------------|----------|
| `PRAISON.md` | PraisonAI native instructions | High |
| `PRAISON.local.md` | Local overrides (gitignored) | Higher |
| `CLAUDE.md` | Claude Code memory file | High |
| `CLAUDE.local.md` | Local overrides (gitignored) | Higher |
| `AGENTS.md` | OpenAI Codex CLI instructions | High |
| `GEMINI.md` | Gemini CLI memory file | High |
| `.cursorrules` | Cursor IDE rules | High |
| `.windsurfrules` | Windsurf IDE rules | High |
| `.claude/rules/*.md` | Claude Code modular rules | Medium |
| `.windsurf/rules/*.md` | Windsurf modular rules | Medium |
| `.cursor/rules/*.mdc` | Cursor modular rules | Medium |
| `.praison/rules/*.md` | Workspace rules | Medium |
| `~/.praison/rules/*.md` | Global rules | Low |

```python
from praisonaiagents import Agent

# Agent auto-discovers CLAUDE.md, AGENTS.md, GEMINI.md, etc.
agent = Agent(name="Assistant", instructions="You are helpful.")
# Rules are injected into system prompt automatically
```

**@Import Syntax:**
```markdown
# CLAUDE.md
See @README for project overview
See @docs/architecture.md for system design
@~/.praison/my-preferences.md
```

**Rule File Format (with YAML frontmatter):**
```markdown
---
description: Python coding guidelines
globs: ["**/*.py"]
activation: always  # always, glob, manual, ai_decision
---

# Guidelines
- Use type hints
- Follow PEP 8
```

### 8. Auto-Generated Memories

```python
from praisonaiagents.memory import FileMemory, AutoMemory

memory = FileMemory(user_id="user123")
auto = AutoMemory(memory, enabled=True)

# Automatically extracts and stores memories from conversations
memories = auto.process_interaction(
    "My name is John and I prefer Python for backend work"
)
# Extracts: name="John", preference="Python for backend"
```

### 9. Agentic Workflows

Create powerful multi-agent workflows with the `Workflow` class:

```python
from praisonaiagents import Agent, Workflow

# Create agents
researcher = Agent(
    name="Researcher",
    role="Research Analyst",
    goal="Research topics thoroughly",
    instructions="Provide concise, factual information."
)

writer = Agent(
    name="Writer",
    role="Content Writer", 
    goal="Write engaging content",
    instructions="Write clear, engaging content based on research."
)

# Create workflow with agents as steps
workflow = Workflow(steps=[researcher, writer])

# Run workflow - agents process sequentially
result = workflow.start("What are the benefits of AI agents?")
print(result["output"])
```

**Key Features:**
- **Agent-first** - Pass `Agent` objects directly as workflow steps
- **Pattern helpers** - Use `route()`, `parallel()`, `loop()`, `repeat()`
- **Planning mode** - Enable with `planning=True`
- **Callbacks** - Monitor with `on_step_complete`, `on_workflow_complete`
- **Async execution** - Use `workflow.astart()` for async

### Workflow Patterns (route, parallel, loop, repeat)

```python
from praisonaiagents import Agent, Workflow
from praisonaiagents.workflows import route, parallel, loop, repeat

# 1. ROUTING - Classifier agent routes to specialized agents
classifier = Agent(name="Classifier", instructions="Respond with 'technical' or 'creative'")
tech_agent = Agent(name="TechExpert", role="Technical Expert")
creative_agent = Agent(name="Creative", role="Creative Writer")

workflow = Workflow(steps=[
    classifier,
    route({
        "technical": [tech_agent],
        "creative": [creative_agent]
    })
])

# 2. PARALLEL - Multiple agents work concurrently
market_agent = Agent(name="Market", role="Market Researcher")
competitor_agent = Agent(name="Competitor", role="Competitor Analyst")
aggregator = Agent(name="Aggregator", role="Synthesizer")

workflow = Workflow(steps=[
    parallel([market_agent, competitor_agent]),
    aggregator
])

# 3. LOOP - Agent processes each item
processor = Agent(name="Processor", role="Item Processor")
summarizer = Agent(name="Summarizer", role="Summarizer")

workflow = Workflow(
    steps=[loop(processor, over="items"), summarizer],
    variables={"items": ["AI", "ML", "NLP"]}
)

# 4. REPEAT - Evaluator-Optimizer pattern
generator = Agent(name="Generator", role="Content Generator")
evaluator = Agent(name="Evaluator", instructions="Say 'APPROVED' if good")

workflow = Workflow(steps=[
    generator,
    repeat(evaluator, until=lambda ctx: "approved" in ctx.previous_result.lower(), max_iterations=3)
])

# 5. CALLBACKS
workflow = Workflow(
    steps=[researcher, writer],
    on_step_complete=lambda name, r: print(f"âœ… {name} done")
)

# 6. WITH PLANNING & REASONING
workflow = Workflow(
    steps=[researcher, writer],
    planning=True,
    reasoning=True
)

# 7. ASYNC EXECUTION
result = asyncio.run(workflow.astart("input"))

# 8. STATUS TRACKING
workflow.status  # "not_started" | "running" | "completed"
workflow.step_statuses  # {"step1": "completed", "step2": "skipped"}
```

### YAML Workflow Template

```yaml
# .praison/workflows/research.yaml
name: Research Workflow
description: Research and write content with all patterns

agents:
  researcher:
    role: Research Expert
    goal: Find accurate information
    tools: [tavily_search, web_scraper]
  writer:
    role: Content Writer
    goal: Write engaging content
  editor:
    role: Editor
    goal: Polish content

steps:
  # Sequential
  - agent: researcher
    action: Research {{topic}}
    output_variable: research_data

  # Routing
  - name: classifier
    action: Classify content type
    route:
      technical: [tech_handler]
      creative: [creative_handler]
      default: [general_handler]

  # Parallel
  - name: parallel_research
    parallel:
      - agent: researcher
        action: Research market
      - agent: researcher
        action: Research competitors

  # Loop
  - agent: writer
    action: Write about {{item}}
    loop_over: topics
    loop_var: item

  # Repeat (evaluator-optimizer)
  - agent: editor
    action: Review and improve
    repeat:
      until: "quality > 8"
      max_iterations: 3

  # Output to file
  - agent: writer
    action: Write final report
    output_file: output/{{topic}}_report.md

variables:
  topic: AI trends
  topics: [ML, NLP, Vision]

workflow:
  planning: true
  planning_llm: gpt-4o
  memory_config:
    provider: chroma
    persist: true
```

### Loading YAML Workflows

```python
from praisonaiagents.workflows import YAMLWorkflowParser, WorkflowManager

# Option 1: Parse YAML string
parser = YAMLWorkflowParser()
workflow = parser.parse_string(yaml_content)
result = workflow.start("Research AI trends")

# Option 2: Load from file with WorkflowManager
manager = WorkflowManager()
workflow = manager.load_yaml("research_workflow.yaml")
result = workflow.start("Research AI trends")

# Option 3: Execute YAML directly
result = manager.execute_yaml(
    "research_workflow.yaml",
    input_data="Research AI trends",
    variables={"topic": "Machine Learning"}
)
```

### Complete workflow.yaml Reference

```yaml
# workflow.yaml - Full feature reference
name: Complete Workflow
description: Demonstrates all workflow.yaml features
framework: praisonai  # praisonai, crewai, autogen
process: workflow     # sequential, hierarchical, workflow

workflow:
  planning: true
  planning_llm: gpt-4o
  reasoning: true
  verbose: true
  memory_config:
    provider: chroma
    persist: true

variables:
  topic: AI trends
  items: [ML, NLP, Vision]

agents:
  researcher:
    name: Researcher
    role: Research Analyst
    goal: Research topics thoroughly
    instructions: "Provide detailed research findings"
    backstory: "Expert researcher with 10 years experience"  # alias for instructions
    llm: gpt-4o-mini
    function_calling_llm: gpt-4o      # For tool calls
    max_rpm: 10                        # Rate limiting
    max_execution_time: 300            # Timeout in seconds
    reflect_llm: gpt-4o               # For self-reflection
    min_reflect: 1
    max_reflect: 3
    system_template: "You are a helpful assistant"
    tools:
      - tavily_search

  writer:
    name: Writer
    role: Content Writer
    goal: Write clear content
    instructions: "Write engaging content"

steps:
  - name: research_step
    agent: researcher
    action: "Research {{topic}}"
    expected_output: "Comprehensive research report"
    output_file: "output/research.md"
    create_directory: true
    
  - name: writing_step
    agent: writer
    action: "Write article based on research"
    context:                          # Task dependencies
      - research_step
    output_json:                      # Structured output
      type: object
      properties:
        title: { type: string }
        content: { type: string }

callbacks:
  on_workflow_start: log_start
  on_step_complete: log_step
  on_workflow_complete: log_complete
```

### 10. Hooks

Intercept and modify agent behavior at various lifecycle points:

```python
from praisonaiagents.hooks import (
    HookRegistry, HookRunner, HookEvent, HookResult,
    BeforeToolInput
)

# Create a hook registry
registry = HookRegistry()

# Log all tool calls
@registry.on(HookEvent.BEFORE_TOOL)
def log_tools(event_data: BeforeToolInput) -> HookResult:
    print(f"Tool: {event_data.tool_name}")
    return HookResult.allow()

# Block dangerous operations
@registry.on(HookEvent.BEFORE_TOOL)
def security_check(event_data: BeforeToolInput) -> HookResult:
    if "delete" in event_data.tool_name.lower():
        return HookResult.deny("Delete operations blocked")
    return HookResult.allow()

# Execute hooks
runner = HookRunner(registry)
```

**CLI Commands:**
```bash
praisonai hooks list                    # List registered hooks
praisonai hooks test before_tool        # Test hooks for an event
praisonai hooks run "echo test"         # Run a command hook
praisonai hooks validate hooks.json     # Validate configuration
```


### 11. Shadow Git Checkpoints

File-level undo/restore using shadow git:

```python
from praisonaiagents.checkpoints import CheckpointService

service = CheckpointService(workspace_dir="./my_project")
await service.initialize()

# Save checkpoint before changes
result = await service.save("Before refactoring")

# Make changes...

# Restore if needed
await service.restore(result.checkpoint.id)

# View diff
diff = await service.diff()
```

**CLI Commands:**
```bash
praisonai checkpoint save "Before changes"  # Save checkpoint
praisonai checkpoint list                   # List checkpoints
praisonai checkpoint diff                   # Show changes
praisonai checkpoint restore abc123         # Restore to checkpoint
```

**Links:**
- [ðŸ“– Coding Docs](https://docs.praison.ai/features/checkpoints)
- [ðŸ“– CLI Docs](https://docs.praison.ai/docs/cli/checkpoint)
- [ðŸ’» Example](examples/checkpoints/basic_checkpoints.py)

---

### 12. Background Tasks

Run agent tasks asynchronously without blocking:

```python
import asyncio
from praisonaiagents.background import BackgroundRunner, BackgroundConfig

async def main():
    config = BackgroundConfig(max_concurrent_tasks=3)
    runner = BackgroundRunner(config=config)
    
    async def my_task(name: str) -> str:
        await asyncio.sleep(2)
        return f"Task {name} completed"
    
    task = await runner.submit(my_task, args=("example",), name="my_task")
    await task.wait(timeout=10.0)
    print(task.result)

asyncio.run(main())
```

**CLI Commands:**
```bash
praisonai background list          # List running tasks
praisonai background status <id>   # Check task status
praisonai background cancel <id>   # Cancel a task
praisonai background clear         # Clear completed tasks
```

**Links:**
- [ðŸ“– Coding Docs](https://docs.praison.ai/features/background-tasks)
- [ðŸ“– CLI Docs](https://docs.praison.ai/docs/cli/background)
- [ðŸ’» Example](examples/background/basic_background.py)

---

### 13. Policy Engine

Control what agents can and cannot do with policy-based execution:

```python
from praisonaiagents.policy import (
    PolicyEngine, Policy, PolicyRule, PolicyAction
)

engine = PolicyEngine()

policy = Policy(
    name="no_delete",
    rules=[
        PolicyRule(
            action=PolicyAction.DENY,
            resource="tool:delete_*",
            reason="Delete operations blocked"
        )
    ]
)
engine.add_policy(policy)

result = engine.check("tool:delete_file", {})
print(f"Allowed: {result.allowed}")
```

**CLI Commands:**
```bash
praisonai policy list                  # List policies
praisonai policy check "tool:name"     # Check if allowed
praisonai policy init                  # Create template
```

**Links:**
- [ðŸ“– Coding Docs](https://docs.praison.ai/features/policy-engine)
- [ðŸ“– CLI Docs](https://docs.praison.ai/docs/cli/policy)
- [ðŸ’» Example](examples/policy/basic_policy.py)

---

### 14. Thinking Budgets

Configure token budgets for extended thinking:

```python
from praisonaiagents.thinking import ThinkingBudget, ThinkingTracker

# Use predefined levels
budget = ThinkingBudget.high()  # 16,000 tokens

# Track usage
tracker = ThinkingTracker()
session = tracker.start_session(budget_tokens=16000)
tracker.end_session(session, tokens_used=12000)

summary = tracker.get_summary()
print(f"Utilization: {summary['average_utilization']:.1%}")
```

**CLI Commands:**
```bash
praisonai thinking status      # Show current budget
praisonai thinking set high    # Set budget level
praisonai thinking stats       # Show usage statistics
```

**Links:**
- [ðŸ“– Coding Docs](https://docs.praison.ai/features/thinking-budgets)
- [ðŸ“– CLI Docs](https://docs.praison.ai/docs/cli/thinking)
- [ðŸ’» Example](examples/thinking/basic_thinking.py)

---

### 15. Output Styles

Configure how agents format their responses:

```python
from praisonaiagents.output import OutputStyle, OutputFormatter

# Use preset styles
style = OutputStyle.concise()
formatter = OutputFormatter(style)

# Format output
text = "# Hello\n\nThis is **bold** text."
plain = formatter.format(text)
print(plain)
```

**CLI Commands:**
```bash
praisonai output status        # Show current style
praisonai output set concise   # Set output style
```

**Links:**
- [ðŸ“– Coding Docs](https://docs.praison.ai/features/output-styles)
- [ðŸ“– CLI Docs](https://docs.praison.ai/docs/cli/output-style)
- [ðŸ’» Example](examples/output/basic_output.py)

---

### 16. Context Compaction

Automatically manage context window size:

```python
from praisonaiagents.compaction import (
    ContextCompactor, CompactionStrategy
)

compactor = ContextCompactor(
    max_tokens=4000,
    strategy=CompactionStrategy.SLIDING,
    preserve_recent=3
)

messages = [...]  # Your conversation history
compacted, result = compactor.compact(messages)

print(f"Compression: {result.compression_ratio:.1%}")
```

**CLI Commands:**
```bash
praisonai compaction status        # Show settings
praisonai compaction set sliding   # Set strategy
praisonai compaction stats         # Show statistics
```

**Links:**
- [ðŸ“– Coding Docs](https://docs.praison.ai/features/context-compaction)
- [ðŸ“– CLI Docs](https://docs.praison.ai/docs/cli/compaction)
- [ðŸ’» Example](examples/compaction/basic_compaction.py)

---

### 17. Field Names Reference (A-I-G-S)

PraisonAI accepts both old (agents.yaml) and new (workflow.yaml) field names. Use the **canonical names** for new projects:

| Canonical (Recommended) | Alias (Also Works) | Purpose |
|-------------------------|-------------------|---------|
| `agents` | `roles` | Define agent personas |
| `instructions` | `backstory` | Agent behavior/persona |
| `action` | `description` | What the step does |
| `steps` | `tasks` (nested) | Define work items |
| `name` | `topic` | Workflow identifier |

**A-I-G-S Mnemonic** - Easy to remember:
- **A**gents - Who does the work
- **I**nstructions - How they behave  
- **G**oal - What they achieve
- **S**teps - What they do

```yaml
# Quick Reference - Canonical Format
name: My Workflow              # Workflow name (not 'topic')
agents:                        # Define agents (not 'roles')
  my_agent:
    role: Job Title            # Agent's role
    goal: What to achieve      # Agent's goal
    instructions: How to act   # Agent's behavior (not 'backstory')
    
steps:                         # Define steps (not 'tasks')
  - agent: my_agent
    action: What to do         # Step action (not 'description')
```

> **Note:** The parser accepts both old and new names. Run `praisonai workflow validate <file.yaml>` to see suggestions for canonical names.

### 18. Extended agents.yaml with Workflow Patterns

**Feature Parity:** Both `agents.yaml` and `workflow.yaml` now support the same features:
- All workflow patterns (route, parallel, loop, repeat)
- All agent fields (function_calling_llm, max_rpm, max_execution_time, reflect_llm, templates)
- All step fields (expected_output, context, output_json, create_directory, callback)
- Framework support (praisonai, crewai, autogen)
- Process types (sequential, hierarchical, workflow)

You can use advanced workflow patterns directly in agents.yaml by setting `process: workflow`:

```yaml
# agents.yaml with workflow patterns
framework: praisonai
process: workflow  # Enables workflow mode
topic: "Research AI trends"

workflow:
  planning: true
  reasoning: true
  verbose: true

variables:
  topic: AI trends

agents:  # Canonical: use 'agents' instead of 'roles'
  classifier:
    role: Request Classifier
    instructions: "Classify requests into categories"  # Canonical: use 'instructions' instead of 'backstory'
    goal: Classify requests
    
  researcher:
    role: Research Analyst
    instructions: "Expert researcher"  # Canonical: use 'instructions' instead of 'backstory'
    goal: Research topics
    tools:
      - tavily_search

steps:
  # Sequential step
  - agent: classifier
    action: "Classify: {{topic}}"
    
  # Route pattern - decision-based branching
  - name: routing
    route:
      technical: [tech_expert]
      default: [researcher]
      
  # Parallel pattern - concurrent execution
  - name: parallel_research
    parallel:
      - agent: researcher
        action: "Research market trends"
      - agent: researcher
        action: "Research competitors"
        
  # Loop pattern - iterate over items
  - agent: researcher
    action: "Analyze {{item}}"
    loop:
      over: topics
      
  # Repeat pattern - evaluator-optimizer
  - agent: aggregator
    action: "Synthesize findings"
    repeat:
      until: "comprehensive"
      max_iterations: 3
```

Run with the same simple command:
```bash
praisonai agents.yaml
```

### 19. MCP (Model Context Protocol)

PraisonAI supports MCP Protocol Revision 2025-11-25 with multiple transports.

#### MCP Client (Consume MCP Servers)
```python
from praisonaiagents import Agent, MCP

# stdio - Local NPX/Python servers
agent = Agent(tools=MCP("npx @modelcontextprotocol/server-memory"))

# Streamable HTTP - Production servers
agent = Agent(tools=MCP("https://api.example.com/mcp"))

# WebSocket - Real-time bidirectional
agent = Agent(tools=MCP("wss://api.example.com/mcp", auth_token="token"))

# SSE (Legacy) - Backward compatibility
agent = Agent(tools=MCP("http://localhost:8080/sse"))

# With environment variables
agent = Agent(
    tools=MCP(
        command="npx",
        args=["-y", "@modelcontextprotocol/server-brave-search"],
        env={"BRAVE_API_KEY": "your-key"}
    )
)

# Multiple MCP servers + regular functions
def my_custom_tool(query: str) -> str:
    """Custom tool function."""
    return f"Result: {query}"

agent = Agent(
    name="MultiToolAgent",
    instructions="Agent with multiple MCP servers",
    tools=[
        MCP("uvx mcp-server-time"),                    # Time tools
        MCP("npx @modelcontextprotocol/server-memory"), # Memory tools
        my_custom_tool                                  # Regular function
    ]
)
```

#### MCP Server (Expose Tools as MCP Server)

Expose your Python functions as MCP tools for Claude Desktop, Cursor, and other MCP clients:

```python
from praisonaiagents.mcp import ToolsMCPServer

def search_web(query: str, max_results: int = 5) -> dict:
    """Search the web for information."""
    return {"results": [f"Result for {query}"]}

def calculate(expression: str) -> dict:
    """Evaluate a mathematical expression."""
    return {"result": eval(expression)}

# Create and run MCP server
server = ToolsMCPServer(name="my-tools")
server.register_tools([search_web, calculate])
server.run()  # stdio for Claude Desktop
# server.run_sse(host="0.0.0.0", port=8080)  # SSE for web clients
```

#### MCP Features
| Feature | Description |
|---------|-------------|
| Session Management | Automatic Mcp-Session-Id handling |
| Protocol Versioning | Mcp-Protocol-Version header |
| Resumability | SSE stream recovery via Last-Event-ID |
| Security | Origin validation, DNS rebinding prevention |
| WebSocket | Auto-reconnect with exponential backoff |

### 20. A2A (Agent2Agent Protocol)

PraisonAI supports the [A2A Protocol](https://a2a-protocol.org) for agent-to-agent communication, enabling your agents to be discovered and collaborate with other AI agents.

#### A2A Server (Expose Agent as A2A Server)
```python
from praisonaiagents import Agent, A2A
from fastapi import FastAPI

# Create an agent with tools
def search_web(query: str) -> str:
    """Search the web for information."""
    return f"Results for: {query}"

agent = Agent(
    name="Research Assistant",
    role="Research Analyst",
    goal="Help users research topics",
    tools=[search_web]
)

# Expose as A2A Server
a2a = A2A(agent=agent, url="http://localhost:8000/a2a")

app = FastAPI()
app.include_router(a2a.get_router())

# Run: uvicorn app:app --reload
# Agent Card: GET /.well-known/agent.json
# Status: GET /status
```

#### A2A Features
| Feature | Description |
|---------|-------------|
| Agent Card | JSON metadata for agent discovery |
| Skills Extraction | Auto-generate skills from tools |
| Task Management | Stateful task lifecycle |
| Streaming | SSE streaming for real-time updates |

> **Documentation**: [docs.praison.ai/a2a](https://docs.praison.ai/a2a) | **Examples**: [examples/python/a2a](https://github.com/MervinPraison/PraisonAI/tree/main/examples/python/a2a)

---

## ðŸŽ¯ CLI / No-Code Interface

PraisonAI provides a powerful CLI for no-code automation and quick prototyping.

### CLI Quick Reference

| Category | Commands |
|----------|----------|
| **Execution** | `praisonai`, `--auto`, `--interactive`, `--chat` |
| **Research** | `research`, `--query-rewrite`, `--deep-research` |
| **Planning** | `--planning`, `--planning-tools`, `--planning-reasoning` |
| **Workflows** | `workflow run`, `workflow list`, `workflow auto` |
| **Memory** | `memory show`, `memory add`, `memory search`, `memory clear` |
| **Knowledge** | `knowledge add`, `knowledge query`, `knowledge list` |
| **Sessions** | `session list`, `session resume`, `session delete` |
| **Tools** | `tools list`, `tools info`, `tools search` |
| **MCP** | `mcp list`, `mcp create`, `mcp enable` |
| **Development** | `commit`, `docs`, `checkpoint`, `hooks` |
| **Scheduling** | `schedule start`, `schedule list`, `schedule stop` |

### Auto Mode
```bash
pip install praisonai
export OPENAI_API_KEY=xxxxxxxxxxxxxxxxxxxxxx
praisonai --auto create a movie script about Robots in Mars
```

### Interactive Mode CLI:
```bash
# Start interactive terminal mode (inspired by Gemini CLI, Codex CLI, Claude Code)
praisonai --interactive
praisonai -i

# Features:
# - Streaming responses (no boxes)
# - Built-in tools: read_file, write_file, list_files, execute_command, internet_search
# - Slash commands: /help, /exit, /tools, /clear

# Chat mode - single prompt with interactive style (for testing/scripting)
# Use --chat (or --chat-mode for backward compatibility)
praisonai "list files in current folder" --chat
praisonai "search the web for AI news" --chat
praisonai "read README.md" --chat
```

### Chat UI (Web Interface):
```bash
# Start web-based Chainlit chat interface (requires praisonai[chat])
pip install "praisonai[chat]"
praisonai chat
# Opens browser at http://localhost:8084
```

### Query Rewriting (works with any command):
```bash
# Rewrite query for better results (uses QueryRewriterAgent)
praisonai "AI trends" --query-rewrite

# Rewrite with search tools (agent decides when to search)
praisonai "latest developments" --query-rewrite --rewrite-tools "internet_search"

# Works with any prompt
praisonai "explain quantum computing" --query-rewrite -v
```

### Deep Research CLI:
```bash
# Default: OpenAI (o4-mini-deep-research)
praisonai research "What are the latest AI trends in 2025?"

# Use Gemini
praisonai research --model deep-research-pro "Your research query"

# Rewrite query before research
praisonai research --query-rewrite "AI trends"

# Rewrite with search tools
praisonai research --query-rewrite --rewrite-tools "internet_search" "AI trends"

# Use custom tools from file (gathers context before deep research)
praisonai research --tools tools.py "Your research query"
praisonai research -t my_tools.py "Your research query"

# Use built-in tools by name (comma-separated)
praisonai research --tools "internet_search,wiki_search" "Your query"
praisonai research -t "yfinance,calculator_tools" "Stock analysis query"

# Save output to file (output/research/{query}.md)
praisonai research --save "Your research query"
praisonai research -s "Your research query"

# Combine options
praisonai research --query-rewrite --tools tools.py --save "Your research query"

# Verbose mode (show debug logs)
praisonai research -v "Your research query"
```

### Planning Mode CLI:
```bash
# Enable planning mode - agent creates a plan before execution
praisonai "Research AI trends and write a summary" --planning

# Planning with tools for research
praisonai "Analyze market trends" --planning --planning-tools tools.py

# Planning with chain-of-thought reasoning
praisonai "Complex analysis task" --planning --planning-reasoning

# Auto-approve plans without confirmation
praisonai "Task" --planning --auto-approve-plan
```

### Tool Approval CLI:
```bash
# Auto-approve ALL tool executions (use with caution!)
praisonai "run ls command" --trust

# Auto-approve tools up to a risk level (prompt for higher)
# Levels: low, medium, high, critical
praisonai "write to file" --approve-level high  # Prompts for critical tools only
praisonai "task" --approve-level medium         # Prompts for high and critical

# Default behavior (no flags): prompts for all dangerous tools
praisonai "run shell command"  # Will prompt for approval
```

### Memory CLI:
```bash
# Enable memory for agent (persists across sessions)
praisonai "My name is John" --memory

# Memory with user isolation
praisonai "Remember my preferences" --memory --user-id user123

# Memory management commands
praisonai memory show                      # Show memory statistics
praisonai memory add "User prefers Python" # Add to long-term memory
praisonai memory search "Python"           # Search memories
praisonai memory clear                     # Clear short-term memory
praisonai memory clear all                 # Clear all memory
praisonai memory save my_session           # Save session
praisonai memory resume my_session         # Resume session
praisonai memory sessions                  # List saved sessions
praisonai memory checkpoint                # Create checkpoint
praisonai memory restore <checkpoint_id>   # Restore checkpoint
praisonai memory checkpoints               # List checkpoints
praisonai memory help                      # Show all commands
```

### Rules CLI:
```bash
# List all loaded rules (from PRAISON.md, CLAUDE.md, etc.)
praisonai rules list

# Show specific rule details
praisonai rules show <rule_name>

# Create a new rule
praisonai rules create my_rule "Always use type hints"

# Delete a rule
praisonai rules delete my_rule

# Show rules statistics
praisonai rules stats

# Include manual rules with prompts
praisonai "Task" --include-rules security,testing
```

### Workflow CLI:
```bash
# List available workflows
praisonai workflow list

# Execute a workflow with tools and save output
praisonai workflow run "Research Blog" --tools tavily --save

# Execute with variables
praisonai workflow run deploy --workflow-var environment=staging --workflow-var branch=main

# Execute with planning mode (AI creates sub-steps for each workflow step)
praisonai workflow run "Research Blog" --planning --verbose

# Execute with reasoning mode (chain-of-thought)
praisonai workflow run "Analysis" --reasoning --verbose

# Execute with memory enabled
praisonai workflow run "Research" --memory

# Show workflow details
praisonai workflow show deploy

# Create a new workflow template
praisonai workflow create my_workflow

# Inline workflow (no template file needed)
praisonai "What is AI?" --workflow "Research,Summarize" --save

# Inline workflow with step actions
praisonai "GPT-5" --workflow "Research:Search for info,Write:Write blog" --tools tavily

# Workflow CLI help
praisonai workflow help
```

#### YAML Workflow Files:
```bash
# Run a YAML workflow file
praisonai workflow run research.yaml

# Run with variables
praisonai workflow run research.yaml --var topic="AI trends"

# Validate a YAML workflow
praisonai workflow validate research.yaml

# Create from template (simple, routing, parallel, loop, evaluator-optimizer)
praisonai workflow template routing --output my_workflow.yaml
```

#### Auto-Generate Workflows:
```bash
# Auto-generate a sequential workflow from topic
praisonai workflow auto "Research AI trends"

# Generate parallel workflow (multiple agents work concurrently)
praisonai workflow auto "Research AI trends" --pattern parallel

# Generate routing workflow (classifier routes to specialists)
praisonai workflow auto "Build a chatbot" --pattern routing

# Generate orchestrator-workers workflow (central orchestrator delegates)
praisonai workflow auto "Comprehensive market analysis" --pattern orchestrator-workers

# Generate evaluator-optimizer workflow (iterative refinement)
praisonai workflow auto "Write and refine article" --pattern evaluator-optimizer

# Specify output file
praisonai workflow auto "Build a chatbot" --pattern routing

# Specify output file
praisonai workflow auto "Research AI" --pattern sequential --output my_workflow.yaml
```

**Workflow CLI Options:**
| Flag | Description |
|------|-------------|
| `--workflow-var key=value` | Set workflow variable (can be repeated) |
| `--var key=value` | Set variable for YAML workflows |
| `--pattern <pattern>` | Pattern for auto-generation (sequential, parallel, routing, loop, orchestrator-workers, evaluator-optimizer) |
| `--output <file>` | Output file for auto-generation |
| `--llm <model>` | LLM model (e.g., openai/gpt-4o-mini) |
| `--tools <tools>` | Tools (comma-separated, e.g., tavily) |
| `--planning` | Enable planning mode |
| `--reasoning` | Enable reasoning mode |
| `--memory` | Enable memory |
| `--verbose` | Enable verbose output |
| `--save` | Save output to file |

### Hooks CLI:
```bash
# List configured hooks
praisonai hooks list

# Show hooks statistics
praisonai hooks stats

# Create hooks.json template
praisonai hooks init
```

### Claude Memory Tool CLI:
```bash
# Enable Claude Memory Tool (Anthropic models only)
praisonai "Research and remember findings" --claude-memory --llm anthropic/claude-sonnet-4-20250514
```

### Guardrail CLI:
```bash
# Validate output with LLM guardrail
praisonai "Write code" --guardrail "Ensure code is secure and follows best practices"

# Combine with other flags
praisonai "Generate SQL query" --guardrail "No DROP or DELETE statements" --save
```

### Metrics CLI:
```bash
# Display token usage and cost metrics
praisonai "Analyze this data" --metrics

# Combine with other features
praisonai "Complex task" --metrics --planning
```

### Scheduler CLI:

```bash
praisonai schedule start <name> "task" --interval hourly
praisonai schedule list
praisonai schedule logs <name> [--follow]
praisonai schedule stop <name>
praisonai schedule restart <name>
praisonai schedule delete <name>
praisonai schedule describe <name>
praisonai schedule save <name> [file.yaml]
praisonai schedule "task" --interval hourly  # foreground mode
praisonai schedule agents.yaml  # foreground mode
```

### Image Processing CLI:
```bash
# Process images with vision-based tasks
praisonai "Describe this image" --image path/to/image.png

# Analyze image content
praisonai "What objects are in this photo?" --image photo.jpg --llm openai/gpt-4o
```

### Telemetry CLI:
```bash
# Enable usage monitoring and analytics
praisonai "Task" --telemetry

# Combine with metrics for full observability
praisonai "Complex analysis" --telemetry --metrics
```

### MCP (Model Context Protocol) CLI:
```bash
# Use MCP server tools
praisonai "Search files" --mcp "npx -y @modelcontextprotocol/server-filesystem ."

# MCP with environment variables
praisonai "Search web" --mcp "npx -y @modelcontextprotocol/server-brave-search" --mcp-env "BRAVE_API_KEY=your_key"

# Multiple MCP options
praisonai "Task" --mcp "npx server" --mcp-env "KEY1=value1,KEY2=value2"
```

### Fast Context CLI:
```bash
# Search codebase for relevant context
praisonai "Find authentication code" --fast-context ./src

# Add code context to any task
praisonai "Explain this function" --fast-context /path/to/project
```

### Knowledge CLI:
```bash
# Add documents to knowledge base
praisonai knowledge add document.pdf
praisonai knowledge add ./docs/

# Search knowledge base
praisonai knowledge search "API authentication"

# List indexed documents
praisonai knowledge list

# Clear knowledge base
praisonai knowledge clear

# Show knowledge base info
praisonai knowledge info

# Show all commands
praisonai knowledge help
```

### Session CLI:
```bash
# List all saved sessions
praisonai session list

# Show session details
praisonai session show my-project

# Resume a session (load into memory)
praisonai session resume my-project

# Delete a session
praisonai session delete my-project

# Auto-save session after each run
praisonai "Analyze this code" --auto-save my-project

# Load history from last N sessions into context
praisonai "Continue our discussion" --history 5
```

### Session Management (Python):
```python
from praisonaiagents import Agent

# Auto-save session after each run
agent = Agent(
    name="Assistant",
    memory=True,
    auto_save="my-project"
)

# Load history from past sessions via context management
agent = Agent(
    name="Assistant",
    memory=True,
    context=True,  # Enable context management for history
)
```

### Workflow Checkpoints:
```python
from praisonaiagents.memory.workflows import WorkflowManager

manager = WorkflowManager()

# Save checkpoint after each step
result = manager.execute("deploy", checkpoint="deploy-v1")

# Resume from checkpoint
result = manager.execute("deploy", resume="deploy-v1")

# List/delete checkpoints
manager.list_checkpoints()
manager.delete_checkpoint("deploy-v1")
```

### Tools CLI:
```bash
praisonai tools list
praisonai tools info internet_search
praisonai tools search "web"
praisonai tools doctor
praisonai tools resolve shell_tool
praisonai tools discover
praisonai tools show-sources
praisonai tools show-sources --template ai-video-editor
```

| Command | Example | Docs |
|---------|---------|------|
| `tools list` | [example](examples/tools/) | [docs](https://docs.praison.ai/docs/cli/tools) |
| `tools resolve` | [example](examples/tools/example_tools_resolve.py) | [docs](https://docs.praison.ai/docs/cli/tools-resolve) |
| `tools discover` | [example](examples/tools/example_tools_discover.py) | [docs](https://docs.praison.ai/docs/cli/tools-discover) |
| `tools show-sources` | [example](examples/tools/example_tools_sources.py) | [docs](https://docs.praison.ai/docs/cli/tools-show-sources) |

### Handoff CLI:
```bash
# Enable agent-to-agent task delegation
praisonai "Research and write article" --handoff "researcher,writer,editor"

# Complex multi-agent workflow
praisonai "Analyze data and create report" --handoff "analyst,visualizer,writer"
```

### Auto Memory CLI:
```bash
# Enable automatic memory extraction
praisonai "Learn about user preferences" --auto-memory

# Combine with user isolation
praisonai "Remember my settings" --auto-memory --user-id user123
```

### Todo CLI:
```bash
# Generate todo list from task
praisonai "Plan the project" --todo

# Add a todo item
praisonai todo add "Implement feature X"

# List all todos
praisonai todo list

# Complete a todo
praisonai todo complete 1

# Delete a todo
praisonai todo delete 1

# Clear all todos
praisonai todo clear

# Show all commands
praisonai todo help
```

### Router CLI:
```bash
# Auto-select best model based on task complexity
praisonai "Simple question" --router

# Specify preferred provider
praisonai "Complex analysis" --router --router-provider anthropic

# Router automatically selects:
# - Simple tasks â†’ gpt-4o-mini, claude-3-haiku
# - Complex tasks â†’ gpt-4-turbo, claude-3-opus

# Create workflow with model routing template
praisonai workflow create --template model-routing --output my_workflow.yaml
```

Custom models can be configured in `agents.yaml`. See [Model Router Docs](https://docs.praison.ai/features/model-router) for details.

### Flow Display CLI:
```bash
# Enable visual workflow tracking
praisonai agents.yaml --flow-display

# Combine with other features
praisonai "Multi-step task" --planning --flow-display
```

### Docs CLI:
```bash
# List all project docs
praisonai docs list

# Create a new doc
praisonai docs create project-overview "This project is a Python web app..."

# Show a specific doc
praisonai docs show project-overview

# Delete a doc
praisonai docs delete old-doc

# Show all commands
praisonai docs help
```

### MCP Config CLI:
```bash
# List all MCP configurations
praisonai mcp list

# Create a new MCP config
praisonai mcp create filesystem npx -y @modelcontextprotocol/server-filesystem .

# Show a specific config
praisonai mcp show filesystem

# Enable/disable a config
praisonai mcp enable filesystem
praisonai mcp disable filesystem

# Delete a config
praisonai mcp delete filesystem

# Show all commands
praisonai mcp help
```

### AI Commit CLI:
```bash
# Full auto mode: stage all, security check, commit, and push
praisonai commit -a

# Interactive mode (requires git add first)
praisonai commit

# Interactive with auto-push
praisonai commit --push

# Skip security check (not recommended)
praisonai commit -a --no-verify
```

**Features:**
- ðŸ¤– AI-generated conventional commit messages
- ðŸ”’ Built-in security scanning (API keys, passwords, secrets, sensitive files)
- ðŸ“¦ Auto-staging with `-a` flag
- ðŸš€ Auto-push in full auto mode
- âœï¸ Edit message before commit in interactive mode

**Security Detection:**
- API keys, secrets, tokens (AWS, GitHub, GitLab, Slack)
- Passwords and private keys
- Sensitive files (`.env`, `id_rsa`, `.pem`, `.key`, etc.)

### Serve CLI (API Server):
```bash
# Start API server for agents defined in YAML
praisonai serve agents.yaml

# With custom port and host
praisonai serve agents.yaml --port 8005 --host 0.0.0.0

# Alternative flag style
praisonai agents.yaml --serve

# The server provides:
# POST /agents          - Run all agents sequentially
# POST /agents/{name}   - Run specific agent (e.g., /agents/researcher)
# GET  /agents/list     - List available agents
```

### n8n Integration CLI:
```bash
# Export workflow to n8n and open in browser
praisonai agents.yaml --n8n

# With custom n8n URL
praisonai agents.yaml --n8n --n8n-url http://localhost:5678

# Set N8N_API_KEY for auto-import
export N8N_API_KEY="your-api-key"
praisonai agents.yaml --n8n
```

### External Agents CLI:

Use external AI coding CLI tools (Claude Code, Gemini CLI, Codex CLI, Cursor CLI) as agent tools:

```bash
# Use Claude Code for coding tasks
praisonai "Refactor the auth module" --external-agent claude

# Use Gemini CLI for code analysis
praisonai "Analyze codebase architecture" --external-agent gemini

# Use OpenAI Codex CLI
praisonai "Fix all bugs in src/" --external-agent codex

# Use Cursor CLI
praisonai "Add comprehensive tests" --external-agent cursor
```

**Python API:**
```python
from praisonai.integrations import (
    ClaudeCodeIntegration,
    GeminiCLIIntegration,
    CodexCLIIntegration,
    CursorCLIIntegration
)

# Create integration
claude = ClaudeCodeIntegration(workspace="/project")

# Execute a coding task
result = await claude.execute("Refactor the auth module")

# Use as agent tool
from praisonai import Agent
tool = claude.as_tool()
agent = Agent(tools=[tool])
```

**Environment Variables:**
```bash
export ANTHROPIC_API_KEY=your-key  # Claude Code
export GEMINI_API_KEY=your-key     # Gemini CLI
export OPENAI_API_KEY=your-key     # Codex CLI
export CURSOR_API_KEY=your-key     # Cursor CLI
```

See [External Agents Documentation](https://docs.praison.ai/code/external-agents) for more details.

### @Mentions in Prompts:
```bash
# Include file content in prompt
praisonai "@file:src/main.py explain this code"

# Include project doc
praisonai "@doc:project-overview help me add a feature"

# Search the web
praisonai "@web:python best practices give me tips"

# Fetch URL content
praisonai "@url:https://docs.python.org summarize this"

# Combine multiple mentions
praisonai "@file:main.py @doc:coding-standards review this code"
```

## Prompt Expansion

Expand short prompts into detailed, actionable prompts:

### CLI Usage
```bash
# Expand a short prompt into detailed prompt
praisonai "write a movie script in 3 lines" --expand-prompt

# With verbose output
praisonai "blog about AI" --expand-prompt -v

# With tools for context gathering
praisonai "latest AI trends" --expand-prompt --expand-tools tools.py

# Combine with query rewrite
praisonai "AI news" --query-rewrite --expand-prompt
```

### Programmatic Usage
```python
from praisonaiagents import PromptExpanderAgent, ExpandStrategy

# Basic usage
agent = PromptExpanderAgent()
result = agent.expand("write a movie script in 3 lines")
print(result.expanded_prompt)

# With specific strategy
result = agent.expand("blog about AI", strategy=ExpandStrategy.DETAILED)

# Available strategies: BASIC, DETAILED, STRUCTURED, CREATIVE, AUTO
```

**Key Difference:**
- `--query-rewrite`: Optimizes queries for search/retrieval (RAG)
- `--expand-prompt`: Expands prompts for detailed task execution

## Web Search, Web Fetch & Prompt Caching

### CLI Usage
```bash
# Web Search - Get real-time information
praisonai "What are the latest AI news today?" --web-search --llm openai/gpt-4o-search-preview

# Web Fetch - Retrieve and analyze URL content (Anthropic only)
praisonai "Summarize https://docs.praison.ai" --web-fetch --llm anthropic/claude-sonnet-4-20250514

# Prompt Caching - Reduce costs for repeated prompts
praisonai "Analyze this document..." --prompt-caching --llm anthropic/claude-sonnet-4-20250514
```

### Programmatic Usage
```python
from praisonaiagents import Agent

# Web Search
agent = Agent(
    instructions="You are a research assistant",
    llm="openai/gpt-4o-search-preview",
    web_search=True
)

# Web Fetch (Anthropic only)
agent = Agent(
    instructions="You are a content analyzer",
    llm="anthropic/claude-sonnet-4-20250514",
    web_fetch=True
)

# Prompt Caching
agent = Agent(
    instructions="You are an AI assistant..." * 50,  # Long system prompt
    llm="anthropic/claude-sonnet-4-20250514",
    prompt_caching=True
)
```

**Supported Providers:**
| Feature | Providers |
|---------|----------|
| Web Search | OpenAI, Gemini, Anthropic, xAI, Perplexity |
| Web Fetch | Anthropic |
| Prompt Caching | OpenAI (auto), Anthropic, Bedrock, Deepseek |

## CLI Features

| Feature | Docs |
|---------|:----:|
| ðŸ”„ Query Rewrite - RAG optimization | [ðŸ“–](https://docs.praison.ai/docs/cli/query-rewrite) |
| ðŸ”¬ Deep Research - Automated research | [ðŸ“–](https://docs.praison.ai/docs/cli/deep-research) |
| ðŸ“‹ Planning - Step-by-step execution | [ðŸ“–](https://docs.praison.ai/docs/cli/planning) |
| ðŸ’¾ Memory - Persistent agent memory | [ðŸ“–](https://docs.praison.ai/docs/cli/memory) |
| ðŸ“œ Rules - Auto-discovered instructions | [ðŸ“–](https://docs.praison.ai/docs/cli/rules) |
| ðŸ”„ Workflow - Multi-step workflows | [ðŸ“–](https://docs.praison.ai/docs/cli/workflow) |
| ðŸª Hooks - Event-driven actions | [ðŸ“–](https://docs.praison.ai/docs/cli/hooks) |
| ðŸ§  Claude Memory - Anthropic memory tool | [ðŸ“–](https://docs.praison.ai/docs/cli/claude-memory) |
| ðŸ›¡ï¸ Guardrail - Output validation | [ðŸ“–](https://docs.praison.ai/docs/cli/guardrail) |
| ðŸ“Š Metrics - Token usage tracking | [ðŸ“–](https://docs.praison.ai/docs/cli/metrics) |
| ðŸ–¼ï¸ Image - Vision processing | [ðŸ“–](https://docs.praison.ai/docs/cli/image) |
| ðŸ“¡ Telemetry - Usage monitoring | [ðŸ“–](https://docs.praison.ai/docs/cli/telemetry) |
| ðŸ”Œ MCP - Model Context Protocol | [ðŸ“–](https://docs.praison.ai/docs/cli/mcp) |
| âš¡ Fast Context - Codebase search | [ðŸ“–](https://docs.praison.ai/docs/cli/fast-context) |
| ðŸ“š Knowledge - RAG management | [ðŸ“–](https://docs.praison.ai/docs/cli/knowledge) |
| ðŸ’¬ Session - Conversation management | [ðŸ“–](https://docs.praison.ai/docs/cli/session) |
| ðŸ”§ Tools - Tool discovery | [ðŸ“–](https://docs.praison.ai/docs/cli/tools) |
| ðŸ¤ Handoff - Agent delegation | [ðŸ“–](https://docs.praison.ai/docs/cli/handoff) |
| ðŸ§  Auto Memory - Memory extraction | [ðŸ“–](https://docs.praison.ai/docs/cli/auto-memory) |
| ðŸ“‹ Todo - Task management | [ðŸ“–](https://docs.praison.ai/docs/cli/todo) |
| ðŸŽ¯ Router - Smart model selection | [ðŸ“–](https://docs.praison.ai/docs/cli/router) |
| ðŸ“ˆ Flow Display - Visual workflow | [ðŸ“–](https://docs.praison.ai/docs/cli/flow-display) |
| âœ¨ Prompt Expansion - Detailed prompts | [ðŸ“–](https://docs.praison.ai/docs/cli/prompt-expansion) |
| ðŸŒ Web Search - Real-time search | [ðŸ“–](https://docs.praison.ai/docs/cli/web-search) |
| ðŸ“¥ Web Fetch - URL content retrieval | [ðŸ“–](https://docs.praison.ai/docs/cli/web-fetch) |
| ðŸ’¾ Prompt Caching - Cost reduction | [ðŸ“–](https://docs.praison.ai/docs/cli/prompt-caching) |
| ðŸ“¦ Template Catalog - Browse & discover templates | [ðŸ“–](https://docs.praison.ai/docs/cli/template-catalog) |

### Template Catalog CLI

| Command | Description |
|---------|-------------|
| `praisonai templates browse` | Open template catalog in browser |
| `praisonai templates browse --print` | Print catalog URL only |
| `praisonai templates validate` | Validate template YAML files |
| `praisonai templates validate --source <dir>` | Validate specific directory |
| `praisonai templates validate --strict` | Strict validation mode |
| `praisonai templates validate --json` | JSON output format |
| `praisonai templates catalog build` | Build catalog locally |
| `praisonai templates catalog build --out <dir>` | Build to specific directory |
| `praisonai templates catalog sync` | Sync template sources |
| `praisonai templates catalog sync --source <name>` | Sync specific source |

**Examples:** [examples/catalog/](examples/catalog/) | **Docs:** [Code](https://docs.praison.ai/docs/cli/template-catalog-code) | [CLI](https://docs.praison.ai/docs/cli/template-catalog)

---

## ðŸ’» Using JavaScript Code

```bash
npm install praisonai
export OPENAI_API_KEY=xxxxxxxxxxxxxxxxxxxxxx
```

```javascript
const { Agent } = require('praisonai');
const agent = new Agent({ instructions: 'You are a helpful AI assistant' });
agent.start('Write a movie script about a robot in Mars');
```

![PraisonAI CLI Demo](docs/demo/praisonai-cli-demo.gif)

---

## â­ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=MervinPraison/PraisonAI&type=Date)](https://docs.praison.ai)

---

## ðŸ“Š Process Types & Patterns

<details>
<summary><strong>View architecture diagrams and workflow patterns</strong></summary>

### AI Agents Flow

```mermaid
graph LR
    %% Define the main flow
    Start([â–¶ Start]) --> Agent1
    Agent1 --> Process[âš™ Process]
    Process --> Agent2
    Agent2 --> Output([âœ“ Output])
    Process -.-> Agent1
    
    %% Define subgraphs for agents and their tasks
    subgraph Agent1[ ]
        Task1[ðŸ“‹ Task]
        AgentIcon1[ðŸ¤– AI Agent]
        Tools1[ðŸ”§ Tools]
        
        Task1 --- AgentIcon1
        AgentIcon1 --- Tools1
    end
    
    subgraph Agent2[ ]
        Task2[ðŸ“‹ Task]
        AgentIcon2[ðŸ¤– AI Agent]
        Tools2[ðŸ”§ Tools]
        
        Task2 --- AgentIcon2
        AgentIcon2 --- Tools2
    end

    classDef input fill:#8B0000,stroke:#7C90A0,color:#fff
    classDef process fill:#189AB4,stroke:#7C90A0,color:#fff
    classDef tools fill:#2E8B57,stroke:#7C90A0,color:#fff
    classDef transparent fill:none,stroke:none

    class Start,Output,Task1,Task2 input
    class Process,AgentIcon1,AgentIcon2 process
    class Tools1,Tools2 tools
    class Agent1,Agent2 transparent
```

## AI Agents with Tools

Create AI agents that can use tools to interact with external systems and perform actions.

```mermaid
flowchart TB
    subgraph Tools
        direction TB
        T3[Internet Search]
        T1[Code Execution]
        T2[Formatting]
    end

    Input[Input] ---> Agents
    subgraph Agents
        direction LR
        A1[Agent 1]
        A2[Agent 2]
        A3[Agent 3]
    end
    Agents ---> Output[Output]

    T3 --> A1
    T1 --> A2
    T2 --> A3

    style Tools fill:#189AB4,color:#fff
    style Agents fill:#8B0000,color:#fff
    style Input fill:#8B0000,color:#fff
    style Output fill:#8B0000,color:#fff
```

## AI Agents with Memory

Create AI agents with memory capabilities for maintaining context and information across tasks.

```mermaid
flowchart TB
    subgraph Memory
        direction TB
        STM[Short Term]
        LTM[Long Term]
    end

    subgraph Store
        direction TB
        DB[(Vector DB)]
    end

    Input[Input] ---> Agents
    subgraph Agents
        direction LR
        A1[Agent 1]
        A2[Agent 2]
        A3[Agent 3]
    end
    Agents ---> Output[Output]

    Memory <--> Store
    Store <--> A1
    Store <--> A2
    Store <--> A3

    style Memory fill:#189AB4,color:#fff
    style Store fill:#2E8B57,color:#fff
    style Agents fill:#8B0000,color:#fff
    style Input fill:#8B0000,color:#fff
    style Output fill:#8B0000,color:#fff
```

## AI Agents with Different Processes

### Sequential Process

The simplest form of task execution where tasks are performed one after another.

```mermaid
graph LR
    Input[Input] --> A1
    subgraph Agents
        direction LR
        A1[Agent 1] --> A2[Agent 2] --> A3[Agent 3]
    end
    A3 --> Output[Output]

    classDef input fill:#8B0000,stroke:#7C90A0,color:#fff
    classDef process fill:#189AB4,stroke:#7C90A0,color:#fff
    classDef transparent fill:none,stroke:none

    class Input,Output input
    class A1,A2,A3 process
    class Agents transparent
```

### Hierarchical Process

Uses a manager agent to coordinate task execution and agent assignments.

```mermaid
graph TB
    Input[Input] --> Manager
    
    subgraph Agents
        Manager[Manager Agent]
        
        subgraph Workers
            direction LR
            W1[Worker 1]
            W2[Worker 2]
            W3[Worker 3]
        end
        
        Manager --> W1
        Manager --> W2
        Manager --> W3
    end
    
    W1 --> Manager
    W2 --> Manager
    W3 --> Manager
    Manager --> Output[Output]

    classDef input fill:#8B0000,stroke:#7C90A0,color:#fff
    classDef process fill:#189AB4,stroke:#7C90A0,color:#fff
    classDef transparent fill:none,stroke:none

    class Input,Output input
    class Manager,W1,W2,W3 process
    class Agents,Workers transparent
```

### Workflow Process

Advanced process type supporting complex task relationships and conditional execution.

```mermaid
graph LR
    Input[Input] --> Start
    
    subgraph Workflow
        direction LR
        Start[Start] --> C1{Condition}
        C1 --> |Yes| A1[Agent 1]
        C1 --> |No| A2[Agent 2]
        A1 --> Join
        A2 --> Join
        Join --> A3[Agent 3]
    end
    
    A3 --> Output[Output]

    classDef input fill:#8B0000,stroke:#7C90A0,color:#fff
    classDef process fill:#189AB4,stroke:#7C90A0,color:#fff
    classDef decision fill:#2E8B57,stroke:#7C90A0,color:#fff
    classDef transparent fill:none,stroke:none

    class Input,Output input
    class Start,A1,A2,A3,Join process
    class C1 decision
    class Workflow transparent
```

#### Agentic Routing Workflow

Create AI agents that can dynamically route tasks to specialized LLM instances.

```mermaid
flowchart LR
    In[In] --> Router[LLM Call Router]
    Router --> LLM1[LLM Call 1]
    Router --> LLM2[LLM Call 2]
    Router --> LLM3[LLM Call 3]
    LLM1 --> Out[Out]
    LLM2 --> Out
    LLM3 --> Out
    
    style In fill:#8B0000,color:#fff
    style Router fill:#2E8B57,color:#fff
    style LLM1 fill:#2E8B57,color:#fff
    style LLM2 fill:#2E8B57,color:#fff
    style LLM3 fill:#2E8B57,color:#fff
    style Out fill:#8B0000,color:#fff
```

#### Agentic Orchestrator Worker

Create AI agents that orchestrate and distribute tasks among specialized workers.

```mermaid
flowchart LR
    In[In] --> Router[LLM Call Router]
    Router --> LLM1[LLM Call 1]
    Router --> LLM2[LLM Call 2]
    Router --> LLM3[LLM Call 3]
    LLM1 --> Synthesizer[Synthesizer]
    LLM2 --> Synthesizer
    LLM3 --> Synthesizer
    Synthesizer --> Out[Out]
    
    style In fill:#8B0000,color:#fff
    style Router fill:#2E8B57,color:#fff
    style LLM1 fill:#2E8B57,color:#fff
    style LLM2 fill:#2E8B57,color:#fff
    style LLM3 fill:#2E8B57,color:#fff
    style Synthesizer fill:#2E8B57,color:#fff
    style Out fill:#8B0000,color:#fff
```

#### Agentic Autonomous Workflow

Create AI agents that can autonomously monitor, act, and adapt based on environment feedback.

```mermaid
flowchart LR
    Human[Human] <--> LLM[LLM Call]
    LLM -->|ACTION| Environment[Environment]
    Environment -->|FEEDBACK| LLM
    LLM --> Stop[Stop]
    
    style Human fill:#8B0000,color:#fff
    style LLM fill:#2E8B57,color:#fff
    style Environment fill:#8B0000,color:#fff
    style Stop fill:#333,color:#fff
```

#### Agentic Parallelization

Create AI agents that can execute tasks in parallel for improved performance.

```mermaid
flowchart LR
    In[In] --> LLM2[LLM Call 2]
    In --> LLM1[LLM Call 1]
    In --> LLM3[LLM Call 3]
    LLM1 --> Aggregator[Aggregator]
    LLM2 --> Aggregator
    LLM3 --> Aggregator
    Aggregator --> Out[Out]
    
    style In fill:#8B0000,color:#fff
    style LLM1 fill:#2E8B57,color:#fff
    style LLM2 fill:#2E8B57,color:#fff
    style LLM3 fill:#2E8B57,color:#fff
    style Aggregator fill:#fff,color:#000
    style Out fill:#8B0000,color:#fff
```

#### Agentic Prompt Chaining

Create AI agents with sequential prompt chaining for complex workflows.

```mermaid
flowchart LR
    In[In] --> LLM1[LLM Call 1] --> Gate{Gate}
    Gate -->|Pass| LLM2[LLM Call 2] -->|Output 2| LLM3[LLM Call 3] --> Out[Out]
    Gate -->|Fail| Exit[Exit]
    
    style In fill:#8B0000,color:#fff
    style LLM1 fill:#2E8B57,color:#fff
    style LLM2 fill:#2E8B57,color:#fff
    style LLM3 fill:#2E8B57,color:#fff
    style Out fill:#8B0000,color:#fff
    style Exit fill:#8B0000,color:#fff
```

#### Agentic Evaluator Optimizer

Create AI agents that can generate and optimize solutions through iterative feedback.

```mermaid
flowchart LR
    In[In] --> Generator[LLM Call Generator] 
    Generator -->|SOLUTION| Evaluator[LLM Call Evaluator] -->|ACCEPTED| Out[Out]
    Evaluator -->|REJECTED + FEEDBACK| Generator
    
    style In fill:#8B0000,color:#fff
    style Generator fill:#2E8B57,color:#fff
    style Evaluator fill:#2E8B57,color:#fff
    style Out fill:#8B0000,color:#fff
```

#### Repetitive Agents

Create AI agents that can efficiently handle repetitive tasks through automated loops.

```mermaid
flowchart LR
    In[Input] --> LoopAgent[("Looping Agent")]
    LoopAgent --> Task[Task]
    Task --> |Next iteration| LoopAgent
    Task --> |Done| Out[Output]
    
    style In fill:#8B0000,color:#fff
    style LoopAgent fill:#2E8B57,color:#fff,shape:circle
    style Task fill:#2E8B57,color:#fff
    style Out fill:#8B0000,color:#fff
```

</details>

---

## ðŸ”§ Configuration & Integration

### Ollama Integration

```bash
export OPENAI_BASE_URL=http://localhost:11434/v1
```

### Groq Integration

Replace xxxx with Groq API KEY:

```bash
export OPENAI_API_KEY=xxxxxxxxxxx
export OPENAI_BASE_URL=https://api.groq.com/openai/v1
```

### 100+ Models Support

PraisonAI supports 100+ LLM models from various providers. Visit our [models documentation](https://docs.praison.ai/models/) for the complete list.

<div align="center">
  <a href="https://docs.praison.ai/models">
    <p align="center">
      <img src="https://img.shields.io/badge/ðŸ“š_Models_Documentation-Visit_docs.praison.ai-blue?style=for-the-badge&logo=bookstack&logoColor=white" alt="Models Documentation" />
    </p>
  </a>
</div>

---

## ðŸ“‹ Agents Playbook

### Simple Playbook Example

Create `agents.yaml` file and add the code below:

```yaml
framework: praisonai
topic: Artificial Intelligence
agents:  # Canonical: use 'agents' instead of 'roles'
  screenwriter:
    instructions: "Skilled in crafting scripts with engaging dialogue about {topic}."  # Canonical: use 'instructions' instead of 'backstory'
    goal: Create scripts from concepts.
    role: Screenwriter
    tasks:
      scriptwriting_task:
        description: "Develop scripts with compelling characters and dialogue about {topic}."
        expected_output: "Complete script ready for production."
```

*To run the playbook:*
```bash
praisonai agents.yaml
```

---

## ðŸ› ï¸ Custom Tools / Create Plugins

PraisonAI supports multiple ways to create and integrate custom tools (plugins) into your agents.

### Using `@tool` Decorator

```python
from praisonaiagents import Agent, tool

@tool
def search(query: str) -> str:
    """Search the web for information."""
    return f"Results for: {query}"

@tool
def calculate(expression: str) -> float:
    """Evaluate a math expression."""
    return eval(expression)

agent = Agent(
    instructions="You are a helpful assistant",
    tools=[search, calculate]
)
agent.start("Search for AI news and calculate 15*4")
```

### Using `BaseTool` Class

```python
from praisonaiagents import Agent, BaseTool

class WeatherTool(BaseTool):
    name = "weather"
    description = "Get current weather for a location"
    
    def run(self, location: str) -> str:
        return f"Weather in {location}: 72Â°F, Sunny"

agent = Agent(
    instructions="You are a weather assistant",
    tools=[WeatherTool()]
)
agent.start("What's the weather in Paris?")
```

### Creating a Tool Package (pip installable)

```toml
# pyproject.toml
[project]
name = "my-praisonai-tools"
version = "1.0.0"
dependencies = ["praisonaiagents"]

[project.entry-points."praisonaiagents.tools"]
my_tool = "my_package:MyTool"
```

```python
# my_package/__init__.py
from praisonaiagents import BaseTool

class MyTool(BaseTool):
    name = "my_tool"
    description = "My custom tool"
    
    def run(self, param: str) -> str:
        return f"Result: {param}"
```

After `pip install`, tools are auto-discovered:
```python
agent = Agent(tools=["my_tool"])  # Works automatically!
```

---

## ðŸ§  Memory & Context

PraisonAI provides zero-dependency persistent memory for agents. For detailed examples, see [section 6. Agent Memory](#6-agent-memory-zero-dependencies) in the Python Code Examples.

---

## ðŸ“š Knowledge & Retrieval (RAG)

PraisonAI provides a complete knowledge stack for building RAG applications with multiple vector stores, retrieval strategies, rerankers, and query modes.

### RAG Quickstart (Agent-first)

```python
from praisonaiagents import Agent
from praisonaiagents.rag.models import RetrievalStrategy

# Agent with RAG - simplest approach
agent = Agent(
    name="Research Assistant",
    knowledge=["docs/manual.pdf", "data/faq.txt"],
    knowledge_config={"vector_store": {"provider": "chroma"}},
    rag_config={
        "include_citations": True,
        "retrieval_strategy": RetrievalStrategy.HYBRID,  # Dense + BM25
        "rerank": True,
    }
)

# Query with citations
result = agent.rag_query("How do I authenticate?")
print(result.answer)
for citation in result.citations:
    print(f"  [{citation.id}] {citation.source}")
```

### RAG CLI Commands

| Command | Description |
|---------|-------------|
| `praisonai rag query "<question>"` | One-shot question answering with citations |
| `praisonai rag chat` | Interactive RAG chat session |
| `praisonai rag serve` | Start RAG as a microservice API |
| `praisonai rag eval <test_file>` | Evaluate RAG retrieval quality |

### RAG CLI Examples

```bash
# Query with hybrid retrieval (dense + BM25 keyword search)
praisonai rag query "What are the key findings?" --hybrid

# Query with hybrid + reranking for best quality
praisonai rag query "Summarize conclusions" --hybrid --rerank

# Interactive chat with hybrid retrieval
praisonai rag chat --collection research --hybrid --rerank

# Start API server with OpenAI-compatible endpoint
praisonai rag serve --hybrid --rerank --openai-compat --port 8080

# Query with profiling
praisonai rag query "Summary?" --profile --profile-out ./profile.json
```

### Knowledge CLI Commands

| Command | Description |
|---------|-------------|
| `praisonai knowledge index <sources>` | Index documents into knowledge base |
| `praisonai knowledge search <query>` | Search knowledge base (no LLM generation) |
| `praisonai knowledge list` | List indexed documents |

### Knowledge CLI Examples

```bash
# Index documents
praisonai knowledge index ./docs/ --collection myproject

# Search with hybrid retrieval
praisonai knowledge search "authentication" --hybrid --collection myproject

# Index with profiling
praisonai knowledge index ./data --profile --profile-out ./profile.json
```

### Knowledge vs RAG vs AutoRagAgent

- **Knowledge** is the indexing and retrieval substrate - use for indexing and raw search
- **RAG** orchestrates on top - use for question answering with LLM-generated responses and citations
- **AutoRagAgent** wraps an Agent with automatic retrieval decision - use when you want the agent to decide when to retrieve
- All share the same underlying index

### AutoRagAgent (Automatic RAG)

AutoRagAgent automatically decides when to retrieve context from knowledge bases vs direct chat, based on query heuristics.

```python
from praisonaiagents import Agent, AutoRagAgent

# Create agent with knowledge
agent = Agent(
    name="Research Assistant",
    knowledge=["docs/manual.pdf"],
    user_id="user123",  # Required for RAG retrieval
)

# Wrap with AutoRagAgent
auto_rag = AutoRagAgent(
    agent=agent,
    retrieval_policy="auto",  # auto, always, never
    top_k=5,
    hybrid=True,
    rerank=True,
)

# Auto-decides: retrieves for questions, skips for greetings
result = auto_rag.chat("What are the key findings?")  # Retrieves
result = auto_rag.chat("Hello!")  # Skips retrieval

# Force retrieval or skip per-call
result = auto_rag.chat("Hi", force_retrieval=True)
result = auto_rag.chat("Summary?", skip_retrieval=True)
```

**CLI Usage:**
```bash
# Enable auto-rag with default policy (auto)
praisonai --auto-rag "What are the key findings?"

# Always retrieve
praisonai --auto-rag --rag-policy always "Tell me about X"

# With hybrid retrieval and reranking
praisonai --auto-rag --rag-hybrid --rag-rerank "Summarize the document"
```

### Configuration Precedence

Settings are applied in this order (highest priority first):
1. **CLI flags** - `--hybrid`, `--rerank`, `--top-k`
2. **Environment variables** - `PRAISONAI_HYBRID=true`
3. **Config file** - YAML configuration (`--config`)
4. **Defaults**

```bash
# Environment variables
export PRAISONAI_HYBRID=true
export PRAISONAI_RERANK=true
export PRAISONAI_TOP_K=10
```

### Lightweight Installs

```bash
# Base install (minimal, fast imports)
pip install praisonaiagents

# With RAG API server support
pip install "praisonai[rag-api]"
```

### Live Tests (Real API Keys)

Run integration tests with real API keys:

```bash
# Enable live tests
export PRAISONAI_LIVE_TESTS=1
export OPENAI_API_KEY="your-key"

# Run live tests
pytest -m live tests/integration/
```

### Knowledge Stack Features Table

| Feature | Description | SDK Docs | CLI Docs |
|---------|-------------|----------|----------|
| **Hybrid Retrieval** | Dense vectors + BM25 keyword search with RRF fusion | [SDK](/docs/rag/module) | [CLI](/docs/cli/rag) |
| **Reranking** | LLM, Cross-Encoder, Cohere rerankers | [SDK](/docs/rag/module) | [CLI](/docs/cli/rag) |
| **RAG Serve** | Microservice API with OpenAI-compatible mode | [SDK](/docs/rag/module) | [CLI](/docs/cli/rag) |
| **Vector Stores** | ChromaDB, Pinecone, Qdrant, Weaviate, In-Memory | [SDK](/docs/sdk/praisonaiagents/knowledge/protocols) | [CLI](/docs/cli/knowledge) |
| **Data Readers** | Load PDF, Markdown, Text, HTML, URLs | [SDK](/docs/sdk/praisonaiagents/knowledge/protocols) | [CLI](/docs/cli/knowledge) |
| **Profiling** | Performance profiling with `--profile` flag | [SDK](/docs/features/profiling) | [CLI](/docs/cli/rag) |

---

## ðŸ”¬ Advanced Features

### Research & Intelligence

- ðŸ”¬ **Deep Research Agents** - OpenAI & Gemini support for automated research
- ðŸ”„ **Query Rewriter Agent** - HyDE, Step-back, Multi-query strategies for RAG optimization
- ðŸŒ **Native Web Search** - Real-time search via OpenAI, Gemini, Anthropic, xAI, Perplexity
- ðŸ“¥ **Web Fetch** - Retrieve full content from URLs (Anthropic)
- ðŸ“ **Prompt Expander Agent** - Expand short prompts into detailed instructions

### Memory & Caching

- ðŸ’¾ **Prompt Caching** - Reduce costs & latency (OpenAI, Anthropic, Bedrock, Deepseek)
- ðŸ§  **Claude Memory Tool** - Persistent cross-conversation memory (Anthropic Beta)
- ðŸ’¾ **File-Based Memory** - Zero-dependency persistent memory for all agents
- ðŸ” **Built-in Search Tools** - Tavily, You.com, Exa for web search, news, content extraction

### Planning & Workflows

- ðŸ“‹ **Planning Mode** - Plan before execution for agents & multi-agent systems
- ðŸ”§ **Planning Tools** - Research with tools during planning phase
- ðŸ§  **Planning Reasoning** - Chain-of-thought planning for complex tasks
- â›“ï¸ **Prompt Chaining** - Sequential prompt workflows with conditional gates
- ðŸ” **Evaluator Optimiser** - Generate and optimize through iterative feedback
- ðŸ‘· **Orchestrator Workers** - Distribute tasks among specialised workers
- âš¡ **Parallelisation** - Execute tasks in parallel for improved performance
- ðŸ” **Repetitive Agents** - Handle repetitive tasks through automated loops
- ðŸ¤– **Autonomous Workflow** - Monitor, act, adapt based on environment feedback

### Specialised Agents

- ðŸ–¼ï¸ **Image Generation Agent** - Create images from text descriptions
- ðŸ“· **Image to Text Agent** - Extract text and descriptions from images
- ðŸŽ¬ **Video Agent** - Analyse and process video content
- ðŸ“Š **Data Analyst Agent** - Analyse data and generate insights
- ðŸ’° **Finance Agent** - Financial analysis and recommendations
- ðŸ›’ **Shopping Agent** - Price comparison and shopping assistance
- â­ **Recommendation Agent** - Personalised recommendations
- ðŸ“– **Wikipedia Agent** - Search and extract Wikipedia information
- ðŸ’» **Programming Agent** - Code development and analysis
- ðŸ“ **Markdown Agent** - Generate and format Markdown content
- ðŸ”€ **Model Router** - Smart model selection based on task complexity

### MCP Protocol

- ðŸ”Œ **MCP Transports** - stdio, Streamable HTTP, WebSocket, SSE (Protocol 2025-11-25)
- ðŸŒ **WebSocket MCP** - Real-time bidirectional connections with auto-reconnect
- ðŸ” **MCP Security** - Origin validation, DNS rebinding prevention, secure sessions
- ðŸ”„ **MCP Resumability** - SSE stream recovery via Last-Event-ID

### A2A & A2UI Protocols

- ðŸ”— **A2A Protocol** - Agent-to-Agent communication for inter-agent collaboration
- ðŸ–¼ï¸ **A2UI Protocol** - Agent-to-User Interface for generating rich UIs from agents
- ðŸ“‹ **UI Templates** - ChatTemplate, ListTemplate, FormTemplate, DashboardTemplate
- ðŸ”§ **Surface Builder** - Fluent API for building declarative UIs

### Safety & Control

- ðŸ¤ **Agent Handoffs** - Transfer context between specialised agents
- ðŸ›¡ï¸ **Guardrails** - Input/output validation and safety checks
- âœ… **Human Approval** - Require human confirmation for critical actions
- ðŸ” **Tool Approval CLI** - `--trust` (auto-approve all) and `--approve-level` (risk-based approval)
- ðŸ’¬ **Sessions Management** - Isolated conversation contexts
- ðŸ”„ **Stateful Agents** - Maintain state across interactions

### Developer Tools

- âš¡ **Fast Context** - Rapid parallel code search (10-20x faster)
- ðŸ“œ **Rules & Instructions** - Auto-discover CLAUDE.md, AGENTS.md, GEMINI.md
- ðŸª **Hooks** - Pre/post operation hooks for custom logic
- ðŸ“ˆ **Telemetry** - Track agent performance and usage
- ðŸ“¹ **Camera Integration** - Capture and analyse camera input

### Other Features

- ðŸ”„ **CrewAI & AG2 Integration** - Use CrewAI or AG2 (Formerly AutoGen) Framework
- ðŸ’» **Codebase Chat** - Chat with entire codebase
- ðŸŽ¨ **Interactive UIs** - Multiple interactive interfaces
- ðŸ“„ **YAML Configuration** - YAML-based agent and workflow configuration
- ðŸ› ï¸ **Custom Tools** - Easy custom tool integration
- ðŸ” **Internet Search** - Multiple providers (Tavily, You.com, Exa, DuckDuckGo, Crawl4AI)
- ðŸ–¼ï¸ **VLM Support** - Vision Language Model support
- ðŸŽ™ï¸ **Voice Interaction** - Real-time voice interaction

---

## ðŸ’¾ Persistence (Databases)

Enable automatic conversation persistence with 2 lines of code:

```python
from praisonaiagents import Agent, db

agent = Agent(
    name="Assistant",
    db=db(database_url="postgresql://localhost/mydb"),  # db(...) shortcut
    session_id="my-session"  # Optional: defaults to per-hour ID (YYYYMMDDHH)
)
agent.chat("Hello!")  # Auto-persists messages, runs, traces
```

### Persistence CLI Commands

| Command | Description |
|---------|-------------|
| `praisonai persistence doctor` | Validate DB connectivity |
| `praisonai persistence run` | Run agent with persistence |
| `praisonai persistence resume` | Resume existing session |
| `praisonai persistence export` | Export session to JSONL |
| `praisonai persistence import` | Import session from JSONL |
| `praisonai persistence migrate` | Apply schema migrations |
| `praisonai persistence status` | Show schema status |

> **Note:** See [Knowledge & Retrieval (RAG)](#-knowledge--retrieval-rag) for complete Knowledge CLI documentation.

### Databases Table

| Database | Store Type | Install | Example | Docs |
|----------|------------|---------|---------|------|
| PostgreSQL | Conversation | `pip install "praisonai[tools]"` | [simple_db_agent.py](examples/persistence/simple_db_agent.py) | [docs](https://docs.praison.ai/docs/databases/postgres) |
| MySQL | Conversation | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| SQLite | Conversation | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| SingleStore | Conversation | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| Supabase | Conversation | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| SurrealDB | Conversation | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| Qdrant | Knowledge | `pip install "praisonai[tools]"` | [knowledge_qdrant.py](examples/persistence/knowledge_qdrant.py) | [docs](https://docs.praison.ai/docs/databases/qdrant) |
| ChromaDB | Knowledge | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| Pinecone | Knowledge | `pip install pinecone` | [pinecone_wow.py](examples/vector/pinecone_wow.py) | [docs](https://docs.praison.ai/docs/databases/pinecone) |
| Weaviate | Knowledge | `pip install weaviate-client` | [weaviate_wow.py](examples/vector/weaviate_wow.py) | [docs](https://docs.praison.ai/docs/databases/weaviate) |
| LanceDB | Knowledge | `pip install lancedb` | [lancedb_real_wow.py](examples/vector/lancedb_real_wow.py) | [docs](https://docs.praison.ai/docs/databases/lancedb) |
| Milvus | Knowledge | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| PGVector | Knowledge | `pip install psycopg2-binary` | [pgvector_real_wow.py](examples/vector/pgvector_real_wow.py) | [docs](https://docs.praison.ai/docs/databases/pgvector) |
| Redis Vector | Knowledge | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| Cassandra | Knowledge | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| ClickHouse | Knowledge | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| Redis | State | `pip install "praisonai[tools]"` | [state_redis.py](examples/persistence/state_redis.py) | [docs](https://docs.praison.ai/docs/databases/redis) |
| MongoDB | State | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| DynamoDB | State | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| Firestore | State | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| Upstash | State | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| Memory | State | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |

---

## ðŸ”§ Tools Table

Install all tools with: `pip install "praisonai[tools]"`

| Tool | Category | Import | Docs |
|------|----------|--------|------|
| Tavily | Web Search | `from praisonai_tools import TavilyTool` | [docs](https://docs.praison.ai/docs/tools/external/tavily) |
| DuckDuckGo | Web Search | `from praisonai_tools import DuckDuckGoTool` | [docs](https://docs.praison.ai/docs/tools/external/duckduckgo) |
| Exa | Web Search | `from praisonai_tools import ExaTool` | [docs](https://docs.praison.ai/docs/tools/external/exa) |
| Serper | Web Search | `from praisonai_tools import SerperTool` | [docs](https://docs.praison.ai/docs/tools/external/serper) |
| Jina | Web Reader | `from praisonai_tools import JinaTool` | [docs](https://docs.praison.ai/docs/tools/external/jina) |
| Firecrawl | Web Scraping | `from praisonai_tools import FirecrawlTool` | [docs](https://docs.praison.ai/docs/tools/external/firecrawl) |
| Crawl4AI | Web Scraping | `from praisonai_tools import Crawl4AITool` | [docs](https://docs.praison.ai/docs/tools/external/crawl4ai) |
| Wikipedia | Knowledge | `from praisonai_tools import WikipediaTool` | [docs](https://docs.praison.ai/docs/tools/external/wikipedia) |
| ArXiv | Research | `from praisonai_tools import ArxivTool` | [docs](https://docs.praison.ai/docs/tools/external/arxiv) |
| HackerNews | News | `from praisonai_tools import HackerNewsTool` | [docs](https://docs.praison.ai/docs/tools/external/hackernews) |
| YouTube | Media | `from praisonai_tools import YouTubeTool` | [docs](https://docs.praison.ai/docs/tools/external/youtube) |
| Weather | Data | `from praisonai_tools import WeatherTool` | [docs](https://docs.praison.ai/docs/tools/external/weather) |
| PostgreSQL | Database | `from praisonai_tools import PostgresTool` | [docs](https://docs.praison.ai/docs/tools/external/postgres) |
| MySQL | Database | `from praisonai_tools import MySQLTool` | [docs](https://docs.praison.ai/docs/tools/external/mysql) |
| SQLite | Database | `from praisonai_tools import SQLiteTool` | [docs](https://docs.praison.ai/docs/tools/external/sqlite) |
| MongoDB | Database | `from praisonai_tools import MongoDBTool` | [docs](https://docs.praison.ai/docs/tools/external/mongodb) |
| Redis | Database | `from praisonai_tools import RedisTool` | [docs](https://docs.praison.ai/docs/tools/external/redis) |
| Qdrant | Vector DB | `from praisonai_tools import QdrantTool` | [docs](https://docs.praison.ai/docs/tools/external/qdrant) |
| GitHub | DevOps | `from praisonai_tools import GitHubTool` | [docs](https://docs.praison.ai/docs/tools/external/github) |
| Slack | Communication | `from praisonai_tools import SlackTool` | [docs](https://docs.praison.ai/docs/tools/external/slack) |
| Discord | Communication | `from praisonai_tools import DiscordTool` | [docs](https://docs.praison.ai/docs/tools/external/discord) |
| Telegram | Communication | `from praisonai_tools import TelegramTool` | [docs](https://docs.praison.ai/docs/tools/external/telegram) |
| Email | Communication | `from praisonai_tools import EmailTool` | [docs](https://docs.praison.ai/docs/tools/external/email) |
| Notion | Productivity | `from praisonai_tools import NotionTool` | [docs](https://docs.praison.ai/docs/tools/external/notion) |
| File | File System | `from praisonai_tools import FileTool` | [docs](https://docs.praison.ai/docs/tools/external/file) |
| Shell | System | `from praisonai_tools import ShellTool` | [docs](https://docs.praison.ai/docs/tools/external/shell) |
| Python | Code | `from praisonai_tools import PythonTool` | [docs](https://docs.praison.ai/docs/tools/external/python) |
| JSON | Data | `from praisonai_tools import JSONTool` | [docs](https://docs.praison.ai/docs/tools/external/json) |
| CSV | Data | `from praisonai_tools import CSVTool` | [docs](https://docs.praison.ai/docs/tools/external/csv) |
| Calculator | Math | `from praisonai_tools import CalculatorTool` | [docs](https://docs.praison.ai/docs/tools/external/calculator) |

> See [full tools documentation](https://docs.praison.ai/docs/tools/tools) for all 100+ available tools.

---

## ðŸŽ“ Video Tutorials

Learn PraisonAI through our comprehensive video series:

<details>
<summary><strong>View all 22 video tutorials</strong></summary>

| Topic | Video |
|-------|--------|
| AI Agents with Self Reflection | [![Self Reflection](https://img.youtube.com/vi/vLXobEN2Vc8/0.jpg)](https://www.youtube.com/watch?v=vLXobEN2Vc8) |
| Reasoning Data Generating Agent | [![Reasoning Data](https://img.youtube.com/vi/fUT332Y2zA8/0.jpg)](https://www.youtube.com/watch?v=fUT332Y2zA8) |
| AI Agents with Reasoning | [![Reasoning](https://img.youtube.com/vi/KNDVWGN3TpM/0.jpg)](https://www.youtube.com/watch?v=KNDVWGN3TpM) |
| Multimodal AI Agents | [![Multimodal](https://img.youtube.com/vi/hjAWmUT1qqY/0.jpg)](https://www.youtube.com/watch?v=hjAWmUT1qqY) |
| AI Agents Workflow | [![Workflow](https://img.youtube.com/vi/yWTH44QPl2A/0.jpg)](https://www.youtube.com/watch?v=yWTH44QPl2A) |
| Async AI Agents | [![Async](https://img.youtube.com/vi/VhVQfgo00LE/0.jpg)](https://www.youtube.com/watch?v=VhVQfgo00LE) |
| Mini AI Agents | [![Mini](https://img.youtube.com/vi/OkvYp5aAGSg/0.jpg)](https://www.youtube.com/watch?v=OkvYp5aAGSg) |
| AI Agents with Memory | [![Memory](https://img.youtube.com/vi/1hVfVxvPnnQ/0.jpg)](https://www.youtube.com/watch?v=1hVfVxvPnnQ) |
| Repetitive Agents | [![Repetitive](https://img.youtube.com/vi/dAYGxsjDOPg/0.jpg)](https://www.youtube.com/watch?v=dAYGxsjDOPg) |
| Introduction | [![Introduction](https://img.youtube.com/vi/Fn1lQjC0GO0/0.jpg)](https://www.youtube.com/watch?v=Fn1lQjC0GO0) |
| Tools Overview | [![Tools Overview](https://img.youtube.com/vi/XaQRgRpV7jo/0.jpg)](https://www.youtube.com/watch?v=XaQRgRpV7jo) |
| Custom Tools | [![Custom Tools](https://img.youtube.com/vi/JSU2Rndh06c/0.jpg)](https://www.youtube.com/watch?v=JSU2Rndh06c) |
| Firecrawl Integration | [![Firecrawl](https://img.youtube.com/vi/UoqUDcLcOYo/0.jpg)](https://www.youtube.com/watch?v=UoqUDcLcOYo) |
| User Interface | [![UI](https://img.youtube.com/vi/tg-ZjNl3OCg/0.jpg)](https://www.youtube.com/watch?v=tg-ZjNl3OCg) |
| Crawl4AI Integration | [![Crawl4AI](https://img.youtube.com/vi/KAvuVUh0XU8/0.jpg)](https://www.youtube.com/watch?v=KAvuVUh0XU8) |
| Chat Interface | [![Chat](https://img.youtube.com/vi/sw3uDqn2h1Y/0.jpg)](https://www.youtube.com/watch?v=sw3uDqn2h1Y) |
| Code Interface | [![Code](https://img.youtube.com/vi/_5jQayO-MQY/0.jpg)](https://www.youtube.com/watch?v=_5jQayO-MQY) |
| Mem0 Integration | [![Mem0](https://img.youtube.com/vi/KIGSgRxf1cY/0.jpg)](https://www.youtube.com/watch?v=KIGSgRxf1cY) |
| Training | [![Training](https://img.youtube.com/vi/aLawE8kwCrI/0.jpg)](https://www.youtube.com/watch?v=aLawE8kwCrI) |
| Realtime Voice Interface | [![Realtime](https://img.youtube.com/vi/frRHfevTCSw/0.jpg)](https://www.youtube.com/watch?v=frRHfevTCSw) |
| Call Interface | [![Call](https://img.youtube.com/vi/m1cwrUG2iAk/0.jpg)](https://www.youtube.com/watch?v=m1cwrUG2iAk) |
| Reasoning Extract Agents | [![Reasoning Extract](https://img.youtube.com/vi/2PPamsADjJA/0.jpg)](https://www.youtube.com/watch?v=2PPamsADjJA) |

</details>

---

## ðŸ‘¥ Contributing

We welcome contributions from the community! Here's how you can contribute:

1. **Fork on GitHub** - Use the "Fork" button on the [repository page](https://github.com/MervinPraison/PraisonAI)
2. **Clone your fork** - `git clone https://github.com/yourusername/praisonAI.git`
3. **Create a branch** - `git checkout -b new-feature`
4. **Make changes and commit** - `git commit -am "Add some feature"`
5. **Push to your fork** - `git push origin new-feature`
6. **Submit a pull request** - Via GitHub's web interface
7. **Await feedback** - From project maintainers

---

## ðŸ”§ Development

### Using uv

```bash
# Install uv if you haven't already
pip install uv

# Install from requirements
uv pip install -r pyproject.toml

# Install with extras
uv pip install -r pyproject.toml --extra code
uv pip install -r pyproject.toml --extra "crewai,autogen"
```

### Bump and Release

```bash
# From project root - bumps version and releases in one command
python src/praisonai/scripts/bump_and_release.py 2.2.99

# With praisonaiagents dependency
python src/praisonai/scripts/bump_and_release.py 2.2.99 --agents 0.0.169

# Then publish
cd src/praisonai && uv publish
```

---

## â“ FAQ & Troubleshooting

<details>
<summary><strong>ModuleNotFoundError: No module named 'praisonaiagents'</strong></summary>

Install the package:
```bash
pip install praisonaiagents
```

</details>

<details>
<summary><strong>API key not found / Authentication error</strong></summary>

Ensure your API key is set:
```bash
export OPENAI_API_KEY=your_key_here
```

For other providers, see [Environment Variables](#environment-variables).

</details>

<details>
<summary><strong>How do I use a local model (Ollama)?</strong></summary>

```bash
# Start Ollama server first
ollama serve

# Set environment variable
export OPENAI_BASE_URL=http://localhost:11434/v1
```

See [Ollama Integration](#ollama-integration) for more details.

</details>

<details>
<summary><strong>How do I persist conversations to a database?</strong></summary>

Use the `db` parameter:
```python
from praisonaiagents import Agent, db

agent = Agent(
    name="Assistant",
    db=db(database_url="postgresql://localhost/mydb"),
    session_id="my-session"
)
```

See [Persistence (Databases)](#-persistence-databases) for supported databases.

</details>

<details>
<summary><strong>How do I enable agent memory?</strong></summary>

```python
from praisonaiagents import Agent

agent = Agent(
    name="Assistant",
    memory=True,  # Enables file-based memory (no extra deps!)
    user_id="user123"
)
```

See [Agent Memory](#6-agent-memory-zero-dependencies) for more options.

</details>

<details>
<summary><strong>How do I run multiple agents together?</strong></summary>

```python
from praisonaiagents import Agent, Agents

agent1 = Agent(instructions="Research topics")
agent2 = Agent(instructions="Summarize findings")
agents = Agents(agents=[agent1, agent2])
agents.start()
```

See [Multi Agents](#2-multi-agents) for more examples.

</details>

<details>
<summary><strong>How do I use MCP tools?</strong></summary>

```python
from praisonaiagents import Agent, MCP

agent = Agent(
    tools=MCP("npx @modelcontextprotocol/server-memory")
)
```

See [MCP Protocol](#19-mcp-model-context-protocol) for all transport options.

</details>

### Getting Help

- ðŸ“š [Full Documentation](https://docs.praison.ai)
- ðŸ› [Report Issues](https://github.com/MervinPraison/PraisonAI/issues)
- ðŸ’¬ [Discussions](https://github.com/MervinPraison/PraisonAI/discussions)

---

<div align="center">
  <p><strong>Made with â¤ï¸ by the PraisonAI Team</strong></p>
  <p>
    <a href="https://docs.praison.ai">Documentation</a> â€¢
    <a href="https://github.com/MervinPraison/PraisonAI">GitHub</a> â€¢
    <a href="https://github.com/MervinPraison/PraisonAI/issues">Issues</a>
  </p>
</div>


## Links discovered
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/agents/single-agent.py)
- [ðŸ“–](https://docs.praison.ai/agents/single)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/general/mini_agents_example.py)
- [ðŸ“–](https://docs.praison.ai/concepts/agents)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/general/auto_agents_example.py)
- [ðŸ“–](https://docs.praison.ai/features/autoagents)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/concepts/self-reflection-details.py)
- [ðŸ“–](https://docs.praison.ai/features/selfreflection)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/concepts/reasoning-extraction.py)
- [ðŸ“–](https://docs.praison.ai/features/reasoning)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/general/multimodal.py)
- [ðŸ“–](https://docs.praison.ai/features/multimodal)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/workflows/simple_workflow.py)
- [ðŸ“–](https://docs.praison.ai/features/workflows)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/workflows/workflow_with_agents.py)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/workflows/workflow_routing.py)
- [ðŸ“–](https://docs.praison.ai/features/routing)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/workflows/workflow_parallel.py)
- [ðŸ“–](https://docs.praison.ai/features/parallelisation)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/workflows/workflow_loop_csv.py)
- [ðŸ“–](https://docs.praison.ai/features/repetitive)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/workflows/workflow_repeat.py)
- [ðŸ“–](https://docs.praison.ai/features/evaluator-optimiser)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/workflows/workflow_conditional.py)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/workflows/workflow_branching.py)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/workflows/workflow_early_stop.py)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/workflows/workflow_checkpoints.py)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/agents/code-agent.py)
- [ðŸ“–](https://docs.praison.ai/features/codeagent)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/code/code_editing_example.py)
- [ðŸ“–](https://docs.praison.ai/code/editing)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/code/external_agents_example.py)
- [ðŸ“–](https://docs.praison.ai/code/external-agents)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/code/claude_code_example.py)
- [ðŸ“–](https://docs.praison.ai/code/claude-code)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/code/gemini_cli_example.py)
- [ðŸ“–](https://docs.praison.ai/code/gemini-cli)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/code/codex_cli_example.py)
- [ðŸ“–](https://docs.praison.ai/code/codex-cli)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/code/cursor_cli_example.py)
- [ðŸ“–](https://docs.praison.ai/code/cursor-cli)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/general/memory_example.py)
- [ðŸ“–](https://docs.praison.ai/concepts/memory)
- [ðŸ“–](https://docs.praison.ai/features/claude-memory-tool)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/concepts/knowledge-agents.py)
- [ðŸ“–](https://docs.praison.ai/features/knowledge)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/concepts/rag-agents.py)
- [ðŸ“–](https://docs.praison.ai/features/rag)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/concepts/chat-with-pdf.py)
- [ðŸ“–](https://docs.praison.ai/features/chat-with-pdf)
- [ðŸ“–](https://docs.praison.ai/api/praisonai/knowledge-readers-api)
- [ðŸ“–](https://docs.praison.ai/api/praisonai/knowledge-vector-store-api)
- [ðŸ“–](https://docs.praison.ai/api/praisonai/knowledge-retrieval-api)
- [ðŸ“–](https://docs.praison.ai/api/praisonai/knowledge-reranker-api)
- [ðŸ“–](https://docs.praison.ai/api/praisonai/knowledge-index-api)
- [ðŸ“–](https://docs.praison.ai/api/praisonai/knowledge-query-engine-api)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/agents/research-agent.py)
- [ðŸ“–](https://docs.praison.ai/agents/deep-research)
- [ðŸ“–](https://docs.praison.ai/agents/query-rewriter)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/agents/websearch-agent.py)
- [ðŸ“–](https://docs.praison.ai/agents/websearch)
- [ðŸ“–](https://docs.praison.ai/tools/tavily)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai-agents/examples/web_search_example.py)
- [ðŸ“–](https://docs.praison.ai/tools/web-search)
- [ðŸ“–](https://docs.praison.ai/features/model-capabilities)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/agents/planning-agent.py)
- [ðŸ“–](https://docs.praison.ai/features/planning-mode)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/general/prompt_chaining.py)
- [ðŸ“–](https://docs.praison.ai/features/promptchaining)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/general/evaluator-optimiser.py)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/general/orchestrator-workers.py)
- [ðŸ“–](https://docs.praison.ai/features/orchestrator-worker)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/agents/data-analyst-agent.py)
- [ðŸ“–](https://docs.praison.ai/agents/data-analyst)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/agents/finance-agent.py)
- [ðŸ“–](https://docs.praison.ai/agents/finance)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/agents/shopping-agent.py)
- [ðŸ“–](https://docs.praison.ai/agents/shopping)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/agents/recommendation-agent.py)
- [ðŸ“–](https://docs.praison.ai/agents/recommendation)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/agents/wikipedia-agent.py)
- [ðŸ“–](https://docs.praison.ai/agents/wikipedia)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/agents/programming-agent.py)
- [ðŸ“–](https://docs.praison.ai/agents/programming)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/agents/math-agent.py)
- [ðŸ“–](https://docs.praison.ai/features/mathagent)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/agents/markdown-agent.py)
- [ðŸ“–](https://docs.praison.ai/agents/markdown)
- [ðŸ“–](https://docs.praison.ai/agents/prompt-expander)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/image/image-agent.py)
- [ðŸ“–](https://docs.praison.ai/features/image-generation)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/agents/image-to-text-agent.py)
- [ðŸ“–](https://docs.praison.ai/agents/image-to-text)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/agents/video-agent.py)
- [ðŸ“–](https://docs.praison.ai/agents/video)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/camera.md)
- [ðŸ“–](https://docs.praison.ai/features/camera-integration)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/mcp/mcp-transports-overview.py)
- [ðŸ“–](https://docs.praison.ai/mcp/transports)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/examples/python/mcp/websocket-mcp.py)

--- docker/README.md ---
# PraisonAI Docker Setup

This directory contains Docker configurations for running PraisonAI services in containerized environments. The setup addresses directory management issues and provides comprehensive multi-service deployment options.

## ðŸ³ Available Services

### Core Services
- **UI Service** (`port 8082`) - Chainlit-based web interface
- **Chat Service** (`port 8083`) - Dedicated chat interface  
- **API Service** (`port 8080`) - REST API endpoint
- **Agents Service** - Standalone PraisonAI Agents runtime

### Docker Files
- `Dockerfile` - Basic API service
- `Dockerfile.ui` - UI service with web interface
- `Dockerfile.chat` - Chat-focused service
- `Dockerfile.dev` - Development environment with tools
- `Dockerfile.praisonaiagents` - Standalone agents framework
- `docker-compose.yml` - Multi-service orchestration

## ðŸš€ Quick Start

### Single Service
```bash
# Run UI service
docker run -p 8082:8082 -e OPENAI_API_KEY=your_key ghcr.io/mervinpraison/praisonai:ui

# Run Chat service  
docker run -p 8083:8083 -e OPENAI_API_KEY=your_key ghcr.io/mervinpraison/praisonai:chat

# Run API service
docker run -p 8080:8080 -e OPENAI_API_KEY=your_key ghcr.io/mervinpraison/praisonai:api
```

### Multi-Service with Docker Compose
```bash
# Create environment file
cat > .env << EOF
OPENAI_API_KEY=your_openai_api_key_here
CHAINLIT_AUTH_SECRET=your_secret_here
EOF

# Start all services
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

## ðŸ“ Directory Management

### Problem Solved
The original issue was that files like `chainlit.md`, `.chainlit` directory, and `public` folder were cluttering the root directory.

### Solution Implemented
All PraisonAI configuration and runtime files are now stored in `~/.praison/`:

```bash
~/.praison/
â”œâ”€â”€ database.sqlite      # Chainlit database
â”œâ”€â”€ chainlit.md         # Chainlit configuration 
â”œâ”€â”€ .chainlit/          # Chainlit runtime files
â””â”€â”€ config/             # PraisonAI configuration
```

### Environment Variables
```bash
PRAISON_CONFIG_DIR=/root/.praison      # Main config directory
CHAINLIT_CONFIG_DIR=/root/.praison     # Chainlit config location
CHAINLIT_DB_DIR=/root/.praison         # Database location
```

## ðŸ”§ Service Configuration

### UI Service (Port 8082)
```yaml
environment:
  - CHAINLIT_PORT=8082
  - CHAINLIT_HOST=0.0.0.0
  - OPENAI_API_KEY=${OPENAI_API_KEY}
  - CHAINLIT_AUTH_SECRET=${CHAINLIT_AUTH_SECRET}
```

### Chat Service (Port 8083)  
```yaml
environment:
  - CHAINLIT_PORT=8083
  - CHAINLIT_HOST=0.0.0.0
  - OPENAI_API_KEY=${OPENAI_API_KEY}
```

### API Service (Port 8080)
```yaml
environment:
  - OPENAI_API_KEY=${OPENAI_API_KEY}
```

## ðŸŽ¯ Service Endpoints

| Service | Port | Endpoint | Description |
|---------|------|----------|-------------|
| UI | 8082 | http://localhost:8082 | Web interface |
| Chat | 8083 | http://localhost:8083 | Chat interface |
| API | 8080 | http://localhost:8080 | REST API |
| API Health | 8080 | http://localhost:8080/health | Health check |

## ðŸ” Health Checks

All services include health checks:
```yaml
healthcheck:
  test: ["CMD", "curl", "-f", "http://localhost:PORT"]
  interval: 10s
  timeout: 5s
  retries: 3
```

## ðŸ“¦ Package Versions

All Docker images use consistent, up-to-date versions:
- PraisonAI: `>=2.2.95`
- PraisonAI Agents: `>=0.0.92`
- Python: `3.11-slim`

## ðŸ”’ Security Features

- Non-root user execution where possible
- Minimal base image (python:3.11-slim)
- No unnecessary packages installed
- Environment variable-based configuration
- Volume mounting for persistent data

## ðŸ›  Development

### Development Environment
```bash
# Use development Dockerfile with additional tools
docker build -f Dockerfile.dev -t praisonai:dev .
docker run -it -v $(pwd):/app praisonai:dev bash
```

### Custom Configuration
```bash
# Mount custom config directory
docker run -v ~/.praison:/root/.praison praisonai:ui
```

## ðŸ“Š Monitoring

### Docker Compose Monitoring
```bash
# View service status
docker-compose ps

# View resource usage
docker-compose top

# View logs for specific service
docker-compose logs ui
docker-compose logs chat
docker-compose logs api
```

## ðŸš¨ Troubleshooting

### Common Issues

1. **Port conflicts**
   ```bash
   # Check port usage
   netstat -tlnp | grep :8082
   
   # Use different ports
   docker run -p 9082:8082 praisonai:ui
   ```

2. **Environment variables not loading**
   ```bash
   # Verify .env file
   cat .env
   
   # Set variables directly
   docker run -e OPENAI_API_KEY=your_key praisonai:ui
   ```

3. **Permission issues**
   ```bash
   # Check volume permissions
   ls -la ~/.praison/
   
   # Fix permissions
   sudo chown -R $(id -u):$(id -g) ~/.praison/
   ```

4. **Service won't start**
   ```bash
   # Check logs
   docker-compose logs service_name
   
   # Restart service
   docker-compose restart service_name
   ```

## ðŸ”„ Updates

### Pulling Latest Images
```bash
# Pull latest images
docker-compose pull

# Restart with new images
docker-compose up -d
```

### Version Pinning
To use specific versions, update the Dockerfile:
```dockerfile
RUN pip install "praisonai==2.2.95" "praisonaiagents==0.0.92"
```

## ðŸŒ Production Deployment

### Recommended Production Setup
```yaml
# docker-compose.prod.yml
version: '3.8'
services:
  ui:
    image: ghcr.io/mervinpraison/praisonai:ui
    restart: unless-stopped
    environment:
      - CHAINLIT_HOST=0.0.0.0
      - CHAINLIT_PORT=8082
    volumes:
      - praison_data:/root/.praison
    networks:
      - praison_network
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M
```

### Load Balancer Configuration
For production environments, consider using nginx or similar:
```nginx
upstream praisonai {
    server localhost:8082;
    server localhost:8083;
}

server {
    listen 80;
    location / {
        proxy_pass http://praisonai;
    }
}
```

This Docker setup provides a clean, organized, and scalable way to deploy PraisonAI services while solving the directory management issues mentioned in the original request.

--- docker/bots/README.md ---
# PraisonAI Bot Docker Deployment

Deploy Slack, Discord, or Telegram bots using Docker.

## Quick Start

```bash
# 1. Copy environment template
cp .env.template .env

# 2. Edit .env with your tokens
nano .env

# 3. Run specific bot
docker compose up slack-bot -d

# Or run all bots
docker compose up -d
```

## Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `OPENAI_API_KEY` | Yes | LLM API key |
| `SLACK_BOT_TOKEN` | For Slack | Bot token (xoxb-...) |
| `SLACK_APP_TOKEN` | For Slack | App token (xapp-...) |
| `DISCORD_BOT_TOKEN` | For Discord | Bot token |
| `TELEGRAM_BOT_TOKEN` | For Telegram | Bot token |

## Slack Setup

1. Create app at https://api.slack.com/apps
2. Enable **Socket Mode** â†’ Generate App-Level Token with `connections:write`
3. Add **OAuth Scopes**: `chat:write`, `app_mentions:read`, `im:history`
4. Enable **Event Subscriptions** â†’ Subscribe to `app_mention`, `message.im`
5. **Install to Workspace**
6. Copy Bot Token and App Token to `.env`

## Commands

```bash
# Start Slack bot
docker compose up slack-bot -d

# View logs
docker compose logs -f slack-bot

# Stop bot
docker compose down slack-bot

# Rebuild after updates
docker compose build --no-cache slack-bot
```

## Custom Agent Configuration

Mount your agent config:

```yaml
services:
  slack-bot:
    volumes:
      - ./agent.yaml:/app/agent.yaml
    command: ["praisonai", "bot", "slack", "--agent", "/app/agent.yaml"]
```


--- docker/call/README.md ---
# Praison AI Call Docker

1. Build the Docker image:
```
docker build -t praisonai-call .
```

2. Run the container:
```
docker run -d -p 8090:8090 praisonai-call -e OPENAI_API_KEY=your_api_key_here
```

Make sure to replace your_api_key_here with your actual OpenAI API key.

--- src/praisonai-agents/README.md ---
../../README.md

--- src/praisonai-rust/README.md ---
# PraisonAI Rust SDK

High-performance, agentic AI framework for Rust.

## Quick Start

```rust
use praisonai::{Agent, tool, Tool};

#[tool(description = "Search the web")]
async fn search(query: String) -> String {
    format!("Results for: {}", query)
}

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    let agent = Agent::new()
        .instructions("You are a helpful assistant")
        .tool(SearchTool::new())
        .build()?;
    
    let response = agent.chat("Hello!").await?;
    println!("{}", response);
    Ok(())
}
```

## Installation

Add to your `Cargo.toml`:

```toml
[dependencies]
praisonai = "0.1"
tokio = { version = "1", features = ["full"] }
```

## CLI Usage

```bash
# Install CLI
cargo install praisonai-cli

# Interactive chat
praisonai-rust chat

# Single-shot prompt
praisonai-rust "What is 2+2?"

# Run workflow from YAML
praisonai-rust run agents.yaml
```

## Features

- **Agent-Centric**: Every design decision centers on Agents
- **Multi-Provider LLM**: OpenAI, Anthropic, Ollama support via rig-core
- **Tool System**: `#[tool]` macro for easy tool creation
- **Multi-Agent Workflows**: AgentTeam, AgentFlow patterns
- **Memory**: Conversation history and long-term storage
- **Async-First**: Built on Tokio for high performance

## Crates

| Crate | Description |
|-------|-------------|
| `praisonai` | Core library (Agent, Tools, Workflows) |
| `praisonai-derive` | Proc macros (`#[tool]`) |
| `praisonai-cli` | CLI binary |

## API Parity with Python

```python
# Python
from praisonaiagents import Agent, tool

@tool
def search(query: str) -> str:
    """Search the web."""
    return f"Results for: {query}"

agent = Agent(instructions="Be helpful", tools=[search])
response = agent.start("Hello!")
```

```rust
// Rust
use praisonai::{Agent, tool, Tool};

#[tool(description = "Search the web")]
async fn search(query: String) -> String {
    format!("Results for: {}", query)
}

let agent = Agent::new()
    .instructions("Be helpful")
    .tool(SearchTool::new())
    .build()?;
let response = agent.chat("Hello!").await?;
```

## License

MIT


--- src/praisonai-ts/README.md ---
# PraisonAI TypeScript Node.js AI Agents Framework

PraisonAI is a production-ready Multi AI Agents framework, designed to create AI Agents to automate and solve problems ranging from simple tasks to complex challenges. It provides a low-code solution to streamline the building and management of multi-agent LLM systems, emphasising simplicity, customisation, and effective human-agent collaboration.

## Installation

```bash
npm install praisonai
```

## Development Setup

1. Clone the repository:
```bash
git clone https://github.com/MervinPraison/PraisonAI.git
cd src/praisonai-ts
```

2. Install dependencies:
```bash
npm install
```

3. Build the package:
```bash
npm run build
```

## Usage

Here are examples of different ways to use PraisonAI:

### 1. Single Agent Example

```typescript
import { Agent, AgentTeam } from 'praisonai';

async function main() {
    // Create a simple agent (no task specified)
    const agent = new Agent({
        name: "BiologyExpert",
        instructions: "Explain the process of photosynthesis in detail.",
        verbose: true
    });

    // Run the agent with AgentTeam
    const team = new AgentTeam({
        agents: [agent],
        tasks: ["Explain the process of photosynthesis in detail."],
        verbose: true
    });

    try {
        console.log('Starting single agent example...');
        const results = await team.start();
        console.log('\nFinal Results:', results);
    } catch (error) {
        console.error('Error:', error);
    }
}

main();
```

### 2. Multi-Agent Example

```typescript
import { Agent, AgentTeam } from 'praisonai';

async function main() {
    // Create multiple agents with different roles
    const researchAgent = new Agent({
        name: "ResearchAgent",
        instructions: "Research and provide detailed information about renewable energy sources.",
        verbose: true
    });

    const summaryAgent = new Agent({
        name: "SummaryAgent",
        instructions: "Create a concise summary of the research findings about renewable energy sources. Use {previous_result} as input.",
        verbose: true
    });

    const recommendationAgent = new Agent({
        name: "RecommendationAgent",
        instructions: "Based on the summary in {previous_result}, provide specific recommendations for implementing renewable energy solutions.",
        verbose: true
    });

    // Run the agents in sequence with AgentTeam
    const team = new AgentTeam({
        agents: [researchAgent, summaryAgent, recommendationAgent],
        tasks: [
            "Research and analyze current renewable energy technologies and their implementation.",
            "Summarize the key findings from the research.",
            "Provide actionable recommendations based on the summary."
        ],
        verbose: true
    });

    try {
        console.log('Starting multi-agent example...');
        const results = await team.start();
        console.log('\nFinal Results:', results);
    } catch (error) {
        console.error('Error:', error);
    }
}

main();
```

### 3. Task-Based Agent Example

```typescript
import { Agent, Task, AgentTeam } from 'praisonai';

async function main() {
    // Create agents first
    const dietAgent = new Agent({
        name: "DietAgent",
        role: "Nutrition Expert",
        goal: "Create healthy and delicious recipes",
        backstory: "You are a certified nutritionist with years of experience in creating balanced meal plans.",
        verbose: true,  // Enable streaming output
        instructions: `You are a professional chef and nutritionist. Create 5 healthy food recipes that are both nutritious and delicious.
Each recipe should include:
1. Recipe name
2. List of ingredients with quantities
3. Step-by-step cooking instructions
4. Nutritional information
5. Health benefits

Format your response in markdown.`
    });

    const blogAgent = new Agent({
        name: "BlogAgent",
        role: "Food Blogger",
        goal: "Write engaging blog posts about food and recipes",
        backstory: "You are a successful food blogger known for your ability to make recipes sound delicious and approachable.",
        verbose: true,  // Enable streaming output
        instructions: `You are a food and health blogger. Write an engaging blog post about the provided recipes.
The blog post should:
1. Have an engaging title
2. Include an introduction about healthy eating`
    });

    // Create tasks
    const createRecipesTask = new Task({
        name: "Create Recipes",
        description: "Create 5 healthy and delicious recipes",
        agent: dietAgent
    });

    const writeBlogTask = new Task({
        name: "Write Blog",
        description: "Write a blog post about the recipes",
        agent: blogAgent,
        dependencies: [createRecipesTask]  // This task depends on the recipes being created first
    });

    // Run the tasks with AgentTeam
    const team = new AgentTeam({
        tasks: [createRecipesTask, writeBlogTask],
        verbose: true
    });

    try {
        console.log('Starting task-based example...');
        const results = await team.start();
        console.log('\nFinal Results:', results);
    } catch (error) {
        console.error('Error:', error);
    }
}

main();
```

### Running the Examples

1. First, set up your environment variables:
```bash
export OPENAI_API_KEY='your-api-key'
```

2. Create a new TypeScript file (e.g., `example.ts`) with any of the above examples.

3. Run the example:
```bash
npx ts-node example.ts
```

For more examples, check out the `examples/concepts/` directory in the repository.

## Package Structure

```
src/
â”œâ”€â”€ agent/         # Agent-related interfaces and implementations
â”œâ”€â”€ agents/        # Multi-agent system management
â”œâ”€â”€ knowledge/     # Knowledge base and management
â”œâ”€â”€ llm/          # Language Model interfaces
â”œâ”€â”€ memory/       # Memory management systems
â”œâ”€â”€ process/      # Process management
â”œâ”€â”€ task/         # Task management
â””â”€â”€ tools/        # Various utility tools
    â”œâ”€â”€ arxivTools.ts
    â””â”€â”€ ... (other tools)
```

## Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

MIT License - see the LICENSE file for details

## Testing

### Manual Testing

```bash
export OPENAI_API_KEY='your-api-key'
npx ts-node tests/development/simple/single-agent.ts
npx ts-node tests/development/simple/multi-agent.ts
npx ts-node tests/development/simple/multi-agents-simple.js
```

## Examples Testing

```bash
export OPENAI_API_KEY='your-api-key'
npx ts-node examples/simple/single-agent.ts
npx ts-node examples/simple/multi-agent.ts
```

### Automated Testing (WIP)

```bash
npm run test
```



--- src/praisonai/README.md ---
<p align="center">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="docs/logo/dark.png" />
    <source media="(prefers-color-scheme: light)" srcset="docs/logo/light.png" />
    <img alt="PraisonAI Logo" src="docs/logo/light.png" />
  </picture>
</p>

<!-- mcp-name: io.github.MervinPraison/praisonai -->

<p align="center">
<a href="https://github.com/MervinPraison/PraisonAI"><img src="https://static.pepy.tech/badge/PraisonAI" alt="Total Downloads" /></a>
<a href="https://github.com/MervinPraison/PraisonAI"><img src="https://img.shields.io/github/v/release/MervinPraison/PraisonAI" alt="Latest Stable Version" /></a>
<a href="https://github.com/MervinPraison/PraisonAI"><img src="https://img.shields.io/badge/License-MIT-yellow.svg" alt="License" /></a>
<a href="https://registry.modelcontextprotocol.io/servers/io.github.MervinPraison/praisonai"><img src="https://img.shields.io/badge/MCP-Registry-blue" alt="MCP Registry" /></a>
</p>

<div align="center">

# Praison AI

<a href="https://trendshift.io/repositories/9130" target="_blank"><img src="https://trendshift.io/api/badge/repositories/9130" alt="MervinPraison%2FPraisonAI | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>

</div>

PraisonAI is a production-ready Multi-AI Agents framework with self-reflection, designed to create AI Agents to automate and solve problems ranging from simple tasks to complex challenges. It provides a low-code solution to streamline the building and management of multi-agent LLM systems, emphasising simplicity, customisation, and effective human-agent collaboration.

<div align="center">
  <a href="https://docs.praison.ai">
    <p align="center">
      <img src="https://img.shields.io/badge/ðŸ“š_Documentation-Visit_docs.praison.ai-blue?style=for-the-badge&logo=bookstack&logoColor=white" alt="Documentation" />
    </p>
  </a>
</div>

---

> **Quick Paths:**
> - ðŸ†• **New here?** â†’ [Quick Start](#-quick-start) *(1 minute to first agent)*
> - ðŸ“¦ **Installing?** â†’ [Installation](#-installation)
> - ðŸ **Python SDK?** â†’ [Python Examples](#-using-python-code)
> - ðŸŽ¯ **CLI user?** â†’ [CLI Quick Reference](#cli-quick-reference)
> - ðŸ”§ **Need config?** â†’ [Configuration](#-configuration--integration)
> - ðŸ¤ **Contributing?** â†’ [Development](#-development)

---

## ðŸ“‘ Table of Contents

<details open>
<summary><strong>Getting Started</strong></summary>

- [ðŸš€ Quick Start](#-quick-start)
- [ðŸ“¦ Installation](#-installation)
- [âš¡ Performance](#-performance)

</details>

<details>
<summary><strong>Python SDK</strong></summary>

- [ðŸ“˜ Python Examples](#-using-python-code)
  - [1. Single Agent](#1-single-agent) | [2. Multi Agents](#2-multi-agents) | [3. Planning Mode](#3-agent-with-planning-mode)
  - [4. Deep Research](#4-deep-research-agent) | [5. Query Rewriter](#5-query-rewriter-agent) | [6. Agent Memory](#6-agent-memory-zero-dependencies)
  - [7. Rules & Instructions](#7-rules--instructions) | [8. Auto-Generated Memories](#8-auto-generated-memories) | [9. Agentic Workflows](#9-agentic-workflows)
  - [10. Hooks](#10-hooks) | [11. Shadow Git Checkpoints](#11-shadow-git-checkpoints) | [12. Background Tasks](#12-background-tasks)
  - [13. Policy Engine](#13-policy-engine) | [14. Thinking Budgets](#14-thinking-budgets) | [15. Output Styles](#15-output-styles)
  - [16. Context Compaction](#16-context-compaction) | [17. Field Names Reference](#17-field-names-reference-a-i-g-s) | [18. Extended agents.yaml](#18-extended-agentsyaml-with-workflow-patterns)
  - [19. MCP Protocol](#19-mcp-model-context-protocol) | [20. A2A Protocol](#20-a2a-agent2agent-protocol)
- [ðŸ› ï¸ Custom Tools](#ï¸-custom-tools)

</details>

<details>
<summary><strong>JavaScript SDK</strong></summary>

- [ðŸ’» JavaScript Examples](#-using-javascript-code)

</details>

<details>
<summary><strong>CLI Reference</strong></summary>

- [ðŸŽ¯ CLI Overview](#-cli--no-code-interface) | [CLI Quick Reference](#cli-quick-reference)
- [Auto Mode](#auto-mode) | [Interactive Mode](#interactive-mode-cli) | [Deep Research CLI](#deep-research-cli) | [Planning Mode CLI](#planning-mode-cli)
- [Memory CLI](#memory-cli) | [Workflow CLI](#workflow-cli) | [Knowledge CLI](#knowledge-cli) | [Session CLI](#session-cli)
- [Tools CLI](#tools-cli) | [MCP Config CLI](#mcp-config-cli) | [External Agents CLI](#external-agents-cli) | [CLI Features Summary](#cli-features)

</details>

<details>
<summary><strong>Configuration & Features</strong></summary>

- [âœ¨ Key Features](#-key-features) | [ðŸŒ Supported Providers](#-supported-providers)
- [ðŸ”§ Configuration & Integration](#-configuration--integration) | [Ollama](#ollama-integration) | [Groq](#groq-integration) | [100+ Models](#100-models-support)
- [ðŸ“‹ Agents Playbook](#-agents-playbook)
- [ðŸ”¬ Advanced Features](#-advanced-features)

</details>

<details>
<summary><strong>Architecture & Patterns</strong></summary>

- [ðŸ“Š Process Types & Patterns](#-process-types--patterns)
- [Sequential](#sequential-process) | [Hierarchical](#hierarchical-process) | [Workflow](#workflow-process) | [Agentic Patterns](#agentic-patterns)

</details>

<details>
<summary><strong>Data & Persistence</strong></summary>

- [ðŸ’¾ Persistence (Databases)](#-persistence-databases)
- [ðŸ“š Knowledge & Retrieval (RAG)](#-knowledge--retrieval-rag)
- [ðŸ”§ Tools Table](#-tools-table)

</details>

<details>
<summary><strong>Learning & Community</strong></summary>

- [ðŸŽ“ Video Tutorials](#-video-tutorials) | [â­ Star History](#-star-history)
- [ðŸ‘¥ Contributing](#-contributing) | [ðŸ”§ Development](#-development) | [â“ FAQ & Troubleshooting](#-faq--troubleshooting)

</details>

---

## âš¡ Performance

PraisonAI Agents is the **fastest AI agent framework** for agent instantiation.

| Framework | Avg Time (Î¼s) | Relative |
|-----------|---------------|----------|
| **PraisonAI** | **3.77** | **1.00x (fastest)** |
| OpenAI Agents SDK | 5.26 | 1.39x |
| Agno | 5.64 | 1.49x |
| PraisonAI (LiteLLM) | 7.56 | 2.00x |
| PydanticAI | 226.94 | 60.16x |
| LangGraph | 4,558.71 | 1,209x |

<details>
<summary>Run benchmarks yourself</summary>

```bash
cd praisonai-agents
python benchmarks/simple_benchmark.py
```

</details>

---

## ðŸš€ Quick Start

Get started with PraisonAI in under 1 minute:

```bash
# Install
pip install praisonaiagents

# Set API key
export OPENAI_API_KEY=your_key_here

# Create a simple agent
python -c "from praisonaiagents import Agent; Agent(instructions='You are a helpful AI assistant').start('Write a haiku about AI')"
```

> **Next Steps:** [Single Agent Example](#1-single-agent) | [Multi Agents](#2-multi-agents) | [CLI Auto Mode](#auto-mode)

---

## ðŸ“¦ Installation

### Python SDK

Lightweight package dedicated for coding:

```bash
pip install praisonaiagents
```

For the full framework with CLI support:

```bash
pip install praisonai
```

### JavaScript SDK

```bash
npm install praisonai
```

### Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `OPENAI_API_KEY` | Yes* | OpenAI API key |
| `ANTHROPIC_API_KEY` | No | Anthropic Claude API key |
| `GOOGLE_API_KEY` | No | Google Gemini API key |
| `GROQ_API_KEY` | No | Groq API key |
| `OPENAI_BASE_URL` | No | Custom API endpoint (for Ollama, Groq, etc.) |

> *At least one LLM provider API key is required.

```bash
# Set your API key
export OPENAI_API_KEY=your_key_here

# For Ollama (local models)
export OPENAI_BASE_URL=http://localhost:11434/v1

# For Groq
export OPENAI_API_KEY=your_groq_key
export OPENAI_BASE_URL=https://api.groq.com/openai/v1
```

---

## âœ¨ Key Features

<details open>
<summary><strong>ðŸ¤– Core Agents</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Single Agent | [Example](examples/python/agents/single-agent.py) | [ðŸ“–](https://docs.praison.ai/agents/single) |
| Multi Agents | [Example](examples/python/general/mini_agents_example.py) | [ðŸ“–](https://docs.praison.ai/concepts/agents) |
| Auto Agents | [Example](examples/python/general/auto_agents_example.py) | [ðŸ“–](https://docs.praison.ai/features/autoagents) |
| Self Reflection AI Agents | [Example](examples/python/concepts/self-reflection-details.py) | [ðŸ“–](https://docs.praison.ai/features/selfreflection) |
| Reasoning AI Agents | [Example](examples/python/concepts/reasoning-extraction.py) | [ðŸ“–](https://docs.praison.ai/features/reasoning) |
| Multi Modal AI Agents | [Example](examples/python/general/multimodal.py) | [ðŸ“–](https://docs.praison.ai/features/multimodal) |

</details>

<details>
<summary><strong>ðŸ”„ Workflows</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Simple Workflow | [Example](examples/python/workflows/simple_workflow.py) | [ðŸ“–](https://docs.praison.ai/features/workflows) |
| Workflow with Agents | [Example](examples/python/workflows/workflow_with_agents.py) | [ðŸ“–](https://docs.praison.ai/features/workflows) |
| Agentic Routing (`route()`) | [Example](examples/python/workflows/workflow_routing.py) | [ðŸ“–](https://docs.praison.ai/features/routing) |
| Parallel Execution (`parallel()`) | [Example](examples/python/workflows/workflow_parallel.py) | [ðŸ“–](https://docs.praison.ai/features/parallelisation) |
| Loop over List/CSV (`loop()`) | [Example](examples/python/workflows/workflow_loop_csv.py) | [ðŸ“–](https://docs.praison.ai/features/repetitive) |
| Evaluator-Optimizer (`repeat()`) | [Example](examples/python/workflows/workflow_repeat.py) | [ðŸ“–](https://docs.praison.ai/features/evaluator-optimiser) |
| Conditional Steps | [Example](examples/python/workflows/workflow_conditional.py) | [ðŸ“–](https://docs.praison.ai/features/workflows) |
| Workflow Branching | [Example](examples/python/workflows/workflow_branching.py) | [ðŸ“–](https://docs.praison.ai/features/workflows) |
| Workflow Early Stop | [Example](examples/python/workflows/workflow_early_stop.py) | [ðŸ“–](https://docs.praison.ai/features/workflows) |
| Workflow Checkpoints | [Example](examples/python/workflows/workflow_checkpoints.py) | [ðŸ“–](https://docs.praison.ai/features/workflows) |

</details>

<details>
<summary><strong>ðŸ’» Code & Development</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Code Interpreter Agents | [Example](examples/python/agents/code-agent.py) | [ðŸ“–](https://docs.praison.ai/features/codeagent) |
| AI Code Editing Tools | [Example](examples/python/code/code_editing_example.py) | [ðŸ“–](https://docs.praison.ai/code/editing) |
| External Agents (All) | [Example](examples/python/code/external_agents_example.py) | [ðŸ“–](https://docs.praison.ai/code/external-agents) |
| Claude Code CLI | [Example](examples/python/code/claude_code_example.py) | [ðŸ“–](https://docs.praison.ai/code/claude-code) |
| Gemini CLI | [Example](examples/python/code/gemini_cli_example.py) | [ðŸ“–](https://docs.praison.ai/code/gemini-cli) |
| Codex CLI | [Example](examples/python/code/codex_cli_example.py) | [ðŸ“–](https://docs.praison.ai/code/codex-cli) |
| Cursor CLI | [Example](examples/python/code/cursor_cli_example.py) | [ðŸ“–](https://docs.praison.ai/code/cursor-cli) |

</details>

<details>
<summary><strong>ðŸ§  Memory & Knowledge</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Memory (Short & Long Term) | [Example](examples/python/general/memory_example.py) | [ðŸ“–](https://docs.praison.ai/concepts/memory) |
| File-Based Memory | [Example](examples/python/general/memory_example.py) | [ðŸ“–](https://docs.praison.ai/concepts/memory) |
| Claude Memory Tool | [Example](#claude-memory-tool-cli) | [ðŸ“–](https://docs.praison.ai/features/claude-memory-tool) |
| Add Custom Knowledge | [Example](examples/python/concepts/knowledge-agents.py) | [ðŸ“–](https://docs.praison.ai/features/knowledge) |
| RAG Agents | [Example](examples/python/concepts/rag-agents.py) | [ðŸ“–](https://docs.praison.ai/features/rag) |
| Chat with PDF Agents | [Example](examples/python/concepts/chat-with-pdf.py) | [ðŸ“–](https://docs.praison.ai/features/chat-with-pdf) |
| Data Readers (PDF, DOCX, etc.) | [CLI](#knowledge-cli) | [ðŸ“–](https://docs.praison.ai/api/praisonai/knowledge-readers-api) |
| Vector Store Selection | [CLI](#knowledge-cli) | [ðŸ“–](https://docs.praison.ai/api/praisonai/knowledge-vector-store-api) |
| Retrieval Strategies | [CLI](#knowledge-cli) | [ðŸ“–](https://docs.praison.ai/api/praisonai/knowledge-retrieval-api) |
| Rerankers | [CLI](#knowledge-cli) | [ðŸ“–](https://docs.praison.ai/api/praisonai/knowledge-reranker-api) |
| Index Types (Vector/Keyword/Hybrid) | [CLI](#knowledge-cli) | [ðŸ“–](https://docs.praison.ai/api/praisonai/knowledge-index-api) |
| Query Engines (Sub-Question, etc.) | [CLI](#knowledge-cli) | [ðŸ“–](https://docs.praison.ai/api/praisonai/knowledge-query-engine-api) |

</details>

<details>
<summary><strong>ðŸ”¬ Research & Intelligence</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Deep Research Agents | [Example](examples/python/agents/research-agent.py) | [ðŸ“–](https://docs.praison.ai/agents/deep-research) |
| Query Rewriter Agent | [Example](#5-query-rewriter-agent) | [ðŸ“–](https://docs.praison.ai/agents/query-rewriter) |
| Native Web Search | [Example](examples/python/agents/websearch-agent.py) | [ðŸ“–](https://docs.praison.ai/agents/websearch) |
| Built-in Search Tools | [Example](examples/python/agents/websearch-agent.py) | [ðŸ“–](https://docs.praison.ai/tools/tavily) |
| Unified Web Search | [Example](src/praisonai-agents/examples/web_search_example.py) | [ðŸ“–](https://docs.praison.ai/tools/web-search) |
| Web Fetch (Anthropic) | [Example](#web-search-web-fetch--prompt-caching) | [ðŸ“–](https://docs.praison.ai/features/model-capabilities) |

</details>

<details>
<summary><strong>ðŸ“‹ Planning & Execution</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Planning Mode | [Example](examples/python/agents/planning-agent.py) | [ðŸ“–](https://docs.praison.ai/features/planning-mode) |
| Planning Tools | [Example](#3-agent-with-planning-mode) | [ðŸ“–](https://docs.praison.ai/features/planning-mode) |
| Planning Reasoning | [Example](#3-agent-with-planning-mode) | [ðŸ“–](https://docs.praison.ai/features/planning-mode) |
| Prompt Chaining | [Example](examples/python/general/prompt_chaining.py) | [ðŸ“–](https://docs.praison.ai/features/promptchaining) |
| Evaluator Optimiser | [Example](examples/python/general/evaluator-optimiser.py) | [ðŸ“–](https://docs.praison.ai/features/evaluator-optimiser) |
| Orchestrator Workers | [Example](examples/python/general/orchestrator-workers.py) | [ðŸ“–](https://docs.praison.ai/features/orchestrator-worker) |

</details>

<details>
<summary><strong>ðŸ‘¥ Specialized Agents</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Data Analyst Agent | [Example](examples/python/agents/data-analyst-agent.py) | [ðŸ“–](https://docs.praison.ai/agents/data-analyst) |
| Finance Agent | [Example](examples/python/agents/finance-agent.py) | [ðŸ“–](https://docs.praison.ai/agents/finance) |
| Shopping Agent | [Example](examples/python/agents/shopping-agent.py) | [ðŸ“–](https://docs.praison.ai/agents/shopping) |
| Recommendation Agent | [Example](examples/python/agents/recommendation-agent.py) | [ðŸ“–](https://docs.praison.ai/agents/recommendation) |
| Wikipedia Agent | [Example](examples/python/agents/wikipedia-agent.py) | [ðŸ“–](https://docs.praison.ai/agents/wikipedia) |
| Programming Agent | [Example](examples/python/agents/programming-agent.py) | [ðŸ“–](https://docs.praison.ai/agents/programming) |
| Math Agents | [Example](examples/python/agents/math-agent.py) | [ðŸ“–](https://docs.praison.ai/features/mathagent) |
| Markdown Agent | [Example](examples/python/agents/markdown-agent.py) | [ðŸ“–](https://docs.praison.ai/agents/markdown) |
| Prompt Expander Agent | [Example](#prompt-expansion) | [ðŸ“–](https://docs.praison.ai/agents/prompt-expander) |

</details>

<details>
<summary><strong>ðŸŽ¨ Media & Multimodal</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Image Generation Agent | [Example](examples/python/image/image-agent.py) | [ðŸ“–](https://docs.praison.ai/features/image-generation) |
| Image to Text Agent | [Example](examples/python/agents/image-to-text-agent.py) | [ðŸ“–](https://docs.praison.ai/agents/image-to-text) |
| Video Agent | [Example](examples/python/agents/video-agent.py) | [ðŸ“–](https://docs.praison.ai/agents/video) |
| Camera Integration | [Example](examples/python/camera/) | [ðŸ“–](https://docs.praison.ai/features/camera-integration) |

</details>

<details>
<summary><strong>ðŸ”Œ Protocols & Integration</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| MCP Transports | [Example](examples/python/mcp/mcp-transports-overview.py) | [ðŸ“–](https://docs.praison.ai/mcp/transports) |
| WebSocket MCP | [Example](examples/python/mcp/websocket-mcp.py) | [ðŸ“–](https://docs.praison.ai/mcp/sse-transport) |
| MCP Security | [Example](examples/python/mcp/mcp-security.py) | [ðŸ“–](https://docs.praison.ai/mcp/transports) |
| MCP Resumability | [Example](examples/python/mcp/mcp-resumability.py) | [ðŸ“–](https://docs.praison.ai/mcp/sse-transport) |
| MCP Config Management | [Example](#mcp-config-cli) | [ðŸ“–](https://docs.praison.ai/docs/cli/mcp) |
| LangChain Integrated Agents | [Example](examples/python/general/langchain_example.py) | [ðŸ“–](https://docs.praison.ai/features/langchain) |

</details>

<details>
<summary><strong>ðŸ›¡ï¸ Safety & Control</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Guardrails | [Example](examples/python/guardrails/comprehensive-guardrails-example.py) | [ðŸ“–](https://docs.praison.ai/features/guardrails) |
| Human Approval | [Example](examples/python/general/human_approval_example.py) | [ðŸ“–](https://docs.praison.ai/features/approval) |
| Rules & Instructions | [Example](#7-rules--instructions) | [ðŸ“–](https://docs.praison.ai/features/rules) |

</details>

<details>
<summary><strong>âš™ï¸ Advanced Features</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Async & Parallel Processing | [Example](examples/python/general/async_example.py) | [ðŸ“–](https://docs.praison.ai/features/async) |
| Parallelisation | [Example](examples/python/general/parallelisation.py) | [ðŸ“–](https://docs.praison.ai/features/parallelisation) |
| Repetitive Agents | [Example](examples/python/concepts/repetitive-agents.py) | [ðŸ“–](https://docs.praison.ai/features/repetitive) |
| Agent Handoffs | [Example](examples/python/handoff/handoff_basic.py) | [ðŸ“–](https://docs.praison.ai/features/handoffs) |
| Stateful Agents | [Example](examples/python/stateful/workflow-state-example.py) | [ðŸ“–](https://docs.praison.ai/features/stateful-agents) |
| Autonomous Workflow | [Example](examples/python/general/autonomous-agent.py) | [ðŸ“–](https://docs.praison.ai/features/autonomous-workflow) |
| Structured Output Agents | [Example](examples/python/general/structured_agents_example.py) | [ðŸ“–](https://docs.praison.ai/features/structured) |
| Model Router | [Example](examples/python/agents/router-agent-cost-optimization.py) | [ðŸ“–](https://docs.praison.ai/features/model-router) |
| Prompt Caching | [Example](#web-search-web-fetch--prompt-caching) | [ðŸ“–](https://docs.praison.ai/features/model-capabilities) |
| Fast Context | [Example](examples/context/00_agent_fast_context_basic.py) | [ðŸ“–](https://docs.praison.ai/features/fast-context) |

</details>

<details>
<summary><strong>ðŸ› ï¸ Tools & Configuration</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| 100+ Custom Tools | [Example](examples/python/general/tools_example.py) | [ðŸ“–](https://docs.praison.ai/tools/tools) |
| YAML Configuration | [Example](examples/cookbooks/yaml/secondary_market_research_agents.yaml) | [ðŸ“–](https://docs.praison.ai/developers/agents-playbook) |
| 100+ LLM Support | [Example](examples/python/providers/openai/openai_gpt4_example.py) | [ðŸ“–](https://docs.praison.ai/models) |
| Callback Agents | [Example](examples/python/general/advanced-callback-systems.py) | [ðŸ“–](https://docs.praison.ai/features/callbacks) |
| Hooks | [Example](#10-hooks) | [ðŸ“–](https://docs.praison.ai/features/hooks) |
| Middleware System | [Example](examples/middleware/basic_middleware.py) | [ðŸ“–](https://docs.praison.ai/features/middleware) |
| Configurable Model | [Example](examples/middleware/configurable_model.py) | [ðŸ“–](https://docs.praison.ai/features/configurable-model) |
| Rate Limiter | [Example](examples/middleware/rate_limiter.py) | [ðŸ“–](https://docs.praison.ai/features/rate-limiter) |
| Injected Tool State | [Example](examples/middleware/injected_state.py) | [ðŸ“–](https://docs.praison.ai/features/injected-state) |
| Shadow Git Checkpoints | [Example](#11-shadow-git-checkpoints) | [ðŸ“–](https://docs.praison.ai/features/checkpoints) |
| Background Tasks | [Example](examples/background/basic_background.py) | [ðŸ“–](https://docs.praison.ai/features/background-tasks) |
| Policy Engine | [Example](examples/policy/basic_policy.py) | [ðŸ“–](https://docs.praison.ai/features/policy-engine) |
| Thinking Budgets | [Example](examples/thinking/basic_thinking.py) | [ðŸ“–](https://docs.praison.ai/features/thinking-budgets) |
| Output Styles | [Example](examples/output/basic_output.py) | [ðŸ“–](https://docs.praison.ai/features/output-styles) |
| Context Compaction | [Example](examples/compaction/basic_compaction.py) | [ðŸ“–](https://docs.praison.ai/features/context-compaction) |

</details>

<details>
<summary><strong>ðŸ“Š Monitoring & Management</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Sessions Management | [Example](examples/python/sessions/comprehensive-session-management.py) | [ðŸ“–](https://docs.praison.ai/features/sessions) |
| Auto-Save Sessions | [Example](#session-management-python) | [ðŸ“–](https://docs.praison.ai/docs/cli/session) |
| History in Context | [Example](#session-management-python) | [ðŸ“–](https://docs.praison.ai/docs/cli/session) |
| Telemetry | [Example](examples/python/telemetry/production-telemetry-example.py) | [ðŸ“–](https://docs.praison.ai/features/telemetry) |
| Project Docs (.praison/docs/) | [Example](#docs-cli) | [ðŸ“–](https://docs.praison.ai/docs/cli/docs) |
| AI Commit Messages | [Example](#ai-commit-cli) | [ðŸ“–](https://docs.praison.ai/docs/cli/commit) |
| @Mentions in Prompts | [Example](#mentions-in-prompts) | [ðŸ“–](https://docs.praison.ai/docs/cli/mentions) |

</details>

<details>
<summary><strong>ðŸ–¥ï¸ CLI Features</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Slash Commands | [Example](examples/python/cli/slash_commands_example.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/slash-commands) |
| Autonomy Modes | [Example](examples/python/cli/autonomy_modes_example.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/autonomy-modes) |
| Cost Tracking | [Example](examples/python/cli/cost_tracking_example.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/cost-tracking) |
| Repository Map | [Example](examples/python/cli/repo_map_example.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/repo-map) |
| Interactive TUI | [Example](examples/python/cli/interactive_tui_example.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/interactive-tui) |
| Git Integration | [Example](examples/python/cli/git_integration_example.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/git-integration) |
| Sandbox Execution | [Example](examples/python/cli/sandbox_execution_example.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/sandbox-execution) |
| CLI Compare | [Example](examples/compare/cli_compare_basic.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/compare) |
| Profile/Benchmark | [Example](#profile-benchmark) | [ðŸ“–](https://docs.praison.ai/docs/cli/profile) |
| Auto Mode | [Example](#auto-mode) | [ðŸ“–](https://docs.praison.ai/docs/cli/auto) |
| Init | [Example](#init) | [ðŸ“–](https://docs.praison.ai/docs/cli/init) |
| File Input | [Example](#file-input) | [ðŸ“–](https://docs.praison.ai/docs/cli/file-input) |
| Final Agent | [Example](#final-agent) | [ðŸ“–](https://docs.praison.ai/docs/cli/final-agent) |
| Max Tokens | [Example](#max-tokens) | [ðŸ“–](https://docs.praison.ai/docs/cli/max-tokens) |

</details>

<details>
<summary><strong>ðŸ§ª Evaluation</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Accuracy Evaluation | [Example](examples/eval/accuracy_example.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/eval) |
| Performance Evaluation | [Example](examples/eval/performance_example.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/eval) |
| Reliability Evaluation | [Example](examples/eval/reliability_example.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/eval) |
| Criteria Evaluation | [Example](examples/eval/criteria_example.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/eval) |

</details>

<details>
<summary><strong>ðŸŽ¯ Agent Skills</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Skills Management | [Example](examples/skills/basic_skill_usage.py) | [ðŸ“–](https://docs.praison.ai/features/skills) |
| Custom Skills | [Example](examples/skills/custom_skill_example.py) | [ðŸ“–](https://docs.praison.ai/features/skills) |

</details>

<details>
<summary><strong>â° 24/7 Scheduling</strong></summary>

| Feature | Code | Docs |
|---------|:----:|:----:|
| Agent Scheduler | [Example](examples/python/scheduled_agents/news_checker_live.py) | [ðŸ“–](https://docs.praison.ai/docs/cli/scheduler) |

</details>

---

## ðŸŒ Supported Providers

PraisonAI supports 100+ LLM providers through seamless integration:

<details>
<summary><strong>View all 24 providers</strong></summary>

| Provider | Example |
|----------|:-------:|
| OpenAI | [Example](examples/python/providers/openai/openai_gpt4_example.py) |
| Anthropic | [Example](examples/python/providers/anthropic/anthropic_claude_example.py) |
| Google Gemini | [Example](examples/python/providers/google/google_gemini_example.py) |
| Ollama | [Example](examples/python/providers/ollama/ollama-agents.py) |
| Groq | [Example](examples/python/providers/groq/kimi_with_groq_example.py) |
| DeepSeek | [Example](examples/python/providers/deepseek/deepseek_example.py) |
| xAI Grok | [Example](examples/python/providers/xai/xai_grok_example.py) |
| Mistral | [Example](examples/python/providers/mistral/mistral_example.py) |
| Cohere | [Example](examples/python/providers/cohere/cohere_example.py) |
| Perplexity | [Example](examples/python/providers/perplexity/perplexity_example.py) |
| Fireworks | [Example](examples/python/providers/fireworks/fireworks_example.py) |
| Together AI | [Example](examples/python/providers/together/together_ai_example.py) |
| OpenRouter | [Example](examples/python/providers/openrouter/openrouter_example.py) |
| HuggingFace | [Example](examples/python/providers/huggingface/huggingface_example.py) |
| Azure OpenAI | [Example](examples/python/providers/azure/azure_openai_example.py) |
| AWS Bedrock | [Example](examples/python/providers/aws/aws_bedrock_example.py) |
| Google Vertex | [Example](examples/python/providers/vertex/vertex_example.py) |
| Databricks | [Example](examples/python/providers/databricks/databricks_example.py) |
| Cloudflare | [Example](examples/python/providers/cloudflare/cloudflare_example.py) |
| AI21 | [Example](examples/python/providers/ai21/ai21_example.py) |
| Replicate | [Example](examples/python/providers/replicate/replicate_example.py) |
| SageMaker | [Example](examples/python/providers/sagemaker/sagemaker_example.py) |
| Moonshot | [Example](examples/python/providers/moonshot/moonshot_example.py) |
| vLLM | [Example](examples/python/providers/vllm/vllm_example.py) |

</details>

---

## ðŸ“˜ Using Python Code

### 1. Single Agent

Create app.py file and add the code below:
```python
from praisonaiagents import Agent
agent = Agent(instructions="Your are a helpful AI assistant")
agent.start("Write a movie script about a robot in Mars")
```

Run:
```bash
python app.py
```

### 2. Multi Agents

Create app.py file and add the code below:
```python
from praisonaiagents import Agent, Agents

research_agent = Agent(instructions="Research about AI")
summarise_agent = Agent(instructions="Summarise research agent's findings")
agents = Agents(agents=[research_agent, summarise_agent])
agents.start()
```

Run:
```bash
python app.py
```

### 3. Agent with Planning Mode

Enable planning for any agent - the agent creates a plan, then executes step by step:

```python
from praisonaiagents import Agent

def search_web(query: str) -> str:
    return f"Search results for: {query}"

agent = Agent(
    name="AI Assistant",
    instructions="Research and write about topics",
    planning=True,              # Enable planning mode
    planning_tools=[search_web], # Tools for planning research
    planning_reasoning=True      # Chain-of-thought reasoning
)

result = agent.start("Research AI trends in 2025 and write a summary")
```

**What happens:**
1. ðŸ“‹ Agent creates a multi-step plan
2. ðŸš€ Executes each step sequentially
3. ðŸ“Š Shows progress with context passing
4. âœ… Returns final result

### 4. Deep Research Agent

Automated research with real-time streaming, web search, and citations using OpenAI or Gemini Deep Research APIs.

```python
from praisonaiagents import DeepResearchAgent

# OpenAI Deep Research
agent = DeepResearchAgent(
    model="o4-mini-deep-research",  # or "o3-deep-research"
    verbose=True
)

result = agent.research("What are the latest AI trends in 2025?")
print(result.report)
print(f"Citations: {len(result.citations)}")
```

```python
# Gemini Deep Research
from praisonaiagents import DeepResearchAgent

agent = DeepResearchAgent(
    model="deep-research-pro",  # Auto-detected as Gemini
    verbose=True
)

result = agent.research("Research quantum computing advances")
print(result.report)
```

**Features:**
- ðŸ” Multi-provider support (OpenAI, Gemini, LiteLLM)
- ðŸ“¡ Real-time streaming with reasoning summaries
- ðŸ“š Structured citations with URLs
- ðŸ› ï¸ Built-in tools: web search, code interpreter, MCP, file search
- ðŸ”„ Automatic provider detection from model name

### 5. Query Rewriter Agent

Transform user queries to improve RAG retrieval quality using multiple strategies.

```python
from praisonaiagents import QueryRewriterAgent, RewriteStrategy

agent = QueryRewriterAgent(model="gpt-4o-mini")

# Basic - expands abbreviations, adds context
result = agent.rewrite("AI trends")
print(result.primary_query)  # "What are the current trends in Artificial Intelligence?"

# HyDE - generates hypothetical document for semantic matching
result = agent.rewrite("What is quantum computing?", strategy=RewriteStrategy.HYDE)

# Step-back - generates broader context question
result = agent.rewrite("GPT-4 vs Claude 3?", strategy=RewriteStrategy.STEP_BACK)

# Sub-queries - decomposes complex questions
result = agent.rewrite("RAG setup and best embedding models?", strategy=RewriteStrategy.SUB_QUERIES)

# Contextual - resolves references using chat history
result = agent.rewrite("What about cost?", chat_history=[...])
```

**Strategies:**
- **BASIC**: Expand abbreviations, fix typos, add context
- **HYDE**: Generate hypothetical document for semantic matching
- **STEP_BACK**: Generate higher-level concept questions
- **SUB_QUERIES**: Decompose multi-part questions
- **MULTI_QUERY**: Generate multiple paraphrased versions
- **CONTEXTUAL**: Resolve references using conversation history
- **AUTO**: Automatically detect best strategy

### 6. Agent Memory (Zero Dependencies)

Enable persistent memory for agents - works out of the box without any extra packages.

```python
from praisonaiagents import Agent
from praisonaiagents.memory import FileMemory

# Enable memory with a single parameter
agent = Agent(
    name="Personal Assistant",
    instructions="You are a helpful assistant that remembers user preferences.",
    memory=True,  # Enables file-based memory (no extra deps!)
    user_id="user123"  # Isolate memory per user
)

# Memory is automatically injected into conversations
result = agent.start("My name is John and I prefer Python")
# Agent will remember this for future conversations
```

**Memory Types:**
- **Short-term**: Rolling buffer of recent context (auto-expires)
- **Long-term**: Persistent important facts (sorted by importance)
- **Entity**: People, places, organizations with attributes
- **Episodic**: Date-based interaction history

**Advanced Features:**
```python
from praisonaiagents.memory import FileMemory

memory = FileMemory(user_id="user123")

# Session Save/Resume
memory.save_session("project_session", conversation_history=[...])
memory.resume_session("project_session")

# Context Compression
memory.compress(llm_func=lambda p: agent.chat(p), max_items=10)

# Checkpointing
memory.create_checkpoint("before_refactor", include_files=["main.py"])
memory.restore_checkpoint("before_refactor", restore_files=True)

# Slash Commands
memory.handle_command("/memory show")
memory.handle_command("/memory save my_session")
```

**Storage Options:**
| Option | Dependencies | Description |
|--------|-------------|-------------|
| `memory=True` | None | File-based JSON storage (default) |
| `memory="file"` | None | Explicit file-based storage |
| `memory="sqlite"` | Built-in | SQLite with indexing |
| `memory="chromadb"` | chromadb | Vector/semantic search |

### 7. Rules & Instructions

PraisonAI auto-discovers instruction files from your project root and git root:

| File | Description | Priority |
|------|-------------|----------|
| `PRAISON.md` | PraisonAI native instructions | High |
| `PRAISON.local.md` | Local overrides (gitignored) | Higher |
| `CLAUDE.md` | Claude Code memory file | High |
| `CLAUDE.local.md` | Local overrides (gitignored) | Higher |
| `AGENTS.md` | OpenAI Codex CLI instructions | High |
| `GEMINI.md` | Gemini CLI memory file | High |
| `.cursorrules` | Cursor IDE rules | High |
| `.windsurfrules` | Windsurf IDE rules | High |
| `.claude/rules/*.md` | Claude Code modular rules | Medium |
| `.windsurf/rules/*.md` | Windsurf modular rules | Medium |
| `.cursor/rules/*.mdc` | Cursor modular rules | Medium |
| `.praison/rules/*.md` | Workspace rules | Medium |
| `~/.praison/rules/*.md` | Global rules | Low |

```python
from praisonaiagents import Agent

# Agent auto-discovers CLAUDE.md, AGENTS.md, GEMINI.md, etc.
agent = Agent(name="Assistant", instructions="You are helpful.")
# Rules are injected into system prompt automatically
```

**@Import Syntax:**
```markdown
# CLAUDE.md
See @README for project overview
See @docs/architecture.md for system design
@~/.praison/my-preferences.md
```

**Rule File Format (with YAML frontmatter):**
```markdown
---
description: Python coding guidelines
globs: ["**/*.py"]
activation: always  # always, glob, manual, ai_decision
---

# Guidelines
- Use type hints
- Follow PEP 8
```

### 8. Auto-Generated Memories

```python
from praisonaiagents.memory import FileMemory, AutoMemory

memory = FileMemory(user_id="user123")
auto = AutoMemory(memory, enabled=True)

# Automatically extracts and stores memories from conversations
memories = auto.process_interaction(
    "My name is John and I prefer Python for backend work"
)
# Extracts: name="John", preference="Python for backend"
```

### 9. Agentic Workflows

Create powerful multi-agent workflows with the `Workflow` class:

```python
from praisonaiagents import Agent, Workflow

# Create agents
researcher = Agent(
    name="Researcher",
    role="Research Analyst",
    goal="Research topics thoroughly",
    instructions="Provide concise, factual information."
)

writer = Agent(
    name="Writer",
    role="Content Writer", 
    goal="Write engaging content",
    instructions="Write clear, engaging content based on research."
)

# Create workflow with agents as steps
workflow = Workflow(steps=[researcher, writer])

# Run workflow - agents process sequentially
result = workflow.start("What are the benefits of AI agents?")
print(result["output"])
```

**Key Features:**
- **Agent-first** - Pass `Agent` objects directly as workflow steps
- **Pattern helpers** - Use `route()`, `parallel()`, `loop()`, `repeat()`
- **Planning mode** - Enable with `planning=True`
- **Callbacks** - Monitor with `on_step_complete`, `on_workflow_complete`
- **Async execution** - Use `workflow.astart()` for async

### Workflow Patterns (route, parallel, loop, repeat)

```python
from praisonaiagents import Agent, Workflow
from praisonaiagents.workflows import route, parallel, loop, repeat

# 1. ROUTING - Classifier agent routes to specialized agents
classifier = Agent(name="Classifier", instructions="Respond with 'technical' or 'creative'")
tech_agent = Agent(name="TechExpert", role="Technical Expert")
creative_agent = Agent(name="Creative", role="Creative Writer")

workflow = Workflow(steps=[
    classifier,
    route({
        "technical": [tech_agent],
        "creative": [creative_agent]
    })
])

# 2. PARALLEL - Multiple agents work concurrently
market_agent = Agent(name="Market", role="Market Researcher")
competitor_agent = Agent(name="Competitor", role="Competitor Analyst")
aggregator = Agent(name="Aggregator", role="Synthesizer")

workflow = Workflow(steps=[
    parallel([market_agent, competitor_agent]),
    aggregator
])

# 3. LOOP - Agent processes each item
processor = Agent(name="Processor", role="Item Processor")
summarizer = Agent(name="Summarizer", role="Summarizer")

workflow = Workflow(
    steps=[loop(processor, over="items"), summarizer],
    variables={"items": ["AI", "ML", "NLP"]}
)

# 4. REPEAT - Evaluator-Optimizer pattern
generator = Agent(name="Generator", role="Content Generator")
evaluator = Agent(name="Evaluator", instructions="Say 'APPROVED' if good")

workflow = Workflow(steps=[
    generator,
    repeat(evaluator, until=lambda ctx: "approved" in ctx.previous_result.lower(), max_iterations=3)
])

# 5. CALLBACKS
workflow = Workflow(
    steps=[researcher, writer],
    on_step_complete=lambda name, r: print(f"âœ… {name} done")
)

# 6. WITH PLANNING & REASONING
workflow = Workflow(
    steps=[researcher, writer],
    planning=True,
    reasoning=True
)

# 7. ASYNC EXECUTION
result = asyncio.run(workflow.astart("input"))

# 8. STATUS TRACKING
workflow.status  # "not_started" | "running" | "completed"
workflow.step_statuses  # {"step1": "completed", "step2": "skipped"}
```

### YAML Workflow Template

```yaml
# .praison/workflows/research.yaml
name: Research Workflow
description: Research and write content with all patterns

agents:
  researcher:
    role: Research Expert
    goal: Find accurate information
    tools: [tavily_search, web_scraper]
  writer:
    role: Content Writer
    goal: Write engaging content
  editor:
    role: Editor
    goal: Polish content

steps:
  # Sequential
  - agent: researcher
    action: Research {{topic}}
    output_variable: research_data

  # Routing
  - name: classifier
    action: Classify content type
    route:
      technical: [tech_handler]
      creative: [creative_handler]
      default: [general_handler]

  # Parallel
  - name: parallel_research
    parallel:
      - agent: researcher
        action: Research market
      - agent: researcher
        action: Research competitors

  # Loop
  - agent: writer
    action: Write about {{item}}
    loop_over: topics
    loop_var: item

  # Repeat (evaluator-optimizer)
  - agent: editor
    action: Review and improve
    repeat:
      until: "quality > 8"
      max_iterations: 3

  # Output to file
  - agent: writer
    action: Write final report
    output_file: output/{{topic}}_report.md

variables:
  topic: AI trends
  topics: [ML, NLP, Vision]

workflow:
  planning: true
  planning_llm: gpt-4o
  memory_config:
    provider: chroma
    persist: true
```

### Loading YAML Workflows

```python
from praisonaiagents.workflows import YAMLWorkflowParser, WorkflowManager

# Option 1: Parse YAML string
parser = YAMLWorkflowParser()
workflow = parser.parse_string(yaml_content)
result = workflow.start("Research AI trends")

# Option 2: Load from file with WorkflowManager
manager = WorkflowManager()
workflow = manager.load_yaml("research_workflow.yaml")
result = workflow.start("Research AI trends")

# Option 3: Execute YAML directly
result = manager.execute_yaml(
    "research_workflow.yaml",
    input_data="Research AI trends",
    variables={"topic": "Machine Learning"}
)
```

### Complete workflow.yaml Reference

```yaml
# workflow.yaml - Full feature reference
name: Complete Workflow
description: Demonstrates all workflow.yaml features
framework: praisonai  # praisonai, crewai, autogen
process: workflow     # sequential, hierarchical, workflow

workflow:
  planning: true
  planning_llm: gpt-4o
  reasoning: true
  verbose: true
  memory_config:
    provider: chroma
    persist: true

variables:
  topic: AI trends
  items: [ML, NLP, Vision]

agents:
  researcher:
    name: Researcher
    role: Research Analyst
    goal: Research topics thoroughly
    instructions: "Provide detailed research findings"
    backstory: "Expert researcher with 10 years experience"  # alias for instructions
    llm: gpt-4o-mini
    function_calling_llm: gpt-4o      # For tool calls
    max_rpm: 10                        # Rate limiting
    max_execution_time: 300            # Timeout in seconds
    reflect_llm: gpt-4o               # For self-reflection
    min_reflect: 1
    max_reflect: 3
    system_template: "You are a helpful assistant"
    tools:
      - tavily_search

  writer:
    name: Writer
    role: Content Writer
    goal: Write clear content
    instructions: "Write engaging content"

steps:
  - name: research_step
    agent: researcher
    action: "Research {{topic}}"
    expected_output: "Comprehensive research report"
    output_file: "output/research.md"
    create_directory: true
    
  - name: writing_step
    agent: writer
    action: "Write article based on research"
    context:                          # Task dependencies
      - research_step
    output_json:                      # Structured output
      type: object
      properties:
        title: { type: string }
        content: { type: string }

callbacks:
  on_workflow_start: log_start
  on_step_complete: log_step
  on_workflow_complete: log_complete
```

### 10. Hooks

Intercept and modify agent behavior at various lifecycle points:

```python
from praisonaiagents.hooks import (
    HookRegistry, HookRunner, HookEvent, HookResult,
    BeforeToolInput
)

# Create a hook registry
registry = HookRegistry()

# Log all tool calls
@registry.on(HookEvent.BEFORE_TOOL)
def log_tools(event_data: BeforeToolInput) -> HookResult:
    print(f"Tool: {event_data.tool_name}")
    return HookResult.allow()

# Block dangerous operations
@registry.on(HookEvent.BEFORE_TOOL)
def security_check(event_data: BeforeToolInput) -> HookResult:
    if "delete" in event_data.tool_name.lower():
        return HookResult.deny("Delete operations blocked")
    return HookResult.allow()

# Execute hooks
runner = HookRunner(registry)
```

**CLI Commands:**
```bash
praisonai hooks list                    # List registered hooks
praisonai hooks test before_tool        # Test hooks for an event
praisonai hooks run "echo test"         # Run a command hook
praisonai hooks validate hooks.json     # Validate configuration
```


### 11. Shadow Git Checkpoints

File-level undo/restore using shadow git:

```python
from praisonaiagents.checkpoints import CheckpointService

service = CheckpointService(workspace_dir="./my_project")
await service.initialize()

# Save checkpoint before changes
result = await service.save("Before refactoring")

# Make changes...

# Restore if needed
await service.restore(result.checkpoint.id)

# View diff
diff = await service.diff()
```

**CLI Commands:**
```bash
praisonai checkpoint save "Before changes"  # Save checkpoint
praisonai checkpoint list                   # List checkpoints
praisonai checkpoint diff                   # Show changes
praisonai checkpoint restore abc123         # Restore to checkpoint
```

**Links:**
- [ðŸ“– Coding Docs](https://docs.praison.ai/features/checkpoints)
- [ðŸ“– CLI Docs](https://docs.praison.ai/docs/cli/checkpoint)
- [ðŸ’» Example](examples/checkpoints/basic_checkpoints.py)

---

### 12. Background Tasks

Run agent tasks asynchronously without blocking:

```python
import asyncio
from praisonaiagents.background import BackgroundRunner, BackgroundConfig

async def main():
    config = BackgroundConfig(max_concurrent_tasks=3)
    runner = BackgroundRunner(config=config)
    
    async def my_task(name: str) -> str:
        await asyncio.sleep(2)
        return f"Task {name} completed"
    
    task = await runner.submit(my_task, args=("example",), name="my_task")
    await task.wait(timeout=10.0)
    print(task.result)

asyncio.run(main())
```

**CLI Commands:**
```bash
praisonai background list          # List running tasks
praisonai background status <id>   # Check task status
praisonai background cancel <id>   # Cancel a task
praisonai background clear         # Clear completed tasks
```

**Links:**
- [ðŸ“– Coding Docs](https://docs.praison.ai/features/background-tasks)
- [ðŸ“– CLI Docs](https://docs.praison.ai/docs/cli/background)
- [ðŸ’» Example](examples/background/basic_background.py)

---

### 13. Policy Engine

Control what agents can and cannot do with policy-based execution:

```python
from praisonaiagents.policy import (
    PolicyEngine, Policy, PolicyRule, PolicyAction
)

engine = PolicyEngine()

policy = Policy(
    name="no_delete",
    rules=[
        PolicyRule(
            action=PolicyAction.DENY,
            resource="tool:delete_*",
            reason="Delete operations blocked"
        )
    ]
)
engine.add_policy(policy)

result = engine.check("tool:delete_file", {})
print(f"Allowed: {result.allowed}")
```

**CLI Commands:**
```bash
praisonai policy list                  # List policies
praisonai policy check "tool:name"     # Check if allowed
praisonai policy init                  # Create template
```

**Links:**
- [ðŸ“– Coding Docs](https://docs.praison.ai/features/policy-engine)
- [ðŸ“– CLI Docs](https://docs.praison.ai/docs/cli/policy)
- [ðŸ’» Example](examples/policy/basic_policy.py)

---

### 14. Thinking Budgets

Configure token budgets for extended thinking:

```python
from praisonaiagents.thinking import ThinkingBudget, ThinkingTracker

# Use predefined levels
budget = ThinkingBudget.high()  # 16,000 tokens

# Track usage
tracker = ThinkingTracker()
session = tracker.start_session(budget_tokens=16000)
tracker.end_session(session, tokens_used=12000)

summary = tracker.get_summary()
print(f"Utilization: {summary['average_utilization']:.1%}")
```

**CLI Commands:**
```bash
praisonai thinking status      # Show current budget
praisonai thinking set high    # Set budget level
praisonai thinking stats       # Show usage statistics
```

**Links:**
- [ðŸ“– Coding Docs](https://docs.praison.ai/features/thinking-budgets)
- [ðŸ“– CLI Docs](https://docs.praison.ai/docs/cli/thinking)
- [ðŸ’» Example](examples/thinking/basic_thinking.py)

---

### 15. Output Styles

Configure how agents format their responses:

```python
from praisonaiagents.output import OutputStyle, OutputFormatter

# Use preset styles
style = OutputStyle.concise()
formatter = OutputFormatter(style)

# Format output
text = "# Hello\n\nThis is **bold** text."
plain = formatter.format(text)
print(plain)
```

**CLI Commands:**
```bash
praisonai output status        # Show current style
praisonai output set concise   # Set output style
```

**Links:**
- [ðŸ“– Coding Docs](https://docs.praison.ai/features/output-styles)
- [ðŸ“– CLI Docs](https://docs.praison.ai/docs/cli/output-style)
- [ðŸ’» Example](examples/output/basic_output.py)

---

### 16. Context Compaction

Automatically manage context window size:

```python
from praisonaiagents.compaction import (
    ContextCompactor, CompactionStrategy
)

compactor = ContextCompactor(
    max_tokens=4000,
    strategy=CompactionStrategy.SLIDING,
    preserve_recent=3
)

messages = [...]  # Your conversation history
compacted, result = compactor.compact(messages)

print(f"Compression: {result.compression_ratio:.1%}")
```

**CLI Commands:**
```bash
praisonai compaction status        # Show settings
praisonai compaction set sliding   # Set strategy
praisonai compaction stats         # Show statistics
```

**Links:**
- [ðŸ“– Coding Docs](https://docs.praison.ai/features/context-compaction)
- [ðŸ“– CLI Docs](https://docs.praison.ai/docs/cli/compaction)
- [ðŸ’» Example](examples/compaction/basic_compaction.py)

---

### 17. Field Names Reference (A-I-G-S)

PraisonAI accepts both old (agents.yaml) and new (workflow.yaml) field names. Use the **canonical names** for new projects:

| Canonical (Recommended) | Alias (Also Works) | Purpose |
|-------------------------|-------------------|---------|
| `agents` | `roles` | Define agent personas |
| `instructions` | `backstory` | Agent behavior/persona |
| `action` | `description` | What the step does |
| `steps` | `tasks` (nested) | Define work items |
| `name` | `topic` | Workflow identifier |

**A-I-G-S Mnemonic** - Easy to remember:
- **A**gents - Who does the work
- **I**nstructions - How they behave  
- **G**oal - What they achieve
- **S**teps - What they do

```yaml
# Quick Reference - Canonical Format
name: My Workflow              # Workflow name (not 'topic')
agents:                        # Define agents (not 'roles')
  my_agent:
    role: Job Title            # Agent's role
    goal: What to achieve      # Agent's goal
    instructions: How to act   # Agent's behavior (not 'backstory')
    
steps:                         # Define steps (not 'tasks')
  - agent: my_agent
    action: What to do         # Step action (not 'description')
```

> **Note:** The parser accepts both old and new names. Run `praisonai workflow validate <file.yaml>` to see suggestions for canonical names.

### 18. Extended agents.yaml with Workflow Patterns

**Feature Parity:** Both `agents.yaml` and `workflow.yaml` now support the same features:
- All workflow patterns (route, parallel, loop, repeat)
- All agent fields (function_calling_llm, max_rpm, max_execution_time, reflect_llm, templates)
- All step fields (expected_output, context, output_json, create_directory, callback)
- Framework support (praisonai, crewai, autogen)
- Process types (sequential, hierarchical, workflow)

You can use advanced workflow patterns directly in agents.yaml by setting `process: workflow`:

```yaml
# agents.yaml with workflow patterns
framework: praisonai
process: workflow  # Enables workflow mode
topic: "Research AI trends"

workflow:
  planning: true
  reasoning: true
  verbose: true

variables:
  topic: AI trends

agents:  # Canonical: use 'agents' instead of 'roles'
  classifier:
    role: Request Classifier
    instructions: "Classify requests into categories"  # Canonical: use 'instructions' instead of 'backstory'
    goal: Classify requests
    
  researcher:
    role: Research Analyst
    instructions: "Expert researcher"  # Canonical: use 'instructions' instead of 'backstory'
    goal: Research topics
    tools:
      - tavily_search

steps:
  # Sequential step
  - agent: classifier
    action: "Classify: {{topic}}"
    
  # Route pattern - decision-based branching
  - name: routing
    route:
      technical: [tech_expert]
      default: [researcher]
      
  # Parallel pattern - concurrent execution
  - name: parallel_research
    parallel:
      - agent: researcher
        action: "Research market trends"
      - agent: researcher
        action: "Research competitors"
        
  # Loop pattern - iterate over items
  - agent: researcher
    action: "Analyze {{item}}"
    loop:
      over: topics
      
  # Repeat pattern - evaluator-optimizer
  - agent: aggregator
    action: "Synthesize findings"
    repeat:
      until: "comprehensive"
      max_iterations: 3
```

Run with the same simple command:
```bash
praisonai agents.yaml
```

### 19. MCP (Model Context Protocol)

PraisonAI supports MCP Protocol Revision 2025-11-25 with multiple transports.

#### MCP Client (Consume MCP Servers)
```python
from praisonaiagents import Agent, MCP

# stdio - Local NPX/Python servers
agent = Agent(tools=MCP("npx @modelcontextprotocol/server-memory"))

# Streamable HTTP - Production servers
agent = Agent(tools=MCP("https://api.example.com/mcp"))

# WebSocket - Real-time bidirectional
agent = Agent(tools=MCP("wss://api.example.com/mcp", auth_token="token"))

# SSE (Legacy) - Backward compatibility
agent = Agent(tools=MCP("http://localhost:8080/sse"))

# With environment variables
agent = Agent(
    tools=MCP(
        command="npx",
        args=["-y", "@modelcontextprotocol/server-brave-search"],
        env={"BRAVE_API_KEY": "your-key"}
    )
)

# Multiple MCP servers + regular functions
def my_custom_tool(query: str) -> str:
    """Custom tool function."""
    return f"Result: {query}"

agent = Agent(
    name="MultiToolAgent",
    instructions="Agent with multiple MCP servers",
    tools=[
        MCP("uvx mcp-server-time"),                    # Time tools
        MCP("npx @modelcontextprotocol/server-memory"), # Memory tools
        my_custom_tool                                  # Regular function
    ]
)
```

#### MCP Server (Expose Tools as MCP Server)

Expose your Python functions as MCP tools for Claude Desktop, Cursor, and other MCP clients:

```python
from praisonaiagents.mcp import ToolsMCPServer

def search_web(query: str, max_results: int = 5) -> dict:
    """Search the web for information."""
    return {"results": [f"Result for {query}"]}

def calculate(expression: str) -> dict:
    """Evaluate a mathematical expression."""
    return {"result": eval(expression)}

# Create and run MCP server
server = ToolsMCPServer(name="my-tools")
server.register_tools([search_web, calculate])
server.run()  # stdio for Claude Desktop
# server.run_sse(host="0.0.0.0", port=8080)  # SSE for web clients
```

#### MCP Features
| Feature | Description |
|---------|-------------|
| Session Management | Automatic Mcp-Session-Id handling |
| Protocol Versioning | Mcp-Protocol-Version header |
| Resumability | SSE stream recovery via Last-Event-ID |
| Security | Origin validation, DNS rebinding prevention |
| WebSocket | Auto-reconnect with exponential backoff |

### 20. A2A (Agent2Agent Protocol)

PraisonAI supports the [A2A Protocol](https://a2a-protocol.org) for agent-to-agent communication, enabling your agents to be discovered and collaborate with other AI agents.

#### A2A Server (Expose Agent as A2A Server)
```python
from praisonaiagents import Agent, A2A
from fastapi import FastAPI

# Create an agent with tools
def search_web(query: str) -> str:
    """Search the web for information."""
    return f"Results for: {query}"

agent = Agent(
    name="Research Assistant",
    role="Research Analyst",
    goal="Help users research topics",
    tools=[search_web]
)

# Expose as A2A Server
a2a = A2A(agent=agent, url="http://localhost:8000/a2a")

app = FastAPI()
app.include_router(a2a.get_router())

# Run: uvicorn app:app --reload
# Agent Card: GET /.well-known/agent.json
# Status: GET /status
```

#### A2A Features
| Feature | Description |
|---------|-------------|
| Agent Card | JSON metadata for agent discovery |
| Skills Extraction | Auto-generate skills from tools |
| Task Management | Stateful task lifecycle |
| Streaming | SSE streaming for real-time updates |

> **Documentation**: [docs.praison.ai/a2a](https://docs.praison.ai/a2a) | **Examples**: [examples/python/a2a](https://github.com/MervinPraison/PraisonAI/tree/main/examples/python/a2a)

---

## ðŸŽ¯ CLI / No-Code Interface

PraisonAI provides a powerful CLI for no-code automation and quick prototyping.

### CLI Quick Reference

| Category | Commands |
|----------|----------|
| **Execution** | `praisonai`, `--auto`, `--interactive`, `--chat` |
| **Research** | `research`, `--query-rewrite`, `--deep-research` |
| **Planning** | `--planning`, `--planning-tools`, `--planning-reasoning` |
| **Workflows** | `workflow run`, `workflow list`, `workflow auto` |
| **Memory** | `memory show`, `memory add`, `memory search`, `memory clear` |
| **Knowledge** | `knowledge add`, `knowledge query`, `knowledge list` |
| **Sessions** | `session list`, `session resume`, `session delete` |
| **Tools** | `tools list`, `tools info`, `tools search` |
| **MCP** | `mcp list`, `mcp create`, `mcp enable` |
| **Development** | `commit`, `docs`, `checkpoint`, `hooks` |
| **Scheduling** | `schedule start`, `schedule list`, `schedule stop` |

### Auto Mode
```bash
pip install praisonai
export OPENAI_API_KEY=xxxxxxxxxxxxxxxxxxxxxx
praisonai --auto create a movie script about Robots in Mars
```

### Interactive Mode CLI:
```bash
# Start interactive terminal mode (inspired by Gemini CLI, Codex CLI, Claude Code)
praisonai --interactive
praisonai -i

# Features:
# - Streaming responses (no boxes)
# - Built-in tools: read_file, write_file, list_files, execute_command, internet_search
# - Slash commands: /help, /exit, /tools, /clear

# Chat mode - single prompt with interactive style (for testing/scripting)
# Use --chat (or --chat-mode for backward compatibility)
praisonai "list files in current folder" --chat
praisonai "search the web for AI news" --chat
praisonai "read README.md" --chat
```

### Chat UI (Web Interface):
```bash
# Start web-based Chainlit chat interface (requires praisonai[chat])
pip install "praisonai[chat]"
praisonai chat
# Opens browser at http://localhost:8084
```

### Query Rewriting (works with any command):
```bash
# Rewrite query for better results (uses QueryRewriterAgent)
praisonai "AI trends" --query-rewrite

# Rewrite with search tools (agent decides when to search)
praisonai "latest developments" --query-rewrite --rewrite-tools "internet_search"

# Works with any prompt
praisonai "explain quantum computing" --query-rewrite -v
```

### Deep Research CLI:
```bash
# Default: OpenAI (o4-mini-deep-research)
praisonai research "What are the latest AI trends in 2025?"

# Use Gemini
praisonai research --model deep-research-pro "Your research query"

# Rewrite query before research
praisonai research --query-rewrite "AI trends"

# Rewrite with search tools
praisonai research --query-rewrite --rewrite-tools "internet_search" "AI trends"

# Use custom tools from file (gathers context before deep research)
praisonai research --tools tools.py "Your research query"
praisonai research -t my_tools.py "Your research query"

# Use built-in tools by name (comma-separated)
praisonai research --tools "internet_search,wiki_search" "Your query"
praisonai research -t "yfinance,calculator_tools" "Stock analysis query"

# Save output to file (output/research/{query}.md)
praisonai research --save "Your research query"
praisonai research -s "Your research query"

# Combine options
praisonai research --query-rewrite --tools tools.py --save "Your research query"

# Verbose mode (show debug logs)
praisonai research -v "Your research query"
```

### Planning Mode CLI:
```bash
# Enable planning mode - agent creates a plan before execution
praisonai "Research AI trends and write a summary" --planning

# Planning with tools for research
praisonai "Analyze market trends" --planning --planning-tools tools.py

# Planning with chain-of-thought reasoning
praisonai "Complex analysis task" --planning --planning-reasoning

# Auto-approve plans without confirmation
praisonai "Task" --planning --auto-approve-plan
```

### Tool Approval CLI:
```bash
# Auto-approve ALL tool executions (use with caution!)
praisonai "run ls command" --trust

# Auto-approve tools up to a risk level (prompt for higher)
# Levels: low, medium, high, critical
praisonai "write to file" --approve-level high  # Prompts for critical tools only
praisonai "task" --approve-level medium         # Prompts for high and critical

# Default behavior (no flags): prompts for all dangerous tools
praisonai "run shell command"  # Will prompt for approval
```

### Memory CLI:
```bash
# Enable memory for agent (persists across sessions)
praisonai "My name is John" --memory

# Memory with user isolation
praisonai "Remember my preferences" --memory --user-id user123

# Memory management commands
praisonai memory show                      # Show memory statistics
praisonai memory add "User prefers Python" # Add to long-term memory
praisonai memory search "Python"           # Search memories
praisonai memory clear                     # Clear short-term memory
praisonai memory clear all                 # Clear all memory
praisonai memory save my_session           # Save session
praisonai memory resume my_session         # Resume session
praisonai memory sessions                  # List saved sessions
praisonai memory checkpoint                # Create checkpoint
praisonai memory restore <checkpoint_id>   # Restore checkpoint
praisonai memory checkpoints               # List checkpoints
praisonai memory help                      # Show all commands
```

### Rules CLI:
```bash
# List all loaded rules (from PRAISON.md, CLAUDE.md, etc.)
praisonai rules list

# Show specific rule details
praisonai rules show <rule_name>

# Create a new rule
praisonai rules create my_rule "Always use type hints"

# Delete a rule
praisonai rules delete my_rule

# Show rules statistics
praisonai rules stats

# Include manual rules with prompts
praisonai "Task" --include-rules security,testing
```

### Workflow CLI:
```bash
# List available workflows
praisonai workflow list

# Execute a workflow with tools and save output
praisonai workflow run "Research Blog" --tools tavily --save

# Execute with variables
praisonai workflow run deploy --workflow-var environment=staging --workflow-var branch=main

# Execute with planning mode (AI creates sub-steps for each workflow step)
praisonai workflow run "Research Blog" --planning --verbose

# Execute with reasoning mode (chain-of-thought)
praisonai workflow run "Analysis" --reasoning --verbose

# Execute with memory enabled
praisonai workflow run "Research" --memory

# Show workflow details
praisonai workflow show deploy

# Create a new workflow template
praisonai workflow create my_workflow

# Inline workflow (no template file needed)
praisonai "What is AI?" --workflow "Research,Summarize" --save

# Inline workflow with step actions
praisonai "GPT-5" --workflow "Research:Search for info,Write:Write blog" --tools tavily

# Workflow CLI help
praisonai workflow help
```

#### YAML Workflow Files:
```bash
# Run a YAML workflow file
praisonai workflow run research.yaml

# Run with variables
praisonai workflow run research.yaml --var topic="AI trends"

# Validate a YAML workflow
praisonai workflow validate research.yaml

# Create from template (simple, routing, parallel, loop, evaluator-optimizer)
praisonai workflow template routing --output my_workflow.yaml
```

#### Auto-Generate Workflows:
```bash
# Auto-generate a sequential workflow from topic
praisonai workflow auto "Research AI trends"

# Generate parallel workflow (multiple agents work concurrently)
praisonai workflow auto "Research AI trends" --pattern parallel

# Generate routing workflow (classifier routes to specialists)
praisonai workflow auto "Build a chatbot" --pattern routing

# Generate orchestrator-workers workflow (central orchestrator delegates)
praisonai workflow auto "Comprehensive market analysis" --pattern orchestrator-workers

# Generate evaluator-optimizer workflow (iterative refinement)
praisonai workflow auto "Write and refine article" --pattern evaluator-optimizer

# Specify output file
praisonai workflow auto "Build a chatbot" --pattern routing

# Specify output file
praisonai workflow auto "Research AI" --pattern sequential --output my_workflow.yaml
```

**Workflow CLI Options:**
| Flag | Description |
|------|-------------|
| `--workflow-var key=value` | Set workflow variable (can be repeated) |
| `--var key=value` | Set variable for YAML workflows |
| `--pattern <pattern>` | Pattern for auto-generation (sequential, parallel, routing, loop, orchestrator-workers, evaluator-optimizer) |
| `--output <file>` | Output file for auto-generation |
| `--llm <model>` | LLM model (e.g., openai/gpt-4o-mini) |
| `--tools <tools>` | Tools (comma-separated, e.g., tavily) |
| `--planning` | Enable planning mode |
| `--reasoning` | Enable reasoning mode |
| `--memory` | Enable memory |
| `--verbose` | Enable verbose output |
| `--save` | Save output to file |

### Hooks CLI:
```bash
# List configured hooks
praisonai hooks list

# Show hooks statistics
praisonai hooks stats

# Create hooks.json template
praisonai hooks init
```

### Claude Memory Tool CLI:
```bash
# Enable Claude Memory Tool (Anthropic models only)
praisonai "Research and remember findings" --claude-memory --llm anthropic/claude-sonnet-4-20250514
```

### Guardrail CLI:
```bash
# Validate output with LLM guardrail
praisonai "Write code" --guardrail "Ensure code is secure and follows best practices"

# Combine with other flags
praisonai "Generate SQL query" --guardrail "No DROP or DELETE statements" --save
```

### Metrics CLI:
```bash
# Display token usage and cost metrics
praisonai "Analyze this data" --metrics

# Combine with other features
praisonai "Complex task" --metrics --planning
```

### Scheduler CLI:

```bash
praisonai schedule start <name> "task" --interval hourly
praisonai schedule list
praisonai schedule logs <name> [--follow]
praisonai schedule stop <name>
praisonai schedule restart <name>
praisonai schedule delete <name>
praisonai schedule describe <name>
praisonai schedule save <name> [file.yaml]
praisonai schedule "task" --interval hourly  # foreground mode
praisonai schedule agents.yaml  # foreground mode
```

### Image Processing CLI:
```bash
# Process images with vision-based tasks
praisonai "Describe this image" --image path/to/image.png

# Analyze image content
praisonai "What objects are in this photo?" --image photo.jpg --llm openai/gpt-4o
```

### Telemetry CLI:
```bash
# Enable usage monitoring and analytics
praisonai "Task" --telemetry

# Combine with metrics for full observability
praisonai "Complex analysis" --telemetry --metrics
```

### MCP (Model Context Protocol) CLI:
```bash
# Use MCP server tools
praisonai "Search files" --mcp "npx -y @modelcontextprotocol/server-filesystem ."

# MCP with environment variables
praisonai "Search web" --mcp "npx -y @modelcontextprotocol/server-brave-search" --mcp-env "BRAVE_API_KEY=your_key"

# Multiple MCP options
praisonai "Task" --mcp "npx server" --mcp-env "KEY1=value1,KEY2=value2"
```

### Fast Context CLI:
```bash
# Search codebase for relevant context
praisonai "Find authentication code" --fast-context ./src

# Add code context to any task
praisonai "Explain this function" --fast-context /path/to/project
```

### Knowledge CLI:
```bash
# Add documents to knowledge base
praisonai knowledge add document.pdf
praisonai knowledge add ./docs/

# Search knowledge base
praisonai knowledge search "API authentication"

# List indexed documents
praisonai knowledge list

# Clear knowledge base
praisonai knowledge clear

# Show knowledge base info
praisonai knowledge info

# Show all commands
praisonai knowledge help
```

### Session CLI:
```bash
# List all saved sessions
praisonai session list

# Show session details
praisonai session show my-project

# Resume a session (load into memory)
praisonai session resume my-project

# Delete a session
praisonai session delete my-project

# Auto-save session after each run
praisonai "Analyze this code" --auto-save my-project

# Load history from last N sessions into context
praisonai "Continue our discussion" --history 5
```

### Session Management (Python):
```python
from praisonaiagents import Agent

# Auto-save session after each run
agent = Agent(
    name="Assistant",
    memory=True,
    auto_save="my-project"
)

# Load history from past sessions via context management
agent = Agent(
    name="Assistant",
    memory=True,
    context=True,  # Enable context management for history
)
```

### Workflow Checkpoints:
```python
from praisonaiagents.memory.workflows import WorkflowManager

manager = WorkflowManager()

# Save checkpoint after each step
result = manager.execute("deploy", checkpoint="deploy-v1")

# Resume from checkpoint
result = manager.execute("deploy", resume="deploy-v1")

# List/delete checkpoints
manager.list_checkpoints()
manager.delete_checkpoint("deploy-v1")
```

### Tools CLI:
```bash
praisonai tools list
praisonai tools info internet_search
praisonai tools search "web"
praisonai tools doctor
praisonai tools resolve shell_tool
praisonai tools discover
praisonai tools show-sources
praisonai tools show-sources --template ai-video-editor
```

| Command | Example | Docs |
|---------|---------|------|
| `tools list` | [example](examples/tools/) | [docs](https://docs.praison.ai/docs/cli/tools) |
| `tools resolve` | [example](examples/tools/example_tools_resolve.py) | [docs](https://docs.praison.ai/docs/cli/tools-resolve) |
| `tools discover` | [example](examples/tools/example_tools_discover.py) | [docs](https://docs.praison.ai/docs/cli/tools-discover) |
| `tools show-sources` | [example](examples/tools/example_tools_sources.py) | [docs](https://docs.praison.ai/docs/cli/tools-show-sources) |

### Handoff CLI:
```bash
# Enable agent-to-agent task delegation
praisonai "Research and write article" --handoff "researcher,writer,editor"

# Complex multi-agent workflow
praisonai "Analyze data and create report" --handoff "analyst,visualizer,writer"
```

### Auto Memory CLI:
```bash
# Enable automatic memory extraction
praisonai "Learn about user preferences" --auto-memory

# Combine with user isolation
praisonai "Remember my settings" --auto-memory --user-id user123
```

### Todo CLI:
```bash
# Generate todo list from task
praisonai "Plan the project" --todo

# Add a todo item
praisonai todo add "Implement feature X"

# List all todos
praisonai todo list

# Complete a todo
praisonai todo complete 1

# Delete a todo
praisonai todo delete 1

# Clear all todos
praisonai todo clear

# Show all commands
praisonai todo help
```

### Router CLI:
```bash
# Auto-select best model based on task complexity
praisonai "Simple question" --router

# Specify preferred provider
praisonai "Complex analysis" --router --router-provider anthropic

# Router automatically selects:
# - Simple tasks â†’ gpt-4o-mini, claude-3-haiku
# - Complex tasks â†’ gpt-4-turbo, claude-3-opus

# Create workflow with model routing template
praisonai workflow create --template model-routing --output my_workflow.yaml
```

Custom models can be configured in `agents.yaml`. See [Model Router Docs](https://docs.praison.ai/features/model-router) for details.

### Flow Display CLI:
```bash
# Enable visual workflow tracking
praisonai agents.yaml --flow-display

# Combine with other features
praisonai "Multi-step task" --planning --flow-display
```

### Docs CLI:
```bash
# List all project docs
praisonai docs list

# Create a new doc
praisonai docs create project-overview "This project is a Python web app..."

# Show a specific doc
praisonai docs show project-overview

# Delete a doc
praisonai docs delete old-doc

# Show all commands
praisonai docs help
```

### MCP Config CLI:
```bash
# List all MCP configurations
praisonai mcp list

# Create a new MCP config
praisonai mcp create filesystem npx -y @modelcontextprotocol/server-filesystem .

# Show a specific config
praisonai mcp show filesystem

# Enable/disable a config
praisonai mcp enable filesystem
praisonai mcp disable filesystem

# Delete a config
praisonai mcp delete filesystem

# Show all commands
praisonai mcp help
```

### AI Commit CLI:
```bash
# Full auto mode: stage all, security check, commit, and push
praisonai commit -a

# Interactive mode (requires git add first)
praisonai commit

# Interactive with auto-push
praisonai commit --push

# Skip security check (not recommended)
praisonai commit -a --no-verify
```

**Features:**
- ðŸ¤– AI-generated conventional commit messages
- ðŸ”’ Built-in security scanning (API keys, passwords, secrets, sensitive files)
- ðŸ“¦ Auto-staging with `-a` flag
- ðŸš€ Auto-push in full auto mode
- âœï¸ Edit message before commit in interactive mode

**Security Detection:**
- API keys, secrets, tokens (AWS, GitHub, GitLab, Slack)
- Passwords and private keys
- Sensitive files (`.env`, `id_rsa`, `.pem`, `.key`, etc.)

### Serve CLI (API Server):
```bash
# Start API server for agents defined in YAML
praisonai serve agents.yaml

# With custom port and host
praisonai serve agents.yaml --port 8005 --host 0.0.0.0

# Alternative flag style
praisonai agents.yaml --serve

# The server provides:
# POST /agents          - Run all agents sequentially
# POST /agents/{name}   - Run specific agent (e.g., /agents/researcher)
# GET  /agents/list     - List available agents
```

### n8n Integration CLI:
```bash
# Export workflow to n8n and open in browser
praisonai agents.yaml --n8n

# With custom n8n URL
praisonai agents.yaml --n8n --n8n-url http://localhost:5678

# Set N8N_API_KEY for auto-import
export N8N_API_KEY="your-api-key"
praisonai agents.yaml --n8n
```

### External Agents CLI:

Use external AI coding CLI tools (Claude Code, Gemini CLI, Codex CLI, Cursor CLI) as agent tools:

```bash
# Use Claude Code for coding tasks
praisonai "Refactor the auth module" --external-agent claude

# Use Gemini CLI for code analysis
praisonai "Analyze codebase architecture" --external-agent gemini

# Use OpenAI Codex CLI
praisonai "Fix all bugs in src/" --external-agent codex

# Use Cursor CLI
praisonai "Add comprehensive tests" --external-agent cursor
```

**Python API:**
```python
from praisonai.integrations import (
    ClaudeCodeIntegration,
    GeminiCLIIntegration,
    CodexCLIIntegration,
    CursorCLIIntegration
)

# Create integration
claude = ClaudeCodeIntegration(workspace="/project")

# Execute a coding task
result = await claude.execute("Refactor the auth module")

# Use as agent tool
from praisonai import Agent
tool = claude.as_tool()
agent = Agent(tools=[tool])
```

**Environment Variables:**
```bash
export ANTHROPIC_API_KEY=your-key  # Claude Code
export GEMINI_API_KEY=your-key     # Gemini CLI
export OPENAI_API_KEY=your-key     # Codex CLI
export CURSOR_API_KEY=your-key     # Cursor CLI
```

See [External Agents Documentation](https://docs.praison.ai/code/external-agents) for more details.

### @Mentions in Prompts:
```bash
# Include file content in prompt
praisonai "@file:src/main.py explain this code"

# Include project doc
praisonai "@doc:project-overview help me add a feature"

# Search the web
praisonai "@web:python best practices give me tips"

# Fetch URL content
praisonai "@url:https://docs.python.org summarize this"

# Combine multiple mentions
praisonai "@file:main.py @doc:coding-standards review this code"
```

## Prompt Expansion

Expand short prompts into detailed, actionable prompts:

### CLI Usage
```bash
# Expand a short prompt into detailed prompt
praisonai "write a movie script in 3 lines" --expand-prompt

# With verbose output
praisonai "blog about AI" --expand-prompt -v

# With tools for context gathering
praisonai "latest AI trends" --expand-prompt --expand-tools tools.py

# Combine with query rewrite
praisonai "AI news" --query-rewrite --expand-prompt
```

### Programmatic Usage
```python
from praisonaiagents import PromptExpanderAgent, ExpandStrategy

# Basic usage
agent = PromptExpanderAgent()
result = agent.expand("write a movie script in 3 lines")
print(result.expanded_prompt)

# With specific strategy
result = agent.expand("blog about AI", strategy=ExpandStrategy.DETAILED)

# Available strategies: BASIC, DETAILED, STRUCTURED, CREATIVE, AUTO
```

**Key Difference:**
- `--query-rewrite`: Optimizes queries for search/retrieval (RAG)
- `--expand-prompt`: Expands prompts for detailed task execution

## Web Search, Web Fetch & Prompt Caching

### CLI Usage
```bash
# Web Search - Get real-time information
praisonai "What are the latest AI news today?" --web-search --llm openai/gpt-4o-search-preview

# Web Fetch - Retrieve and analyze URL content (Anthropic only)
praisonai "Summarize https://docs.praison.ai" --web-fetch --llm anthropic/claude-sonnet-4-20250514

# Prompt Caching - Reduce costs for repeated prompts
praisonai "Analyze this document..." --prompt-caching --llm anthropic/claude-sonnet-4-20250514
```

### Programmatic Usage
```python
from praisonaiagents import Agent

# Web Search
agent = Agent(
    instructions="You are a research assistant",
    llm="openai/gpt-4o-search-preview",
    web_search=True
)

# Web Fetch (Anthropic only)
agent = Agent(
    instructions="You are a content analyzer",
    llm="anthropic/claude-sonnet-4-20250514",
    web_fetch=True
)

# Prompt Caching
agent = Agent(
    instructions="You are an AI assistant..." * 50,  # Long system prompt
    llm="anthropic/claude-sonnet-4-20250514",
    prompt_caching=True
)
```

**Supported Providers:**
| Feature | Providers |
|---------|----------|
| Web Search | OpenAI, Gemini, Anthropic, xAI, Perplexity |
| Web Fetch | Anthropic |
| Prompt Caching | OpenAI (auto), Anthropic, Bedrock, Deepseek |

## CLI Features

| Feature | Docs |
|---------|:----:|
| ðŸ”„ Query Rewrite - RAG optimization | [ðŸ“–](https://docs.praison.ai/docs/cli/query-rewrite) |
| ðŸ”¬ Deep Research - Automated research | [ðŸ“–](https://docs.praison.ai/docs/cli/deep-research) |
| ðŸ“‹ Planning - Step-by-step execution | [ðŸ“–](https://docs.praison.ai/docs/cli/planning) |
| ðŸ’¾ Memory - Persistent agent memory | [ðŸ“–](https://docs.praison.ai/docs/cli/memory) |
| ðŸ“œ Rules - Auto-discovered instructions | [ðŸ“–](https://docs.praison.ai/docs/cli/rules) |
| ðŸ”„ Workflow - Multi-step workflows | [ðŸ“–](https://docs.praison.ai/docs/cli/workflow) |
| ðŸª Hooks - Event-driven actions | [ðŸ“–](https://docs.praison.ai/docs/cli/hooks) |
| ðŸ§  Claude Memory - Anthropic memory tool | [ðŸ“–](https://docs.praison.ai/docs/cli/claude-memory) |
| ðŸ›¡ï¸ Guardrail - Output validation | [ðŸ“–](https://docs.praison.ai/docs/cli/guardrail) |
| ðŸ“Š Metrics - Token usage tracking | [ðŸ“–](https://docs.praison.ai/docs/cli/metrics) |
| ðŸ–¼ï¸ Image - Vision processing | [ðŸ“–](https://docs.praison.ai/docs/cli/image) |
| ðŸ“¡ Telemetry - Usage monitoring | [ðŸ“–](https://docs.praison.ai/docs/cli/telemetry) |
| ðŸ”Œ MCP - Model Context Protocol | [ðŸ“–](https://docs.praison.ai/docs/cli/mcp) |
| âš¡ Fast Context - Codebase search | [ðŸ“–](https://docs.praison.ai/docs/cli/fast-context) |
| ðŸ“š Knowledge - RAG management | [ðŸ“–](https://docs.praison.ai/docs/cli/knowledge) |
| ðŸ’¬ Session - Conversation management | [ðŸ“–](https://docs.praison.ai/docs/cli/session) |
| ðŸ”§ Tools - Tool discovery | [ðŸ“–](https://docs.praison.ai/docs/cli/tools) |
| ðŸ¤ Handoff - Agent delegation | [ðŸ“–](https://docs.praison.ai/docs/cli/handoff) |
| ðŸ§  Auto Memory - Memory extraction | [ðŸ“–](https://docs.praison.ai/docs/cli/auto-memory) |
| ðŸ“‹ Todo - Task management | [ðŸ“–](https://docs.praison.ai/docs/cli/todo) |
| ðŸŽ¯ Router - Smart model selection | [ðŸ“–](https://docs.praison.ai/docs/cli/router) |
| ðŸ“ˆ Flow Display - Visual workflow | [ðŸ“–](https://docs.praison.ai/docs/cli/flow-display) |
| âœ¨ Prompt Expansion - Detailed prompts | [ðŸ“–](https://docs.praison.ai/docs/cli/prompt-expansion) |
| ðŸŒ Web Search - Real-time search | [ðŸ“–](https://docs.praison.ai/docs/cli/web-search) |
| ðŸ“¥ Web Fetch - URL content retrieval | [ðŸ“–](https://docs.praison.ai/docs/cli/web-fetch) |
| ðŸ’¾ Prompt Caching - Cost reduction | [ðŸ“–](https://docs.praison.ai/docs/cli/prompt-caching) |
| ðŸ“¦ Template Catalog - Browse & discover templates | [ðŸ“–](https://docs.praison.ai/docs/cli/template-catalog) |

### Template Catalog CLI

| Command | Description |
|---------|-------------|
| `praisonai templates browse` | Open template catalog in browser |
| `praisonai templates browse --print` | Print catalog URL only |
| `praisonai templates validate` | Validate template YAML files |
| `praisonai templates validate --source <dir>` | Validate specific directory |
| `praisonai templates validate --strict` | Strict validation mode |
| `praisonai templates validate --json` | JSON output format |
| `praisonai templates catalog build` | Build catalog locally |
| `praisonai templates catalog build --out <dir>` | Build to specific directory |
| `praisonai templates catalog sync` | Sync template sources |
| `praisonai templates catalog sync --source <name>` | Sync specific source |

**Examples:** [examples/catalog/](examples/catalog/) | **Docs:** [Code](https://docs.praison.ai/docs/cli/template-catalog-code) | [CLI](https://docs.praison.ai/docs/cli/template-catalog)

---

## ðŸ’» Using JavaScript Code

```bash
npm install praisonai
export OPENAI_API_KEY=xxxxxxxxxxxxxxxxxxxxxx
```

```javascript
const { Agent } = require('praisonai');
const agent = new Agent({ instructions: 'You are a helpful AI assistant' });
agent.start('Write a movie script about a robot in Mars');
```

![PraisonAI CLI Demo](docs/demo/praisonai-cli-demo.gif)

---

## â­ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=MervinPraison/PraisonAI&type=Date)](https://docs.praison.ai)

---

## ðŸ“Š Process Types & Patterns

<details>
<summary><strong>View architecture diagrams and workflow patterns</strong></summary>

### AI Agents Flow

```mermaid
graph LR
    %% Define the main flow
    Start([â–¶ Start]) --> Agent1
    Agent1 --> Process[âš™ Process]
    Process --> Agent2
    Agent2 --> Output([âœ“ Output])
    Process -.-> Agent1
    
    %% Define subgraphs for agents and their tasks
    subgraph Agent1[ ]
        Task1[ðŸ“‹ Task]
        AgentIcon1[ðŸ¤– AI Agent]
        Tools1[ðŸ”§ Tools]
        
        Task1 --- AgentIcon1
        AgentIcon1 --- Tools1
    end
    
    subgraph Agent2[ ]
        Task2[ðŸ“‹ Task]
        AgentIcon2[ðŸ¤– AI Agent]
        Tools2[ðŸ”§ Tools]
        
        Task2 --- AgentIcon2
        AgentIcon2 --- Tools2
    end

    classDef input fill:#8B0000,stroke:#7C90A0,color:#fff
    classDef process fill:#189AB4,stroke:#7C90A0,color:#fff
    classDef tools fill:#2E8B57,stroke:#7C90A0,color:#fff
    classDef transparent fill:none,stroke:none

    class Start,Output,Task1,Task2 input
    class Process,AgentIcon1,AgentIcon2 process
    class Tools1,Tools2 tools
    class Agent1,Agent2 transparent
```

## AI Agents with Tools

Create AI agents that can use tools to interact with external systems and perform actions.

```mermaid
flowchart TB
    subgraph Tools
        direction TB
        T3[Internet Search]
        T1[Code Execution]
        T2[Formatting]
    end

    Input[Input] ---> Agents
    subgraph Agents
        direction LR
        A1[Agent 1]
        A2[Agent 2]
        A3[Agent 3]
    end
    Agents ---> Output[Output]

    T3 --> A1
    T1 --> A2
    T2 --> A3

    style Tools fill:#189AB4,color:#fff
    style Agents fill:#8B0000,color:#fff
    style Input fill:#8B0000,color:#fff
    style Output fill:#8B0000,color:#fff
```

## AI Agents with Memory

Create AI agents with memory capabilities for maintaining context and information across tasks.

```mermaid
flowchart TB
    subgraph Memory
        direction TB
        STM[Short Term]
        LTM[Long Term]
    end

    subgraph Store
        direction TB
        DB[(Vector DB)]
    end

    Input[Input] ---> Agents
    subgraph Agents
        direction LR
        A1[Agent 1]
        A2[Agent 2]
        A3[Agent 3]
    end
    Agents ---> Output[Output]

    Memory <--> Store
    Store <--> A1
    Store <--> A2
    Store <--> A3

    style Memory fill:#189AB4,color:#fff
    style Store fill:#2E8B57,color:#fff
    style Agents fill:#8B0000,color:#fff
    style Input fill:#8B0000,color:#fff
    style Output fill:#8B0000,color:#fff
```

## AI Agents with Different Processes

### Sequential Process

The simplest form of task execution where tasks are performed one after another.

```mermaid
graph LR
    Input[Input] --> A1
    subgraph Agents
        direction LR
        A1[Agent 1] --> A2[Agent 2] --> A3[Agent 3]
    end
    A3 --> Output[Output]

    classDef input fill:#8B0000,stroke:#7C90A0,color:#fff
    classDef process fill:#189AB4,stroke:#7C90A0,color:#fff
    classDef transparent fill:none,stroke:none

    class Input,Output input
    class A1,A2,A3 process
    class Agents transparent
```

### Hierarchical Process

Uses a manager agent to coordinate task execution and agent assignments.

```mermaid
graph TB
    Input[Input] --> Manager
    
    subgraph Agents
        Manager[Manager Agent]
        
        subgraph Workers
            direction LR
            W1[Worker 1]
            W2[Worker 2]
            W3[Worker 3]
        end
        
        Manager --> W1
        Manager --> W2
        Manager --> W3
    end
    
    W1 --> Manager
    W2 --> Manager
    W3 --> Manager
    Manager --> Output[Output]

    classDef input fill:#8B0000,stroke:#7C90A0,color:#fff
    classDef process fill:#189AB4,stroke:#7C90A0,color:#fff
    classDef transparent fill:none,stroke:none

    class Input,Output input
    class Manager,W1,W2,W3 process
    class Agents,Workers transparent
```

### Workflow Process

Advanced process type supporting complex task relationships and conditional execution.

```mermaid
graph LR
    Input[Input] --> Start
    
    subgraph Workflow
        direction LR
        Start[Start] --> C1{Condition}
        C1 --> |Yes| A1[Agent 1]
        C1 --> |No| A2[Agent 2]
        A1 --> Join
        A2 --> Join
        Join --> A3[Agent 3]
    end
    
    A3 --> Output[Output]

    classDef input fill:#8B0000,stroke:#7C90A0,color:#fff
    classDef process fill:#189AB4,stroke:#7C90A0,color:#fff
    classDef decision fill:#2E8B57,stroke:#7C90A0,color:#fff
    classDef transparent fill:none,stroke:none

    class Input,Output input
    class Start,A1,A2,A3,Join process
    class C1 decision
    class Workflow transparent
```

#### Agentic Routing Workflow

Create AI agents that can dynamically route tasks to specialized LLM instances.

```mermaid
flowchart LR
    In[In] --> Router[LLM Call Router]
    Router --> LLM1[LLM Call 1]
    Router --> LLM2[LLM Call 2]
    Router --> LLM3[LLM Call 3]
    LLM1 --> Out[Out]
    LLM2 --> Out
    LLM3 --> Out
    
    style In fill:#8B0000,color:#fff
    style Router fill:#2E8B57,color:#fff
    style LLM1 fill:#2E8B57,color:#fff
    style LLM2 fill:#2E8B57,color:#fff
    style LLM3 fill:#2E8B57,color:#fff
    style Out fill:#8B0000,color:#fff
```

#### Agentic Orchestrator Worker

Create AI agents that orchestrate and distribute tasks among specialized workers.

```mermaid
flowchart LR
    In[In] --> Router[LLM Call Router]
    Router --> LLM1[LLM Call 1]
    Router --> LLM2[LLM Call 2]
    Router --> LLM3[LLM Call 3]
    LLM1 --> Synthesizer[Synthesizer]
    LLM2 --> Synthesizer
    LLM3 --> Synthesizer
    Synthesizer --> Out[Out]
    
    style In fill:#8B0000,color:#fff
    style Router fill:#2E8B57,color:#fff
    style LLM1 fill:#2E8B57,color:#fff
    style LLM2 fill:#2E8B57,color:#fff
    style LLM3 fill:#2E8B57,color:#fff
    style Synthesizer fill:#2E8B57,color:#fff
    style Out fill:#8B0000,color:#fff
```

#### Agentic Autonomous Workflow

Create AI agents that can autonomously monitor, act, and adapt based on environment feedback.

```mermaid
flowchart LR
    Human[Human] <--> LLM[LLM Call]
    LLM -->|ACTION| Environment[Environment]
    Environment -->|FEEDBACK| LLM
    LLM --> Stop[Stop]
    
    style Human fill:#8B0000,color:#fff
    style LLM fill:#2E8B57,color:#fff
    style Environment fill:#8B0000,color:#fff
    style Stop fill:#333,color:#fff
```

#### Agentic Parallelization

Create AI agents that can execute tasks in parallel for improved performance.

```mermaid
flowchart LR
    In[In] --> LLM2[LLM Call 2]
    In --> LLM1[LLM Call 1]
    In --> LLM3[LLM Call 3]
    LLM1 --> Aggregator[Aggregator]
    LLM2 --> Aggregator
    LLM3 --> Aggregator
    Aggregator --> Out[Out]
    
    style In fill:#8B0000,color:#fff
    style LLM1 fill:#2E8B57,color:#fff
    style LLM2 fill:#2E8B57,color:#fff
    style LLM3 fill:#2E8B57,color:#fff
    style Aggregator fill:#fff,color:#000
    style Out fill:#8B0000,color:#fff
```

#### Agentic Prompt Chaining

Create AI agents with sequential prompt chaining for complex workflows.

```mermaid
flowchart LR
    In[In] --> LLM1[LLM Call 1] --> Gate{Gate}
    Gate -->|Pass| LLM2[LLM Call 2] -->|Output 2| LLM3[LLM Call 3] --> Out[Out]
    Gate -->|Fail| Exit[Exit]
    
    style In fill:#8B0000,color:#fff
    style LLM1 fill:#2E8B57,color:#fff
    style LLM2 fill:#2E8B57,color:#fff
    style LLM3 fill:#2E8B57,color:#fff
    style Out fill:#8B0000,color:#fff
    style Exit fill:#8B0000,color:#fff
```

#### Agentic Evaluator Optimizer

Create AI agents that can generate and optimize solutions through iterative feedback.

```mermaid
flowchart LR
    In[In] --> Generator[LLM Call Generator] 
    Generator -->|SOLUTION| Evaluator[LLM Call Evaluator] -->|ACCEPTED| Out[Out]
    Evaluator -->|REJECTED + FEEDBACK| Generator
    
    style In fill:#8B0000,color:#fff
    style Generator fill:#2E8B57,color:#fff
    style Evaluator fill:#2E8B57,color:#fff
    style Out fill:#8B0000,color:#fff
```

#### Repetitive Agents

Create AI agents that can efficiently handle repetitive tasks through automated loops.

```mermaid
flowchart LR
    In[Input] --> LoopAgent[("Looping Agent")]
    LoopAgent --> Task[Task]
    Task --> |Next iteration| LoopAgent
    Task --> |Done| Out[Output]
    
    style In fill:#8B0000,color:#fff
    style LoopAgent fill:#2E8B57,color:#fff,shape:circle
    style Task fill:#2E8B57,color:#fff
    style Out fill:#8B0000,color:#fff
```

</details>

---

## ðŸ”§ Configuration & Integration

### Ollama Integration

```bash
export OPENAI_BASE_URL=http://localhost:11434/v1
```

### Groq Integration

Replace xxxx with Groq API KEY:

```bash
export OPENAI_API_KEY=xxxxxxxxxxx
export OPENAI_BASE_URL=https://api.groq.com/openai/v1
```

### 100+ Models Support

PraisonAI supports 100+ LLM models from various providers. Visit our [models documentation](https://docs.praison.ai/models/) for the complete list.

<div align="center">
  <a href="https://docs.praison.ai/models">
    <p align="center">
      <img src="https://img.shields.io/badge/ðŸ“š_Models_Documentation-Visit_docs.praison.ai-blue?style=for-the-badge&logo=bookstack&logoColor=white" alt="Models Documentation" />
    </p>
  </a>
</div>

---

## ðŸ“‹ Agents Playbook

### Simple Playbook Example

Create `agents.yaml` file and add the code below:

```yaml
framework: praisonai
topic: Artificial Intelligence
agents:  # Canonical: use 'agents' instead of 'roles'
  screenwriter:
    instructions: "Skilled in crafting scripts with engaging dialogue about {topic}."  # Canonical: use 'instructions' instead of 'backstory'
    goal: Create scripts from concepts.
    role: Screenwriter
    tasks:
      scriptwriting_task:
        description: "Develop scripts with compelling characters and dialogue about {topic}."
        expected_output: "Complete script ready for production."
```

*To run the playbook:*
```bash
praisonai agents.yaml
```

---

## ðŸ› ï¸ Custom Tools / Create Plugins

PraisonAI supports multiple ways to create and integrate custom tools (plugins) into your agents.

### Using `@tool` Decorator

```python
from praisonaiagents import Agent, tool

@tool
def search(query: str) -> str:
    """Search the web for information."""
    return f"Results for: {query}"

@tool
def calculate(expression: str) -> float:
    """Evaluate a math expression."""
    return eval(expression)

agent = Agent(
    instructions="You are a helpful assistant",
    tools=[search, calculate]
)
agent.start("Search for AI news and calculate 15*4")
```

### Using `BaseTool` Class

```python
from praisonaiagents import Agent, BaseTool

class WeatherTool(BaseTool):
    name = "weather"
    description = "Get current weather for a location"
    
    def run(self, location: str) -> str:
        return f"Weather in {location}: 72Â°F, Sunny"

agent = Agent(
    instructions="You are a weather assistant",
    tools=[WeatherTool()]
)
agent.start("What's the weather in Paris?")
```

### Creating a Tool Package (pip installable)

```toml
# pyproject.toml
[project]
name = "my-praisonai-tools"
version = "1.0.0"
dependencies = ["praisonaiagents"]

[project.entry-points."praisonaiagents.tools"]
my_tool = "my_package:MyTool"
```

```python
# my_package/__init__.py
from praisonaiagents import BaseTool

class MyTool(BaseTool):
    name = "my_tool"
    description = "My custom tool"
    
    def run(self, param: str) -> str:
        return f"Result: {param}"
```

After `pip install`, tools are auto-discovered:
```python
agent = Agent(tools=["my_tool"])  # Works automatically!
```

---

## ðŸ§  Memory & Context

PraisonAI provides zero-dependency persistent memory for agents. For detailed examples, see [section 6. Agent Memory](#6-agent-memory-zero-dependencies) in the Python Code Examples.

---

## ðŸ“š Knowledge & Retrieval (RAG)

PraisonAI provides a complete knowledge stack for building RAG applications with multiple vector stores, retrieval strategies, rerankers, and query modes.

### RAG Quickstart (Agent-first)

```python
from praisonaiagents import Agent
from praisonaiagents.rag.models import RetrievalStrategy

# Agent with RAG - simplest approach
agent = Agent(
    name="Research Assistant",
    knowledge=["docs/manual.pdf", "data/faq.txt"],
    knowledge_config={"vector_store": {"provider": "chroma"}},
    rag_config={
        "include_citations": True,
        "retrieval_strategy": RetrievalStrategy.HYBRID,  # Dense + BM25
        "rerank": True,
    }
)

# Query with citations
result = agent.rag_query("How do I authenticate?")
print(result.answer)
for citation in result.citations:
    print(f"  [{citation.id}] {citation.source}")
```

### RAG CLI Commands

| Command | Description |
|---------|-------------|
| `praisonai rag query "<question>"` | One-shot question answering with citations |
| `praisonai rag chat` | Interactive RAG chat session |
| `praisonai rag serve` | Start RAG as a microservice API |
| `praisonai rag eval <test_file>` | Evaluate RAG retrieval quality |

### RAG CLI Examples

```bash
# Query with hybrid retrieval (dense + BM25 keyword search)
praisonai rag query "What are the key findings?" --hybrid

# Query with hybrid + reranking for best quality
praisonai rag query "Summarize conclusions" --hybrid --rerank

# Interactive chat with hybrid retrieval
praisonai rag chat --collection research --hybrid --rerank

# Start API server with OpenAI-compatible endpoint
praisonai rag serve --hybrid --rerank --openai-compat --port 8080

# Query with profiling
praisonai rag query "Summary?" --profile --profile-out ./profile.json
```

### Knowledge CLI Commands

| Command | Description |
|---------|-------------|
| `praisonai knowledge index <sources>` | Index documents into knowledge base |
| `praisonai knowledge search <query>` | Search knowledge base (no LLM generation) |
| `praisonai knowledge list` | List indexed documents |

### Knowledge CLI Examples

```bash
# Index documents
praisonai knowledge index ./docs/ --collection myproject

# Search with hybrid retrieval
praisonai knowledge search "authentication" --hybrid --collection myproject

# Index with profiling
praisonai knowledge index ./data --profile --profile-out ./profile.json
```

### Knowledge vs RAG vs AutoRagAgent

- **Knowledge** is the indexing and retrieval substrate - use for indexing and raw search
- **RAG** orchestrates on top - use for question answering with LLM-generated responses and citations
- **AutoRagAgent** wraps an Agent with automatic retrieval decision - use when you want the agent to decide when to retrieve
- All share the same underlying index

### AutoRagAgent (Automatic RAG)

AutoRagAgent automatically decides when to retrieve context from knowledge bases vs direct chat, based on query heuristics.

```python
from praisonaiagents import Agent, AutoRagAgent

# Create agent with knowledge
agent = Agent(
    name="Research Assistant",
    knowledge=["docs/manual.pdf"],
    user_id="user123",  # Required for RAG retrieval
)

# Wrap with AutoRagAgent
auto_rag = AutoRagAgent(
    agent=agent,
    retrieval_policy="auto",  # auto, always, never
    top_k=5,
    hybrid=True,
    rerank=True,
)

# Auto-decides: retrieves for questions, skips for greetings
result = auto_rag.chat("What are the key findings?")  # Retrieves
result = auto_rag.chat("Hello!")  # Skips retrieval

# Force retrieval or skip per-call
result = auto_rag.chat("Hi", force_retrieval=True)
result = auto_rag.chat("Summary?", skip_retrieval=True)
```

**CLI Usage:**
```bash
# Enable auto-rag with default policy (auto)
praisonai --auto-rag "What are the key findings?"

# Always retrieve
praisonai --auto-rag --rag-policy always "Tell me about X"

# With hybrid retrieval and reranking
praisonai --auto-rag --rag-hybrid --rag-rerank "Summarize the document"
```

### Configuration Precedence

Settings are applied in this order (highest priority first):
1. **CLI flags** - `--hybrid`, `--rerank`, `--top-k`
2. **Environment variables** - `PRAISONAI_HYBRID=true`
3. **Config file** - YAML configuration (`--config`)
4. **Defaults**

```bash
# Environment variables
export PRAISONAI_HYBRID=true
export PRAISONAI_RERANK=true
export PRAISONAI_TOP_K=10
```

### Lightweight Installs

```bash
# Base install (minimal, fast imports)
pip install praisonaiagents

# With RAG API server support
pip install "praisonai[rag-api]"
```

### Live Tests (Real API Keys)

Run integration tests with real API keys:

```bash
# Enable live tests
export PRAISONAI_LIVE_TESTS=1
export OPENAI_API_KEY="your-key"

# Run live tests
pytest -m live tests/integration/
```

### Knowledge Stack Features Table

| Feature | Description | SDK Docs | CLI Docs |
|---------|-------------|----------|----------|
| **Hybrid Retrieval** | Dense vectors + BM25 keyword search with RRF fusion | [SDK](/docs/rag/module) | [CLI](/docs/cli/rag) |
| **Reranking** | LLM, Cross-Encoder, Cohere rerankers | [SDK](/docs/rag/module) | [CLI](/docs/cli/rag) |
| **RAG Serve** | Microservice API with OpenAI-compatible mode | [SDK](/docs/rag/module) | [CLI](/docs/cli/rag) |
| **Vector Stores** | ChromaDB, Pinecone, Qdrant, Weaviate, In-Memory | [SDK](/docs/sdk/praisonaiagents/knowledge/protocols) | [CLI](/docs/cli/knowledge) |
| **Data Readers** | Load PDF, Markdown, Text, HTML, URLs | [SDK](/docs/sdk/praisonaiagents/knowledge/protocols) | [CLI](/docs/cli/knowledge) |
| **Profiling** | Performance profiling with `--profile` flag | [SDK](/docs/features/profiling) | [CLI](/docs/cli/rag) |

---

## ðŸ”¬ Advanced Features

### Research & Intelligence

- ðŸ”¬ **Deep Research Agents** - OpenAI & Gemini support for automated research
- ðŸ”„ **Query Rewriter Agent** - HyDE, Step-back, Multi-query strategies for RAG optimization
- ðŸŒ **Native Web Search** - Real-time search via OpenAI, Gemini, Anthropic, xAI, Perplexity
- ðŸ“¥ **Web Fetch** - Retrieve full content from URLs (Anthropic)
- ðŸ“ **Prompt Expander Agent** - Expand short prompts into detailed instructions

### Memory & Caching

- ðŸ’¾ **Prompt Caching** - Reduce costs & latency (OpenAI, Anthropic, Bedrock, Deepseek)
- ðŸ§  **Claude Memory Tool** - Persistent cross-conversation memory (Anthropic Beta)
- ðŸ’¾ **File-Based Memory** - Zero-dependency persistent memory for all agents
- ðŸ” **Built-in Search Tools** - Tavily, You.com, Exa for web search, news, content extraction

### Planning & Workflows

- ðŸ“‹ **Planning Mode** - Plan before execution for agents & multi-agent systems
- ðŸ”§ **Planning Tools** - Research with tools during planning phase
- ðŸ§  **Planning Reasoning** - Chain-of-thought planning for complex tasks
- â›“ï¸ **Prompt Chaining** - Sequential prompt workflows with conditional gates
- ðŸ” **Evaluator Optimiser** - Generate and optimize through iterative feedback
- ðŸ‘· **Orchestrator Workers** - Distribute tasks among specialised workers
- âš¡ **Parallelisation** - Execute tasks in parallel for improved performance
- ðŸ” **Repetitive Agents** - Handle repetitive tasks through automated loops
- ðŸ¤– **Autonomous Workflow** - Monitor, act, adapt based on environment feedback

### Specialised Agents

- ðŸ–¼ï¸ **Image Generation Agent** - Create images from text descriptions
- ðŸ“· **Image to Text Agent** - Extract text and descriptions from images
- ðŸŽ¬ **Video Agent** - Analyse and process video content
- ðŸ“Š **Data Analyst Agent** - Analyse data and generate insights
- ðŸ’° **Finance Agent** - Financial analysis and recommendations
- ðŸ›’ **Shopping Agent** - Price comparison and shopping assistance
- â­ **Recommendation Agent** - Personalised recommendations
- ðŸ“– **Wikipedia Agent** - Search and extract Wikipedia information
- ðŸ’» **Programming Agent** - Code development and analysis
- ðŸ“ **Markdown Agent** - Generate and format Markdown content
- ðŸ”€ **Model Router** - Smart model selection based on task complexity

### MCP Protocol

- ðŸ”Œ **MCP Transports** - stdio, Streamable HTTP, WebSocket, SSE (Protocol 2025-11-25)
- ðŸŒ **WebSocket MCP** - Real-time bidirectional connections with auto-reconnect
- ðŸ” **MCP Security** - Origin validation, DNS rebinding prevention, secure sessions
- ðŸ”„ **MCP Resumability** - SSE stream recovery via Last-Event-ID

### A2A & A2UI Protocols

- ðŸ”— **A2A Protocol** - Agent-to-Agent communication for inter-agent collaboration
- ðŸ–¼ï¸ **A2UI Protocol** - Agent-to-User Interface for generating rich UIs from agents
- ðŸ“‹ **UI Templates** - ChatTemplate, ListTemplate, FormTemplate, DashboardTemplate
- ðŸ”§ **Surface Builder** - Fluent API for building declarative UIs

### Safety & Control

- ðŸ¤ **Agent Handoffs** - Transfer context between specialised agents
- ðŸ›¡ï¸ **Guardrails** - Input/output validation and safety checks
- âœ… **Human Approval** - Require human confirmation for critical actions
- ðŸ” **Tool Approval CLI** - `--trust` (auto-approve all) and `--approve-level` (risk-based approval)
- ðŸ’¬ **Sessions Management** - Isolated conversation contexts
- ðŸ”„ **Stateful Agents** - Maintain state across interactions

### Developer Tools

- âš¡ **Fast Context** - Rapid parallel code search (10-20x faster)
- ðŸ“œ **Rules & Instructions** - Auto-discover CLAUDE.md, AGENTS.md, GEMINI.md
- ðŸª **Hooks** - Pre/post operation hooks for custom logic
- ðŸ“ˆ **Telemetry** - Track agent performance and usage
- ðŸ“¹ **Camera Integration** - Capture and analyse camera input

### Other Features

- ðŸ”„ **CrewAI & AG2 Integration** - Use CrewAI or AG2 (Formerly AutoGen) Framework
- ðŸ’» **Codebase Chat** - Chat with entire codebase
- ðŸŽ¨ **Interactive UIs** - Multiple interactive interfaces
- ðŸ“„ **YAML Configuration** - YAML-based agent and workflow configuration
- ðŸ› ï¸ **Custom Tools** - Easy custom tool integration
- ðŸ” **Internet Search** - Multiple providers (Tavily, You.com, Exa, DuckDuckGo, Crawl4AI)
- ðŸ–¼ï¸ **VLM Support** - Vision Language Model support
- ðŸŽ™ï¸ **Voice Interaction** - Real-time voice interaction

---

## ðŸ’¾ Persistence (Databases)

Enable automatic conversation persistence with 2 lines of code:

```python
from praisonaiagents import Agent, db

agent = Agent(
    name="Assistant",
    db=db(database_url="postgresql://localhost/mydb"),  # db(...) shortcut
    session_id="my-session"  # Optional: defaults to per-hour ID (YYYYMMDDHH)
)
agent.chat("Hello!")  # Auto-persists messages, runs, traces
```

### Persistence CLI Commands

| Command | Description |
|---------|-------------|
| `praisonai persistence doctor` | Validate DB connectivity |
| `praisonai persistence run` | Run agent with persistence |
| `praisonai persistence resume` | Resume existing session |
| `praisonai persistence export` | Export session to JSONL |
| `praisonai persistence import` | Import session from JSONL |
| `praisonai persistence migrate` | Apply schema migrations |
| `praisonai persistence status` | Show schema status |

> **Note:** See [Knowledge & Retrieval (RAG)](#-knowledge--retrieval-rag) for complete Knowledge CLI documentation.

### Databases Table

| Database | Store Type | Install | Example | Docs |
|----------|------------|---------|---------|------|
| PostgreSQL | Conversation | `pip install "praisonai[tools]"` | [simple_db_agent.py](examples/persistence/simple_db_agent.py) | [docs](https://docs.praison.ai/docs/databases/postgres) |
| MySQL | Conversation | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| SQLite | Conversation | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| SingleStore | Conversation | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| Supabase | Conversation | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| SurrealDB | Conversation | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| Qdrant | Knowledge | `pip install "praisonai[tools]"` | [knowledge_qdrant.py](examples/persistence/knowledge_qdrant.py) | [docs](https://docs.praison.ai/docs/databases/qdrant) |
| ChromaDB | Knowledge | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| Pinecone | Knowledge | `pip install pinecone` | [pinecone_wow.py](examples/vector/pinecone_wow.py) | [docs](https://docs.praison.ai/docs/databases/pinecone) |
| Weaviate | Knowledge | `pip install weaviate-client` | [weaviate_wow.py](examples/vector/weaviate_wow.py) | [docs](https://docs.praison.ai/docs/databases/weaviate) |
| LanceDB | Knowledge | `pip install lancedb` | [lancedb_real_wow.py](examples/vector/lancedb_real_wow.py) | [docs](https://docs.praison.ai/docs/databases/lancedb) |
| Milvus | Knowledge | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| PGVector | Knowledge | `pip install psycopg2-binary` | [pgvector_real_wow.py](examples/vector/pgvector_real_wow.py) | [docs](https://docs.praison.ai/docs/databases/pgvector) |
| Redis Vector | Knowledge | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| Cassandra | Knowledge | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| ClickHouse | Knowledge | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| Redis | State | `pip install "praisonai[tools]"` | [state_redis.py](examples/persistence/state_redis.py) | [docs](https://docs.praison.ai/docs/databases/redis) |
| MongoDB | State | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| DynamoDB | State | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| Firestore | State | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| Upstash | State | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |
| Memory | State | `pip install "praisonai[tools]"` | - | [docs](https://docs.praison.ai/docs/databases/overview) |

---

## ðŸ”§ Tools Table

Install all tools with: `pip install "praisonai[tools]"`

| Tool | Category | Import | Docs |
|------|----------|--------|------|
| Tavily | Web Search | `from praisonai_tools import TavilyTool` | [docs](https://docs.praison.ai/docs/tools/external/tavily) |
| DuckDuckGo | Web Search | `from praisonai_tools import DuckDuckGoTool` | [docs](https://docs.praison.ai/docs/tools/external/duckduckgo) |
| Exa | Web Search | `from praisonai_tools import ExaTool` | [docs](https://docs.praison.ai/docs/tools/external/exa) |
| Serper | Web Search | `from praisonai_tools import SerperTool` | [docs](https://docs.praison.ai/docs/tools/external/serper) |
| Jina | Web Reader | `from praisonai_tools import JinaTool` | [docs](https://docs.praison.ai/docs/tools/external/jina) |
| Firecrawl | Web Scraping | `from praisonai_tools import FirecrawlTool` | [docs](https://docs.praison.ai/docs/tools/external/firecrawl) |
| Crawl4AI | Web Scraping | `from praisonai_tools import Crawl4AITool` | [docs](https://docs.praison.ai/docs/tools/external/crawl4ai) |
| Wikipedia | Knowledge | `from praisonai_tools import WikipediaTool` | [docs](https://docs.praison.ai/docs/tools/external/wikipedia) |
| ArXiv | Research | `from praisonai_tools import ArxivTool` | [docs](https://docs.praison.ai/docs/tools/external/arxiv) |
| HackerNews | News | `from praisonai_tools import HackerNewsTool` | [docs](https://docs.praison.ai/docs/tools/external/hackernews) |
| YouTube | Media | `from praisonai_tools import YouTubeTool` | [docs](https://docs.praison.ai/docs/tools/external/youtube) |
| Weather | Data | `from praisonai_tools import WeatherTool` | [docs](https://docs.praison.ai/docs/tools/external/weather) |
| PostgreSQL | Database | `from praisonai_tools import PostgresTool` | [docs](https://docs.praison.ai/docs/tools/external/postgres) |
| MySQL | Database | `from praisonai_tools import MySQLTool` | [docs](https://docs.praison.ai/docs/tools/external/mysql) |
| SQLite | Database | `from praisonai_tools import SQLiteTool` | [docs](https://docs.praison.ai/docs/tools/external/sqlite) |
| MongoDB | Database | `from praisonai_tools import MongoDBTool` | [docs](https://docs.praison.ai/docs/tools/external/mongodb) |
| Redis | Database | `from praisonai_tools import RedisTool` | [docs](https://docs.praison.ai/docs/tools/external/redis) |
| Qdrant | Vector DB | `from praisonai_tools import QdrantTool` | [docs](https://docs.praison.ai/docs/tools/external/qdrant) |
| GitHub | DevOps | `from praisonai_tools import GitHubTool` | [docs](https://docs.praison.ai/docs/tools/external/github) |
| Slack | Communication | `from praisonai_tools import SlackTool` | [docs](https://docs.praison.ai/docs/tools/external/slack) |
| Discord | Communication | `from praisonai_tools import DiscordTool` | [docs](https://docs.praison.ai/docs/tools/external/discord) |
| Telegram | Communication | `from praisonai_tools import TelegramTool` | [docs](https://docs.praison.ai/docs/tools/external/telegram) |
| Email | Communication | `from praisonai_tools import EmailTool` | [docs](https://docs.praison.ai/docs/tools/external/email) |
| Notion | Productivity | `from praisonai_tools import NotionTool` | [docs](https://docs.praison.ai/docs/tools/external/notion) |
| File | File System | `from praisonai_tools import FileTool` | [docs](https://docs.praison.ai/docs/tools/external/file) |
| Shell | System | `from praisonai_tools import ShellTool` | [docs](https://docs.praison.ai/docs/tools/external/shell) |
| Python | Code | `from praisonai_tools import PythonTool` | [docs](https://docs.praison.ai/docs/tools/external/python) |
| JSON | Data | `from praisonai_tools import JSONTool` | [docs](https://docs.praison.ai/docs/tools/external/json) |
| CSV | Data | `from praisonai_tools import CSVTool` | [docs](https://docs.praison.ai/docs/tools/external/csv) |
| Calculator | Math | `from praisonai_tools import CalculatorTool` | [docs](https://docs.praison.ai/docs/tools/external/calculator) |

> See [full tools documentation](https://docs.praison.ai/docs/tools/tools) for all 100+ available tools.

---

## ðŸŽ“ Video Tutorials

Learn PraisonAI through our comprehensive video series:

<details>
<summary><strong>View all 22 video tutorials</strong></summary>

| Topic | Video |
|-------|--------|
| AI Agents with Self Reflection | [![Self Reflection](https://img.youtube.com/vi/vLXobEN2Vc8/0.jpg)](https://www.youtube.com/watch?v=vLXobEN2Vc8) |
| Reasoning Data Generating Agent | [![Reasoning Data](https://img.youtube.com/vi/fUT332Y2zA8/0.jpg)](https://www.youtube.com/watch?v=fUT332Y2zA8) |
| AI Agents with Reasoning | [![Reasoning](https://img.youtube.com/vi/KNDVWGN3TpM/0.jpg)](https://www.youtube.com/watch?v=KNDVWGN3TpM) |
| Multimodal AI Agents | [![Multimodal](https://img.youtube.com/vi/hjAWmUT1qqY/0.jpg)](https://www.youtube.com/watch?v=hjAWmUT1qqY) |
| AI Agents Workflow | [![Workflow](https://img.youtube.com/vi/yWTH44QPl2A/0.jpg)](https://www.youtube.com/watch?v=yWTH44QPl2A) |
| Async AI Agents | [![Async](https://img.youtube.com/vi/VhVQfgo00LE/0.jpg)](https://www.youtube.com/watch?v=VhVQfgo00LE) |
| Mini AI Agents | [![Mini](https://img.youtube.com/vi/OkvYp5aAGSg/0.jpg)](https://www.youtube.com/watch?v=OkvYp5aAGSg) |
| AI Agents with Memory | [![Memory](https://img.youtube.com/vi/1hVfVxvPnnQ/0.jpg)](https://www.youtube.com/watch?v=1hVfVxvPnnQ) |
| Repetitive Agents | [![Repetitive](https://img.youtube.com/vi/dAYGxsjDOPg/0.jpg)](https://www.youtube.com/watch?v=dAYGxsjDOPg) |
| Introduction | [![Introduction](https://img.youtube.com/vi/Fn1lQjC0GO0/0.jpg)](https://www.youtube.com/watch?v=Fn1lQjC0GO0) |
| Tools Overview | [![Tools Overview](https://img.youtube.com/vi/XaQRgRpV7jo/0.jpg)](https://www.youtube.com/watch?v=XaQRgRpV7jo) |
| Custom Tools | [![Custom Tools](https://img.youtube.com/vi/JSU2Rndh06c/0.jpg)](https://www.youtube.com/watch?v=JSU2Rndh06c) |
| Firecrawl Integration | [![Firecrawl](https://img.youtube.com/vi/UoqUDcLcOYo/0.jpg)](https://www.youtube.com/watch?v=UoqUDcLcOYo) |
| User Interface | [![UI](https://img.youtube.com/vi/tg-ZjNl3OCg/0.jpg)](https://www.youtube.com/watch?v=tg-ZjNl3OCg) |
| Crawl4AI Integration | [![Crawl4AI](https://img.youtube.com/vi/KAvuVUh0XU8/0.jpg)](https://www.youtube.com/watch?v=KAvuVUh0XU8) |
| Chat Interface | [![Chat](https://img.youtube.com/vi/sw3uDqn2h1Y/0.jpg)](https://www.youtube.com/watch?v=sw3uDqn2h1Y) |
| Code Interface | [![Code](https://img.youtube.com/vi/_5jQayO-MQY/0.jpg)](https://www.youtube.com/watch?v=_5jQayO-MQY) |
| Mem0 Integration | [![Mem0](https://img.youtube.com/vi/KIGSgRxf1cY/0.jpg)](https://www.youtube.com/watch?v=KIGSgRxf1cY) |
| Training | [![Training](https://img.youtube.com/vi/aLawE8kwCrI/0.jpg)](https://www.youtube.com/watch?v=aLawE8kwCrI) |
| Realtime Voice Interface | [![Realtime](https://img.youtube.com/vi/frRHfevTCSw/0.jpg)](https://www.youtube.com/watch?v=frRHfevTCSw) |
| Call Interface | [![Call](https://img.youtube.com/vi/m1cwrUG2iAk/0.jpg)](https://www.youtube.com/watch?v=m1cwrUG2iAk) |
| Reasoning Extract Agents | [![Reasoning Extract](https://img.youtube.com/vi/2PPamsADjJA/0.jpg)](https://www.youtube.com/watch?v=2PPamsADjJA) |

</details>

---

## ðŸ‘¥ Contributing

We welcome contributions from the community! Here's how you can contribute:

1. **Fork on GitHub** - Use the "Fork" button on the [repository page](https://github.com/MervinPraison/PraisonAI)
2. **Clone your fork** - `git clone https://github.com/yourusername/praisonAI.git`
3. **Create a branch** - `git checkout -b new-feature`
4. **Make changes and commit** - `git commit -am "Add some feature"`
5. **Push to your fork** - `git push origin new-feature`
6. **Submit a pull request** - Via GitHub's web interface
7. **Await feedback** - From project maintainers

---

## ðŸ”§ Development

### Using uv

```bash
# Install uv if you haven't already
pip install uv

# Install from requirements
uv pip install -r pyproject.toml

# Install with extras
uv pip install -r pyproject.toml --extra code
uv pip install -r pyproject.toml --extra "crewai,autogen"
```

### Bump and Release

```bash
# From project root - bumps version and releases in one command
python src/praisonai/scripts/bump_and_release.py 2.2.99

# With praisonaiagents dependency
python src/praisonai/scripts/bump_and_release.py 2.2.99 --agents 0.0.169

# Then publish
cd src/praisonai && uv publish
```

---

## â“ FAQ & Troubleshooting

<details>
<summary><strong>ModuleNotFoundError: No module named 'praisonaiagents'</strong></summary>

Install the package:
```bash
pip install praisonaiagents
```

</details>

<details>
<summary><strong>API key not found / Authentication error</strong></summary>

Ensure your API key is set:
```bash
export OPENAI_API_KEY=your_key_here
```

For other providers, see [Environment Variables](#environment-variables).

</details>

<details>
<summary><strong>How do I use a local model (Ollama)?</strong></summary>

```bash
# Start Ollama server first
ollama serve

# Set environment variable
export OPENAI_BASE_URL=http://localhost:11434/v1
```

See [Ollama Integration](#ollama-integration) for more details.

</details>

<details>
<summary><strong>How do I persist conversations to a database?</strong></summary>

Use the `db` parameter:
```python
from praisonaiagents import Agent, db

agent = Agent(
    name="Assistant",
    db=db(database_url="postgresql://localhost/mydb"),
    session_id="my-session"
)
```

See [Persistence (Databases)](#-persistence-databases) for supported databases.

</details>

<details>
<summary><strong>How do I enable agent memory?</strong></summary>

```python
from praisonaiagents import Agent

agent = Agent(
    name="Assistant",
    memory=True,  # Enables file-based memory (no extra deps!)
    user_id="user123"
)
```

See [Agent Memory](#6-agent-memory-zero-dependencies) for more options.

</details>

<details>
<summary><strong>How do I run multiple agents together?</strong></summary>

```python
from praisonaiagents import Agent, Agents

agent1 = Agent(instructions="Research topics")
agent2 = Agent(instructions="Summarize findings")
agents = Agents(agents=[agent1, agent2])
agents.start()
```

See [Multi Agents](#2-multi-agents) for more examples.

</details>

<details>
<summary><strong>How do I use MCP tools?</strong></summary>

```python
from praisonaiagents import Agent, MCP

agent = Agent(
    tools=MCP("npx @modelcontextprotocol/server-memory")
)
```

See [MCP Protocol](#19-mcp-model-context-protocol) for all transport options.

</details>

### Getting Help

- ðŸ“š [Full Documentation](https://docs.praison.ai)
- ðŸ› [Report Issues](https://github.com/MervinPraison/PraisonAI/issues)
- ðŸ’¬ [Discussions](https://github.com/MervinPraison/PraisonAI/discussions)

---

<div align="center">
  <p><strong>Made with â¤ï¸ by the PraisonAI Team</strong></p>
  <p>
    <a href="https://docs.praison.ai">Documentation</a> â€¢
    <a href="https://github.com/MervinPraison/PraisonAI">GitHub</a> â€¢
    <a href="https://github.com/MervinPraison/PraisonAI/issues">Issues</a>
  </p>
</div>


## Links discovered
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/agents/single-agent.py)
- [ðŸ“–](https://docs.praison.ai/agents/single)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/general/mini_agents_example.py)
- [ðŸ“–](https://docs.praison.ai/concepts/agents)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/general/auto_agents_example.py)
- [ðŸ“–](https://docs.praison.ai/features/autoagents)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/concepts/self-reflection-details.py)
- [ðŸ“–](https://docs.praison.ai/features/selfreflection)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/concepts/reasoning-extraction.py)
- [ðŸ“–](https://docs.praison.ai/features/reasoning)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/general/multimodal.py)
- [ðŸ“–](https://docs.praison.ai/features/multimodal)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/workflows/simple_workflow.py)
- [ðŸ“–](https://docs.praison.ai/features/workflows)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/workflows/workflow_with_agents.py)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/workflows/workflow_routing.py)
- [ðŸ“–](https://docs.praison.ai/features/routing)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/workflows/workflow_parallel.py)
- [ðŸ“–](https://docs.praison.ai/features/parallelisation)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/workflows/workflow_loop_csv.py)
- [ðŸ“–](https://docs.praison.ai/features/repetitive)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/workflows/workflow_repeat.py)
- [ðŸ“–](https://docs.praison.ai/features/evaluator-optimiser)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/workflows/workflow_conditional.py)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/workflows/workflow_branching.py)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/workflows/workflow_early_stop.py)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/workflows/workflow_checkpoints.py)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/agents/code-agent.py)
- [ðŸ“–](https://docs.praison.ai/features/codeagent)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/code/code_editing_example.py)
- [ðŸ“–](https://docs.praison.ai/code/editing)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/code/external_agents_example.py)
- [ðŸ“–](https://docs.praison.ai/code/external-agents)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/code/claude_code_example.py)
- [ðŸ“–](https://docs.praison.ai/code/claude-code)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/code/gemini_cli_example.py)
- [ðŸ“–](https://docs.praison.ai/code/gemini-cli)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/code/codex_cli_example.py)
- [ðŸ“–](https://docs.praison.ai/code/codex-cli)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/code/cursor_cli_example.py)
- [ðŸ“–](https://docs.praison.ai/code/cursor-cli)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/general/memory_example.py)
- [ðŸ“–](https://docs.praison.ai/concepts/memory)
- [ðŸ“–](https://docs.praison.ai/features/claude-memory-tool)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/concepts/knowledge-agents.py)
- [ðŸ“–](https://docs.praison.ai/features/knowledge)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/concepts/rag-agents.py)
- [ðŸ“–](https://docs.praison.ai/features/rag)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/concepts/chat-with-pdf.py)
- [ðŸ“–](https://docs.praison.ai/features/chat-with-pdf)
- [ðŸ“–](https://docs.praison.ai/api/praisonai/knowledge-readers-api)
- [ðŸ“–](https://docs.praison.ai/api/praisonai/knowledge-vector-store-api)
- [ðŸ“–](https://docs.praison.ai/api/praisonai/knowledge-retrieval-api)
- [ðŸ“–](https://docs.praison.ai/api/praisonai/knowledge-reranker-api)
- [ðŸ“–](https://docs.praison.ai/api/praisonai/knowledge-index-api)
- [ðŸ“–](https://docs.praison.ai/api/praisonai/knowledge-query-engine-api)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/agents/research-agent.py)
- [ðŸ“–](https://docs.praison.ai/agents/deep-research)
- [ðŸ“–](https://docs.praison.ai/agents/query-rewriter)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/agents/websearch-agent.py)
- [ðŸ“–](https://docs.praison.ai/agents/websearch)
- [ðŸ“–](https://docs.praison.ai/tools/tavily)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/src/praisonai-agents/examples/web_search_example.py)
- [ðŸ“–](https://docs.praison.ai/tools/web-search)
- [ðŸ“–](https://docs.praison.ai/features/model-capabilities)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/agents/planning-agent.py)
- [ðŸ“–](https://docs.praison.ai/features/planning-mode)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/general/prompt_chaining.py)
- [ðŸ“–](https://docs.praison.ai/features/promptchaining)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/general/evaluator-optimiser.py)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/general/orchestrator-workers.py)
- [ðŸ“–](https://docs.praison.ai/features/orchestrator-worker)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/agents/data-analyst-agent.py)
- [ðŸ“–](https://docs.praison.ai/agents/data-analyst)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/agents/finance-agent.py)
- [ðŸ“–](https://docs.praison.ai/agents/finance)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/agents/shopping-agent.py)
- [ðŸ“–](https://docs.praison.ai/agents/shopping)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/agents/recommendation-agent.py)
- [ðŸ“–](https://docs.praison.ai/agents/recommendation)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/agents/wikipedia-agent.py)
- [ðŸ“–](https://docs.praison.ai/agents/wikipedia)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/agents/programming-agent.py)
- [ðŸ“–](https://docs.praison.ai/agents/programming)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/agents/math-agent.py)
- [ðŸ“–](https://docs.praison.ai/features/mathagent)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/agents/markdown-agent.py)
- [ðŸ“–](https://docs.praison.ai/agents/markdown)
- [ðŸ“–](https://docs.praison.ai/agents/prompt-expander)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/image/image-agent.py)
- [ðŸ“–](https://docs.praison.ai/features/image-generation)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/agents/image-to-text-agent.py)
- [ðŸ“–](https://docs.praison.ai/agents/image-to-text)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/agents/video-agent.py)
- [ðŸ“–](https://docs.praison.ai/agents/video)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/camera.md)
- [ðŸ“–](https://docs.praison.ai/features/camera-integration)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/mcp/mcp-transports-overview.py)
- [ðŸ“–](https://docs.praison.ai/mcp/transports)
- [Example](https://github.com/MervinPraison/PraisonAI/blob/main/src/praisonai/examples/python/mcp/websocket-mcp.py)

--- src/praisonai/praisonai/README.md ---
# PraisonAI Package

This is the PraisonAI package, which serves as a wrapper for PraisonAIAgents.

It provides a simple and intuitive interface for working with AI agents and their capabilities.

--- src/praisonai/tests/README.md ---
# PraisonAI Agents - Comprehensive Testing Suite

This directory contains a comprehensive testing suite for PraisonAI Agents, organized into different categories to ensure thorough coverage of all functionality.

## ðŸ“ Test Structure

```
tests/
â”œâ”€â”€ conftest.py                    # Pytest configuration and fixtures
â”œâ”€â”€ test_runner.py                 # Comprehensive test runner script
â”œâ”€â”€ simple_test_runner.py          # Simple test runner (no pytest import dependency)
â”œâ”€â”€ README.md                      # This documentation
â”œâ”€â”€ unit/                          # Unit tests for core functionality
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_core_agents.py        # Core agent, task, and LLM tests
â”‚   â”œâ”€â”€ test_async_agents.py       # Async functionality tests
â”‚   â”œâ”€â”€ test_tools_and_ui.py       # Tools and UI integration tests
â”‚   â””â”€â”€ agent/                     # Legacy agent tests
â”‚       â”œâ”€â”€ test_mini_agents_fix.py
â”‚       â”œâ”€â”€ test_mini_agents_sequential.py
â”‚       â””â”€â”€ test_type_casting.py
â”œâ”€â”€ integration/                   # Integration tests for complex features
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_base_url_api_base_fix.py  # Base URL mapping tests
â”‚   â”œâ”€â”€ test_mcp_integration.py        # MCP protocol tests
â”‚   â””â”€â”€ test_rag_integration.py        # RAG functionality tests
â”œâ”€â”€ test.py                        # Legacy example tests
â”œâ”€â”€ basic_example.py              # Basic agent example
â”œâ”€â”€ advanced_example.py           # Advanced agent example
â”œâ”€â”€ auto_example.py               # Auto agent example
â”œâ”€â”€ agents.yaml                   # Sample agent configuration
â””â”€â”€ test_basic.py                  # Basic diagnostic test script
```

## ðŸ§ª Test Categories

### 1. Unit Tests (`tests/unit/`)
Fast, isolated tests for core functionality:

- **Core Agents** (`test_core_agents.py`)
  - Agent creation and configuration
  - Task management and execution
  - LLM integration and chat functionality
  - Multi-agent orchestration

- **Async Functionality** (`test_async_agents.py`)
  - Async agents and tasks
  - Async tool integration
  - Mixed sync/async workflows
  - Async memory operations

- **Tools & UI** (`test_tools_and_ui.py`)
  - Custom tool creation and integration
  - Multi-modal tools (image, audio, document)
  - UI framework configurations (Gradio, Streamlit, Chainlit)
  - API endpoint simulation

### 2. Integration Tests (`tests/integration/`)
Complex tests for integrated systems:

- **MCP Integration** (`test_mcp_integration.py`)
  - Model Context Protocol server connections
  - Tool execution via MCP
  - Multiple server management
  - Error handling and recovery

- **RAG Integration** (`test_rag_integration.py`)
  - Knowledge base creation and indexing
  - Vector store operations (ChromaDB, Pinecone, Weaviate)
  - Document processing and retrieval
  - Memory persistence and updates

- **Base URL Mapping** (`test_base_url_api_base_fix.py`)
  - LiteLLM compatibility fixes
  - OpenAI-compatible endpoint support
  - KoboldCPP integration

## ðŸš€ Running Tests

### Quick Start
```bash
# Run all tests with the comprehensive test runner
python tests/test_runner.py

# Run specific test categories
python tests/test_runner.py --unit
python tests/test_runner.py --integration
python tests/test_runner.py --fast

# Run tests matching a pattern
python tests/test_runner.py --pattern "agent"
python tests/test_runner.py --markers "not slow"
```

### Alternative Test Runners

#### Simple Test Runner (No pytest dependency at import)
If you encounter pytest import issues, use the simple test runner:
```bash
# Run all tests via subprocess (works without pytest import)
python tests/simple_test_runner.py

# Run only fast tests with basic diagnostics
python tests/simple_test_runner.py --fast

# Run only unit tests
python tests/simple_test_runner.py --unit
```

#### Basic Diagnostic Tests
For quick system validation:
```bash
# Run basic Python and import tests
python tests/test_basic.py
```

### ðŸ”§ Troubleshooting Test Issues

#### Pytest Import Errors
If you see `ModuleNotFoundError: No module named 'pytest'`:

1. **Use the simple test runner** (recommended):
   ```bash
   python tests/simple_test_runner.py --fast
   ```

2. **Install pytest in your environment**:
   ```bash
   # For UV (if using UV virtual env)
   uv pip install pytest pytest-asyncio
   
   # For pip
   pip install pytest pytest-asyncio
   
   # For conda
   conda install pytest pytest-asyncio
   ```

3. **Use the fixed test runner** (automatically handles missing pytest):
   ```bash
   python tests/test_runner.py --unit
   ```

#### Environment Setup Issues
The test runners have been designed to handle common environment issues:
- **Automatic fallback**: If pytest import fails, falls back to subprocess
- **Path handling**: Automatically sets up Python paths for imports
- **Mock environments**: Sets up test API keys and configurations
- **Timeout protection**: Prevents hanging tests with timeouts

#### Known Test Issues and Solutions

##### 1. LiteLLM Attribute Errors
**Issue**: `AttributeError: <module 'praisonaiagents.llm.llm'> does not have the attribute 'litellm'`

**Cause**: Some tests attempt to mock `praisonaiagents.llm.llm.litellm` but this attribute path may not exist in the current codebase structure.

**Solution**: These are primarily in integration tests for base URL mapping. The tests may need updates to match the current code structure.

##### 2. Agent Attribute Errors  
**Issue**: `AttributeError: 'Agent' object has no attribute 'llm'` or missing `knowledge_config`

**Cause**: Test expectations don't match the current Agent class implementation.

**Solution**: Tests may need updating to reflect the current Agent class API.

##### 3. DuckDuckGo Rate Limiting
**Issue**: `Error during DuckDuckGo search: https://lite.duckduckgo.com/lite/ 202 Ratelimit`

**Cause**: External API rate limiting during test execution.

**Solution**: Tests include proper mocking to avoid external dependencies.

##### 4. Legacy Test Output Format
**Issue**: `TypeError: argument of type 'NoneType' is not iterable` in legacy tests

**Cause**: Some example functions return `None` instead of expected string outputs.

**Solution**: Legacy tests have been updated to handle various return types.

#### Running Tests with Known Issues

For the most reliable test experience:

```bash
# Run only the stable core tests
python tests/test_runner.py --unit --markers "not slow and not integration"

# Run basic functionality tests (most reliable)
python tests/simple_test_runner.py --fast

# Run specific test files that are known to work
pytest tests/unit/agent/test_type_casting.py -v
pytest tests/unit/agent/test_mini_agents_fix.py -v
```

### Using Pytest Directly
```bash
# Run all unit tests
pytest tests/unit/ -v

# Run specific test files
pytest tests/unit/test_core_agents.py -v
pytest tests/integration/test_mcp_integration.py -v

# Run with coverage
pytest tests/ --cov=praisonaiagents --cov-report=html

# Run async tests only
pytest tests/ -k "async" -v

# Run with specific markers
pytest tests/ -m "not slow" -v
```

### GitHub Actions
The comprehensive test suite runs automatically on push/pull request with:
- Multiple Python versions (3.9, 3.10, 3.11)
- All test categories
- Coverage reporting
- Performance benchmarking
- Example script validation

**Note**: GitHub Actions may show some test failures due to:
- External API rate limits
- Evolving codebase with comprehensive test coverage
- Integration tests for experimental features

The key indicator is that core functionality tests pass and the build completes successfully.

## ðŸ”§ Key Features Tested

### Core Functionality
- âœ… Agent creation and configuration
- âœ… Task management and execution
- âœ… LLM integrations (OpenAI, Anthropic, Gemini, Ollama, DeepSeek)
- âœ… Multi-agent workflows (sequential, hierarchical, workflow)

### Advanced Features
- âœ… **Async Operations**: Async agents, tasks, and tools
- âœ… **RAG (Retrieval Augmented Generation)**: Knowledge bases, vector stores
- âœ… **MCP (Model Context Protocol)**: Server connections and tool execution
- âœ… **Memory Systems**: Persistent memory and knowledge updates
- âœ… **Multi-modal Tools**: Image, audio, and document processing

### Integrations
- âœ… **Search Tools**: DuckDuckGo, web scraping
- âœ… **UI Frameworks**: Gradio, Streamlit, Chainlit
- âœ… **API Endpoints**: REST API simulation and testing
- âœ… **Vector Stores**: ChromaDB, Pinecone, Weaviate support

### Error Handling & Performance
- âœ… **Error Recovery**: Tool failures, connection errors
- âœ… **Performance**: Agent creation, import speed
- âœ… **Compatibility**: Base URL mapping, provider switching

## ðŸ“Š Test Configuration

### Fixtures (`conftest.py`)
Common test fixtures available across all tests:
- `mock_llm_response`: Mock LLM API responses
- `sample_agent_config`: Standard agent configuration
- `sample_task_config`: Standard task configuration
- `mock_vector_store`: Mock vector store operations
- `mock_duckduckgo`: Mock search functionality
- `temp_directory`: Temporary file system for tests

### Environment Variables
Tests automatically set up mock environment variables:
- `OPENAI_API_KEY=test-key`
- `ANTHROPIC_API_KEY=test-key`
- `GOOGLE_API_KEY=test-key`

### Markers
Custom pytest markers for test organization:
- `@pytest.mark.asyncio`: Async tests
- `@pytest.mark.slow`: Long-running tests
- `@pytest.mark.integration`: Integration tests
- `@pytest.mark.unit`: Unit tests

## ðŸ” Adding New Tests

### 1. Unit Tests
Add to `tests/unit/` for isolated functionality:
```python
def test_new_feature(sample_agent_config):
    """Test new feature functionality."""
    agent = Agent(**sample_agent_config)
    result = agent.new_feature()
    assert result is not None
```

### 2. Integration Tests
Add to `tests/integration/` for complex workflows:
```python
@pytest.mark.asyncio
async def test_complex_workflow(mock_vector_store):
    """Test complex multi-component workflow."""
    # Setup multiple components
    # Test interaction between them
    assert workflow_result.success is True
```

### 3. Async Tests
Use the `@pytest.mark.asyncio` decorator:
```python
@pytest.mark.asyncio
async def test_async_functionality():
    """Test async operations."""
    result = await async_function()
    assert result is not None
```

## ðŸ“ˆ Coverage Goals

- **Unit Tests**: 90%+ coverage of core functionality
- **Integration Tests**: All major feature combinations
- **Error Handling**: All exception paths tested
- **Performance**: Benchmarks for critical operations

## ðŸ“Š Interpreting Test Results

### Expected Test Status
Due to the comprehensive nature of the test suite and some evolving APIs:

- **âœ… Always Pass**: Basic agent creation, type casting, async tools, UI configurations
- **âš ï¸ May Fail**: LiteLLM integration tests, some RAG tests, external API dependent tests
- **ðŸ”„ In Development**: MCP integration tests, advanced agent orchestration

### Success Criteria
A successful test run should have:
- âœ… Core agent functionality working
- âœ… Basic task creation and execution
- âœ… Tool integration capabilities
- âœ… UI framework configurations

### Test Result Summary Example
```
54 passed, 25 failed, 28 warnings
```
This is **normal and expected** during development. The key metrics are:
- Core functionality tests passing
- No critical import or setup failures
- Warnings are generally acceptable (deprecated dependencies, etc.)

## ðŸ› ï¸ Dependencies

### Core Testing
- `pytest`: Test framework
- `pytest-asyncio`: Async test support
- `pytest-cov`: Coverage reporting

### Mocking
- `unittest.mock`: Built-in mocking
- Mock external APIs and services

### Test Data
- Temporary directories for file operations
- Mock configurations for all integrations
- Sample data for various scenarios

## ðŸ“ Best Practices

1. **Isolation**: Each test should be independent
2. **Mocking**: Mock external dependencies and APIs
3. **Naming**: Clear, descriptive test names
4. **Documentation**: Document complex test scenarios
5. **Performance**: Keep unit tests fast (under 1s each)
6. **Coverage**: Aim for high coverage of critical paths
7. **Maintainability**: Regular test maintenance and updates

## ðŸ”„ Continuous Integration

The test suite integrates with GitHub Actions for:
- Automated testing on all PRs
- Multi-Python version compatibility
- Performance regression detection
- Test result artifacts and reporting

## âš¡ Recent Improvements

### Pytest Import Issue Fixes
The testing framework has been enhanced to handle common import issues:

#### Problem
- Original `test_runner.py` had `import pytest` at the top level
- When pytest wasn't available in the Python environment, tests failed immediately
- Different package managers (uv, pip, conda) install packages in different locations

#### Solutions Implemented

1. **Fixed Test Runner** (`tests/test_runner.py`):
   - âœ… Moved pytest import inside functions (conditional import)
   - âœ… Added automatic fallback to subprocess when pytest import fails
   - âœ… Maintains all original functionality while being more robust

2. **Simple Test Runner** (`tests/simple_test_runner.py`):
   - âœ… Works entirely without pytest dependency at import time
   - âœ… Uses subprocess to run pytest commands
   - âœ… Includes fast diagnostic tests and timeout protection
   - âœ… Perfect for environments where pytest isn't properly installed

3. **Basic Diagnostic Script** (`tests/test_basic.py`):
   - âœ… Tests basic Python imports and praisonaiagents functionality
   - âœ… Runs legacy examples to verify core functionality
   - âœ… Provides detailed diagnostic information

#### Backward Compatibility
- âœ… All existing tests remain unchanged
- âœ… GitHub Actions workflows continue to work
- âœ… Legacy test.py still runs as before
- âœ… Complete backward compatibility maintained

## ðŸ“ž Support

For questions about testing:
1. Check this README for guidance
2. Review existing tests for patterns
3. Check the `conftest.py` for available fixtures
4. Run `python tests/test_runner.py --help` for options
5. For import issues, try `python tests/simple_test_runner.py --fast`

### Reporting Test Issues

**When to report an issue:**
- âœ… All tests fail due to import errors
- âœ… Basic agent creation fails
- âœ… Core functionality completely broken
- âœ… Test runner scripts don't execute

**Normal behavior (not issues):**
- âŒ Some integration tests fail (25-30% failure rate expected)
- âŒ External API rate limiting (DuckDuckGo, etc.)
- âŒ LiteLLM attribute errors in specific tests
- âŒ Deprecation warnings from dependencies

**Quick Health Check:**
```bash
# This should work without major issues
python tests/simple_test_runner.py --fast

# If this fails, there may be a real problem
python tests/test_basic.py
``` 

--- src/praisonai-agents/praisonaiagents/session/README.md ---
# Session Persistence for PraisonAI Agents

This module provides automatic session persistence with zero configuration.

## Quick Start

```python
from praisonaiagents import Agent

# With session persistence (auto-enabled)
agent = Agent(
    name="Assistant",
    session_id="my-session-123"
)
agent.start("Hello, my name is Alice")

# Later, in a new process - history is restored automatically
agent = Agent(
    name="Assistant", 
    session_id="my-session-123"
)
agent.start("What is my name?")  # Agent remembers: "Alice"
```

## How It Works

When you provide a `session_id` to an Agent:

1. **Automatic Persistence**: Conversation history is automatically saved to disk
2. **Automatic Restoration**: When a new Agent is created with the same `session_id`, history is restored
3. **Zero Configuration**: No database setup required - uses JSON files by default

### Default Storage Location

Sessions are stored in: `~/.praison/sessions/{session_id}.json`

### Session File Format

```json
{
  "session_id": "my-session-123",
  "messages": [
    {"role": "user", "content": "Hello", "timestamp": 1704153600.0},
    {"role": "assistant", "content": "Hi there!", "timestamp": 1704153601.5}
  ],
  "created_at": "2026-01-02T04:00:00+00:00",
  "updated_at": "2026-01-02T04:01:00+00:00",
  "agent_name": "Assistant"
}
```

## Behavior Matrix

| Scenario | Behavior |
|----------|----------|
| `session_id` provided, no DB | JSON persistence (auto) |
| `session_id` provided, with DB | DB adapter used |
| No `session_id`, same Agent instance | In-memory only |
| No `session_id`, new Agent instance | No history |

## Advanced Usage

### Direct Session Store Access

```python
from praisonaiagents.session import get_default_session_store

store = get_default_session_store()

# Add messages
store.add_user_message("session-123", "Hello")
store.add_assistant_message("session-123", "Hi there!")

# Get history
history = store.get_chat_history("session-123")
# [{"role": "user", "content": "Hello"}, {"role": "assistant", "content": "Hi there!"}]

# List all sessions
sessions = store.list_sessions()

# Delete a session
store.delete_session("session-123")
```

### Custom Session Directory

```python
from praisonaiagents.session import DefaultSessionStore

store = DefaultSessionStore(
    session_dir="/custom/path/sessions",
    max_messages=200,  # Default: 100
    lock_timeout=10.0,  # Default: 5.0 seconds
)
```

### Using with DB Adapter

When a DB adapter is provided, it takes precedence over JSON persistence:

```python
from praisonaiagents import Agent
from praisonai.db import PostgresAdapter

agent = Agent(
    name="Assistant",
    session_id="my-session",
    db=PostgresAdapter(connection_string="...")
)
```

## Multi-Process Safety

The session store uses file locking to ensure safe concurrent access:

- **Unix**: Uses `fcntl.flock()` for file locking
- **Windows**: Uses `msvcrt.locking()` for file locking
- **Atomic Writes**: Uses temp file + rename to prevent corruption

Multiple processes can safely read/write to the same session.

## Context Caching

For cost optimization, use `prompt_caching=True` with Anthropic models:

```python
agent = Agent(
    name="Assistant",
    session_id="my-session",
    prompt_caching=True,  # Enables Anthropic prompt caching
)
```

This caches the system prompt, reducing token costs for repeated conversations.

## API Reference

### DefaultSessionStore

```python
class DefaultSessionStore:
    def __init__(
        self,
        session_dir: Optional[str] = None,  # Default: ~/.praison/sessions/
        max_messages: int = 100,
        lock_timeout: float = 5.0,
    ): ...
    
    def add_message(self, session_id: str, role: str, content: str) -> bool: ...
    def add_user_message(self, session_id: str, content: str) -> bool: ...
    def add_assistant_message(self, session_id: str, content: str) -> bool: ...
    def get_chat_history(self, session_id: str, max_messages: int = None) -> List[Dict]: ...
    def get_session(self, session_id: str) -> SessionData: ...
    def clear_session(self, session_id: str) -> bool: ...
    def delete_session(self, session_id: str) -> bool: ...
    def list_sessions(self, limit: int = 50) -> List[Dict]: ...
    def session_exists(self, session_id: str) -> bool: ...
```

### SessionData

```python
@dataclass
class SessionData:
    session_id: str
    messages: List[SessionMessage]
    created_at: str
    updated_at: str
    agent_name: Optional[str]
    user_id: Optional[str]
    metadata: Dict[str, Any]
    
    def get_chat_history(self, max_messages: int = None) -> List[Dict[str, str]]: ...
```

### SessionMessage

```python
@dataclass
class SessionMessage:
    role: str  # "user", "assistant", "system"
    content: str
    timestamp: float
    metadata: Dict[str, Any]
```


--- src/praisonai-agents/praisonaiagents/telemetry/README.md ---
# PraisonAI Agents Telemetry

This module provides minimal, privacy-focused telemetry for PraisonAI Agents.

## Privacy Guarantees

- **No personal data is collected** - No prompts, responses, or user content
- **Anonymous metrics only** - Usage counts and feature adoption
- **Disabled by default** - Requires explicit opt-in to enable
- **Transparent collection** - See exactly what's tracked below

## What We Collect

We collect only anonymous usage metrics:
- Number of agent executions
- Number of task completions  
- Tool usage (names only, no arguments)
- Error types (no error messages)
- Framework version and OS type
- Anonymous session ID (regenerated each run)

## Enabling Telemetry

**NEW BEHAVIOR**: Telemetry is now **disabled by default** for better privacy. To enable telemetry, you must explicitly opt-in.

### 1. Environment Variables (Recommended)

To enable telemetry, set any of these environment variables:
```bash
export PRAISONAI_TELEMETRY_ENABLED=true
export PRAISONAI_PERFORMANCE_ENABLED=true
```

### Legacy Disable Flags (Still Supported)

The legacy disable flags still work for backward compatibility:
```bash
export PRAISONAI_TELEMETRY_DISABLED=true
export PRAISONAI_DISABLE_TELEMETRY=true
export DO_NOT_TRACK=true  # Universal standard
export PRAISONAI_PERFORMANCE_DISABLED=true
```

### 2. Programmatically Enable

```python
from praisonaiagents.telemetry import enable_telemetry
enable_telemetry()
```

### 3. At Runtime

```python
from praisonaiagents.telemetry import get_telemetry
telemetry = get_telemetry()
telemetry.enabled = True  # Enable telemetry (disabled by default)
```

### 4. Programmatically Disable (Legacy)

```python
from praisonaiagents.telemetry import disable_telemetry
disable_telemetry()
```

## Usage

The telemetry module integrates automatically with PraisonAI Agents:

```python
import os
from praisonaiagents import Agent, Task, PraisonAIAgents

# Enable telemetry (disabled by default)
os.environ['PRAISONAI_TELEMETRY_ENABLED'] = 'true'

agent = Agent(name="MyAgent", role="Assistant")
task = Task(description="Help user", agent=agent)

workflow = PraisonAIAgents(agents=[agent], tasks=[task])
result = workflow.start()

# Check telemetry metrics
from praisonaiagents.telemetry import get_telemetry
telemetry = get_telemetry()
print(telemetry.get_metrics())
```

## Implementation Details

The telemetry implementation is minimal and lightweight:
- No external dependencies required
- No network calls in current implementation
- Metrics stored in memory only
- Future versions may send to PostHog or similar privacy-focused services

## Backward Compatibility

The module maintains compatibility with the previous telemetry interface:

```python
from praisonaiagents.telemetry import TelemetryCollector

collector = TelemetryCollector()
collector.start()

with collector.trace_agent_execution("MyAgent"):
    # Agent execution code
    pass

collector.stop()
```

## Contributing

When contributing to telemetry:
1. Never collect personal data or user content
2. Always make new metrics opt-out
3. Document what's collected
4. Keep the implementation minimal

## Future Plans

- Integration with PostHog for anonymous analytics
- Aggregate usage statistics dashboard
- Opt-in detailed performance metrics
- Self-hosted telemetry endpoint option

--- src/praisonai-agents/praisonaiagents/tools/README.md ---
# PraisonAI Tools Guide

Welcome to the PraisonAI Tools directory! This guide will help you understand how our tools work and how to create new ones, whether you're a beginner or an experienced programmer.

## What is a Tool?

A tool is a piece of code that helps our AI agents perform specific tasks. Think of tools as special abilities that we give to our agents. For example:
- An internet search tool lets agents search the web
- A stock market tool lets agents check stock prices
- A weather tool lets agents check the weather

## Plugin System (NEW!)

PraisonAI now supports a plugin system for creating and distributing tools. External developers can create pip-installable tool packages!

### Quick Start

```python
from praisonaiagents import BaseTool, tool, Agent

# Method 1: Class-based tool
class WeatherTool(BaseTool):
    name = "get_weather"
    description = "Get weather for a location"
    
    def run(self, location: str) -> dict:
        return {"temp": 72, "condition": "sunny"}

# Method 2: Decorator-based tool
@tool
def search(query: str) -> list:
    """Search the web."""
    return [{"title": "Result", "url": "https://..."}]

# Use with Agent
agent = Agent(
    name="Assistant",
    tools=[WeatherTool(), search]
)
```

### Creating a Plugin Package

External developers can create pip-installable plugins:

**1. Create your tool package:**
```
praisonai-weather/
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ src/
â”‚   â””â”€â”€ praisonai_weather/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ tools.py
```

**2. Define your tool in `tools.py`:**
```python
from praisonaiagents import BaseTool

class WeatherTool(BaseTool):
    name = "weather"
    description = "Get current weather"
    
    def run(self, location: str) -> dict:
        # Your implementation
        return {"temp": 72}
```

**3. Register via entry_points in `pyproject.toml`:**
```toml
[project]
name = "praisonai-weather"
version = "1.0.0"

[project.entry-points."praisonaiagents.tools"]
weather = "praisonai_weather.tools:WeatherTool"
```

**4. Users install and use:**
```bash
pip install praisonai-weather
```

```python
from praisonaiagents import Agent

# Tool is auto-discovered!
agent = Agent(tools=["weather"])
```

## Creating New Tools: The Two Approaches

### 1. Function-Based Approach (Simple Tools)

Best for simple tools that do one specific thing. Like a calculator that just adds numbers.

**When to use:**
- Tool does one simple task
- Doesn't need to remember information between uses
- Doesn't need to share information with other tools
- Quick, one-time operations

**Example:**
```python
def internet_search(query: str):
    # Search the internet and return results
    return search_results
```

**Usage:**
```python
from praisonaiagents.tools import internet_search

results = internet_search("AI news")
```

### 2. Class-Based Approach (Complex Tools)

Best for tools that do multiple related things or need to remember information. Like a smart calculator that remembers your previous calculations and can do many different math operations.

**When to use:**
- Tool has multiple related functions
- Needs to remember or share information
- Needs to manage resources efficiently
- Has complex setup requirements

**Example:**
```python
class StockTools:
    def get_stock_price(self, symbol):
        # Get current stock price
        return price
        
    def get_stock_info(self, symbol):
        # Get detailed stock information
        return info
```

**Usage:**
```python
from praisonaiagents.tools import get_stock_price, get_stock_info

price = get_stock_price("AAPL")
info = get_stock_info("AAPL")
```

## How to Choose Your Approach

Ask yourself these questions:

1. **Is your tool doing one simple thing?**
   - Yes â†’ Use Function-Based Approach
   - No â†’ Consider Class-Based Approach

2. **Does your tool need to remember information?**
   - Yes â†’ Use Class-Based Approach
   - No â†’ Use Function-Based Approach

3. **Are your tool's operations related to each other?**
   - Yes â†’ Use Class-Based Approach
   - No â†’ Use Function-Based Approach

4. **Does your tool need to manage resources efficiently?**
   - Yes â†’ Use Class-Based Approach
   - No â†’ Use Function-Based Approach

## Real-World Examples

### Internet Search Tool (Function-Based)
- Does one thing: searches the internet
- Doesn't need to remember previous searches
- Each search is independent
- Simple input/output operation

### SearxNG Search Tool (Function-Based)
- Privacy-focused web search using local SearxNG instance
- Simple search operation with customizable parameters
- Each search is independent and secure
- Alternative to traditional search engines for privacy


### Spider Tool (Function-Based)
- General-purpose web scraping and crawling
- Supports CSS selectors for precise content extraction
- Can crawl multiple pages and extract links/images
- Flexible for various web scraping needs

### Newspaper Tool (Function-Based)
- Specialized for news article extraction
- Extracts article title, text, authors, and publish date
- Includes NLP processing for keywords and summaries
- Categorizes news sources by topic

### Stock Market Tool (Class-Based)
- Does multiple things: check prices, get company info, get historical data
- Remembers stock information to avoid repeated downloads
- Operations are related (all about stocks)
- Manages connections efficiently

## Getting Started

1. **Choose Your Approach** based on the guidelines above

2. **Create Your Tool File**:
   - Name it descriptively (e.g., `weather_tools.py`)
   - Place it in the `praisonaiagents/tools` directory

3. **Write Your Tool**:
   - Add clear documentation
   - Include type hints for better understanding
   - Handle errors gracefully

4. **Test Your Tool**:
   - Make sure it works as expected
   - Test error cases
   - Check performance

## Best Practices

1. **Documentation**:
   - Explain what your tool does
   - Provide examples
   - List any requirements

2. **Error Handling**:
   - Always handle possible errors
   - Return helpful error messages
   - Don't let your tool crash

3. **Performance**:
   - Keep it efficient
   - Don't waste resources
   - Cache when helpful

4. **User-Friendly**:
   - Make it easy to use
   - Use clear function/method names
   - Keep it simple

## Need Help?

- Check existing tools for examples
- Ask in our community
- Read the documentation
- Don't hesitate to ask questions!

Remember: The goal is to make tools that are easy to use and maintain. Choose the approach that makes the most sense for your specific tool's needs.


--- src/praisonai-ts/src/tools/README.md ---
