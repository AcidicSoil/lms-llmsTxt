# llms-full (private-aware)
> Built from GitHub files and website pages. Large files may be truncated.

--- .clinerules/cline-overview.md ---
# Cline Extension Architecture & Development Guide

## Project Overview

Cline is a VSCode extension that provides AI assistance through a combination of a core extension backend and a React-based webview frontend. The extension is built with TypeScript and follows a modular architecture pattern.

## Architecture Overview

```mermaid
graph TB
    subgraph VSCodeExtensionHost[VSCode Extension Host]
        subgraph CoreExtension[Core Extension]
            ExtensionEntry[Extension Entry<br/>src/extension.ts]
            WebviewProvider[WebviewProvider<br/>src/core/webview/index.ts]
            Controller[Controller<br/>src/core/controller/index.ts]
            Task[Task<br/>src/core/task/index.ts]
            GlobalState[VSCode Global State]
            SecretsStorage[VSCode Secrets Storage]
            McpHub[McpHub<br/>src/services/mcp/McpHub.ts]
        end

        subgraph WebviewUI[Webview UI]
            WebviewApp[React App<br/>webview-ui/src/App.tsx]
            ExtStateContext[ExtensionStateContext<br/>webview-ui/src/context/ExtensionStateContext.tsx]
            ReactComponents[React Components]
        end

        subgraph Storage
            TaskStorage[Task Storage<br/>Per-Task Files & History]
            CheckpointSystem[Git-based Checkpoints]
        end

        subgraph apiProviders[API Providers]
            AnthropicAPI[Anthropic]
            OpenRouterAPI[OpenRouter]
            BedrockAPI[AWS Bedrock]
            OtherAPIs[Other Providers]
        end

        subgraph MCPServers[MCP Servers]
            ExternalMcpServers[External MCP Servers]
        end
    end

    %% Core Extension Data Flow
    ExtensionEntry --> WebviewProvider
    WebviewProvider --> Controller
    Controller --> Task
    Controller --> McpHub
    Task --> GlobalState
    Task --> SecretsStorage
    Task --> TaskStorage
    Task --> CheckpointSystem
    Task --> |API Requests| apiProviders
    McpHub --> |Connects to| ExternalMcpServers
    Task --> |Uses| McpHub

    %% Webview Data Flow
    WebviewApp --> ExtStateContext
    ExtStateContext --> ReactComponents

    %% Bidirectional Communication
    WebviewProvider <-->|postMessage| ExtStateContext

    style GlobalState fill:#f9f,stroke:#333,stroke-width:2px
    style SecretsStorage fill:#f9f,stroke:#333,stroke-width:2px
    style ExtStateContext fill:#bbf,stroke:#333,stroke-width:2px
    style WebviewProvider fill:#bfb,stroke:#333,stroke-width:2px
    style McpHub fill:#bfb,stroke:#333,stroke-width:2px
    style apiProviders fill:#fdb,stroke:#333,stroke-width:2px
```

## Definitions 

- **Core Extension**: Anything inside the src folder, organized into modular components
- **Core Extension State**: Managed by the Controller class in src/core/controller/index.ts, which serves as the single source of truth for the extension's state. It manages multiple types of persistent storage (global state, workspace state, and secrets), handles state distribution to both the core extension and webview components, and coordinates state across multiple extension instances. This includes managing API configurations, task history, settings, and MCP configurations.
- **Webview**: Anything inside the webview-ui. All the react or view's seen by the user and user interaction components
- **Webview State**: Managed by ExtensionStateContext in webview-ui/src/context/ExtensionStateContext.tsx, which provides React components with access to the extension's state through a context provider pattern. It maintains local state for UI components, handles real-time updates through message events, manages partial message updates, and provides methods for state modifications. The context includes extension version, messages, task history, theme, API configurations, MCP servers, marketplace catalog, and workspace file paths. It synchronizes with the core extension through VSCode's message passing system and provides type-safe access to state through a custom hook (useExtensionState).

### Core Extension Architecture

The core extension follows a clear hierarchical structure:

1. **WebviewProvider** (src/core/webview/index.ts): Manages the webview lifecycle and communication
2. **Controller** (src/core/controller/index.ts): Handles webview messages and task management
3. **Task** (src/core/task/index.ts): Executes API requests and tool operations

This architecture provides clear separation of concerns:
- WebviewProvider focuses on VSCode webview integration
- Controller manages state and coordinates tasks
- Task handles the execution of AI requests and tool operations

### WebviewProvider Implementation

The WebviewProvider class in `src/core/webview/index.ts` is responsible for:

- Managing multiple active instances through a static set (`activeInstances`)
- Handling webview lifecycle events (creation, visibility changes, disposal)
- Implementing HTML content generation with proper CSP headers
- Supporting Hot Module Replacement (HMR) for development
- Setting up message listeners between the webview and extension

The WebviewProvider maintains a reference to the Controller and delegates message handling to it. It also handles the creation of both sidebar and tab panel webviews, allowing Cline to be used in different contexts within VSCode.

### Core Extension State

The `Controller` class manages multiple types of persistent storage:

- **Global State:** Stored across all VSCode instances. Used for settings and data that should persist globally.
- **Workspace State:** Specific to the current workspace. Used for task-specific data and settings.
- **Secrets:** Secure storage for sensitive information like API keys.

The `Controller` handles the distribution of state to both the core extension and webview components. It also coordinates state across multiple extension instances, ensuring consistency.

State synchronization between instances is handled through:
- File-based storage for task history and conversation data
- VSCode's global state API for settings and configuration
- Secrets storage for sensitive information
- Event listeners for file changes and configuration updates

The Controller implements methods for:
- Saving and loading task state
- Managing API configurations
- Handling user authentication
- Coordinating MCP server connections
- Managing task history and checkpoints

### Webview State

The `ExtensionStateContext` in `webview-ui/src/context/ExtensionStateContext.tsx` provides React components with access to the extension's state. It uses a context provider pattern and maintains local state for UI components. The context includes:

- Extension version
- Messages
- Task history
- Theme
- API configurations
- MCP servers
- Marketplace catalog
- Workspace file paths

It synchronizes with the core extension through VSCode's message passing system and provides type-safe access to the state via a custom hook (`useExtensionState`).

The ExtensionStateContext handles:
- Real-time updates through message events
- Partial message updates for streaming content
- State modifications through setter methods
- Type-safe access to state through a custom hook

## API Provider System

Cline supports multiple AI providers through a modular API provider system. Each provider is implemented as a separate module in the `src/api/providers/` directory and follows a common interface.

### API Provider Architecture

The API system consists of:

1. **API Handlers**: Provider-specific implementations in `src/api/providers/`
2. **API Transformers**: Stream transformation utilities in `src/api/transform/`
3. **API Configuration**: User settings for API keys and endpoints
4. **API Factory**: Builder function to create the appropriate handler

Key providers include:
- **Anthropic**: Direct integration with Claude models
- **OpenRouter**: Meta-provider supporting multiple model providers
- **AWS Bedrock**: Integration with Amazon's AI services
- **Gemini**: Google's AI models
- **Cerebras**: High-performance inference with Llama, Qwen, and DeepSeek models
- **Ollama**: Local model hosting
- **LM Studio**: Local model hosting
- **VSCode LM**: VSCode's built-in language models

### API Configuration Management

API configurations are stored securely:
- API keys are stored in VSCode's secrets storage
- Model selections and non-sensitive settings are stored in global state
- The Controller manages switching between providers and updating configurations

The system supports:
- Secure storage of API keys
- Model selection and configuration
- Automatic retry and error handling
- Token usage tracking and cost calculation
- Context window management

### Plan/Act Mode API Configuration

Cline supports separate model configurations for Plan and Act modes:
- Different models can be used for planning vs. execution
- The system preserves model selections when switching modes
- The Controller handles the transition between modes and updates the API configuration accordingly

## Task Execution System

The Task class is responsible for executing AI requests and tool operations. Each task runs in its own instance of the Task class, ensuring isolation and proper state management.

### Task Execution Loop

The core task execution loop follows this pattern:

```typescript
class Task {
  async initiateTaskLoop(userContent: UserContent, isNewTask: boolean) {
    while (!this.abort) {
      // 1. Make API request and stream response
      const stream = this.attemptApiRequest()
      
      // 2. Parse and present content blocks
      for await (const chunk of stream) {
        switch (chunk.type) {
          case "text":
            // Parse into content blocks
            this.assistantMessageContent = parseAssistantMessageV2(chunk.text)
            // Present blocks to user
            await this.presentAssistantMessage()
            break
        }
      }
      
      // 3. Wait for tool execution to complete
      await pWaitFor(() => this.userMessageContentReady)
      
      // 4. Continue loop with tool result
      const recDidEndLoop = await this.recursivelyMakeClineRequests(
        this.userMessageContent
      )
    }
  }
}
```

### Message Streaming System

The streaming system handles real-time updates and partial content:

```typescript
class Task {
  async presentAssistantMessage() {
    // Handle streaming locks to prevent race conditions
    if (this.presentAssistantMessageLocked) {
      this.presentAssistantMessageHasPendingUpdates = true
      return
    }
    this.presentAssistantMessageLocked = true

    // Present current content block
    const block = this.assistantMessageContent[this.currentStreamingContentIndex]
    
    // Handle different types of content
    switch (block.type) {
      case "text":
        await this.say("text", content, undefined, block.partial)
        break
      case "tool_use":
        // Handle tool execution
        break
    }

    // Move to next block if complete
    if (!block.partial) {
      this.currentStreamingContentIndex++
    }
  }
}
```

### Tool Execution Flow

Tools follow a strict execution pattern:

```typescript
class Task {
  async executeToolWithApproval(block: ToolBlock) {
    // 1. Check auto-approval settings
    if (this.shouldAutoApproveTool(block.name)) {
      await this.say("tool", message)
      this.consecutiveAutoApprovedRequestsCount++
    } else {
      // 2. Request user approval
      const didApprove = await askApproval("tool", message)
      if (!didApprove) {
        this.didRejectTool = true
        return
      }
    }

    // 3. Execute tool
    const result = await this.executeTool(block)

    // 4. Save checkpoint
    await this.saveCheckpoint()

    // 5. Return result to API
    return result
  }
}
```

### Error Handling & Recovery

The system includes robust error handling:

```typescript
class Task {
  async handleError(action: string, error: Error) {
    // 1. Check if task was abandoned
    if (this.abandoned) return
    
    // 2. Format error message
    const errorString = `Error ${action}: ${error.message}`
    
    // 3. Present error to user
    await this.say("error", errorString)
    
    // 4. Add error to tool results
    pushToolResult(formatResponse.toolError(errorString))
    
    // 5. Cleanup resources
    await this.diffViewProvider.revertChanges()
    await this.browserSession.closeBrowser()
  }
}
```

### API Request & Token Management

The Task class handles API requests with built-in retry, streaming, and token management:

```typescript
class Task {
  async *attemptApiRequest(previousApiReqIndex: number): ApiStream {
    // 1. Wait for MCP servers to connect
    await pWaitFor(() => this.controllerRef.deref()?.mcpHub?.isConnecting !== true)

    // 2. Manage context window
    const previousRequest = this.clineMessages[previousApiReqIndex]
    if (previousRequest?.text) {
      const { tokensIn, tokensOut } = JSON.parse(previousRequest.text || "{}")
      const totalTokens = (tokensIn || 0) + (tokensOut || 0)
      
      // Truncate conversation if approaching context limit
      if (totalTokens >= maxAllowedSize) {
        this.conversationHistoryDeletedRange = this.contextManager.getNextTruncationRange(
          this.apiConversationHistory,
          this.conversationHistoryDeletedRange,
          totalTokens / 2 > maxAllowedSize ? "quarter" : "half"
        )
      }
    }

    // 3. Handle streaming with automatic retry
    try {
      this.isWaitingForFirstChunk = true
      const firstChunk = await iterator.next()
      yield firstChunk.value
      this.isWaitingForFirstChunk = false
      
      // Stream remaining chunks
      yield* iterator
    } catch (error) {
      // 4. Error handling with retry
      if (isOpenRouter && !this.didAutomaticallyRetryFailedApiRequest) {
        await setTimeoutPromise(1000)
        this.didAutomaticallyRetryFailedApiRequest = true
        yield* this.attemptApiRequest(previousApiReqIndex)
        return
      }
      
      // 5. Ask user to retry if automatic retry failed
      const { response } = await this.ask(
        "api_req_failed",
        this.formatErrorWithStatusCode(error)
      )
      if (response === "yesButtonClicked") {
        await this.say("api_req_retried")
        yield* this.attemptApiRequest(previousApiReqIndex)
        return
      }
    }
  }
}
```

Key features:

1. **Context Window Management**
   - Tracks token usage across requests
   - Automatically truncates conversation when needed
   - Preserves important context while freeing space
   - Handles different model context sizes

2. **Streaming Architecture**
   - Real-time chunk processing
   - Partial content handling
   - Race condition prevention
   - Error recovery during streaming

3. **Error Handling**
   - Automatic retry for transient failures
   - User-prompted retry for persistent issues
   - Detailed error reporting
   - State cleanup on failure

4. **Token Tracking**
   - Per-request token counting
   - Cumulative usage tracking
   - Cost calculation
   - Cache hit monitoring

### Context Management System

The Context Management System handles conversation history truncation to prevent context window overflow errors. Implemented in the `ContextManager` class, it ensures long-running conversations remain within model context limits while preserving critical context.

Key features:

1. **Model-Aware Sizing**: Dynamically adjusts based on different model context windows (64K for DeepSeek, 128K for most models, 200K for Claude).

2. **Proactive Truncation**: Monitors token usage and preemptively truncates conversations when approaching limits, maintaining buffers of 27K-40K tokens depending on the model.

3. **Intelligent Preservation**: Always preserves the original task message and maintains the user-assistant conversation structure when truncating.

4. **Adaptive Strategies**: Uses different truncation strategies based on context pressure - removing half of the conversation for moderate pressure or three-quarters for severe pressure.

5. **Error Recovery**: Includes specialized detection for context window errors from different providers with automatic retry and more aggressive truncation when needed.

### Task State & Resumption

The Task class provides robust task state management and resumption capabilities:

```typescript
class Task {
  async resumeTaskFromHistory() {
    // 1. Load saved state
    this.clineMessages = await getSavedClineMessages(this.getContext(), this.taskId)
    this.apiConversationHistory = await getSavedApiConversationHistory(this.getContext(), this.taskId)

    // 2. Handle interrupted tool executions
    const lastMessage = this.apiConversationHistory[this.apiConversationHistory.length - 1]
    if (lastMessage.role === "assistant") {
      const toolUseBlocks = content.filter(block => block.type === "tool_use")
      if (toolUseBlocks.length > 0) {
        // Add interrupted tool responses
        const toolResponses = toolUseBlocks.map(block => ({
          type: "tool_result",
          tool_use_id: block.id,
          content: "Task was interrupted before this tool call could be completed."
        }))
        modifiedOldUserContent = [...toolResponses]
      }
    }

    // 3. Notify about interruption
    const agoText = this.getTimeAgoText(lastMessage?.ts)
    newUserContent.push({
      type: "text",
      text: `[TASK RESUMPTION] This task was interrupted ${agoText}. It may or may not be complete, so please reassess the task context.`
    })

    // 4. Resume task execution
    await this.initiateTaskLoop(newUserContent, false)
  }

  private async saveTaskState() {
    // Save conversation history
    await saveApiConversationHistory(this.getContext(), this.taskId, this.apiConversationHistory)
    await saveClineMessages(this.getContext(), this.taskId, this.clineMessages)
    
    // Create checkpoint
    const commitHash = await this.checkpointTracker?.commit()
    
    // Update task history
    await this.controllerRef.deref()?.updateTaskHistory({
      id: this.taskId,
      ts: lastMessage.ts,
      task: taskMessage.text,
      // ... other metadata
    })
  }
}
```

Key aspects of task state management:

1. **Task Persistence**
   - Each task has a unique ID and dedicated storage directory
   - Conversation history is saved after each message
   - File changes are tracked through Git-based checkpoints
   - Terminal output and browser state are preserved

2. **State Recovery**
   - Tasks can be resumed from any point
   - Interrupted tool executions are handled gracefully
   - File changes can be restored from checkpoints
   - Context is preserved across VSCode sessions

3. **Workspace Synchronization**
   - File changes are tracked through Git
   - Checkpoints are created after tool executions
   - State can be restored to any checkpoint
   - Changes can be compared between checkpoints

4. **Error Recovery**
   - Failed API requests can be retried
   - Interrupted tool executions are marked
   - Resources are cleaned up properly
   - User is notified of state changes

## Plan/Act Mode System

Cline implements a dual-mode system that separates planning from execution:

### Mode Architecture

The Plan/Act mode system consists of:

1. **Mode State**: Stored in `chatSettings.mode` in the Controller's state
2. **Mode Switching**: Handled by `togglePlanActModeWithChatSettings` in the Controller
3. **Mode-specific Models**: Optional configuration to use different models for each mode
4. **Mode-specific Prompting**: Different system prompts for planning vs. execution

### Mode Switching Process

When switching between modes:

1. The current model configuration is saved to mode-specific state
2. The previous mode's model configuration is restored
3. The Task instance is updated with the new mode
4. The webview is notified of the mode change
5. Telemetry events are captured for analytics

### Plan Mode

Plan mode is designed for:
- Information gathering and context building
- Asking clarifying questions
- Creating detailed execution plans
- Discussing approaches with the user

In Plan mode, the AI uses the `plan_mode_respond` tool to engage in conversational planning without executing actions.

### Act Mode

Act mode is designed for:
- Executing the planned actions
- Using tools to modify files, run commands, etc.
- Implementing the solution
- Providing results and completion feedback

In Act mode, the AI has access to all tools except `plan_mode_respond` and focuses on implementation rather than discussion.

## Data Flow & State Management

### Core Extension Role

The Controller acts as the single source of truth for all persistent state. It:
- Manages VSCode global state and secrets storage
- Coordinates state updates between components
- Ensures state consistency across webview reloads
- Handles task-specific state persistence
- Manages checkpoint creation and restoration

### Terminal Management

The Task class manages terminal instances and command execution:

```typescript
class Task {
  async executeCommandTool(command: string): Promise<[boolean, ToolResponse]> {
    // 1. Get or create terminal
    const terminalInfo = await this.terminalManager.getOrCreateTerminal(cwd)
    terminalInfo.terminal.show()

    // 2. Execute command with output streaming
    const process = this.terminalManager.runCommand(terminalInfo, command)
    
    // 3. Handle real-time output
    let result = ""
    process.on("line", (line) => {
      result += line + "\n"
      if (!didContinue) {
        sendCommandOutput(line)
      } else {
        this.say("command_output", line)
      }
    })

    // 4. Wait for completion or user feedback
    let completed = false
    process.once("completed", () => {
      completed = true
    })

    await process

    // 5. Return result
    if (completed) {
      return [false, `Command executed.\n${result}`]
    } else {
      return [
        false,
        `Command is still running in the user's terminal.\n${result}\n\nYou will be updated on the terminal status and new output in the future.`
      ]
    }
  }
}
```

Key features:
1. **Terminal Instance Management**
   - Multiple terminal support
   - Terminal state tracking (busy/inactive)
   - Process cooldown monitoring
   - Output history per terminal

2. **Command Execution**
   - Real-time output streaming
   - User feedback handling
   - Process state monitoring
   - Error recovery

### Browser Session Management

The Task class handles browser automation through Puppeteer:

```typescript
class Task {
  async executeBrowserAction(action: BrowserAction): Promise<BrowserActionResult> {
    switch (action) {
      case "launch":
        // 1. Launch browser with fixed resolution
        await this.browserSession.launchBrowser()
        return await this.browserSession.navigateToUrl(url)

      case "click":
        // 2. Handle click actions with coordinates
        return await this.browserSession.click(coordinate)

      case "type":
        // 3. Handle keyboard input
        return await this.browserSession.type(text)

      case "close":
        // 4. Clean up resources
        return await this.browserSession.closeBrowser()
    }
  }
}
```

Key aspects:
1. **Browser Control**
   - Fixed 900x600 resolution window
   - Single instance per task lifecycle
   - Automatic cleanup on task completion
   - Console log capture

2. **Interaction Handling**
   - Coordinate-based clicking
   - Keyboard input simulation
   - Screenshot capture
   - Error recovery

## MCP (Model Context Protocol) Integration

### MCP Architecture

The MCP system consists of:

1. **McpHub Class**: Central manager in `src/services/mcp/McpHub.ts`
2. **MCP Connections**: Manages connections to external MCP servers
3. **MCP Settings**: Configuration stored in a JSON file
4. **MCP Marketplace**: Online catalog of available MCP servers
5. **MCP Tools & Resources**: Capabilities exposed by connected servers

The McpHub class:
- Manages the lifecycle of MCP server connections
- Handles server configuration through a settings file
- Provides methods for calling tools and accessing resources
- Implements auto-approval settings for MCP tools
- Monitors server health and handles reconnection

### MCP Server Types

Cline supports two types of MCP server connections:
- **Stdio**: Command-line based servers that communicate via standard I/O
- **SSE**: HTTP-based servers that communicate via Server-Sent Events

### MCP Server Management

The McpHub class provides methods for:
- Discovering and connecting to MCP servers
- Monitoring server health and status
- Restarting servers when needed
- Managing server configurations
- Setting timeouts and auto-approval rules

### MCP Tool Integration

MCP tools are integrated into the Task execution system:
- Tools are discovered and registered at connection time
- The Task class can call MCP tools through the McpHub
- Tool results are streamed back to the AI
- Auto-approval settings can be configured per tool

### MCP Marketplace

The MCP Marketplace provides:
- A catalog of available MCP servers
- One-click installation
- README previews
- Server status monitoring

The Controller class manages MCP servers through the McpHub service:

```typescript
class Controller {
  mcpHub?: McpHub

  constructor(context: vscode.ExtensionContext, webviewProvider: WebviewProvider) {
    this.mcpHub = new McpHub(this)
  }

  async downloadMcp(mcpId: string) {
    // Fetch server details from marketplace
    const response = await axios.post<McpDownloadResponse>(
      "https://api.cline.bot/v1/mcp/download",
      { mcpId },
      {
        headers: { "Content-Type": "application/json" },
        timeout: 10000,
      }
    )

    // Create task with context from README
    const task = `Set up the MCP server from ${mcpDetails.githubUrl}...`

    // Initialize task and show chat view
    await this.initClineWithTask(task)
  }
}
```

## Conclusion

This guide provides a comprehensive overview of the Cline extension architecture, with special focus on state management, data persistence, and code organization. Following these patterns ensures robust feature implementation with proper state handling across the extension's components.

Remember:
- Always persist important state in the extension
- The core extension follows a WebviewProvider -> Controller -> Task flow
- Use proper typing for all state and messages
- Handle errors and edge cases
- Test state persistence across webview reloads
- Follow the established patterns for consistency
- Place new code in appropriate directories
- Maintain clear separation of concerns
- Install dependencies in correct package.json

## Contributing

Contributions to the Cline extension are welcome! Please follow these guidelines:

When adding new tools or API providers, follow the existing patterns in the `src/integrations/` and `src/api/providers/` directories, respectively. Ensure that your code is well-documented and includes appropriate error handling.

The `.clineignore` file allows users to specify files and directories that Cline should not access. When implementing new features, respect the `.clineignore` rules and ensure that your code does not attempt to read or modify ignored files.


--- docs/getting-started/authorizing-with-cline.mdx ---
---
title: "Authorization & Model Selection"
description: "Authenticate with Cline and choose your first AI model"
---

Cline connects to AI models through a **provider**. You have two paths:

<CardGroup cols={2}>
  <Card title="Cline Provider" icon="bolt">
    Sign in with Google, GitHub, or email. No API keys to manage — access multiple models with built-in billing, free options, and early access to new releases.

    **Best for:** Most users, fastest setup
  </Card>
  <Card title="Bring Your Own Key (BYOK)" icon="key">
    Use 3rd party provider or API keys from Anthropic, OpenAI, OpenRouter, or any supported provider. Run models locally with Ollama or LM Studio for complete privacy.

    **Best for:** Enterprise, custom billing, local models
  </Card>
</CardGroup>

<Tip>
  **Watch:** [Selecting Your Model](https://youtu.be/GuPmu5TVtfA) walks through choosing and configuring your first model.
</Tip>

## Setup Steps

<Steps>
  <Step title="Open Settings">
    Click the settings icon in the top-right of the Cline panel.
  </Step>

  <Step title="Select a Provider">
    Choose from the **API Provider** dropdown:
    - **Cline** — simplest setup, no API key needed
    - **OpenRouter** — many models, one API key
    - **Anthropic** — direct Claude access
    - **Ollama / LM Studio** — run models locally
  </Step>

  <Step title="Authenticate">
    **Cline Provider:** Click **Sign In** and authenticate via Google, GitHub, or email. See [OAuth details](#how-oauth-works) below.

    **BYOK:** Paste your API key from your provider's dashboard.

    **Local:** No key needed — just ensure your local server is running.

    <Note>
    API keys are stored in your system's credential manager and sent only to your selected provider. They are never logged or transmitted to Cline's servers.
    </Note>
  </Step>

  <Step title="Choose a Model">
    Select a model from the **Model** dropdown. Consider:
    - **Context window** — how much code the model can process at once
    - **Speed** — smaller models respond faster
    - **Cost** — varies by model; local models are free
  </Step>

  <Step title="Verify">
    Send any message. If Cline responds, you're ready.
  </Step>
</Steps>

## Cline Provider

The Cline Provider gives you one account, one billing relationship, and access to models from Anthropic, OpenAI, Google, and more.

- **No API key juggling** — one sign-in, multiple models
- **Built-in billing** — add credits once, use across all models
- **Free models** — search "free" in the model selector to find no-cost options tagged **FREE**
- **Stealth models** — early access to new releases before they're widely available
- **Always current** — new models added as they launch

### Adding Credits

Click **Add Credits** in Cline settings or visit your [account dashboard](https://app.cline.bot/dashboard). Credits work across all available models.

### How OAuth Works

<Steps>
  <Step title="Sign In">
    Click **Sign In** in Cline settings. Your browser opens to `app.cline.bot`.
  </Step>
  <Step title="Authenticate">
    Choose Google, GitHub, or email.
  </Step>
  <Step title="Return to IDE">
    After authentication, you're redirected back with an authorization code.
  </Step>
  <Step title="Secure Storage">
    Tokens are stored in your IDE's native secret storage (VS Code Secrets, JetBrains Credential Store, etc.).
  </Step>
</Steps>

## Bring Your Own Key (BYOK)

Use your own API keys when you need specific billing arrangements, higher rate limits, access to beta models, or local privacy.

### Cloud Providers

| Provider | Best For | Setup Guide |
|----------|----------|-------------|
| **OpenRouter** | Multiple models, competitive pricing | [Setup](/provider-config/openrouter) |
| **Anthropic** | Direct Claude access | [Setup](/provider-config/anthropic) |
| **Claude Code** | Claude Max/Pro subscription | [Setup](/provider-config/claude-code) |
| **OpenAI** | GPT models | [Setup](/provider-config/openai) |
| **Google Gemini** | Large context windows | [Setup](/provider-config/gcp-vertex-ai) |
| **AWS Bedrock** | Enterprise | [Setup](/provider-config/aws-bedrock/api-key) |
| **DeepSeek** | Great value | [Setup](/provider-config/deepseek) |

### Local Models

Run models on your own hardware for complete privacy and zero per-request costs.

| Provider | Best For | Setup Guide |
|----------|----------|-------------|
| **Ollama** | Easy setup, wide model selection | [Setup](/running-models-locally/ollama) |
| **LM Studio** | GUI-based model management | [Setup](/running-models-locally/lm-studio) |

Local models require sufficient hardware (especially GPU memory). See [Running Models Locally](/running-models-locally/overview) for requirements.

## Which Model Should I Choose?

| Priority | Recommended Model |
|----------|-------------------|
| **Reliability** | Claude Sonnet 4 |
| **Value** | Qwen3 Coder |
| **Speed** | Cerebras GLm 4.6 |
| **Privacy** | Any Ollama/LM Studio model |
| **Existing subscription** | Claude Code with Max/Pro |

<Note>
Learn more about LLMs and models in [Chapter 2 of AI Coding University](https://cline.bot/learn).
</Note>

## CLI Authentication

```bash
# Authenticate from the terminal
cline auth

# Shorthand
cline a
```

Opens a browser for OAuth, same as the IDE extension. Your session persists until you sign out.

## Account Management

- **Balance & usage:** Open Cline settings — your credit balance is at the top. Click **View Usage** for transaction history.
- **Switch organization:** Go to Cline settings → **Switch Organization** to change which billing account is charged.

## Troubleshooting

| Issue | Fix |
|-------|-----|
| "Unauthorized: Please sign in" | Session expired. Click **Sign In** to re-authenticate. |
| Browser doesn't open | Check default browser settings. Copy the URL from the Cline output panel manually. |
| Frequent re-authentication | Check org security policies. Ensure you're not clearing IDE secrets. Try a full sign-out/sign-in. |
| Can't access organization | Verify membership at [app.cline.bot](https://app.cline.bot). Ask your admin about permissions. Sign out and back in. |

## Next Steps

- [Your First Project](/getting-started/your-first-project) — build something with Cline
- [Core Workflows](/core-workflows/task-management) — patterns you'll use daily
- [Customization](/customization/overview) — tailor Cline to your workflow


## Links discovered
- [Selecting Your Model](https://youtu.be/GuPmu5TVtfA)
- [account dashboard](https://app.cline.bot/dashboard)
- [Setup](https://github.com/cline/cline/blob/main/provider-config/openrouter.md)
- [Setup](https://github.com/cline/cline/blob/main/provider-config/anthropic.md)
- [Setup](https://github.com/cline/cline/blob/main/provider-config/claude-code.md)
- [Setup](https://github.com/cline/cline/blob/main/provider-config/openai.md)
- [Setup](https://github.com/cline/cline/blob/main/provider-config/gcp-vertex-ai.md)
- [Setup](https://github.com/cline/cline/blob/main/provider-config/aws-bedrock/api-key.md)
- [Setup](https://github.com/cline/cline/blob/main/provider-config/deepseek.md)
- [Setup](https://github.com/cline/cline/blob/main/running-models-locally/ollama.md)
- [Setup](https://github.com/cline/cline/blob/main/running-models-locally/lm-studio.md)
- [Running Models Locally](https://github.com/cline/cline/blob/main/running-models-locally/overview.md)
- [Chapter 2 of AI Coding University](https://cline.bot/learn)
- [app.cline.bot](https://app.cline.bot)
- [Your First Project](https://github.com/cline/cline/blob/main/getting-started/your-first-project.md)
- [Core Workflows](https://github.com/cline/cline/blob/main/core-workflows/task-management.md)
- [Customization](https://github.com/cline/cline/blob/main/customization/overview.md)

--- docs/cline-cli/getting-started.mdx ---
---
title: "Getting Started"
description: "Run Cline AI coding agents directly in your terminal with an interactive CLI or automated workflows"
---

## What is Cline CLI?

Cline CLI brings the full power of Cline to your terminal. Whether you prefer an interactive experience or automated workflows for CI/CD pipelines, the CLI adapts to your needs.

The CLI supports macOS, Linux, and Windows, and works with all the same AI providers as the VS Code extension.

## Two Ways to Use Cline CLI

The CLI operates in two distinct modes, automatically selecting the appropriate one based on how you invoke it:

### Interactive Mode

Interactive mode is designed for **hands-on development sessions** where you want to collaborate with Cline in real-time. It provides a rich terminal interface that feels like chatting with an AI assistant.

**When it activates:** Running `cline` without arguments, or when stdin is a TTY (terminal).

```bash
cline
```

Key features:

- **Real-time conversation** - Type messages, see Cline's responses, and iterate on tasks
- **Visual feedback** - Animated welcome screen, syntax-highlighted code, and progress indicators
- **File mentions** with `@` - Reference workspace files with fuzzy search autocomplete
- **Slash commands** with `/` - Quick access to `/settings`, `/history`, `/models`, and workflows
- **Keyboard shortcuts** - `Tab` to toggle Plan/Act, `Shift+Tab` for auto-approve all
- **Session summaries** - See tasks completed, files modified, and token usage on exit
- **Settings panel** - Configure providers, models, and features without leaving the CLI

Interactive mode keeps you in control. You review Cline's plan, approve or modify actions, and guide the conversation.

[Learn more about interactive mode →](/cline-cli/interactive-mode)

### Headless Mode (Non-Interactive)

Headless mode is designed for **automation, scripting, and CI/CD pipelines** where human interaction isn't possible or desired.

**When it activates:** Using the `-y`/`--yolo` flag, `--json` flag, piping input/output, or when stdin is not a TTY.

```bash
# Headless with auto-approval (YOLO mode)
cline -y "Run tests and fix any failures"

# Headless with JSON output for parsing
cline --json "List all TODO comments" | jq '.text'

# Headless via piped input
cat README.md | cline "Summarize this document"

# Chain multiple headless commands
git diff | cline -y "explain these changes" | cline -y "write a commit message"
```

Key features:

- **No visual interface** - Clean text or JSON output suitable for scripting
- **Automatic execution** - With `-y`, Cline approves all actions and runs autonomously
- **Process control** - Exits automatically when the task completes
- **Piped workflows** - Read from stdin, write to stdout, chain with other commands
- **Machine-readable output** - Use `--json` to get structured output for parsing

<Warning>
Headless mode with `-y` gives Cline full autonomy. Run on a clean git branch so you can easily revert changes if needed.
</Warning>

### Mode Detection Summary

Cline automatically detects which mode to use based on your invocation. This table shows how different command patterns trigger each mode, helping you predict behavior in scripts and interactive sessions.

| Invocation | Mode | Reason |
|------------|------|--------|
| `cline` | Interactive | No arguments, TTY connected |
| `cline "task"` | Interactive | TTY connected |
| `cline -y "task"` | Headless | YOLO flag forces headless |
| `cline --json "task"` | Headless | JSON flag forces headless |
| `cat file \| cline "task"` | Headless | stdin is piped |
| `cline "task" > output.txt` | Headless | stdout is redirected |

[Learn more about headless mode →](/cline-cli/three-core-flows)

## Supported Model Providers

Cline CLI supports all providers available in the VS Code extension:

- **Anthropic** (Claude)
- **OpenAI** (GPT-4o, GPT-4)
- **OpenAI Codex** (ChatGPT subscription)
- **OpenRouter**
- **AWS Bedrock**
- **Google Gemini**
- **X AI (Grok)**
- **Cerebras**
- **DeepSeek**
- **Ollama** (local models)
- **LM Studio** (local models)
- **OpenAI Compatible** (any compatible API)

During setup, authenticate with `cline auth` to configure your preferred provider. [See authentication →](#authenticate)

## What You Can Build

### Automated Code Maintenance

Keep your codebase healthy with automated fixes. Cline scans for issues and applies corrections across multiple files.

```bash
cline -y "Fix all ESLint errors in src/"
```
Finds and fixes linting violations throughout your source directory.

```bash
cline -y "Update all deprecated React lifecycle methods"
```
Migrates legacy code patterns to modern equivalents (e.g., `componentWillMount` → `useEffect`).

```bash
cline -y "Update dependencies with known vulnerabilities"
```
Identifies outdated packages with security issues and updates them to safe versions.

### CI/CD Integration

Integrate Cline into your continuous integration pipelines for automated code review and documentation.

```bash
git diff origin/main | cline -y "Review these changes for issues"
```
Pipes your PR diff to Cline for automated code review, catching bugs and style issues before merge.

```bash
git log --oneline v1.0..v1.1 | cline -y "Write release notes"
```
Generates human-readable release notes from your commit history between two tags.

```bash
cline -y "Run tests and fix failures" --timeout 600
```
Executes your test suite, analyzes failures, and attempts fixes with a 10-minute timeout.

### Development Workflows

From quick edits to complex refactors, Cline adapts to your workflow.

```bash
cline
```
Launches interactive mode for exploratory development and back-and-forth collaboration.

```bash
cline "Refactor this function to use async/await"
```
Executes a focused task directly from the command line with approval prompts at key steps.

```bash
cline "Based on @src/api.ts, add error handling to all endpoints"
```
Uses file mentions (`@`) to give Cline context about specific files in your workspace.

### Custom Shell Pipelines

Chain Cline with other CLI tools to build powerful automation workflows.

```bash
gh pr diff 123 | cline -y "Review this PR"
```
Fetches a GitHub PR diff and pipes it directly to Cline for review.

```bash
cline --json "List all TODO comments" | jq '.text'
```
Outputs structured JSON that you can process with tools like `jq` for scripting.

```bash
git diff | cline -y "explain" | cline -y "write a haiku about these changes"
```
Chains multiple Cline invocations together for creative multi-step workflows.

## Features at a Glance

| Feature | Interactive Mode | Non-Interactive Mode |
|---------|------------------|----------------------|
| Interactive chat | ✓ | - |
| File mentions (@) | ✓ | ✓ (inline) |
| Slash commands (/) | ✓ | - |
| Settings panel | ✓ | `cline config` |
| Plan/Act toggle | ✓ (Tab) | `-p` / `-a` flags |
| Auto-approve | ✓ (Shift+Tab) | `-y` flag |
| Session summary | ✓ | - |
| JSON output | - | `--json` |
| Piped input | - | ✓ |

---

## Installation & Setup

In just a few minutes, you can install the CLI, authenticate with your preferred AI provider, and start running tasks from any directory on your machine.

### Prerequisites

Cline CLI requires **Node.js version 20 or higher**. We recommend Node.js 22 for the best experience.

Check your Node.js version:

```bash
node --version
```

If you need to install or update Node.js, visit [nodejs.org](https://nodejs.org) or use a version manager like [nvm](https://github.com/nvm-sh/nvm).

### Install Cline CLI

Install globally via npm:

```bash
npm install -g cline
```

Verify the installation:

```bash
cline version
```

<Tip>
To install a specific version, use `npm install -g cline@2.0.0`. Check [npm](https://www.npmjs.com/package/cline) for available versions.
</Tip>

### Authenticate

After installation, run the authentication wizard:

```bash
cline auth
```

This launches an interactive wizard with multiple options. Choose the method that works best for your workflow.

#### Option 1: Sign in with Cline (Recommended)

Select **"Sign in with Cline"** to authenticate with your Cline account via OAuth. Your browser opens automatically to complete sign-in.

#### Option 2: Sign in with ChatGPT Subscription

If you have a ChatGPT Plus or Pro subscription, select **"Sign in with ChatGPT Subscription"**. This uses OpenAI's Codex OAuth to authenticate with your existing subscription.

#### Option 3: Import from Existing Tools

Already using another AI coding CLI? Cline can import your existing configuration:

- **Import from Codex CLI** - Imports credentials from `~/.codex/auth.json`
- **Import from OpenCode** - Imports configuration from `~/.local/share/opencode/auth.json`

#### Option 4: Bring Your Own API Key

Select **"Bring your own API key"** to manually configure any supported provider. Or skip the wizard entirely with flags:

```bash
# Anthropic (Claude)
cline auth -p anthropic -k sk-ant-api-xxxxx -m claude-sonnet-4-5-20250929

# OpenAI
cline auth -p openai-native -k sk-xxxxx -m gpt-4o

# OpenRouter
cline auth -p openrouter -k sk-or-xxxxx -m anthropic/claude-sonnet-4-5-20250929

# OpenAI-compatible provider with custom base URL
cline auth -p openai -k your-api-key -b https://api.example.com/v1
```

**Quick Setup Flags:**

| Flag | Description |
|------|-------------|
| `-p, --provider <id>` | Provider ID (e.g., `anthropic`, `openai-native`, `openrouter`) |
| `-k, --apikey <key>` | Your API key |
| `-m, --modelid <id>` | Model ID (e.g., `claude-sonnet-4-5-20250929`, `gpt-4o`) |
| `-b, --baseurl <url>` | Base URL for OpenAI-compatible providers |

<Tip>
Flags are especially useful for scripting, CI/CD environments, or setting up multiple machines.
</Tip>

#### Supported Providers

| Provider | Provider ID | Notes |
|----------|-------------|-------|
| Anthropic | `anthropic` | Direct Claude API access |
| OpenAI | `openai-native` | GPT-4o, GPT-4, etc. |
| OpenAI Codex | `openai-codex` | ChatGPT subscription OAuth |
| OpenRouter | `openrouter` | Access multiple providers |
| AWS Bedrock | `bedrock` | Claude via AWS |
| Google Gemini | `gemini` | Gemini Pro, etc. |
| X AI (Grok) | `xai` | Grok models |
| Cerebras | `cerebras` | Fast inference |
| DeepSeek | `deepseek` | DeepSeek models |
| Ollama | `ollama` | Local models |
| LM Studio | `lmstudio` | Local models |
| OpenAI Compatible | `openai` | Any OpenAI-compatible API |

### Verify Your Setup

Confirm everything is working with a simple test:

```bash
cline "What is 2 + 2?"
```

If Cline responds with an answer, your installation and authentication are complete.

Check your current configuration:

```bash
cline config
```

### Quick Start

Now you're ready to use Cline. Choose how you want to work:

#### Interactive Mode

Launch the interactive CLI for development:

```bash
cline
```

You'll see the Cline welcome screen. Type your task and press Enter. Use:
- `Tab` to toggle between Plan and Act modes
- `Shift+Tab` to enable auto-approve
- `/help` for available commands

[Learn more about interactive mode →](/cline-cli/interactive-mode)

#### Direct Task Execution

Run a task directly from your shell:

```bash
cline "Add error handling to utils.js"
```

For non-interactive execution (perfect for scripts and CI/CD):

```bash
cline -y "Run tests and fix any failures"
```

[Learn more about headless mode →](/cline-cli/three-core-flows)

### Switching Providers

To change your configured provider at any time:

```bash
cline auth
```

You can also use the settings panel in interactive mode:

```bash
cline
# Then type: /settings
# Navigate to the API tab
```

### Updating

Check for updates and install the latest version:

```bash
cline update
```

Or update manually via npm:

```bash
npm update -g cline
```

### Troubleshooting

#### Command Not Found

If `cline` is not found after installation:

1. Ensure npm global bin is in your PATH:
   ```bash
   npm bin -g
   ```

2. Add the path to your shell configuration (`.bashrc`, `.zshrc`, etc.):
   ```bash
   export PATH="$PATH:$(npm bin -g)"
   ```

3. Restart your terminal or source your shell config.

#### Permission Errors

If you get permission errors during installation:

```bash
# Option 1: Use a Node version manager (recommended)
# nvm, fnm, or volta handle permissions automatically

# Option 2: Fix npm permissions
# See: https://docs.npmjs.com/resolving-eacces-permissions-errors-when-installing-packages-globally
```

#### OAuth Flow Issues

If the browser doesn't open automatically during OAuth:
1. Copy the URL from the terminal
2. Paste it in your browser manually
3. Complete the sign-in flow
4. Return to the terminal

#### API Key Validation

If your API key is rejected:
1. Verify the key is correct and hasn't expired
2. Check that you've selected the correct provider
3. Ensure your API account has the necessary permissions

**Provider-specific tips:**
- **Anthropic**: Keys start with `sk-ant-`
- **OpenAI**: Keys start with `sk-`
- **AWS Bedrock**: Requires AWS credentials configured separately. See [AWS Bedrock documentation](/provider-config/aws-bedrock/api-key).

### Uninstallation

To remove Cline CLI:

```bash
npm uninstall -g cline
```

To also remove configuration data:

```bash
rm -rf ~/.cline
```

## Next Steps

- **[Interactive Mode](/cline-cli/interactive-mode)** - Master the interactive CLI with shortcuts and slash commands
- **[Headless Mode](/cline-cli/three-core-flows)** - Run Cline autonomously in scripts, CI/CD pipelines, and automated workflows
- **[Configuration](/cline-cli/configuration)** - Configure settings, rules, workflows, and environment variables
- **[CLI Reference](/cline-cli/cli-reference)** - Complete command documentation with all flags and options


## Links discovered
- [Learn more about interactive mode →](https://github.com/cline/cline/blob/main/cline-cli/interactive-mode.md)
- [Learn more about headless mode →](https://github.com/cline/cline/blob/main/cline-cli/three-core-flows.md)
- [nodejs.org](https://nodejs.org)
- [nvm](https://github.com/nvm-sh/nvm)
- [npm](https://www.npmjs.com/package/cline)
- [AWS Bedrock documentation](https://github.com/cline/cline/blob/main/provider-config/aws-bedrock/api-key.md)
- [Interactive Mode](https://github.com/cline/cline/blob/main/cline-cli/interactive-mode.md)
- [Headless Mode](https://github.com/cline/cline/blob/main/cline-cli/three-core-flows.md)
- [Configuration](https://github.com/cline/cline/blob/main/cline-cli/configuration.md)
- [CLI Reference](https://github.com/cline/cline/blob/main/cline-cli/cli-reference.md)

--- docs/cline-cli/installation.mdx ---
---
title: "Installation & Setup"
description: "Install Cline CLI on macOS, Linux, or Windows and configure your AI provider"
---

Cline CLI brings the full power of Cline to your terminal. In just a few minutes, you can install the CLI, authenticate with your preferred AI provider, and start running tasks from any directory on your machine.

## Prerequisites

Cline CLI requires **Node.js version 20 or higher**. We recommend Node.js 22 for the best experience.

Check your Node.js version:

```bash
node --version
```

If you need to install or update Node.js, visit [nodejs.org](https://nodejs.org) or use a version manager like [nvm](https://github.com/nvm-sh/nvm).

## Install Cline CLI

Install globally via npm:

```bash
npm install -g cline
```

Verify the installation:

```bash
cline version
```

<Tip>
To install a specific version, use `npm install -g cline@2.0.0`. Check [npm](https://www.npmjs.com/package/cline) for available versions.
</Tip>

## Authenticate

After installation, run the authentication wizard:

```bash
cline auth
```

This launches an interactive wizard with multiple options. Choose the method that works best for your workflow.

### Option 1: Sign in with Cline (Recommended)

Select **"Sign in with Cline"** to authenticate with your Cline account via OAuth. Your browser opens automatically to complete sign-in.

### Option 2: Sign in with ChatGPT Subscription

If you have a ChatGPT Plus or Pro subscription, select **"Sign in with ChatGPT Subscription"**. This uses OpenAI's Codex OAuth to authenticate with your existing subscription.

### Option 3: Import from Existing Tools

Already using another AI coding CLI? Cline can import your existing configuration:

- **Import from Codex CLI** - Imports credentials from `~/.codex/auth.json`
- **Import from OpenCode** - Imports configuration from `~/.local/share/opencode/auth.json`

### Option 4: Bring Your Own API Key

Select **"Bring your own API key"** to manually configure any supported provider. Or skip the wizard entirely with flags:

```bash
# Anthropic (Claude)
cline auth -p anthropic -k sk-ant-api-xxxxx -m claude-sonnet-4-5-20250929

# OpenAI
cline auth -p openai-native -k sk-xxxxx -m gpt-4o

# OpenRouter
cline auth -p openrouter -k sk-or-xxxxx -m anthropic/claude-sonnet-4-5-20250929

# Moonshot
cline auth -p moonshot -k sk-xxxxx -m kimi-k2.5

# OpenAI-compatible provider with custom base URL
cline auth -p openai -k your-api-key -b https://api.example.com/v1
```

**Quick Setup Flags:**

| Flag | Description |
|------|-------------|
| `-p, --provider <id>` | Provider ID (e.g., `anthropic`, `openai-native`, `openrouter`, `moonshot`) |
| `-k, --apikey <key>` | Your API key |
| `-m, --modelid <id>` | Model ID (e.g., `claude-sonnet-4-5-20250929`, `gpt-4o`) |
| `-b, --baseurl <url>` | Base URL for OpenAI-compatible providers |

<Tip>
Flags are especially useful for scripting, CI/CD environments, or setting up multiple machines.
</Tip>

### Supported Providers

| Provider | Provider ID | Notes |
|----------|-------------|-------|
| Anthropic | `anthropic` | Direct Claude API access |
| OpenAI | `openai-native` | GPT-4o, GPT-4, etc. |
| OpenAI Codex | `openai-codex` | ChatGPT subscription OAuth |
| OpenRouter | `openrouter` | Access multiple providers |
| AWS Bedrock | `bedrock` | Claude via AWS |
| Google Gemini | `gemini` | Gemini Pro, etc. |
| X AI (Grok) | `xai` | Grok models |
| Cerebras | `cerebras` | Fast inference |
| DeepSeek | `deepseek` | DeepSeek models |
| Moonshot | `moonshot` | Kimi models via Moonshot AI |
| Ollama | `ollama` | Local models |
| LM Studio | `lmstudio` | Local models |
| OpenAI Compatible | `openai` | Any OpenAI-compatible API |

## Verify Your Setup

Confirm everything is working with a simple test:

```bash
cline "What is 2 + 2?"
```

If Cline responds with an answer, your installation and authentication are complete.

Check your current configuration:

```bash
cline config
```

## Quick Start

Now you're ready to use Cline. Choose how you want to work:

### Interactive Mode

Launch the interactive CLI for development:

```bash
cline
```

You'll see the Cline welcome screen. Type your task and press Enter. Use:
- `Tab` to toggle between Plan and Act modes
- `Shift+Tab` to enable auto-approve
- `/help` for available commands

[Learn more about interactive mode →](/cline-cli/interactive-mode)

### Direct Task Execution

Run a task directly from your shell:

```bash
cline "Add error handling to utils.js"
```

For non-interactive execution (perfect for scripts and CI/CD):

```bash
cline -y "Run tests and fix any failures"
```

[Learn more about headless mode →](/cline-cli/three-core-flows)

## Switching Providers

To change your configured provider at any time:

```bash
cline auth
```

You can also use the settings panel in interactive mode:

```bash
cline
# Then type: /settings
# Navigate to the API tab
```

## Updating

Check for updates and install the latest version:

```bash
cline update
```

Or update manually via npm:

```bash
npm update -g cline
```

## Troubleshooting

### Command Not Found

If `cline` is not found after installation:

1. Ensure npm global bin is in your PATH:
   ```bash
   npm bin -g
   ```

2. Add the path to your shell configuration (`.bashrc`, `.zshrc`, etc.):
   ```bash
   export PATH="$PATH:$(npm bin -g)"
   ```

3. Restart your terminal or source your shell config.

### Permission Errors

If you get permission errors during installation:

```bash
# Option 1: Use a Node version manager (recommended)
# nvm, fnm, or volta handle permissions automatically

# Option 2: Fix npm permissions
# See: https://docs.npmjs.com/resolving-eacces-permissions-errors-when-installing-packages-globally
```

### OAuth Flow Issues

If the browser doesn't open automatically during OAuth:
1. Copy the URL from the terminal
2. Paste it in your browser manually
3. Complete the sign-in flow
4. Return to the terminal

### API Key Validation

If your API key is rejected:
1. Verify the key is correct and hasn't expired
2. Check that you've selected the correct provider
3. Ensure your API account has the necessary permissions

**Provider-specific tips:**
- **Anthropic**: Keys start with `sk-ant-`
- **OpenAI**: Keys start with `sk-`
- **AWS Bedrock**: Requires AWS credentials configured separately. See [AWS Bedrock documentation](/provider-config/aws-bedrock/api-key).

## Uninstallation

To remove Cline CLI:

```bash
npm uninstall -g cline
```

To also remove configuration data:

```bash
rm -rf ~/.cline
```

## Next Steps

<Columns cols={2}>
  <Card title="Interactive Mode" icon="terminal" href="/cline-cli/interactive-mode">
    Master the interactive CLI with shortcuts and slash commands.
  </Card>
  
  <Card title="Headless Mode" icon="robot" href="/cline-cli/three-core-flows">
    Run Cline autonomously in scripts, CI/CD pipelines, and automated workflows.
  </Card>
  
  <Card title="Configuration" icon="gear" href="/cline-cli/configuration">
    Configure settings, rules, workflows, and environment variables.
  </Card>
  
  <Card title="CLI Reference" icon="book" href="/cline-cli/cli-reference">
    Complete command documentation with all flags and options.
  </Card>
</Columns>


## Links discovered
- [nodejs.org](https://nodejs.org)
- [nvm](https://github.com/nvm-sh/nvm)
- [npm](https://www.npmjs.com/package/cline)
- [Learn more about interactive mode →](https://github.com/cline/cline/blob/main/cline-cli/interactive-mode.md)
- [Learn more about headless mode →](https://github.com/cline/cline/blob/main/cline-cli/three-core-flows.md)
- [AWS Bedrock documentation](https://github.com/cline/cline/blob/main/provider-config/aws-bedrock/api-key.md)

--- docs/getting-started/installing-cline.mdx ---
---
title: "Installing Cline"
description: "Get Cline up and running in your favorite IDE or terminal with these simple installation steps"
---

## Before You Begin

1. **Create your account** at [app.cline.bot](https://app.cline.bot/login) for access to multiple AI models, seamless setup without managing API keys, and occasional free inferencing.

2. **Choose your platform**: VS Code, Cursor, JetBrains IDEs, CLI (macOS/Linux preview), Zed, Neovim, VSCodium, or Windsurf.

## Installation Instructions

<Tabs>
  <Tab title="VS Code/Cursor">
    <Steps>
      <Step title="Open VS Code or Cursor">
        Launch the editor on your computer.
      </Step>
      <Step title="Open Extensions">
        Press `Ctrl/Cmd + Shift + X` or click the Extensions icon in the Activity Bar.
      </Step>
      <Step title="Search for Cline">
        Type "Cline" in the search bar.
      </Step>
      <Step title="Install">
        Click the **Install** button on the Cline extension.
      </Step>
      <Step title="Access Cline">
        Click the Cline icon in the Activity Bar, or open Command Palette (`Ctrl/Cmd + Shift + P`) and type "Cline: Open In New Tab".
      </Step>
    </Steps>

    <Tip>
    If VS Code shows "Running extensions might..." dialog, click **Allow**. If you don't see the Cline icon, restart VS Code.
    </Tip>
  </Tab>
  <Tab title="JetBrains IDEs">
    <Note>
    Cline for JetBrains works almost identically to VS Code, with all core features: diff editing, tools, multiple API providers, MCP servers, Cline rules/workflows, and more.
    </Note>

    <Steps>
      <Step title="Open your JetBrains IDE">
        Launch IntelliJ IDEA, PyCharm, WebStorm, or any JetBrains IDE.
      </Step>
      <Step title="Open Settings">
        Press `Ctrl+Alt+S` (Windows/Linux) or `Cmd+,` (macOS).
      </Step>
      <Step title="Navigate to Plugins">
        Go to **Plugins** → **Marketplace** tab.
      </Step>
      <Step title="Install Cline">
        Search for "Cline" and click **Install**.
      </Step>
      <Step title="Restart IDE">
        Restart your IDE to complete the installation.
      </Step>
    </Steps>

    Find Cline in **View** → **Tool Windows** → **Cline** (usually on the right side).

    <AccordionGroup>
      <Accordion title="Alternative Installation Methods">
        **Browser Install:**
        1. Go to the [JetBrains Marketplace](https://plugins.jetbrains.com/plugin/28247-cline)
        2. Click **Install to IDE**
        3. Confirm in your IDE and restart

        **Manual Install:**
        1. Download from the [marketplace page](https://plugins.jetbrains.com/plugin/28247-cline)
        2. Go to **Settings** → **Plugins** → gear icon → **Install Plugin from Disk**
        3. Select the downloaded `.zip` file and restart
      </Accordion>

      <Accordion title="Terminal Integration Difference">
        JetBrains shows terminal output differently than VS Code. In VS Code, output streams directly to chat. In JetBrains, output appears in collapsible "Command Output" sections.
        
        Commands execute successfully in both. Expand the section to see results in JetBrains.
      </Accordion>
    </AccordionGroup>
  </Tab>
  <Tab title="CLI">
    <Warning>
    **Preview Release**: Cline CLI is currently in preview and only available for macOS and Linux. Windows support is coming soon.
    </Warning>

    Cline CLI runs AI coding agents directly in your terminal. Use it for automated code reviews in CI/CD, multi-instance development, or shell workflow integration.

    <Steps>
      <Step title="Install Node.js 20+">
        Check your version with `node --version`. If needed, visit [nodejs.org](https://nodejs.org) or use nvm.
      </Step>
      <Step title="Install Cline CLI">
        Run `npm install -g cline` in your terminal.
      </Step>
      <Step title="Authenticate">
        Run `cline auth` to sign in and configure your AI model provider.
      </Step>
      <Step title="Run Cline">
        Run `cline` to start an interactive session, or `cline "Your task here"` for headless execution.
      </Step>
    </Steps>

    <Tip>
    Want to learn more? See the [Cline CLI documentation](/cline-cli/getting-started) for advanced usage patterns like multi-instance development and CI/CD integration.
    </Tip>
  </Tab>
  <Tab title="Zed/Neovim (ACP via CLI)">
    <Note>
    **ACP (Agent Client Protocol)** lets you run Cline in any ACP-compatible editor via the CLI. This gives you full access to Cline's capabilities—including Skills, Hooks, and MCP integrations—in your preferred editor.
    </Note>

    <Steps>
      <Step title="Install Node.js 20+">
        Check your version with `node --version`. If needed, visit [nodejs.org](https://nodejs.org) or use nvm.
      </Step>
      <Step title="Install Cline CLI">
        ```bash
        npm install -g cline
        ```
      </Step>
      <Step title="Authenticate">
        ```bash
        cline auth
        ```
      </Step>
      <Step title="Configure your editor">
        <Tabs>
          <Tab title="Zed">
            Open Zed settings (`Cmd/Ctrl + ,`) and add Cline to your `settings.json`:
            ```json
            {
              "agent_servers": {
                "Cline": {
                  "type": "custom",
                  "command": "cline",
                  "args": ["--acp"],
                  "env": {}
                }
              }
            }
            ```
            Then open the AI assistant panel, select **Cline** from the agent dropdown, and start coding.
          </Tab>
          <Tab title="Neovim (agentic.nvim)">
            Install [agentic.nvim](https://github.com/carlos-algms/agentic.nvim) using lazy.nvim:
            ```lua
            {
              "carlos-algms/agentic.nvim",
              opts = {
                provider = "cline-acp",
                acp_providers = {
                  ["cline-acp"] = {
                    command = "cline",
                    args = {"--acp"},
                  },
                },
              },
              keys = {
                {"<C-\\>", function() require("agentic").toggle() end, mode={"n","v","i"}, desc="Toggle Cline Chat"},
              },
            }
            ```
            Press `<C-\>` to toggle the Cline chat panel.
          </Tab>
          <Tab title="Neovim (avante.nvim)">
            Follow the [avante.nvim documentation](https://github.com/yetone/avante.nvim) for configuring external ACP agents and point it to `cline --acp`.
          </Tab>
        </Tabs>
      </Step>
    </Steps>

    <Tip>
    For full details on ACP editor integrations—including JetBrains ACP setup and troubleshooting—see the [ACP Editor Integrations](/cline-cli/acp-editor-integrations) guide.
    </Tip>
  </Tab>
  <Tab title="VSCodium/Windsurf">
    <Note>
    These editors use the **Open VSX Registry** instead of the VS Code Marketplace, but the installation process is nearly identical.
    </Note>

    <Steps>
      <Step title="Open your editor">
        Launch VSCodium, Windsurf, or another Open VSX-compatible editor.
      </Step>
      <Step title="Open Extensions">
        Press `Ctrl/Cmd + Shift + X`.
      </Step>
      <Step title="Search for Cline">
        Type "Cline" in the search bar.
      </Step>
      <Step title="Install">
        Select "Cline" by saoudrizwan and click **Install**.
      </Step>
      <Step title="Reload">
        Reload your editor if prompted.
      </Step>
    </Steps>

    Look for the Cline icon in your Activity Bar or use the Command Palette.
  </Tab>
</Tabs>

## Sign In & Start Building

<Note>
**CLI users:** If you installed via CLI, you already authenticated during setup with `cline auth`. You're ready to go!
</Note>

1. **Open Cline** in your editor:
   - **VS Code/Cursor/VSCodium/Windsurf:** Click the Cline icon in the Activity Bar
   - **JetBrains:** Go to **View** → **Tool Windows** → **Cline**

2. **Sign in** by clicking the **Sign Up** button in the Cline interface. You'll be redirected to [app.cline.bot](https://app.cline.bot) to authenticate.

   <Tip>
     Learn more about [authorizing with Cline](/getting-started/authorizing-with-cline), including how OAuth authentication works, using API keys with other providers, and troubleshooting auth issues.
   </Tip>

3. **Start building!** After signing in, you'll automatically return to your editor-Cline is ready to help.

## Setting Up Cline in the Right Sidebar

For the best coding experience, we recommend moving Cline to the right sidebar. This keeps your project files visible on the left while you chat with Cline on the right, giving you full visibility of your codebase as Cline works.

<Tabs>
  <Tab title="VS Code">
    <Steps>
      <Step title="Align Extension View">
        Make sure your extension view is aligned vertically to the left.
      </Step>
      <Step title="Open Right Side View">
        Click the button that opens the right side panel (typically used for GitHub Copilot chat), or use `Option + Cmd/Ctrl + B`.
      </Step>
      <Step title="Drag Cline Icon">
        Drag the Cline icon over to the nav panel at the top of that right view.
      </Step>
    </Steps>

    <Frame>
      <img
        src="https://storage.googleapis.com/cline_public_images/vscode_right_view.gif"
        alt="VS Code Right Sidebar Setup"
      />
    </Frame>
  </Tab>
  <Tab title="Cursor">
    <Steps>
      <Step title="Set Vertical Activity Bar">
        Cursor uses a horizontal activity bar by default. To switch to vertical:
        1. Open Command Palette (`Cmd/Ctrl + Shift + P`)
        2. Search for "Preferences: Open Settings (UI)"
        3. Search for `workbench.activityBar.orientation`
        4. Set the value to `vertical`
        5. Restart Cursor
      </Step>
      <Step title="Open the AI Pane">
        Click the Cursor cube icon (AI Pane) to open the right side view panel.
      </Step>
      <Step title="Drag Cline to the AI Pane">
        Drag the Cline icon directly into the AI Pane sidebar.
      </Step>
    </Steps>

    <Frame>
      <img
        src="https://storage.googleapis.com/cline_public_images/Cursor-sidebar.gif"
        alt="Cursor Right Sidebar Setup"
      />
    </Frame>
  </Tab>
</Tabs>

## Troubleshooting

### Can't Find Cline in the Marketplace

Sometimes Cline doesn't show up in search results if you're looking in the wrong tab or using an incompatible IDE version. Make sure you're searching in the **Marketplace** tab (not Installed), try searching for "Cline AI" instead, and verify your IDE is up to date. If installation fails, restart your IDE and check your internet connection.

### Cline Icon Not Appearing After Install

The most common fix is a full restart-close your IDE completely (File → Exit) and reopen it. In VS Code/Cursor/VSCodium, you can also open the Command Palette (`Ctrl/Cmd + Shift + P`) and type "Cline: Open In New Tab". In JetBrains, check **View** → **Tool Windows** → **Cline**. If it's still missing, verify the plugin is enabled in your Extensions/Plugins settings.

### CLI: Node.js or Permission Errors

Cline CLI requires Node.js 20 or higher. Run `node --version` to check-if you need to upgrade, use nvm (`nvm install 22 && nvm use 22`) or download from [nodejs.org](https://nodejs.org). For permission errors on `npm install -g`, either prefix with `sudo` on macOS/Linux or configure npm to use a user-owned directory for global packages.

### Plugin Installed But Not Working

If Cline appears installed but doesn't respond, try disabling and re-enabling the extension in your IDE's settings. Check the Developer Console (VS Code: Help → Toggle Developer Tools) or Event Log (JetBrains) for error messages. Also ensure you're using a supported IDE version and close any resource-intensive extensions that might interfere.

## Need Help?

- Join our [Discord community](https://discord.gg/cline) for support, tips, and discussions.
- [Read the docs](/getting-started/authorizing-with-cline) to explore model selection guides and advanced features.


## Links discovered
- [app.cline.bot](https://app.cline.bot/login)
- [JetBrains Marketplace](https://plugins.jetbrains.com/plugin/28247-cline)
- [marketplace page](https://plugins.jetbrains.com/plugin/28247-cline)
- [nodejs.org](https://nodejs.org)
- [Cline CLI documentation](https://github.com/cline/cline/blob/main/cline-cli/getting-started.md)
- [agentic.nvim](https://github.com/carlos-algms/agentic.nvim)
- [avante.nvim documentation](https://github.com/yetone/avante.nvim)
- [ACP Editor Integrations](https://github.com/cline/cline/blob/main/cline-cli/acp-editor-integrations.md)
- [app.cline.bot](https://app.cline.bot)
- [authorizing with Cline](https://github.com/cline/cline/blob/main/getting-started/authorizing-with-cline.md)
- [Discord community](https://discord.gg/cline)
- [Read the docs](https://github.com/cline/cline/blob/main/getting-started/authorizing-with-cline.md)

--- docs/mcp/mcp-overview.mdx ---
---
title: "MCP Overview"
description: "Learn about Model Context Protocol (MCP) servers, their capabilities, and how Cline can help build and use them. MCP standardizes how applications provide context to LLMs, acting like a USB-C port for AI applications."
---

## Quick Links

-   [Adding and Configuring MCP Servers](/mcp/adding-and-configuring-servers)
-   [Building Custom MCP Servers from Scratch](/mcp/mcp-server-development-protocol)

## Overview

Model Context Protocol is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications; it provides a standardized way to connect AI models to different data sources and tools. MCP servers act as intermediaries between large language models (LLMs), such as Claude, and external tools or data sources. They are small programs that expose functionalities to LLMs, enabling them to interact with the outside world through the MCP. An MCP server is essentially like an API that an LLM can use.

<Frame>
	<img
		src="https://storage.googleapis.com/cline_public_images/docs/assets/mcp-diagram.png"
		alt="MCP diagram showing how MCP servers connect LLMs to external tools and data sources"
	/>
</Frame>

## Key Concepts

MCP servers define a set of "**tools,**" which are functions the LLM can execute. These tools offer a wide range of capabilities.

**Here's how MCP works:**

-   **MCP hosts** discover the capabilities of connected servers and load their tools, prompts, and resources.
-   **Resources** provide consistent access to read-only data, akin to file paths or database queries.
-   **Security** is ensured as servers isolate credentials and sensitive data. Interactions require explicit user approval.

## Use Cases

The potential of MCP servers is vast. They can be used for a variety of purposes.

**Here are some concrete examples of how MCP servers can be used:**

-   **Web Services and API Integration:**
    -   Monitor GitHub repositories for new issues
    -   Post updates to Twitter based on specific triggers
    -   Retrieve real-time weather data for location-based services
-   **Browser Automation:**
    -   Automate web application testing
    -   Scrape e-commerce sites for price comparisons
    -   Generate screenshots for website monitoring
-   **Database Queries:**
    -   Generate weekly sales reports
    -   Analyze customer behavior patterns
    -   Create real-time dashboards for business metrics
-   **Project and Task Management:**
    -   Automate Jira ticket creation based on code commits
    -   Generate weekly progress reports
    -   Create task dependencies based on project requirements
-   **Codebase Documentation:**
    -   Generate API documentation from code comments
    -   Create architecture diagrams from code structure
    -   Maintain up-to-date README files

## Getting Started

Cline does not come with any pre-installed MCP servers. You'll need to find and install them separately.

**Choose the right approach for your needs:**

-   **Community Repositories:** Check for community-maintained lists of MCP servers on GitHub. See [Adding and Configuring Servers](/mcp/adding-and-configuring-servers)
-   **Cline Marketplace:** Install one from Cline's [MCP Marketplace](/mcp/mcp-marketplace)
-   **Ask Cline:** You can ask Cline to help you find or create MCP servers
-   **Build Your Own:** Create custom MCP servers using the [MCP SDK](https://github.com/modelcontextprotocol/)
-   **Customize Existing Servers:** Modify existing servers to fit your specific requirements

## Integration with Cline

Cline simplifies the building and use of MCP servers through its AI capabilities.

### Building MCP Servers

-   **Natural language understanding:** Instruct Cline in natural language to build an MCP server by describing its functionalities. Cline will interpret your instructions and generate the necessary code.
-   **Cloning and building servers:** Cline can clone existing MCP server repositories from GitHub and build them automatically.
-   **Configuration and dependency management:** Cline handles configuration files, environment variables, and dependencies.
-   **Troubleshooting and debugging:** Cline helps identify and resolve errors during development.

### Using MCP Servers

-   **Tool execution:** Cline seamlessly integrates with MCP servers, allowing you to execute their defined tools.
-   **Context-aware interactions:** Cline can intelligently suggest using relevant tools based on conversation context.
-   **Dynamic integrations:** Combine multiple MCP server capabilities for complex tasks. For example, Cline could use a GitHub server to get data and a Notion server to create a formatted report.

## Security Considerations

When working with MCP servers, it's important to follow security best practices:

-   **Authentication:** Always use secure authentication methods for API access
-   **Environment Variables:** Store sensitive information in environment variables
-   **Access Control:** Limit server access to authorized users only
-   **Data Validation:** Validate all inputs to prevent injection attacks
-   **Logging:** Implement secure logging practices without exposing sensitive data

## Resources

There are various resources available for finding and learning about MCP servers.

**Here are some links to resources for finding and learning about MCP servers:**

-   **GitHub Repositories:** [https://github.com/modelcontextprotocol/servers](https://github.com/modelcontextprotocol/servers) and [https://github.com/punkpeye/awesome-mcp-servers](https://github.com/punkpeye/awesome-mcp-servers)
-   **Online Directories:** [https://mcpservers.org/](https://mcpservers.org/), [https://mcp.so/](https://mcp.so/), and [https://glama.ai/mcp/servers](https://glama.ai/mcp/servers)
-   **PulseMCP:** [https://www.pulsemcp.com/](https://www.pulsemcp.com/)
-   **YouTube Tutorial (AI-Driven Coder):** A video guide for building and using MCP servers: [https://www.youtube.com/watch?v=b5pqTNiuuJg](https://www.youtube.com/watch?v=b5pqTNiuuJg)


## Links discovered
- [Adding and Configuring MCP Servers](https://github.com/cline/cline/blob/main/mcp/adding-and-configuring-servers.md)
- [Building Custom MCP Servers from Scratch](https://github.com/cline/cline/blob/main/mcp/mcp-server-development-protocol.md)
- [Adding and Configuring Servers](https://github.com/cline/cline/blob/main/mcp/adding-and-configuring-servers.md)
- [MCP Marketplace](https://github.com/cline/cline/blob/main/mcp/mcp-marketplace.md)
- [MCP SDK](https://github.com/modelcontextprotocol/)
- [https://github.com/modelcontextprotocol/servers](https://github.com/modelcontextprotocol/servers)
- [https://github.com/punkpeye/awesome-mcp-servers](https://github.com/punkpeye/awesome-mcp-servers)
- [https://mcpservers.org/](https://mcpservers.org/)
- [https://mcp.so/](https://mcp.so/)
- [https://glama.ai/mcp/servers](https://glama.ai/mcp/servers)
- [https://www.pulsemcp.com/](https://www.pulsemcp.com/)
- [https://www.youtube.com/watch?v=b5pqTNiuuJg](https://www.youtube.com/watch?v=b5pqTNiuuJg)

--- docs/cline-cli/overview.mdx ---
---
title: "Overview"
description: "Run Cline AI coding agents directly in your terminal with an interactive CLI or automated workflows"
---

## What is Cline CLI?

Cline CLI brings the full power of Cline to your terminal. Whether you prefer an interactive experience or automated workflows for CI/CD pipelines, the CLI adapts to your needs.

The CLI supports macOS, Linux, and Windows, and works with all the same AI providers as the VS Code extension.

<Tip>
Ready to get started? Check out the [installation guide](/cline-cli/installation) to install Cline CLI and run your first task.
</Tip>

## Two Ways to Use Cline CLI

<Columns cols={2}>
  <Card title="Interactive Mode" icon="terminal" href="/cline-cli/interactive-mode">
    **For hands-on development.** Launch `cline` in your terminal and collaborate with Cline in real-time — chat, review plans, approve actions, and iterate on tasks with a rich visual interface.
  </Card>
  <Card title="Headless Mode" icon="robot" href="/cline-cli/three-core-flows">
    **For automation & CI/CD.** Run `cline -y "task"` to let Cline work autonomously — no interaction needed. Pipe input/output, get JSON results, and chain commands in scripts and pipelines.
  </Card>
</Columns>

The CLI operates in two distinct modes, automatically selecting the appropriate one based on how you invoke it:

### Interactive Mode

Interactive mode is designed for **hands-on development sessions** where you want to collaborate with Cline in real-time. It provides a rich terminal interface that feels like chatting with an AI assistant.

**When it activates:** Running `cline` without arguments, or when stdin is a TTY (terminal).

```bash
cline
```

Key features:

- **Real-time conversation** - Type messages, see Cline's responses, and iterate on tasks
- **Visual feedback** - Animated welcome screen, syntax-highlighted code, and progress indicators
- **File mentions** with `@` - Reference workspace files with fuzzy search autocomplete
- **Slash commands** with `/` - Quick access to `/settings`, `/history`, `/models`, and workflows
- **Keyboard shortcuts** - `Tab` to toggle Plan/Act, `Shift+Tab` for auto-approve all
- **Session summaries** - See tasks completed, files modified, and token usage on exit
- **Settings panel** - Configure providers, models, and features without leaving the CLI

Interactive mode keeps you in control. You review Cline's plan, approve or modify actions, and guide the conversation.

[Learn more about interactive mode →](/cline-cli/interactive-mode)

### Headless Mode (Non-Interactive)

Headless mode is designed for **automation, scripting, and CI/CD pipelines** where human interaction isn't possible or desired.

**When it activates:** Using the `-y`/`--yolo` flag, `--json` flag, piping input/output, or when stdin is not a TTY.

```bash
# Headless with auto-approval (YOLO mode)
cline -y "Run tests and fix any failures"

# Headless with JSON output for parsing
cline --json "List all TODO comments" | jq '.text'

# Headless via piped input
cat README.md | cline "Summarize this document"

# Chain multiple headless commands
git diff | cline -y "explain these changes" | cline -y "write a commit message"
```

Key features:

- **No visual interface** - Clean text or JSON output suitable for scripting
- **Automatic execution** - With `-y`, Cline approves all actions and runs autonomously
- **Process control** - Exits automatically when the task completes
- **Piped workflows** - Read from stdin, write to stdout, chain with other commands
- **Machine-readable output** - Use `--json` to get structured output for parsing

<Warning>
Headless mode with `-y` gives Cline full autonomy. Run on a clean git branch so you can easily revert changes if needed.
</Warning>

### Mode Detection Summary

Cline automatically detects which mode to use based on your invocation. This table shows how different command patterns trigger each mode, helping you predict behavior in scripts and interactive sessions.

| Invocation | Mode | Reason |
|------------|------|--------|
| `cline` | Interactive | No arguments, TTY connected |
| `cline "task"` | Interactive | TTY connected |
| `cline -y "task"` | Headless | YOLO flag forces headless |
| `cline --json "task"` | Headless | JSON flag forces headless |
| `cat file \| cline "task"` | Headless | stdin is piped |
| `cline "task" > output.txt` | Headless | stdout is redirected |

[Learn more about headless mode →](/cline-cli/three-core-flows)

## Supported Model Providers

Cline CLI supports all providers available in the VS Code extension:

- **Anthropic** (Claude)
- **OpenAI** (GPT-4o, GPT-4)
- **OpenAI Codex** (ChatGPT subscription)
- **OpenRouter**
- **AWS Bedrock**
- **Google Gemini**
- **X AI (Grok)**
- **Cerebras**
- **DeepSeek**
- **Ollama** (local models)
- **LM Studio** (local models)
- **OpenAI Compatible** (any compatible API)

During setup, authenticate with `cline auth` to configure your preferred provider. [See setup guide →](/cline-cli/installation#authenticate)

## What You Can Build

### Automated Code Maintenance

Keep your codebase healthy with automated fixes. Cline scans for issues and applies corrections across multiple files.

```bash
cline -y "Fix all ESLint errors in src/"
```
Finds and fixes linting violations throughout your source directory.

```bash
cline -y "Update all deprecated React lifecycle methods"
```
Migrates legacy code patterns to modern equivalents (e.g., `componentWillMount` → `useEffect`).

```bash
cline -y "Update dependencies with known vulnerabilities"
```
Identifies outdated packages with security issues and updates them to safe versions.

### CI/CD Integration

Integrate Cline into your continuous integration pipelines for automated code review and documentation.

```bash
git diff origin/main | cline -y "Review these changes for issues"
```
Pipes your PR diff to Cline for automated code review, catching bugs and style issues before merge.

```bash
git log --oneline v1.0..v1.1 | cline -y "Write release notes"
```
Generates human-readable release notes from your commit history between two tags.

```bash
cline -y "Run tests and fix failures" --timeout 600
```
Executes your test suite, analyzes failures, and attempts fixes with a 10-minute timeout.

### Development Workflows

From quick edits to complex refactors, Cline adapts to your workflow.

```bash
cline
```
Launches interactive mode for exploratory development and back-and-forth collaboration.

```bash
cline "Refactor this function to use async/await"
```
Executes a focused task directly from the command line with approval prompts at key steps.

```bash
cline "Based on @src/api.ts, add error handling to all endpoints"
```
Uses file mentions (`@`) to give Cline context about specific files in your workspace.

### Custom Shell Pipelines

Chain Cline with other CLI tools to build powerful automation workflows.

```bash
gh pr diff 123 | cline -y "Review this PR"
```
Fetches a GitHub PR diff and pipes it directly to Cline for review.

```bash
cline --json "List all TODO comments" | jq '.text'
```
Outputs structured JSON that you can process with tools like `jq` for scripting.

```bash
git diff | cline -y "explain" | cline -y "write a haiku about these changes"
```
Chains multiple Cline invocations together for creative multi-step workflows.

## Features at a Glance

| Feature | Interactive Mode | Non-Interactive Mode |
|---------|------------------|----------------------|
| Interactive chat | ✓ | - |
| File mentions (@) | ✓ | ✓ (inline) |
| Slash commands (/) | ✓ | - |
| Settings panel | ✓ | `cline config` |
| Plan/Act toggle | ✓ (Tab) | `-p` / `-a` flags |
| Auto-approve | ✓ (Shift+Tab) | `-y` flag |
| Session summary | ✓ | - |
| JSON output | - | `--json` |
| Piped input | - | ✓ |

## Learn More

<Columns cols={2}>
  <Card title="Installation & Setup" icon="download" href="/cline-cli/installation">
    Install Cline CLI and authenticate with your preferred provider.
  </Card>
  
  <Card title="Interactive Mode" icon="terminal" href="/cline-cli/interactive-mode">
    Master the interactive CLI with keyboard shortcuts and slash commands.
  </Card>
  
  <Card title="Headless Mode" icon="robot" href="/cline-cli/three-core-flows">
    Run Cline autonomously in scripts, CI/CD pipelines, and automated workflows.
  </Card>
  
  <Card title="Configuration" icon="gear" href="/cline-cli/configuration">
    Configure settings, rules, workflows, and environment variables.
  </Card>
  
  <Card title="Use in Other Editors" icon="code" href="/cline-cli/acp-editor-integrations">
    Run Cline as an ACP agent in JetBrains, Neovim, Zed, and more.
  </Card>
  
  <Card title="CLI Samples" icon="flask" href="/cline-cli/samples/overview">
    Real-world examples of headless workflows and automation patterns.
  </Card>
</Columns>


## Links discovered
- [installation guide](https://github.com/cline/cline/blob/main/cline-cli/installation.md)
- [Learn more about interactive mode →](https://github.com/cline/cline/blob/main/cline-cli/interactive-mode.md)
- [Learn more about headless mode →](https://github.com/cline/cline/blob/main/cline-cli/three-core-flows.md)
- [See setup guide →](https://github.com/cline/cline/blob/main/cline-cli/installation#authenticate.md)

--- docs/customization/overview.mdx ---
---
title: "Overview"
sidebarTitle: "Overview"
description: "Understand how Rules, Skills, Workflows, Hooks, and .clineignore work together to customize Cline."
---

Out of the box, Cline is a general-purpose AI assistant. Customizations transform it into an expert on your codebase, your team's conventions, and your workflows. Instead of repeating the same instructions every task, you define them once and Cline follows them automatically.

Cline offers five systems for this: Rules, Skills, Workflows, Hooks, and .clineignore. Each serves a different purpose and activates at different times.

## Quick Comparison

| Feature | Purpose | When Active | Best For |
|---------|---------|-------------|----------|
| **[Rules](/customization/cline-rules)** | Define how Cline behaves | Always (or contextually) | Coding standards, project constraints, team conventions |
| **[Skills](/customization/skills)** | Domain expertise loaded on-demand | Triggered by matching requests | Specialized knowledge, complex procedures, institutional expertise |
| **[Workflows](/customization/workflows)** | Step-by-step task automation | Invoked with `/workflow.md` | Repetitive processes, release procedures, setup scripts |
| **[Hooks](/customization/hooks)** | Inject custom logic at key moments | Automatically on specific events | Validation, enforcement, monitoring, automation triggers |
| **[.clineignore](/customization/clineignore)** | Control file access | Always | Excluding dependencies, build artifacts, large data files |

## Understanding Each Tool

**[Rules](/customization/cline-rules)** are always-on guidance. Use them when you want Cline to consistently follow certain patterns: coding standards, naming conventions, architectural constraints, or project-specific context. Rules shape *how* Cline works across all tasks. For example, a rule might say "always use TypeScript" or "follow the repository pattern for data access."

**[Skills](/customization/skills)** are domain expertise that loads only when relevant. Use them when you have extensive knowledge that would waste context if always active. Cline sees skill descriptions at startup and activates the full instructions only when your request matches. A data analysis skill might include pandas patterns, visualization preferences, and output formats that Cline only loads when you're working with data files.

**[Workflows](/customization/workflows)** are explicit task scripts you invoke on demand. Use them when you have a repeatable multi-step process that should run the same way every time. Type `/release.md` and Cline executes your release sequence: bump version, run tests, update changelog, commit, tag, push. Workflows define *what* to do, step by step.

**[Hooks](/customization/hooks)** are programmatic guardrails that run automatically at key moments. Use them when you need to validate, enforce, or extend Cline's behavior with custom code. A hook might block `.js` file creation in a TypeScript project, run linters before saves, or notify external services after deployments.

**[.clineignore](/customization/clineignore)** controls which files and directories Cline can access. Use it to exclude dependencies, build artifacts, generated files, and large data files from Cline's context. This reduces token usage, lowers costs, and keeps Cline focused on the code that matters. It works like `.gitignore`: add patterns to a `.clineignore` file in your project root and matching files are automatically excluded.

### Example: A Release Process

Consider how all five work together for releasing a new version:

1. **Rules** ensure Cline follows your team's commit message format and versioning policy
2. **Skills** offer deep knowledge about your CI/CD system that Cline loads when deployment questions arise
3. **Workflows** provide the explicit `/release.md` sequence: bump version, update changelog, tag, push
4. **Hooks** validate that tests pass before allowing any commit or that the changelog was actually updated
5. **.clineignore** keeps build artifacts, `node_modules/`, and generated files out of Cline's context so it stays focused

## Storage Locations

All five systems support both global and project-specific configurations:

| System | Global Location | Project Location |
|--------|-----------------|------------------|
| Rules | `~/Documents/Cline/Rules/` | `.clinerules/` |
| Skills | `~/.cline/skills/` | `.cline/skills/` |
| Workflows | `~/Documents/Cline/Workflows/` | `.clinerules/workflows/` |
| Hooks | `~/Documents/Cline/Hooks/` | `.clinerules/hooks/` |
| .clineignore | N/A | `.clineignore` |

### When to Use Each

**Start with project storage.** Most customizations belong in your project's directory because they're tied to that specific codebase. Team coding standards, deployment workflows, and architectural constraints all live with the code they describe. This also means your customizations travel with the repository, so collaborators get them automatically and changes can be reviewed in pull requests.

**Use global storage for personal preferences.** If you find yourself adding the same customization to every project, move it to global storage. Your preferred communication style, personal productivity workflows, and tools you use everywhere belong here. Global customizations apply to all projects but stay out of version control, so they won't affect your teammates.

When names conflict, project-specific configurations take precedence (except for Skills, where global takes precedence). This lets you override global defaults for specific projects when needed.

## Security Considerations

<Warning>
Always review customizations before adding them to your projects. Only use customizations from sources you trust.
</Warning>

Customizations are powerful. They shape how Cline writes code, execute commands automatically, and influence every interaction. Treat customization files with the same scrutiny you'd give any code running in your environment.

### Best Practices

Review any customization file before adding it to your project or global configuration. Understand what it does and why.

When downloading customizations from GitHub repositories, community shares, or other external sources, verify the source:
- Is the author reputable?
- Has the community reviewed it?
- Does the code do what it claims?

Look for dangerous commands:
- Shell commands that delete files (`rm`, `del`)
- Commands that transmit data (`curl`, `wget` with POST)
- File operations outside your project directory
- Commands that modify system configuration

Keep your customizations in version control so you can track changes, review diffs, and roll back if something goes wrong. When creating hooks, use the most restrictive event triggers necessary. Don't run hooks on every file save if you only need them before commits.


## Links discovered
- [Rules](https://github.com/cline/cline/blob/main/customization/cline-rules.md)
- [Skills](https://github.com/cline/cline/blob/main/customization/skills.md)
- [Workflows](https://github.com/cline/cline/blob/main/customization/workflows.md)
- [Hooks](https://github.com/cline/cline/blob/main/customization/hooks.md)
- [.clineignore](https://github.com/cline/cline/blob/main/customization/clineignore.md)

--- docs/enterprise-solutions/overview.mdx ---
---
title: "Cline Enterprise"
sidebarTitle: "Overview"
description: "Enterprise security, governance, and observability for the coding agent millions of developers trust"
---

Cline Enterprise brings centralized governance to the same open-source architecture that millions of developers already use. Your code stays in your environment, you use your own inference at your negotiated rates, and you get the security and observability capabilities that platform teams need for org-wide deployment.

<Card title="Learn More About Enterprise" icon="building" href="https://cline.bot/enterprise">
  Visit our website for detailed information about enterprise features, pricing, and deployment options.
</Card>

## What You Get

It delivers five core capabilities that platform teams need for production deployment. Each addresses a specific requirement for scaling AI coding across your organization.

### Security by Design

Your code never leaves your environment. Cline processes everything locally - no uploads, no indexing, no training on your data.

<CardGroup cols={2}>
  <Card title="Client-side execution" icon="computer">
    All processing happens within your environment
  </Card>

  <Card title="No data exfiltration" icon="shield-check">
    Code and context never transmitted externally
  </Card>

  <Card title="No codebase indexing" icon="database">
    Repositories are never indexed or cached
  </Card>

  <Card title="No model training" icon="ban">
    Your code and prompts aren't used for training
  </Card>
</CardGroup>

### Bring Your Own Inference

Use your existing cloud contracts and negotiated rates. Most AI tools force you to buy inference through them with markup. Cline connects directly to your providers.

Connect to any inference provider:
- AWS Bedrock
- Google Vertex AI
- Azure OpenAI
- Anthropic direct
- OpenAI direct
- Cerebras
- Any OpenAI-compatible endpoint

Switch models instantly as new ones release. Use Claude Sonnet 4.5 as your daily driver, GPT-5 for complex refactoring, open-source models for simple tasks. Your existing cloud credits and startup program contracts now cover AI coding. We handle the agent loop. You handle the inference. No markup, no vendor lock-in.

### Governance at Scale

Platform teams need central control when thousands of developers use AI. Individual API keys scattered across laptops create security risks and cost overruns.

Enterprise governance provides:
- **SSO authentication**: Corporate credentials instead of personal API keys
- **Role-based access control**: Three-tier hierarchy (Member/Admin/Owner) with organization-scoped permissions
- **Model and tool controls**: Govern which models and tools each team accesses
- **Remote configuration**: Manage settings for all developers from one dashboard
- **Usage tracking and observability**: OpenTelemetry integration for monitoring usage, costs, and performance with selective audit logging for administrative operations

Configure once, deploy everywhere. Developers work how they prefer while you maintain control.

### Complete Observability

Export logs to your existing observability stack. Track usage, costs, and performance across all teams.

- **OpenTelemetry export**: Direct integration with Datadog, Grafana, Splunk
- **Real-time analytics**: Track adoption, performance, and patterns
- **Cost breakdown**: See exactly what each team spends on which models
- **JSON output**: Build custom dashboards in your existing tools

The same observability standards you require for production systems.

## Deployment

Cline Enterprise connects securely to your infrastructure. Deploy in cloud environments. Configure to work with your existing security policies and compliance requirements.

Rolling out to your organization:
1. Configure Cline Core to connect to your infrastructure
2. Set SSO, RBAC, and governance policies
3. Deploy to developers via your existing software distribution
4. Monitor usage through your observability tools

## Next Steps

- Review security architecture
- Configure [cloud provider setup](/provider-config/aws-bedrock/api-key) (AWS Bedrock, Vertex AI, Azure)
- Set up [MCP servers](/mcp/mcp-overview) for custom tooling
- Add [custom instructions](/customization/cline-rules) for your codebase

Schedule a walkthrough to see how Cline Enterprise fits your infrastructure. We'll work with your security and compliance requirements to deploy in your environment.


## Links discovered
- [cloud provider setup](https://github.com/cline/cline/blob/main/provider-config/aws-bedrock/api-key.md)
- [MCP servers](https://github.com/cline/cline/blob/main/mcp/mcp-overview.md)
- [custom instructions](https://github.com/cline/cline/blob/main/customization/cline-rules.md)

--- docs/running-models-locally/overview.mdx ---
---
title: "Local Models Overview"
---

## Running Models Locally with Cline

Run Cline completely offline with genuinely capable models on your own hardware. No API costs, no data leaving your machine, no internet dependency.

Local models have reached a turning point where they're now practical for real development work. This guide covers everything you need to know about running Cline with local models.

## Quick Start

1. **Check your hardware** - 32GB+ RAM minimum
2. **Choose your runtime** - [LM Studio](/running-models-locally/lm-studio) or [Ollama](/running-models-locally/ollama)
3. **Download Qwen3 Coder 30B** - The recommended model
4. **Configure settings** - Enable compact prompts, set max context
5. **Start coding** - Completely offline

## Hardware Requirements

Your RAM determines which models you can run effectively:

| RAM | Recommended Model | Quantization | Performance Level |
| --- | --- | --- | --- |
| 32GB | Qwen3 Coder 30B | 4-bit | Entry-level local coding |
| 64GB | Qwen3 Coder 30B | 8-bit | Full Cline features |
| 128GB+ | GLM-4.5-Air | 4-bit | Cloud-competitive performance |

## Recommended Models

### Primary Recommendation: Qwen3 Coder 30B

After extensive testing, **Qwen3 Coder 30B** is the most reliable model under 70B parameters for Cline:

- **256K native context window** - Handle entire repositories
- **Strong tool-use capabilities** - Reliable command execution
- **Repository-scale understanding** - Maintains context across files
- **Proven reliability** - Consistent outputs with Cline's tool format

Download sizes:
- 4-bit: ~17GB (recommended for 32GB RAM)
- 8-bit: ~32GB (recommended for 64GB RAM)
- 16-bit: ~60GB (requires 128GB+ RAM)

### Why Not Smaller Models?

Most models under 30B parameters (7B-20B) fail with Cline because they:
- Produce broken tool-use outputs
- Refuse to execute commands
- Can't maintain conversation context
- Struggle with complex coding tasks

## Runtime Options

### LM Studio
- **Pros**: User-friendly GUI, easy model management, built-in server
- **Cons**: Memory overhead from UI, limited to single model at a time
- **Best for**: Desktop users who want simplicity
- [Setup Guide →](/running-models-locally/lm-studio)

### Ollama
- **Pros**: Command-line based, lower memory overhead, scriptable
- **Cons**: Requires terminal comfort, manual model management
- **Best for**: Power users and server deployments
- [Setup Guide →](/running-models-locally/ollama)

## Critical Configuration

### Required Settings

**In Cline:**
- ✅ Enable "Use Compact Prompt" - Reduces prompt size by 90%
- ✅ Set appropriate model in settings
- ✅ Configure Base URL to match your server

**In LM Studio:**
- Context Length: `262144` (maximum)
- KV Cache Quantization: `OFF` (critical for proper function)
- Flash Attention: `ON` (if available on your hardware)

**In Ollama:**
- Set context window: `num_ctx 262144`
- Enable flash attention if supported

### Understanding Quantization

Quantization reduces model precision to fit on consumer hardware:

| Type | Size Reduction | Quality | Use Case |
| --- | --- | --- | --- |
| 4-bit | ~75% | Good | Most coding tasks, limited RAM |
| 8-bit | ~50% | Better | Professional work, more nuance |
| 16-bit | None | Best | Maximum quality, requires high RAM |

### Model Formats

**GGUF (Universal)**
- Works on all platforms (Windows, Linux, Mac)
- Extensive quantization options
- Broader tool compatibility
- Recommended for most users

**MLX (Mac only)**
- Optimized for Apple Silicon (M1/M2/M3)
- Leverages Metal and AMX acceleration
- Faster inference on Mac
- Requires macOS 13+

## Performance Expectations

### What's Normal

- **Initial load time**: 10-30 seconds for model warmup
- **Token generation**: 5-20 tokens/second on consumer hardware
- **Context processing**: Slower with large codebases
- **Memory usage**: Close to your quantization size

### Performance Tips

1. **Use compact prompts** - Essential for local inference
2. **Limit context when possible** - Start with smaller windows
3. **Choose right quantization** - Balance quality vs speed
4. **Close other applications** - Free up RAM for the model
5. **Use SSD storage** - Faster model loading

## Use Case Comparison

### When to Use Local Models

✅ **Perfect for:**
- Offline development environments
- Privacy-sensitive projects
- Learning without API costs
- Unlimited experimentation
- Air-gapped environments
- Cost-conscious development

### When to Use Cloud Models

☁️ **Better for:**
- Very large codebases (>256K tokens)
- Multi-hour refactoring sessions
- Teams needing consistent performance
- Latest model capabilities
- Time-critical projects

## Troubleshooting

### Common Issues & Solutions

**"Shell integration unavailable"**
- Switch to bash in Cline Settings → Terminal → Default Terminal Profile
- Resolves 90% of terminal integration problems

**"No connection could be made"**
- Verify server is running (LM Studio or Ollama)
- Check Base URL matches server address
- Ensure no firewall blocking connection
- Default ports: LM Studio (1234), Ollama (11434)

**Slow or incomplete responses**
- Normal for local models (5-20 tokens/sec typical)
- Try smaller quantization (4-bit instead of 8-bit)
- Enable compact prompts if not already
- Reduce context window size

**Model confusion or errors**
- Verify KV Cache Quantization is OFF (LM Studio)
- Ensure compact prompts enabled
- Check context length set to maximum
- Confirm sufficient RAM for quantization

### Performance Optimization

**For faster inference:**
1. Use 4-bit quantization
2. Enable Flash Attention
3. Reduce context window if not needed
4. Close unnecessary applications
5. Use NVMe SSD for model storage

**For better quality:**
1. Use 8-bit or higher quantization
2. Maximize context window
3. Ensure adequate cooling
4. Allocate maximum RAM to model

## Advanced Configuration

### Multi-GPU Setup
If you have multiple GPUs, you can split model layers:
- LM Studio: Automatic GPU detection
- Ollama: Set `num_gpu` parameter

### Custom Models
While Qwen3 Coder 30B is recommended, you can experiment with:
- DeepSeek Coder V2
- Codestral 22B
- StarCoder2 15B

Note: These may require additional configuration and testing.

## Community & Support

- **Discord**: [Join our community](https://discord.gg/cline) for real-time help
- **Reddit**: [r/cline](https://www.reddit.com/r/CLine/) for discussions
- **GitHub**: [Report issues](https://github.com/cline/cline/issues)

## Next Steps

Ready to get started? Choose your path:

<CardGroup cols={2}>
  <Card title="LM Studio Setup" icon="desktop" href="/running-models-locally/lm-studio">
    User-friendly GUI approach with detailed configuration guide
  </Card>
  <Card title="Ollama Setup" icon="terminal" href="/running-models-locally/ollama">
    Command-line setup for power users and automation
  </Card>
</CardGroup>

## Summary

Local models with Cline are now genuinely practical. While they won't match top-tier cloud APIs in speed, they offer complete privacy, zero costs, and offline capability. With proper configuration and the right hardware, Qwen3 Coder 30B can handle most coding tasks effectively.

The key is proper setup: adequate RAM, correct configuration, and realistic expectations. Follow this guide, and you'll have a capable coding assistant running entirely on your hardware.


## Links discovered
- [LM Studio](https://github.com/cline/cline/blob/main/running-models-locally/lm-studio.md)
- [Ollama](https://github.com/cline/cline/blob/main/running-models-locally/ollama.md)
- [Setup Guide →](https://github.com/cline/cline/blob/main/running-models-locally/lm-studio.md)
- [Setup Guide →](https://github.com/cline/cline/blob/main/running-models-locally/ollama.md)
- [Join our community](https://discord.gg/cline)
- [r/cline](https://www.reddit.com/r/CLine/)
- [Report issues](https://github.com/cline/cline/issues)

--- src/core/prompts/system-prompt/components/tool_use/examples.ts ---
import { TemplateEngine } from "../../templates/TemplateEngine"
import type { PromptVariant, SystemPromptContext } from "../../types"

const FOCUS_CHAIN_EXAMPLE_BASH = `<task_progress>
- [x] Set up project structure
- [x] Install dependencies
- [ ] Run command to start server
- [ ] Test application
</task_progress>
`

const FOCUS_CHAIN_EXAMPLE_NEW_FILE = `<task_progress>
- [x] Set up project structure
- [x] Install dependencies
- [ ] Create components
- [ ] Test application
</task_progress>
`

const FOCUS_CHAIN_EXAMPLE_EDIT = `<task_progress>
- [x] Set up project structure
- [x] Install dependencies
- [ ] Create components
- [ ] Test application
</task_progress>
`

const TOOL_USE_EXAMPLES_TEMPLATE_TEXT = `# Tool Use Examples

## Example 1: Requesting to execute a command

<execute_command>
<command>npm run dev</command>
<requires_approval>false</requires_approval>
{{FOCUS_CHAIN_EXAMPLE_BASH}}</execute_command>

## Example 2: Requesting to create a new file

<write_to_file>
<path>src/frontend-config.json</path>
<content>
{
  "apiEndpoint": "https://api.example.com",
  "theme": {
    "primaryColor": "#007bff",
    "secondaryColor": "#6c757d",
    "fontFamily": "Arial, sans-serif"
  },
  "features": {
    "darkMode": true,
    "notifications": true,
    "analytics": false
  },
  "version": "1.0.0"
}
</content>
{{FOCUS_CHAIN_EXAMPLE_NEW_FILE}}</write_to_file>

## Example 3: Creating a new task

<new_task>
<context>
1. Current Work:
   [Detailed description]

2. Key Technical Concepts:
   - [Concept 1]
   - [Concept 2]
   - [...]

3. Relevant Files and Code:
   - [File Name 1]
      - [Summary of why this file is important]
      - [Summary of the changes made to this file, if any]
      - [Important Code Snippet]
   - [File Name 2]
      - [Important Code Snippet]
   - [...]

4. Problem Solving:
   [Detailed description]

5. Pending Tasks and Next Steps:
   - [Task 1 details & next steps]
   - [Task 2 details & next steps]
   - [...]
</context>
</new_task>

## Example 4: Requesting to make targeted edits to a file

<replace_in_file>
<path>src/components/App.tsx</path>
<diff>
------- SEARCH
import React from 'react';
=======
import React, { useState } from 'react';
+++++++ REPLACE

------- SEARCH
function handleSubmit() {
  saveData();
  setLoading(false);
}

=======
+++++++ REPLACE

------- SEARCH
return (
  <div>
=======
function handleSubmit() {
  saveData();
  setLoading(false);
}

return (
  <div>
+++++++ REPLACE
</diff>
{{FOCUS_CHAIN_EXAMPLE_EDIT}}</replace_in_file>


## Example 5: Requesting to use an MCP tool

<use_mcp_tool>
<server_name>weather-server</server_name>
<tool_name>get_forecast</tool_name>
<arguments>
{
  "city": "San Francisco",
  "days": 5
}
</arguments>
</use_mcp_tool>

## Example 6: Another example of using an MCP tool (where the server name is a unique identifier such as a URL)

<use_mcp_tool>
<server_name>github.com/modelcontextprotocol/servers/tree/main/src/github</server_name>
<tool_name>create_issue</tool_name>
<arguments>
{
  "owner": "octocat2",
  "repo": "hello-world",
  "title": "Found a bug",
  "body": "I'm having a problem with this.",
  "labels": ["bug", "help wanted"],
  "assignees": ["octocat"]
}
</arguments>
</use_mcp_tool>`

export async function getToolUseExamplesSection(_variant: PromptVariant, context: SystemPromptContext): Promise<string> {
	// Return the placeholder that will be replaced with actual tools
	const focusChainEnabled = context.focusChainSettings?.enabled

	return new TemplateEngine().resolve(TOOL_USE_EXAMPLES_TEMPLATE_TEXT, context, {
		FOCUS_CHAIN_EXAMPLE_BASH: focusChainEnabled ? FOCUS_CHAIN_EXAMPLE_BASH : "",
		FOCUS_CHAIN_EXAMPLE_NEW_FILE: focusChainEnabled ? FOCUS_CHAIN_EXAMPLE_NEW_FILE : "",
		FOCUS_CHAIN_EXAMPLE_EDIT: focusChainEnabled ? FOCUS_CHAIN_EXAMPLE_EDIT : "",
	})
}


--- scripts/test-standalone-core-api-server.ts ---
#!/usr/bin/env npx tsx

/**
 * Simple Cline gRPC Server
 *
 * This script provides a minimal way to run the Cline core gRPC service
 * without requiring the full installation, while automatically mocking all external services. Simply run:
 *
 *   # One-time setup (generates protobuf files)
 *	 npm run compile-standalone
 *   npm run test:sca-server
 *
 * The following components are started automatically:
 *   1. HostBridge test server
 *   2. ClineApiServerMock (mock implementation of the Cline API)
 *   3. AuthServiceMock (activated if E2E_TEST="true")
 *
 * Environment Variables for Customization:
 *   PROJECT_ROOT - Override project root directory (default: parent of scripts dir)
 *   CLINE_DIST_DIR - Override distribution directory (default: PROJECT_ROOT/dist-standalone)
 *   CLINE_CORE_FILE - Override core file name (default: cline-core.js)
 *   PROTOBUS_PORT - gRPC server port (default: 26040)
 *   HOSTBRIDGE_PORT - HostBridge server port (default: 26041)
 *   WORKSPACE_DIR - Working directory (default: current directory)
 *   E2E_TEST - Enable E2E test mode (default: true)
 *   CLINE_ENVIRONMENT - Environment setting (default: local)
 *
 * Ideal for local development, testing, or lightweight E2E scenarios.
 */

import * as fs from "node:fs"
import { mkdtempSync, rmSync } from "node:fs"
import * as os from "node:os"
import { ChildProcess, execSync, spawn } from "child_process"
import * as path from "path"
import { ClineApiServerMock } from "../src/test/e2e/fixtures/server/index"

const PROTOBUS_PORT = process.env.PROTOBUS_PORT || "26040"
const HOSTBRIDGE_PORT = process.env.HOSTBRIDGE_PORT || "26041"
const WORKSPACE_DIR = process.env.WORKSPACE_DIR || process.cwd()
const E2E_TEST = process.env.E2E_TEST || "true"
const CLINE_ENVIRONMENT = process.env.CLINE_ENVIRONMENT || "local"
const USE_C8 = process.env.USE_C8 === "true"

// Locate the standalone build directory and core file with flexible path resolution
const projectRoot = process.env.PROJECT_ROOT || path.resolve(__dirname, "..")
const distDir = process.env.CLINE_DIST_DIR || path.join(projectRoot, "dist-standalone")
const clineCoreFile = process.env.CLINE_CORE_FILE || "cline-core.js"
const coreFile = path.join(distDir, clineCoreFile)

const childProcesses: ChildProcess[] = []

async function main(): Promise<void> {
	console.log("Starting Simple Cline gRPC Server...")
	console.log(`Project Root: ${projectRoot}`)
	console.log(`Workspace: ${WORKSPACE_DIR}`)
	console.log(`ProtoBus Port: ${PROTOBUS_PORT}`)
	console.log(`HostBridge Port: ${HOSTBRIDGE_PORT}`)

	console.log(`Looking for standalone build at: ${coreFile}`)

	if (!fs.existsSync(coreFile)) {
		console.error(`Standalone build not found at: ${coreFile}`)
		console.error("Available environment variables for customization:")
		console.error("  PROJECT_ROOT - Override project root directory")
		console.error("  CLINE_DIST_DIR - Override distribution directory")
		console.error("  CLINE_CORE_FILE - Override core file name")
		console.error("")
		console.error("To build the standalone version, run: npm run compile-standalone")
		process.exit(1)
	}

	try {
		await ClineApiServerMock.startGlobalServer()
		console.log("Cline API Server started in-process")
	} catch (error) {
		console.error("Failed to start Cline API Server:", error)
		process.exit(1)
	}

	const extensionsDir = path.join(distDir, "vsce-extension")
	const userDataDir = mkdtempSync(path.join(os.tmpdir(), "vsce"))
	const clineTestWorkspace = mkdtempSync(path.join(os.tmpdir(), "cline-test-workspace-"))

	console.log("Starting HostBridge test server...")
	const hostbridge: ChildProcess = spawn("npx", ["tsx", path.join(__dirname, "test-hostbridge-server.ts")], {
		stdio: "pipe",
		env: {
			...process.env,
			TEST_HOSTBRIDGE_WORKSPACE_DIR: clineTestWorkspace,
			HOST_BRIDGE_ADDRESS: `127.0.0.1:${HOSTBRIDGE_PORT}`,
		},
	})
	childProcesses.push(hostbridge)

	console.log(`Temp user data dir: ${userDataDir}`)
	console.log(`Temp extensions dir: ${extensionsDir}`)
	// Extract standalone.zip if needed
	const standaloneZipPath = path.join(distDir, "standalone.zip")
	if (!fs.existsSync(standaloneZipPath)) {
		console.error(`standalone.zip not found at: ${standaloneZipPath}`)
		process.exit(1)
	}

	console.log("Extracting standalone.zip to extensions directory...")
	try {
		if (!fs.existsSync(extensionsDir)) {
			execSync(`unzip -o -q "${standaloneZipPath}" -d "${extensionsDir}"`, { stdio: "inherit" })
		}
		console.log(`Successfully extracted standalone.zip to: ${extensionsDir}`)
	} catch (error) {
		console.error("Failed to extract standalone.zip:", error)
		process.exit(1)
	}

	const covDir = path.join(projectRoot, `coverage/coverage-core-${PROTOBUS_PORT}`)

	const baseArgs = ["--enable-source-maps", path.join(distDir, "cline-core.js")]

	const spawnArgs = USE_C8 ? ["c8", "--report-dir", covDir, "node", ...baseArgs] : ["node", ...baseArgs]

	console.log(`Starting Cline Core Service... (useC8=${USE_C8})`)

	const coreService: ChildProcess = spawn("npx", spawnArgs, {
		cwd: projectRoot,
		env: {
			...process.env,
			NODE_PATH: "./node_modules",
			DEV_WORKSPACE_FOLDER: WORKSPACE_DIR,
			PROTOBUS_ADDRESS: `127.0.0.1:${PROTOBUS_PORT}`,
			HOST_BRIDGE_ADDRESS: `localhost:${HOSTBRIDGE_PORT}`,
			E2E_TEST,
			CLINE_ENVIRONMENT,
			CLINE_DIR: userDataDir,
			INSTALL_DIR: extensionsDir,
		},
		stdio: "inherit",
	})
	childProcesses.push(coreService)

	const shutdown = async () => {
		console.log("\nShutting down services...")

		while (childProcesses.length > 0) {
			const child = childProcesses.pop()
			if (child && !child.killed) child.kill("SIGINT")
		}

		await ClineApiServerMock.stopGlobalServer()

		try {
			rmSync(userDataDir, { recursive: true, force: true })
			rmSync(clineTestWorkspace, { recursive: true, force: true })
			console.log("Cleaned up temporary directories")
		} catch (err) {
			console.warn("Failed to cleanup temp directories:", err)
		}

		process.exit(0)
	}

	process.on("SIGINT", shutdown)
	process.on("SIGTERM", shutdown)

	coreService.on("exit", (code) => {
		console.log(`Core service exited with code ${code}`)
		shutdown()
	})
	hostbridge.on("exit", (code) => {
		console.log(`HostBridge exited with code ${code}`)
		shutdown()
	})

	console.log(`Cline gRPC Server is running on 127.0.0.1:${PROTOBUS_PORT}`)
	console.log("Press Ctrl+C to stop")
}

if (require.main === module) {
	main().catch((err) => {
		console.error("Failed to start simple Cline server:", err)
		process.exit(1)
	})
}


--- src/shared/api.ts ---
import { ApiFormat } from "./proto/cline/models"
import type { ApiHandlerSettings } from "./storage/state-keys"

export type ApiProvider =
	| "anthropic"
	| "claude-code"
	| "openrouter"
	| "bedrock"
	| "vertex"
	| "openai"
	| "ollama"
	| "lmstudio"
	| "gemini"
	| "openai-native"
	| "openai-codex"
	| "requesty"
	| "together"
	| "deepseek"
	| "qwen"
	| "qwen-code"
	| "doubao"
	| "mistral"
	| "vscode-lm"
	| "cline"
	| "litellm"
	| "moonshot"
	| "nebius"
	| "fireworks"
	| "asksage"
	| "xai"
	| "sambanova"
	| "cerebras"
	| "sapaicore"
	| "groq"
	| "huggingface"
	| "huawei-cloud-maas"
	| "dify"
	| "baseten"
	| "vercel-ai-gateway"
	| "zai"
	| "oca"
	| "aihubmix"
	| "minimax"
	| "hicap"
	| "nousResearch"

export const DEFAULT_API_PROVIDER = "openrouter" as ApiProvider

export interface ApiHandlerOptions extends Partial<ApiHandlerSettings> {
	ulid?: string // Used to identify the task in API requests
	onRetryAttempt?: (attempt: number, maxRetries: number, delay: number, error: any) => void // Callback function
}

export type ApiConfiguration = ApiHandlerOptions

// Models

interface PriceTier {
	tokenLimit: number // Upper limit (inclusive) of *input* tokens for this price. Use Infinity for the highest tier.
	price: number // Price per million tokens for this tier.
}

export interface ModelInfo {
	name?: string
	maxTokens?: number
	contextWindow?: number
	supportsImages?: boolean
	supportsPromptCache: boolean // this value is hardcoded for now
	supportsReasoning?: boolean // Whether the model supports reasoning/thinking mode
	inputPrice?: number // Keep for non-tiered input models
	outputPrice?: number // Keep for non-tiered output models
	thinkingConfig?: {
		maxBudget?: number // Max allowed thinking budget tokens
		outputPrice?: number // Output price per million tokens when budget > 0
		outputPriceTiers?: PriceTier[] // Optional: Tiered output price when budget > 0
		geminiThinkingLevel?: "low" | "high" // Optional: preset thinking level
		supportsThinkingLevel?: boolean // Whether the model supports thinking level (low/high)
	}
	supportsGlobalEndpoint?: boolean // Whether the model supports a global endpoint with Vertex AI
	cacheWritesPrice?: number
	cacheReadsPrice?: number
	description?: string
	tiers?: {
		contextWindow: number
		inputPrice?: number
		outputPrice?: number
		cacheWritesPrice?: number
		cacheReadsPrice?: number
	}[]
	temperature?: number
	apiFormat?: ApiFormat // The API format used by this model
}

export interface OpenAiCompatibleModelInfo extends ModelInfo {
	temperature?: number
	isR1FormatRequired?: boolean
	systemRole?: "developer" | "system"
	supportsReasoningEffort?: boolean
	supportsTools?: boolean
	supportsStreaming?: boolean
}

export interface OcaModelInfo extends OpenAiCompatibleModelInfo {
	modelName: string
	surveyId?: string
	banner?: string
	surveyContent?: string
	supportsReasoning?: boolean
	reasoningEffortOptions: string[]
}

export const CLAUDE_SONNET_1M_SUFFIX = ":1m"
export const CLAUDE_SONNET_1M_TIERS = [
	{
		contextWindow: 200000,
		inputPrice: 3.0,
		outputPrice: 15,
		cacheWritesPrice: 3.75,
		cacheReadsPrice: 0.3,
	},
	{
		contextWindow: Number.MAX_SAFE_INTEGER, // storing infinity in vs storage is not possible, it converts to 'null', which causes crash in webview ModelInfoView
		inputPrice: 6,
		outputPrice: 22.5,
		cacheWritesPrice: 7.5,
		cacheReadsPrice: 0.6,
	},
]
export const CLAUDE_OPUS_1M_TIERS = [
	{
		contextWindow: 200000,
		inputPrice: 5.0,
		outputPrice: 25,
		cacheWritesPrice: 6.25,
		cacheReadsPrice: 0.5,
	},
	{
		contextWindow: Number.MAX_SAFE_INTEGER,
		inputPrice: 10,
		outputPrice: 37.5,
		cacheWritesPrice: 12.5,
		cacheReadsPrice: 1.0,
	},
]

export interface HicapCompatibleModelInfo extends ModelInfo {
	temperature?: number
}

export const hicapModelInfoSaneDefaults: HicapCompatibleModelInfo = {
	maxTokens: -1,
	contextWindow: 128_000,
	supportsImages: true,
	supportsPromptCache: true,
	inputPrice: 0,
	outputPrice: 0,
	temperature: 1,
}

// Anthropic
// https://docs.anthropic.com/en/docs/about-claude/models // prices updated 2025-01-02
export type AnthropicModelId = keyof typeof anthropicModels
export const anthropicDefaultModelId: AnthropicModelId = "claude-sonnet-4-5-20250929"
export const ANTHROPIC_MIN_THINKING_BUDGET = 1_024
export const ANTHROPIC_MAX_THINKING_BUDGET = 6_000
export const anthropicModels = {
	"claude-sonnet-4-5-20250929": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoning: true,
		inputPrice: 3.0,
		outputPrice: 15.0,
		cacheWritesPrice: 3.75,
		cacheReadsPrice: 0.3,
	},
	"claude-sonnet-4-5-20250929:1m": {
		maxTokens: 8192,
		contextWindow: 1_000_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoning: true,
		inputPrice: 3.0,
		outputPrice: 15.0,
		cacheWritesPrice: 3.75,
		cacheReadsPrice: 0.3,
		tiers: CLAUDE_SONNET_1M_TIERS,
	},
	"claude-haiku-4-5-20251001": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoning: true,
		inputPrice: 1,
		outputPrice: 5.0,
		cacheWritesPrice: 1.25,
		cacheReadsPrice: 0.1,
	},
	"claude-sonnet-4-20250514": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoning: true,
		inputPrice: 3.0,
		outputPrice: 15.0,
		cacheWritesPrice: 3.75,
		cacheReadsPrice: 0.3,
	},
	"claude-sonnet-4-20250514:1m": {
		maxTokens: 8192,
		contextWindow: 1_000_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoning: true,
		inputPrice: 3.0,
		outputPrice: 15.0,
		cacheWritesPrice: 3.75,
		cacheReadsPrice: 0.3,
		tiers: CLAUDE_SONNET_1M_TIERS,
	},
	"claude-opus-4-6": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoning: true,
		inputPrice: 5.0,
		outputPrice: 25.0,
		cacheWritesPrice: 6.25,
		cacheReadsPrice: 0.5,
	},
	"claude-opus-4-6:1m": {
		maxTokens: 8192,
		contextWindow: 1_000_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoning: true,
		inputPrice: 5.0,
		outputPrice: 25.0,
		cacheWritesPrice: 6.25,
		cacheReadsPrice: 0.5,
		tiers: CLAUDE_OPUS_1M_TIERS,
	},
	"claude-opus-4-5-20251101": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoning: true,
		inputPrice: 5.0,
		outputPrice: 25.0,
		cacheWritesPrice: 6.25,
		cacheReadsPrice: 0.5,
	},
	"claude-opus-4-1-20250805": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoning: true,
		inputPrice: 15.0,
		outputPrice: 75.0,
		cacheWritesPrice: 18.75,
		cacheReadsPrice: 1.5,
	},
	"claude-opus-4-20250514": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoning: true,
		inputPrice: 15.0,
		outputPrice: 75.0,
		cacheWritesPrice: 18.75,
		cacheReadsPrice: 1.5,
	},
	"claude-3-7-sonnet-20250219": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,

		supportsPromptCache: true,
		supportsReasoning: true,
		inputPrice: 3.0,
		outputPrice: 15.0,
		cacheWritesPrice: 3.75,
		cacheReadsPrice: 0.3,
	},
	"claude-3-5-sonnet-20241022": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,

		supportsPromptCache: true,
		inputPrice: 3.0, // $3 per million input tokens
		outputPrice: 15.0, // $15 per million output tokens
		cacheWritesPrice: 3.75, // $3.75 per million tokens
		cacheReadsPrice: 0.3, // $0.30 per million tokens
	},
	"claude-3-5-haiku-20241022": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 0.8,
		outputPrice: 4.0,
		cacheWritesPrice: 1.0,
		cacheReadsPrice: 0.08,
	},
	"claude-3-opus-20240229": {
		maxTokens: 4096,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 15.0,
		outputPrice: 75.0,
		cacheWritesPrice: 18.75,
		cacheReadsPrice: 1.5,
	},
	"claude-3-haiku-20240307": {
		maxTokens: 4096,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 0.25,
		outputPrice: 1.25,
		cacheWritesPrice: 0.3,
		cacheReadsPrice: 0.03,
	},
} as const satisfies Record<string, ModelInfo> // as const assertion makes the object deeply readonly

// Claude Code
export type ClaudeCodeModelId = keyof typeof claudeCodeModels
export const claudeCodeDefaultModelId: ClaudeCodeModelId = "claude-sonnet-4-5-20250929"
export const claudeCodeModels = {
	sonnet: {
		...anthropicModels["claude-sonnet-4-5-20250929"],
		supportsImages: false,
		supportsPromptCache: false,
	},
	"sonnet[1m]": {
		...anthropicModels["claude-sonnet-4-5-20250929:1m"],
		supportsImages: false,
		supportsPromptCache: false,
	},
	opus: {
		...anthropicModels["claude-opus-4-6"],
		supportsImages: false,
		supportsPromptCache: false,
	},
	"opus[1m]": {
		...anthropicModels["claude-opus-4-6:1m"],
		supportsImages: false,
		supportsPromptCache: false,
	},
	"claude-haiku-4-5-20251001": {
		...anthropicModels["claude-haiku-4-5-20251001"],
		supportsImages: false,
		supportsPromptCache: false,
	},
	"claude-sonnet-4-5-20250929": {
		...anthropicModels["claude-sonnet-4-5-20250929"],
		supportsImages: false,
		supportsPromptCache: false,
	},
	"claude-sonnet-4-5-20250929[1m]": {
		...anthropicModels["claude-sonnet-4-5-20250929:1m"],
		supportsImages: false,
		supportsPromptCache: false,
	},
	"claude-sonnet-4-20250514": {
		...anthropicModels["claude-sonnet-4-20250514"],
		supportsImages: false,
		supportsPromptCache: false,
	},
	"claude-opus-4-6": {
		...anthropicModels["claude-opus-4-6"],
		supportsImages: false,
		supportsPromptCache: false,
	},
	"claude-opus-4-6[1m]": {
		...anthropicModels["claude-opus-4-6:1m"],
		supportsImages: false,
		supportsPromptCache: false,
	},
	"claude-opus-4-5-20251101": {
		...anthropicModels["claude-opus-4-5-20251101"],
		supportsImages: false,
		supportsPromptCache: false,
	},
	"claude-opus-4-1-20250805": {
		...anthropicModels["claude-opus-4-1-20250805"],
		supportsImages: false,
		supportsPromptCache: false,
	},
	"claude-opus-4-20250514": {
		...anthropicModels["claude-opus-4-20250514"],
		supportsImages: false,
		supportsPromptCache: false,
	},
	"claude-3-7-sonnet-20250219": {
		...anthropicModels["claude-3-7-sonnet-20250219"],
		supportsImages: false,
		supportsPromptCache: false,
	},
	"claude-3-5-haiku-20241022": {
		...anthropicModels["claude-3-5-haiku-20241022"],
		supportsImages: true,
		supportsPromptCache: false,
	},
} as const satisfies Record<string, ModelInfo>

// AWS Bedrock
// https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html
export type BedrockModelId = keyof typeof bedrockModels
export const bedrockDefaultModelId: BedrockModelId = "anthropic.claude-sonnet-4-5-20250929-v1:0"
export const bedrockModels = {
	"anthropic.claude-sonnet-4-5-20250929-v1:0": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoning: true,
		supportsGlobalEndpoint: true,
		inputPrice: 3.0,
		outputPrice: 15.0,
		cacheWritesPrice: 3.75,
		cacheReadsPrice: 0.3,
	},
	"anthropic.claude-sonnet-4-5-20250929-v1:0:1m": {
		maxTokens: 8192,
		contextWindow: 1_000_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoning: true,
		supportsGlobalEndpoint: true,
		inputPrice: 3.0,
		outputPrice: 15.0,
		cacheWritesPrice: 3.75,
		cacheReadsPrice: 0.3,
		tiers: CLAUDE_SONNET_1M_TIERS,
	},
	"anthropic.claude-haiku-4-5-20251001-v1:0": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoning: true,
		supportsGlobalEndpoint: true,
		inputPrice: 1,
		outputPrice: 5.0,
		cacheWritesPrice: 1.25,
		cacheReadsPrice: 0.1,
	},
	"anthropic.claude-sonnet-4-20250514-v1:0": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoning: true,
		supportsGlobalEndpoint: true,
		inputPrice: 3.0,
		outputPrice: 15.0,
		cacheWritesPrice: 3.75,
		cacheReadsPrice: 0.3,
	},
	"anthropic.claude-sonnet-4-20250514-v1:0:1m": {
		maxTokens: 8192,
		contextWindow: 1_000_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoning: true,
		supportsGlobalEndpoint: true,
		inputPrice: 3.0,
		outputPrice: 15.0,
		cacheWritesPrice: 3.75,
		cacheReadsPrice: 0.3,
		tiers: CLAUDE_SONNET_1M_TIERS,
	},
	"anthropic.claude-opus-4-6-v1": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoning: true,
		supportsGlobalEndpoint: true,
		inputPrice: 5.0,
		outputPrice: 25.0,
		cacheWritesPrice: 6.25,
		cacheReadsPrice: 0.5,
	},
	"anthropic.claude-opus-4-6-v1:1m": {
		maxTokens: 8192,
		contextWindow: 1_000_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoning: true,
		supportsGlobalEndpoint: true,
		inputPrice: 5.0,
		outputPrice: 25.0,
		cacheWritesPrice: 6.25,
		cacheReadsPrice: 0.5,
		tiers: CLAUDE_OPUS_1M_TIERS,
	},
	"anthropic.claude-opus-4-5-20251101-v1:0": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoning: true,
		supportsGlobalEndpoint: true,
		inputPrice: 5.0,
		outputPrice: 25.0,
		cacheWritesPrice: 6.25,
		cacheReadsPrice: 0.5,
	},
	"anthropic.claude-opus-4-20250514-v1:0": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoning: true,
		inputPrice: 15.0,
		outputPrice: 75.0,
		cacheWritesPrice: 18.75,
		cacheReadsPrice: 1.5,
	},
	"anthropic.claude-opus-4-1-20250805-v1:0": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoning: true,
		inputPrice: 15.0,
		outputPrice: 75.0,
		cacheWritesPrice: 18.75,
		cacheReadsPrice: 1.5,
	},
	"amazon.nova-premier-v1:0": {
		maxTokens: 10_000,
		contextWindow: 1_000_000,
		supportsImages: true,

		supportsPromptCache: false,
		inputPrice: 2.5,
		outputPrice: 12.5,
	},
	"amazon.nova-pro-v1:0": {
		maxTokens: 5000,
		contextWindow: 300_000,
		supportsImages: true,

		supportsPromptCache: true,
		inputPrice: 0.8,
		outputPrice: 3.2,
		// cacheWritesPrice: 3.2, // not written
		cacheReadsPrice: 0.2,
	},
	"amazon.nova-lite-v1:0": {
		maxTokens: 5000,
		contextWindow: 300_000,
		supportsImages: true,

		supportsPromptCache: true,
		inputPrice: 0.06,
		outputPrice: 0.24,
		// cacheWritesPrice: 0.24, // not written
		cacheReadsPrice: 0.015,
	},
	"amazon.nova-2-lite-v1:0": {
		maxTokens: 5000,
		contextWindow: 1_000_000,
		supportsImages: true,

		supportsPromptCache: true,
		inputPrice: 0.3,
		outputPrice: 2.5,
		// cacheWritesPrice: 2.5, // not written
		cacheReadsPrice: 0.075,
		supportsGlobalEndpoint: true,
	},
	"amazon.nova-micro-v1:0": {
		maxTokens: 5000,
		contextWindow: 128_000,
		supportsImages: false,

		supportsPromptCache: true,
		inputPrice: 0.035,
		outputPrice: 0.14,
		// cacheWritesPrice: 0.14, // not written
		cacheReadsPrice: 0.00875,
	},
	"anthropic.claude-3-7-sonnet-20250219-v1:0": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,

		supportsPromptCache: true,
		supportsReasoning: true,
		inputPrice: 3.0,
		outputPrice: 15.0,
		cacheWritesPrice: 3.75,
		cacheReadsPrice: 0.3,
	},
	"anthropic.claude-3-5-sonnet-20241022-v2:0": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,

		supportsPromptCache: true,
		inputPrice: 3.0,
		outputPrice: 15.0,
		cacheWritesPrice: 3.75,
		cacheReadsPrice: 0.3,
	},
	"anthropic.claude-3-5-haiku-20241022-v1:0": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 0.8,
		outputPrice: 4.0,
		cacheWritesPrice: 1.0,
		cacheReadsPrice: 0.08,
	},
	"anthropic.claude-3-5-sonnet-20240620-v1:0": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 3.0,
		outputPrice: 15.0,
	},
	"anthropic.claude-3-opus-20240229-v1:0": {
		maxTokens: 4096,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 15.0,
		outputPrice: 75.0,
	},
	"anthropic.claude-3-sonnet-20240229-v1:0": {
		maxTokens: 4096,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 3.0,
		outputPrice: 15.0,
	},
	"anthropic.claude-3-haiku-20240307-v1:0": {
		maxTokens: 4096,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0.25,
		outputPrice: 1.25,
	},
	"deepseek.r1-v1:0": {
		maxTokens: 8_000,
		contextWindow: 64_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 1.35,
		outputPrice: 5.4,
	},
	"openai.gpt-oss-120b-1:0": {
		maxTokens: 8192,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.15,
		outputPrice: 0.6,
		description:
			"A state-of-the-art 120B open-weight Mixture-of-Experts language model optimized for strong reasoning, tool use, and efficient deployment on large GPUs",
	},
	"openai.gpt-oss-20b-1:0": {
		maxTokens: 8192,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.07,
		outputPrice: 0.3,
		description:
			"A compact 20B open-weight Mixture-of-Experts language model designed for strong reasoning and tool use, ideal for edge devices and local inference.",
	},
	"qwen.qwen3-coder-30b-a3b-v1:0": {
		maxTokens: 8192,
		contextWindow: 262_144,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.15,
		outputPrice: 0.6,
		description:
			"Qwen3 Coder 30B MoE model with 3.3B activated parameters, optimized for code generation and analysis with 256K context window.",
	},
	"qwen.qwen3-coder-480b-a35b-v1:0": {
		maxTokens: 8192,
		contextWindow: 262_144,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.22,
		outputPrice: 1.8,
		description:
			"Qwen3 Coder 480B flagship MoE model with 35B activated parameters, designed for complex coding tasks with advanced reasoning capabilities and 256K context window.",
	},
} as const satisfies Record<string, ModelInfo>

// OpenRouter
// https://openrouter.ai/models?order=newest&supported_parameters=tools
export const openRouterDefaultModelId = "anthropic/claude-sonnet-4.5" // will always exist in openRouterModels
export const openRouterClaudeSonnet41mModelId = `anthropic/claude-sonnet-4${CLAUDE_SONNET_1M_SUFFIX}`
export const openRouterClaudeSonnet451mModelId = `anthropic/claude-sonnet-4.5${CLAUDE_SONNET_1M_SUFFIX}`
export const openRouterClaudeOpus461mModelId = `anthropic/claude-opus-4.6${CLAUDE_SONNET_1M_SUFFIX}`
export const openRouterDefaultModelInfo: ModelInfo = {
	maxTokens: 8192,
	contextWindow: 200_000,
	supportsImages: true,
	supportsPromptCache: true,
	inputPrice: 3.0,
	outputPrice: 15.0,
	cacheWritesPrice: 3.75,
	cacheReadsPrice: 0.3,
	description:
		"Claude Sonnet 4.5 delivers superior intelligence across coding, agentic search, and AI agent capabilities. It's a powerful choice for agentic coding, and can complete tasks across the entire software development lifecycle—from initial planning to bug fixes, maintenance to large refactors. It offers strong performance in both planning and solving for complex coding tasks, making it an ideal choice to power end-to-end software development processes.\n\nRead more in the [blog post here](https://www.anthropic.com/claude/sonnet)",
}

// Cline custom model - Devstral
export const clineDevstralModelInfo: ModelInfo = {
	contextWindow: 256000,
	supportsImages: false,
	supportsPromptCache: false,
	inputPrice: 0,
	outputPrice: 0,
	cacheReadsPrice: 0,
	cacheWritesPrice: 0,
	description: "A stealth model for agentic coding tasks",
}

export const OPENROUTER_PROVIDER_PREFERENCES: Record<string, { order: string[]; allow_fallbacks: boolean }> = {
	// Exacto Providers
	"moonshotai/kimi-k2:exacto": {
		order: ["groq", "moonshotai"],
		allow_fallbacks: false,
	},
	"z-ai/glm-4.6:exacto": {
		order: ["z-ai", "novita"],
		allow_fallbacks: false,
	},
	"deepseek/deepseek-v3.1-terminus:exacto": {
		order: ["novita", "deepinfra"],
		allow_fallbacks: false,
	},
	"qwen/qwen3-coder:exacto": {
		order: ["baseten"],
		allow_fallbacks: false,
	},
	"openai/gpt-oss-120b:exacto": {
		order: ["groq", "novita"],
		allow_fallbacks: false,
	},

	// Normal Providers
	"moonshotai/kimi-k2": {
		order: ["groq", "fireworks", "baseten", "parasail", "novita", "deepinfra"],
		allow_fallbacks: false,
	},
	"qwen/qwen3-coder": {
		order: ["nebius", "baseten", "fireworks", "together", "deepinfra"],
		allow_fallbacks: false,
	},
	"qwen/qwen3-235b-a22b-thinking-2507": {
		order: ["nebius", "baseten", "fireworks", "together", "deepinfra"],
		allow_fallbacks: false,
	},
	"qwen/qwen3-235b-a22b-07-25": {
		order: ["nebius", "baseten", "fireworks", "together", "deepinfra"],
		allow_fallbacks: false,
	},
	"qwen/qwen3-30b-a3b-thinking-2507": {
		order: ["nebius", "baseten", "fireworks", "together", "deepinfra"],
		allow_fallbacks: false,
	},
	"qwen/qwen3-30b-a3b-instruct-2507": {
		order: ["nebius", "baseten", "fireworks", "together", "deepinfra"],
		allow_fallbacks: false,
	},
	"qwen/qwen3-30b-a3b:free": {
		order: ["nebius", "baseten", "fireworks", "together", "deepinfra"],
		allow_fallbacks: false,
	},
	"qwen/qwen3-next-80b-a3b-thinking": {
		order: ["nebius", "baseten", "fireworks", "together", "deepinfra"],
		allow_fallbacks: false,
	},
	"qwen/qwen3-next-80b-a3b-instruct": {
		order: ["nebius", "baseten", "fireworks", "together", "deepinfra"],
		allow_fallbacks: false,
	},
	"qwen/qwen3-max": {
		order: ["nebius", "baseten", "fireworks", "together", "deepinfra"],
		allow_fallbacks: false,
	},
	"deepseek/deepseek-v3.2-exp": {
		order: ["deepseek", "novita", "fireworks", "nebius"],
		allow_fallbacks: false,
	},
	"z-ai/glm-4.6": {
		order: ["z-ai", "novita", "baseten", "fireworks", "chutes"],
		allow_fallbacks: false,
	},
	"z-ai/glm-4.5v": {
		order: ["z-ai", "novita", "baseten", "fireworks", "chutes"],
		allow_fallbacks: false,
	},
	"z-ai/glm-4.5": {
		order: ["z-ai", "novita", "baseten", "fireworks", "chutes"],
		allow_fallbacks: false,
	},
	"z-ai/glm-4.5-air": {
		order: ["z-ai", "novita", "baseten", "fireworks", "chutes"],
		allow_fallbacks: false,
	},
}

// Vertex AI
// https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude
// https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models
export type VertexModelId = keyof typeof vertexModels
export const vertexDefaultModelId: VertexModelId = "gemini-3-pro-preview"
export const vertexModels = {
	"gemini-3-pro-preview": {
		maxTokens: 8192,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: true,
		supportsGlobalEndpoint: true,
		inputPrice: 2.0,
		outputPrice: 12.0,
		temperature: 1.0,
		supportsReasoning: true,
		thinkingConfig: {
			geminiThinkingLevel: "high",
			supportsThinkingLevel: true,
		},
	},
	"gemini-3-flash-preview": {
		maxTokens: 65536,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: true,
		supportsGlobalEndpoint: true,
		inputPrice: 0.5,
		outputPrice: 3.0,
		cacheWritesPrice: 0.05,
		temperature: 1.0,
		supportsReasoning: true,
		thinkingConfig: {
			geminiThinkingLevel: "high",
			supportsThinkingLevel: true,
		},
	},
	"claude-sonnet-4-5@20250929": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 3.0,
		outputPrice: 15.0,
		cacheWritesPrice: 3.75,
		cacheReadsPrice: 0.3,
		supportsReasoning: true,
	},
	"claude-sonnet-4@20250514": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 3.0,
		outputPrice: 15.0,
		cacheWritesPrice: 3.75,
		cacheReadsPrice: 0.3,
		supportsReasoning: true,
	},
	"claude-haiku-4-5@20251001": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 1.0,
		outputPrice: 5.0,
		cacheWritesPrice: 1.25,
		cacheReadsPrice: 0.1,
		supportsReasoning: true,
	},
	"claude-opus-4-6": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsGlobalEndpoint: true,
		inputPrice: 5.0,
		outputPrice: 25.0,
		cacheWritesPrice: 6.25,
		cacheReadsPrice: 0.5,
		supportsReasoning: true,
	},
	"claude-opus-4-6:1m": {
		maxTokens: 8192,
		contextWindow: 1_000_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsGlobalEndpoint: true,
		inputPrice: 5.0,
		outputPrice: 25.0,
		cacheWritesPrice: 6.25,
		cacheReadsPrice: 0.5,
		supportsReasoning: true,
		tiers: CLAUDE_OPUS_1M_TIERS,
	},
	"claude-opus-4-5@20251101": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 5.0,
		outputPrice: 25.0,
		cacheWritesPrice: 6.25,
		cacheReadsPrice: 0.5,
		supportsReasoning: true,
	},
	"claude-opus-4-1@20250805": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 15.0,
		outputPrice: 75.0,
		cacheWritesPrice: 18.75,
		cacheReadsPrice: 1.5,
		supportsReasoning: true,
	},
	"claude-opus-4@20250514": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 15.0,
		outputPrice: 75.0,
		cacheWritesPrice: 18.75,
		cacheReadsPrice: 1.5,
		supportsReasoning: true,
	},
	"claude-3-7-sonnet@20250219": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 3.0,
		outputPrice: 15.0,
		cacheWritesPrice: 3.75,
		cacheReadsPrice: 0.3,
		thinkingConfig: {
			maxBudget: 64000,
			outputPrice: 15.0,
		},
		supportsReasoning: true,
	},
	"claude-3-5-sonnet-v2@20241022": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,

		supportsPromptCache: true,
		inputPrice: 3.0,
		outputPrice: 15.0,
		cacheWritesPrice: 3.75,
		cacheReadsPrice: 0.3,
	},
	"claude-3-5-sonnet@20240620": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 3.0,
		outputPrice: 15.0,
		cacheWritesPrice: 3.75,
		cacheReadsPrice: 0.3,
	},
	"claude-3-5-haiku@20241022": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 1.0,
		outputPrice: 5.0,
		cacheWritesPrice: 1.25,
		cacheReadsPrice: 0.1,
	},
	"claude-3-opus@20240229": {
		maxTokens: 4096,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 15.0,
		outputPrice: 75.0,
		cacheWritesPrice: 18.75,
		cacheReadsPrice: 1.5,
	},
	"claude-3-haiku@20240307": {
		maxTokens: 4096,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 0.25,
		outputPrice: 1.25,
		cacheWritesPrice: 0.3,
		cacheReadsPrice: 0.03,
	},
	"mistral-large-2411": {
		maxTokens: 128_000,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 2.0,
		outputPrice: 6.0,
	},
	"mistral-small-2503": {
		maxTokens: 128_000,
		contextWindow: 128_000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0.1,
		outputPrice: 0.3,
	},
	"codestral-2501": {
		maxTokens: 256_000,
		contextWindow: 256_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.3,
		outputPrice: 0.9,
	},
	"llama-4-maverick-17b-128e-instruct-maas": {
		maxTokens: 128_000,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0.35,
		outputPrice: 1.15,
	},
	"llama-4-scout-17b-16e-instruct-maas": {
		maxTokens: 1_000_000,
		contextWindow: 10_485_760,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0.25,
		outputPrice: 0.7,
	},
	"gemini-2.0-flash-001": {
		maxTokens: 8192,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: true,
		supportsGlobalEndpoint: true,
		inputPrice: 0.15,
		outputPrice: 0.6,
		cacheWritesPrice: 1.0,
		cacheReadsPrice: 0.025,
	},
	"gemini-2.0-flash-lite-001": {
		maxTokens: 8192,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: false,
		supportsGlobalEndpoint: true,
		inputPrice: 0.075,
		outputPrice: 0.3,
	},
	"gemini-2.0-flash-thinking-exp-1219": {
		maxTokens: 8192,
		contextWindow: 32_767,
		supportsImages: true,
		supportsPromptCache: false,
		supportsGlobalEndpoint: true,
		inputPrice: 0,
		outputPrice: 0,
	},
	"gemini-2.0-flash-exp": {
		maxTokens: 8192,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: false,
		supportsGlobalEndpoint: true,
		inputPrice: 0,
		outputPrice: 0,
	},
	"gemini-2.5-pro-exp-03-25": {
		maxTokens: 65536,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"gemini-2.5-pro": {
		maxTokens: 65536,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: true,
		supportsGlobalEndpoint: true,
		inputPrice: 2.5,
		outputPrice: 15,
		cacheReadsPrice: 0.625,
		thinkingConfig: {
			maxBudget: 32767,
		},
		tiers: [
			{
				contextWindow: 200000,
				inputPrice: 1.25,
				outputPrice: 10,
				cacheReadsPrice: 0.31,
			},
			{
				contextWindow: Number.POSITIVE_INFINITY,
				inputPrice: 2.5,
				outputPrice: 15,
				cacheReadsPrice: 0.625,
			},
		],
	},
	"gemini-2.5-flash": {
		maxTokens: 65536,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: true,
		supportsGlobalEndpoint: true,
		inputPrice: 0.3,
		outputPrice: 2.5,
		thinkingConfig: {
			maxBudget: 24576,
			outputPrice: 3.5,
		},
	},

	"gemini-2.5-flash-lite-preview-06-17": {
		maxTokens: 64000,
		contextWindow: 1_000_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsGlobalEndpoint: true,
		inputPrice: 0.1,
		outputPrice: 0.4,
		cacheReadsPrice: 0.025,
		description: "Preview version - may not be available in all regions",
		thinkingConfig: {
			maxBudget: 24576,
		},
	},
	"gemini-2.0-flash-thinking-exp-01-21": {
		maxTokens: 65_536,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: false,
		supportsGlobalEndpoint: true,
		inputPrice: 0,
		outputPrice: 0,
	},
	"gemini-exp-1206": {
		maxTokens: 8192,
		contextWindow: 2_097_152,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"gemini-1.5-flash-002": {
		maxTokens: 8192,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 0.15,
		outputPrice: 0.6,
		cacheWritesPrice: 1.0,
		cacheReadsPrice: 0.0375,
		tiers: [
			{
				contextWindow: 128000,
				inputPrice: 0.075,
				outputPrice: 0.3,
				cacheReadsPrice: 0.01875,
			},
			{
				contextWindow: Number.POSITIVE_INFINITY,
				inputPrice: 0.15,
				outputPrice: 0.6,
				cacheReadsPrice: 0.0375,
			},
		],
	},
	"gemini-1.5-flash-exp-0827": {
		maxTokens: 8192,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"gemini-1.5-flash-8b-exp-0827": {
		maxTokens: 8192,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"gemini-1.5-pro-002": {
		maxTokens: 8192,
		contextWindow: 2_097_152,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 1.25,
		outputPrice: 5,
	},
	"gemini-1.5-pro-exp-0827": {
		maxTokens: 8192,
		contextWindow: 2_097_152,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
} as const satisfies Record<string, ModelInfo>

export const vertexGlobalModels: Record<string, ModelInfo> = Object.fromEntries(
	Object.entries(vertexModels).filter(([_k, v]) => Object.hasOwn(v, "supportsGlobalEndpoint")),
) as Record<string, ModelInfo>

export const openAiModelInfoSaneDefaults: OpenAiCompatibleModelInfo = {
	maxTokens: -1,
	contextWindow: 128_000,
	supportsImages: true,
	supportsPromptCache: false,
	isR1FormatRequired: false,
	inputPrice: 0,
	outputPrice: 0,
	temperature: 0,
}

// Gemini
// https://ai.google.dev/gemini-api/docs/models/gemini
export type GeminiModelId = keyof typeof geminiModels
export const geminiDefaultModelId: GeminiModelId = "gemini-3-pro-preview"
export const geminiModels = {
	"gemini-3-pro-preview": {
		maxTokens: 65536,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 4.0,
		outputPrice: 18.0,
		cacheReadsPrice: 0.4,
		thinkingConfig: {
			// If you don't specify a thinking level, Gemini will use the model's default
			// dynamic thinking level, "high", for Gemini 3 Pro Preview.
			geminiThinkingLevel: "high",
			supportsThinkingLevel: true,
		},
		tiers: [
			{
				contextWindow: 200000,
				inputPrice: 2.0,
				outputPrice: 12.0,
				cacheReadsPrice: 0.2,
			},
			{
				contextWindow: Number.POSITIVE_INFINITY,
				inputPrice: 4.0,
				outputPrice: 18.0,
				cacheReadsPrice: 0.4,
			},
		],
	},
	"gemini-3-flash-preview": {
		maxTokens: 65536,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: true,
		supportsGlobalEndpoint: true,
		inputPrice: 0.5,
		outputPrice: 3.0,
		cacheWritesPrice: 0.05,
		supportsReasoning: true,
		thinkingConfig: {
			geminiThinkingLevel: "low",
			supportsThinkingLevel: true,
		},
		tiers: [
			{
				contextWindow: 200000,
				inputPrice: 0.3,
				outputPrice: 2.5,
				cacheReadsPrice: 0.03,
			},
			{
				contextWindow: Number.POSITIVE_INFINITY,
				inputPrice: 0.3,
				outputPrice: 2.5,
				cacheReadsPrice: 0.03,
			},
		],
	},
	"gemini-2.5-pro": {
		maxTokens: 65536,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 2.5,
		outputPrice: 15,
		cacheReadsPrice: 0.625,
		thinkingConfig: {
			maxBudget: 32767,
		},
		tiers: [
			{
				contextWindow: 200000,
				inputPrice: 1.25,
				outputPrice: 10,
				cacheReadsPrice: 0.31,
			},
			{
				contextWindow: Number.POSITIVE_INFINITY,
				inputPrice: 2.5,
				outputPrice: 15,
				cacheReadsPrice: 0.625,
			},
		],
	},
	"gemini-2.5-flash-lite-preview-06-17": {
		maxTokens: 64000,
		contextWindow: 1_000_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsGlobalEndpoint: true,
		inputPrice: 0.1,
		outputPrice: 0.4,
		cacheReadsPrice: 0.025,
		description: "Preview version - may not be available in all regions",
		thinkingConfig: {
			maxBudget: 24576,
		},
	},
	"gemini-2.5-flash": {
		maxTokens: 65536,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 0.3,
		outputPrice: 2.5,
		cacheReadsPrice: 0.075,
		thinkingConfig: {
			maxBudget: 24576,
			outputPrice: 3.5,
		},
	},
	"gemini-2.0-flash-001": {
		maxTokens: 8192,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 0.1,
		outputPrice: 0.4,
		cacheReadsPrice: 0.025,
		cacheWritesPrice: 1.0,
	},
	"gemini-2.0-flash-lite-preview-02-05": {
		maxTokens: 8192,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"gemini-2.0-pro-exp-02-05": {
		maxTokens: 8192,
		contextWindow: 2_097_152,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"gemini-2.0-flash-thinking-exp-01-21": {
		maxTokens: 65_536,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"gemini-2.0-flash-thinking-exp-1219": {
		maxTokens: 8192,
		contextWindow: 32_767,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"gemini-2.0-flash-exp": {
		maxTokens: 8192,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"gemini-1.5-flash-002": {
		maxTokens: 8192,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 0.15, // Default price (highest tier)
		outputPrice: 0.6, // Default price (highest tier)
		cacheReadsPrice: 0.0375,
		cacheWritesPrice: 1.0,
		tiers: [
			{
				contextWindow: 128000,
				inputPrice: 0.075,
				outputPrice: 0.3,
				cacheReadsPrice: 0.01875,
			},
			{
				contextWindow: Number.POSITIVE_INFINITY,
				inputPrice: 0.15,
				outputPrice: 0.6,
				cacheReadsPrice: 0.0375,
			},
		],
	},
	"gemini-1.5-flash-exp-0827": {
		maxTokens: 8192,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"gemini-1.5-flash-8b-exp-0827": {
		maxTokens: 8192,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"gemini-1.5-pro-002": {
		maxTokens: 8192,
		contextWindow: 2_097_152,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"gemini-1.5-pro-exp-0827": {
		maxTokens: 8192,
		contextWindow: 2_097_152,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"gemini-exp-1206": {
		maxTokens: 8192,
		contextWindow: 2_097_152,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
} as const satisfies Record<string, ModelInfo>

// OpenAI Native
// https://openai.com/api/pricing/
export type OpenAiNativeModelId = keyof typeof openAiNativeModels
export const openAiNativeDefaultModelId: OpenAiNativeModelId = "gpt-5.2"
export const openAiNativeModels = {
	"gpt-5.2": {
		maxTokens: 8_192,
		contextWindow: 272000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 1.75,
		outputPrice: 14.0,
		cacheReadsPrice: 0.175,
		temperature: 1,
		systemRole: "developer",
		supportsReasoning: true,
		supportsReasoningEffort: true,
	},
	"gpt-5.2-codex": {
		maxTokens: 8_192, // 128000 breaks context window truncation
		contextWindow: 400000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 1.75,
		outputPrice: 14.0,
		cacheReadsPrice: 0.175,
		apiFormat: ApiFormat.OPENAI_RESPONSES,
		temperature: 1,
		systemRole: "developer",
		supportsReasoning: true,
		supportsReasoningEffort: true,
	},
	"gpt-5.1-2025-11-13": {
		maxTokens: 8_192,
		contextWindow: 272000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 1.25,
		outputPrice: 10.0,
		cacheReadsPrice: 0.125,
		temperature: 1,
		systemRole: "developer",
		supportsReasoning: true,
		supportsReasoningEffort: true,
	},
	"gpt-5.1": {
		maxTokens: 8_192, // 128000 breaks context window truncation
		contextWindow: 272000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 1.25,
		outputPrice: 10.0,
		cacheReadsPrice: 0.125,
		temperature: 1,
		systemRole: "developer",
		supportsReasoning: true,
		supportsReasoningEffort: true,
	},
	"gpt-5.1-codex": {
		maxTokens: 8_192, // 128000 breaks context window truncation
		contextWindow: 400000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 1.25,
		outputPrice: 10.0,
		cacheReadsPrice: 0.125,
		apiFormat: ApiFormat.OPENAI_RESPONSES,
		temperature: 1,
		systemRole: "developer",
		supportsReasoning: true,
		supportsReasoningEffort: true,
	},
	"gpt-5.1-chat-latest": {
		maxTokens: 8_192,
		contextWindow: 400000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 1.25,
		outputPrice: 10,
		cacheReadsPrice: 0.125,
		temperature: 1,
		systemRole: "developer",
		supportsReasoning: true,
		supportsReasoningEffort: true,
	},
	"gpt-5-2025-08-07": {
		maxTokens: 8_192, // 128000 breaks context window truncation
		contextWindow: 272000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 1.25,
		outputPrice: 10.0,
		cacheReadsPrice: 0.125,
		temperature: 1,
		systemRole: "developer",
		supportsReasoning: true,
		supportsReasoningEffort: true,
	},
	"gpt-5-codex": {
		maxTokens: 8_192, // 128000 breaks context window truncation
		contextWindow: 400000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 1.25,
		outputPrice: 10.0,
		cacheReadsPrice: 0.125,
		apiFormat: ApiFormat.OPENAI_RESPONSES,
		temperature: 1,
		systemRole: "developer",
		supportsReasoning: true,
		supportsReasoningEffort: true,
	},
	"gpt-5-mini-2025-08-07": {
		maxTokens: 8_192,
		contextWindow: 272000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 0.25,
		outputPrice: 2.0,
		cacheReadsPrice: 0.025,
		temperature: 1,
		systemRole: "developer",
		supportsReasoning: true,
		supportsReasoningEffort: true,
	},
	"gpt-5-nano-2025-08-07": {
		maxTokens: 8_192,
		contextWindow: 272000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 0.05,
		outputPrice: 0.4,
		cacheReadsPrice: 0.005,
		temperature: 1,
		systemRole: "developer",
		supportsReasoning: true,
		supportsReasoningEffort: true,
	},
	"gpt-5-chat-latest": {
		maxTokens: 8_192,
		contextWindow: 400000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 1.25,
		outputPrice: 10,
		cacheReadsPrice: 0.125,
		temperature: 1,
		systemRole: "developer",
		supportsReasoning: true,
		supportsReasoningEffort: true,
	},
	o3: {
		maxTokens: 100_000,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 2.0,
		outputPrice: 8.0,
		cacheReadsPrice: 0.5,
		systemRole: "developer",
		supportsReasoning: true,
		supportsReasoningEffort: true,
		supportsTools: false,
	},
	"o4-mini": {
		maxTokens: 100_000,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 1.1,
		outputPrice: 4.4,
		cacheReadsPrice: 0.275,
		systemRole: "developer",
		supportsReasoning: true,
		supportsReasoningEffort: true,
		supportsTools: false,
	},
	"gpt-4.1": {
		maxTokens: 32_768,
		contextWindow: 1_047_576,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 2,
		outputPrice: 8,
		cacheReadsPrice: 0.5,
		temperature: 0,
	},
	"gpt-4.1-mini": {
		maxTokens: 32_768,
		contextWindow: 1_047_576,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 0.4,
		outputPrice: 1.6,
		cacheReadsPrice: 0.1,
		temperature: 0,
	},
	"gpt-4.1-nano": {
		maxTokens: 32_768,
		contextWindow: 1_047_576,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 0.1,
		outputPrice: 0.4,
		cacheReadsPrice: 0.025,
		temperature: 0,
	},
	"o3-mini": {
		maxTokens: 100_000,
		contextWindow: 200_000,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 1.1,
		outputPrice: 4.4,
		cacheReadsPrice: 0.55,
		systemRole: "developer",
		supportsReasoning: true,
		supportsReasoningEffort: true,
		supportsTools: false,
	},
	// don't support tool use yet
	o1: {
		maxTokens: 100_000,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 15,
		outputPrice: 60,
		cacheReadsPrice: 7.5,
		supportsStreaming: false,
	},
	"o1-preview": {
		maxTokens: 32_768,
		contextWindow: 128_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 15,
		outputPrice: 60,
		cacheReadsPrice: 7.5,
		supportsStreaming: false,
	},
	"o1-mini": {
		maxTokens: 65_536,
		contextWindow: 128_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 1.1,
		outputPrice: 4.4,
		cacheReadsPrice: 0.55,
		supportsStreaming: false,
	},
	"gpt-4o": {
		maxTokens: 4_096,
		contextWindow: 128_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 2.5,
		outputPrice: 10,
		cacheReadsPrice: 1.25,
		temperature: 0,
	},
	"gpt-4o-mini": {
		maxTokens: 16_384,
		contextWindow: 128_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 0.15,
		outputPrice: 0.6,
		cacheReadsPrice: 0.075,
		temperature: 0,
	},
	"chatgpt-4o-latest": {
		maxTokens: 16_384,
		contextWindow: 128_000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 5,
		outputPrice: 15,
		temperature: 0,
	},
} as const satisfies Record<string, OpenAiCompatibleModelInfo>

// OpenAI Codex (ChatGPT Plus/Pro subscription)
// Uses OAuth authentication via ChatGPT, routes to chatgpt.com/backend-api/codex/responses
// Subscription-based pricing (all costs are $0)
export type OpenAiCodexModelId = keyof typeof openAiCodexModels
export const openAiCodexDefaultModelId: OpenAiCodexModelId = "gpt-5.3-codex"
export const openAiCodexModels = {
	"gpt-5.3-codex": {
		maxTokens: 128_000,
		contextWindow: 400_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoning: true,
		apiFormat: ApiFormat.OPENAI_RESPONSES,
		// Subscription-based: no per-token costs
		inputPrice: 0,
		outputPrice: 0,
		description: "GPT-5.3 Codex: OpenAI's latest flagship coding model via ChatGPT subscription",
	},
	"gpt-5.2-codex": {
		maxTokens: 128_000,
		contextWindow: 400_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoning: true,
		apiFormat: ApiFormat.OPENAI_RESPONSES,
		// Subscription-based: no per-token costs
		inputPrice: 0,
		outputPrice: 0,
		description: "GPT-5.2 Codex: OpenAI's flagship coding model via ChatGPT subscription",
	},
	"gpt-5.1-codex-max": {
		maxTokens: 128_000,
		contextWindow: 400_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoning: true,
		apiFormat: ApiFormat.OPENAI_RESPONSES,
		inputPrice: 0,
		outputPrice: 0,
		description: "GPT-5.1 Codex Max: Maximum capability coding model via ChatGPT subscription",
	},
	"gpt-5.1-codex-mini": {
		maxTokens: 128_000,
		contextWindow: 400_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoning: true,
		apiFormat: ApiFormat.OPENAI_RESPONSES,
		inputPrice: 0,
		outputPrice: 0,
		description: "GPT-5.1 Codex Mini: Faster version for coding tasks via ChatGPT subscription",
	},
	"gpt-5.2": {
		maxTokens: 128_000,
		contextWindow: 400_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoning: true,
		apiFormat: ApiFormat.OPENAI_RESPONSES,
		inputPrice: 0,
		outputPrice: 0,
		description: "GPT-5.2: Latest GPT model via ChatGPT subscription",
	},
} as const satisfies Record<string, ModelInfo>

// Azure OpenAI
// https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation
// https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#api-specs
export const azureOpenAiDefaultApiVersion = "2024-08-01-preview"

// DeepSeek
// https://api-docs.deepseek.com/quick_start/pricing
export type DeepSeekModelId = keyof typeof deepSeekModels
export const deepSeekDefaultModelId: DeepSeekModelId = "deepseek-chat"
export const deepSeekModels = {
	"deepseek-chat": {
		maxTokens: 8_000,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: true, // supports context caching, but not in the way anthropic does it (deepseek reports input tokens and reads/writes in the same usage report) FIXME: we need to show users cache stats how deepseek does it
		inputPrice: 0, // technically there is no input price, it's all either a cache hit or miss (ApiOptions will not show this). Input is the sum of cache reads and writes
		outputPrice: 1.1,
		cacheWritesPrice: 0.27,
		cacheReadsPrice: 0.07,
	},
	"deepseek-reasoner": {
		maxTokens: 8_000,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: true, // supports context caching, but not in the way anthropic does it (deepseek reports input tokens and reads/writes in the same usage report) FIXME: we need to show users cache stats how deepseek does it
		inputPrice: 0, // technically there is no input price, it's all either a cache hit or miss (ApiOptions will not show this)
		outputPrice: 2.19,
		cacheWritesPrice: 0.55,
		cacheReadsPrice: 0.14,
	},
} as const satisfies Record<string, ModelInfo>

// Hugging Face Inference Providers
// https://huggingface.co/docs/inference-providers/en/index
export type HuggingFaceModelId = keyof typeof huggingFaceModels
export const huggingFaceDefaultModelId: HuggingFaceModelId = "moonshotai/Kimi-K2-Instruct"
export const huggingFaceModels = {
	"openai/gpt-oss-120b": {
		maxTokens: 32766,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
		description:
			"Large open-weight reasoning model for high-end desktops and data centers, built for complex coding, math, and general AI tasks.",
	},
	"openai/gpt-oss-20b": {
		maxTokens: 32766,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
		description:
			"Medium open-weight reasoning model that runs on most desktops, balancing strong reasoning with broad accessibility.",
	},
	"moonshotai/Kimi-K2-Instruct": {
		maxTokens: 131_072,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
		description: "Advanced reasoning model with superior performance across coding, math, and general capabilities.",
	},
	"deepseek-ai/DeepSeek-V3-0324": {
		maxTokens: 8192,
		contextWindow: 64_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
		description: "Advanced reasoning model with superior performance across coding, math, and general capabilities.",
	},
	"deepseek-ai/DeepSeek-R1": {
		maxTokens: 8192,
		contextWindow: 64_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
		description: "DeepSeek's reasoning model with step-by-step thinking capabilities.",
	},
	"deepseek-ai/DeepSeek-R1-0528": {
		maxTokens: 64_000,
		contextWindow: 64_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
		description: "DeepSeek's reasoning model's latest version with step-by-step thinking capabilities",
	},
	"meta-llama/Llama-3.1-8B-Instruct": {
		maxTokens: 8192,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
		description: "Efficient 8B parameter Llama model for general-purpose tasks.",
	},
} as const satisfies Record<string, ModelInfo>

// Qwen
// https://bailian.console.aliyun.com/
// The first model in the list is used as the default model for each region
export const internationalQwenModels = {
	"qwen3-coder-plus": {
		maxTokens: 65_536,
		contextWindow: 1_000_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 1,
		outputPrice: 5,
	},
	"qwen3-coder-480b-a35b-instruct": {
		maxTokens: 65_536,
		contextWindow: 204_800,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 1.5,
		outputPrice: 7.5,
	},
	"qwen3-235b-a22b": {
		maxTokens: 16_384,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 2,
		outputPrice: 8,
		cacheWritesPrice: 2,
		cacheReadsPrice: 8,
		thinkingConfig: {
			maxBudget: 38_912,
			outputPrice: 20,
		},
	},
	"qwen3-32b": {
		maxTokens: 16_384,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 2,
		outputPrice: 8,
		cacheWritesPrice: 2,
		cacheReadsPrice: 8,
		thinkingConfig: {
			maxBudget: 38_912,
			outputPrice: 20,
		},
	},
	"qwen3-30b-a3b": {
		maxTokens: 16_384,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.75,
		outputPrice: 3,
		cacheWritesPrice: 0.75,
		cacheReadsPrice: 3,
		thinkingConfig: {
			maxBudget: 38_912,
			outputPrice: 7.5,
		},
	},
	"qwen3-14b": {
		maxTokens: 8_192,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 1,
		outputPrice: 4,
		cacheWritesPrice: 1,
		cacheReadsPrice: 4,
		thinkingConfig: {
			maxBudget: 38_912,
			outputPrice: 10,
		},
	},
	"qwen3-8b": {
		maxTokens: 8_192,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.5,
		outputPrice: 2,
		cacheWritesPrice: 0.5,
		cacheReadsPrice: 2,
		thinkingConfig: {
			maxBudget: 38_912,
			outputPrice: 5,
		},
	},
	"qwen3-4b": {
		maxTokens: 8_192,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.3,
		outputPrice: 1.2,
		cacheWritesPrice: 0.3,
		cacheReadsPrice: 1.2,
		thinkingConfig: {
			maxBudget: 38_912,
			outputPrice: 3,
		},
	},
	"qwen3-1.7b": {
		maxTokens: 8_192,
		contextWindow: 32_768,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.3,
		outputPrice: 1.2,
		cacheWritesPrice: 0.3,
		cacheReadsPrice: 1.2,
		thinkingConfig: {
			maxBudget: 30_720,
			outputPrice: 3,
		},
	},
	"qwen3-0.6b": {
		maxTokens: 8_192,
		contextWindow: 32_768,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.3,
		outputPrice: 1.2,
		cacheWritesPrice: 0.3,
		cacheReadsPrice: 1.2,
		thinkingConfig: {
			maxBudget: 30_720,
			outputPrice: 3,
		},
	},
	"qwen2.5-coder-32b-instruct": {
		maxTokens: 8_192,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.002,
		outputPrice: 0.006,
		cacheWritesPrice: 0.002,
		cacheReadsPrice: 0.006,
	},
	"qwen2.5-coder-14b-instruct": {
		maxTokens: 8_192,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.002,
		outputPrice: 0.006,
		cacheWritesPrice: 0.002,
		cacheReadsPrice: 0.006,
	},
	"qwen2.5-coder-7b-instruct": {
		maxTokens: 8_192,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.001,
		outputPrice: 0.002,
		cacheWritesPrice: 0.001,
		cacheReadsPrice: 0.002,
	},
	"qwen2.5-coder-3b-instruct": {
		maxTokens: 8_192,
		contextWindow: 32_768,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.0,
		outputPrice: 0.0,
		cacheWritesPrice: 0.0,
		cacheReadsPrice: 0.0,
	},
	"qwen2.5-coder-1.5b-instruct": {
		maxTokens: 8_192,
		contextWindow: 32_768,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.0,
		outputPrice: 0.0,
		cacheWritesPrice: 0.0,
		cacheReadsPrice: 0.0,
	},
	"qwen2.5-coder-0.5b-instruct": {
		maxTokens: 8_192,
		contextWindow: 32_768,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.0,
		outputPrice: 0.0,
		cacheWritesPrice: 0.0,
		cacheReadsPrice: 0.0,
	},
	"qwen-coder-plus-latest": {
		maxTokens: 129_024,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 3.5,
		outputPrice: 7,
		cacheWritesPrice: 3.5,
		cacheReadsPrice: 7,
	},
	"qwen-plus-latest": {
		maxTokens: 16_384,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.8,
		outputPrice: 2,
		cacheWritesPrice: 0.8,
		cacheReadsPrice: 2,
		thinkingConfig: {
			maxBudget: 38_912,
			outputPrice: 16,
		},
	},
	"qwen-turbo-latest": {
		maxTokens: 16_384,
		contextWindow: 1_000_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.3,
		outputPrice: 0.6,
		cacheWritesPrice: 0.3,
		cacheReadsPrice: 0.6,
		thinkingConfig: {
			maxBudget: 38_912,
			outputPrice: 6,
		},
	},
	"qwen-max-latest": {
		maxTokens: 30_720,
		contextWindow: 32_768,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 2.4,
		outputPrice: 9.6,
		cacheWritesPrice: 2.4,
		cacheReadsPrice: 9.6,
	},
	"qwen-coder-plus": {
		maxTokens: 129_024,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 3.5,
		outputPrice: 7,
		cacheWritesPrice: 3.5,
		cacheReadsPrice: 7,
	},
	"qwen-plus": {
		maxTokens: 129_024,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.8,
		outputPrice: 2,
		cacheWritesPrice: 0.8,
		cacheReadsPrice: 0.2,
	},
	"qwen-turbo": {
		maxTokens: 1_000_000,
		contextWindow: 1_000_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.3,
		outputPrice: 0.6,
		cacheWritesPrice: 0.3,
		cacheReadsPrice: 0.6,
	},
	"qwen-max": {
		maxTokens: 30_720,
		contextWindow: 32_768,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 2.4,
		outputPrice: 9.6,
		cacheWritesPrice: 2.4,
		cacheReadsPrice: 9.6,
	},
	"deepseek-v3": {
		maxTokens: 8_000,
		contextWindow: 64_000,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 0,
		outputPrice: 0.28,
		cacheWritesPrice: 0.14,
		cacheReadsPrice: 0.014,
	},
	"deepseek-r1": {
		maxTokens: 8_000,
		contextWindow: 64_000,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 0,
		outputPrice: 2.19,
		cacheWritesPrice: 0.55,
		cacheReadsPrice: 0.14,
	},
	"qwen-vl-max": {
		maxTokens: 30_720,
		contextWindow: 32_768,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 3,
		outputPrice: 9,
		cacheWritesPrice: 3,
		cacheReadsPrice: 9,
	},
	"qwen-vl-max-latest": {
		maxTokens: 129_024,
		contextWindow: 131_072,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 3,
		outputPrice: 9,
		cacheWritesPrice: 3,
		cacheReadsPrice: 9,
	},
	"qwen-vl-plus": {
		maxTokens: 6_000,
		contextWindow: 8_000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 1.5,
		outputPrice: 4.5,
		cacheWritesPrice: 1.5,
		cacheReadsPrice: 4.5,
	},
	"qwen-vl-plus-latest": {
		maxTokens: 129_024,
		contextWindow: 131_072,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 1.5,
		outputPrice: 4.5,
		cacheWritesPrice: 1.5,
		cacheReadsPrice: 4.5,
	},
} as const satisfies Record<string, ModelInfo>

export const mainlandQwenModels = {
	"qwen3-235b-a22b": {
		maxTokens: 16_384,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 2,
		outputPrice: 8,
		cacheWritesPrice: 2,
		cacheReadsPrice: 8,
		thinkingConfig: {
			maxBudget: 38_912,
			outputPrice: 20,
		},
	},
	"qwen3-32b": {
		maxTokens: 16_384,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 2,
		outputPrice: 8,
		cacheWritesPrice: 2,
		cacheReadsPrice: 8,
		thinkingConfig: {
			maxBudget: 38_912,
			outputPrice: 20,
		},
	},
	"qwen3-30b-a3b": {
		maxTokens: 16_384,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.75,
		outputPrice: 3,
		cacheWritesPrice: 0.75,
		cacheReadsPrice: 3,
		thinkingConfig: {
			maxBudget: 38_912,
			outputPrice: 7.5,
		},
	},
	"qwen3-14b": {
		maxTokens: 8_192,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 1,
		outputPrice: 4,
		cacheWritesPrice: 1,
		cacheReadsPrice: 4,
		thinkingConfig: {
			maxBudget: 38_912,
			outputPrice: 10,
		},
	},
	"qwen3-8b": {
		maxTokens: 8_192,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.5,
		outputPrice: 2,
		cacheWritesPrice: 0.5,
		cacheReadsPrice: 2,
		thinkingConfig: {
			maxBudget: 38_912,
			outputPrice: 5,
		},
	},
	"qwen3-4b": {
		maxTokens: 8_192,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.3,
		outputPrice: 1.2,
		cacheWritesPrice: 0.3,
		cacheReadsPrice: 1.2,
		thinkingConfig: {
			maxBudget: 38_912,
			outputPrice: 3,
		},
	},
	"qwen3-1.7b": {
		maxTokens: 8_192,
		contextWindow: 32_768,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.3,
		outputPrice: 1.2,
		cacheWritesPrice: 0.3,
		cacheReadsPrice: 1.2,
		thinkingConfig: {
			maxBudget: 30_720,
			outputPrice: 3,
		},
	},
	"qwen3-0.6b": {
		maxTokens: 8_192,
		contextWindow: 32_768,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.3,
		outputPrice: 1.2,
		cacheWritesPrice: 0.3,
		cacheReadsPrice: 1.2,
		thinkingConfig: {
			maxBudget: 30_720,
			outputPrice: 3,
		},
	},
	"qwen2.5-coder-32b-instruct": {
		maxTokens: 8_192,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.002,
		outputPrice: 0.006,
		cacheWritesPrice: 0.002,
		cacheReadsPrice: 0.006,
	},
	"qwen2.5-coder-14b-instruct": {
		maxTokens: 8_192,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.002,
		outputPrice: 0.006,
		cacheWritesPrice: 0.002,
		cacheReadsPrice: 0.006,
	},
	"qwen2.5-coder-7b-instruct": {
		maxTokens: 8_192,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.001,
		outputPrice: 0.002,
		cacheWritesPrice: 0.001,
		cacheReadsPrice: 0.002,
	},
	"qwen2.5-coder-3b-instruct": {
		maxTokens: 8_192,
		contextWindow: 32_768,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.0,
		outputPrice: 0.0,
		cacheWritesPrice: 0.0,
		cacheReadsPrice: 0.0,
	},
	"qwen2.5-coder-1.5b-instruct": {
		maxTokens: 8_192,
		contextWindow: 32_768,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.0,
		outputPrice: 0.0,
		cacheWritesPrice: 0.0,
		cacheReadsPrice: 0.0,
	},
	"qwen2.5-coder-0.5b-instruct": {
		maxTokens: 8_192,
		contextWindow: 32_768,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.0,
		outputPrice: 0.0,
		cacheWritesPrice: 0.0,
		cacheReadsPrice: 0.0,
	},
	"qwen-coder-plus-latest": {
		maxTokens: 129_024,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 3.5,
		outputPrice: 7,
		cacheWritesPrice: 3.5,
		cacheReadsPrice: 7,
	},
	"qwen-plus-latest": {
		maxTokens: 16_384,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.8,
		outputPrice: 2,
		cacheWritesPrice: 0.8,
		cacheReadsPrice: 2,
		thinkingConfig: {
			maxBudget: 38_912,
			outputPrice: 16,
		},
	},
	"qwen-turbo-latest": {
		maxTokens: 16_384,
		contextWindow: 1_000_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.3,
		outputPrice: 0.6,
		cacheWritesPrice: 0.3,
		cacheReadsPrice: 0.6,
		thinkingConfig: {
			maxBudget: 38_912,
			outputPrice: 6,
		},
	},
	"qwen-max-latest": {
		maxTokens: 30_720,
		contextWindow: 32_768,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 2.4,
		outputPrice: 9.6,
		cacheWritesPrice: 2.4,
		cacheReadsPrice: 9.6,
	},
	"qwq-plus-latest": {
		maxTokens: 8_192,
		contextWindow: 131_071,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.0,
		outputPrice: 0.0,
		cacheWritesPrice: 0.0,
		cacheReadsPrice: 0.0,
	},
	"qwq-plus": {
		maxTokens: 8_192,
		contextWindow: 131_071,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.0,
		outputPrice: 0.0,
		cacheWritesPrice: 0.0,
		cacheReadsPrice: 0.0,
	},
	"qwen-coder-plus": {
		maxTokens: 129_024,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 3.5,
		outputPrice: 7,
		cacheWritesPrice: 3.5,
		cacheReadsPrice: 7,
	},
	"qwen-plus": {
		maxTokens: 129_024,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.8,
		outputPrice: 2,
		cacheWritesPrice: 0.8,
		cacheReadsPrice: 0.2,
	},
	"qwen-turbo": {
		maxTokens: 1_000_000,
		contextWindow: 1_000_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.3,
		outputPrice: 0.6,
		cacheWritesPrice: 0.3,
		cacheReadsPrice: 0.6,
	},
	"qwen-max": {
		maxTokens: 30_720,
		contextWindow: 32_768,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 2.4,
		outputPrice: 9.6,
		cacheWritesPrice: 2.4,
		cacheReadsPrice: 9.6,
	},
	"deepseek-v3": {
		maxTokens: 8_000,
		contextWindow: 64_000,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 0,
		outputPrice: 0.28,
		cacheWritesPrice: 0.14,
		cacheReadsPrice: 0.014,
	},
	"deepseek-r1": {
		maxTokens: 8_000,
		contextWindow: 64_000,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 0,
		outputPrice: 2.19,
		cacheWritesPrice: 0.55,
		cacheReadsPrice: 0.14,
	},
	"qwen-vl-max": {
		maxTokens: 30_720,
		contextWindow: 32_768,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 3,
		outputPrice: 9,
		cacheWritesPrice: 3,
		cacheReadsPrice: 9,
	},
	"qwen-vl-max-latest": {
		maxTokens: 129_024,
		contextWindow: 131_072,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 3,
		outputPrice: 9,
		cacheWritesPrice: 3,
		cacheReadsPrice: 9,
	},
	"qwen-vl-plus": {
		maxTokens: 6_000,
		contextWindow: 8_000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 1.5,
		outputPrice: 4.5,
		cacheWritesPrice: 1.5,
		cacheReadsPrice: 4.5,
	},
	"qwen-vl-plus-latest": {
		maxTokens: 129_024,
		contextWindow: 131_072,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 1.5,
		outputPrice: 4.5,
		cacheWritesPrice: 1.5,
		cacheReadsPrice: 4.5,
	},
} as const satisfies Record<string, ModelInfo>
export enum QwenApiRegions {
	CHINA = "china",
	INTERNATIONAL = "international",
}
export type MainlandQwenModelId = keyof typeof mainlandQwenModels
export type InternationalQwenModelId = keyof typeof internationalQwenModels
// Set first model in the list as the default model for each region
export const internationalQwenDefaultModelId: InternationalQwenModelId = Object.keys(
	internationalQwenModels,
)[0] as InternationalQwenModelId
export const mainlandQwenDefaultModelId: MainlandQwenModelId = Object.keys(mainlandQwenModels)[0] as MainlandQwenModelId

// Doubao
// https://www.volcengine.com/docs/82379/1298459
// https://console.volcengine.com/ark/region:ark+cn-beijing/openManagement
export type DoubaoModelId = keyof typeof doubaoModels
export const doubaoDefaultModelId: DoubaoModelId = "doubao-1-5-pro-256k-250115"
export const doubaoModels = {
	"doubao-1-5-pro-256k-250115": {
		maxTokens: 12_288,
		contextWindow: 256_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.7,
		outputPrice: 1.3,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0,
	},
	"doubao-1-5-pro-32k-250115": {
		maxTokens: 12_288,
		contextWindow: 32_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.11,
		outputPrice: 0.3,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0,
	},
	"deepseek-v3-250324": {
		maxTokens: 12_288,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.55,
		outputPrice: 2.19,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0,
	},
	"deepseek-r1-250120": {
		maxTokens: 32_768,
		contextWindow: 64_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.27,
		outputPrice: 1.09,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0,
	},
} as const satisfies Record<string, ModelInfo>

// Mistral
// https://docs.mistral.ai/getting-started/models/models_overview/
export type MistralModelId = keyof typeof mistralModels
export const mistralDefaultModelId: MistralModelId = "devstral-2512"
export const mistralModels = {
	"devstral-2512": {
		maxTokens: 256_000,
		contextWindow: 256_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"labs-devstral-small-2512": {
		maxTokens: 256_000,
		contextWindow: 256_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"mistral-large-2512": {
		maxTokens: 256_000,
		contextWindow: 256_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.5,
		outputPrice: 1.5,
	},
	"ministral-14b-2512": {
		maxTokens: 256_000,
		contextWindow: 256_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.2,
		outputPrice: 0.2,
	},
	"mistral-large-2411": {
		maxTokens: 128_000,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 2.0,
		outputPrice: 6.0,
	},
	"pixtral-large-2411": {
		maxTokens: 131_000,
		contextWindow: 131_000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 2.0,
		outputPrice: 6.0,
	},
	"ministral-3b-2410": {
		maxTokens: 128_000,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.04,
		outputPrice: 0.04,
	},
	"ministral-8b-2410": {
		maxTokens: 128_000,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.1,
		outputPrice: 0.1,
	},
	"mistral-small-latest": {
		maxTokens: 128_000,
		contextWindow: 128_000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0.1,
		outputPrice: 0.3,
	},
	"mistral-medium-latest": {
		maxTokens: 128_000,
		contextWindow: 128_000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0.4,
		outputPrice: 2.0,
	},
	"mistral-small-2501": {
		maxTokens: 32_000,
		contextWindow: 32_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.1,
		outputPrice: 0.3,
	},
	"pixtral-12b-2409": {
		maxTokens: 128_000,
		contextWindow: 128_000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0.15,
		outputPrice: 0.15,
	},
	"open-mistral-nemo-2407": {
		maxTokens: 128_000,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.15,
		outputPrice: 0.15,
	},
	"open-codestral-mamba": {
		maxTokens: 256_000,
		contextWindow: 256_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.15,
		outputPrice: 0.15,
	},
	"codestral-2501": {
		maxTokens: 256_000,
		contextWindow: 256_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.3,
		outputPrice: 0.9,
	},
	"devstral-small-2505": {
		maxTokens: 128_000,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.1,
		outputPrice: 0.3,
	},
	"devstral-medium-latest": {
		maxTokens: 128_000,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.4,
		outputPrice: 2.0,
	},
} as const satisfies Record<string, ModelInfo>

// LiteLLM
// https://docs.litellm.ai/docs/
export type LiteLLMModelId = string
export const liteLlmDefaultModelId = "anthropic/claude-3-7-sonnet-20250219"
export interface LiteLLMModelInfo extends ModelInfo {
	temperature?: number
}

export const liteLlmModelInfoSaneDefaults: LiteLLMModelInfo = {
	maxTokens: -1,
	contextWindow: 128_000,
	supportsImages: true,
	supportsPromptCache: true,
	inputPrice: 0,
	outputPrice: 0,
	cacheWritesPrice: 0,
	cacheReadsPrice: 0,
	temperature: 0,
}

// AskSage Models
// https://docs.asksage.ai/
export type AskSageModelId = keyof typeof askSageModels
export const askSageDefaultModelId: AskSageModelId = "claude-4-sonnet"
export const askSageDefaultURL: string = "https://api.asksage.ai/server"
export const askSageModels = {
	"gpt-4o": {
		maxTokens: 4096,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"gpt-4o-gov": {
		maxTokens: 4096,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"gpt-4.1": {
		maxTokens: 32_768,
		contextWindow: 1_047_576,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"claude-35-sonnet": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"aws-bedrock-claude-35-sonnet-gov": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"claude-37-sonnet": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"claude-4-sonnet": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"claude-4-opus": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"google-gemini-2.5-pro": {
		maxTokens: 65536,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"google-claude-45-sonnet": {
		maxTokens: 64000,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"google-claude-4-opus": {
		maxTokens: 32000,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"gpt-5": {
		maxTokens: 65536,
		contextWindow: 2_097_152,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"gpt-5-mini": {
		maxTokens: 32768,
		contextWindow: 1_048_576,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
	"gpt-5-nano": {
		maxTokens: 16384,
		contextWindow: 262_144,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
	},
}

// Nebius AI Studio
// https://docs.nebius.com/studio/inference/models
export const nebiusModels = {
	"deepseek-ai/DeepSeek-V3": {
		maxTokens: 32_000,
		contextWindow: 96_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.5,
		outputPrice: 1.5,
	},
	"deepseek-ai/DeepSeek-V3-0324-fast": {
		maxTokens: 128_000,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 2,
		outputPrice: 6,
	},
	"deepseek-ai/DeepSeek-R1": {
		maxTokens: 32_000,
		contextWindow: 96_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.8,
		outputPrice: 2.4,
	},
	"deepseek-ai/DeepSeek-R1-fast": {
		maxTokens: 32_000,
		contextWindow: 96_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 2,
		outputPrice: 6,
	},
	"deepseek-ai/DeepSeek-R1-0528": {
		maxTokens: 128_000,
		contextWindow: 163_840,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.8,
		outputPrice: 2.4,
	},
	"meta-llama/Llama-3.3-70B-Instruct-fast": {
		maxTokens: 32_000,
		contextWindow: 96_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.25,
		outputPrice: 0.75,
	},
	"Qwen/Qwen2.5-32B-Instruct-fast": {
		maxTokens: 8_192,
		contextWindow: 32_768,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.13,
		outputPrice: 0.4,
	},
	"Qwen/Qwen2.5-Coder-32B-Instruct-fast": {
		maxTokens: 128_000,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.1,
		outputPrice: 0.3,
	},
	"Qwen/Qwen3-4B-fast": {
		maxTokens: 32_000,
		contextWindow: 41_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.08,
		outputPrice: 0.24,
	},
	"Qwen/Qwen3-30B-A3B-fast": {
		maxTokens: 32_000,
		contextWindow: 41_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.3,
		outputPrice: 0.9,
	},
	"Qwen/Qwen3-235B-A22B": {
		maxTokens: 32_000,
		contextWindow: 41_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.2,
		outputPrice: 0.6,
	},
	"openai/gpt-oss-120b": {
		maxTokens: 32766, // Quantization: fp4
		contextWindow: 131_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.15,
		outputPrice: 0.6,
	},
	"moonshotai/Kimi-K2-Instruct": {
		maxTokens: 16384, // Quantization: fp4
		contextWindow: 131_000,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 0.5,
		outputPrice: 2.4,
	},
	"Qwen/Qwen3-Coder-480B-A35B-Instruct": {
		maxTokens: 163800, // Quantization: fp8
		contextWindow: 262_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.4,
		outputPrice: 1.8,
	},
	"openai/gpt-oss-20b": {
		maxTokens: 32766, // Quantization: fp4
		contextWindow: 131_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.05,
		outputPrice: 0.2,
	},
	"zai-org/GLM-4.5": {
		maxTokens: 98304, // Quantization: fp8
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 0.6,
		outputPrice: 2.2,
	},
	"zai-org/GLM-4.5-Air": {
		maxTokens: 98304, // Quantization: fp8
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 0.2,
		outputPrice: 1.2,
	},
	"deepseek-ai/DeepSeek-R1-0528-fast": {
		maxTokens: 128000, // Quantization: fp4
		contextWindow: 164_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 2.0,
		outputPrice: 6.0,
	},
	"Qwen/Qwen3-235B-A22B-Instruct-2507": {
		maxTokens: 64000, // Quantization: fp8
		contextWindow: 262_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.2,
		outputPrice: 0.6,
	},
	"Qwen/Qwen3-30B-A3B": {
		maxTokens: 32000, // Quantization: fp8
		contextWindow: 41_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.1,
		outputPrice: 0.3,
	},
	"Qwen/Qwen3-32B": {
		maxTokens: 16384, // Quantization: fp8
		contextWindow: 41_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.1,
		outputPrice: 0.3,
	},
	"Qwen/Qwen3-32B-fast": {
		maxTokens: 16384, // Quantization: fp8
		contextWindow: 41_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.2,
		outputPrice: 0.6,
	},
} as const satisfies Record<string, ModelInfo>
export type NebiusModelId = keyof typeof nebiusModels
export const nebiusDefaultModelId = "Qwen/Qwen2.5-32B-Instruct-fast" satisfies NebiusModelId

// X AI
// https://docs.x.ai/docs/api-reference
export type XAIModelId = keyof typeof xaiModels
export const xaiDefaultModelId: XAIModelId = "grok-4"
export const xaiModels = {
	"grok-4-1-fast-reasoning": {
		contextWindow: 2_000_000,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 0.2,
		cacheReadsPrice: 0.05,
		outputPrice: 0.5,
		description: "xAI's Grok 4.1 Reasoning Fast - multimodal model with 2M context.",
	},
	"grok-4-1-fast-non-reasoning": {
		contextWindow: 2_000_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 0.2,
		cacheReadsPrice: 0.05,
		outputPrice: 0.5,
		description: "xAI's Grok 4.1 Non-Reasoning Fast - multimodal model with 2M context.",
	},
	"grok-code-fast-1": {
		contextWindow: 256_000,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 0.2,
		cacheReadsPrice: 0.02,
		outputPrice: 1.5,
		description: "xAI's Grok Coding model.",
	},
	"grok-4-fast-reasoning": {
		maxTokens: 30000,
		contextWindow: 2000000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0.2,
		cacheReadsPrice: 0.05,
		outputPrice: 0.5,
		description: "xAI's Grok 4 Fast (free) multimodal model with 2M context.",
	},
	"grok-4": {
		maxTokens: 8192,
		contextWindow: 262144,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 3.0, // will have different pricing for long context vs short context
		cacheReadsPrice: 0.75,
		outputPrice: 15.0,
	},
	"grok-3-beta": {
		maxTokens: 8192,
		contextWindow: 131072,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 3.0,
		outputPrice: 15.0,
		description: "X AI's Grok-3 beta model with 131K context window",
	},
	"grok-3-fast-beta": {
		maxTokens: 8192,
		contextWindow: 131072,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 5.0,
		outputPrice: 25.0,
		description: "X AI's Grok-3 fast beta model with 131K context window",
	},
	"grok-3-mini-beta": {
		maxTokens: 8192,
		contextWindow: 131072,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 0.3,
		outputPrice: 0.5,
		description: "X AI's Grok-3 mini beta model with 131K context window",
	},
	"grok-3-mini-fast-beta": {
		maxTokens: 8192,
		contextWindow: 131072,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 0.6,
		outputPrice: 4.0,
		description: "X AI's Grok-3 mini fast beta model with 131K context window",
	},
	"grok-3": {
		maxTokens: 8192,
		contextWindow: 131072,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 3.0,
		outputPrice: 15.0,
		description: "X AI's Grok-3 model with 131K context window",
	},
	"grok-3-fast": {
		maxTokens: 8192,
		contextWindow: 131072,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 5.0,
		outputPrice: 25.0,
		description: "X AI's Grok-3 fast model with 131K context window",
	},
	"grok-3-mini": {
		maxTokens: 8192,
		contextWindow: 131072,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 0.3,
		outputPrice: 0.5,
		description: "X AI's Grok-3 mini model with 131K context window",
	},
	"grok-3-mini-fast": {
		maxTokens: 8192,
		contextWindow: 131072,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 0.6,
		outputPrice: 4.0,
		description: "X AI's Grok-3 mini fast model with 131K context window",
	},
	"grok-2-latest": {
		maxTokens: 8192,
		contextWindow: 131072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 2.0,
		outputPrice: 10.0,
		description: "X AI's Grok-2 model - latest version with 131K context window",
	},
	"grok-2": {
		maxTokens: 8192,
		contextWindow: 131072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 2.0,
		outputPrice: 10.0,
		description: "X AI's Grok-2 model with 131K context window",
	},
	"grok-2-1212": {
		maxTokens: 8192,
		contextWindow: 131072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 2.0,
		outputPrice: 10.0,
		description: "X AI's Grok-2 model (version 1212) with 131K context window",
	},
	"grok-2-vision-latest": {
		maxTokens: 8192,
		contextWindow: 32768,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 2.0,
		outputPrice: 10.0,
		description: "X AI's Grok-2 Vision model - latest version with image support and 32K context window",
	},
	"grok-2-vision": {
		maxTokens: 8192,
		contextWindow: 32768,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 2.0,
		outputPrice: 10.0,
		description: "X AI's Grok-2 Vision model with image support and 32K context window",
	},
	"grok-2-vision-1212": {
		maxTokens: 8192,
		contextWindow: 32768,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 2.0,
		outputPrice: 10.0,
		description: "X AI's Grok-2 Vision model (version 1212) with image support and 32K context window",
	},
	"grok-vision-beta": {
		maxTokens: 8192,
		contextWindow: 8192,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 5.0,
		outputPrice: 15.0,
		description: "X AI's Grok Vision Beta model with image support and 8K context window",
	},
	"grok-beta": {
		maxTokens: 8192,
		contextWindow: 131072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 5.0,
		outputPrice: 15.0,
		description: "X AI's Grok Beta model (legacy) with 131K context window",
	},
} as const satisfies Record<string, ModelInfo>

// SambaNova
// https://docs.sambanova.ai/cloud/docs/get-started/supported-models
export type SambanovaModelId = keyof typeof sambanovaModels
export const sambanovaDefaultModelId: SambanovaModelId = "Meta-Llama-3.3-70B-Instruct"
export const sambanovaModels = {
	"Llama-4-Maverick-17B-128E-Instruct": {
		maxTokens: 4096,
		contextWindow: 8_000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0.63,
		outputPrice: 1.8,
	},
	"Llama-4-Scout-17B-16E-Instruct": {
		maxTokens: 4096,
		contextWindow: 8_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.4,
		outputPrice: 0.7,
	},
	"Meta-Llama-3.3-70B-Instruct": {
		maxTokens: 4096,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.6,
		outputPrice: 1.2,
	},
	"DeepSeek-R1-Distill-Llama-70B": {
		maxTokens: 4096,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.7,
		outputPrice: 1.4,
	},
	"DeepSeek-R1": {
		maxTokens: 4096,
		contextWindow: 16_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 5.0,
		outputPrice: 7.0,
	},
	"Meta-Llama-3.1-405B-Instruct": {
		maxTokens: 4096,
		contextWindow: 16_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 5.0,
		outputPrice: 10.0,
	},
	"Meta-Llama-3.1-8B-Instruct": {
		maxTokens: 4096,
		contextWindow: 16_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.1,
		outputPrice: 0.2,
	},
	"Meta-Llama-3.2-1B-Instruct": {
		maxTokens: 4096,
		contextWindow: 16_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.04,
		outputPrice: 0.08,
	},
	"Meta-Llama-3.2-3B-Instruct": {
		maxTokens: 4096,
		contextWindow: 8_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.08,
		outputPrice: 0.16,
	},
	"Qwen3-32B": {
		maxTokens: 4096,
		contextWindow: 16_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.4,
		outputPrice: 0.8,
	},
	"QwQ-32B": {
		maxTokens: 4096,
		contextWindow: 16_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.5,
		outputPrice: 1.0,
	},
	"DeepSeek-V3-0324": {
		maxTokens: 4096,
		contextWindow: 8_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 3.0,
		outputPrice: 4.5,
	},
	"DeepSeek-V3.1": {
		maxTokens: 7168,
		contextWindow: 32_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 3.0,
		outputPrice: 4.5,
	},
} as const satisfies Record<string, ModelInfo>

// Cerebras
// https://inference-docs.cerebras.ai/api-reference/models
export type CerebrasModelId = keyof typeof cerebrasModels
export const cerebrasDefaultModelId: CerebrasModelId = "zai-glm-4.7"
export const cerebrasModels = {
	"zai-glm-4.7": {
		maxTokens: 40000,
		contextWindow: 131072,
		supportsImages: false,
		supportsPromptCache: false,
		temperature: 0.9,
		inputPrice: 0,
		outputPrice: 0,
		description:
			"Highly capable general-purpose model on Cerebras (up to 1,000 tokens/s), competitive with leading proprietary models on coding tasks.",
	},
	"gpt-oss-120b": {
		maxTokens: 65536,
		contextWindow: 128000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
		description: "Intelligent general purpose model with 3,000 tokens/s",
	},
	"qwen-3-235b-a22b-instruct-2507": {
		maxTokens: 64000,
		contextWindow: 64000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
		description: "Intelligent model with ~1400 tokens/s",
	},
	"llama-3.3-70b": {
		maxTokens: 64000,
		contextWindow: 64000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
		description: "Powerful model with ~2600 tokens/s",
	},
	"qwen-3-32b": {
		maxTokens: 64000,
		contextWindow: 64000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
		description: "SOTA coding performance with ~2500 tokens/s",
	},
} as const satisfies Record<string, ModelInfo>

// Groq
// https://console.groq.com/docs/models
// https://groq.com/pricing/
export type GroqModelId = keyof typeof groqModels
export const groqDefaultModelId: GroqModelId = "moonshotai/kimi-k2-instruct-0905"
export const groqModels = {
	"openai/gpt-oss-120b": {
		maxTokens: 32766, // Model fails if you try to use more than 32K tokens
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.15,
		outputPrice: 0.75,
		description:
			"A state-of-the-art 120B open-weight Mixture-of-Experts language model optimized for strong reasoning, tool use, and efficient deployment on large GPUs",
	},
	"openai/gpt-oss-20b": {
		maxTokens: 32766, // Model fails if you try to use more than 32K tokens
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.1,
		outputPrice: 0.5,
		description:
			"A compact 20B open-weight Mixture-of-Experts language model designed for strong reasoning and tool use, ideal for edge devices and local inference.",
	},
	// Compound Beta Models - Hybrid architectures optimized for tool use
	"compound-beta": {
		maxTokens: 8192,
		contextWindow: 128000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.0,
		outputPrice: 0.0,
		description:
			"Compound model using Llama 4 Scout for core reasoning with Llama 3.3 70B for routing and tool use. Excellent for plan/act workflows.",
	},
	"compound-beta-mini": {
		maxTokens: 8192,
		contextWindow: 128000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.0,
		outputPrice: 0.0,
		description: "Lightweight compound model for faster inference while maintaining tool use capabilities.",
	},
	// DeepSeek Models - Reasoning-optimized
	"deepseek-r1-distill-llama-70b": {
		maxTokens: 131072,
		contextWindow: 131072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.75,
		outputPrice: 0.99,
		description:
			"DeepSeek R1 reasoning capabilities distilled into Llama 70B architecture. Excellent for complex problem-solving and planning.",
	},
	// Llama 4 Models
	"meta-llama/llama-4-maverick-17b-128e-instruct": {
		maxTokens: 8192,
		contextWindow: 131072,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0.2,
		outputPrice: 0.6,
		description: "Meta's Llama 4 Maverick 17B model with 128 experts, supports vision and multimodal tasks.",
	},
	"meta-llama/llama-4-scout-17b-16e-instruct": {
		maxTokens: 8192,
		contextWindow: 131072,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0.11,
		outputPrice: 0.34,
		description: "Meta's Llama 4 Scout 17B model with 16 experts, optimized for fast inference and general tasks.",
	},
	// Llama 3.3 Models
	"llama-3.3-70b-versatile": {
		maxTokens: 32768,
		contextWindow: 131072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.59,
		outputPrice: 0.79,
		description: "Meta's latest Llama 3.3 70B model optimized for versatile use cases with excellent performance and speed.",
	},
	// Llama 3.1 Models - Fast inference
	"llama-3.1-8b-instant": {
		maxTokens: 131072,
		contextWindow: 131072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.05,
		outputPrice: 0.08,
		description: "Fast and efficient Llama 3.1 8B model optimized for speed, low latency, and reliable tool execution.",
	},
	// Moonshot Models
	"moonshotai/kimi-k2-instruct": {
		maxTokens: 16384,
		contextWindow: 131072,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 1.0,
		outputPrice: 3.0,
		cacheReadsPrice: 0.5, // 50% discount for cached input tokens
		description:
			"Kimi K2 is Moonshot AI's state-of-the-art Mixture-of-Experts (MoE) language model with 1 trillion total parameters and 32 billion activated parameters.",
	},
	"moonshotai/kimi-k2-instruct-0905": {
		maxTokens: 16384,
		contextWindow: 262144,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 0.6,
		outputPrice: 2.5,
		cacheReadsPrice: 0.15,
		description:
			"Kimi K2 model gets a new version update: Agentic coding: more accurate, better generalization across scaffolds. Frontend coding: improved aesthetics and functionalities on web, 3d, and other tasks. Context length: extended from 128k to 256k, providing better long-horizon support.",
	},
} as const satisfies Record<string, ModelInfo>

// Requesty
// https://requesty.ai/models
export const requestyDefaultModelId = "anthropic/claude-3-7-sonnet-latest"
export const requestyDefaultModelInfo: ModelInfo = {
	maxTokens: 8192,
	contextWindow: 200_000,
	supportsImages: true,

	supportsPromptCache: true,
	inputPrice: 3.0,
	outputPrice: 15.0,
	cacheWritesPrice: 3.75,
	cacheReadsPrice: 0.3,
	description: "Anthropic's most intelligent model. Highest level of intelligence and capability.",
}

// SAP AI Core
export type SapAiCoreModelId = keyof typeof sapAiCoreModels
export const sapAiCoreDefaultModelId: SapAiCoreModelId = "anthropic--claude-3.5-sonnet"
// Pricing is calculated using Capacity Units, not directly in USD
const sapAiCoreModelDescription = "Pricing is calculated using SAP's Capacity Units rather than direct USD pricing."
export const sapAiCoreModels = {
	"anthropic--claude-4.5-haiku": {
		maxTokens: 64000,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		description: sapAiCoreModelDescription,
	},
	"anthropic--claude-4.5-sonnet": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		description: sapAiCoreModelDescription,
	},
	"anthropic--claude-4-sonnet": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		description: sapAiCoreModelDescription,
	},
	"anthropic--claude-4.5-opus": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		description: sapAiCoreModelDescription,
	},
	"anthropic--claude-4-opus": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		description: sapAiCoreModelDescription,
	},
	"anthropic--claude-3.7-sonnet": {
		maxTokens: 64_000,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		description: sapAiCoreModelDescription,
	},
	"anthropic--claude-3.5-sonnet": {
		maxTokens: 8192,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: false,
		description: sapAiCoreModelDescription,
	},
	"anthropic--claude-3-sonnet": {
		maxTokens: 4096,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: false,
		description: sapAiCoreModelDescription,
	},
	"anthropic--claude-3-haiku": {
		maxTokens: 4096,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: false,
		description: sapAiCoreModelDescription,
	},
	"anthropic--claude-3-opus": {
		maxTokens: 4096,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: false,
		description: sapAiCoreModelDescription,
	},
	"gemini-2.5-pro": {
		maxTokens: 65536,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: true,
		thinkingConfig: {
			maxBudget: 32767,
		},
		description: sapAiCoreModelDescription,
	},
	"gemini-2.5-flash": {
		maxTokens: 65536,
		contextWindow: 1_048_576,
		supportsImages: true,
		supportsPromptCache: true,
		thinkingConfig: {
			maxBudget: 24576,
		},
		description: sapAiCoreModelDescription,
	},
	"gpt-4": {
		maxTokens: 4096,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: false,
		description: sapAiCoreModelDescription,
	},
	"gpt-4o": {
		maxTokens: 4096,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: false,
		description: sapAiCoreModelDescription,
	},
	"gpt-4o-mini": {
		maxTokens: 4096,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: false,
		description: sapAiCoreModelDescription,
	},
	"gpt-4.1": {
		maxTokens: 32_768,
		contextWindow: 1_047_576,
		supportsImages: true,
		supportsPromptCache: true,
		description: sapAiCoreModelDescription,
	},
	"gpt-4.1-nano": {
		maxTokens: 32_768,
		contextWindow: 1_047_576,
		supportsImages: true,
		supportsPromptCache: true,
		description: sapAiCoreModelDescription,
	},
	"gpt-5": {
		maxTokens: 128_000,
		contextWindow: 272_000,
		supportsImages: true,
		supportsPromptCache: true,
		description: sapAiCoreModelDescription,
	},
	"gpt-5-nano": {
		maxTokens: 128_000,
		contextWindow: 272_000,
		supportsImages: true,
		supportsPromptCache: true,
		description: sapAiCoreModelDescription,
	},
	"gpt-5-mini": {
		maxTokens: 128_000,
		contextWindow: 272_000,
		supportsImages: true,
		supportsPromptCache: true,
		description: sapAiCoreModelDescription,
	},
	o1: {
		maxTokens: 4096,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: false,
		description: sapAiCoreModelDescription,
	},
	o3: {
		maxTokens: 100_000,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		description: sapAiCoreModelDescription,
	},
	"o3-mini": {
		maxTokens: 4096,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: false,
		description: sapAiCoreModelDescription,
	},
	"o4-mini": {
		maxTokens: 100_000,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		description: sapAiCoreModelDescription,
	},
	sonar: {
		maxTokens: 128_000,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		description: sapAiCoreModelDescription,
	},
	"sonar-pro": {
		maxTokens: 128_000,
		contextWindow: 200_000,
		supportsImages: false,
		supportsPromptCache: false,
		description: sapAiCoreModelDescription,
	},
} as const satisfies Record<string, ModelInfo>

// Moonshot AI Studio
// https://platform.moonshot.ai/docs/pricing/chat
export const moonshotModels = {
	"kimi-k2.5": {
		maxTokens: 32_000,
		contextWindow: 262_144,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 0.6,
		outputPrice: 3.0,
		cacheReadsPrice: 0.1,
		temperature: 1.0,
	},
	"kimi-k2-0905-preview": {
		maxTokens: 16384,
		contextWindow: 262144,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.6,
		outputPrice: 2.5,
		temperature: 0.6,
	},
	"kimi-k2-0711-preview": {
		maxTokens: 32_000,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.6,
		outputPrice: 2.5,
		temperature: 0.6,
	},
	"kimi-k2-turbo-preview": {
		maxTokens: 32_000,
		contextWindow: 262_144,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 2.4,
		outputPrice: 10,
		temperature: 0.6,
	},
	"kimi-k2-thinking": {
		maxTokens: 32_000,
		contextWindow: 262_144,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.6,
		outputPrice: 2.5,
		temperature: 1.0,
	},
	"kimi-k2-thinking-turbo": {
		maxTokens: 32_000,
		contextWindow: 262_144,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 2.4,
		outputPrice: 10,
		temperature: 1.0,
	},
} as const satisfies Record<string, OpenAiCompatibleModelInfo>
export type MoonshotModelId = keyof typeof moonshotModels
export const moonshotDefaultModelId = "kimi-k2-0905-preview" satisfies MoonshotModelId

// Huawei Cloud MaaS
// Dify.ai - No model selection needed, models are configured in Dify workflows

export type HuaweiCloudMaasModelId = keyof typeof huaweiCloudMaasModels
export const huaweiCloudMaasDefaultModelId: HuaweiCloudMaasModelId = "DeepSeek-V3"
export const huaweiCloudMaasModels = {
	"DeepSeek-V3": {
		maxTokens: 16_384,
		contextWindow: 64_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.27,
		outputPrice: 1.1,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0,
	},
	"DeepSeek-R1": {
		maxTokens: 16_384,
		contextWindow: 64_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.55,
		outputPrice: 2.2,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0,
		thinkingConfig: {
			maxBudget: 8192,
			outputPrice: 2.2,
		},
	},
	"deepseek-r1-250528": {
		maxTokens: 16_384,
		contextWindow: 64_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.55,
		outputPrice: 2.2,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0,
		thinkingConfig: {
			maxBudget: 8192,
			outputPrice: 2.2,
		},
	},
	"qwen3-235b-a22b": {
		maxTokens: 8_192,
		contextWindow: 32_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.27,
		outputPrice: 1.1,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0,
		thinkingConfig: {
			maxBudget: 4096,
			outputPrice: 1.1,
		},
	},
	"qwen3-32b": {
		maxTokens: 8_192,
		contextWindow: 32_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.27,
		outputPrice: 1.1,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0,
		thinkingConfig: {
			maxBudget: 4096,
			outputPrice: 1.1,
		},
	},
} as const satisfies Record<string, ModelInfo>

// Baseten
// https://baseten.co/products/model-apis/
// Extended ModelInfo to include supportedFeatures, like tools
export interface BasetenModelInfo extends ModelInfo {
	supportedFeatures?: string[]
}

export const basetenModels = {
	"moonshotai/Kimi-K2-Thinking": {
		maxTokens: 163_800,
		contextWindow: 262_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.6,
		outputPrice: 2.5,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0,
		description: "Kimi K2 Thinking - A model with enhanced reasoning capabilities from Kimi K2",
		supportsReasoning: true,
	},
	"zai-org/GLM-4.6": {
		maxTokens: 200_000,
		contextWindow: 200_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.6,
		outputPrice: 2.2,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0,
		description: "Frontier open model with advanced agentic, reasoning and coding capabilities",
		supportsReasoning: true,
	},
	"deepseek-ai/DeepSeek-R1": {
		maxTokens: 131_072,
		contextWindow: 163_840,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 2.55,
		outputPrice: 5.95,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0,
		description: "DeepSeek's first-generation reasoning model",
		supportsReasoning: true,
	},
	"deepseek-ai/DeepSeek-R1-0528": {
		maxTokens: 131_072,
		contextWindow: 163_840,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 2.55,
		outputPrice: 5.95,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0,
		description: "The latest revision of DeepSeek's first-generation reasoning model",
		supportsReasoning: true,
	},
	"deepseek-ai/DeepSeek-V3-0324": {
		maxTokens: 131_072,
		contextWindow: 163_840,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.77,
		outputPrice: 0.77,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0,
		description: "Fast general-purpose LLM with enhanced reasoning capabilities",
		supportsReasoning: true,
	},
	"deepseek-ai/DeepSeek-V3.1": {
		maxTokens: 131_072,
		contextWindow: 163_840,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.5,
		outputPrice: 1.5,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0,
		description: "Extremely capable general-purpose LLM with hybrid reasoning capabilities and advanced tool calling",
		supportsReasoning: true,
	},
	"deepseek-ai/DeepSeek-V3.2": {
		maxTokens: 131_072,
		contextWindow: 163_840,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.3,
		outputPrice: 0.45,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0,
		description: "DeepSeek's hybrid reasoning model with efficient long context scaling with GPT-5 level performance",
		supportsReasoning: true,
	},
	"Qwen/Qwen3-235B-A22B-Instruct-2507": {
		maxTokens: 262_144,
		contextWindow: 262_144,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.22,
		outputPrice: 0.8,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0,
		description: "Mixture-of-experts LLM with math and reasoning capabilities",
		supportsReasoning: false,
	},
	"Qwen/Qwen3-Coder-480B-A35B-Instruct": {
		maxTokens: 262_144,
		contextWindow: 262_144,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.38,
		outputPrice: 1.53,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0,
		description: "Mixture-of-experts LLM with advanced coding and reasoning capabilities",
		supportsReasoning: false,
	},
	"openai/gpt-oss-120b": {
		maxTokens: 128_072,
		contextWindow: 128_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.1,
		outputPrice: 0.5,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0,
		description: "Extremely capable general-purpose LLM with strong, controllable reasoning capabilities",
		supportsReasoning: true,
	},
	"moonshotai/Kimi-K2-Instruct-0905": {
		maxTokens: 168_000,
		contextWindow: 262_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.6,
		outputPrice: 2.5,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0,
		description: "State of the art language model for agentic and coding tasks. September Update.",
		supportsReasoning: false,
	},
} as const satisfies Record<string, ModelInfo>
export type BasetenModelId = keyof typeof basetenModels
export const basetenDefaultModelId = "zai-org/GLM-4.6" satisfies BasetenModelId

// Z AI
// https://docs.z.ai/guides/llm/glm-5
// https://docs.z.ai/guides/overview/pricing
export type internationalZAiModelId = keyof typeof internationalZAiModels
export const internationalZAiDefaultModelId: internationalZAiModelId = "glm-5"
export const internationalZAiModels = {
	"glm-5": {
		maxTokens: 128_000,
		contextWindow: 200_000,
		supportsImages: false,
		supportsPromptCache: true,
		cacheReadsPrice: 0.2,
		inputPrice: 1.0,
		outputPrice: 3.2,
	},
	"glm-4.7": {
		maxTokens: 131_000,
		contextWindow: 200_000,
		supportsImages: false,
		supportsPromptCache: true,
		cacheReadsPrice: 0.11,
		inputPrice: 0.6,
		outputPrice: 2.2,
	},
	"glm-4.6": {
		maxTokens: 128_000,
		contextWindow: 200_000,
		supportsImages: false,
		supportsPromptCache: true,
		cacheReadsPrice: 0.11,
		inputPrice: 0.6,
		outputPrice: 2.2,
	},
	"glm-4.5": {
		maxTokens: 98_304,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 0.6,
		outputPrice: 2.2,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0.11,
		description:
			"GLM-4.5 is Zhipu's latest featured model. Its comprehensive capabilities in reasoning, coding, and agent reach the state-of-the-art (SOTA) level among open-source models, with a context length of up to 128k.",
	},
	"glm-4.5-air": {
		maxTokens: 98304, // Quantization: fp8
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 0.2,
		outputPrice: 1.2,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0.03,
		description:
			"GLM-4.5-Air is the lightweight version of GLM-4.5. It balances performance and cost-effectiveness, and can flexibly switch to hybrid thinking models.",
	},
} as const satisfies Record<string, ModelInfo>

export type mainlandZAiModelId = keyof typeof mainlandZAiModels
export const mainlandZAiDefaultModelId: mainlandZAiModelId = "glm-5"
export const mainlandZAiModels = {
	"glm-5": {
		maxTokens: 128_000,
		contextWindow: 200_000,
		supportsImages: false,
		supportsPromptCache: true,
		cacheReadsPrice: 0.2,
		inputPrice: 1.0,
		outputPrice: 3.2,
	},
	"glm-4.7": {
		maxTokens: 131_000,
		contextWindow: 200_000,
		supportsImages: false,
		supportsPromptCache: true,
		cacheReadsPrice: 0.11,
		inputPrice: 0.6,
		outputPrice: 2.2,
	},
	"glm-4.6": {
		maxTokens: 128_000,
		contextWindow: 200_000,
		supportsImages: false,
		supportsPromptCache: true,
		cacheReadsPrice: 0.11,
		inputPrice: 0.6,
		outputPrice: 2.2,
	},
	"glm-4.5": {
		maxTokens: 98_304,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 0.29,
		outputPrice: 1.14,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0.057,
		description:
			"GLM-4.5 is Zhipu's latest featured model. Its comprehensive capabilities in reasoning, coding, and agent reach the state-of-the-art (SOTA) level among open-source models, with a context length of up to 128k.",
		tiers: [
			{
				contextWindow: 32_000,
				inputPrice: 0.21,
				outputPrice: 1.0,
				cacheReadsPrice: 0.043,
			},
			{
				contextWindow: 128_000,
				inputPrice: 0.29,
				outputPrice: 1.14,
				cacheReadsPrice: 0.057,
			},
			{
				contextWindow: Number.POSITIVE_INFINITY,
				inputPrice: 0.29,
				outputPrice: 1.14,
				cacheReadsPrice: 0.057,
			},
		],
	},
	"glm-4.5-air": {
		maxTokens: 98304, // Quantization: fp8
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 0.086,
		outputPrice: 0.57,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0.017,
		description:
			"GLM-4.5-Air is the lightweight version of GLM-4.5. It balances performance and cost-effectiveness, and can flexibly switch to hybrid thinking models.",
		tiers: [
			{
				contextWindow: 32_000,
				inputPrice: 0.057,
				outputPrice: 0.43,
				cacheReadsPrice: 0.011,
			},
			{
				contextWindow: 128_000,
				inputPrice: 0.086,
				outputPrice: 0.57,
				cacheReadsPrice: 0.017,
			},
			{
				contextWindow: Number.POSITIVE_INFINITY,
				inputPrice: 0.086,
				outputPrice: 0.57,
				cacheReadsPrice: 0.017,
			},
		],
	},
} as const satisfies Record<string, ModelInfo>

// Fireworks AI
export type FireworksModelId = keyof typeof fireworksModels
export const fireworksDefaultModelId: FireworksModelId = "accounts/fireworks/models/kimi-k2-instruct-0905"
export const fireworksModels = {
	"accounts/fireworks/models/kimi-k2-instruct-0905": {
		maxTokens: 16384,
		contextWindow: 262144,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 0.6,
		outputPrice: 2.5,
		cacheReadsPrice: 0.15,
		description:
			"Kimi K2 model gets a new version update: Agentic coding: more accurate, better generalization across scaffolds. Frontend coding: improved aesthetics and functionalities on web, 3d, and other tasks. Context length: extended from 128k to 256k, providing better long-horizon support.",
	},
	"accounts/fireworks/models/qwen3-235b-a22b-instruct-2507": {
		maxTokens: 32768,
		contextWindow: 256000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.22,
		outputPrice: 0.88,
		description: "Latest Qwen3 thinking model, competitive against the best closed source models in Jul 2025.",
	},
	"accounts/fireworks/models/qwen3-coder-480b-a35b-instruct": {
		maxTokens: 32768,
		contextWindow: 256000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.45,
		outputPrice: 1.8,
		description: "Qwen3's most agentic code model to date.",
	},
	"accounts/fireworks/models/deepseek-r1-0528": {
		maxTokens: 20480,
		contextWindow: 160000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 3,
		outputPrice: 8,
		description:
			"05/28 updated checkpoint of Deepseek R1. Its overall performance is now approaching that of leading models, such as O3 and Gemini 2.5 Pro. Compared to the previous version, the upgraded model shows significant improvements in handling complex reasoning tasks, and this version also offers a reduced hallucination rate, enhanced support for function calling, and better experience for vibe coding. Note that fine-tuning for this model is only available through contacting fireworks at https://fireworks.ai/company/contact-us.",
	},
	"accounts/fireworks/models/deepseek-v3": {
		maxTokens: 16384,
		contextWindow: 128000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.9,
		outputPrice: 0.9,
		description:
			"A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token from Deepseek. Note that fine-tuning for this model is only available through contacting fireworks at https://fireworks.ai/company/contact-us.",
	},
} as const satisfies Record<string, ModelInfo>

// Qwen Code
// https://chat.qwen.ai/
export const qwenCodeModels = {
	"qwen3-coder-plus": {
		maxTokens: 65_536,
		contextWindow: 1_000_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0,
		description: "Qwen3 Coder Plus - High-performance coding model with 1M context window for large codebases",
	},
	"qwen3-coder-flash": {
		maxTokens: 65_536,
		contextWindow: 1_000_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0,
		outputPrice: 0,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0,
		description: "Qwen3 Coder Flash - Fast coding model with 1M context window optimized for speed",
	},
} as const satisfies Record<string, ModelInfo>
export type QwenCodeModelId = keyof typeof qwenCodeModels
export const qwenCodeDefaultModelId: QwenCodeModelId = "qwen3-coder-plus"

// Minimax
// https://www.minimax.io/platform/document/text_api_intro
// https://www.minimax.io/platform/document/pricing
export type MinimaxModelId = keyof typeof minimaxModels
export const minimaxDefaultModelId: MinimaxModelId = "MiniMax-M2.1"
export const minimaxModels = {
	"MiniMax-M2.1": {
		maxTokens: 128_000,
		contextWindow: 192_000,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 0.3,
		outputPrice: 1.2,
		cacheWritesPrice: 0.0375,
		cacheReadsPrice: 0.03,
	},
	"MiniMax-M2.1-lightning": {
		maxTokens: 128_000,
		contextWindow: 192_000,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 0.3,
		outputPrice: 2.4,
		cacheWritesPrice: 0.0375,
		cacheReadsPrice: 0.03,
	},
	"MiniMax-M2": {
		maxTokens: 128_000,
		contextWindow: 192_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.3,
		outputPrice: 1.2,
		cacheWritesPrice: 0,
		cacheReadsPrice: 0,
	},
} as const satisfies Record<string, ModelInfo>

// NousResearch
// https://inference-api.nousResearch.com
export type NousResearchModelId = keyof typeof nousResearchModels
export const nousResearchDefaultModelId: NousResearchModelId = "Hermes-4-405B"
export const nousResearchModels = {
	"Hermes-4-405B": {
		maxTokens: 8192,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.09,
		outputPrice: 0.37,
		description:
			"This is the largest model in the Hermes 4 family, and it is the fullest expression of our design, focused on advanced reasoning and creative depth rather than optimizing inference speed or cost.",
	},
	"Hermes-4-70B": {
		maxTokens: 8192,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.05,
		outputPrice: 0.2,
		description:
			"This incarnation of Hermes 4 balances scale and size. It handles complex reasoning tasks, while staying fast and cost effective. A versatile choice for many use cases.",
	},
} as const satisfies Record<string, ModelInfo>


## Links discovered
- [blog post here](https://www.anthropic.com/claude/sonnet)

--- src/test/cline-api.test.ts ---
import { afterEach, beforeEach, describe, it } from "mocha"
import * as should from "should"
import * as sinon from "sinon"
import { createClineAPI } from "@/exports"
import { Logger } from "@/shared/services/Logger"
import type { ClineAPI } from "../exports/cline"
import { setVscodeHostProviderMock } from "./host-provider-test-utils"

describe("ClineAPI Core Functionality", () => {
	let api: ClineAPI
	let mockController: any
	let mockLoggerError: sinon.SinonStub
	let sandbox: sinon.SinonSandbox
	let _getGlobalStateStub: sinon.SinonStub

	beforeEach(async () => {
		sandbox = sinon.createSandbox()

		// Stub Logger.error
		mockLoggerError = sandbox.stub(Logger, "error")
		setVscodeHostProviderMock({})

		// Create a mock controller that matches what the real createClineAPI expects
		// We don't import the real Controller to avoid the webview dependencies
		mockController = {
			id: "test-controller-id",
			context: {
				globalState: {
					get: sandbox.stub(),
					update: sandbox.stub(),
					keys: sandbox.stub().returns([]),
					setKeysForSync: sandbox.stub(),
				},
				secrets: {
					get: sandbox.stub(),
					store: sandbox.stub(),
					delete: sandbox.stub(),
					onDidChange: sandbox.stub(),
				},
			},
			updateCustomInstructions: sandbox.stub().resolves(),
			clearTask: sandbox.stub().resolves(),
			postStateToWebview: sandbox.stub().resolves(),
			postMessageToWebview: sandbox.stub().resolves(),
			initTask: sandbox.stub().resolves(),
			task: undefined,
		}

		// Create API instance
		api = createClineAPI(mockController)
	})

	afterEach(() => {
		sandbox.restore()
	})

	describe("startNewTask", () => {
		it("should clear existing task and start new one with description", async () => {
			const taskDescription = "Create a test function"
			const images = ["image1.png", "image2.png"]

			await api.startNewTask(taskDescription, images)

			// Verify task clearing sequence
			sinon.assert.called(mockController.clearTask)
			sinon.assert.called(mockController.postStateToWebview)
			sinon.assert.calledWith(mockController.initTask, taskDescription, images)
		})

		it("should handle undefined task description", async () => {
			await api.startNewTask(undefined, [])

			sinon.assert.called(mockController.clearTask)
			sinon.assert.calledWith(mockController.initTask, undefined, [])
		})

		it("should handle task with no images", async () => {
			await api.startNewTask("Task without images")

			sinon.assert.calledWith(mockController.initTask, "Task without images", undefined)
		})
	})

	describe("sendMessage", () => {
		it("should send message to active task", async () => {
			const mockTask = {
				handleWebviewAskResponse: sandbox.stub().resolves(),
			}
			mockController.task = mockTask

			await api.sendMessage("Test message", ["image.png"])

			sinon.assert.calledWith(mockTask.handleWebviewAskResponse, "messageResponse", "Test message", ["image.png"])
		})

		it("should handle no active task gracefully", async () => {
			mockController.task = undefined

			await api.sendMessage("Message to nowhere", [])
		})

		it("should handle empty message", async () => {
			const mockTask = {
				handleWebviewAskResponse: sandbox.stub().resolves(),
			}
			mockController.task = mockTask

			await api.sendMessage("", [])

			sinon.assert.calledWith(mockTask.handleWebviewAskResponse, "messageResponse", "", [])
		})

		it("should handle undefined message", async () => {
			const mockTask = {
				handleWebviewAskResponse: sandbox.stub().resolves(),
			}
			mockController.task = mockTask

			await api.sendMessage(undefined, [])

			sinon.assert.calledWith(mockTask.handleWebviewAskResponse, "messageResponse", "", [])
		})
	})

	describe("Button Press Methods", () => {
		describe("pressPrimaryButton", () => {
			it("should handle primary button press with active task", async () => {
				const mockTask = {
					handleWebviewAskResponse: sandbox.stub().resolves(),
				}
				mockController.task = mockTask

				await api.pressPrimaryButton()

				sinon.assert.calledWith(mockTask.handleWebviewAskResponse, "yesButtonClicked", "", [])
			})

			it("should handle primary button press with no active task", async () => {
				mockController.task = undefined

				await api.pressPrimaryButton()

				sinon.assert.calledWith(mockLoggerError, "No active task to press button for")
			})
		})

		describe("pressSecondaryButton", () => {
			it("should handle secondary button press with active task", async () => {
				const mockTask = {
					handleWebviewAskResponse: sandbox.stub().resolves(),
				}
				mockController.task = mockTask

				await api.pressSecondaryButton()

				sinon.assert.calledWith(mockTask.handleWebviewAskResponse, "noButtonClicked", "", [])
			})

			it("should handle secondary button press with no active task", async () => {
				mockController.task = undefined

				await api.pressSecondaryButton()

				sinon.assert.calledWith(mockLoggerError, "No active task to press button for")
			})
		})
	})

	describe("Error Handling", () => {
		it("should handle errors in task initialization", async () => {
			mockController.initTask.rejects(new Error("Init failed"))

			try {
				await api.startNewTask("test task")
				should.fail("", "", "Should have thrown an error", "")
			} catch (error: any) {
				error.message.should.equal("Init failed")
			}
		})
	})
})


--- src/shared/combineApiRequests.ts ---
import { ClineMessage } from "./ExtensionMessage"

/**
 * Combines API request start and finish messages in an array of ClineMessages.
 *
 * This function looks for pairs of 'api_req_started' and 'api_req_finished' messages.
 * When it finds a pair, it combines them into a single 'api_req_combined' message.
 * The JSON data in the text fields of both messages are merged.
 *
 * @param messages - An array of ClineMessage objects to process.
 * @returns A new array of ClineMessage objects with API requests combined.
 *
 * @example
 * const messages = [
 *   { type: "say", say: "api_req_started", text: '{"request":"GET /api/data"}', ts: 1000 },
 *   { type: "say", say: "api_req_finished", text: '{"cost":0.005}', ts: 1001 }
 * ];
 * const result = combineApiRequests(messages);
 * // Result: [{ type: "say", say: "api_req_started", text: '{"request":"GET /api/data","cost":0.005}', ts: 1000 }]
 */
export function combineApiRequests(messages: ClineMessage[]): ClineMessage[] {
	const combinedApiRequests: ClineMessage[] = []

	for (let i = 0; i < messages.length; i++) {
		if (messages[i].type === "say" && messages[i].say === "api_req_started") {
			const startedRequest = JSON.parse(messages[i].text || "{}")
			let j = i + 1

			while (j < messages.length) {
				if (messages[j].type === "say" && messages[j].say === "api_req_finished") {
					const finishedRequest = JSON.parse(messages[j].text || "{}")
					const combinedRequest = {
						...startedRequest,
						...finishedRequest,
					}

					combinedApiRequests.push({
						...messages[i],
						text: JSON.stringify(combinedRequest),
					})

					i = j // Skip to the api_req_finished message
					break
				}
				j++
			}

			if (j === messages.length) {
				// If no matching api_req_finished found, keep the original api_req_started
				combinedApiRequests.push(messages[i])
			}
		}
	}

	// Replace original api_req_started and remove api_req_finished
	return messages
		.filter((msg) => !(msg.type === "say" && msg.say === "api_req_finished"))
		.map((msg) => {
			if (msg.type === "say" && msg.say === "api_req_started") {
				const combinedRequest = combinedApiRequests.find((req) => req.ts === msg.ts)
				return combinedRequest || msg
			}
			return msg
		})
}


--- src/shared/getApiMetrics.ts ---
import { ClineMessage } from "./ExtensionMessage"

interface ApiMetrics {
	totalTokensIn: number
	totalTokensOut: number
	totalCacheWrites?: number
	totalCacheReads?: number
	totalCost: number
}

/**
 * Calculates API metrics from an array of ClineMessages.
 *
 * This function processes usage-carrying say messages.
 * It includes:
 * - 'api_req_started' messages that have been combined with their corresponding 'api_req_finished' messages
 * - 'deleted_api_reqs' messages, which are aggregated from deleted messages
 * - 'subagent_usage' messages, which are aggregated usage snapshots emitted by subagent batches
 * It extracts and sums up the tokensIn, tokensOut, cacheWrites, cacheReads, and cost from these messages.
 *
 * @param messages - An array of ClineMessage objects to process.
 * @returns An ApiMetrics object containing totalTokensIn, totalTokensOut, totalCacheWrites, totalCacheReads, and totalCost.
 *
 * @example
 * const messages = [
 *   { type: "say", say: "api_req_started", text: '{"request":"GET /api/data","tokensIn":10,"tokensOut":20,"cost":0.005}', ts: 1000 }
 * ];
 * const { totalTokensIn, totalTokensOut, totalCost } = getApiMetrics(messages);
 * // Result: { totalTokensIn: 10, totalTokensOut: 20, totalCost: 0.005 }
 */
export function getApiMetrics(messages: ClineMessage[]): ApiMetrics {
	const result: ApiMetrics = {
		totalTokensIn: 0,
		totalTokensOut: 0,
		totalCacheWrites: undefined,
		totalCacheReads: undefined,
		totalCost: 0,
	}

	messages.forEach((message) => {
		if (
			message.type === "say" &&
			(message.say === "api_req_started" || message.say === "deleted_api_reqs" || message.say === "subagent_usage") &&
			message.text
		) {
			try {
				const parsedData = JSON.parse(message.text)
				const { tokensIn, tokensOut, cacheWrites, cacheReads, cost } = parsedData

				if (typeof tokensIn === "number") {
					result.totalTokensIn += tokensIn
				}
				if (typeof tokensOut === "number") {
					result.totalTokensOut += tokensOut
				}
				if (typeof cacheWrites === "number") {
					result.totalCacheWrites = (result.totalCacheWrites ?? 0) + cacheWrites
				}
				if (typeof cacheReads === "number") {
					result.totalCacheReads = (result.totalCacheReads ?? 0) + cacheReads
				}
				if (typeof cost === "number") {
					result.totalCost += cost
				}
			} catch {
				// Ignore JSON parse errors
			}
		}
	})

	return result
}

/**
 * Gets the total token count from the last API request.
 *
 * This is used for context window progress display - it shows how much of the
 * context window is used in the current/most recent request, not cumulative totals.
 *
 * @param messages - An array of ClineMessage objects to process.
 * @returns The total tokens (tokensIn + tokensOut + cacheWrites + cacheReads) from the last api_req_started message, or 0 if none found.
 */
export function getLastApiReqTotalTokens(messages: ClineMessage[]): number {
	for (let i = messages.length - 1; i >= 0; i--) {
		const msg = messages[i]
		if (msg.type === "say" && msg.say === "api_req_started" && msg.text) {
			try {
				const { tokensIn, tokensOut, cacheWrites, cacheReads } = JSON.parse(msg.text)
				const total = (tokensIn || 0) + (tokensOut || 0) + (cacheWrites || 0) + (cacheReads || 0)
				if (total > 0) {
					return total
				}
			} catch {
				// Ignore JSON parse errors, continue searching
			}
		}
	}
	return 0
}


--- src/shared/cline/api.ts ---
enum CLINE_API_AUTH_ENDPOINTS {
	AUTH = "/api/v1/auth/authorize",
	REFRESH_TOKEN = "/api/v1/auth/refresh",
}

enum CLINE_API_ENDPOINT_V1 {
	TOKEN_EXCHANGE = "/api/v1/auth/token",
	USER_INFO = "/api/v1/users/me",
	ACTIVE_ACCOUNT = "/api/v1/users/active-account",
	REMOTE_CONFIG = "/api/v1/organizations/{id}/remote-config",
	API_KEYS = "/api/v1/organizations/{id}/api-keys",
}

export const CLINE_API_ENDPOINT = {
	...CLINE_API_AUTH_ENDPOINTS,
	...CLINE_API_ENDPOINT_V1,
}


--- evals/analysis/src/classifier.ts ---
/**
 * Failure classification system for Cline evaluations
 *
 * Classifies failures by matching log patterns against known issues:
 * - Provider bugs (Gemini #7974, Claude #7998)
 * - Transient failures (rate limits, timeouts)
 * - Infrastructure issues (harness, environment)
 * - Policy/safety refusals
 * - Auth errors
 */

import * as fs from "fs"
import * as yaml from "js-yaml"
import * as path from "path"
import type { FailureCategory, FailureInfo } from "./schemas"

export interface FailurePattern {
	name: string
	pattern: string // Regex pattern as string
	category: FailureCategory
	issue?: string // GitHub issue URL
	description: string
}

export interface FailurePatternsConfig {
	version: string
	patterns: FailurePattern[]
}

export class FailureClassifier {
	private patterns: Array<FailurePattern & { regex: RegExp }>

	constructor(patternsPath?: string) {
		const defaultPath = path.join(__dirname, "../patterns/cline-failures.yaml")
		const configPath = patternsPath || defaultPath

		const config = this.loadPatternsFromYaml(configPath)
		this.patterns = config.patterns.map((p) => ({
			...p,
			regex: new RegExp(p.pattern, "i"), // Case-insensitive matching
		}))
	}

	private loadPatternsFromYaml(filePath: string): FailurePatternsConfig {
		const content = fs.readFileSync(filePath, "utf-8")
		const config = yaml.load(content) as FailurePatternsConfig

		if (!config.version || !config.patterns) {
			throw new Error("Invalid patterns YAML: missing version or patterns")
		}

		return config
	}

	/**
	 * Classify failures in log text
	 * @param logs Full log text (e.g., cline.txt content)
	 * @returns Array of matched failure categories with excerpts
	 */
	classify(logs: string): FailureInfo[] {
		const failures: FailureInfo[] = []

		for (const pattern of this.patterns) {
			const match = pattern.regex.exec(logs)
			if (match) {
				failures.push({
					name: pattern.name,
					category: pattern.category,
					excerpt: this.extractExcerpt(logs, match.index, match[0].length),
					issue_url: pattern.issue,
				})
			}
		}

		return failures
	}

	/**
	 * Extract a context snippet around the matched pattern
	 * @param logs Full log text
	 * @param matchIndex Index where pattern matched
	 * @param matchLength Length of the matched text
	 * @returns Context snippet (up to 200 chars before/after match)
	 */
	private extractExcerpt(logs: string, matchIndex: number, matchLength: number): string {
		const contextSize = 200
		const start = Math.max(0, matchIndex - contextSize)
		const end = Math.min(logs.length, matchIndex + matchLength + contextSize)

		let excerpt = logs.slice(start, end)

		// Trim to complete lines for readability
		excerpt = excerpt.replace(/^\s*\S*\s*/, "") // Remove partial first line
		excerpt = excerpt.replace(/\s*\S*\s*$/, "") // Remove partial last line

		// Truncate if still too long
		if (excerpt.length > 400) {
			excerpt = excerpt.slice(0, 400) + "..."
		}

		return excerpt.trim()
	}

	/**
	 * Check if logs contain any known provider bug patterns
	 */
	hasProviderBug(logs: string): boolean {
		return this.classify(logs).some((f) => f.category === "provider_bug")
	}

	/**
	 * Check if logs contain transient failure patterns (retriable)
	 */
	hasTransientFailure(logs: string): boolean {
		return this.classify(logs).some((f) => f.category === "transient")
	}

	/**
	 * Get all pattern names for a specific category
	 */
	getPatternsByCategory(category: FailureCategory): string[] {
		return this.patterns.filter((p) => p.category === category).map((p) => p.name)
	}
}


--- src/shared/__tests__/getApiMetrics.test.ts ---
import { strict as assert } from "node:assert"
import { describe, it } from "mocha"
import type { ClineMessage } from "../ExtensionMessage"
import { getApiMetrics, getLastApiReqTotalTokens } from "../getApiMetrics"

describe("getApiMetrics", () => {
	it("includes subagent_usage in aggregate totals", () => {
		const messages: ClineMessage[] = [
			{
				ts: 1,
				type: "say",
				say: "api_req_started",
				text: JSON.stringify({
					tokensIn: 10,
					tokensOut: 20,
					cacheWrites: 3,
					cacheReads: 1,
					cost: 0.12,
				}),
			},
			{
				ts: 2,
				type: "say",
				say: "subagent_usage",
				text: JSON.stringify({
					source: "subagents",
					tokensIn: 4,
					tokensOut: 8,
					cacheWrites: 2,
					cacheReads: 1,
					cost: 0.05,
				}),
			},
			{
				ts: 3,
				type: "say",
				say: "deleted_api_reqs",
				text: JSON.stringify({
					tokensIn: 6,
					tokensOut: 9,
					cacheWrites: 1,
					cacheReads: 0,
					cost: 0.03,
				}),
			},
		]

		const metrics = getApiMetrics(messages)

		assert.equal(metrics.totalTokensIn, 20)
		assert.equal(metrics.totalTokensOut, 37)
		assert.equal(metrics.totalCacheWrites, 6)
		assert.equal(metrics.totalCacheReads, 2)
		assert.ok(Math.abs(metrics.totalCost - 0.2) < 1e-9)
	})

	it("ignores malformed usage payloads", () => {
		const messages: ClineMessage[] = [
			{
				ts: 1,
				type: "say",
				say: "subagent_usage",
				text: "{not-json",
			},
		]

		const metrics = getApiMetrics(messages)
		assert.equal(metrics.totalTokensIn, 0)
		assert.equal(metrics.totalTokensOut, 0)
		assert.equal(metrics.totalCost, 0)
	})
})

describe("getLastApiReqTotalTokens", () => {
	it("uses only the latest api_req_started payload", () => {
		const messages: ClineMessage[] = [
			{
				ts: 1,
				type: "say",
				say: "subagent_usage",
				text: JSON.stringify({
					source: "subagents",
					tokensIn: 100,
					tokensOut: 200,
				}),
			},
			{
				ts: 2,
				type: "say",
				say: "api_req_started",
				text: JSON.stringify({
					tokensIn: 11,
					tokensOut: 7,
					cacheWrites: 2,
					cacheReads: 3,
				}),
			},
		]

		const total = getLastApiReqTotalTokens(messages)
		assert.equal(total, 23)
	})
})


--- .github/scripts/coverage_check/github_api.py ---
"""
GitHub API module.
This module handles interactions with the GitHub API for posting comments to PRs.
"""

import os
import requests
from .util import log, file_exists

def generate_comment(base_ext_cov, pr_ext_cov, ext_decreased, ext_diff, 
                    base_web_cov, pr_web_cov, web_decreased, web_diff):
    """
    Generate a PR comment with coverage comparison.
    
    Args:
        base_ext_cov: Base branch extension coverage
        pr_ext_cov: PR branch extension coverage
        ext_decreased: Whether extension coverage decreased
        ext_diff: Extension coverage difference
        base_web_cov: Base branch webview coverage
        pr_web_cov: PR branch webview coverage
        web_decreased: Whether webview coverage decreased
        web_diff: Webview coverage difference
        
    Returns:
        Comment text
    """
    from datetime import datetime
    
    # Convert string inputs to appropriate types
    try:
        base_ext_cov = float(base_ext_cov)
        pr_ext_cov = float(pr_ext_cov)
        # Handle ext_decreased as either string or boolean
        if isinstance(ext_decreased, str):
            ext_decreased = ext_decreased.lower() == 'true'
        else:
            ext_decreased = bool(ext_decreased)
        ext_diff = float(ext_diff)
        base_web_cov = float(base_web_cov)
        pr_web_cov = float(pr_web_cov)
        # Handle web_decreased as either string or boolean
        if isinstance(web_decreased, str):
            web_decreased = web_decreased.lower() == 'true'
        else:
            web_decreased = bool(web_decreased)
        web_diff = float(web_diff)
    except ValueError as e:
        log(f"Error converting input values: {e}")
        return ""
    
    # Add a unique identifier to find this comment later
    comment = '<!-- COVERAGE_REPORT -->\n'
    comment += '## Coverage Report\n\n'

    # Extension coverage
    comment += '### Extension Coverage\n\n'
    comment += f'Base branch: {base_ext_cov:.0f}%\n\n'
    comment += f'PR branch: {pr_ext_cov:.0f}%\n\n'

    if ext_decreased:
        comment += f'⚠️ **Warning: Coverage decreased by {ext_diff:.2f}%**\n\n'
        comment += 'Consider adding tests to cover your changes.\n\n'
    else:
        comment += '✅ Coverage increased or remained the same\n\n'

    # Webview coverage
    comment += '### Webview Coverage\n\n'
    comment += f'Base branch: {base_web_cov:.0f}%\n\n'
    comment += f'PR branch: {pr_web_cov:.0f}%\n\n'

    if web_decreased:
        comment += f'⚠️ **Warning: Coverage decreased by {web_diff:.2f}%**\n\n'
        comment += 'Consider adding tests to cover your changes.\n\n'
    else:
        comment += '✅ Coverage increased or remained the same\n\n'

    # Overall assessment
    comment += '### Overall Assessment\n\n'
    if ext_decreased or web_decreased:
        comment += '⚠️ **Test coverage has decreased in this PR**\n\n'
        comment += 'Please consider adding tests to maintain or improve coverage.\n\n'
    else:
        comment += '✅ **Test coverage has been maintained or improved**\n\n'

    # Add timestamp
    comment += f'\n\n<sub>Last updated: {datetime.now().isoformat()}</sub>'
    
    return comment

def post_comment(comment_path, pr_number, repo, token=None):
    """
    Post a comment to a GitHub PR.
    
    Args:
        comment_path: Path to the file containing the comment text
        pr_number: PR number
        repo: Repository in the format "owner/repo"
        token: GitHub token
    """
    if not file_exists(comment_path):
        log(f"Error: Comment file {comment_path} does not exist")
        return
    
    with open(comment_path, 'r') as f:
        comment_body = f.read()
    
    if not token:
        token = os.environ.get('GITHUB_TOKEN')
        if not token:
            log("Error: GitHub token not provided")
            return
    
    # Find existing comment
    headers = {
        'Authorization': f'token {token}',
        'Accept': 'application/vnd.github.v3+json'
    }
    
    # Get all comments
    comments_url = f'https://api.github.com/repos/{repo}/issues/{pr_number}/comments'
    log(f"Getting comments from: {comments_url}")
    response = requests.get(comments_url, headers=headers)
    
    if response.status_code != 200:
        log(f"Error getting comments: {response.status_code} - {response.text}")
        return
    
    comments = response.json()
    log(f"Found {len(comments)} existing comments")
    
    # Find comment with our identifier
    comment_id = None
    for comment in comments:
        if '<!-- COVERAGE_REPORT -->' in comment['body']:
            comment_id = comment['id']
            log(f"Found existing coverage report comment with ID: {comment_id}")
            break
    
    if comment_id:
        # Update existing comment
        update_url = f'https://api.github.com/repos/{repo}/issues/comments/{comment_id}'
        log(f"Updating existing comment at: {update_url}")
        response = requests.patch(update_url, headers=headers, json={'body': comment_body})
        
        if response.status_code == 200:
            log(f"Successfully updated existing comment: {comment_id}")
        else:
            log(f"Error updating comment: {response.status_code} - {response.text}")
    else:
        # Create new comment
        log(f"Creating new comment at: {comments_url}")
        response = requests.post(comments_url, headers=headers, json={'body': comment_body})
        
        if response.status_code == 201:
            log("Successfully created new comment")
        else:
            log(f"Error creating comment: {response.status_code} - {response.text}")

def set_github_output(name, value):
    """
    Set GitHub Actions output variable.
    
    Args:
        name: Output variable name
        value: Output variable value
    """
    # Write to the GitHub output file if available
    if 'GITHUB_OUTPUT' in os.environ:
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write(f"{name}={value}\n")
    else:
        # Fallback to the deprecated method for backward compatibility
        log(f"::set-output name={name}::{value}")
    
    # Also print for human readability
    log(f"{name}: {value}")


--- src/core/api/index.ts ---
import { ApiConfiguration, ModelInfo, QwenApiRegions } from "@shared/api"
import { Mode } from "@shared/storage/types"
import { ClineStorageMessage } from "@/shared/messages/content"
import { Logger } from "@/shared/services/Logger"
import { ClineTool } from "@/shared/tools"
import { AIhubmixHandler } from "./providers/aihubmix"
import { AnthropicHandler } from "./providers/anthropic"
import { AskSageHandler } from "./providers/asksage"
import { BasetenHandler } from "./providers/baseten"
import { AwsBedrockHandler } from "./providers/bedrock"
import { CerebrasHandler } from "./providers/cerebras"
import { ClaudeCodeHandler } from "./providers/claude-code"
import { ClineHandler } from "./providers/cline"
import { DeepSeekHandler } from "./providers/deepseek"
import { DifyHandler } from "./providers/dify"
import { DoubaoHandler } from "./providers/doubao"
import { FireworksHandler } from "./providers/fireworks"
import { GeminiHandler } from "./providers/gemini"
import { GroqHandler } from "./providers/groq"
import { HicapHandler } from "./providers/hicap"
import { HuaweiCloudMaaSHandler } from "./providers/huawei-cloud-maas"
import { HuggingFaceHandler } from "./providers/huggingface"
import { LiteLlmHandler } from "./providers/litellm"
import { LmStudioHandler } from "./providers/lmstudio"
import { MinimaxHandler } from "./providers/minimax"
import { MistralHandler } from "./providers/mistral"
import { MoonshotHandler } from "./providers/moonshot"
import { NebiusHandler } from "./providers/nebius"
import { NousResearchHandler } from "./providers/nousresearch"
import { OcaHandler } from "./providers/oca"
import { OllamaHandler } from "./providers/ollama"
import { OpenAiHandler } from "./providers/openai"
import { OpenAiCodexHandler } from "./providers/openai-codex"
import { OpenAiNativeHandler } from "./providers/openai-native"
import { OpenRouterHandler } from "./providers/openrouter"
import { QwenHandler } from "./providers/qwen"
import { QwenCodeHandler } from "./providers/qwen-code"
import { RequestyHandler } from "./providers/requesty"
import { SambanovaHandler } from "./providers/sambanova"
import { SapAiCoreHandler } from "./providers/sapaicore"
import { TogetherHandler } from "./providers/together"
import { VercelAIGatewayHandler } from "./providers/vercel-ai-gateway"
import { VertexHandler } from "./providers/vertex"
import { VsCodeLmHandler } from "./providers/vscode-lm"
import { XAIHandler } from "./providers/xai"
import { ZAiHandler } from "./providers/zai"
import { ApiStream, ApiStreamUsageChunk } from "./transform/stream"

export type CommonApiHandlerOptions = {
	onRetryAttempt?: ApiConfiguration["onRetryAttempt"]
}
export interface ApiHandler {
	createMessage(systemPrompt: string, messages: ClineStorageMessage[], tools?: ClineTool[], useResponseApi?: boolean): ApiStream
	getModel(): ApiHandlerModel
	getApiStreamUsage?(): Promise<ApiStreamUsageChunk | undefined>
	abort?(): void
}

export interface ApiHandlerModel {
	id: string
	info: ModelInfo
}

export interface ApiProviderInfo {
	providerId: string
	model: ApiHandlerModel
	mode: Mode
	customPrompt?: string // "compact"
	autoCondenseThreshold?: number // 0-1 range
}

export interface SingleCompletionHandler {
	completePrompt(prompt: string): Promise<string>
}

function createHandlerForProvider(
	apiProvider: string | undefined,
	options: Omit<ApiConfiguration, "apiProvider">,
	mode: Mode,
): ApiHandler {
	switch (apiProvider) {
		case "anthropic":
			return new AnthropicHandler({
				onRetryAttempt: options.onRetryAttempt,
				apiKey: options.apiKey,
				anthropicBaseUrl: options.anthropicBaseUrl,
				apiModelId: mode === "plan" ? options.planModeApiModelId : options.actModeApiModelId,
				thinkingBudgetTokens:
					mode === "plan" ? options.planModeThinkingBudgetTokens : options.actModeThinkingBudgetTokens,
			})
		case "openrouter":
			return new OpenRouterHandler({
				onRetryAttempt: options.onRetryAttempt,
				openRouterApiKey: options.openRouterApiKey,
				openRouterModelId: mode === "plan" ? options.planModeOpenRouterModelId : options.actModeOpenRouterModelId,
				openRouterModelInfo: mode === "plan" ? options.planModeOpenRouterModelInfo : options.actModeOpenRouterModelInfo,
				openRouterProviderSorting: options.openRouterProviderSorting,
				reasoningEffort: mode === "plan" ? options.planModeReasoningEffort : options.actModeReasoningEffort,
				thinkingBudgetTokens:
					mode === "plan" ? options.planModeThinkingBudgetTokens : options.actModeThinkingBudgetTokens,
			})
		case "bedrock":
			return new AwsBedrockHandler({
				onRetryAttempt: options.onRetryAttempt,
				apiModelId: mode === "plan" ? options.planModeApiModelId : options.actModeApiModelId,
				awsAccessKey: options.awsAccessKey,
				awsSecretKey: options.awsSecretKey,
				awsSessionToken: options.awsSessionToken,
				awsRegion: options.awsRegion,
				awsAuthentication: options.awsAuthentication,
				awsBedrockApiKey: options.awsBedrockApiKey,
				awsUseCrossRegionInference: options.awsUseCrossRegionInference,
				awsUseGlobalInference: options.awsUseGlobalInference,
				awsBedrockUsePromptCache: options.awsBedrockUsePromptCache,
				awsUseProfile: options.awsUseProfile,
				awsProfile: options.awsProfile,
				awsBedrockEndpoint: options.awsBedrockEndpoint,
				awsBedrockCustomSelected:
					mode === "plan" ? options.planModeAwsBedrockCustomSelected : options.actModeAwsBedrockCustomSelected,
				awsBedrockCustomModelBaseId:
					mode === "plan" ? options.planModeAwsBedrockCustomModelBaseId : options.actModeAwsBedrockCustomModelBaseId,
				thinkingBudgetTokens:
					mode === "plan" ? options.planModeThinkingBudgetTokens : options.actModeThinkingBudgetTokens,
			})
		case "vertex":
			return new VertexHandler({
				onRetryAttempt: options.onRetryAttempt,
				vertexProjectId: options.vertexProjectId,
				vertexRegion: options.vertexRegion,
				apiModelId: mode === "plan" ? options.planModeApiModelId : options.actModeApiModelId,
				thinkingBudgetTokens:
					mode === "plan" ? options.planModeThinkingBudgetTokens : options.actModeThinkingBudgetTokens,
				geminiApiKey: options.geminiApiKey,
				geminiBaseUrl: options.geminiBaseUrl,
				reasoningEffort: mode === "plan" ? options.planModeReasoningEffort : options.actModeReasoningEffort,
				ulid: options.ulid,
			})
		case "openai":
			return new OpenAiHandler({
				onRetryAttempt: options.onRetryAttempt,
				openAiApiKey: options.openAiApiKey,
				openAiBaseUrl: options.openAiBaseUrl,
				azureApiVersion: options.azureApiVersion,
				azureIdentity: options.azureIdentity,
				openAiHeaders: options.openAiHeaders,
				openAiModelId: mode === "plan" ? options.planModeOpenAiModelId : options.actModeOpenAiModelId,
				openAiModelInfo: mode === "plan" ? options.planModeOpenAiModelInfo : options.actModeOpenAiModelInfo,
				reasoningEffort: mode === "plan" ? options.planModeReasoningEffort : options.actModeReasoningEffort,
			})
		case "ollama":
			return new OllamaHandler({
				onRetryAttempt: options.onRetryAttempt,
				ollamaBaseUrl: options.ollamaBaseUrl,
				ollamaApiKey: options.ollamaApiKey,
				ollamaModelId: mode === "plan" ? options.planModeOllamaModelId : options.actModeOllamaModelId,
				ollamaApiOptionsCtxNum: options.ollamaApiOptionsCtxNum,
				requestTimeoutMs: options.requestTimeoutMs,
			})
		case "lmstudio":
			return new LmStudioHandler({
				onRetryAttempt: options.onRetryAttempt,
				lmStudioBaseUrl: options.lmStudioBaseUrl,
				lmStudioModelId: mode === "plan" ? options.planModeLmStudioModelId : options.actModeLmStudioModelId,
				lmStudioMaxTokens: options.lmStudioMaxTokens,
			})
		case "gemini":
			return new GeminiHandler({
				onRetryAttempt: options.onRetryAttempt,
				vertexProjectId: options.vertexProjectId,
				vertexRegion: options.vertexRegion,
				geminiApiKey: options.geminiApiKey,
				geminiBaseUrl: options.geminiBaseUrl,
				thinkingBudgetTokens:
					mode === "plan" ? options.planModeThinkingBudgetTokens : options.actModeThinkingBudgetTokens,
				reasoningEffort: mode === "plan" ? options.planModeReasoningEffort : options.actModeReasoningEffort,
				apiModelId: mode === "plan" ? options.planModeApiModelId : options.actModeApiModelId,
				ulid: options.ulid,
			})
		case "openai-native":
			return new OpenAiNativeHandler({
				onRetryAttempt: options.onRetryAttempt,
				openAiNativeApiKey: options.openAiNativeApiKey,
				reasoningEffort: mode === "plan" ? options.planModeReasoningEffort : options.actModeReasoningEffort,
				apiModelId: mode === "plan" ? options.planModeApiModelId : options.actModeApiModelId,
				thinkingBudgetTokens:
					mode === "plan" ? options.planModeThinkingBudgetTokens : options.actModeThinkingBudgetTokens,
			})
		case "openai-codex":
			return new OpenAiCodexHandler({
				onRetryAttempt: options.onRetryAttempt,
				reasoningEffort: mode === "plan" ? options.planModeReasoningEffort : options.actModeReasoningEffort,
				apiModelId: mode === "plan" ? options.planModeApiModelId : options.actModeApiModelId,
			})
		case "deepseek":
			return new DeepSeekHandler({
				onRetryAttempt: options.onRetryAttempt,
				deepSeekApiKey: options.deepSeekApiKey,
				apiModelId: mode === "plan" ? options.planModeApiModelId : options.actModeApiModelId,
			})
		case "requesty":
			return new RequestyHandler({
				onRetryAttempt: options.onRetryAttempt,
				requestyBaseUrl: options.requestyBaseUrl,
				requestyApiKey: options.requestyApiKey,
				reasoningEffort: mode === "plan" ? options.planModeReasoningEffort : options.actModeReasoningEffort,
				thinkingBudgetTokens:
					mode === "plan" ? options.planModeThinkingBudgetTokens : options.actModeThinkingBudgetTokens,
				requestyModelId: mode === "plan" ? options.planModeRequestyModelId : options.actModeRequestyModelId,
				requestyModelInfo: mode === "plan" ? options.planModeRequestyModelInfo : options.actModeRequestyModelInfo,
			})
		case "fireworks":
			return new FireworksHandler({
				onRetryAttempt: options.onRetryAttempt,
				fireworksApiKey: options.fireworksApiKey,
				fireworksModelId: mode === "plan" ? options.planModeFireworksModelId : options.actModeFireworksModelId,
			})
		case "together":
			return new TogetherHandler({
				onRetryAttempt: options.onRetryAttempt,
				togetherApiKey: options.togetherApiKey,
				togetherModelId: mode === "plan" ? options.planModeTogetherModelId : options.actModeTogetherModelId,
			})
		case "qwen":
			return new QwenHandler({
				onRetryAttempt: options.onRetryAttempt,
				qwenApiKey: options.qwenApiKey,
				qwenApiLine:
					options.qwenApiLine === QwenApiRegions.INTERNATIONAL ? QwenApiRegions.INTERNATIONAL : QwenApiRegions.CHINA,
				apiModelId: mode === "plan" ? options.planModeApiModelId : options.actModeApiModelId,
				thinkingBudgetTokens:
					mode === "plan" ? options.planModeThinkingBudgetTokens : options.actModeThinkingBudgetTokens,
			})
		case "qwen-code":
			return new QwenCodeHandler({
				onRetryAttempt: options.onRetryAttempt,
				qwenCodeOauthPath: options.qwenCodeOauthPath,
				apiModelId: mode === "plan" ? options.planModeApiModelId : options.actModeApiModelId,
			})
		case "doubao":
			return new DoubaoHandler({
				onRetryAttempt: options.onRetryAttempt,
				doubaoApiKey: options.doubaoApiKey,
				apiModelId: mode === "plan" ? options.planModeApiModelId : options.actModeApiModelId,
			})
		case "mistral":
			return new MistralHandler({
				onRetryAttempt: options.onRetryAttempt,
				mistralApiKey: options.mistralApiKey,
				apiModelId: mode === "plan" ? options.planModeApiModelId : options.actModeApiModelId,
			})
		case "vscode-lm":
			return new VsCodeLmHandler({
				onRetryAttempt: options.onRetryAttempt,
				vsCodeLmModelSelector:
					mode === "plan" ? options.planModeVsCodeLmModelSelector : options.actModeVsCodeLmModelSelector,
			})
		case "cline":
			return new ClineHandler({
				onRetryAttempt: options.onRetryAttempt,
				clineAccountId: options.clineAccountId,
				clineApiKey: options.clineApiKey,
				ulid: options.ulid,
				reasoningEffort: mode === "plan" ? options.planModeReasoningEffort : options.actModeReasoningEffort,
				thinkingBudgetTokens:
					mode === "plan" ? options.planModeThinkingBudgetTokens : options.actModeThinkingBudgetTokens,
				openRouterProviderSorting: options.openRouterProviderSorting,
				openRouterModelId: mode === "plan" ? options.planModeOpenRouterModelId : options.actModeOpenRouterModelId,
				openRouterModelInfo: mode === "plan" ? options.planModeOpenRouterModelInfo : options.actModeOpenRouterModelInfo,
			})
		case "litellm":
			return new LiteLlmHandler({
				onRetryAttempt: options.onRetryAttempt,
				liteLlmApiKey: options.liteLlmApiKey,
				liteLlmBaseUrl: options.liteLlmBaseUrl,
				liteLlmModelId: mode === "plan" ? options.planModeLiteLlmModelId : options.actModeLiteLlmModelId,
				liteLlmModelInfo: mode === "plan" ? options.planModeLiteLlmModelInfo : options.actModeLiteLlmModelInfo,
				thinkingBudgetTokens:
					mode === "plan" ? options.planModeThinkingBudgetTokens : options.actModeThinkingBudgetTokens,
				liteLlmUsePromptCache: options.liteLlmUsePromptCache,
				ulid: options.ulid,
			})
		case "moonshot":
			return new MoonshotHandler({
				onRetryAttempt: options.onRetryAttempt,
				moonshotApiKey: options.moonshotApiKey,
				moonshotApiLine: options.moonshotApiLine,
				apiModelId: mode === "plan" ? options.planModeApiModelId : options.actModeApiModelId,
			})
		case "huggingface":
			return new HuggingFaceHandler({
				onRetryAttempt: options.onRetryAttempt,
				huggingFaceApiKey: options.huggingFaceApiKey,
				huggingFaceModelId: mode === "plan" ? options.planModeHuggingFaceModelId : options.actModeHuggingFaceModelId,
				huggingFaceModelInfo:
					mode === "plan" ? options.planModeHuggingFaceModelInfo : options.actModeHuggingFaceModelInfo,
			})
		case "nebius":
			return new NebiusHandler({
				onRetryAttempt: options.onRetryAttempt,
				nebiusApiKey: options.nebiusApiKey,
				apiModelId: mode === "plan" ? options.planModeApiModelId : options.actModeApiModelId,
			})
		case "asksage":
			return new AskSageHandler({
				onRetryAttempt: options.onRetryAttempt,
				asksageApiKey: options.asksageApiKey,
				asksageApiUrl: options.asksageApiUrl,
				apiModelId: mode === "plan" ? options.planModeApiModelId : options.actModeApiModelId,
			})
		case "xai":
			return new XAIHandler({
				onRetryAttempt: options.onRetryAttempt,
				xaiApiKey: options.xaiApiKey,
				reasoningEffort: mode === "plan" ? options.planModeReasoningEffort : options.actModeReasoningEffort,
				apiModelId: mode === "plan" ? options.planModeApiModelId : options.actModeApiModelId,
			})
		case "sambanova":
			return new SambanovaHandler({
				onRetryAttempt: options.onRetryAttempt,
				sambanovaApiKey: options.sambanovaApiKey,
				apiModelId: mode === "plan" ? options.planModeApiModelId : options.actModeApiModelId,
			})
		case "cerebras":
			return new CerebrasHandler({
				onRetryAttempt: options.onRetryAttempt,
				cerebrasApiKey: options.cerebrasApiKey,
				apiModelId: mode === "plan" ? options.planModeApiModelId : options.actModeApiModelId,
			})
		case "groq":
			return new GroqHandler({
				onRetryAttempt: options.onRetryAttempt,
				groqApiKey: options.groqApiKey,
				groqModelId: mode === "plan" ? options.planModeGroqModelId : options.actModeGroqModelId,
				groqModelInfo: mode === "plan" ? options.planModeGroqModelInfo : options.actModeGroqModelInfo,
				apiModelId: mode === "plan" ? options.planModeApiModelId : options.actModeApiModelId,
			})
		case "baseten":
			return new BasetenHandler({
				onRetryAttempt: options.onRetryAttempt,
				basetenApiKey: options.basetenApiKey,
				basetenModelId: mode === "plan" ? options.planModeBasetenModelId : options.actModeBasetenModelId,
				basetenModelInfo: mode === "plan" ? options.planModeBasetenModelInfo : options.actModeBasetenModelInfo,
				apiModelId: mode === "plan" ? options.planModeApiModelId : options.actModeApiModelId,
			})
		case "sapaicore":
			return new SapAiCoreHandler({
				onRetryAttempt: options.onRetryAttempt,
				sapAiCoreClientId: options.sapAiCoreClientId,
				sapAiCoreClientSecret: options.sapAiCoreClientSecret,
				sapAiCoreTokenUrl: options.sapAiCoreTokenUrl,
				sapAiResourceGroup: options.sapAiResourceGroup,
				sapAiCoreBaseUrl: options.sapAiCoreBaseUrl,
				apiModelId: mode === "plan" ? options.planModeApiModelId : options.actModeApiModelId,
				thinkingBudgetTokens:
					mode === "plan" ? options.planModeThinkingBudgetTokens : options.actModeThinkingBudgetTokens,
				reasoningEffort: mode === "plan" ? options.planModeReasoningEffort : options.actModeReasoningEffort,
				deploymentId: mode === "plan" ? options.planModeSapAiCoreDeploymentId : options.actModeSapAiCoreDeploymentId,
				sapAiCoreUseOrchestrationMode: options.sapAiCoreUseOrchestrationMode,
			})
		case "claude-code":
			return new ClaudeCodeHandler({
				onRetryAttempt: options.onRetryAttempt,
				claudeCodePath: options.claudeCodePath,
				apiModelId: mode === "plan" ? options.planModeApiModelId : options.actModeApiModelId,
				thinkingBudgetTokens:
					mode === "plan" ? options.planModeThinkingBudgetTokens : options.actModeThinkingBudgetTokens,
			})
		case "huawei-cloud-maas":
			return new HuaweiCloudMaaSHandler({
				onRetryAttempt: options.onRetryAttempt,
				huaweiCloudMaasApiKey: options.huaweiCloudMaasApiKey,
				huaweiCloudMaasModelId:
					mode === "plan" ? options.planModeHuaweiCloudMaasModelId : options.actModeHuaweiCloudMaasModelId,
				huaweiCloudMaasModelInfo:
					mode === "plan" ? options.planModeHuaweiCloudMaasModelInfo : options.actModeHuaweiCloudMaasModelInfo,
			})
		case "dify": // Add Dify.ai handler
			return new DifyHandler({
				difyApiKey: options.difyApiKey,
				difyBaseUrl: options.difyBaseUrl,
			})
		case "vercel-ai-gateway":
			return new VercelAIGatewayHandler({
				onRetryAttempt: options.onRetryAttempt,
				vercelAiGatewayApiKey: options.vercelAiGatewayApiKey,
				openRouterModelId:
					mode === "plan" ? options.planModeVercelAiGatewayModelId : options.actModeVercelAiGatewayModelId,
				openRouterModelInfo:
					mode === "plan" ? options.planModeVercelAiGatewayModelInfo : options.actModeVercelAiGatewayModelInfo,
				reasoningEffort: mode === "plan" ? options.planModeReasoningEffort : options.actModeReasoningEffort,
				thinkingBudgetTokens:
					mode === "plan" ? options.planModeThinkingBudgetTokens : options.actModeThinkingBudgetTokens,
			})
		case "zai":
			return new ZAiHandler({
				onRetryAttempt: options.onRetryAttempt,
				zaiApiLine: options.zaiApiLine,
				zaiApiKey: options.zaiApiKey,
				apiModelId: mode === "plan" ? options.planModeApiModelId : options.actModeApiModelId,
			})
		case "oca":
			return new OcaHandler({
				ocaMode: options.ocaMode || "internal",
				ocaBaseUrl: options.ocaBaseUrl,
				ocaModelId: mode === "plan" ? options.planModeOcaModelId : options.actModeOcaModelId,
				ocaModelInfo: mode === "plan" ? options.planModeOcaModelInfo : options.actModeOcaModelInfo,
				ocaReasoningEffort: mode === "plan" ? options.planModeOcaReasoningEffort : options.actModeOcaReasoningEffort,
				thinkingBudgetTokens:
					mode === "plan" ? options.planModeThinkingBudgetTokens : options.actModeThinkingBudgetTokens,
				ocaUsePromptCache:
					mode === "plan"
						? options.planModeOcaModelInfo?.supportsPromptCache
						: options.actModeOcaModelInfo?.supportsPromptCache,
				taskId: options.ulid,
			})
		case "aihubmix":
			return new AIhubmixHandler({
				onRetryAttempt: options.onRetryAttempt,
				apiKey: options.aihubmixApiKey,
				baseURL: options.aihubmixBaseUrl,
				appCode: options.aihubmixAppCode,
				modelId: mode === "plan" ? (options as any).planModeAihubmixModelId : (options as any).actModeAihubmixModelId,
				modelInfo:
					mode === "plan" ? (options as any).planModeAihubmixModelInfo : (options as any).actModeAihubmixModelInfo,
			})
		case "minimax":
			return new MinimaxHandler({
				onRetryAttempt: options.onRetryAttempt,
				minimaxApiKey: options.minimaxApiKey,
				minimaxApiLine: options.minimaxApiLine,
				apiModelId: mode === "plan" ? options.planModeApiModelId : options.actModeApiModelId,
			})
		case "hicap":
			return new HicapHandler({
				onRetryAttempt: options.onRetryAttempt,
				hicapApiKey: options.hicapApiKey,
				hicapModelId: mode === "plan" ? options.planModeHicapModelId : options.actModeHicapModelId,
			})
		case "nousResearch":
			return new NousResearchHandler({
				onRetryAttempt: options.onRetryAttempt,
				nousResearchApiKey: options.nousResearchApiKey,
				apiModelId: mode === "plan" ? options.planModeNousResearchModelId : options.actModeNousResearchModelId,
			})
		default:
			return new AnthropicHandler({
				onRetryAttempt: options.onRetryAttempt,
				apiKey: options.apiKey,
				anthropicBaseUrl: options.anthropicBaseUrl,
				apiModelId: mode === "plan" ? options.planModeApiModelId : options.actModeApiModelId,
				thinkingBudgetTokens:
					mode === "plan" ? options.planModeThinkingBudgetTokens : options.actModeThinkingBudgetTokens,
			})
	}
}

export function buildApiHandler(configuration: ApiConfiguration, mode: Mode): ApiHandler {
	const { planModeApiProvider, actModeApiProvider, ...options } = configuration

	const apiProvider = mode === "plan" ? planModeApiProvider : actModeApiProvider

	// Validate thinking budget tokens against model's maxTokens to prevent API errors
	// wrapped in a try-catch for safety, but this should never throw
	try {
		const thinkingBudgetTokens = mode === "plan" ? options.planModeThinkingBudgetTokens : options.actModeThinkingBudgetTokens
		if (thinkingBudgetTokens && thinkingBudgetTokens > 0) {
			const handler = createHandlerForProvider(apiProvider, options, mode)

			const modelInfo = handler.getModel().info
			if (modelInfo?.maxTokens && modelInfo.maxTokens > 0 && thinkingBudgetTokens > modelInfo.maxTokens) {
				const clippedValue = modelInfo.maxTokens - 1
				if (mode === "plan") {
					options.planModeThinkingBudgetTokens = clippedValue
				} else {
					options.actModeThinkingBudgetTokens = clippedValue
				}
			} else {
				return handler // don't rebuild unless its necessary
			}
		}
	} catch (error) {
		Logger.error("buildApiHandler error:", error)
	}

	return createHandlerForProvider(apiProvider, options, mode)
}


--- evals/ARCHITECTURE.md ---
# Cline Evals Architecture

## Overview

The evals system provides multi-layered testing for Cline's AI capabilities.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           TESTING PYRAMID                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│                              ┌─────────┐                                    │
│                             /   E2E    \         Layer 3: Full Agent        │
│                            /  cline-   \         - Real coding tasks        │
│                           /   bench     \        - Harbor execution         │
│                          /_______________\       - Nightly runs             │
│                                                                             │
│                        ┌───────────────────┐                                │
│                       /    Smoke Tests     \     Layer 2: Provider          │
│                      /   run-smoke-tests    \    - 5 curated scenarios      │
│                     /    (cline provider)    \   - 3 models via Vercel      │
│                    /_________________________\   - pass@k metrics           │
│                                                                             │
│              ┌─────────────────────────────────┐                            │
│             /        Contract Tests            \  Layer 1: Unit             │
│            /   thinking-traces.test.ts          \ - No LLM calls            │
│           /    tool-parsing.test.ts              \ - Fast, deterministic    │
│          /______________________________________ \ - API format validation  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

## Directory Structure

```
evals/
├── ARCHITECTURE.md          # This file
├── README.md                # Quick start guide
│
├── analysis/                # Shared metrics & reporting
│   └── src/
│       ├── metrics.ts       # pass@k, pass^k, flakiness calculations
│       └── cli.ts           # Analysis CLI
│
├── smoke-tests/             # Layer 2: Provider smoke tests
│   ├── run-smoke-tests.ts   # Main runner
│   ├── README.md            # Usage docs
│   ├── scenarios/           # Test definitions
│   │   ├── 01-create-file/
│   │   │   ├── config.json  # Prompt, expected files/content
│   │   │   ├── template/    # Initial files (if any)
│   │   │   └── workspace/   # Working dir (cleaned each run)
│   │   ├── 02-edit-file/
│   │   ├── 03-read-summarize/
│   │   ├── 04-multi-file/
│   │   └── 05-typescript-function/
│   └── results/             # Generated outputs
│       ├── latest -> 2026-01-27T.../  # Symlink to most recent
│       └── 2026-01-27T19-50-54-391Z/
│           ├── report.json  # Full results
│           ├── summary.md   # CI-friendly markdown
│           └── 01-create-file/
│               └── claude-sonnet/
│                   ├── trial-1.log           # CLI stdout/stderr
│                   └── workspace-trial-1/    # Kept for failures only
│
├── e2e/                     # Layer 3: Full agent E2E
│   ├── run-cline-bench.ts   # Harbor runner
│   └── README.md
│
└── cline-bench/             # Git submodule with real coding tasks
    └── tasks/               # SWE-bench style problems
```

## Smoke Test Workflow

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                         SMOKE TEST EXECUTION FLOW                            │
└──────────────────────────────────────────────────────────────────────────────┘

 npm run eval:smoke
        │
        ▼
┌───────────────────┐
│ Load scenarios    │  Read config.json from each scenarios/* dir
│ from disk         │
└────────┬──────────┘
         │
         ▼
┌───────────────────┐
│ Create results    │  evals/smoke-tests/results/2026-01-27T.../
│ directory         │
└────────┬──────────┘
         │
         ▼
┌───────────────────────────────────────────────────────────────┐
│                    FOR EACH SCENARIO                          │
│  ┌─────────────────────────────────────────────────────────┐  │
│  │                  FOR EACH MODEL                         │  │
│  │  ┌───────────────────────────────────────────────────┐  │  │
│  │  │           RUN 3 TRIALS SEQUENTIALLY               │  │  │
│  │  │                                                   │  │  │
│  │  │  Trial 1 ──► Trial 2 ──► Trial 3 ──► Results      │  │  │
│  │  │  (Sequential - Cline instance handles one at a time)  │  │
│  │  │                                                   │  │  │
│  │  │  Each trial:                                      │  │  │
│  │  │  1. Create workspace-trial-N/                     │  │  │
│  │  │  2. Copy template files (if any)                  │  │  │
│  │  │  3. Run: cline -y -o "prompt"                     │  │  │
│  │  │  4. Verify expected files exist                   │  │  │
│  │  │  5. Verify expected content                       │  │  │
│  │  │  6. Save trial-N.log                              │  │  │
│  │  │  7. If failed, copy workspace to results/         │  │  │
│  │  └───────────────────────────────────────────────────┘  │  │
│  │                         │                               │  │
│  │                         ▼                               │  │
│  │  ┌───────────────────────────────────────────────────┐  │  │
│  │  │ Calculate metrics: pass@1, pass@3, pass^3, flaky  │  │  │
│  │  └───────────────────────────────────────────────────┘  │  │
│  └─────────────────────────────────────────────────────────┘  │
└───────────────────────────────────────────────────────────────┘
         │
         ▼
┌───────────────────┐
│ Generate outputs  │
│ - report.json     │
│ - summary.md      │
│ - latest symlink  │
└───────────────────┘
```

## Models Tested

```
┌─────────────────────────────────────────────────────────────────┐
│                    CLINE PROVIDER ROUTING                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   ┌─────────────┐                                               │
│   │ Smoke Test  │                                               │
│   │   Runner    │                                               │
│   └──────┬──────┘                                               │
│          │                                                      │
│          │  cline -y -o "prompt" --model <model>                │
│          │                                                      │
│          ▼                                                      │
│   ┌─────────────┐                                               │
│   │   Cline     │                                               │
│   │  Provider   │ ◄─── Uses your Cline auth (cline auth)        │
│   └──────┬──────┘                                               │
│          │                                                      │
│          │ Routes to backend                                    │
│          ▼                                                      │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │                   Default Models                        │   │
│   ├─────────────────────────────────────────────────────────┤   │
│   │  claude-sonnet-4-20250514                               │   │
│   │  gpt-4o                                                 │   │
│   │  gemini-2.5-pro-preview-06-05                           │   │
│   └─────────────────────────────────────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

## Metrics Explained

```
┌─────────────────────────────────────────────────────────────────┐
│                        METRICS                                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  pass@k   "What's the probability of getting at least one      │
│            success if I run k trials?"                          │
│                                                                 │
│            Example: 2/3 trials pass → pass@3 ≈ 96%              │
│            (Very likely to pass if you run 3 times)             │
│                                                                 │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  pass^k   "What's the probability of ALL k trials succeeding?" │
│                                                                 │
│            Example: 2/3 trials pass → pass^3 ≈ 30%              │
│            (Only 30% chance all 3 would pass)                   │
│                                                                 │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Status   PASS  = All trials passed                             │
│           FLAKY = Some passed, some failed                      │
│           FAIL  = All trials failed                             │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

## Quick Commands

```bash
# Run all smoke tests (all models, 3 trials each)
npm run eval:smoke

# Run single model (use exact model ID for reproducibility)
npm run eval:smoke -- --model claude-sonnet-4-20250514

# Run single scenario
npm run eval:smoke -- --scenario 01-create-file

# Quick check (1 trial)
npm run eval:smoke -- --trials 1

# CI-like run (builds CLI from source, single trial)
npm run eval:smoke:ci

# View latest results
cat evals/smoke-tests/results/latest/summary.md

# Debug a failure
cat evals/smoke-tests/results/latest/<scenario>/<model>/trial-1.log
ls evals/smoke-tests/results/latest/<scenario>/<model>/workspace-trial-1/
```

## CI Integration

Smoke tests run automatically on merge to `main` via `.github/workflows/cline-evals-regression.yml`.

**Triggers:**
- Push to `main` branch (paths: `src/core/**`, `src/shared/**`, `proto/**`)
- Manual dispatch via `workflow_dispatch`

**What it does:**
1. Builds the Go CLI from source via `scripts/run-smoke-tests.sh`
2. Runs all 5 scenarios × 3 models × 1 trial
3. Uploads results as artifact
4. Posts summary to GitHub Actions job summary

```bash
# The CI runs this script which handles proto generation + CLI build:
bash scripts/run-smoke-tests.sh --trials 1
```

### Viewing CI Results

1. **Job Summary**: Each run posts results to the Actions tab
2. **Artifacts**: Full results downloadable as `smoke-test-results-<run_id>`

### Running CI-like Tests Locally

```bash
# One command - builds CLI from source and runs tests
npm run eval:smoke:ci

# Or manually:
npm run protos-go
cd cli && go build -o cline ./cmd/cline
export PATH="$(pwd)/cli:$PATH"
npx tsx evals/smoke-tests/run-smoke-tests.ts --trials 1
```

### Why Build CLI in CI?

We build the Go CLI from source rather than using a pre-built release because:
- Tests actual CLI code from the commit (catches CLI regressions)
- Proto definitions may have changed
- No dependency on external releases

## Contract Tests (Layer 1)

> **Note**: The old `evals/benchmarks/tool-precision/` tests have been removed. Their functionality is now covered by contract tests in `src/core/**/__tests__/` and the 52 system prompt snapshot tests that run with `npm run test:unit`.

Located in `src/core/api/transform/__tests__/`:

```
thinking-traces.test.ts
├── convertToOpenAiMessages preserves reasoning_details
├── convertToAnthropicMessage preserves thinking blocks
└── sanitizeGeminiMessages handles provider-specific cleaning

tool-parsing.test.ts
├── Anthropic tool_use → OpenAI tool_calls conversion
├── Tool call ID truncation (>40 chars)
├── OpenAI Responses API ID transformation
└── Tool result matching
```

Run with:
```bash
npm run test:unit -- --grep "Thinking Trace"  # 9 tests
npm run test:unit -- --grep "Tool Call"       # 11 tests
```


--- evals/benchmarks/tool-precision/replace-in-file/README.md ---
# A Note on Cline's Diff Evaluation Setup

Hey there, this note explains what we're doing with Cline's diff evaluation (evals) system. It's all about checking how well various AI models (which users connect to Cline via their own API keys), prompts, and diffing tools can handle file changes.

## What We're Trying to Figure Out

The main idea here is to figure out which AI models (configured by users) are best at making `replace_in_file` tool calls that work correctly. This helps us understand model capabilities and also speeds up our own experiments with prompts and diffing algorithms to make Cline better over time. We want to know a few key things.

First, can the model create diffs, which are just sets of SEARCH and REPLACE blocks, that apply cleanly to a file? This is what we call `diffEditSuccess`.

Second, how do different LLMs, like Claude or Grok, stack up against each other when they try to make these diff edits? We use a standard set of real-world test cases for this.

Third, do different system prompts, say our `basicSystemPrompt` versus the `claude4SystemPrompt`, change how well a model does at diff editing?

Fourth, we're also looking at different ways to apply the diffs themselves. We have a few algorithms like `constructNewFileContentV1`, `V2`, and `V3`, and we want to see which ones are more robust when fed model-generated diffs.

Fifth, we track how fast the model starts making an edit. The `timeToFirstEditMs` metric gives us a hint about how quickly a user would see changes happening in their editor.

And finally, we keep an eye on how many tokens are used and what it costs for each model and each try. This helps us compare how efficient they are.

Right now, these evals are mostly about whether the diff *applies* correctly. That means, do the SEARCH blocks find a match, and can the REPLACE blocks be put in without an error? We're not yet deeply analyzing if the change is valid code or matches what the user *wanted* semantically. That's a problem for another day, and will require a lot more scaffolding.

## How We Run These Tests

Two prerequisites:

1. Make sure you have an `evals/.env` file with `OPENROUTER_API_KEY=<your-openrouter-key>` 

2. Make sure you add a `evals/diff-edits/cases` folder with all the conversation jsons prior to running this. 


Our testing strategy is based on replaying situations from actual user sessions where diff edits were tried.

It starts with our test cases. Each one is a JSON file in `./cases` that has the conversation history that led to a diff edit, the original file content and its path, and the info needed to rebuild the system prompt from that original session.

Then, for every test run, we set up a specific configuration. This includes which LLM we're testing, which system prompt it gets, which function we use to parse the model's raw output, and which function we use to actually apply the diff. Here's the command I've been using:

```bash
npm run diff-eval -- --model-ids "anthropic/claude-3-5-sonnet,x-ai/grok-3-beta,anthropic/claude-3.7-sonnet,anthropic/claude-sonnet-4,google/gemini-2.5-pro-preview,google/gemini-2.5-flash" --max-cases 5 --valid-attempts-per-case 5 --parallel --diff-edit-function diff-06-26-25 --verbose 
```

This will build the eval script, run it, and then open the streamlit dashboard to show the results.

The `TestRunner.ts` script is the main coordinator. For each test case and setup, `ClineWrapper.ts` takes over and sends the conversation and system prompt to the LLM. We then watch the model's response as it streams in and parse it to find any tool calls.

We're specifically looking for the model to make a single `replace_in_file` tool call. Multiple edits in one tool call are allowed, and recorded (in case you want to filter results by number of edits in a single tool call and compare success rate for that slice across different models/system prompts/etc). If it does, and it's for the correct file, we grab the diff content it produced. Then, the chosen diff application algorithm tries to apply that diff to the original file. We record whether this worked or not as `diffEditSuccess`.

We record a bunch of data for every attempt into a database. This includes details about the model and prompt, token counts, costs, the raw output from the model, the parsed tool calls, whether it succeeded or failed, any error messages, and timing info. For a detailed explanation of the database schema, see [database.md](./database.md).

A big part of this is how we handle "valid attempts," which I'll explain next.

## Keeping it Fair with "Valid Attempts"

LLMs can be unpredictable. If we replay an old scenario, a new model, or even the same model later, might do something completely different than what happened originally. It might call another tool or ask a question instead of trying a diff edit.

Since we really want to test the *diff editing* part, we need a way to make sure we're comparing fairly. That's why we have this idea of "valid attempts."

An attempt is "valid" for this benchmark if the model actually tries to do what we're interested in. This means two things. One, it must call the `replace_in_file` tool. Two, it must target the *same file path* that was targeted in the original recorded conversation for that test case.

If the model does something else, like calling a different tool or picking the wrong file, we don't count that attempt against its diff editing score. Instead, we consider it an "invalid attempt" for *this specific benchmark* and simply re-run that test case with that model. We keep doing this until we've collected a set number of these "valid attempts."

For example, if we ask for 5 valid attempts per test case, the system will keep re-rolling for that case until the model has tried to edit the correct file using the `replace_in_file` tool 5 times. Only then do we look at how many of those 5 valid attempts actually resulted in a successful diff application (`diffEditSuccess`).

This way, if we're comparing two models and one gets a 10% success rate on its valid diff edit attempts, and another gets 90%, we have a much clearer picture of their actual diff-generating capabilities. It avoids muddying the waters with attempts where the model didn't even try to perform the specific action we're evaluating. This approach helps us isolate and measure the diff-editing skill more directly, despite the non-deterministic nature of these models.

## Replays

You can also use the replay argument to replay a previous benchmark run. This is super useful for iterating on our diffing algorithms without having to re-run expensive and time-consuming LLM calls.

When you run an evaluation, every detail is stored in the database—including the raw, unmodified output from the model. The replay feature takes advantage of this by pulling that raw output and feeding it into a *different* diffing algorithm. This lets you isolate the performance of the diffing logic itself. We can see if a new algorithm is better at applying the exact same set of diffs that a model generated in a previous run.

This process is blazingly fast and free, as it completely bypasses the need to make new API calls. It ensures a true apples-to-apples comparison between diffing strategies, since the model's output—the "ground truth" for the evaluation—remains identical.

Here’s an example of how you would replay a previous run with a new diffing algorithm:

```shell
cd evals && npm run diff-eval -- --replay-run-id 9902189e-63a8-4210-a4fc-fe59e2eaf2c2 --diff-apply-file diff-06-23-25 --verbose
```

In this command:
-   `--replay-run-id` specifies the original run we want to use as our ground truth.
-   `--diff-apply-file` tells the script to use the new diffing logic from the `diff-06-23-25.ts` file.

The script will then create a new run in the database that mirrors the original, but with the results of applying the new diffing algorithm. This allows for a direct comparison in the dashboard, helping us quickly see which of our diffing strategies is the most robust.


## Links discovered
- [database.md](https://github.com/cline/cline/blob/main/evals/benchmarks/tool-precision/replace-in-file/database.md)

--- evals/benchmarks/tool-precision/replace-in-file/dashboard/README.md ---
# 🚀 The Sickest Diff Edits Evaluation Dashboard Ever!

A beautiful, modern Streamlit dashboard for visualizing and analyzing diff editing evaluation results with deep drill-down capabilities.

## ✨ Features

### 🎯 **Smart Model Comparison**
- **Latest Run Focus**: Automatically loads and displays your most recent evaluation run
- **Beautiful Performance Cards**: Each model gets a stunning card with performance grades (A+ to C)
- **Best Performer Highlighting**: The top model gets special styling and a trophy 🏆
- **Interactive Charts**: Success rate comparisons and latency vs cost analysis

### 🔍 **Deep Drill-Down Analysis**
- **Individual Result Inspection**: Click any model to see detailed results
- **Side-by-Side File Views**: See original file content with line numbers
- **Parsed Tool Call Analysis**: View exactly what the model tried to do
- **Error Analysis**: Detailed error information for failed attempts
- **Success Metrics**: Line changes, edit counts, and timing breakdowns

### 🎨 **Aesthetic Design**
- **Modern UI**: Custom CSS with Inter font, gradients, and shadows
- **Responsive Layout**: Looks great on any screen size
- **Color-Coded Performance**: Green for excellent, yellow for good, red for poor
- **Smooth Animations**: Hover effects and transitions
- **Professional Styling**: Clean, modern design that looks amazing

### 📊 **Comprehensive Metrics**
- **Success Rates**: Color-coded percentages with performance grades
- **Timing Analysis**: First token, first edit, and round trip times
- **Cost Tracking**: Per-result and total cost analysis
- **Token Metrics**: Context tokens and completion tokens
- **Edit Statistics**: Number of edits, lines added/deleted

## 🚀 Quick Start

1. **Install dependencies**:
   ```bash
   cd diff-edits/dashboard
   pip install -r requirements.txt
   ```

2. **Launch the dashboard**:
   ```bash
   streamlit run app.py
   ```
   
   Or use the convenient launch script:
   ```bash
   ./launch.sh
   ```

3. **Open your browser** to http://localhost:8501

## 🎯 Dashboard Sections

### **Hero Section**
- Beautiful gradient header with run information
- Key metrics overview (models tested, total results, success rate, cost)

### **Model Performance Cards**
- Each model displayed as a beautiful card
- Large success rate display with color coding
- Performance grade badges (A+, A, B+, B, C+, C)
- Key metrics: latency, cost, results count, first token time
- "Drill Down" button for detailed analysis

### **Performance Analytics**
- Interactive bar chart showing success rates
- Scatter plot of latency vs cost with bubble sizes
- Hover details and zoom capabilities

### **Detailed Analysis (Drill-Down)**
- Model-specific success rate, latency, and cost metrics
- Individual result selector with status icons
- Tabbed interface for different views:

#### 📄 **File & Edits Tab**
- **Side-by-side view**: Original file content with line numbers
- **Edit analysis**: Success/failure status with detailed metrics
- **Error display**: Clear error information for failed attempts
- **Success metrics**: Lines added/deleted, number of edits
- **Parsed tool calls**: JSON view of what the model attempted

#### 🤖 **Raw Output Tab**
- Complete raw model output in a code viewer
- Monospace font for easy reading

#### 🔧 **Parsed Tool Call Tab**
- Pretty-printed JSON of parsed tool calls
- Diff block visualization for replace_in_file calls
- Error handling for malformed JSON

#### 📊 **Metrics Tab**
- Detailed timing metrics (first token, first edit, round trip)
- Token and cost information
- Context size and completion tokens

## 🛠 **Technical Features**

### **Smart Data Loading**
- Automatic latest run detection
- Efficient SQL queries with proper JOINs
- Streamlit caching for performance
- Error handling for missing data

### **Interactive Navigation**
- Session state management for drill-down views
- Back button to return to overview
- Smooth transitions between views

### **Beautiful Styling**
- Custom CSS with Google Fonts (Inter)
- Gradient backgrounds and shadows
- Hover effects and animations
- Color-coded performance indicators
- Professional card-based layout

### **Responsive Design**
- Works on desktop, tablet, and mobile
- Flexible column layouts
- Scalable text and metrics

## 🎨 **Design Philosophy**

This dashboard follows modern design principles:
- **Clarity**: Information is easy to find and understand
- **Beauty**: Visually appealing with professional styling
- **Functionality**: Deep drill-down capabilities for detailed analysis
- **Performance**: Fast loading with efficient data queries
- **Usability**: Intuitive navigation and clear visual hierarchy

## 📊 **Data Visualization**

- **Plotly Charts**: Interactive, professional-looking visualizations
- **Color Coding**: Consistent color scheme for performance levels
- **Performance Badges**: A+ to C grading system
- **Status Icons**: ✅ for success, ❌ for failure
- **Metric Cards**: Clean, card-based metric display

## 🔧 **Customization**

The dashboard is highly customizable:
- **CSS Styling**: Easy to modify colors, fonts, and layouts
- **Performance Grades**: Adjustable thresholds for A/B/C grades
- **Metrics Display**: Add or remove metrics as needed
- **Chart Types**: Easily swap chart types or add new visualizations

## 🚀 **Future Enhancements**

Potential additions:
- **Historical Trends**: Compare performance across multiple runs
- **Export Functionality**: Download results as CSV/PDF
- **Real-time Updates**: Auto-refresh for ongoing evaluations
- **Custom Filters**: Filter by date range, model type, etc.
- **Comparison Mode**: Side-by-side model comparisons

---

**This is the sickest eval dashboard ever!** 🔥 It combines beautiful design with powerful analysis capabilities, making it easy to understand model performance at a glance while providing deep drill-down capabilities for detailed investigation.


--- CHANGELOG.md ---
# Changelog

## [3.62.0]

### Fixed
- Banners now display immediately when opening the extension instead of requiring user interaction first
- Resolved 17 security vulnerabilities including high-severity DoS issues in dependencies (body-parser, axios, qs, tar, and others)


## [3.61.0]

- UI/UX fixes with minimax model family

## [3.60.0]

- Fixes for Minimax model family

## [3.59.0]

- Added Minimax 2.5 Free Promo
- Fixed Response chaining for OpenAI's Responses API 

## [3.58.0]

### Added
- Subagent: replace legacy subagents with the native `use_subagents` tool
- Bundle `endpoints.json` support so packaged distributions can ship required endpoints out-of-the-box
- Amazon Bedrock: support parallel tool calling
- New "double-check completion" experimental feature to verify work before marking tasks complete
- CLI: new task controls/flags including custom `--thinking` token budget and `--max-consecutive-mistakes` for yolo runs
- Remote config: new UI/options (including connection/test buttons) and support for syncing deletion of remotely configured MCP servers
- Vertex / Claude Code: add 1M context model options for Claude Opus 4.6
- ZAI/GLM: add GLM-5

### Fixed
- CLI: handle stdin redirection correctly in CI/headless environments
- CLI: preserve OAuth callback paths during auth redirects
- VS Code Web: generate auth callback URLs via `vscode.env.asExternalUri` (OAuth callback reliability)
- Terminal: surface command exit codes in results and improve long-running `execute_command` timeout behavior
- UI: add loading indicator and fix `api_req_started` rendering
- Task streaming: prevent duplicate streamed text rows after completion
- API: preserve selected Vercel model when model metadata is missing
- Telemetry: route PostHog networking through proxy-aware shared fetch and ensure telemetry flushes on shutdown
- CI: increase Windows E2E test timeout to reduce flakiness

### Changed
- Settings/model UX: move "reasoning effort" into model configuration and expose it in settings
- CLI provider selection: limit provider list to those remotely configured
- UI: consolidate ViewHeader component/styling across views
- Tools: add auto-approval support for `attempt_completion` commands
- Remotely configured MCP server schema now supports custom headers

## [3.57.1]

### Fixed

- Fixed Opus 4.6 for bedrock provider

## [3.57.0]

### Added

- Cline CLI 2.0 now available. Install with `npm install -g cline`
- Anthopic Opus 4.6 
- Minimax-2.1 and Kimi-k2.5 now available for free for a limited time promo
- Codex-5.3 through ChatGPT subscription

### Fixed

- Fix read file tool to support reading large files
- Fix decimal input crash in OpenAI Compatible price fields (#8129)
- Fix build complete handlers when updating the api config
- Fixed missing provider from list
- Fixed Favorite Icon / Star from getting clipped in the task history view

### Changed

- Make skills always enabled and remove feature toggle setting

## [3.56.0]

### Added

- __CLI authentication:__ Added Vercel AI Gateway and Cline API key provider support for headless CI/automation workflows
- __New model:__ Added Kimi-K2.5 model to Moonshot provider (262K context, image support, prompt caching)
- __Prompt variant:__ Added Trinity Large prompt variant for improved tool-calling support
- __OpenTelemetry:__ Added support for custom headers on metrics and logs endpoints
- __Social links:__ Added community icons (X, Discord, GitHub, Reddit, LinkedIn) to the What's New modal

### Fixed

- __LiteLLM:__ Fixed thinking configuration not appearing for reasoning-capable models
- __OpenTelemetry:__ Fixed endpoint path handling (no longer incorrectly appends `/v1/logs` or `/v1/metrics`) and ensured logs are sent regardless of VSCode telemetry settings
- __CLI auth:__ Fixed `cline auth` displaying incorrect provider information after configuration

### Changed

- __Hooks:__ Hook scripts now run from the workspace repository root instead of filesystem root
- __Default settings:__ Enabled multi-root workspaces, parallel tool calling, and skills by default; disabled strict plan mode by default
- __Settings UI:__ Refreshed feature settings section with collapsible design

## [3.55.0]

- Add new model: Arcee Trinity Large Preview
- Add new model: Moonshot Kimi K2.5
- Add MCP prompts support - prompts from connected MCP servers now appear in slash command autocomplete as `/mcp:<server>:<prompt>`

## [3.54.0]

### Added

- Native tool calls support for Ollama provider
- Sonnet 4.5 is now the default Amazon Bedrock model id

### Fixed

- Prevent infinite retry loops when replace_in_file fails repeatedly. The system now detects repeated failures and provides better guidance to break out of retry cycles.
- Skip diff error UI handling during streaming to prevent flickering. Error handling is deferred until streaming completes.
- Strip notebook cell outputs when extracting text content from Jupyter notebooks, significantly reducing context size sent to the LLM.
- Throttle diff view updates during streaming to reduce UI flickering and improve performance.

### Changed

- Removed Mistral's Devstral-2512 free from the free models list
- Removed deprecated zai-glm-4.6 model from Cerebras provider

## [3.53.1]

### Fixed

- Bug in responses API

## [3.53.0]

### Fixed

- Removed grok model from free tier

## [3.52.0]

### Added

- Users with ChatGPT Plus or Pro subscriptions can now use GPT-5 models directly through Cline without needing an API key. Authentication is handled via OAuth through OpenAI's authentication system.
- Grok models are now moving out of free tier and into paid plans.
- Introduces comprehensive Jupyter Notebook support for Cline, enabling AI-assisted editing of `.ipynb` files with full cell-level context awareness.

### Fixed

- Bugs in DiffViewProvider for file editing
- Ollama's recommended models to use correct identifiers

## [3.51.0]

### Added

- Adding OpenAI gpt-5.2-codex model to the model picker

## [3.50.0]

### Added

- Add gpt-5.2-codex OpenAI model support
- Add create-pull-request skill

### Fixed

- Fix the selection of remotely configured providers
- Fix act_mode_respond to prevent consecutive calls
- Fix invalid tool call IDs when switching between model formats

## [3.49.1]

### Added

- Add telemetry to track usage of skills feature
- Add version headers to Cline backend requests
- Phase in Responses API usage instead of defaulting for every supported model

### Fixed

- Fix workflow slash command search to be case-insensitive
- Fix model display in ModelPickerModal when using LiteLLM
- Fix LiteLLM model fetching with default base URL
- Fix crash when OpenAI-compatible APIs send usage chunks with empty or null choices arrays at end of streaming
- Fix model ID for Kat Coder Pro Free model

## [3.49.0]

- Enable configuring an OTEL collector at runtime
- Removing Minimax-2.1 from free model list as the free trial has ended
- Improved image display in MCP responses
- Auto-sync remote MCP servers from remote config to local settings

## [3.48.0]

### Added

- Add Skills system for reusable, on-demand agent instructions
- Add new websearch tooling in Cline provider
- Add zai-glm-4.7 to Cerebras model list
- Add model refresh and improve reasoning support for Vercel AI Gateway

### Fixed

- Revert #8341 due to regressions in diff view/document truncation (see #8423, #8429)
- Fixed extension crash when using context menu selector

## [3.47.0]

### Added

- Added experimental support for Background Edits (allows editing files in background without opening the diff view)
- Updated free model to MiniMax M2.1 (replacing MiniMax M2)
- Added support for Azure based identity authentication in OpenAI Compatible provider and Azure OpenAI
- Add `supportsReasoning` property to Baseten models

### Fixed

- Prevent expired token usage in authenticated requests
- Exclude binary files without extensions from diffs
- Preserve file endings and trailing newlines
- Fix Cerebras rate limiting
- Fix Auto Compact for Claude Code provider
- Make Workspace and Favorites history filters independent
- Fix remote MCP server connection failures (404 response handling)
- Disable native tool calling for Deepseek 3.2 speciale
- Show notification instead of opening sidebar on update
- Fix Baseten model selector

### Refactored

- Modify prompts for parallel tool usage in Claude and Gemini 3 models

## [3.46.1]

### Fixed

- Remove GLM 4.6 from free models

## [3.46.0]

### Added

- Added GLM 4.7 model
- Enhanced background terminal execution with command tracking, log file output, zombie process prevention (10-minute timeout), and clickable log paths in UI
- Apply Patch tool for GPT-5+ models (replacing current diff edit tools)

### Fixed

- Duplicate error messages during streaming for Diff Edit tool when Parallel Tool Calling is not enabled
- Banner carousel styling and dismiss functionality
- Typos in Gemini system prompt overrides
- Model picker favorites ordering, star toggle, and keyboard navigation for OpenRouter and Vercel AI Gateway providers
- Fetch remote config values from the cache

### Refactored

- Anthropic handler to use metadata for reasoning support
- Bedrock provider to use metadata for reasoning support

## [3.45.1]

- Fixed MCP settings race condition where toggling auto-approve or changing timeout settings would cause the UI to flash and revert

## [3.45.0]

- Added Gemini 3 Flash Preview model

## [3.44.2]

- Polished the model picker UI with checkmarks for selected models, tooltips on Plan/Act tabs, and consistent arrow pointers across all popup modals
- Improved WhatsNew modal responsiveness and cleaned up redundant UI elements
- Fixed GLM models outputting garbled text in thinking tags—reasoning is now properly disabled for these models

## [3.44.1]

- Fixed a critical bug where local MCP servers stopped connecting after v3.42.0—all user-configured stdio-based MCP servers should now work again
- Fixed remotely configured API keys not being extracted correctly for enterprise users
- Added support for dynamic tool instructions that adapt based on runtime context, laying groundwork for future context-aware features

## [3.44.0]

## Added

- Updating minor version to show a proper banner for the release

## [3.43.1]

### Patch Changes

- Fix GLM-4.6 Model reference id

## [3.43.0]

### Added

- GLM-4.6
- kat-coder-pro
- Add parsing of env variable patterns to the mcpconfig.json

### Fixed

- TLS Proxy support issues for VSCode
- Add supportsReasoning flag to OpenAI reasoning models
- Fix thinking not available for some models in the OpenAI provider
- Fix invalid signature field issues when switching between Gemini and Anthropic providers
- Extract OpenRouter model filtering into reusable utility and use it in different model pickers
- Fix a11y for auto approve checkbox
- Improve ModelPickerModal provider list layout

### Refactored

- Migrate WhatsNewModal to new shared dialogue component

## [3.42.0]

### Added

- Expose `getAvailableSlashCommands` rpc endpoint to UI clients
- Made slash command menu and context menu accessible and screenreader-friendly
- Made expanding/collapsing UI components accessible

### Fixed

- Devstral OpenRouter model ID and routing issues
- Incorrect pricing display for Devstral model in the extension

## [3.41.0]

### Added

- OpenAI GPT-5.2
- Devstral-2512 (formerly stealth model "Microwave")
- Improvements to chat modal model picker
- Amazon Nova 2 Lite
- DeepSeek 3.2 to native tool calling allow list
- Responses API support for Codex models in OpenAI provider (requires native tool calling)
- Xmas Special Santa Cline
- Welcome screen UI enhancements

### Fixed

- Initial checkpoint commit now non-blocking for improved responsiveness in large repositories
- Gemini Vertex models erroring when thinking parameters are not supported
- Restrictive file permissions for secrets.json
- Ollama streaming requests not aborting when task is cancelled

### Refactored

- OpenAI provider to centralize temperature configuration and include missing GPT-5 model settings
- OpenAI native handler to use metadata for model capabilities
- Vertex provider to use metadata for model capabilities

## [3.40.2]

- Fix logout on network errors during token refresh (e.g., opening laptop while offline)

## [3.40.1]

- Fix cost calculation display for Anthropic API requests

## [3.40.0]

- Fix highlighted text flashing when task header is collapsed
- Add X-Cerebras-3rd-Party-Integration header to Cerebras API requests
- Add microwave family system prompt configuration
- Remove tooltips from auto approve menu
- Fix Standalone, ensure cwd is the install dir to find resources reliably
- Fix a bug where terminal commands with double quotes are broken when "Terminal Execution Mode" is set to "Background Exec"
- Add support for slash commands anywhere in a message, not just at the beginning. This matches the behavior of @ mentions for a more flexible input experience.
- Add bottom padding to the last message to fix last response text getting cut off by auto approve settings bar.
- Add default thinking level for Gemini 3 Pro models in Gemini provider

## [3.39.2]

- Fix for microwave model and thinking settings

## [3.39.1]

- Fix Openrouter and Cline Provider model info

## [3.39.0]

- Add Explain Changes feature
- Add microwave Stealth model
- Add Tabbed Model Picker with Recommended and Free tabs
- Add support to View remote rules and workflows in the editor
- Enable NTC (Native Tool Calling) by default
- Bug fixes and improvements for LiteLLM provider

## [3.38.3]

- Task export feature now opens the task directory, allowing easy access to the full task files
- Add Grok 4.1 and Grok Code to XAI provider
- Enabled native tool calling for Baseten and Kimi K2 models
- Add thinking level to Gemini 3.0 Pro preview
- Expanded Hooks functionality
- Removed Task Timeline from Task Header
- Bug fix for slash commands
- Bug fixes for Vertex provider
- Bug fixes for thinking/reasoning issues across multiple providers when using native tool calling
- Bug fixes for terminal usage on Windows devices

## [3.38.2]

- Add Claude Opus 4.5

## [3.38.1]

### Fixed

- Fixed handling of 'signature' field in sanitizeAnthropicContentBlock to properly preserve it when thinking is enabled, as required by Anthropic's API.

## [3.38.0]

### Added

- Gemini 3 Pro Preview model
- AquaVoice Avalon model for voice-to-text dictation

### Fixed

- Automatic context truncation when AWS Bedrock token usage rate limits are exceeded
- Removed new_task tool from system prompts, updated slash command prompts, and added helper function for native tool calling validation

## [3.37.1]

- Comprehensive changes to better support GPT 5.1 - System prompt, tools, deep-planning, focus chain, etc.
- Add AGENTS.md support
- feat(models): Add free minimax/mimax-m2 model to the model picker

## [3.37.0]

### Added

- GPT-5.1 with model-specific prompting: tailored system prompts, tool usage, focus chain, and deep-planning optimizations
- Nous Research provider with Hermes 4 model family and custom system prompts
- Switched to Aqua Voice's Avalon model in speech to text transcription
- Added Linux support for speech to text
- Model-family breakouts for deep-planning prompting, laying groundwork for enhanced slash commands
- Expanded HTTP proxy support throughout the codebase
- Improved focus chain prompting for frontier models (Anthropic, OpenAI, Gemini, xAI)

### Fixed

- Duplicate tool results prevention through existence checking
- XML entity escaping in model content processor
- Commit message generation in command palette
- OpenAI Compatible provider temperature parameter type conversion

## Documentation

- Added missing proto generation step in CONTRIBUTING.md
- New `npm run dev` script for streamlined terminal workflow (fixes #7335)

## [3.36.1]

- fix: remove native tool calling support from Gemini and XAI provider due to invalid tool names issues
- fix: disable native tool callings for grok code models
- Add MCP tool usage to GLM
- Removes reasoning_details content field from Anthropic providers

## [3.36.0]

- Add: Hooks allow you to inject custom logic into Cline's workflow
- Add: new provider AIhubmix
- Add: Use http_proxy, https_proxy and no_proxy in JetBrains
- Fix: Oca Token Refresh logic
- Fix: issues where assistant message with empty content is added to conversation history
- Fix: bug where the checkbox shows in the model selector dropdown
- Fix: Switch from defaultUserAgentProvider to customUserAgent for Bedrock
- Fix: support for `<think>` tags for better compatibility with open-source models
- Fix: refinements to the GLM-4.6 system prompt

## [3.35.1]

- Add: Hicap API integration as provider
- Fix: enable Add Header button in OpenAICompatibleProvider UI
- Fix: Remove orphaned tool_results after truncation and empty content field issues in native tool call
- Fix: render model description in markdown

## [3.35.0]

- Add native tool calling support with configurable setting.
- Auto-approve is now always-on with a redesigned expanding menu. Settings simplified and notifications moved to General Settings.
- added zai-glm-4.6 as a Cerebras model
- Created GPT5 family specific system prompt template
- Fix: show reasoning budget slider to models with valid thinking config
- Requesty base URL, and API key fixes
- Delete all Auth Tokens when logging out
- Support for <think> tags for models that prefer that over <thinking>

## [3.34.1]

- Added support for MiniMax provider with MiniMax-M2 model
- Remove Cline/code-supernova-1-million model
- Changes to allow users to manually enter model names (eg. presets) when using OpenRouter

## [3.34.0]

- Cline Teams is now free through 2025 for unlimited users. Includes Jetbrains, RBAC, centralized billing and more.
- Use the “exacto” versions of GLM-4.6, Kimi-K2, and Qwen3-Coder in the Cline provider for the best balance of cost, speed, accuracy and tool-calling.

## [3.33.1]

- Fix CLI installation copy text

## [3.33.0]

- Added Cline CLI (Preview)
- Added Subagent support (Experimental)
- Added Multi-Root Workspaces support (Enable in feature settings)
- Add auto-retry with exponential backof for failed API requests

## [3.32.8]

- Add Claude Haiku 4.5 support

## [3.32.7]

- Add JP and Global inference profile options to AWS Bedrock
- Adding Improvements to VSCode multi root workspaces
- Added markdown support to focus chain text, allowing the model to display more interesting focus chains

## [3.32.6]

- Add experimental support for VSCode multi root workspaces
- Add Claude Sonnet 4.5 to Claude Code provider
- Add Glm 4.6 to Z AI provider

## [3.32.5]

- Improve thinking budget slider UI to take up less space
- Fix Vercel provider cost note and sign-up url
- Fix repeated API error 400 in SAP AI Core provider
- Add us-west-1 to Amazon Bedrock regions
- Fix OCA provider refresh logic

## [3.32.4]

- Add 1m context window support to Claude Sonnet 4.5
- Add Claude Sonnet 4.5 to GCP Vertex
- Add prompt caching support for OpenRouter accidental `anthropic/claude-4.5-sonnet` model ID

## [3.32.3]

- Add Claude Sonnet 4.5 to Bedrock provider
- Add Alert banner for new Claude Sonnet 4.5 model

## [3.32.2]

- Add Claude Sonnet 4.5 to Cline/OpenRouter/Anthropic providers
- Add /task deep link handler

## [3.32.1]

- Preserve reasoning traces for Cline/OpenRouter/Anthropic providers to maintain conversation integrity
- Add automatically retry on rate limit errors with SAP AI Core provider
- Fix Cline accounts using stale id token at refresh response
- Minor UI improvements to Settings and Task Header

## [3.32.0]

- Added the new code-supernova-1-million stealth model, available for free and delivering a 1 million token context window
- Changes to inform Cline about commands that are available on your system

## [3.31.1]

- Version bump

## [3.31.0]

- UI Improvements: New task header and focus chain design to take up less space for a cleaner experience
- Voice Mode: Experimental feature that must be enabled in settings for hands-free coding
- YOLO Mode: Enable in settings to let Cline approve all actions and automatically switch between plan/act mode
- Fix Oracle Code Assist provider issues

## [3.30.3]

- Add Oracle Code Assist provider

## [3.30.2]

- Fix UI tests

## [3.30.1]

- Fix model list not being updated in time for user to use shortcut button to update model to stealth model
- Fix flicker issue when switching modes
- Fix Sticky header in settings view overlaping with content on scroll
- Add experimental yolo mode feature that disables all user approvals and automatically executes a task and navigates through plan to act mode until the task is complete

## [3.30.0]

- Add code-supernova stealth model

## [3.29.2]

- Fix: Reverted change that caused formatting issues
- Fix: Moonshot - Pass max_tokens value to provider

## [3.29.1]

- Changeset bump + Announcement banner update

## [3.29.0]

- Updated Baseten provider to fetch models from server
- Fix: Updated insufficient balance URL for easy Cline balance top-ups
- Accessibility: Improvements to screen readers in MCP, Cline Rules, workflows, and history views.

## [3.28.4]

- Fix bug where some Windows machines had API request hanging
- Fix bug where 'Proceed while running' action button would be disabled after running an interactive command
- Fix prompt cache info not being displayed in History

## [3.28.3]

- Fixed issue with start new task button
- Feature to generate commit message for staged changes, with unstaged as fallback

## [3.28.2]

- Fix for focus chain settings

## [3.28.1]

- Requesty: use base URL to get models and API keys
- Removed focus chain feature flag

## [3.28.0]

- Synchronized Task History: Real-time task history synchronization across all Cline instances
- Optimized GPT-5 Integration: Fine-tuned system prompts for improved performance with GPT-5 model family
- Deep Planning Improvements: Optimized prompts for Windows/PowerShell environments and dependency exclusion
- Streamlined UI Experience: ESC key navigation, cleaner approve/reject buttons, and improved editor panel focus
- Smart Provider Search: Improved search functionality in API provider dropdown for faster model selection
- Added per-provider thinking tokens configurability
- Added Ollama custom prompt options
- Enhanced SAP AI Core Provider: Orchestration mode support and improved model visibility
- Added Dify.ai API Integration
- SambaNova Updates: Added DeepSeek-V3.1 model
- Better Gemini rate limit handling
- OpenAI Reasoning Effort: Minimal reasoning effort configuration for OpenAI models
- Fixed LiteLLM Caching: Anthropic caching compatibility when using LiteLLM
- Fixed Ollama default endpoint connections
- Fixed AutoApprove menu overflow
- Fixed extended thinking token issue with Anthropic models
- Fixed issue with slash commands removing text from prompt

## [3.27.2]

- Remove `grok-code-fast-1` promotion deadline

## [3.27.1]

- Add new Kimi K2 model to groq and moonshot providers

## [3.27.0]

- Fix `grok-code-fast-1` model information
- Add call to action for trying free `grok-code-fast-1` in Announcement banner

## [3.26.7]

- Add 200k context window variant for Claude Sonnet 4 to OpenRouter and Cline providers

## [3.26.6]

- Add free Grok Coder model to Cline provider for users looking for a fast, free coding model option
- Fix GPT-5 models not respecting auto-compact setting when enabled, improving context window management
- Fix provider retry attempts not showing proper user feedback during rate limiting scenarios
- Improve markdown and code block styling to automatically adapt when switching VS Code themes

## [3.26.5]

- fix (provider/vercel-ai-gateway): reduce model list load frequency in settings view
- Fix OVSX publish command to resolve deployment failure

## [3.26.4]

- Update nebius ai studio models
- Update sap provider - support reasoning effort for open ai models
- Fix Claude 4 image input in SAP AI Core Provider

## [3.26.3]

- Add compact system prompt option for LM Studio and Ollama models, optimized for smaller context windows (8k or less)
- Add token usage tracking for LM Studio models to better monitor API consumption
- Add "Use compact prompt" checkbox in LM Studio provider settings
- Fix "Unexpected API Response" bug with gpt-5

## [3.26.2]

- Improve OpenRouter model parsing to show reasoning budget sliders for all models that support thinking, not just Claude models
- Fix OpenRouter context window error handling to properly extract error codes from error messages, resolving "Unexpected API Response" errors with GPT-5 on Cline provider
- Fix GPT-5 context window configuration for OpenAI/OpenRouter/Cline providers to use correct 272K limit
- Remove max tokens configuration from Sonic Alpha model
- Add Go language support to deep-planning feature (Thanks @yuvalman!)
- Fix typo in Focus Chain settings page (Thanks @joyceerhl!)

## [3.26.1]

- Add Vercel AI Gateway as a new API provider option (Thanks @joshualipman123!)
- Improve SAP AI Core provider to show deployed and undeployed models in the UI (Thanks @yuvalman!)
- Fix Fireworks provider configuration and functionality (Thanks @ershang-fireworks!)
- Add telemetry tracking for MCP tool usage to help improve the extension
- Improve telemetry tracking for rules and workflow usage analytics
- Set Plan mode to use strict mode by default for better planning results

## [3.26.0]

- Add Z AI as a new API provider with GLM-4.5 and GLM-4.5 Air models, offering competitive performance with cost-effective pricing especially for Chinese language tasks (Thanks @jues!)
- Add Cline Sonic Alpha model - experimental advanced model with 262K context window for complex coding tasks
- Add support for LM Studio local models from v0 API endpoint with configurable max tokens
- Fix Ollama context window configuration not being used in requests

## [3.25.3]

- Fix bug where 'Enable checkpoints' and 'Disable MCP Marketplace' settings would be reset to default on reload
- Move the position of the focus chain edit button when a scrollbar is present. Make the pencil icon bigger and better centered.

## [3.25.2]

- Fix attempt_completion showing twice in chat due to partial logic not being handled correctly
- Fix OpenRouter showing cline credits error after 402 response

## [3.25.1]

- Fix attempt_completion command showing twice in chat view when updating progress checklist
- Fix bug where announcement banner could not be dismissed
- Add GPT-OSS models to AWS Bedrock

## [3.25.0]

- **Focus Chain:** Automatically creates and maintains todo lists as you work with Cline, breaking down complex tasks into manageable steps with real-time progress tracking
- **Auto Compact:** Intelligently manages conversation context to prevent token limit errors by automatically compacting older messages while preserving important context
- **Deep Planning:** New `/deep-planning` slash command for structured 4-step implementation planning that integrates with Focus Chain for automatic progress tracking
- Add support for 200k context window for Claude Sonnet 4 in OpenRouter and Cline providers
- Add option to configure custom base URL for Requesty provider

## [3.24.0]

- Add OpenAI GPT-5 Chat(gpt-5-chat-latest)
- Add custom browser arguments setting to allow passing flags to the Chrome executable for better headless compatibility.
- Add 1m context window model support for claude sonnet 4
- Fis the API Keys URL for Requesty
- Set gpt5 max tokens to 8_192 to fix 'context window exceeded' error
- Fix issue where fallback request to retrieve cost was not using correct auth token
- Add OpenAI context window exceeded error handling
- Calibrate input token counts when using anthropic models of sap ai core provider

## [3.23.0]

- Add caching support for Bedrock inferences using SAP AI Core and minor refactor
- Improve visibility for mode switch background color on different themes
- Fix terminal commands putting webview in blocked state

## [3.22.0]

- Implemented a retry strategy for Cerebras to handle rate limit issues due to its generation speed
- Add support for GPT-5 models to SAP AI Core Provider
- Support sending context to active webview when editor panels are opened.
- Fix bug where running out of credits on Cline accounts would show '402 empty body' response instead of 'buy credits' component
- Fix LiteLLM Proxy Provider Cost Tracking

## [3.21.0]

- Add support for GPT-5 model family including GPT-5, GPT-5 Mini, and GPT-5 Nano with prompt caching support and set GPT-5 as the new default model
- Add "Take a Tour" button for new users to easily access the VSCode walkthrough and improve onboarding experience
- Enhance plan mode response handling with better exploration parameter support

## [3.20.13]

- Fix prompt caching support for Opus 4.1 on OpenRouter/Cline

## [3.20.12]

- Add Claude Opus 4.1 model support to AWS Bedrock provider (Thanks @omercelik!)
- Fix prompt caching and extended thinking support for Claude Opus 4.1 in Anthropic provider

## [3.20.11]

Add gpt-oss-120b as a Cerebras model
Add Opus 4.1 through Claude Code

## [3.20.10]

- Add OpenAI's new open-source models (GPT-OSS-120B and GPT-OSS-20B) to Hugging Face and Groq providers

## [3.20.9]

- Add support for Claude Opus 4.1 model in Anthropic provider
- Add Baseten as a new API provider with support for DeepSeek, Llama, and Kimi K2 models (Thanks @AlexKer!)
- Fix error messages not clearing from UI when retrying failed tasks
- Fix chat input box positioning issues

## [3.20.8]

- Add navbar tooltips on hover

## [3.20.7]

- Fix circular dependency that affect the github workflow Tests / test (pull_request)

## [3.20.6]

- Fix login check on extension restart

## [3.20.5]

- Fix authentication persistence issues that could cause users to be logged out unexpectedly

## [3.20.4]

- Add new Cerebras models
- Update rate limits for existing Cerebras models
- Fix for delete task dialog

## [3.20.3]

- Add Huawei Cloud MaaS Provider (Thanks @ddling!)
- Add Cerebras Qwen 3 235B instruct model (Thanks @kevint-cerebras!)
- Add DeepSeek R1 0528 support under Hugging Face (Thanks @0ne0rZer0!)
- Fix Global Rules directory documentation for Linux/WSL systems
- Fix token counting when using VSCode LM API provider
- Fix input field stealing focus issue by only focusing on visible and active editor panels
- Fix duplicate tool registration for claude4-experimental
- Trim input value for URL fields

## [3.20.2]

- Fixed issue with sap ai core client credentials storage
- Fix Qwen Api option inconsistency between UI and API layer
- Fix credit balance out of sync issue on account switching
- Fix Claude Code CLAUDE_CODE_MAX_OUTPUT_TOKENS
- Fix cursor state after restoring files to be disabled after checked out
- Fix issue where checkpointing blocked UI

## [3.20.1]

- Fix for files being deleted when switching modes or closing tasks

## [3.20.0]

- Add account balance display for all organization members, allowing non-admin users to view their organization's credit balance and add credits

## [3.19.8]

- Add Claude Code support on Windows with improved system prompt handling to fix E2BIG errors (Thanks @BarreiroT!)
- Improve Cerebras provider with updated model selection (Qwen and Llama 3.3 70B only) and increased context window for Qwen 3 32B from 16K to 64K tokens
- Improve Cerebras Qwen model performance by removing thinking tokens from model input
- Add robust checkpoint timeout handling with early warning at 7 seconds and timeout at 15 seconds to prevent hanging on large repositories
- Fix MCP servers incorrectly starting when disabled in configuration (Thanks @mohanraj-r!)
- Refactor Git commit message generation with streaming support and improved module organization
- Fix settings navigation to open correct tab when accessing from checkpoint warnings

## [3.19.7]

- Add Hugging Face as a new API provider with support for their inference API models
- Improve Claude Code error messages with better guidance for common setup issues (Thanks @BarreiroT!)
- Fix authentication sync issues when using multiple VSCode windows

## [3.19.6]

- Improve Kimi K2 model provider routing with additional provider options for better availability and performance
- Fixed terminal bug where Cline failed to capture output of certain fast-running commands
- Fixed bug with increasing auto approved number of requests not resetting the counter mid-task

## [3.19.5]

- Add Groq as a new API provider with support for all Groq models including Kimi-K2
- Add user role display in organization UI for Cline account users
- Fix message dialogs not showing option buttons properly
- Fix authentication issues when using multiple VSCode windows

## [3.19.4]

- Add ability to choose Chinese endpoint for Moonshot provider

## [3.19.3]

- Add Moonshot AI provider

## [3.19.2]

- Show request ID in error messages returned by Cline Accounts API to help debug user reported issues

## [3.19.1]

- Fix documentation

## [3.19.0]

- Add Kimi-K2 as a recommended model in the Cline Provider, and route to Together/Groq for 131k context window and high throughput
- Added API Key support for Bedrock integration

## [3.18.14]

- Fix bug where Cline account users logged in with invalid token would not be shown as logged out in webview presentation layer

## [3.18.13]

- Fix authentication issue where Cline accounts users would keep getting logged out or seeing 'Unexpected API response' errors

## [3.18.12]

- Fix flaky organization switching behavior in Cline provider that caused UI inconsistencies and double loading
- Fix insufficient credits error display to properly show error messages when account balance is too low
- Improve credit balance validation and error handling for Cline provider requests

## [3.18.11]

- Fix authentication issues with Cline provider by ensuring the client always uses the latest auth token

## [3.18.10]

- Update recommended fast & cheap model to Grok 4 in OpenRouter model picker
- Fix Gemini 2.5 Pro thinking budget slider and add support for Gemini 2.5 Flash Lite Preview model (Thanks @arafatkatze!)

## [3.18.9]

- Fix streaming reliability issues with Cline provider that could cause connection problems during long conversations
- Fix authentication error handling for Cline provider to show clearer error messages when not signed in and prevent recursive failed requests
- Remove incorrect pricing display for SAP AI Core provider since it uses non-USD "Capacity Units" that cannot be directly converted (Thanks @ncryptedV1!)

## [3.18.8]

- Update pricing for Grok 3 model because the promotion ended

## [3.18.7]

- Remove promotional "free" messaging for Grok 3 model in UI

## [3.18.6]

- Update request header to include `"ai-client-type": "Cline"` to SAP Api Provider
- Add organization accounts

## [3.18.5]

- Fix Plan/Act mode persistence across sessions and multi-workspace conflicts
- Improve provider switching performance by 18x (from 550ms to 30ms) with batched storage operations
- Improve SAP AI Core provider model organization and fix exception handling (Thanks @schardosin!)

## [3.18.4]

- Add support for Gemini 2.5 Pro and Flash to SAP AI Core Provider
- Fix logging in with Cline account not getting past welcome screen

## [3.18.3]

- Improve Cerebras Qwen model performance by removing thinking tokens from model input (Thanks @kevint-cerebras!)
- Improve Claude Code provider with better error handling and performance optimizations (Thanks @BarreiroT!)

## [3.18.2]

- Fix issue where terminal output would not be captured if shell integration fails by falling back to capturing the terminal content.
- Add confirmation popup when deleting tasks
- Add support for Claude Sonnet 4 and Opus 4 model in SAP AI Core provider (Thanks @lizzzcai!)
- Add support for `litellm_session_id` to group requests in a single session (Thanks @jorgegarciarey!)
- Add "Thinking Budget" customization for Claude Code (Thanks @BarreiroT!)
- Fix issue where the extension would use the user's environment variables for authentication when using Claude Code (Thanks @BarreiroT!)

## [3.18.1]

- Add support for Claude 4 Sonnet in SAP AI Core provider (Thanks @GTxx!)
- Fix ENAMETOOLONG error when using Claude Code provider with long conversation histories (Thanks @BarreiroT!)
- Remove Gemini CLI provider because Google asked us to
- Fix bug with "Delete All Tasks" functionality

## [3.18.0]

- Optimized Cline to work with the Claude 4 family of models, resulting in improved performance, reliability, and new capabilities
- Added a new Gemini CLI provider that allows you to use your local Gemini CLI authentication to access Gemini models for free (Thanks @google-gemini!)
- Optimized Cline to work with the Gemini 2.5 family of models
- Updated the default and recommended model to Claude 4 Sonnet for the best performance
- Fix race condition in Plan/Act mode switching
- Improve robustness of search and replace parsing

## [3.17.16]

- Fix Claude Code provider error handling for incomplete messages during long-running tasks (Thanks @BarreiroT!)
- Add taskId as metadata to LiteLLM API requests for better request tracing (Thanks @jorgegarciarey!)

## [3.17.15]

- Fix LiteLLM provider to properly respect selected model IDs when switching between Plan and Act modes (Thanks @sammcj!)
- Fix chat input being cleared when switching between Plan/Act modes without sending a message (Thanks @BarreiroT!)
- Fix MCP server name display to avoid showing "undefined" for SSE servers, preventing tool/resource invocation failures (Thanks @ramybenaroya!)
- Fix AWS Bedrock provider by removing deprecated custom model encoding (Thanks @watany-dev!)
- Fix timeline tooltips for followup messages and improve color retrieval code (Thanks @char8x!)
- Improve accessibility by making task header buttons properly announced by screen readers (Thanks @yncat!)
- Improve accessibility by adding proper state reporting for Plan/Act mode switch for screen readers (Thanks @yncat!)
- Prevent reading development environment variables from user's environment (Thanks @BarreiroT!)

## [3.17.14]

- Add Claude Code as a new API provider, allowing integration with Anthropic's Claude Code CLI tool and Claude Max Plan (Thanks @BarreiroT!)
- Add SAP AI Core as a new API provider with support for Claude and GPT models (Thanks @schardosin!)
- Add configurable default terminal profile setting, allowing users to specify which terminal Cline should use (Thanks @valinha!)
- Add terminal output size constraint setting to limit how much terminal output is processed
- Add MCP Rich Display settings to the settings page for persistent configuration (Thanks @Vl4diC0de!)
- Improve copy button functionality with refactored reusable components (Thanks @shouhanzen!)
- Improve AWS Bedrock provider by removing deprecated dependency and using standard AWS SDK (Thanks @watany-dev!)
- Fix list_files tool to properly return files when targeting hidden directories
- Fix search and replace edge case that could cause file deletion, making the algorithm more lenient for models using different diff formats
- Fix task restoration issues that could occur when resuming interrupted tasks
- Fix checkpoint saving to properly track all file changes
- Improve file context warnings to reduce diff edit errors when resuming restored tasks
- Clear chat input when switching between Plan/Act modes within a task
- Exclude .clinerules files from checkpoint tracking

## [3.17.13]

- Add Thinking UX for Gemini models, providing visual feedback during model reasoning
- Add support for Notifications MCP integration with Cline
- Add prompt caching indicator for Grok 3 models
- Sort MCP marketplace by newest listings by default for easier discovery of recent servers
- Update O3 model family pricing to reflect latest OpenAI rates
- Remove '-beta' suffix from Grok model identifiers
- Fix AWS Bedrock provider by removing deprecated Anthropic-Bedrock SDK (Thanks @watany-dev!)
- Fix menu display issue for terminal timeout settings
- Improve chat input field styling and behavior

## [3.17.12]

- **Free Grok Model Available!** Access Grok 3 completely free through the Cline provider
- Add collapsible MCP response panels to keep conversations focused on the main AI responses while still allowing access to detailed MCP output (Thanks @valinha!)
- Prioritize active files (open tabs) at the top of the file context menu when using @ mentions (Thanks @abeatrix!)
- Fix context menu to properly default to "File" option instead of incorrectly selecting "Git Commits"
- Fix diff editing to handle out-of-order SEARCH/REPLACE blocks, improving reliability with models that don't follow strict ordering
- Fix telemetry warning popup appearing repeatedly for users who have telemetry disabled

## [3.17.11]

- Add support for Gemini 2.5 Pro Preview 06-05 model to Vertex AI and Google Gemini providers

## [3.17.10]

- Add support for Qwen 3 series models with thinking mode options (Thanks @Jonny-china!)
- Add new AskSage models: Claude 4 Sonnet, Claude 4 Opus, GPT 4.1, Gemini 2.5 Pro (Thanks @swhite24!)
- Add VSCode walkthrough to help new users get started with Cline
- Add support for streamable MCP servers
- Improve Ollama model selection with filterable dropdown instead of radio buttons (Thanks @paulgear!)
- Add setting to disable aggressive terminal reuse to help users experiencing task lockout issues
- Fix settings dialog applying changes even when cancel button is clicked

## [3.17.9]

- Aligning Cline to work with Claude 4 model family (Experimental)
- Add task timeline scrolling feature
- Add support for uploading CSV and XLSX files for data analysis and processing
- Add stable Grok-3 models to xAI provider (grok-3, grok-3-fast, grok-3-mini, grok-3-mini-fast) and update default model from grok-3-beta to grok-3 (Thanks @PeterDaveHello!)
- Add new models to Vertex AI provider
- Add new model to Nebius AI Studio
- Remove hard-coded temperature from LM Studio API requests and add support for reasoning_content in LM Studio responses
- Display delay information when retrying API calls for better user feedback
- Fix AWS Bedrock credential caching issue where externally updated credentials (e.g., by AWS Identity Manager) were not detected, requiring extension restart (Thanks @DaveFres!)
- Fix search tool overloading conversation with massive outputs by setting maximum byte limit for responses
- Fix checkpoints functionality
- Fix token counting for xAI provider
- Fix Ollama provider issues
- Fix window title display for Windows users
- Improve chat box UI

## [3.17.8]

- Fix bug where terminal would get stuck and output "capture failure"

## [3.17.7]

- Fix diff editing reliability for Claude 4 family models by adding constraints to prevent errors with large replacements

## [3.17.6]

- Add Cerebras as a new API provider with 5 high-performance models including reasoning-capable models (Thanks @kevint-cerebras!)
- Add support for uploading various file types (XML, JSON, TXT, LOG, MD, DOCX, IPYNB, PDF) alongside images
- Add improved onboarding experience for new users with guided setup
- Add prompt cache indicator for Gemini 2.5 Flash models
- Update SambaNova provider with new model list and documentation links (Thanks @luisfucros!)
- Fix diff editing support for Claude 4 family of models
- Improve telemetry and analytics for better user experience insights

## [3.17.5]

- Fix issue with Claude 4 models where after several conversation turns, it would start making invalid diff edits

## [3.17.4]

- Fix thinking budget slider for Claude 4

## [3.17.3]

- Fix diff edit errors with Claude 4 models

## [3.17.2]

- Add support for Claude 4 models (Sonnet 4 and Opus 4) in AWS Bedrock and Vertex AI providers
- Add support for global workflows, allowing workflows to be shared across workspaces with local workflows taking precedence
- Fix settings page z-index UI issues that caused display problems
- Fix AWS Bedrock environment variable handling to properly restore process.env after API calls (Thanks @DaveFres!)

## [3.17.1]

- Add prompt caching for Claude 4 models on Cline and OpenRouter providers
- Increase max tokens for Claude Opus 4 from 4096 to 8192

## [3.17.0]

- Add support for Anthropic Claude Sonnet 4 and Claude Opus 4 in both Anthropic and Vertex providers
- Add integration with Nebius AI Studio as a new provider (Thanks @Aktsvigun!)
- Add custom highlight and hotkey suggestion when the assistant prompts to switch to Act mode
- Update settings page design, now split into tabs for easier navigation (Thanks Yellow Bat @dlab-anton, and Roo Team!)
- Fix MCP Server configuration bug
- Fix model listing for Requesty provider
- Move all advanced settings to settings page

## [3.16.3]

- Add devstral-small-2505 to the Mistral model list, a new specialized coding model from Mistral AI (Thanks @BarreiroT!)
- Add documentation links to rules & workflows UI
- Add support for Streameable HTTP Transport for MCPs (Thanks @alejandropta!)
- Improve error handling for Mistral SDK API

## [3.16.2]

- Add support for Gemini 2.5 Flash Preview 05-20 model to Vertex AI provider with massive 1M token context window (Thanks @omercelik!)
- Add keyboard shortcut (Cmd+') to quickly focus Cline from anywhere in VS Code
- Add lightbulb actions for selected text with options to "Add to Cline", "Explain with Cline", and "Improve with Cline"
- Automatically focus Cline window after extension updates

## [3.16.1]

- Add Enable auto approve toggle switch, allowing users to easily turn auto-approve functionality on or off without losing their action settings
- Improve Gemini retry handling with better UI feedback, showing retry progress during API request attempts
- Fix memory leak issue that could occur during long sessions with multiple tasks
- Improve UI for Gemini model retry attempts with clearer status updates
- Fix quick actions functionality in auto-approve settings
- Update UI styling for auto-approve menu items to conserve space

## [3.16.0]

- Add new workflow feature allowing users to create and manage workflow files that can be injected into conversations via slash commands
- Add collapsible recent task list, allowing users to hide their task history when sharing their screen (Thanks @cosmix!)
- Add global endpoint option for Vertex AI users, providing higher availability and reducing 429 errors (Thanks @soniqua!)
- Add detection for new users to display special components and guidance
- Add Tailwind CSS IntelliSense to the recommended extensions list
- Fix eternal loading states when the last message is a checkpoint (Thanks @BarreiroT!)
- Improve settings organization by migrating VSCode Advanced settings to Settings Webview

## [3.15.5]

- Fix inefficient memory management in the task timeline
- Fix Gemini rate limitation response not being handled properly (Thanks @BarreiroT!)

## [3.15.4]

- Add gemini model back to vertex provider
- Add gemini telemetry
- Add filtering for tasks tied to the current workspace

## [3.15.3]

- Add Fireworks API Provider
- Fix minor visual issues with auto-approve menu
- Fix one instance of terminal not getting output
- Fix 'Chrome was launched but debug port is not responding' error

## [3.15.2]

- Added details to auto approve menu and more sensible default controls
- Add detailed configuration options for LiteLLM provider
- Add webview telemetry for users who have opted in to telemetry
- Update Gemini in OpenRouter/Cline providers to use implicit caching
- Fix freezing issues during rendering of large streaming text
- Fix grey screen webview crashes by releasing memory after every diff edit
- Fix breaking out of diff auto-scroll
- Fix IME composition Enter auto‑sending edited message

## [3.15.1]

- Fix bug where PowerShell commands weren't given enough time before giving up and showing an error

## [3.15.0]

- Add Task Timeline visualization to tasks (Thanks eomcaleb!)
- Add cache to ui for OpenAi provider
- Add FeatureFlagProvider service for the Node.js extension side
- Add copy buttons to task header and assistant messages
- Add a more simplified home header was added
- Add ability to favorite a task, allowing it to be kept when clearing all tasks
- Add npm script for issue creation (Thanks DaveFres!)
- Add confirmation dialog to Delete All History button
- Add ability to allow the user to type their next message into the chat while Cline is taking action
- Add ability to generate commit message via cline (Thanks zapp88!)
- Add improvements to caching for gemini models on OpenRouter and Cline providers
- Add improvements to allow scrolling the file being edited.
- Add ui for windsurf and cursor rules
- Add mistral medium-3 model
- Add option to collect events to send them in a bundle to avoid sending too many events
- Add support to quote a previous message in chat
- Add support for Gemini Implicit Caching
- Add support for batch selection and deletion of tasks in history (Thanks danix800!)
- Update change suggested models
- Update fetch cache details from generation endpoint
- Update converted docs to Mintlify
- Update the isOminiModel to include o4-mini model (Thanks PeterDaveHello!)
- Update file size that can be read by Cline, allowing larger files
- Update defaults for bedrock API models (Thanks Watany!)
- Update to extend ReasoningEffort to non-o3-mini reasoning models for all providers (Thanks PeterDaveHello!)
- Update to give error when a user tries to upload an image larger than 7500x7500 pixels
- Update announcement so that previous updates are in a dropdown
- Update UI for auto approve with favorited settings
- Fix bug where certain terminal commands would lock you out of a task
- Fix only initialize posthog in the webview if the user has opted into telemetry
- Fix bug where autocapture was on for front-end telemetry
- Fix for markdown copy excessively escaping characters (Thanks weshoke!)
- Fix an issue where loading never finished when using an application inference profile for the model ID (Thanks WinterYukky!)

## [3.14.1]

- Disables autocaptures when initializing feature flags

## [3.14.0]

- Add support for custom model ID in AWS Bedrock provider, enabling use of Application Inference Profile (Thanks @clicube!)
- Add more robust caching & cache tracking for gemini & vertex providers
- Add support for LaTeX rendering
- Add support for custom API request timeout. Timeouts were 15-30s, but can now be configured via settings for OpenRouter/Cline & Ollama (Thanks @WingsDrafterwork!)
- Add truncation notice when truncating manually
- Add a timeout setting for the terminal connection, allowing users to set a time to wait for terminal startup
- Add copy button to code blocks
- Add copy button to markdown blocks (Thanks @weshoke!)
- Add checkpoints to more messages
- Add slash command to create a new rules file (/newrule)
- Add cache ui for open router and cline provider
- Add Amazon Nova Premier model to Bedrock (Thanks @watany!)
- Add support for cursorrules and windsurfrules
- Add support for batch history deletion (Thanks @danix800!)
- Improve Drag & Drop experience
- Create clinerules folder when creating new rule if it's needed
- Enable pricing calculation for gemini and vertex providers
- Refactor message handling to not show the MCP View of the server modal
- Migrate the addRemoteServer to protobus (Thanks @DaveFres!)
- Update task header to be expanded by default
- Update Gemini cache TTL time to 15 minutes
- Fix race condition in terminal command usage
- Fix to correctly handle `import.meta.url`, avoiding leading slash in pathname for Windows (Thanks @DaveFres!)
- Fix @withRetry() decoration syntax error when running extension locally (Thanks @DaveFres!)
- Fix for git commit mentions in repos with no git commits
- Fix cost calculation (Thanks @BarreiroT!)

## [3.13.3]

- Add download counts to MCP marketplace items
- Add `/compact` command
- Add prompt caching to gemini models in cline / openrouter providers
- Add tooltips to bottom row menu

## [3.13.2]

- Add Gemini 2.5 Flash model to Vertex and Gemini Providers (Thanks monotykamary!)
- Add Caching to gemini provider (Thanks arafatkatze!)
- Add thinking budget support to Gemini Models (Thanks monotykamary!)
- Add !include .file directive support for .clineignore (Thanks watany-dev!)
- Improve slash command functionality
- Improve prompting for new task tool
- Fix o1 temperature being passed to the azure api (Thanks treeleaves30760!)
- Fix to make "add new rule file" button functional
- Fix Ollama provider timeout, allowing for a larger loading time (Thanks suvarchal!)
- Fix Non-UTF-8 File Handling: Improve Encoding Detection to Prevent Garbled Text and Binary Misclassification (Thanks yt3trees!)
- Fix settings to not reset by changing providers
- Fix terminal outputs missing commas
- Fix terminal errors caused by starting non-alphanumeric outputs
- Fix auto approve settings becoming unset
- Fix Mermaid syntax error in documentation (Thanks tuki0918!)
- Remove supportsComputerUse restriction and support browser use through any model that supports images (Thanks arafatkatze!)

## [3.13.1]

- Fix bug where task cancellation during thinking stream would result in error state

## [3.13.0]

- Add Cline rules popover under the chat field, allowing you to easily add, enable & disable workspace level or global rule files
- Add new slash command menu letting you type “/“ to do quick actions like creating new tasks
- Add ability to edit past messages, with options to restore your workspace back to that point
- Allow sending a message when selecting an option provided by the question or plan tool
- Add command to jump to Cline's chat input
- Add support for OpenAI o3 & 4o-mini (Thanks @PeterDaveHello and @arafatkatze!)
- Add baseURL option for Google Gemini provider (Thanks @owengo and @olivierhub!)
- Add support for Azure's DeepSeek model. (Thanks @yt3trees!)
- Add ability for models that support it to receive image responses from MCP servers (Thanks @rikaaa0928!)
- Improve search and replace diff editing by making it more flexible with models that fail to follow structured output instructions. (Thanks @chi-cat!)
- Add detection of Ctrl+C termination in terminal, improving output reading issues
- Fix issue where some commands with large output would cause UI to freeze
- Fix token usage tracking issues with vertex provider (Thanks @mzsima!)
- Fix issue with xAI reasoning content not being parsed (Thanks @mrubens!)

## [3.12.3]

- Add copy button to MermaidBlock component (Thanks @cacosub7!)
- Add the ability to fetch from global cline rules files
- Add icon to indicate when a file outside of the users workspace is edited

## [3.12.2]

- Add gpt-4.1

## [3.12.1]

- Use visual checkpoint indicator to make it clear when checkpoints are created
- Big shoutout to @samuel871211 for numerous code quality improvements, refactoring contributions, and webview performance improvements!
- Use improved context manager

## [3.12.0]

- Add favorite toggles for models when using the Cline & OpenRouter providers
- Add auto-approve options for edits/reads outside of the workspace
- Improve diff editing animation for large files
- Add indicator showing number of diff edits when Cline edits a file
- Add streaming support and reasoning effort option to xAI's Grok 3 Mini
- Add settings button to MCP popover to easily modify installed servers
- Fix bug where browser tool actions would show unparsed results in the chat view
- Fix issue with new checkpoints popover hiding too quickly
- Fix duplicate checkpoints bug
- Improve Ollama provider with retry mechanism, timeout handling, and improved error handling (thanks suvarchal!)

## [3.11.0]

- Redesign checkpoint UI to declutter chat view by using a subtle indicator line that expands to a popover on hover, with a new date indicator for when it was created
- Add support for xAI's provider's Grok 3 models
- Add more robust error tracking for users opted in to telemetry (thank you for helping us make Cline better!)

## [3.10.1]

- Add CMD+' keyboard shortcut to add selected text to Cline
- Cline now auto focuses the text field when using 'Add to Cline' shortcut
- Add new 'Create New Task' tool to let Cline start a new task autonomously!
- Fix Mermaid diagram issues
- Fix Gemini provider cost calculation to take new tiered pricing structure into account

## [3.10.0]

- Add setting to let browser tool use local Chrome via remote debugging, enabling session-based browsing. Replaces sessionless Chromium, unlocking debugging and productivity workflows tied to your real browser state.
- Add new auto-approve option to approve _ALL_ commands (use at your own risk!)
- Add modal in the chat area to more easily enable or disable MCP servers
- Add drag and drop of file/folders into cline chat (Thanks eljapi!)
- Add prompt caching for LiteLLM + Claude (Thanks sammcj!)
- Add Improved context management
- Fix MCP auto approve toggle issues being out of sync with settings

## [3.9.2]

- Add recommended models for Cline provider
- Add ability to detect when user edits files manually so Cline knows to re-read, leading to reduced diff edit errors
- Add improvements to file mention searching for faster searching
- Add scoring logic to file mentions to sort and exclude results based on relevance
- Add Support for Bytedance Doubao (Thanks Tunixer!)
- Fix to prevent duplicate BOM (Thanks bamps53!)

## [3.9.1]

- Add Gemini 2.5 Pro Preview 03-25 to Google Provider

## [3.9.0]

- Add Enable extended thinking for LiteLLM provider (Thanks @jorgegarciarey!)
- Add a tab for configuring local MCP Servers
- Fix issue with DeepSeek API provider token counting + context management
- Fix issues with checkpoints hanging under certain conditions

## [3.8.6]

- Add UI for adding remote servers
- Add Mentions Feature Guide and update related documentation
- Fix bug where menu would open in sidebar and open tab
- Fix issue with Cline accounts not showing user info in popout tabs
- Fix bug where menu buttons wouldn't open view in sidebar

## [3.8.5]

- Add support for remote MCP Servers using SSE
- Add gemini-2.5-pro-exp-03-25 to Vertex AI (thanks @arri-cc!)
- Add access to history, mcp, and new task buttons in popout view
- Add task feedback telemetry (thumbs up/down on task completion)
- Add toggle disabled for remote servers
- Move the MCP Restart and Delete buttons and add an auto-approve all toggle
- Update Requestly UX for model selection (thanks @arafatkatze!)
- Add escape for html content for gemini when running commands
- Improve search and replace edit failure behaviors

## [3.8.4]

- Add Sambanova Deepseek-V3-0324
- Add cost calculation support for LiteLLM provider
- Fix bug where Cline would use plan_mode_response bug without response parameter

## [3.8.3]

- Add support for SambaNova QwQ-32B model
- Add OpenAI "dynamic" model chatgpt-4o-latest
- Add Amazon Nova models to AWS Bedrock
- Improve file handling for NextJS folder naming (fixes issues with parentheses in folder names)
- Add Gemini 2.5 Pro to Google AI Studio available models
- Handle "input too large" errors for Anthropic
- Fix "See more" not showing up for tasks after task un-fold
- Fix gpt-4.5-preview's supportsPromptCache value to true

## [3.8.2]

- Fix bug where switching to plan/act would result in VS Code LM/OpenRouter model being reset

## [3.8.0]

- Add 'Add to Cline' as an option when you right-click in a file or the terminal, making it easier to add context to your current task
- Add 'Fix with Cline' code action - when you see a lightbulb icon in your editor, you can now select 'Fix with Cline' to send the code and associated errors for Cline to fix. (Cursor users can also use the 'Quick Fix (CMD + .)' menu to see this option)
- Add Account view to display billing and usage history for Cline account users. You can now keep track of credits used and transaction history right in the extension!
- Add 'Sort underling provider routing' setting to Cline/OpenRouter allowing you to sort provider used by throughput, price, latency, or the default (combination of price and uptime)
- Improve rich MCP display with dynamic image loading and support for GIFs
- Add 'Documentation' menu item to easily access Cline's docs
- Add OpenRouter's new usage_details feature for more reliable cost reporting
- Display total space Cline takes on disk next to 'Delete all Tasks' button in History view
- Fix 'Context Window Exceeded' error for OpenRouter/Cline Accounts (additional support coming soon)
- Fix bug where OpenRouter model ID would be set to invalid value
- Add button to delete MCP servers in a failure state

## [3.7.1]

- Fix issue with 'See more' button in task header not showing when starting new tasks
- Fix issue with checkpoints using local git commit hooks

## [3.7.0]

- Cline now displays selectable options when asking questions or presenting a plan, saving you from having to type out responses!
- Add support for a `.clinerules/` directory to load multiple files at once (thanks @ryo-ma!)
- Prevent Cline from reading extremely large files into context that would overload context window
- Improve checkpoints loading performance and display warning for large projects not suited for checkpoints
- Add SambaNova API provider (thanks @saad-noodleseed!)
- Add VPC endpoint option for AWS Bedrock profiles (thanks @minorunara!)
- Add DeepSeek-R1 to AWS Bedrock (thanks @watany-dev!)

## [3.6.5]

- Add 'Delete all Task History' button to History view
- Add toggle to disable model switching between Plan/Act modes in Settings (new users default to disabled)
- Add temperature option to OpenAI Compatible
- Add Kotlin support to tree-sitter parser (thanks @fumiya-kume!)

## [3.6.3]

- Improve QwQ support for Alibaba (thanks @meglinge!) and OpenRouter
- Improve diff edit prompting to prevent immediately reverting to write_to_file when a model uses search patterns that don't match anything in the file
- Fix bug where new checkpoints system would revert file changes when switching between tasks
- Fix issue with incorrect token count for some OpenAI compatible providers

## [3.6.0]

- Add Cline API as a provider option, allowing new users to sign up and get started with Cline for free
- Optimize checkpoints with branch-per-task strategy, reducing storage required and first task load times
- Fix problem with Plan/Act toggle keyboard shortcut not working in Windows (thanks @yt3trees!)
- Add new Gemini models to GCP Vertex (thanks @shohei-ihaya!) and Claude models AskSage (thanks @swhite24!)
- Improve OpenRouter/Cline error reporting

## [3.5.1]

- Add timeout option to MCP servers
- Add Gemini Flash models to Vertex provider (thanks @jpaodev!)
- Add prompt caching support for AWS Bedrock provider (thanks @buger!)
- Add AskSage provider (thanks @swhite24!)

## [3.5.0]

- Add 'Enable extended thinking' option for Claude 3.7 Sonnet, with ability to set different budgets for Plan and Act modes
- Add support for rich MCP responses with automatic image previews, website thumbnails, and WolframAlpha visualizations
- Add language preference option in Advanced Settings
- Add xAI Provider Integration with support for all Grok models (thanks @andrewmonostate!)
- Fix issue with Linux XDG pointing to incorrect path for Document folder (thanks @jonatkinson!)

## [3.4.10]

- Add support for GPT-4.5 preview model

## [3.4.9]

- Add toggle to let users opt-in to anonymous telemetry and error reporting

## [3.4.6]

- Add support for Claude 3.7 Sonnet

## [3.4.0]

- Introducing MCP Marketplace! You can now discover and install the best MCP servers right from within the extension, with new servers added regularly
- Add mermaid diagram support in Plan mode! You can now see visual representations of mermaid code blocks in chat, and click on them to see an expanded view
- Use more visual checkpoints indicators after editing files & running commands
- Create a checkpoint at the beginning of each task to easily revert to the initial state
- Add 'Terminal' context mention to reference the active terminal's contents
- Add 'Git Commits' context mention to reference current working changes or specific commits (thanks @mrubens!)
- Send current textfield contents as additional feedback when toggling from Plan to Act Mode, or when hitting 'Approve' button
- Add advanced configuration options for OpenAI Compatible (context window, max output, pricing, etc.)
- Add Alibaba Qwen 2.5 coder models, VL models, and DeepSeek-R1/V3 support
- Improve support for AWS Bedrock Profiles
- Fix Mistral provider support for non-codestral models
- Add advanced setting to disable browser tool
- Add advanced setting to set chromium executable path for browser tool

## [3.3.2]

- Fix bug where OpenRouter requests would periodically not return cost/token stats, leading to context window limit errors
- Make checkpoints more visible and keep track of restored checkpoints

## [3.3.0]

- Add .clineignore to block Cline from accessing specified file patterns
- Add keyboard shortcut + tooltips for Plan/Act toggle
- Fix bug where new files won't show up in files dropdown
- Add automatic retry for rate limited requests (thanks @ViezeVingertjes!)
- Adding reasoning_effort support for o3-mini in Advanced Settings
- Added support for AWS provider profiles using the AWS CLI to make the profile, enabling long lived connections to AWS bedrock
- Adding Requesty API provider
- Add Together API provider
- Add Alibaba Qwen API provider (thanks @aicccode!)

## [3.2.13]

- Add new gemini models gemini-2.0-flash-lite-preview-02-05 and gemini-2.0-flash-001
- Add all available Mistral API models (thanks @ViezeVingertjes!)
- Add LiteLLM API provider support (thanks @him0!)

## [3.2.12]

- Fix command chaining for Windows users
- Fix reasoning_content error for OpenAI providers

## [3.2.11]

- Add OpenAI o3-mini model

## [3.2.10]

- Improve support for DeepSeek-R1 (deepseek-reasoner) model for OpenRouter, OpenAI-compatible, and DeepSeek direct (thanks @Szpadel!)
- Show Reasoning tokens for models that support it
- Fix issues with switching models between Plan/Act modes

## [3.2.6]

- Save last used API/model when switching between Plan and Act, for users that like to use different models for each mode
- New Context Window progress bar in the task header to understand increased cost/generation degradation as the context increases
- Localize READMEs and add language selector for English, Spanish, German, Chinese, and Japanese
- Add Advanced Settings to remove MCP prompts from requests to save tokens, enable/disable checkpoints for users that don't use git (more coming soon!)
- Add Gemini 2.0 Flash Thinking experimental model
- Allow new users to subscribe to mailing list to get notified when new Accounts option is available

## [3.2.5]

- Use yellow textfield outline in Plan mode to better distinguish from Act mode

## [3.2.3]

- Add DeepSeek-R1 (deepseek-reasoner) model support with proper parameter handling (thanks @slavakurilyak!)

## [3.2.0]

- Add Plan/Act mode toggle to let you plan tasks with Cline before letting him get to work
- Easily switch between API providers and models using a new popup menu under the chat field
- Add VS Code LM API provider to run models provided by other VS Code extensions (e.g. GitHub Copilot). Shoutout to @julesmons, @RaySinner, and @MrUbens for putting this together!
- Add on/off toggle for MCP servers to disable them when not in use. Thanks @MrUbens!
- Add Auto-approve option for individual tools in MCP servers. Thanks @MrUbens!

## [3.1.10]

- New icon!

## [3.1.9]

- Add Mistral API provider with codestral-latest model

## [3.1.7]

- Add ability to change viewport size and headless mode when Cline asks to launch the browser

## [3.1.6]

- Fix bug where filepaths with Chinese characters would not show up in context mention menu (thanks @chi-chat!)
- Update Anthropic model prices (thanks @timoteostewart!)

## [3.1.5]

- Fix bug where Cline couldn't read "@/" import path aliases from tool results

## [3.1.4]

- Fix issue where checkpoints would not work for users with git commit signing enabled globally

## [3.1.2]

- Fix issue where LFS files would be not be ignored when creating checkpoints

## [3.1.0]

- Added checkpoints: Snapshots of workspace are automatically created whenever Cline uses a tool
- Compare changes: Hover over any tool use to see a diff between the snapshot and current workspace state
- Restore options: Choose to restore just the task state, just the workspace files, or both
- New 'See new changes' button appears after task completion, providing an overview of all workspace changes
- Task header now shows disk space usage with a delete button to help manage snapshot storage

## [3.0.12]

- Fix DeepSeek API cost reporting (input price is 0 since it's all either a cache read or write, different than how Anthropic reports cache usage)

## [3.0.11]

- Emphasize auto-formatting done by the editor in file edit responses for more reliable diff editing

## [3.0.10]

- Add DeepSeek provider to API Provider options
- Fix context window limit errors for DeepSeek v3

## [3.0.9]

- Fix bug where DeepSeek v3 would incorrectly escape HTML entities in diff edits

## [3.0.8]

- Mitigate DeepSeek v3 diff edit errors by adding 'auto-formatting considerations' to system prompt, encouraging model to use updated file contents as reference point for SEARCH blocks

## [3.0.7]

- Revert to using batched file watcher to fix crash when many files would be created at once

## [3.0.6]

- Fix bug where some files would be missing in the `@` context mention menu
- Add Bedrock support in additional regions
- Diff edit improvements
- Add OpenRouter's middle-out transform for models that don't use prompt caching (prevents context window limit errors, but cannot be applied to models like Claude since it would continuously break the cache)

## [3.0.4]

- Fix bug where gemini models would add code block artifacts to the end of text content
- Fix context mention menu visual issues on light themes

## [3.0.2]

- Adds block anchor matching for more reliable diff edits (if 3+ lines, first and last line are used as anchors to search for)
- Add instruction to system prompt to use complete lines in diff edits to work properly with fallback strategies
- Improves diff edit error handling
- Adds new Gemini models

## [3.0.0]

- Cline now uses a search & replace diff based approach when editing large files to prevent code deletion issues.
- Adds support for a more comprehensive auto-approve configuration, allowing you to specify which tools require approval and which don't.
- Adds ability to enable system notifications for when Cline needs approval or completes a task.
- Adds support for a root-level `.clinerules` file that can be used to specify custom instructions for the project.

## [2.2.0]

- Add support for Model Context Protocol (MCP), enabling Cline to use custom tools like web-search tool or GitHub tool
- Add MCP server management tab accessible via the server icon in the menu bar
- Add ability for Cline to dynamically create new MCP servers based on user requests (e.g., "add a tool that gets the latest npm docs")

## [2.1.6]

- Add LM Studio as an API provider option (make sure to start the LM Studio server to use it with the extension!)

## [2.1.5]

- Add support for prompt caching for new Claude model IDs on OpenRouter (e.g. `anthropic/claude-3.5-sonnet-20240620`)

## [2.1.4]

- AWS Bedrock fixes (add missing regions, support for cross-region inference, and older Sonnet model for regions where new model is not available)

## [2.1.3]

- Add support for Claude 3.5 Haiku, 66% cheaper than Sonnet with similar intelligence

## [2.1.2]

- Misc. bug fixes
- Update README with new browser feature

## [2.1.1]

- Add stricter prompt to prevent Cline from editing files during a browser session without first closing the browser

## [2.1.0]

- Cline now uses Anthropic's new "Computer Use" feature to launch a browser, click, type, and scroll. This gives him more autonomy in runtime debugging, end-to-end testing, and even general web use. Try asking "Look up the weather in Colorado" to see it in action! (Available with Claude 3.5 Sonnet v2)

## [2.0.19]

- Fix model info for Claude 3.5 Sonnet v1 on OpenRouter

## [2.0.18]

- Add support for both v1 and v2 of Claude 3.5 Sonnet for GCP Vertex and AWS Bedrock (for cases where the new model is not enabled yet or unavailable in your region)

## [2.0.17]

- Update Anthropic model IDs

## [2.0.16]

- Adjustments to system prompt

## [2.0.15]

- Fix bug where modifying Cline's edits would lead him to try to re-apply the edits
- Fix bug where weaker models would display file contents before using the write_to_file tool
- Fix o1-mini and o1-preview errors when using OpenAI native

## [2.0.14]

- Gracefully cancel requests while stream could be hanging

## [2.0.13]

- Detect code omission and show warning with troubleshooting link

## [2.0.12]

- Keep cursor out of the way during file edit streaming animation

## [2.0.11]

- Adjust prompts around read_file to prevent re-reading files unnecessarily

## [2.0.10]

- More adjustments to system prompt to prevent lazy coding

## [2.0.9]

- Update system prompt to try to prevent Cline from lazy coding (`// rest of code here...`)

## [2.0.8]

- Fix o1-mini and o1-preview for OpenAI
- Fix diff editor not opening sometimes in slow environments like project idx

## [2.0.7]

- Misc. bug fixes

## [2.0.6]

- Update URLs to https://github.com/cline/cline

## [2.0.5]

- Fixed bug where Cline's edits would stream into the active tab when switching tabs during a write_to_file
- Added explanation in task continuation prompt that an interrupted write_to_file reverts the file to its original contents, preventing unnecessary re-reads
- Fixed non-first chunk error handling in case stream fails mid-way through

## [2.0.0]

- New name! Meet Cline, an AI assistant that can use your CLI and Editor
- Responses are now streamed with a yellow text decoration animation to keep track of Cline's progress as he edits files
- New Cancel button to give Cline feedback if he goes off in the wrong direction, giving you more control over tasks
- Re-imagined tool calling prompt resulting in ~40% fewer requests to accomplish tasks + better performance with other models
- Search and use any model with OpenRouter

## [1.9.7]

- Only auto-include error diagnostics after file edits, removed warnings to keep Claude from getting distracted in projects with strict linting rules

## [1.9.6]

- Added support for new Google Gemini models `gemini-1.5-flash-002` and `gemini-1.5-pro-002`
- Updated system prompt to be more lenient when terminal output doesn't stream back properly
- Adjusted system prompt to prevent overuse of the inspect_site tool
- Increased global line height for improved readability

## [1.9.0]

- Claude can now use a browser! This update adds a new `inspect_site` tool that captures screenshots and console logs from websites (including localhost), making it easier for Claude to troubleshoot issues on his own.
- Improved automatic linter/compiler debugging by only sending Claude new errors that result from his edits, rather than reporting all workspace problems.

## [1.8.0]

- You can now use '@' in the textarea to add context!
- @url: Paste in a URL for the extension to fetch and convert to markdown, useful when you want to give Claude the latest docs!
- @problems: Add workspace errors and warnings for Claude to fix, no more back-and-forth about debugging
- @file: Adds a file's contents so you don't have to waste API requests approving read file (+ type to search files)
- @folder: Adds folder's files all at once to speed up your workflow even more

## [1.7.0]

- Adds problems monitoring to keep Claude updated on linter/compiler/build issues, letting him proactively fix errors on his own! (adding missing imports, fixing type errors, etc.)

## [1.6.5]

- Adds support for OpenAI o1, Azure OpenAI, and Google Gemini (free for up to 15 requests per minute!)
- Task header can now be collapsed to provide more space for viewing conversations
- Adds fuzzy search and sorting to Task History, making it easier to find specific tasks

## [1.6.0]

- Commands now run directly in your terminal thanks to VSCode 1.93's new shell integration updates! Plus a new 'Proceed While Running' button to let Claude continue working while commands run, sending him new output along the way (i.e. letting him react to server errors as he edits files)

## [1.5.27]

- Claude's changes now appear in your file's Timeline, allowing you to easily view a diff of each edit. This is especially helpful if you want to revert to a previous version. No need for git—everything is tracked by VSCode's local history!
- Updated system prompt to keep Claude from re-reading files unnecessarily

## [1.5.19]

- Adds support for OpenAI compatible API providers (e.g. Ollama!)

## [1.5.13]

- New terminal emulator! When Claude runs commands, you can now type directly in the terminal (+ support for Python environments)
- Adds search to Task History

## [1.5.6]

- You can now edit Claude's changes before accepting! When he edits or creates a file, you can modify his changes directly in the right side of the diff view (+ hover over the 'Revert Block' arrow button in the center to undo `// rest of code here` shenanigans)

## [1.5.4]

- Adds support for reading .pdf and .docx files (try "turn my business_plan.docx into a company website")

## [1.5.0]

- Adds new `search_files` tool that lets Claude perform regex searches in your project, making it easy for him to refactor code, address TODOs and FIXMEs, remove dead code, and more!

## [1.4.0]

- Adds "Always allow read-only operations" setting to let Claude read files and view directories without needing approval (off by default)
- Implement sliding window context management to keep tasks going past 200k tokens
- Adds Google Cloud Vertex AI support and updates Claude 3.5 Sonnet max output to 8192 tokens for all providers.
- Improves system prompt to guard against lazy edits (less "//rest of code here")

## [1.3.0]

- Adds task history

## [1.2.0]

- Adds support for Prompt Caching to significantly reduce costs and response times (currently only available through Anthropic API for Claude 3.5 Sonnet and Claude 3.0 Haiku)

## [1.1.1]

- Adds option to choose other Claude models (+ GPT-4o, DeepSeek, and Mistral if you use OpenRouter)
- Adds option to add custom instructions to the end of the system prompt

## [1.1.0]

- Paste images in chat to use Claude's vision capabilities and turn mockups into fully functional applications or fix bugs with screenshots

## [1.0.9]

- Add support for OpenRouter and AWS Bedrock

## [1.0.8]

- Shows diff view of new or edited files right in the editor

## [1.0.7]

- Replace `list_files` and `analyze_project` with more explicit `list_files_top_level`, `list_files_recursive`, and `view_source_code_definitions_top_level` to get source code definitions only for files relevant to the task

## [1.0.6]

- Interact with CLI commands by sending messages to stdin and terminating long-running processes like servers
- Export tasks to markdown files (useful as context for future tasks)

## [1.0.5]

- Claude now has context about vscode's visible editors and opened tabs

## [1.0.4]

- Open in the editor (using menu bar or `Claude Dev: Open In New Tab` in command palette) to see how Claude updates your workspace more clearly
- New `analyze_project` tool to help Claude get a comprehensive overview of your project's source code definitions and file structure
- Provide feedback to tool use like terminal commands and file edits
- Updated max output tokens to 8192 so less lazy coding (`// rest of code here...`)
- Added ability to retry failed API requests (helpful for rate limits)
- Quality of life improvements like markdown rendering, memory optimizations, better theme support

## [0.0.6]

- Initial release


--- CONTRIBUTING.md ---
# Contributing to Cline

We're thrilled you're interested in contributing to Cline. Whether you're fixing a bug, adding a feature, or improving our docs, every contribution makes Cline smarter! To keep our community vibrant and welcoming, all members must adhere to our [Code of Conduct](CODE_OF_CONDUCT.md).

## Reporting Bugs or Issues

Bug reports help make Cline better for everyone! Before creating a new issue, please [search existing ones](https://github.com/cline/cline/issues) to avoid duplicates. When you're ready to report a bug, head over to our [issues page](https://github.com/cline/cline/issues/new/choose) where you'll find a template to help you with filling out the relevant information.

<blockquote class='warning-note'>
     🔐 <b>Important:</b> If you discover a security vulnerability, please use the <a href="https://github.com/cline/cline/security/advisories/new">Github security tool to report it privately</a>.
</blockquote>


## Before Contributing

All contributions must begin with a GitHub Issue, unless the change is for small bug fixes, typo corrections, minor wording improvements, or simple type fixes that don't change functionality.
**For features and contributions**:
- First check the [Feature Requests discussions board](https://github.com/cline/cline/discussions/categories/feature-requests) for similar ideas
- If your idea is new, create a new feature request  
- Wait for approval from core maintainers before starting implementation
- Once approved, feel free to begin working on a PR with the help of our community!

**PRs without approved issues may be closed.**


## Deciding What to Work On

Looking for a good first contribution? Check out issues labeled ["good first issue"](https://github.com/cline/cline/labels/good%20first%20issue) or ["help wanted"](https://github.com/cline/cline/labels/help%20wanted). These are specifically curated for new contributors and areas where we'd love some help!

We also welcome contributions to our [documentation](https://github.com/cline/cline/tree/main/docs)! Whether it's fixing typos, improving existing guides, or creating new educational content - we'd love to build a community-driven repository of resources that helps everyone get the most out of Cline. You can start by diving into `/docs` and looking for areas that need improvement.

## Development Setup


### Local Development Instructions

1. Clone the repository _(Requires [git-lfs](https://git-lfs.com/))_:
    ```bash
    git clone https://github.com/cline/cline.git
    ```
2. Open the project in VSCode:
    ```bash
    code cline
    ```
3. Install the necessary dependencies for the extension and webview-gui:
    ```bash
    npm run install:all
    ```
4. Generate Protocol Buffer files (required before first build):
    ```bash
    npm run protos
    ```
5. Launch by pressing `F5` (or `Run`->`Start Debugging`) to open a new VSCode window with the extension loaded. (You may need to install the [esbuild problem matchers extension](https://marketplace.visualstudio.com/items?itemName=connor4312.esbuild-problem-matchers) if you run into issues building the project.)




### Creating a Pull Request

1. Before creating a PR, generate a changeset entry:
    ```bash
    npm run changeset
    ```
   This will prompt you for:
   - Type of change (major, minor, patch)
     - `major` → breaking changes (1.0.0 → 2.0.0)
     - `minor` → new features (1.0.0 → 1.1.0)
     - `patch` → bug fixes (1.0.0 → 1.0.1)
   - Description of your changes

2. Commit your changes and the generated `.changeset` file

3. Push your branch and create a PR on GitHub. Our CI will:
   - Run tests and checks
   - Changesetbot will create a comment showing the version impact
   - When merged to main, changesetbot will create a Version Packages PR
   - When the Version Packages PR is merged, a new release will be published
4. Testing
    - Run `npm run test` to run tests locally. 
    - Before submitting PR, run `npm run format:fix` to format your code

### Extension

1. **VS Code Extensions**

    - When opening the project, VS Code will prompt you to install recommended extensions
    - These extensions are required for development - please accept all installation prompts
    - If you dismissed the prompts, you can install them manually from the Extensions panel

2. **Local Development**
    - Run `npm run install:all` to install dependencies
    - Run `npm run protos` to generate Protocol Buffer files (required before first build)
    - Run `npm run test` to run tests locally
    - Run → Start Debugging or `>Debug: Select and Start Debugging` and wait for a new VS Code instance to open
    - **Terminal Workflow**: Use `npm run dev` (generates protos + runs watch mode) or `npm run watch` (if protos already generated)
    - Before submitting PR, run `npm run format:fix` to format your code

3. **Linux-specific Setup**
    VS Code extension tests on Linux require the following system libraries:

    - `dbus`
    - `libasound2`
    - `libatk-bridge2.0-0`
    - `libatk1.0-0`
    - `libdrm2`
    - `libgbm1`
    - `libgtk-3-0`
    - `libnss3`
    - `libx11-xcb1`
    - `libxcomposite1`
    - `libxdamage1`
    - `libxfixes3`
    - `libxkbfile1`
    - `libxrandr2`
    - `xvfb`

    These libraries provide necessary GUI components and system services for the test environment.

    For example, on Debian-based distributions (e.g., Ubuntu), you can install these libraries using apt:
    ```bash
    sudo apt update
    sudo apt install -y \
      dbus \
      libasound2 \
      libatk-bridge2.0-0 \
      libatk1.0-0 \
      libdrm2 \
      libgbm1 \
      libgtk-3-0 \
      libnss3 \
      libx11-xcb1 \
      libxcomposite1 \
      libxdamage1 \
      libxfixes3 \
      libxkbfile1 \
      libxrandr2 \
      xvfb
    ```

## Writing and Submitting Code

Anyone can contribute code to Cline, but we ask that you follow these guidelines to ensure your contributions can be smoothly integrated:

1. **Keep Pull Requests Focused**

    - Limit PRs to a single feature or bug fix
    - Split larger changes into smaller, related PRs
    - Break changes into logical commits that can be reviewed independently

2. **Code Quality**

    - Run `npm run lint` to check code style
    - Run `npm run format` to automatically format code
    - All PRs must pass CI checks which include both linting and formatting
    - Address any warnings or errors from linter before submitting
    - Follow TypeScript best practices and maintain type safety

3. **Testing**

    - Add tests for new features
    - Run `npm test` to ensure all tests pass
    - Update existing tests if your changes affect them
    - Include both unit tests and integration tests where appropriate

    **End-to-End (E2E) Testing**
    
    Cline includes comprehensive E2E tests using Playwright that simulate real user interactions with the extension in VS Code:
    
    - **Running E2E tests:**
      ```bash
      npm run test:e2e        # Build and run all E2E tests
      npm run e2e             # Run tests without rebuilding
      npm run test:e2e -- --debug  # Run with interactive debugger
      ```
    
    - **Writing E2E tests:**
      - Tests are located in `src/test/e2e/`
      - Use the `e2e` fixture for single-root workspace tests
      - Use `e2eMultiRoot` fixture for multi-root workspace tests
      - Follow existing patterns in `auth.test.ts`, `chat.test.ts`, `diff.test.ts`, and `editor.test.ts`
      - See `src/test/e2e/README.md` for detailed documentation
    
    - **Debug mode features:**
      - Interactive Playwright Inspector for step-by-step debugging
      - Record new interactions and generate test code automatically
      - Visual VS Code instance for manual testing
      - Element inspection and selector validation
    
    - **Test environment:**
      - Automated VS Code setup with Cline extension loaded
      - Mock API server for backend testing
      - Temporary workspaces with test fixtures
      - Video recording for failed tests

4. **Version Management with Changesets**

    - Create a changeset for any user-facing changes using `npm run changeset`
    - Choose the appropriate version bump:
        - `major` for breaking changes (1.0.0 → 2.0.0)
        - `minor` for new features (1.0.0 → 1.1.0)
        - `patch` for bug fixes (1.0.0 → 1.0.1)
    - Write clear, descriptive changeset messages that explain the impact
    - Documentation-only changes don't require changesets

5. **Commit Guidelines**

    - Write clear, descriptive commit messages
    - Use conventional commit format (e.g., "feat:", "fix:", "docs:")
    - Reference relevant issues in commits using #issue-number

6. **Before Submitting**

    - Rebase your branch on the latest main
    - Ensure your branch builds successfully
    - Double-check all tests are passing
    - Review your changes for any debugging code or console logs

7. **Pull Request Description**
    - Clearly describe what your changes do
    - Include steps to test the changes
    - List any breaking changes
    - Add screenshots for UI changes

## Contribution Agreement

By submitting a pull request, you agree that your contributions will be licensed under the same license as the project ([Apache 2.0](LICENSE)).

Remember: Contributing to Cline isn't just about writing code - it's about being part of a community that's shaping the future of AI-assisted development. Let's build something amazing together! 🚀


## Links discovered
- [Code of Conduct](https://github.com/cline/cline/blob/main/CODE_OF_CONDUCT.md)
- [search existing ones](https://github.com/cline/cline/issues)
- [issues page](https://github.com/cline/cline/issues/new/choose)
- [Feature Requests discussions board](https://github.com/cline/cline/discussions/categories/feature-requests)
- ["good first issue"](https://github.com/cline/cline/labels/good%20first%20issue)
- ["help wanted"](https://github.com/cline/cline/labels/help%20wanted)
- [documentation](https://github.com/cline/cline/tree/main/docs)
- [git-lfs](https://git-lfs.com/)
- [esbuild problem matchers extension](https://marketplace.visualstudio.com/items?itemName=connor4312.esbuild-problem-matchers)
- [Apache 2.0](https://github.com/cline/cline/blob/main/LICENSE.md)
- [Github security tool to report it privately](https://github.com/cline/cline/security/advisories/new)

--- cli/CHANGELOG.md ---
# cline


### Fixed
- Banners now display immediately when opening the extension instead of requiring user interaction first
- Resolved 17 security vulnerabilities including high-severity DoS issues in dependencies (body-parser, axios, qs, tar, and others)

## [2.2.2]

- Allows users to enter custom aws region when selecting bedrock as a provider
- Prevent Parent Container Scrolling In Dropdowns

## [2.2.1]

- Added Minimax 2.5 Free Promo
- Fixed Response chaining for OpenAI's Responses API

## [2.2.0]

### Added

- Subagent: replace legacy subagents with the native `use_subagents` tool
- Bundle `endpoints.json` support so packaged distributions can ship required endpoints out-of-the-box
- Amazon Bedrock: support parallel tool calling
- New "double-check completion" experimental feature to verify work before marking tasks complete
- CLI: new task controls/flags including custom `--thinking` token budget and `--max-consecutive-mistakes` for yolo runs
- Remote config: new UI/options (including connection/test buttons) and support for syncing deletion of remotely configured MCP servers
- Vertex / Claude Code: add 1M context model options for Claude Opus 4.6
- ZAI/GLM: add GLM-5

### Fixed

- CLI: handle stdin redirection correctly in CI/headless environments
- CLI: preserve OAuth callback paths during auth redirects
- VS Code Web: generate auth callback URLs via `vscode.env.asExternalUri` (OAuth callback reliability)
- Terminal: surface command exit codes in results and improve long-running `execute_command` timeout behavior
- UI: add loading indicator and fix `api_req_started` rendering
- Task streaming: prevent duplicate streamed text rows after completion
- API: preserve selected Vercel model when model metadata is missing
- Telemetry: route PostHog networking through proxy-aware shared fetch and ensure telemetry flushes on shutdown
- CI: increase Windows E2E test timeout to reduce flakiness

### Changed

- Settings/model UX: move "reasoning effort" into model configuration and expose it in settings
- CLI provider selection: limit provider list to those remotely configured
- UI: consolidate ViewHeader component/styling across views
- Tools: add auto-approval support for `attempt_completion` commands
- Remotely configured MCP server schema now supports custom headers

## [2.1.0]

### Minor Changes

- 42ce100: Add Generate API Key on Hicap Provider selection

### Patch Changes

- 195294f: Add support for bundled endpoints.json in enterprise distributions. Extensions can now include a pre-configured endpoints.json file that automatically switches Cline to self-hosted mode. Includes packaging scripts for VSIX, NPM, and JetBrains plugins.
- a1f2601: Replace the LiteLLM model list with a selector
- 739d75a: Add Claude Code provider support for Claude Opus 4.6 and Sonnet 4.5 1M variants via both full model names and aliases (`opus[1m]`, `sonnet[1m]`), and align the `opus` alias with Opus 4.6.
- 8440380: Add GitHub Actions workflow to build CLI from any commit for testing
- b1a8db2: fix(cli): prevent hang when spawned without TTY
- 7c87017: Add Claude Opus 4.6 model support
- d116ac5: Supports rendering markdown table in chat view.
- 6d8fb85: Fix CLI crashing in CI environments and with stdin redirection (e.g., `cline "prompt" < /dev/null`). Now checks both stdin and stdout TTY status before using Ink, and only errors on empty stdin when no prompt is provided.
- 70a9904: Fix JetBrains sign-in regression by adding fallback for openExternal RPC
- f440f3a: fix: use vscode.env.openExternal for auth in remote environments

  Fixes OAuth authentication in VS Code Server and remote environments by routing browser URL opening through VS Code's native openExternal API instead of the npm 'open' package.

- 70a9904: fix: use vscode.env.asExternalUri for auth callback URLs only in VS Code Web

  Fixes OAuth callback redirect in VS Code Web (`code serve-web`, Codespaces) by using `vscode.env.asExternalUri()` to resolve the callback URI. This is gated behind a `vscode.env.uiKind === UIKind.Web` check so regular desktop VS Code continues to use the `vscode://` URI directly. The `getCallbackUrl` API now accepts a `path` parameter so the full callback URI (including route) is resolved correctly, and callers pass their path directly instead of appending after.

- 5308ded: Updating script documentation and removing unnecessary continue on error
- b514f18: Prevent duplicate streamed text rows when a partial text update arrives after the same text was already finalized.
- 26391c9: Fix Bedrock model id
- d19a877: Unify ViewHeader Styles Across All Views
- 5dcaa8c: Add Vertex Claude Opus 4.6 1M model option and global endpoint support, and pass the 1M beta header for Vertex Claude requests.


--- locales/ar-sa/CONTRIBUTING.md ---
# المساهمة في Cline

نحن سعداء لاهتمامك بالمساهمة في Cline. سواء كنت تصلح خطأً أو تضيف ميزة أو تحسن الوثائق لدينا، فإن كل مساهمة تجعل Cline أذكى! للحفاظ على مجتمعنا نابضًا بالحياة وترحيبيًا، يجب على جميع الأعضاء الالتزام بـ [مدونة قواعد السلوك](CODE_OF_CONDUCT.md) لدينا.

## الإبلاغ عن الأخطاء أو المشكلات

تساعد تقارير الأخطاء على جعل Cline أفضل للجميع! قبل إنشاء مشكلة جديدة، يرجى [البحث عن المشكلات الموجودة](https://github.com/cline/cline/issues) لتجنب الازدواجية. عندما تكون جاهزًا للإبلاغ عن خطأ، انتقل إلى [صفحة المشكلات](https://github.com/cline/cline/issues/new/choose) حيث ستجد قالبًا لمساعدتك في ملء المعلومات ذات الصلة.

<blockquote class='warning-note'>
     🔐 <b>مهم:</b> إذا اكتشفت ثغرة أمنية، فيرجى استخدام <a href="https://github.com/cline/cline/security/advisories/new">أداة الأمان على Github للإبلاغ عنها بشكل خاص</a>.
</blockquote>

## تحديد ما يجب العمل عليه

تبحث عن مساهمة أولى جيدة؟ تحقق من المشكلات المميزة بـ ["good first issue"](https://github.com/cline/cline/labels/good%20first%20issue) أو ["help wanted"](https://github.com/cline/cline/labels/help%20wanted). تم تحديد هذه المشكلات خصيصًا للمساهمين الجدد والمجالات التي نرحب فيها بالمساعدة!

نرحب أيضًا بالمساهمات في [الوثائق](https://github.com/cline/cline/tree/main/docs) لدينا! سواء كان تصحيح أخطاء إملائية، أو تحسين الأدلة الحالية، أو إنشاء محتوى تعليمي جديد - نود بناء مستودع موارد مدفوع من المجتمع يساعد الجميع على الاستفادة القصوى من Cline. يمكنك البدء بالغوص في `/docs` والبحث عن مجالات تحتاج إلى تحسين.

إذا كنت تخطط للعمل على ميزة أكبر، فيرجى إنشاء [طلب ميزة](https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop) أولاً حتى نتمكن من مناقشة ما إذا كان ذلك يتماشى مع رؤية Cline.

## إعداد التطوير

1. **إضافات VS Code**

    - عند فتح المشروع، سيطالبك VS Code بتثبيت الإضافات الموصى بها
    - هذه الإضافات مطلوبة للتطوير - يرجى قبول جميع مطالبات التثبيت
    - إذا تجاهلت المطالبات، يمكنك تثبيتها يدويًا من لوحة الإضافات

2. **التطوير المحلي**
    - قم بتشغيل `npm run install:all` لتثبيت التبعيات
    - قم بتشغيل `npm run test` لتشغيل الاختبارات محليًا
    - قبل تقديم طلب السحب، قم بتشغيل `npm run format:fix` لتنسيق التعليمات البرمجية الخاصة بك

## كتابة وتقديم التعليمات البرمجية

يمكن لأي شخص المساهمة بالتعليمات البرمجية في Cline، لكننا نطلب منك اتباع هذه الإرشادات لضمان دمج مساهماتك بسلاسة:

1. **احتفظ بطلبات السحب مركزة**

    - قيد طلبات السحب بميزة واحدة أو إصلاح خطأ
    - قسم التغييرات الأكبر إلى طلبات سحب أصغر ومتصلة
    - قسم التغييرات إلى التزامات منطقية يمكن مراجعتها بشكل مستقل

2. **جودة التعليمات البرمجية**

    - قم بتشغيل `npm run lint` للتحقق من نمط التعليمات البرمجية
    - قم بتشغيل `npm run format` لتنسيق التعليمات البرمجية تلقائيًا
    - يجب أن تجتاز جميع طلبات السحب عمليات التحقق المستمر التي تشمل كلاً من التنضيد والتنسيق
    - تعامل مع أي تحذيرات أو أخطاء ESLint قبل التقديم
    - اتبع أفضل ممارسات TypeScript والحفاظ على سلامة النوع

3. **الاختبار**

    - أضف اختبارات للميزات الجديدة
    - قم بتشغيل `npm test` للتأكد من اجتياز جميع الاختبارات
    - قم بتحديث الاختبارات الحالية إذا كانت تغييراتك تؤثر عليها
    - تضمين كل من اختبارات الوحدة واختبارات التكامل حيثما كان ذلك مناسبًا

4. **إدارة الإصدار مع Changesets**

    - أنشئ changeset لأي تغييرات واجهة المستخدم باستخدام `npm run changeset`
    - اختر زيادة الإصدار المناسبة:
        - `major` للتغييرات الكبيرة (1.0.0 → 2.0.0)
        - `minor` للميزات الجديدة (1.0.0 → 1.1.0)
        - `patch` لإصلاحات الأخطاء (1.0.0 → 1.0.1)
    - اكتب رسائل changeset واضحة ووصفية تشرح التأثير
    - لا تتطلب التغييرات في الوثائق فقط changesets

5. **إرشادات الالتزام (Commit Guidelines)**

   - اكتب رسائل التزام واضحة وواصفة  
   - استخدم تنسيق الالتزام التقليدي (مثل: "feat:", "fix:", "docs:")  
   - أشر إلى القضايا ذات الصلة في الالتزامات باستخدام #رقم-القضية  

6. **قبل الإرسال**

   - قم بإعادة دمج فرعك مع أحدث إصدار من الفرع الرئيسي  
   - تأكد من أن الفرع الخاص بك يُبنى بنجاح  
   - تحقق من اجتياز جميع الاختبارات  
   - راجع التغييرات الخاصة بك للتأكد من عدم وجود تعليمات تصحيح الأخطاء أو سجلات وحدة التحكم  

7. **وصف طلب السحب (Pull Request Description)**

   - صف بوضوح ما تقوم به التغييرات  
   - قم بتضمين خطوات لاختبار التغييرات  
   - أدرج أي تغييرات غير متوافقة  
   - أضف لقطات شاشة للتغييرات في واجهة المستخدم  

## اتفاقية المساهمة  

من خلال إرسال طلب سحب، فإنك توافق على أن مساهماتك سيتم ترخيصها بنفس ترخيص المشروع ([Apache 2.0](LICENSE)).  

تذكر: المساهمة في Cline لا تقتصر فقط على كتابة الكود - إنها تتعلق بأن تكون جزءًا من مجتمع يُشكل مستقبل التطوير بمساعدة الذكاء الاصطناعي. لنبنِ شيئًا رائعًا معًا! 🚀

## Links discovered
- [مدونة قواعد السلوك](https://github.com/cline/cline/blob/main/locales/ar-sa/CODE_OF_CONDUCT.md)
- [البحث عن المشكلات الموجودة](https://github.com/cline/cline/issues)
- [صفحة المشكلات](https://github.com/cline/cline/issues/new/choose)
- ["good first issue"](https://github.com/cline/cline/labels/good%20first%20issue)
- ["help wanted"](https://github.com/cline/cline/labels/help%20wanted)
- [الوثائق](https://github.com/cline/cline/tree/main/docs)
- [طلب ميزة](https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop)
- [Apache 2.0](https://github.com/cline/cline/blob/main/locales/ar-sa/LICENSE.md)
- [أداة الأمان على Github للإبلاغ عنها بشكل خاص](https://github.com/cline/cline/security/advisories/new)

--- locales/de/CONTRIBUTING.md ---
# Beitrag zu Cline

Wir freuen uns, dass du daran interessiert bist, zu Cline beizutragen. Ob du einen Fehler behebst, eine Funktion hinzufügst oder unsere Dokumentation verbesserst – jeder Beitrag macht Cline intelligenter! Um unsere Community lebendig und einladend zu halten, müssen alle Mitglieder unseren [Verhaltenskodex](CODE_OF_CONDUCT.md) einhalten.

## Fehler oder Probleme melden

Fehlermeldungen helfen, Cline für alle zu verbessern! Bevor du ein neues Problem erstellst, überprüfe bitte die [bestehenden Probleme](https://github.com/cline/cline/issues), um Duplikate zu vermeiden. Wenn du bereit bist, einen Fehler zu melden, gehe zu unserer [Issues-Seite](https://github.com/cline/cline/issues/new/choose), wo du eine Vorlage findest, die dir hilft, die relevanten Informationen auszufüllen.

<blockquote class='warning-note'>
    🔐 <b>Wichtig:</b> Wenn du eine Sicherheitslücke entdeckst, verwende das <a href="https://github.com/cline/cline/security/advisories/new">GitHub-Sicherheitstool, um sie privat zu melden</a>.
</blockquote>

## Entscheiden, woran man arbeiten möchte

Suchst du nach einem guten ersten Beitrag? Schau dir die mit ["good first issue"](https://github.com/cline/cline/labels/good%20first%20issue) oder ["help wanted"](https://github.com/cline/cline/labels/help%20wanted) gekennzeichneten Issues an. Diese sind speziell für neue Mitwirkende ausgewählt und Bereiche, in denen wir gerne Hilfe erhalten würden!

Wir begrüßen auch Beiträge zu unserer [Dokumentation](https://github.com/cline/cline/tree/main/docs). Ob du Tippfehler korrigierst, bestehende Anleitungen verbesserst oder neue Bildungsinhalte erstellst – wir möchten ein von der Community verwaltetes Ressourcen-Repository aufbauen, das allen hilft, das Beste aus Cline herauszuholen. Du kannst beginnen, indem du `/docs` erkundest und nach Bereichen suchst, die verbessert werden müssen.

Wenn du planst, an einer größeren Funktion zu arbeiten, erstelle bitte zuerst eine [Funktionsanfrage](https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop), damit wir besprechen können, ob sie mit der Vision von Cline übereinstimmt.

## Entwicklungsumgebung einrichten

1. **VS Code Erweiterungen**

    - Beim Öffnen des Projekts wird VS Code dich auffordern, die empfohlenen Erweiterungen zu installieren
    - Diese Erweiterungen sind für die Entwicklung erforderlich, bitte akzeptiere alle Installationsanfragen
    - Wenn du die Anfragen abgelehnt hast, kannst du sie manuell im Erweiterungsbereich installieren

2. **Lokale Entwicklung**
    - Führe `npm run install:all` aus, um die Abhängigkeiten zu installieren
    - Führe `npm run test` aus, um die Tests lokal auszuführen
    - Bevor du einen PR einreichst, führe `npm run format:fix` aus, um deinen Code zu formatieren

## Code schreiben und einreichen

Jeder kann Code zu Cline beitragen, aber wir bitten dich, diese Richtlinien zu befolgen, um sicherzustellen, dass deine Beiträge reibungslos integriert werden:

1. **Pull Requests fokussiert halten**

    - Begrenze PRs auf eine einzelne Funktion oder Fehlerbehebung
    - Teile größere Änderungen in kleinere, kohärente PRs auf
    - Teile Änderungen in logische Commits auf, die unabhängig überprüft werden können

2. **Codequalität**

    - Führe `npm run lint` aus, um den Code-Stil zu überprüfen
    - Führe `npm run format` aus, um den Code automatisch zu formatieren
    - Alle PRs müssen die CI-Prüfungen bestehen, die Linting und Formatierung umfassen
    - Behebe alle ESLint-Warnungen oder -Fehler, bevor du einreichst
    - Befolge die Best Practices für TypeScript und halte die Typensicherheit ein

3. **Tests**

    - Füge Tests für neue Funktionen hinzu
    - Führe `npm test` aus, um sicherzustellen, dass alle Tests bestehen
    - Aktualisiere bestehende Tests, wenn deine Änderungen sie beeinflussen
    - Füge sowohl Unit- als auch Integrationstests hinzu, wo es angebracht ist

4. **Commit-Richtlinien**

    - Schreibe klare und beschreibende Commit-Nachrichten
    - Verwende das konventionelle Commit-Format (z.B. "feat:", "fix:", "docs:")
    - Verweise auf relevante Issues in den Commits mit #Issue-Nummer

5. **Vor dem Einreichen**

    - Rebase deinen Branch mit dem neuesten Main
    - Stelle sicher, dass dein Branch korrekt gebaut wird
    - Überprüfe, dass alle Tests bestehen
    - Überprüfe deine Änderungen, um jeglichen Debug-Code oder Konsolenprotokolle zu entfernen

6. **Beschreibung des Pull Requests**
    - Beschreibe klar, was deine Änderungen bewirken
    - Füge Schritte hinzu, um die Änderungen zu testen
    - Liste alle wichtigen Änderungen auf
    - Füge Screenshots für Änderungen an der Benutzeroberfläche hinzu

## Beitragsvereinbarung

Durch das Einreichen eines Pull Requests erklärst du dich damit einverstanden, dass deine Beiträge unter derselben Lizenz wie das Projekt ([Apache 2.0](LICENSE)) lizenziert werden.

Denke daran: Zu Cline beizutragen bedeutet nicht nur, Code zu schreiben, sondern Teil einer Community zu sein, die die Zukunft der KI-gestützten Entwicklung gestaltet. Lass uns gemeinsam etwas Großartiges schaffen! 🚀


## Links discovered
- [Verhaltenskodex](https://github.com/cline/cline/blob/main/locales/de/CODE_OF_CONDUCT.md)
- [bestehenden Probleme](https://github.com/cline/cline/issues)
- [Issues-Seite](https://github.com/cline/cline/issues/new/choose)
- ["good first issue"](https://github.com/cline/cline/labels/good%20first%20issue)
- ["help wanted"](https://github.com/cline/cline/labels/help%20wanted)
- [Dokumentation](https://github.com/cline/cline/tree/main/docs)
- [Funktionsanfrage](https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop)
- [Apache 2.0](https://github.com/cline/cline/blob/main/locales/de/LICENSE.md)
- [GitHub-Sicherheitstool, um sie privat zu melden](https://github.com/cline/cline/security/advisories/new)

--- locales/es/CONTRIBUTING.md ---
# Contribuir a Cline

Nos alegra que estés interesado en contribuir a Cline. Ya sea que corrijas un error, añadas una función o mejores nuestra documentación, ¡cada contribución hace que Cline sea más inteligente! Para mantener nuestra comunidad viva y acogedora, todos los miembros deben cumplir con nuestro [Código de Conducta](CODE_OF_CONDUCT.md).

## Informar de errores o problemas

¡Los informes de errores ayudan a mejorar Cline para todos! Antes de crear un nuevo problema, por favor revisa los [problemas existentes](https://github.com/cline/cline/issues) para evitar duplicados. Cuando estés listo para informar un error, dirígete a nuestra [página de Issues](https://github.com/cline/cline/issues/new/choose), donde encontrarás una plantilla que te ayudará a completar la información relevante.

<blockquote class='warning-note'>
    🔐 <b>Importante:</b> Si descubres una vulnerabilidad de seguridad, utiliza la <a href="https://github.com/cline/cline/security/advisories/new">herramienta de seguridad de GitHub para informarla de manera privada</a>.
</blockquote>

## Decidir en qué trabajar

¿Buscas una buena primera contribución? Revisa los issues etiquetados con ["good first issue"](https://github.com/cline/cline/labels/good%20first%20issue) o ["help wanted"](https://github.com/cline/cline/labels/help%20wanted). ¡Estos están especialmente seleccionados para nuevos colaboradores y son áreas donde nos encantaría recibir ayuda!

También damos la bienvenida a contribuciones a nuestra [documentación](https://github.com/cline/cline/tree/main/docs). Ya sea corrigiendo errores tipográficos, mejorando guías existentes o creando nuevos contenidos educativos, queremos construir un repositorio de recursos gestionado por la comunidad que ayude a todos a sacar el máximo provecho de Cline. Puedes comenzar explorando `/docs` y buscando áreas que necesiten mejoras.

Si planeas trabajar en una función más grande, por favor crea primero una [solicitud de función](https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop) para que podamos discutir si se alinea con la visión de Cline.

## Configurar el entorno de desarrollo

1. **Extensiones de VS Code**

    - Al abrir el proyecto, VS Code te pedirá que instales las extensiones recomendadas
    - Estas extensiones son necesarias para el desarrollo, por favor acepta todas las solicitudes de instalación
    - Si rechazaste las solicitudes, puedes instalarlas manualmente en la sección de extensiones

2. **Desarrollo local**
    - Ejecuta `npm run install:all` para instalar las dependencias
    - Ejecuta `npm run test` para ejecutar las pruebas localmente
    - Antes de enviar un PR, ejecuta `npm run format:fix` para formatear tu código

## Escribir y enviar código

Cualquiera puede contribuir código a Cline, pero te pedimos que sigas estas pautas para asegurar que tus contribuciones se integren sin problemas:

1. **Mantén los Pull Requests enfocados**

    - Limita los PRs a una sola función o corrección de errores
    - Divide los cambios más grandes en PRs más pequeños y coherentes
    - Divide los cambios en commits lógicos que puedan ser revisados independientemente

2. **Calidad del código**

    - Ejecuta `npm run lint` para verificar el estilo del código
    - Ejecuta `npm run format` para formatear el código automáticamente
    - Todos los PRs deben pasar las verificaciones de CI, que incluyen linting y formateo
    - Corrige todas las advertencias o errores de ESLint antes de enviar
    - Sigue las mejores prácticas para TypeScript y mantén la seguridad de tipos

3. **Pruebas**

    - Añade pruebas para nuevas funciones
    - Ejecuta `npm test` para asegurarte de que todas las pruebas pasen
    - Actualiza las pruebas existentes si tus cambios las afectan
    - Añade tanto pruebas unitarias como de integración donde sea apropiado

4. **Pautas de commits**

    - Escribe mensajes de commit claros y descriptivos
    - Usa el formato de commit convencional (por ejemplo, "feat:", "fix:", "docs:")
    - Haz referencia a los issues relevantes en los commits con #número-del-issue

5. **Antes de enviar**

    - Rebasea tu rama con el último Main
    - Asegúrate de que tu rama se construya correctamente
    - Verifica que todas las pruebas pasen
    - Revisa tus cambios para eliminar cualquier código de depuración o registros de consola

6. **Descripción del Pull Request**
    - Describe claramente lo que hacen tus cambios
    - Añade pasos para probar los cambios
    - Enumera cualquier cambio importante
    - Añade capturas de pantalla para cambios en la interfaz de usuario

## Acuerdo de contribución

Al enviar un Pull Request, aceptas que tus contribuciones se licencien bajo la misma licencia que el proyecto ([Apache 2.0](LICENSE)).

Recuerda: Contribuir a Cline no solo significa escribir código, sino ser parte de una comunidad que está dando forma al futuro del desarrollo asistido por IA. ¡Hagamos algo grandioso juntos! 🚀


## Links discovered
- [Código de Conducta](https://github.com/cline/cline/blob/main/locales/es/CODE_OF_CONDUCT.md)
- [problemas existentes](https://github.com/cline/cline/issues)
- [página de Issues](https://github.com/cline/cline/issues/new/choose)
- ["good first issue"](https://github.com/cline/cline/labels/good%20first%20issue)
- ["help wanted"](https://github.com/cline/cline/labels/help%20wanted)
- [documentación](https://github.com/cline/cline/tree/main/docs)
- [solicitud de función](https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop)
- [Apache 2.0](https://github.com/cline/cline/blob/main/locales/es/LICENSE.md)
- [herramienta de seguridad de GitHub para informarla de manera privada](https://github.com/cline/cline/security/advisories/new)

--- locales/ja/CONTRIBUTING.md ---
# Cline

Clineへの貢献に興味をお持ちいただきありがとうございます。

## バグや問題の報告

バグ報告は、Clineを皆さんにとってより良いものにするために役立ちます！新しい問題を作成する前に、重複を避けるために[既存の問題を検索](https://github.com/cline/cline/issues)してください。バグを報告する準備ができたら、[問題ページ](https://github.com/cline/cline/issues/new/choose)に移動し、関連情報を記入するためのテンプレートをご利用ください。

<blockquote class='warning-note'>
    🔐 <b>重要:</b> セキュリティ脆弱性を発見した場合は、<a href="https://github.com/cline/cline/security/advisories/new">Githubセキュリティツールを使用して非公開で報告</a>してください。
</blockquote>

## 作業内容の決定

最初の貢献をお探しですか？["good first issue"](https://github.com/cline/cline/labels/good%20first%20issue)や["help wanted"](https://github.com/cline/cline/labels/help%20wanted)のラベルが付いた問題をチェックしてください。これらは新しい貢献者向けに特に選ばれたもので、私たちが助けを求めている分野です！

また、[ドキュメント](https://github.com/cline/cline/tree/main/docs)への貢献も歓迎します！誤字の修正、既存のガイドの改善、新しい教育コンテンツの作成など、コミュニティ主導のリソースリポジトリを構築するために皆さんの力をお借りしたいと考えています。`/docs`に飛び込んで、改善が必要な箇所を探してみてください。

大きな機能に取り組む予定がある場合は、まず[機能リクエスト](https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop)を作成し、それがClineのビジョンに合致するかどうかを議論しましょう。

## 開発環境のセットアップ

1. **VS Code拡張機能**

    - プロジェクトを開くと、VS Codeは推奨される拡張機能のインストールを促します
    - これらの拡張機能は開発に必要です - すべてのインストールプロンプトを受け入れてください
    - プロンプトを閉じた場合は、拡張機能パネルから手動でインストールできます

2. **ローカル開発**
    - `npm run install:all`を実行して依存関係をインストールします
    - `npm run test`を実行してローカルでテストを実行します
    - PRを提出する前に、`npm run format:fix`を実行してコードをフォーマットします

## コードの作成と提出

誰でもClineにコードを貢献できますが、貢献がスムーズに統合されるように以下のガイドラインに従ってください：

1. **プルリクエストを集中させる**

    - PRは単一の機能またはバグ修正に限定してください
    - 大きな変更は小さな関連PRに分割してください
    - 論理的なコミットに分けて、独立してレビューできるようにしてください

2. **コード品質**

    - `npm run lint`を実行してコードスタイルをチェックします
    - `npm run format`を実行してコードを自動的にフォーマットします
    - すべてのPRは、リンティングとフォーマットを含むCIチェックに合格する必要があります
    - 提出前にESLintの警告やエラーをすべて解決してください
    - TypeScriptのベストプラクティスに従い、型の安全性を維持してください

3. **テスト**

    - 新しい機能にはテストを追加してください
    - `npm test`を実行してすべてのテストが合格することを確認してください
    - 変更が既存のテストに影響を与える場合は、それらを更新してください
    - 適切な場合には、ユニットテストと統合テストの両方を含めてください

4. **コミットガイドライン**

    - 明確で説明的なコミットメッセージを書いてください
    - 従来のコミット形式（例："feat:", "fix:", "docs:"）を使用してください
    - コミットで関連する問題を#issue-numberを使用して参照してください

5. **提出前に**

    - 最新のmainにブランチをリベースしてください
    - ブランチが正常にビルドされることを確認してください
    - すべてのテストが合格していることを再確認してください
    - デバッグコードやコンソールログがないか変更を確認してください

6. **プルリクエストの説明**
    - 変更内容を明確に説明してください
    - 変更をテストする手順を含めてください
    - 破壊的な変更がある場合はリストしてください
    - UIの変更にはスクリーンショットを追加してください

## 貢献契約

プルリクエストを提出することで、あなたの貢献がプロジェクトと同じライセンス（[Apache 2.0](LICENSE)）の下でライセンスされることに同意したことになります。

覚えておいてください：Clineへの貢献はコードを書くことだけではなく、AI支援開発の未来を形作るコミュニティの一員になることです。一緒に素晴らしいものを作りましょう！🚀


## Links discovered
- [既存の問題を検索](https://github.com/cline/cline/issues)
- [問題ページ](https://github.com/cline/cline/issues/new/choose)
- ["good first issue"](https://github.com/cline/cline/labels/good%20first%20issue)
- ["help wanted"](https://github.com/cline/cline/labels/help%20wanted)
- [ドキュメント](https://github.com/cline/cline/tree/main/docs)
- [機能リクエスト](https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop)
- [Apache 2.0](https://github.com/cline/cline/blob/main/locales/ja/LICENSE.md)
- [Githubセキュリティツールを使用して非公開で報告](https://github.com/cline/cline/security/advisories/new)

--- locales/ko/CONTRIBUTING.md ---
# Cline

Cline에 기여하는 것에 관심을 가져주셔서 감사합니다! 버그 수정, 기능 추가, 문서 개선 등 모든 기여는 Cline을 더욱 스마트하게 만드는 데 기여합니다. 활기차고 환영하는 커뮤니티를 유지하기 위해 모든 구성원은 [행동 강령](CODE_OF_CONDUCT.md)을 준수해야 합니다.

## 버그와 문제 보고

버그 보고는 Cline을 모두에게 더 나은 것으로 만드는 데 도움이 됩니다! 새로운 이슈를 생성하기 전에, 중복을 피하기 위해 [기존 이슈를 검색](https://github.com/cline/cline/issues)해 주세요. 버그를 보고할 준비가 되었다면, [이슈 페이지](https://github.com/cline/cline/issues/new/choose)로 이동하여 관련 정보를 작성하기 위한 템플릿을 사용해 주세요.

<blockquote class='warning-note'>
    🔐 <b>중요:</b> 보안 취약점을 발견한 경우, <a href="https://github.com/cline/cline/security/advisories/new">GitHub 보안 도구를 사용하여 비공개로 보고</a>해 주세요.
</blockquote>

## 작업 내용 결정하기

첫 기여를 찾고 계신가요? ["good first issue"](https://github.com/cline/cline/labels/good%20first%20issue)나 ["help wanted"](https://github.com/cline/cline/labels/help%20wanted) 라벨이 붙은 이슈를 확인해 보세요. 이러한 이슈들은 새로운 기여자를 위해 특별히 선정된 작업으로, 도움이 필요한 영역이 표시되어 있습니다!

또한, [문서](https://github.com/cline/cline/tree/main/docs)에 대한 기여도 환영합니다! 오타 수정, 기존 가이드 개선, 새로운 교육 콘텐츠 작성 등, 커뮤니티 주도의 리소스 저장소를 구축하는 데 여러분의 도움이 필요합니다. `/docs`를 살펴보고 개선이 필요한 부분을 찾아보세요.

큰 기능에 대해 작업할 계획이 있다면, 먼저 [기능 요청](https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop)을 생성하여 이것이 Cline의 비전과 부합하는지 논의하는 것이 좋습니다.

## 개발 환경 설정

1. **VS Code 확장 프로그램**

    - 프로젝트를 열면 VS Code가 권장 확장 프로그램 설치를 안내합니다
    - 개발을 위해 이 확장 프로그램들이 필요하므로, 설치 안내를 수락해 주세요.
    - 프롬프트를 닫은 경우 확장 프로그램 패널에서 수동으로 설치할 수 있습니다

2. **로컬 개발**
    - `npm run install:all`을 실행하여 의존성을 설치합니다
    - `npm run test`를 실행하여 로컬에서 테스트를 실행합니다
    - PR을 제출하기 전에 `npm run format:fix`를 실행하여 코드를 포맷팅합니다

## 코드 작성과 제출

누구나 Cline에 코드를 기여할 수 있지만, 기여가 원활하게 통합되도록 다음 가이드라인을 따라주세요:

1. **Pull Request 집중하기**

    - PR은 단일 기능 또는 버그 수정으로 제한해 주세요
    - 큰 변경사항은 작은 관련 PR로 분할해 주세요
    - 논리적으로 독립적인 커밋 단위로 나누어 리뷰가 용이하도록 구성하세요.

2. **코드 품질**

    - `npm run lint`를 실행하여 코드 스타일을 체크합니다
    - `npm run format`을 실행하여 코드를 자동으로 포맷팅합니다
    - 모든 PR은 린팅과 포맷팅을 포함한 CI 체크를 통과해야 합니다
    - 제출 전에 ESLint 경고나 에러를 모두 해결해 주세요
    - TypeScript 모범 사례를 따르고, 타입 안전성을 유지해 주세요

3. **테스트**

    - 새로운 기능에는 테스트를 추가해 주세요
    - `npm test`를 실행하여 모든 테스트가 통과하는지 확인해 주세요
    - 변경사항이 기존 테스트에 영향을 미치는 경우 해당 테스트를 업데이트해 주세요
    - 적절한 경우 단위 테스트와 통합 테스트를 모두 포함해 주세요

4. **Changesets를 활용한 버전 관리**

    - 사용자에게 영향을 미치는 변경 사항이 있는 경우, `npm run changeset`을 실행하여 changeset을 생성해 주세요
    - 적절한 버전 증가 옵션을 선택하세요:
        - `major` 호환되지 않는 변경 (1.0.0 → 2.0.0)
        - `minor` 새로운 기능 추가 (1.0.0 → 1.1.0)
        - `patch` 버그 수정 (1.0.0 → 1.0.1)
    - 영향을 설명하는 명확한 변경사항 메시지를 작성해 주세요
    - 문서 변경만 있는 경우 changeset이 필요하지 않습니다

5. **커밋 가이드라인**

    - 명확하고 설명적인 커밋 메시지를 작성해 주세요
    - 컨벤셔널 커밋 형식(예: "feat:", "fix:", "docs:")을 사용해 주세요
    - 커밋에서 관련 이슈를 #issue-number를 사용하여 참조해 주세요

6. **제출 전 확인사항**

    - 최신 main에 브랜치를 리베이스해 주세요
    - 브랜치가 정상적으로 빌드되는지 확인해 주세요
    - 모든 테스트가 통과하는지 다시 확인해 주세요
    - 디버그 코드나 콘솔 로그가 없는지 변경사항을 확인해 주세요

7. **Pull Request 설명**
    - 변경 내용을 명확하게 설명해 주세요
    - 변경사항을 테스트하는 방법을 포함해 주세요
    - 호환되지 않는 변경 사항이 있다면 목록으로 작성해주세요
    - UI 변경이 있는 경우, 스크린샷을 추가해 주세요

## 기여 동의서

Pull Request를 제출함으로써, 귀하의 기여가 프로젝트와 동일한 라이선스([Apache 2.0](/LICENSE)) 에 따라 제공됨에 동의하는 것입니다.

기억하세요: Cline에 기여하는 것은 코드를 작성하는 것뿐만 아니라, AI 지원 개발의 미래를 형성하는 커뮤니티의 일원이 되는 것입니다. 함께 멋진 것을 만들어봅시다! 🚀


## Links discovered
- [행동 강령](https://github.com/cline/cline/blob/main/locales/ko/CODE_OF_CONDUCT.md)
- [기존 이슈를 검색](https://github.com/cline/cline/issues)
- [이슈 페이지](https://github.com/cline/cline/issues/new/choose)
- ["good first issue"](https://github.com/cline/cline/labels/good%20first%20issue)
- ["help wanted"](https://github.com/cline/cline/labels/help%20wanted)
- [문서](https://github.com/cline/cline/tree/main/docs)
- [기능 요청](https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop)
- [Apache 2.0](https://github.com/cline/cline/blob/main/LICENSE.md)
- [GitHub 보안 도구를 사용하여 비공개로 보고](https://github.com/cline/cline/security/advisories/new)

--- .cline/skills/create-pull-request/SKILL.md ---
---
name: create-pull-request
description: Create a GitHub pull request following project conventions. Use when the user asks to create a PR, submit changes for review, or open a pull request. Handles commit analysis, branch management, and PR creation using the gh CLI tool.
---

# Create Pull Request

This skill guides you through creating a well-structured GitHub pull request that follows project conventions and best practices.

## Prerequisites Check

Before proceeding, verify the following:

### 1. Check if `gh` CLI is installed

```bash
gh --version
```

If not installed, inform the user:
> The GitHub CLI (`gh`) is required but not installed. Please install it:
> - macOS: `brew install gh`
> - Other: https://cli.github.com/

### 2. Check if authenticated with GitHub

```bash
gh auth status
```

If not authenticated, guide the user to run `gh auth login`.

### 3. Verify clean working directory

```bash
git status
```

If there are uncommitted changes, ask the user whether to:
- Commit them as part of this PR
- Stash them temporarily
- Discard them (with caution)

## Gather Context

### 1. Identify the current branch

```bash
git branch --show-current
```

Ensure you're not on `main` or `master`. If so, ask the user to create or switch to a feature branch.

### 2. Find the base branch

```bash
git remote show origin | grep "HEAD branch"
```

This is typically `main` or `master`.

### 3. Analyze recent commits relevant to this PR

```bash
git log origin/main..HEAD --oneline --no-decorate
```

Review these commits to understand:
- What changes are being introduced
- The scope of the PR (single feature/fix or multiple changes)
- Whether commits should be squashed or reorganized

### 4. Review the diff

```bash
git diff origin/main..HEAD --stat
```

This shows which files changed and helps identify the type of change.

## Information Gathering

Before creating the PR, you need the following information. Check if it can be inferred from:
- Commit messages
- Branch name (e.g., `fix/issue-123`, `feature/new-login`)
- Changed files and their content

If any critical information is missing, use `ask_followup_question` to ask the user:

### Required Information

1. **Related Issue Number**: Look for patterns like `#123`, `fixes #123`, or `closes #123` in commit messages
2. **Description**: What problem does this solve? Why were these changes made?
3. **Type of Change**: Bug fix, new feature, breaking change, refactor, cosmetic, documentation, or workflow
4. **Test Procedure**: How was this tested? What could break?

### Example clarifying question

If the issue number is not found:
> I couldn't find a related issue number in the commit messages or branch name. What GitHub issue does this PR address? (Enter the issue number, e.g., "123" or "N/A" for small fixes)

## Git Best Practices

Before creating the PR, consider these best practices:

### Commit Hygiene

1. **Atomic commits**: Each commit should represent a single logical change
2. **Clear commit messages**: Follow conventional commit format when possible
3. **No merge commits**: Prefer rebasing over merging to keep history clean

### Branch Management

1. **Rebase on latest main** (if needed):
   ```bash
   git fetch origin
   git rebase origin/main
   ```

2. **Squash if appropriate**: If there are many small "WIP" commits, consider interactive rebase:
   ```bash
   git rebase -i origin/main
   ```
   Only suggest this if commits appear messy and the user is comfortable with rebasing.

### Push Changes

Ensure all commits are pushed:
```bash
git push origin HEAD
```

If the branch was rebased, you may need:
```bash
git push origin HEAD --force-with-lease
```

## Create the Pull Request

**IMPORTANT**: Read and use the PR template at `.github/pull_request_template.md`. The PR body format must **strictly match** the template structure. Do not deviate from the template format.

When filling out the template:
- Replace `#XXXX` with the actual issue number, or keep as `#XXXX` if no issue exists (for small fixes)
- Fill in all sections with relevant information gathered from commits and context
- Mark the appropriate "Type of Change" checkbox(es)
- Complete the "Pre-flight Checklist" items that apply

### Create PR with gh CLI

**Use a temporary file for the PR body** to avoid shell escaping issues, newline problems, and other command-line flakiness:

1. Write the PR body to a temporary file:
   ```
   /tmp/pr-body.md
   ```

2. Create the PR using the file:
   ```bash
   gh pr create --title "PR_TITLE" --body-file /tmp/pr-body.md --base main
   ```

3. Clean up the temporary file:
   ```bash
   rm /tmp/pr-body.md
   ```

For draft PRs:
```bash
gh pr create --title "PR_TITLE" --body-file /tmp/pr-body.md --base main --draft
```

**Why use a file?** Passing complex markdown with newlines, special characters, and checkboxes directly via `--body` is error-prone. The `--body-file` flag handles all content reliably.

## Post-Creation

After creating the PR:

1. **Display the PR URL** so the user can review it
2. **Remind about CI checks**: Tests and linting will run automatically
3. **Suggest next steps**:
   - Add reviewers if needed: `gh pr edit --add-reviewer USERNAME`
   - Add labels if needed: `gh pr edit --add-label "bug"`

## Error Handling

### Common Issues

1. **No commits ahead of main**: The branch has no changes to submit
   - Ask if the user meant to work on a different branch

2. **Branch not pushed**: Remote doesn't have the branch
   - Push the branch first: `git push -u origin HEAD`

3. **PR already exists**: A PR for this branch already exists
   - Show the existing PR: `gh pr view`
   - Ask if they want to update it instead

4. **Merge conflicts**: Branch conflicts with base
   - Guide user through resolving conflicts or rebasing

## Summary Checklist

Before finalizing, ensure:
- [ ] `gh` CLI is installed and authenticated
- [ ] Working directory is clean
- [ ] All commits are pushed
- [ ] Branch is up-to-date with base branch
- [ ] Related issue number is identified, or placeholder is used
- [ ] PR description follows the template exactly
- [ ] Appropriate type of change is selected
- [ ] Pre-flight checklist items are addressed

--- CLAUDE.md ---
@.clinerules/general.md
@.clinerules/network.md
@.clinerules/cli.md


--- CODE_OF_CONDUCT.md ---
# Contributor Covenant Code of Conduct

## Our Pledge

In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to making participation in our project and
our community a harassment-free experience for everyone, regardless of age, body
size, disability, ethnicity, sex characteristics, gender identity and expression,
level of experience, education, socio-economic status, nationality, personal
appearance, race, religion, or sexual identity and orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment
include:

-   Using welcoming and inclusive language
-   Being respectful of differing viewpoints and experiences
-   Gracefully accepting constructive criticism
-   Focusing on what is best for the community
-   Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

-   The use of sexualized language or imagery and unwelcome sexual attention or
    advances
-   Trolling, insulting/derogatory comments, and personal or political attacks
-   Public or private harassment
-   Publishing others' private information, such as a physical or electronic
    address, without explicit permission
-   Other conduct which could reasonably be considered inappropriate in a
    professional setting

## Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable
behavior and are expected to take appropriate and fair corrective action in
response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or
reject comments, commits, code, wiki edits, issues, and other contributions
that are not aligned to this Code of Conduct, or to ban temporarily or
permanently any contributor for other behaviors that they deem inappropriate,
threatening, offensive, or harmful.

## Scope

This Code of Conduct applies both within project spaces and in public spaces
when an individual is representing the project or its community. Examples of
representing a project or community include using an official project e-mail
address, posting via an official social media account, or acting as an appointed
representative at an online or offline event. Representation of a project may be
further defined and clarified by project maintainers.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by contacting the project team at hi@cline.bot. All complaints
will be reviewed and investigated and will result in a response that
is deemed necessary and appropriate to the circumstances. The project team is
obligated to maintain confidentiality with regard to the reporter of an incident.
Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good
faith may face temporary or permanent repercussions as determined by other
members of the project's leadership.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,
available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see
https://www.contributor-covenant.org/faq


--- README.md ---
<div align="center"><sub>
English | <a href="https://github.com/cline/cline/blob/main/locales/es/README.md" target="_blank">Español</a> | <a href="https://github.com/cline/cline/blob/main/locales/de/README.md" target="_blank">Deutsch</a> | <a href="https://github.com/cline/cline/blob/main/locales/ja/README.md" target="_blank">日本語</a> | <a href="https://github.com/cline/cline/blob/main/locales/zh-cn/README.md" target="_blank">简体中文</a> | <a href="https://github.com/cline/cline/blob/main/locales/zh-tw/README.md" target="_blank">繁體中文</a> | <a href="https://github.com/cline/cline/blob/main/locales/ko/README.md" target="_blank">한국어</a>
</sub></div>

# Cline

<p align="center">
  <img src="https://media.githubusercontent.com/media/cline/cline/main/assets/docs/demo.gif" width="100%" />
</p>

<div align="center">
<table>
<tbody>
<td align="center">
<a href="https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev" target="_blank"><strong>Download on VS Marketplace</strong></a>
</td>
<td align="center">
<a href="https://discord.gg/cline" target="_blank"><strong>Discord</strong></a>
</td>
<td align="center">
<a href="https://www.reddit.com/r/cline/" target="_blank"><strong>r/cline</strong></a>
</td>
<td align="center">
<a href="https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop" target="_blank"><strong>Feature Requests</strong></a>
</td>
<td align="center">
<a href="https://docs.cline.bot/getting-started/for-new-coders" target="_blank"><strong>Getting Started</strong></a>
</td>
</tbody>
</table>
</div>

Meet Cline, an AI assistant that can use your **CLI** a**N**d **E**ditor.

Thanks to [Claude Sonnet's agentic coding capabilities](https://www.anthropic.com/claude/sonnet), Cline can handle complex software development tasks step-by-step. With tools that let him create & edit files, explore large projects, use the browser, and execute terminal commands (after you grant permission), he can assist you in ways that go beyond code completion or tech support. Cline can even use the Model Context Protocol (MCP) to create new tools and extend his own capabilities. While autonomous AI scripts traditionally run in sandboxed environments, this extension provides a human-in-the-loop GUI to approve every file change and terminal command, providing a safe and accessible way to explore the potential of agentic AI.

1. Enter your task and add images to convert mockups into functional apps or fix bugs with screenshots.
2. Cline starts by analyzing your file structure & source code ASTs, running regex searches, and reading relevant files to get up to speed in existing projects. By carefully managing what information is added to context, Cline can provide valuable assistance even for large, complex projects without overwhelming the context window.
3. Once Cline has the information he needs, he can:
    - Create and edit files + monitor linter/compiler errors along the way, letting him proactively fix issues like missing imports and syntax errors on his own.
    - Execute commands directly in your terminal and monitor their output as he works, letting him e.g., react to dev server issues after editing a file.
    - For web development tasks, Cline can launch the site in a headless browser, click, type, scroll, and capture screenshots + console logs, allowing him to fix runtime errors and visual bugs.
4. When a task is completed, Cline will present the result to you with a terminal command like `open -a "Google Chrome" index.html`, which you run with a click of a button.

> [!TIP]
> Follow [this guide](https://docs.cline.bot/features/customization/opening-cline-in-sidebar) to open Cline on the right side of your editor. This lets you use Cline side-by-side with your file explorer, and see how he changes your workspace more clearly.

---

<img align="right" width="340" src="https://github.com/user-attachments/assets/3cf21e04-7ce9-4d22-a7b9-ba2c595e88a4">

### Use any API and Model

Cline supports API providers like OpenRouter, Anthropic, OpenAI, Google Gemini, AWS Bedrock, Azure, GCP Vertex, Cerebras and Groq. You can also configure any OpenAI compatible API, or use a local model through LM Studio/Ollama. If you're using OpenRouter, the extension fetches their latest model list, allowing you to use the newest models as soon as they're available.

The extension also keeps track of total tokens and API usage cost for the entire task loop and individual requests, keeping you informed of spend every step of the way.

<!-- Transparent pixel to create line break after floating image -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="370" src="https://github.com/user-attachments/assets/81be79a8-1fdb-4028-9129-5fe055e01e76">

### Run Commands in Terminal

Thanks to the new [shell integration updates in VSCode v1.93](https://code.visualstudio.com/updates/v1_93#_terminal-shell-integration-api), Cline can execute commands directly in your terminal and receive the output. This allows him to perform a wide range of tasks, from installing packages and running build scripts to deploying applications, managing databases, and executing tests, all while adapting to your dev environment & toolchain to get the job done right.

For long running processes like dev servers, use the "Proceed While Running" button to let Cline continue in the task while the command runs in the background. As Cline works he’ll be notified of any new terminal output along the way, letting him react to issues that may come up, such as compile-time errors when editing files.

<!-- Transparent pixel to create line break after floating image -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="400" src="https://github.com/user-attachments/assets/c5977833-d9b8-491e-90f9-05f9cd38c588">

### Create and Edit Files

Cline can create and edit files directly in your editor, presenting you a diff view of the changes. You can edit or revert Cline's changes directly in the diff view editor, or provide feedback in chat until you're satisfied with the result. Cline also monitors linter/compiler errors (missing imports, syntax errors, etc.) so he can fix issues that come up along the way on his own.

All changes made by Cline are recorded in your file's Timeline, providing an easy way to track and revert modifications if needed.

<!-- Transparent pixel to create line break after floating image -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="370" src="https://github.com/user-attachments/assets/bc2e85ba-dfeb-4fe6-9942-7cfc4703cbe5">

### Use the Browser

With Claude Sonnet's new [Computer Use](https://www.anthropic.com/news/3-5-models-and-computer-use) capability, Cline can launch a browser, click elements, type text, and scroll, capturing screenshots and console logs at each step. This allows for interactive debugging, end-to-end testing, and even general web use! This gives him autonomy to fixing visual bugs and runtime issues without you needing to handhold and copy-pasting error logs yourself.

Try asking Cline to "test the app", and watch as he runs a command like `npm run dev`, launches your locally running dev server in a browser, and performs a series of tests to confirm that everything works. [See a demo here.](https://x.com/sdrzn/status/1850880547825823989)

<!-- Transparent pixel to create line break after floating image -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="350" src="https://github.com/user-attachments/assets/ac0efa14-5c1f-4c26-a42d-9d7c56f5fadd">

### "add a tool that..."

Thanks to the [Model Context Protocol](https://github.com/modelcontextprotocol), Cline can extend his capabilities through custom tools. While you can use [community-made servers](https://github.com/modelcontextprotocol/servers), Cline can instead create and install tools tailored to your specific workflow. Just ask Cline to "add a tool" and he will handle everything, from creating a new MCP server to installing it into the extension. These custom tools then become part of Cline's toolkit, ready to use in future tasks.

-   "add a tool that fetches Jira tickets": Retrieve ticket ACs and put Cline to work
-   "add a tool that manages AWS EC2s": Check server metrics and scale instances up or down
-   "add a tool that pulls the latest PagerDuty incidents": Fetch details and ask Cline to fix bugs

<!-- Transparent pixel to create line break after floating image -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="360" src="https://github.com/user-attachments/assets/7fdf41e6-281a-4b4b-ac19-020b838b6970">

### Add Context

**`@url`:** Paste in a URL for the extension to fetch and convert to markdown, useful when you want to give Cline the latest docs

**`@problems`:** Add workspace errors and warnings ('Problems' panel) for Cline to fix

**`@file`:** Adds a file's contents so you don't have to waste API requests approving read file (+ type to search files)

**`@folder`:** Adds folder's files all at once to speed up your workflow even more

<!-- Transparent pixel to create line break after floating image -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="350" src="https://github.com/user-attachments/assets/140c8606-d3bf-41b9-9a1f-4dbf0d4c90cb">

### Checkpoints: Compare and Restore

As Cline works through a task, the extension takes a snapshot of your workspace at each step. You can use the 'Compare' button to see a diff between the snapshot and your current workspace, and the 'Restore' button to roll back to that point.

For example, when working with a local web server, you can use 'Restore Workspace Only' to quickly test different versions of your app, then use 'Restore Task and Workspace' when you find the version you want to continue building from. This lets you safely explore different approaches without losing progress.

<!-- Transparent pixel to create line break after floating image -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

## Contributing

To contribute to the project, start with our [Contributing Guide](CONTRIBUTING.md) to learn the basics. You can also join our [Discord](https://discord.gg/cline) to chat with other contributors in the `#contributors` channel. If you're looking for full-time work, check out our open positions on our [careers page](https://cline.bot/join-us)!

## Enterprise

Get the same Cline experience with enterprise-grade controls: SSO (SAML/OIDC), global policies and configuration, observability with audit trails, private networking (VPC/private link), and self-hosted or on-prem deployments, and enterprise support. Learn more at our [enterprise page](https://cline.bot/enterprise) or [talk to us](https://cline.bot/contact-sales).


## License

[Apache 2.0 © 2026 Cline Bot Inc.](./LICENSE)


## Links discovered
- [Claude Sonnet's agentic coding capabilities](https://www.anthropic.com/claude/sonnet)
- [this guide](https://docs.cline.bot/features/customization/opening-cline-in-sidebar)
- [shell integration updates in VSCode v1.93](https://code.visualstudio.com/updates/v1_93#_terminal-shell-integration-api)
- [Computer Use](https://www.anthropic.com/news/3-5-models-and-computer-use)
- [See a demo here.](https://x.com/sdrzn/status/1850880547825823989)
- [Model Context Protocol](https://github.com/modelcontextprotocol)
- [community-made servers](https://github.com/modelcontextprotocol/servers)
- [Contributing Guide](https://github.com/cline/cline/blob/main/CONTRIBUTING.md)
- [Discord](https://discord.gg/cline)
- [careers page](https://cline.bot/join-us)
- [enterprise page](https://cline.bot/enterprise)
- [talk to us](https://cline.bot/contact-sales)
- [Apache 2.0 © 2026 Cline Bot Inc.](https://github.com/cline/cline/blob/main/LICENSE.md)
- [Español](https://github.com/cline/cline/blob/main/locales/es/README.md)
- [Deutsch](https://github.com/cline/cline/blob/main/locales/de/README.md)
- [日本語](https://github.com/cline/cline/blob/main/locales/ja/README.md)
- [简体中文](https://github.com/cline/cline/blob/main/locales/zh-cn/README.md)
- [繁體中文](https://github.com/cline/cline/blob/main/locales/zh-tw/README.md)
- [한국어](https://github.com/cline/cline/blob/main/locales/ko/README.md)
- [<strong>Download on VS Marketplace</strong>](https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev)
- [<strong>Discord</strong>](https://discord.gg/cline)
- [<strong>r/cline</strong>](https://www.reddit.com/r/cline/)
- [<strong>Feature Requests</strong>](https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop)
- [<strong>Getting Started</strong>](https://docs.cline.bot/getting-started/for-new-coders)

--- playwright.config.ts ---
import { defineConfig } from "@playwright/test"

const isCI = !!process?.env?.CI
const isWindow = process?.platform?.startsWith("win")

export default defineConfig({
	workers: 1,
	retries: 1,
	forbidOnly: isCI,
	testDir: "src/test/e2e",
	testMatch: /.*\.test\.ts/,
	timeout: isCI || isWindow ? 60000 : 20000,
	expect: {
		timeout: isCI || isWindow ? 5000 : 2000,
	},
	fullyParallel: true,
	reporter: isCI ? [["github"], ["list"]] : [["list"]],
	use: {
		video: "retain-on-failure",
	},
	projects: [
		{
			name: "setup test environment",
			testMatch: /global\.setup\.ts/,
		},
		{
			name: "e2e tests",
			dependencies: ["setup test environment"],
		},
	],
})


--- test-setup.js ---
const tsConfigPaths = require("tsconfig-paths")
const fs = require("fs")
const path = require("path")
const Module = require("module")

const baseUrl = path.resolve(__dirname)

const tsConfig = JSON.parse(fs.readFileSync(path.join(baseUrl, "tsconfig.json"), "utf-8"))

/**
 * The aliases point towards the `src` directory.
 * However, `tsc` doesn't compile paths by itself
 * (https://www.typescriptlang.org/docs/handbook/modules/reference.html#paths-does-not-affect-emit)
 * So we need to use tsconfig-paths to resolve the aliases when running tests,
 * but pointing to `out` instead.
 */
const outPaths = {}
Object.keys(tsConfig.compilerOptions.paths).forEach((key) => {
	const value = tsConfig.compilerOptions.paths[key]
	outPaths[key] = value.map((path) => path.replace("src", "out/src"))
})

tsConfigPaths.register({
	baseUrl: baseUrl,
	paths: outPaths,
})

// Mock the @google/genai module to avoid ESM compatibility issues in tests
// The module is ES6 only, but the integration tests are compiled to commonJS.
const originalRequire = Module.prototype.require
Module.prototype.require = function (id) {
	// Intercept requires for @google/genai
	if (id === "@google/genai") {
		// Return the mock instead
		const mockPath = path.join(baseUrl, "out/src/core/api/providers/gemini-mock.test.js")
		return originalRequire.call(this, mockPath)
	}
	return originalRequire.call(this, id)
}


--- .changeset/README.md ---
# Changesets

Hello and welcome! This folder has been automatically generated by `@changesets/cli`, a build tool that works
with multi-package repos, or single-package repos to help you version and publish your code. You can
find the full documentation for it [in our repository](https://github.com/changesets/changesets)

We have a quick list of common questions to get you started engaging with this project in
[our documentation](https://github.com/changesets/changesets/blob/main/docs/common-questions.md)


## Links discovered
- [in our repository](https://github.com/changesets/changesets)
- [our documentation](https://github.com/changesets/changesets/blob/main/docs/common-questions.md)

--- .changeset/five-olives-behave.md ---
---
"cline": patch
---

Allows users to enter custom aws region when selecting bedrock as a provider in CLI


--- .changeset/quick-cycles-appear.md ---
---
"cline": patch
---

Keep reasoning rows visible when low-stakes tool groups start immediately after reasoning.


--- .changeset/tidy-reasoning-traces.md ---
---
"cline": patch
---

Restore reasoning trace visibility in chat and improve the thinking row UX so streamed reasoning is visible, then collapsible after completion.


--- .clinerules/hooks/README.md ---
# Cline Hooks Documentation

## Overview

Cline hooks allow you to execute custom scripts at specific points in the agentic workflow. Hooks can be placed in either:
- **Global hooks directory**: `~/Documents/Cline/Hooks/` (applies to all workspaces)
- **Workspace hooks directory**: `.clinerules/hooks/` (applies to the workspace the repo is part of)

Hooks run automatically when enabled.

## Enabling Hooks

1. Open Cline settings in VSCode
2. Navigate to the Feature Settings section
3. Check the "Enable Hooks" checkbox
4. Hooks must be executable files (on Unix/Linux/macOS use `chmod +x hookname`)

## Available Hooks

### TaskStart Hook
- **When**: Runs when a NEW task is started (not when resuming)
- **Purpose**: Initialize task context, validate task requirements, set up environment
- **Global Location**: `~/Documents/Cline/Hooks/TaskStart`
- **Workspace Location**: `.clinerules/hooks/TaskStart`

### TaskResume Hook
- **When**: Runs when an EXISTING task is resumed (after user clicks resume button)
- **Purpose**: Validate resumed task state, restore context, check for changes since last run
- **Global Location**: `~/Documents/Cline/Hooks/TaskResume`
- **Workspace Location**: `.clinerules/hooks/TaskResume`

### TaskCancel Hook
- **When**: Runs when a task is cancelled or a hook is aborted by the user (only if there's actual active work or work was started)
- **Purpose**: Clean up resources, log cancellation, save state
- **Global Location**: `~/Documents/Cline/Hooks/TaskCancel`
- **Workspace Location**: `.clinerules/hooks/TaskCancel`
- **Note**: This hook is NOT cancellable

### TaskComplete Hook (coming soon!)
- **When**: Runs when a task is marked as complete
- **Purpose**: Log completion status, perform final cleanup, generate reports
- **Global Location**: `~/Documents/Cline/Hooks/TaskComplete`
- **Workspace Location**: `.clinerules/hooks/TaskComplete`

### UserPromptSubmit Hook
- **When**: Runs when the user submits a prompt/message (initial task, resume, or feedback)
- **Purpose**: Validate user input, preprocess prompts, add context to user messages
- **Global Location**: `~/Documents/Cline/Hooks/UserPromptSubmit`
- **Workspace Location**: `.clinerules/hooks/UserPromptSubmit`

### PreToolUse Hook
- **When**: Runs BEFORE a tool is executed
- **Purpose**: Validate parameters, block execution, or add context
- **Global Location**: `~/Documents/Cline/Hooks/PreToolUse`
- **Workspace Location**: `.clinerules/hooks/PreToolUse`

### PostToolUse Hook
- **When**: Runs AFTER a tool completes
- **Purpose**: Observe results, track patterns, or add context
- **Global Location**: `~/Documents/Cline/Hooks/PostToolUse`
- **Workspace Location**: `.clinerules/hooks/PostToolUse`

### PreCompact Hook (coming soon!)
- **When**: Runs BEFORE the conversation context is compacted/truncated
- **Purpose**: Observe compaction events, log context management, track token usage
- **Global Location**: `~/Documents/Cline/Hooks/PreCompact`
- **Workspace Location**: `.clinerules/hooks/PreCompact`

## Cross-Platform Hook Format

Cline uses a git-style approach for hooks that works consistently across all platforms:

### Hook Files (All Platforms)
- **No file extensions**: Hooks are named exactly `PreToolUse` or `PostToolUse` (no `.bat`, `.cmd`, `.sh` etc.)
- **Shebang required**: First line must be a shebang (e.g., `#!/usr/bin/env bash` or `#!/usr/bin/env node`)
- **Executable on Unix**: On Unix/Linux/macOS, hooks must be executable: `chmod +x PreToolUse`
- **Windows**: Not currently supported.

### How It Works

Like git hooks, Cline executes hook files through a shell that interprets the shebang line:
- On Unix/Linux/macOS: Native shell execution with shebang support

This means:
- ✅ Same hook script works on all platforms
- ✅ Write once, run anywhere
- ✅ Use any scripting language (bash, node, python, etc.)

### Creating Hooks

**On Unix/Linux/macOS:**
```bash
# Create hook file
nano ~/Documents/Cline/Hooks/PreToolUse

# Make executable
chmod +x ~/Documents/Cline/Hooks/PreToolUse
```

## Context Injection Timing

**IMPORTANT**: Context injected by hooks affects **FUTURE AI decisions**, not the current tool execution.

### Why This Matters

When a hook runs:
1. The AI has already decided what tool to use and with what parameters
2. The hook cannot modify those parameters
3. Context from the hook is added to the conversation
4. The AI sees this context in the **NEXT API request** and can adjust future decisions

### PreToolUse Hook Flow
```
1. AI decides: "I'll use write_to_file with these parameters"
2. PreToolUse hook runs → can block or add context
3. If allowed, tool executes with original parameters
4. Context is added to conversation
5. Next API request includes this context
6. AI adjusts future decisions based on context
```

### PostToolUse Hook Flow
```
1. Tool completes execution
2. PostToolUse hook runs → observes results
3. Hook adds context about the outcome
4. Context is added to conversation
5. Next API request includes this context
6. AI can learn from the results
```

## Hook Input/Output

### Input (via stdin as JSON)

All hooks receive:
```json
{
  "clineVersion": "string",
  "hookName": "TaskStart" | "TaskResume" | "TaskCancel" | "TaskComplete" | "UserPromptSubmit" | "PreToolUse" | "PostToolUse" | "PreCompact",
  "timestamp": "string",
  "taskId": "string",
  "workspaceRoots": ["string"],
  "userId": "string",
  "taskStart": {  // Only for TaskStart
    "taskMetadata": {
      "taskId": "string",
      "ulid": "string",
      "initialTask": "string"
    }
  },
  "taskResume": {  // Only for TaskResume
    "taskMetadata": {
      "taskId": "string",
      "ulid": "string"
    },
    "previousState": {
      "lastMessageTs": "string",
      "messageCount": "string",
      "conversationHistoryDeleted": "string"
    }
  },
  "taskCancel": {  // Only for TaskCancel
    "taskMetadata": {
      "taskId": "string",
      "ulid": "string",
      "completionStatus": "string"
    }
  },
  "taskComplete": {  // Only for TaskComplete
    "taskMetadata": {
      "taskId": "string",
      "ulid": "string"
    }
  },
  "userPromptSubmit": {  // Only for UserPromptSubmit
    "prompt": "string",
    "attachments": ["string"]
  },
  "preToolUse": {  // Only for PreToolUse
    "toolName": "string",
    "parameters": {}
  },
  "postToolUse": {  // Only for PostToolUse
    "toolName": "string",
    "parameters": {},
    "result": "string",
    "success": boolean,
    "executionTimeMs": number
  },
  "preCompact": {  // Only for PreCompact
    "contextSize": number,
    "messagesToCompact": number,
    "compactionStrategy": "string"
  }
}
```

### Output (via stdout as JSON)

All hooks must return:
```json
{
  "cancel": boolean,                   // Required: false to continue, true to block execution
  "contextModification": "string",     // Optional: Context for future AI decisions
  "errorMessage": "string"             // Optional: Error details if blocking
}
```

**Note**: The `cancel` field works as follows:
- `false` (or omitted): Allow execution to continue
- `true`: Block execution and show error message to user

## Hook Execution Limits

- **Timeout**: Hooks must complete within 30 seconds (configurable via `HOOK_EXECUTION_TIMEOUT_MS`)
- **Context Size**: Context modifications are limited to 50KB (configurable via `MAX_CONTEXT_MODIFICATION_SIZE`)
- **Error Handling**: Expected errors (file not found, permission denied, not a directory) are handled silently; unexpected file system errors are propagated

## Common Use Cases

### 1. Validation - Block Invalid Operations

```bash
#!/usr/bin/env bash
input=$(cat)
tool_name=$(echo "$input" | jq -r '.preToolUse.toolName')
path=$(echo "$input" | jq -r '.preToolUse.parameters.path // ""')

if [[ "$tool_name" == "write_to_file" && "$path" == *.js ]]; then
  cat <<EOF
{
  "cancel": true,
  "errorMessage": "Cannot create .js files in TypeScript project",
  "contextModification": "Use .ts/.tsx extensions only"
}
EOF
  exit 0
fi

echo '{"cancel": false}'
```

### 2. Context Building - Learn from Operations

```bash
#!/usr/bin/env bash
input=$(cat)
tool_name=$(echo "$input" | jq -r '.postToolUse.toolName')
success=$(echo "$input" | jq -r '.postToolUse.success')
path=$(echo "$input" | jq -r '.postToolUse.parameters.path // ""')

if [[ "$tool_name" == "write_to_file" && "$success" == "true" ]]; then
  cat <<EOF
{
  "cancel": false,
  "contextModification": "Created '$path'. Maintain consistency with this file's patterns in future operations."
}
EOF
else
  echo '{"cancel": false}'
fi
```

### 3. Performance Monitoring

```bash
#!/usr/bin/env bash
input=$(cat)
execution_time=$(echo "$input" | jq -r '.postToolUse.executionTimeMs')
tool_name=$(echo "$input" | jq -r '.postToolUse.toolName')

if [[ "$execution_time" -gt 5000 ]]; then
  cat <<EOF
{
  "cancel": false,
  "contextModification": "Tool '$tool_name' took ${execution_time}ms. Consider optimizing future similar operations."
}
EOF
else
  echo '{"cancel": false}'
fi
```

### 4. Logging and Telemetry

```bash
#!/usr/bin/env bash
input=$(cat)

# Log to file
echo "$input" >> ~/.cline/hook-logs/tool-usage.jsonl

# Allow execution
echo '{"cancel": false}'
```

## Global vs Workspace Hooks

Cline supports two levels of hooks:

### Global Hooks
- **Location**: `~/Documents/Cline/Hooks/` (macOS/Linux)
- **Scope**: Apply to ALL workspaces and projects
- **Use Case**: Organization-wide policies, personal preferences, universal validations
- **Priority**: Order not guaranteed when combined with workspace hooks

### Workspace Hooks
- **Location**: `.clinerules/hooks/` in each workspace root
- **Scope**: Apply only to the specific workspace
- **Use Case**: Project-specific rules, team conventions, repository requirements
- **Priority**: Order not guaranteed when combined with global hooks

### Hook Execution

When multiple hooks exist (global and/or workspace):
- All hooks for a given step are executed **concurrently** using `Promise.all`
- **Execution order is not guaranteed** - hooks run in parallel
- If ALL hooks allow execution (`cancel: false`), the tool proceeds
- If ANY hook blocks (`cancel: true`), execution is blocked

**Result Combination:**
- `cancel`: If ANY hook returns `true`, execution is blocked
- `contextModification`: All context strings are concatenated with double newlines (`\n\n`)
- `errorMessage`: All error messages are concatenated with single newlines (`\n`)

### Setting Up Global Hooks

1. The global hooks directory is automatically created at:
   - macOS/Linux: `~/Documents/Cline/Hooks/`

2. Add your hook script:
   ```bash
   # Unix/Linux/macOS
   nano ~/Documents/Cline/Hooks/PreToolUse
   chmod +x ~/Documents/Cline/Hooks/PreToolUse
   ```

3. Enable hooks in Cline settings

### Example: Global + Workspace Hooks

**Global Hook** (applies to all projects):
```bash
#!/usr/bin/env bash
# ~/Documents/Cline/Hooks/PreToolUse
# Universal rule: Never delete package.json
input=$(cat)
tool_name=$(echo "$input" | jq -r '.preToolUse.toolName')
path=$(echo "$input" | jq -r '.preToolUse.parameters.path // ""')

if [[ "$tool_name" == "write_to_file" && "$path" == *"package.json"* ]]; then
  echo '{"cancel": true, "errorMessage": "Global policy: Cannot modify package.json"}'
  exit 0
fi

echo '{"cancel": false}'
```

**Workspace Hook** (applies to specific project):
```bash
#!/usr/bin/env bash
# .clinerules/hooks/PreToolUse
# Project rule: Only TypeScript files
input=$(cat)
tool_name=$(echo "$input" | jq -r '.preToolUse.toolName')
path=$(echo "$input" | jq -r '.preToolUse.parameters.path // ""')

if [[ "$tool_name" == "write_to_file" && "$path" == *.js ]]; then
  echo '{"cancel": true, "errorMessage": "Project rule: Use .ts files only"}'
  exit 0
fi

echo '{"cancel": false}'
```

**All hooks must allow execution for the tool to proceed.** Hooks may execute concurrently.

## Multi-Root Workspaces

If you have multiple workspace roots, you can place hooks in each root's `.clinerules/hooks/` directory. All hooks (global and workspace) may execute concurrently. Their results will be combined:

- **cancel**: If ANY hook returns `true`, execution is blocked
- **contextModification**: All context modifications are concatenated
- **errorMessage**: All error messages are concatenated

**Note:** No execution order is guaranteed between hooks from different directories.

## Troubleshooting

### Hook Not Running
- Ensure the "Enable Hooks" setting is checked
- Verify the hook file is executable (`chmod +x hookname`)
- Check the hook file has no syntax errors
- Look for errors in VSCode's Output panel (Cline channel)

### Hook Timing Out
- Reduce complexity of the hook script
- Avoid expensive operations (network calls, heavy computations)
- Consider moving complex logic to a background process

### Context Not Affecting Behavior
- Remember: context affects FUTURE decisions, not the current tool
- Ensure context modifications are clear and actionable
- Check that context isn't being truncated (50KB limit)

## Security Considerations

- Hooks run with the same permissions as VSCode
- Be cautious with hooks from untrusted sources
- Review hook scripts before enabling them
- Consider using `.gitignore` to avoid committing sensitive hook logic
- Hooks can access all workspace files and environment variables

## Best Practices

1. **Keep hooks fast** - Aim for <100ms execution time
2. **Make context actionable** - Be specific about what the AI should do
3. **Use structured prefixes** - Help the AI categorize context
4. **Handle errors gracefully** - Always return valid JSON
5. **Log for debugging** - Keep logs of hook executions for troubleshooting
6. **Test incrementally** - Start with simple hooks and add complexity
7. **Document your hooks** - Add comments explaining the purpose and logic


--- .clinerules/cli.md ---
# CLI Development

The CLI lives in `cli/` and uses React Ink for terminal UI.

- If needed, look at `cli/src/constants/colors.ts` for re-used terminal colors, e.g. `COLORS.primaryBlue` highlight color (selections, spinners, success states).
- Never use `dimColor` with gray (e.g. `<Text color="gray" dimColor>`) - it's too hard to read. Use `color="gray"` for secondary text and normal foreground (no color) for primary text.
- When thinking about how to handle state or messages from core, look at webview for how it communicates with the vs code extension.
- When updating the webview, consider and suggest to the user to update the CLI TUI since we want to provide a similar experience to our terminal users as we do our vs code extension users.

## Adding New API Providers

When adding a new API provider to the extension, you must also update the CLI:

1. **Update `cli/src/components/ModelPicker.tsx`**: Add the provider to the `providerModels` map so `getDefaultModelId()` returns the correct default model. Import the models and default ID from `@shared/api`:
   ```typescript
   import { newProviderDefaultModelId, newProviderModels } from "@/shared/api"

   export const providerModels = {
     // ...existing providers
     "new-provider": { models: newProviderModels, defaultId: newProviderDefaultModelId },
   }
   ```

2. **Use `applyProviderConfig()` for auth flows**: When implementing OAuth or other auth flows for the provider, use the shared utility at `cli/src/utils/provider-config.ts`:
   ```typescript
   import { applyProviderConfig } from "../utils/provider-config"

   // After successful auth:
   await applyProviderConfig({ providerId: "new-provider", controller })
   ```
   This handles setting provider, default model, API key mapping, state persistence, and rebuilding the API handler.

3. **Provider-specific auth**: If the provider uses OAuth (like `openai-codex`), add handling in `SettingsPanelContent.tsx`'s `handleProviderSelect` callback. See the existing Codex OAuth flow as a reference.

--- .clinerules/general.md ---
This file is the secret sauce for working effectively in this codebase. It captures tribal knowledge—the nuanced, non-obvious patterns that make the difference between a quick fix and hours of back-and-forth & human intervention.

**When to add to this file:**
- User had to intervene, correct, or hand-hold
- Multiple back-and-forth attempts were needed to get something working
- You discovered something that required reading many files to understand
- A change touched files you wouldn't have guessed
- Something worked differently than you expected
- User explicitly asks to "add this to CLAUDE.md"

**Proactively suggest additions** when any of the above happen—don't wait to be asked.

**What NOT to add:** Stuff you can figure out from reading a few files, obvious patterns, or standard practices. This file should be high-signal, not comprehensive.

## Miscellaneous
- This is a VS Code extension—check `package.json` for available scripts before trying to verify builds (e.g., `npm run compile`, not `npm run build`).
- When creating PRs, if the change is user-facing and significant enough to warrant a changelog entry, run `npm run changeset` and create a patch changeset. Never create minor or major version bumps. Skip changesets for trivial fixes, internal refactors, or minor UI tweaks that users wouldn't notice.
- When adding new feature flags, see this PR as a reference https://github.com/cline/cline/pull/7566
- Additional instructions about making requests: @.clinerules/network.md

## gRPC/Protobuf Communication
The extension and webview communicate via gRPC-like protocol over VS Code message passing.

**Proto files live in `proto/`** (e.g., `proto/cline/task.proto`, `proto/cline/ui.proto`)
- Each feature domain has its own `.proto` file
- For simple data, use shared types in `proto/cline/common.proto` (`StringRequest`, `Empty`, `Int64Request`)
- For complex data, define custom messages in the feature's `.proto` file
- Naming: Services `PascalCaseService`, RPCs `camelCase`, Messages `PascalCase`
- For streaming responses, use `stream` keyword (see `subscribeToAuthCallback` in `account.proto`)

**Run `npm run protos`** after any proto changes—generates types in:
- `src/shared/proto/` - Shared type definitions
- `src/generated/grpc-js/` - Service implementations
- `src/generated/nice-grpc/` - Promise-based clients
- `src/generated/hosts/` - Generated handlers

**Adding new enum values** (like a new `ClineSay` type) requires updating conversion mappings in `src/shared/proto-conversions/cline-message.ts`

**Adding new RPC methods** requires:
- Handler in `src/core/controller/<domain>/`
- Call from webview via generated client: `UiServiceClient.scrollToSettings(StringRequest.create({ value: "browser" }))`

**Example—the `explain-changes` feature touched:**
- `proto/cline/task.proto` - Added `ExplainChangesRequest` message and `explainChanges` RPC
- `proto/cline/ui.proto` - Added `GENERATE_EXPLANATION = 29` to `ClineSay` enum
- `src/shared/ExtensionMessage.ts` - Added `ClineSayGenerateExplanation` type
- `src/shared/proto-conversions/cline-message.ts` - Added mapping for new say type
- `src/core/controller/task/explainChanges.ts` - Handler implementation
- `webview-ui/src/components/chat/ChatRow.tsx` - UI rendering

## Adding a New API Provider
When adding a new provider (e.g., "openai-codex"), you must update the proto conversion layer in THREE places or the provider will silently reset to Anthropic:

1. `proto/cline/models.proto` - Add to the `ApiProvider` enum (e.g., `OPENAI_CODEX = 40;`)
2. `convertApiProviderToProto()` in `src/shared/proto-conversions/models/api-configuration-conversion.ts` - Add case mapping string to proto enum
3. `convertProtoToApiProvider()` in the same file - Add case mapping proto enum back to string

**Why this matters:** Without these, the provider string hits the `default` case and returns `ANTHROPIC`. The webview, provider list, and handler all work fine, but the state silently resets when it round-trips through proto serialization. No error is thrown.

**Other files to update when adding a provider:**
- `src/shared/api.ts` - Add to `ApiProvider` union type, define models
- `src/shared/providers/providers.json` - Add to provider list for dropdown
- `src/core/api/index.ts` - Register handler in `createHandlerForProvider()`
- `webview-ui/src/components/settings/utils/providerUtils.ts` - Add cases in `getModelsForProvider()` and `normalizeApiConfiguration()`
- `webview-ui/src/utils/validate.ts` - Add validation case
- `webview-ui/src/components/settings/ApiOptions.tsx` - Render provider component

## Responses API Providers (OpenAI Codex, OpenAI Native)
Providers using OpenAI's Responses API require native tool calling. XML tools don't work with the Responses API.

**Symptoms of broken native tool calling:**
- Tools get called multiple times (e.g., `ask_followup_question` asks the same question twice)
- Tool arguments get duplicated or malformed
- The model responds but tools aren't recognized

**Root causes to check:**
1. **Provider missing from `isNextGenModelProvider()`** in `src/utils/model-utils.ts`. The native variant matchers (e.g., `native-gpt-5/config.ts`) call this function. If your provider isn't in the list, the matcher returns false and falls back to XML tools.

2. **Model missing `apiFormat: ApiFormat.OPENAI_RESPONSES`** in its model info (`src/shared/api.ts`). This property signals that the model requires native tool calling. The task runner in `src/core/task/index.ts` checks this and forces `enableNativeToolCalls: true` regardless of user settings.

**When adding a new Responses API provider:**
1. Add provider to `isNextGenModelProvider()` list in `src/utils/model-utils.ts`
2. Set `apiFormat: ApiFormat.OPENAI_RESPONSES` on all models that use the Responses API
3. The variant matcher and task runner will handle the rest automatically

## Adding Tools to System Prompt
This is tricky—multiple prompt variants and configs. **Always search for existing similar tools first and follow their pattern.** Look at the full chain from prompt definition → variant configs → handler → UI before implementing.

1. **Add to `ClineDefaultTool` enum** in `src/shared/tools.ts`
2. **Tool definition** in `src/core/prompts/system-prompt/tools/` (create file like `generate_explanation.ts`)
   - Define variants for each `ModelFamily` (generic, next-gen, xs, etc.)
   - Export variants array (e.g., `export const my_tool_variants = [GENERIC, NATIVE_NEXT_GEN, XS]`)
   - **Fallback behavior**: If a variant isn't defined for a model family, `ClineToolSet.getToolByNameWithFallback()` automatically falls back to GENERIC. So you only need to export `[GENERIC]` unless the tool needs model-specific behavior.
3. **Register in `src/core/prompts/system-prompt/tools/init.ts`** - Import and spread into `allToolVariants`
4. **Add to variant configs** - Each model family has its own config in `src/core/prompts/system-prompt/variants/*/config.ts`. Add your tool's enum to the `.tools()` list:
   - `generic/config.ts`, `next-gen/config.ts`, `gpt-5/config.ts`, `native-gpt-5/config.ts`, `native-gpt-5-1/config.ts`, `native-next-gen/config.ts`, `gemini-3/config.ts`, `glm/config.ts`, `hermes/config.ts`, `xs/config.ts`
   - **Important**: If you add to a variant's config, make sure the tool spec exports a variant for that ModelFamily (or relies on GENERIC fallback)
5. **Create handler** in `src/core/task/tools/handlers/`
6. **Wire up in `ToolExecutor.ts`** if needed for execution flow
7. **Add to tool parsing** in `src/core/assistant-message/index.ts` if needed
8. **If tool has UI feedback**: add `ClineSay` enum in proto, update `src/shared/ExtensionMessage.ts`, update `src/shared/proto-conversions/cline-message.ts`, update `webview-ui/src/components/chat/ChatRow.tsx`

## Modifying System Prompt
**Read these first:** `src/core/prompts/system-prompt/README.md`, `tools/README.md`, `__tests__/README.md`

System prompt is modular: **components** (reusable sections) + **variants** (model-specific configs) + **templates** (with `{{PLACEHOLDER}}` resolution).

**Key directories:**
- `components/` - Shared sections: `rules.ts`, `capabilities.ts`, `editing_files.ts`, etc.
- `variants/` - Model-specific: `generic/`, `next-gen/`, `xs/`, `gpt-5/`, `gemini-3/`, `hermes/`, `glm/`, etc.
- `templates/` - Template engine and placeholder definitions

**Variant tiers (ask user which to modify):**
- **Next-gen** (Claude 4, GPT-5, Gemini 2.5): `next-gen/`, `native-next-gen/`, `native-gpt-5/`, `native-gpt-5-1/`, `gemini-3/`, `gpt-5/`
- **Standard** (default fallback): `generic/`
- **Local/small models**: `xs/`, `hermes/`, `glm/`

**How overrides work:** Variants can override components via `componentOverrides` in their `config.ts`, or provide a custom template in `template.ts` (e.g., `next-gen/template.ts` exports `rules_template`). If no override, the shared component from `components/` is used.

**Example: Adding a rule to RULES section**
1. Check if variant overrides rules: look for `rules_template` in `variants/*/template.ts` or `componentOverrides.RULES` in `config.ts`
2. If shared: modify `components/rules.ts`
3. If overridden: modify that variant's template
4. XS variant is special—has heavily condensed inline content in `template.ts`

**After any changes, regenerate snapshots:**
```bash
UPDATE_SNAPSHOTS=true npm run test:unit
```
Snapshots live in `__tests__/__snapshots__/`. Tests validate across model families and context variations (browser, MCP, focus chain).

## Modifying Default Slash Commands
Three places need updates:
- `src/core/slash-commands/index.ts` - Command definitions
- `src/core/prompts/commands.ts` - System prompt integration
- `webview-ui/src/utils/slash-commands.ts` - Webview autocomplete

## Adding New Global State Keys
Adding a new key to global state requires updates in multiple places. Missing any step causes silent failures.

Required steps:
1. Type definition in `src/shared/storage/state-keys.ts` - Add to `GlobalState` or `Settings` interface
2. Read from globalState in `src/core/storage/utils/state-helpers.ts`:
   - Add `const myKey = context.globalState.get<GlobalStateAndSettings["myKey"]>("myKey")` in `readGlobalStateFromDisk()`
   - Add to the return object: `myKey: myKey ?? defaultValue,`
3. StateManager handles read/write via `setGlobalState()`/`getGlobalStateKey()` after initialization

Common mistake: Adding only the return value without the `context.globalState.get()` call. This compiles but the value is always `undefined` on load.

Settings plumbing gotcha: if a key is user-toggleable from settings, wire both controller update paths:
- `src/core/controller/state/updateSettings.ts` for webview `updateSetting(...)`
- `src/core/controller/state/updateSettingsCli.ts` for CLI/ACP settings updates
Missing one path causes a toggle to appear to change in one surface while the backend state stays unchanged.

Webview toggle gotcha: settings changes must also round-trip back in state payloads.
- Add the field to `UpdateSettingsRequest` in `proto/cline/state.proto` (for webview update requests), then run `npm run protos`
- Include the key in `Controller.getStateToPostToWebview()` (`src/core/controller/index.ts`)
- Ensure `ExtensionState` and webview defaults include the key (`src/shared/ExtensionMessage.ts`, `webview-ui/src/context/ExtensionStateContext.tsx`)
If this round-trip wiring is missing, the backend value can update but the toggle in webview appears stuck or reverts.

## StateManager Cache vs Direct globalState Access
StateManager uses an in-memory cache populated during `StateManager.initialize(context)` in `common.ts`. For most state, use `controller.stateManager.setGlobalState()`/`getGlobalStateKey()`.

Exception: State needed immediately at extension startup (before cache is ready)

When Window A sets state and immediately opens Window B, the new window's StateManager cache is populated from `context.globalState` during initialization. If you need to read state in Window B right at startup (e.g., in `common.ts` during `initialize()`), read directly from `context.globalState.get()` instead of StateManager's cache.

Example pattern (see `lastShownAnnouncementId` and `worktreeAutoOpenPath`):
```typescript
// Writing (normal pattern)
controller.stateManager.setGlobalState("myKey", value)

// Reading at startup in common.ts (bypass cache)
const value = context.globalState.get<string>("myKey")
```

This is only needed for cross-window state read during the brief startup window before StateManager cache is fully usable. Normal state access after initialization should use StateManager.

## ChatRow Cancelled/Interrupted States
When a ChatRow displays a loading/in-progress state (spinner), you must handle what happens when the task is cancelled. This is non-obvious because cancellation doesn't update the message content—you have to infer it from context.

**The pattern:**
1. A message has a `status` field (e.g., `"generating"`, `"complete"`, `"error"`) stored in `message.text` as JSON
2. When cancelled mid-operation, the status stays `"generating"` forever—no one updates it
3. To detect cancellation, check TWO conditions:
   - `!isLast` — if this message is no longer the last message, something else happened after it (interrupted)
   - `lastModifiedMessage?.ask === "resume_task" || "resume_completed_task"` — task was just cancelled and is waiting to resume

**Example from `generate_explanation`:**
```tsx
const wasCancelled =
    explanationInfo.status === "generating" &&
    (!isLast ||
        lastModifiedMessage?.ask === "resume_task" ||
        lastModifiedMessage?.ask === "resume_completed_task")
const isGenerating = explanationInfo.status === "generating" && !wasCancelled
```

**Why both checks?**
- `!isLast` catches: cancelled → resumed → did other stuff → this old message is stale
- `lastModifiedMessage?.ask === "resume_task"` catches: just cancelled, hasn't resumed yet, this message is still technically "last"

**See also:** `BrowserSessionRow.tsx` uses similar pattern with `isLastApiReqInterrupted` and `isLastMessageResume`.

**Backend side:** When streaming is cancelled, clean up properly (close tabs, clear comments, etc.) by checking `taskState.abort` after the streaming function returns.


--- .clinerules/network.md ---
# Networking & Proxy Support

To ensure Cline works correctly in all environments (VSCode, JetBrains, CLI) and with various network configurations (especially corporate proxies), strictly follow these guidelines for all network activity.

In extension code, do NOT use the global `fetch` or a default `axios` instance. (Note, `shared/net.ts` is exempt from these rules because it sets up the fetch wrappers.) In Webview code, you SHOULD use global `fetch`.

Global `fetch` and default `axios` do not automatically pick up proxy configurations in all environments (specifically JetBrains and CLI). You MUST use the provided utilities in `@/shared/net` which handle proxy agent configuration. In the webview, the browser/embedder handles proxies.

## Guidelines

### 1. Using `fetch`

Instead of `fetch(...)`, import the proxy-aware wrapper:

```typescript
import { fetch } from '@/shared/net'

// Usage is identical to global fetch
const response = await fetch('https://api.example.com/data')
```

### 2. Using `axios`

When using `axios`, you must apply the settings from `getAxiosSettings()`:

```typescript
import axios from 'axios'
import { getAxiosSettings } from '@/shared/net'

const response = await axios.get('https://api.example.com/data', {
  headers: { 'Authorization': '...' },
  ...getAxiosSettings() // <--- CRITICAL: Injects the proxy agent if needed
})
```

### 3. Third-Party Clients (OpenAI, Ollama, etc.)

Most API client libraries allow you to customize the `fetch` implementation. You **MUST** pass the proxy-aware `fetch` to these clients.

**Example (OpenAI):**
```typescript
import OpenAI from "openai"
import { fetch } from "@/shared/net"

this.client = new OpenAI({
  apiKey: '...',
  fetch, // <--- CRITICAL: Pass our fetch wrapper
})
```

### 4. Tests

Use `mockFetchForTesting` to mock the underlying fetch implementation.

**Example (callback):**

```
import { mockFetchForTesting } from "@/shared/net"

...
  let mockFetch = ...
  mockFetchForTesting(mockFetch, () => {
    // This calls mockFetch
    fetch('https://foo.example').then(...)
  })
  // Original fetch is restored immediately when the call returns.
```

**Example (Promise):**

```
import { mockFetchForTesting } from "@/shared/net"

...
  let mockFetch = ...
  await mockFetchForTesting(mockFetch, async () => {
    await ...
    // This calls mockFetch
    await fetch('https://foo.example')
    ...
  })
  // Original fetch is restored when the Promise from the callback settles
```

## Verification

If you are adding a new network call or integration:
1.  Check `@/shared/net.ts` is imported.
2.  Ensure `fetch` or `getAxiosSettings` is being used.
3.  Verify that third-party clients are configured to use the custom fetch.


--- .clinerules/protobuf-development.md ---
# Cline Protobuf Development Guide

This guide outlines how to add new gRPC endpoints for communication between the webview (frontend) and the extension host (backend).

## Overview

Cline uses [Protobuf](https://protobuf.dev/) to define a strongly-typed API, ensuring efficient and type-safe communication. All definitions are in the `/proto` directory. The compiler and plugins are included as project dependencies, so no manual installation is needed.

## Key Concepts & Best Practices

-   **File Structure**: Each feature domain should have its own `.proto` file (e.g., `account.proto`, `task.proto`).
-   **Message Design**:
    -   For simple, single-value data, use the shared types in `proto/common.proto` (e.g., `StringRequest`, `Empty`, `Int64Request`). This promotes consistency.
    -   For complex data structures, define custom messages within the feature's `.proto` file (see `task.proto` for examples like `NewTaskRequest`).
-   **Naming Conventions**:
    -   Services: `PascalCaseService` (e.g., `AccountService`).
    -   RPCs: `camelCase` (e.g., `accountEmailIdentified`).
    -   Messages: `PascalCase` (e.g., `StringRequest`).
-   **Streaming**: For server-to-client streaming, use the `stream` keyword on the response type. See `subscribeToAuthCallback` in `account.proto` for an example.

---

## 4-Step Development Workflow

Here’s how to add a new RPC, using `scrollToSettings` as an example.

### 1. Define the RPC in a `.proto` File

Add your service method to the appropriate file in the `proto/` directory.

**File: `proto/ui.proto`**
```proto
service UiService {
  // ... other RPCs
  // Scrolls to a specific settings section in the settings view
  rpc scrollToSettings(StringRequest) returns (KeyValuePair);
}
```
Here, we use the common `StringRequest` and `KeyValuePair` types.

### 2. Compile Definitions

After editing a `.proto` file, regenerate the TypeScript code. From the project root, run:
```bash
npm run protos
```
This command compiles all `.proto` files and outputs the generated code to `src/generated/` and `src/shared/`. Do not edit these generated files manually.

### 3. Implement the Backend Handler

Create the RPC implementation in the backend. Handlers are located in `src/core/controller/[service-name]/`.

**File: `src/core/controller/ui/scrollToSettings.ts`**
```typescript
import { Controller } from ".."
import { StringRequest, KeyValuePair } from "../../../shared/proto/common"

/**
 * Executes a scroll to settings action
 * @param controller The controller instance
 * @param request The request containing the ID of the settings section to scroll to
 * @returns KeyValuePair with action and value fields for the UI to process
 */
export async function scrollToSettings(controller: Controller, request: StringRequest): Promise<KeyValuePair> {
	return KeyValuePair.create({
		key: "scrollToSettings",
		value: request.value || "",
	})
}
```

### 4. Call the RPC from the Webview

Call the new RPC from a React component in `webview-ui/`. The generated client makes this simple.

**File: `webview-ui/src/components/browser/BrowserSettingsMenu.tsx`** (Example)
```tsx
import { UiServiceClient } from "../../../services/grpc"
import { StringRequest } from "../../../../shared/proto/common"

// ... inside a React component
const handleMenuClick = async () => {
    try {
        await UiServiceClient.scrollToSettings(StringRequest.create({ value: "browser" }))
    } catch (error) {
        console.error("Error scrolling to browser settings:", error)
    }
}
```


## Links discovered
- [Protobuf](https://protobuf.dev/)

--- .clinerules/workflows/address-pr-comments.md ---
# Address PR Comments

Review and address all comments on the current branch's PR.

## Steps

1. Get the current branch name and find the associated PR:
   ```bash
   gh pr view --json number,title,body
   ```

2. Understand the PR context:
   - Get the full diff: `git diff origin/main...HEAD`
   - Read the changed files to understand what the PR is doing
   - Read related files if needed to understand the broader context
   - Understand the intent and spirit of the changes, not just the code

3. Fetch all PR comments:
   - Inline comments: `gh api repos/{owner}/{repo}/pulls/{pr_number}/comments`
   - General comments: `gh pr view {pr_number} --json comments,reviews`

4. Present a summary of all comments with your recommendation for each (apply, skip, or respond). Ignore bot noise (changeset-bot, CI status, etc.).

5. **Wait for my approval** before proceeding.

6. After approval:
   - Apply code changes and commit
   - Reply to comments that were addressed or intentionally skipped
   - Push commits


--- .clinerules/workflows/find-pr-reviewers.md ---
# Find Best Reviewers for Current Branch

Analyze my current branch to find the best people to review my PR based on **domain expertise** and git history.

## Steps

1. Get the current branch name and verify it's not `main`
2. Get the diff between the current branch and `origin/main`:
   - Use `git diff origin/main...HEAD --name-only` to get changed files
   - Use `git diff origin/main...HEAD` to understand the nature/spirit of the changes
3. **Identify the domain/feature area** being changed:
   - Read the diff carefully to understand WHAT is being changed conceptually (e.g., "slash commands", "authentication", "API client", "UI components")
   - This semantic understanding is crucial for finding the right reviewers
4. Find domain experts by searching for related files and their contributors:
   - Identify all files related to the feature/domain (not just the ones changed)
   - Example: if changing slash commands, find ALL slash-command related files across the codebase
   - Use `git log --format="%an <%ae>" -- <related-files-pattern>` to find who has expertise in that domain
5. For additional context, also gather:
   - `git blame -L <start>,<end> origin/main -- <file-path>` for exact lines changed
   - Recent commit activity on related files
6. Score and rank contributors by:
   - **Highest weight: Domain expertise** - who has the most commits to files in this feature area (even files not touched by this PR)
   - **Medium weight: Direct file expertise** - commits to the specific files being changed
   - **Lower weight: Line-level ownership** - authored the exact lines being modified
7. Exclude myself (check against my git config user.email)
8. Present the top 5 reviewers as an ordered list

## Output Format

Output an ordered list:

1. **Name** - Domain expert: 15 commits to slash-command related files, authored core parsing logic
2. **Name** - 8 commits to affected files, recently added the feature being modified
3. ...

## Commands Reference
```bash
git config user.email
git diff origin/main...HEAD --name-only
git diff origin/main...HEAD
# Find related files for a domain (adjust pattern based on what you learn from the diff)
find . -type f \( -name "*slash-command*" -o -name "*SlashCommand*" \) | head -20
# Get contributors for related files
find . -type f \( -name "*slash-command*" -o -name "*SlashCommand*" \) -print0 | xargs -0 git log --format="%an <%ae>" -- | sort | uniq -c | sort -rn
git log --format="%an <%ae>" -- <file> | sort | uniq -c | sort -rn
git blame -L 10,20 origin/main -- <file>
```

Do NOT ask questions - analyze the changes, identify the domain, and output the reviewer list.


--- .clinerules/workflows/git-branch-analysis.md ---
# Git Diff Analysis Workflow

## Objective
Analyze the current branch's changes against main to provide informed insights and context for development decisions.

## Step 1: Gather Git Information
<important>Do not return any text or conversation other than what is necessary to run these commands</important>

**Run the following command to get the latest changes (bash):**
```bash
B=$(for c in main master origin/main origin/master; do git rev-parse --verify -q "$c" >/dev/null && echo "$c" && break; done); B=${B:-HEAD}; r(){ git branch --show-current; printf "=== STATUS ===\n"; git status --porcelain | cat; printf "=== COMMIT MESSAGES ===\n"; git log "$B"..HEAD --oneline | cat; printf "=== CHANGED FILES ===\n"; git diff "$B" --name-only | cat; printf "=== FULL DIFF ===\n"; git diff "$B" | cat; }; L=$(r | wc -l); if [ "$L" -gt 500 ]; then r > cline-git-analysis.temp && echo "::OUTPUT_FILE=cline-git-analysis.temp"; else r; fi
```

```powershell
$B=$null;foreach($c in 'main','master','origin/main','origin/master'){git rev-parse --verify -q $c *> $null;if($LASTEXITCODE -eq 0){$B=$c;break}};if(-not $B){$B='HEAD'};function r([string]$b){git rev-parse --abbrev-ref HEAD; '=== STATUS ==='; git status --porcelain | cat; '=== COMMIT MESSAGES ==='; git log "$b"..HEAD --oneline | cat; '=== CHANGED FILES ==='; git diff "$b" --name-only | cat; '=== FULL DIFF ==='; git diff "$b" | cat};$out=r $B|Out-String;$lines=($out -split "`r?`n").Count;if($lines -gt 500){$out|Set-Content -NoNewline cline-git-analysis.temp; '::OUTPUT_FILE=cline-git-analysis.temp'}else{$out}
```

## Step 2: Silent, Structured Analysis Phase
- Analyze all git output without providing commentary or narration
- Read the full diff to understand the scope and nature of changes
- Identify patterns, architectural modifications, or potential impacts
- Use `read_file` to examine any related files providing additional context on the changes you have observed

## Step 3: Context Gathering
- Analyze related code without providing commentary or narration
- Read relevant related source files if needed for complete understanding
- Check dependencies, imports, or cross-references spanning the changes
- Understand the broader codebase context around modifications
- This additional context gathering should include related backend code, as well as related ui/frontend code
- You will typically need to analyze at least several files, potentially many, in order to fully complete this step
- You should not continue reading additional context if you have exhausted more than 60% of your available context window
- If you have exhausted less than 40% of your context window, you should continue reviewing additional context

## Step 4: Ready for User Interaction
**Only after completing the full analysis:**
- Engage with the user based on comprehensive understanding
- Provide insights about specific modifications and their impacts
- If you are certain they exist, note potential breaking changes or compatibility issues
- Answer questions with informed context from the complete change set and context gathering
- If the user has not provided a question, or the question is insufficient to provide a quality response, ask brief (one sentence) clarifying questions.
- Only offer recommendations if they are applicable to the user's request and relevant to the changes that you have observed 

## Key Rules
- **No prose or conversation during git research phase**
- **No prose or conversation during context gathering phase**
- **Complete all analysis before any user interaction**
- **Use gathered information for all subsequent questions and insights**
- **Focus on understanding the complete picture before discussing**

## Optional: Additional Analysis Commands
For deeper investigation when needed:

```shell
# Detailed commit history with author info
git log main..HEAD --format="%h %s (%an)" | cat

# Change statistics
git diff main --stat | cat

# Specific file type changes
git diff main --name-only | grep -E '\.(ts|js|tsx|jsx|py|md)$' | cat


--- .clinerules/workflows/pr-review.md ---
You have access to the `gh` terminal command. I already authenticated it for you. Please review it to use the PR that I asked you to review. You're already in the `cline` repo.

<detailed_sequence_of_steps>
# GitHub PR Review Process - Detailed Sequence of Steps

## 1. Gather PR Information
1. Get the PR title, description, and comments:
   ```bash
   gh pr view <PR-number> --json title,body,comments
   ```

2. Get the full diff of the PR:
   ```bash
   gh pr diff <PR-number>
   ```

## 2. Understand the Context
1. Identify which files were modified in the PR:
   ```bash
   gh pr view <PR-number> --json files
   ```

2. Examine the original files in the main branch to understand the context:
   ```xml
   <read_file>
   <path>path/to/file</path>
   </read_file>
   ```

3. For specific sections of a file, you can use search_files:
   ```xml
   <search_files>
   <path>path/to/directory</path>
   <regex>search term</regex>
   <file_pattern>*.ts</file_pattern>
   </search_files>
   ```

## 3. Analyze the Changes
1. For each modified file, understand:
   - What was changed
   - Why it was changed (based on PR description)
   - How it affects the codebase
   - Potential side effects

2. Look for:
   - Code quality issues
   - Potential bugs
   - Performance implications
   - Security concerns
   - Test coverage

## 4. Ask for User Confirmation
1. Before making a decision, ask the user if you should approve the PR, providing your assessment and justification:
   ```xml
   <ask_followup_question>
   <question>Based on my review of PR #<PR-number>, I recommend [approving/requesting changes]. Here's my justification:
   
   [Detailed justification with key points about the PR quality, implementation, and any concerns]
   
   Would you like me to proceed with this recommendation?</question>
   <options>["Yes, approve the PR", "Yes, request changes", "No, I'd like to discuss further"]</options>
   </ask_followup_question>
   ```

## 5. Ask if User Wants a Comment Drafted
1. After the user decides on approval/rejection, ask if they would like a comment drafted:
   ```xml
   <ask_followup_question>
   <question>Would you like me to draft a comment for this PR that you can copy and paste?</question>
   <options>["Yes, please draft a comment", "No, I'll handle the comment myself"]</options>
   </ask_followup_question>
   ```

2. If the user wants a comment drafted, provide a well-structured comment they can copy:
   ```
   Thank you for this PR! Here's my assessment:

   [Detailed assessment with key points about the PR quality, implementation, and any suggestions]

   [Include specific feedback on code quality, functionality, and testing]
   ```

## 6. Make a Decision
1. Approve the PR if it meets quality standards:
   ```bash
   # For single-line comments:
   gh pr review <PR-number> --approve --body "Your approval message"
   
   # For multi-line comments with proper whitespace formatting:
   cat << EOF | gh pr review <PR-number> --approve --body-file -
   Thanks @username for this PR! The implementation looks good.

   I particularly like how you've handled X and Y.

   Great work!
   EOF
   ```

2. Request changes if improvements are needed:
   ```bash
   # For single-line comments:
   gh pr review <PR-number> --request-changes --body "Your feedback message"
   
   # For multi-line comments with proper whitespace formatting:
   cat << EOF | gh pr review <PR-number> --request-changes --body-file -
   Thanks @username for this PR!

   The implementation looks promising, but there are a few things to address:

   1. Issue one
   2. Issue two

   Please make these changes and we can merge this.
   EOF
   ```

   Note: The `cat << EOF | ... --body-file -` approach preserves all whitespace and formatting without requiring temporary files. The `-` parameter tells the command to read from standard input.
</detailed_sequence_of_steps>

<example_review_process>
# Example PR Review Process

Let's walk through a real example of reviewing PR #3627 which fixes the thinking mode calculation for Claude 3.7 models.

## Step 1: Gather PR Information

```bash
# Get PR details
gh pr view 3627 --json title,body,comments

# Get the full diff
gh pr diff 3627
```

## Step 2: Understand the Context

```xml
# Examine the original files to understand what's being changed
<read_file>
<path>src/shared/api.ts</path>
</read_file>

# Look at the ThinkingBudgetSlider component implementation
<read_file>
<path>webview-ui/src/components/settings/ThinkingBudgetSlider.tsx</path>
</read_file>

# Check how thinking mode is currently handled in API providers
<search_files>
<path>src/api/providers</path>
<regex>reasoningOn</regex>
<file_pattern>*.ts</file_pattern>
</search_files>
```

## Step 3: Analyze the Changes

From the PR information and code examination, we can see that:

1. The PR fixes an issue with the thinking mode budget calculation for Claude 3.7 models
2. Currently, the thinking budget is incorrectly calculated as 80% of maxTokens (8192), resulting in 6553 tokens
3. Claude 3.7 actually supports a much larger thinking budget (up to 64000 tokens)
4. The PR adds a `thinkingConfig` property with `maxBudget: 64000` to model definitions
5. It updates API handlers to use this value when reasoning mode is enabled
6. It enhances the slider component to calculate max values based on model-specific percentages
7. Comprehensive tests were added to verify the calculations

## Step 4: Ask for User Confirmation

```xml
<ask_followup_question>
<question>Based on my review of PR #3627, I recommend approving this PR. Here's my justification:

1. The PR correctly fixes the thinking mode budget calculation for Claude 3.7 models
2. It properly implements the thinkingConfig.maxBudget property (64000) for Claude 3.7 models
3. It applies the appropriate percentage (50%) for the slider calculation
4. The changes are well-tested with comprehensive unit tests
5. The implementation is clean and follows the project's coding standards
6. The PR includes helpful screenshots demonstrating the fix

Would you like me to proceed with approving this PR?</question>
<options>["Yes, approve the PR", "No, I'd like to discuss further", "Let me review it myself first"]</options>
</ask_followup_question>
```

## Step 5: Ask if User Wants a Comment Drafted

```xml
<ask_followup_question>
<question>Would you like me to draft a comment for this PR that you can copy and paste?</question>
<options>["Yes, please draft a comment", "No, I'll handle the comment myself"]</options>
</ask_followup_question>
```

## Step 6: Make a Decision

```bash
# Option 1: Simple one-line comment
gh pr review 3627 --approve --body "This PR looks good! It correctly fixes the thinking mode budget calculation for Claude 3.7 models."

# Option 2: Multi-line comment with proper whitespace formatting
cat << EOF | gh pr review 3627 --approve --body-file -
This PR looks good! It correctly fixes the thinking mode budget calculation for Claude 3.7 models.

I particularly like:
1. The proper implementation of thinkingConfig.maxBudget property (64000)
2. The appropriate percentage (50%) for the slider calculation
3. The comprehensive unit tests
4. The clean implementation that follows project coding standards

Great work!
EOF
```
</example_review_process>

<common_gh_commands>
# Common GitHub CLI Commands for PR Review

## Basic PR Commands
```bash
# Get current PR number
gh pr view --json number -q .number

# List open PRs
gh pr list

# View a specific PR
gh pr view <PR-number>

# View PR with specific fields
gh pr view <PR-number> --json title,body,comments,files,commits

# Check PR status
gh pr status
```

## Diff and File Commands
```bash
# Get the full diff of a PR
gh pr diff <PR-number>

# List files changed in a PR
gh pr view <PR-number> --json files

# Check out a PR locally
gh pr checkout <PR-number>
```

## Review Commands
```bash
# Approve a PR (single-line comment)
gh pr review <PR-number> --approve --body "Your approval message"

# Approve a PR (multi-line comment with proper whitespace)
cat << EOF | gh pr review <PR-number> --approve --body-file -
Your multi-line
approval message with

proper whitespace formatting
EOF

# Request changes on a PR (single-line comment)
gh pr review <PR-number> --request-changes --body "Your feedback message"

# Request changes on a PR (multi-line comment with proper whitespace)
cat << EOF | gh pr review <PR-number> --request-changes --body-file -
Your multi-line
change request with

proper whitespace formatting
EOF

# Add a comment review (without approval/rejection)
gh pr review <PR-number> --comment --body "Your comment message"

# Add a comment review with proper whitespace
cat << EOF | gh pr review <PR-number> --comment --body-file -
Your multi-line
comment with

proper whitespace formatting
EOF
```

## Additional Commands
```bash
# View PR checks status
gh pr checks <PR-number>

# View PR commits
gh pr view <PR-number> --json commits

# Merge a PR (if you have permission)
gh pr merge <PR-number> --merge
```
</common_gh_commands>

<general_guidelines_for_commenting>
When reviewing a PR, please talk normally and like a friendly reviwer. You should keep it short, and start out by thanking the author of the pr and @ mentioning them. 

Whether or not you approve the PR, you should then give a quick summary of the changes without being too verbose or definitive, staying humble like that this is your understanding of the changes. Kind of how I'm talking to you right now.

If you have any suggestions, or things that need to be changed, request changes instead of approving the PR.

Leaving inline comments in code is good, but only do so if you have something specific to say about the code. And make sure you leave those comments first, and then request changes in the PR with a short comment explaining the overall theme of what you're asking them to change.
</general_guidelines_for_commenting>

<example_comments_that_i_have_written_before>
<brief_approve_comment>
Looks good, though we should make this generic for all providers & models at some point
</brief_approve_comment>
<brief_approve_comment>
Will this work for models that may not match across OR/Gemini? Like the thinking models?
</brief_approve_comment>
<approve_comment>
This looks great! I like how you've handled the global endpoint support - adding it to the ModelInfo interface makes total sense since it's just another capability flag, similar to how we handle other model features.

The filtered model list approach is clean and will be easier to maintain than hardcoding which models work with global endpoints. And bumping the genai library was obviously needed for this to work.

Thanks for adding the docs about the limitations too - good for users to know they can't use context caches with global endpoints but might get fewer 429 errors.
</approve_comment>
<requesst_changes_comment>
This is awesome. Thanks @scottsus.

My main concern though - does this work for all the possible VS Code themes? We struggled with this initially which is why it's not super styled currently. Please test and share screenshots with the different themes to make sure before we can merge
</request_changes_comment>
<request_changes_comment>
Hey, the PR looks good overall but I'm concerned about removing those timeouts. Those were probably there for a reason - VSCode's UI can be finicky with timing.

Could you add back the timeouts after focusing the sidebar? Something like:

```typescript
await vscode.commands.executeCommand("claude-dev.SidebarProvider.focus")
await setTimeoutPromise(100)  // Give UI time to update
visibleWebview = WebviewProvider.getSidebarInstance()
```
</request_changes_comment>
<request_changes_comment>
Heya @alejandropta thanks for working on this! 

A few notes:
1 - Adding additional info to the environment variables is fairly problematic because env variables get appended to **every single message**. I don't think this is justifiable for a somewhat niche use case. 
2 - Adding this option to settings to include that could be an option, but we want our options to be simple and straightforward for new users
3 - We're working on revisualizing the way our settings page is displayed/organized, and this could potentially be reconciled once that is in and our settings page is more clearly delineated. 

So until the settings page is update, and this is added to settings in a way that's clean and doesn't confuse new users, I don't think we can merge this. Please bear with us.
</request_changes_comment>
<request_changes_comment>
Also, don't forget to add a changeset since this fixes a user-facing bug.

The architectural change is solid - moving the focus logic to the command handlers makes sense. Just don't want to introduce subtle timing issues by removing those timeouts.
</request_changes_comment>
</example_comments_that_i_have_written_before>


--- .clinerules/workflows/writing-documentation.md ---
# General writing guide

# How I want you to write 

I'm gonna write something technical.

It's often less about the nitty-gritty details of the tech stuff and more about learning something new or getting a solution handed to me on a silver platter.

Look, when I read, I want something out of it. So when I write, I gotta remember that my readers want something too. This whole piece? It's about cluing in anyone who writes for me, or wants me to write for them, on how I see this whole writing product thing.

I'm gonna lay out a checklist of stuff I'd like to have. It'll make the whole writing gig a bit smoother, you know?

## Crafting Compelling Titles

I often come across titles like "How to do X with Y,Z technology." These don't excite me because X or Y are usually unfamiliar unless they're already well-known. Its rarely the dream to use X unless X is the dream. 

My dream isn’t to use instructor, its to do something valueble with the data it extracts

An effective title should:

- Evoke an emotional response
- Highlight someone's goal
- Offer a dream or aspiration
- Challenge or comment on a belief
- Address someone's problems

I believe it's more impactful to write about specific problems. If this approach works, you can replicate it across various scenarios rather than staying too general.

- Time management for everyone can be a 15$ ebook
- Time management for executives is a 2000$ workshop

Aim for titles that answer questions you think everyone is asking, or address thoughts people have but can't quite articulate.

Instead of "How I do something" or "How to do something," frame it from the reader's perspective with "How you can do something." This makes the title more engaging. Just make sure the difference is advisory if the content is subjective. “How I made a million dollars” might be more reasonable than “How to make a million dollars” since you are the subject and the goal might be to share your story in hopes of helping others.

This approach ultimately trains the reader to have a stronger emotional connection to your content.

- "How I do X"
- "How You Can do X"

Between these two titles, it's obvious which one resonates more emotionally.

You can take it further by adding specific conditions. For instance, you could target a particular audience or set a timeframe:

- How to set up Braintrust
- How to set up Braintrust in 5 minutes

## NO adjectiives

I want you to almost always avoid adjectives and try to use evidence instead. Instead of saying "production ready," you can write something like "scaling this to 100 servers or 1 million documents per second." Numbers like that will tell you exactly what the specificity of your product is. If you have to use adjectives rather than evidence, you are probably making something up. 

There's no reason to say something like "blazingly fast" unless those things are already known phrases.

Instead, say "200 times faster" or "30% faster." A 30% improvement in recommendation system speed is insane.

There's a 200 times performance improvement because we went from one programming language to another. It's just something that's a little bit more expected and understandable.

Another test that I really like using recently is tracking whether or not the statements you make can be:

- Visualized
- Proven false
- Said only by you

If you can nail all three, the claim you make will be more likely to resonate with an audience because only you can say it.

Earlier this year, I had an example where I embedded all of Wikipedia in 17 minutes with 20 bucks, and it got half a million views. All we posted was a video of me kicking off the job, and then you can see all the log lines go through. You see the number of containers go from 1 out of 50 to 50 out of 50.

It was easy to visualize and could have been proven false by being unreproducible. Lastly, Modal is the only company that could do that in such an effortless way, which made it unique.

## Keep It Digestible 
    - Aim for 5-minute reads
    - Write at a Grade 10 reading level
    - Break up long paragraphs
    - Use headers and bullet points

## Make It Scannable
    - Bold key points
    - Use subheadings every 3-4 paragraphs
    - Include plenty of white space
    - Add relevant examples

This structure works whether you're writing a tweet thread or a full blog post. The key is making complex ideas accessible.

# Guide to Writing Cline Documentation

## Some general principles for explaining features

If you're talking about a feature, it's helpful to start with a human-readable explanations that cover what the feature is in simple terms. Skip jargon and explain it like you're talking to someone who's never seen it before. This sets the foundation for everything that follows.

Combine location and usage into one flowing section. Tell users exactly where to find the feature and how to use it, but weave the instructions into natural prose with a good balance of bullet points, numbered lists, code examples (if applicable), mintlify components, and headers/subheaders. Users shouldn't have to jump between separate "where is it" and "how do I use it" sections.

Show the feature in action with real examples like actual files, workflows, or code. Users need to see concrete implementations, not just abstract descriptions. This is where understanding turns into practical knowledge.

When talking about a feature, include an inspiration section that sparks imagination. This section pushes people from understanding to action by showing them what becomes possible when they use this feature creatively. It's what separates good documentation from great documentation.

## Writing Principles That Actually Work

### Write for Action, Not Just Understanding

Documentation should motivate users to try things. Instead of just explaining how something works, focus on what users can accomplish with it. The inspiration section is crucial - it's what transforms passive readers into active users.

### Create a Natural Story Flow

It should feel like a conversation that naturally progresses from "what is this?" to "how do I use it?" to "here's a real example" to "imagine what you could do with this." 

### Show Real Examples, Not Toy Demos

Provide actual workflow files, real code snippets, and concrete implementations that users can copy and adapt. Abstract examples don't help anyone - users want to see exactly what they'll be working with.

### Keep It Scannable But Not Fragmented

Write in prose that flows naturally when read completely, but structure it so users can quickly find specific information when they're troubleshooting. Avoid dense walls of text, but also avoid over-formatting with excessive bullet points and bold headers. There should be a nice visual heirarchy of balance between all elements, so you can quickly scan the page and find what you're looking for.

## Language and Tone Guidelines

Write clearly without dumbing things down. Use simple language when possible, but don't avoid technical terms that users need to know. Explain concepts in terms of what users can achieve rather than how the software works internally.

Make your writing conversational and encouraging. Phrases like "you can also try" or "when that works" feel more natural than rigid instructional language. Help users feel confident about trying new things.

Keep content concise and purposeful. Every sentence should either help users understand something or help them do something. If it doesn't serve one of those purposes, cut it.

Build in context and reasoning. Users want to understand why they're doing something, not just what to do. This builds confidence and helps them troubleshoot when things don't work exactly as expected.

## Practical Implementation

Structure each feature page consistently with the four-section approach, but let the content flow naturally within that structure. Use visual assets like videos and screenshots to complement the written content - they often communicate more effectively than paragraphs of description.

Link generously to related resources, examples, and deeper documentation. Users should never feel stuck or wonder where to go next. Maintain a repository of real examples that users can reference and adapt to their own needs.

The goal is documentation that feels more like helpful guidance from an experienced colleague than a technical manual. Users should finish reading feeling excited about what they can accomplish, not just informed about what the feature does.

## Balance Structure with Flexibility

While they discuss having consistent documentation structure, there's also mention of making content feel less rigid and more natural. The writing should follow guidelines while still feeling conversational and engaging. 

## Bad examples

I personally hate this pattern of bullet point **Bold Text** colon and then more text:
<bad_example_of_writing>
#### macOS

1. **Switch to bash**: Go to Cline Settings → Terminal → Default Terminal Profile → Select "bash"
2. **Disable Oh-My-Zsh temporarily**: If using zsh, try `mv ~/.zshrc ~/.zshrc.backup` and restart VSCode
3. **Set environment**: Add to your shell config: `export TERM=xterm-256color`

#### Windows

1. **Use PowerShell 7**: Install from Microsoft Store, then select it in Cline settings
2. **Disable Windows ConPTY**: VSCode Settings → Terminal › Integrated: Windows Enable Conpty → Uncheck
3. **Try Command Prompt**: Sometimes simpler is better - switch to cmd.exe

#### Linux

1. **Use bash**: Most reliable option - select in Cline settings
2. **Check permissions**: Ensure VSCode has terminal access permissions
3. **Disable custom prompts**: Comment out prompt customizations in `.bashrc`

</bad_example_of_writing>

We should instead strive to write beautiful docs that read well. We can use bullet points and numbered lists but it should read naturally and be delightful to look at hierachally when scanning through the doc. There should be a good balance between blocks of text, code snippets, paragraphs, numbered lists, and bullet points. When scanning the documentation visually, you should feel like you're adminiring a tasteful art piece.

<good_example_of_writing>
#### macOS

The most common fix is switching to bash. Navigate to Cline Settings → Terminal → Default Terminal Profile and select "bash" from the dropdown.

If you're still having issues, Oh-My-Zsh might be interfering with terminal integration. Try temporarily disabling it:
- Run `mv ~/.zshrc ~/.zshrc.backup` 
- Restart VSCode

You can also add `export TERM=xterm-256color` to your shell configuration file to improve compatibility.

#### Windows

PowerShell 7 provides the most reliable experience. Install it from the Microsoft Store, then select it in your Cline settings.

Still seeing problems? Try these solutions:
- Disable Windows ConPTY: VSCode Settings → Terminal › Integrated: Windows Enable Conpty → uncheck
- Switch to Command Prompt (cmd.exe) - sometimes simpler shells work better

#### Linux

Bash is your most dependable option. Select it in Cline settings if you haven't already.

Check these common issues:
- Ensure VSCode has terminal access permissions
- Temporarily comment out custom prompt configurations in your `.bashrc`
</good_example_of_writing>

This is much more natural to read. Writing this way creates a conversational flow, and bullet points are used idiomatically.

# Using Mintlify Components Idiomatically

Mintlify's custom components can transform basic documentation into engaging, scannable content that users actually want to read. Here's how to use them effectively.

## Visual Content with Frames

Videos and images should be wrapped in `<Frame>` components rather than using raw HTML or markdown. This creates consistent styling and proper responsive behavior.

For videos, embed them directly rather than linking externally. Users are much more likely to watch a 30-second demonstration than click through to another platform:

```jsx
<Frame>
	<iframe
		style={{ width: "100%", aspectRatio: "16/9" }}
		src="https://www.youtube.com/embed/your-video-id"
		title="Feature demonstration"
		frameBorder="0"
		allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
		allowFullScreen
	/>
</Frame>
```

Screenshots work similarly - the frame provides visual polish and consistency:

```jsx
<Frame>
	<img src="/path/to/screenshot.png" alt="Descriptive alt text" />
</Frame>
```

## Cards for Navigation and Overview

Cards excel at creating scannable overviews that link to detailed documentation. They're perfect for feature listings, getting started guides, or any section where users need to choose their path.

Use the two-column layout for related features:

```jsx
<Columns cols={2}>
  <Card title="Feature Name" icon="relevant-icon" href="/link/to/docs">
    Brief description that explains what this feature does and why someone would use it.
  </Card>
  
  <Card title="Related Feature" icon="another-icon" href="/another/link">
    Another concise explanation that helps users understand the value proposition.
  </Card>
</Columns>
```

The key is writing card descriptions that are informative enough to help users decide whether to click through, but concise enough to scan quickly. Each card should answer "what does this do?" and "why would I need this?"

## Tips and Notes for Context

Use `<Tip>` components for helpful information that enhances the main content without cluttering it:

```jsx
<Tip>
	Pro tip: You can combine multiple @ mentions in a single message to give Cline 
	comprehensive context about your issue.
</Tip>
```

`<Note>` components work well for important caveats or technical limitations:

```jsx
<Note>
	Due to VS Code limitations, some features require specific settings to work properly.
</Note>
```

`<Info>` is also cool:

<Info>
	**Quick Fix**: If you're experiencing terminal issues, try switching to a simpler shell like `bash` in the Cline settings.
	This resolves 90% of terminal integration problems.
</Info>

**Never** fall into that awful **Bold Text** - description pattern that we specifically identified as bad writing. The content should flow naturally as connected thoughts rather than feeling like a templated AI response with forced formatting.


## When to Use Bullet Points and Numbered Lists Strategically

Bullet points serve functional purposes - use them for:

**Sequential actions or troubleshooting steps** where users need to follow a specific order:
1. Install the extension
2. Restart VSCode  
3. Check the settings panel

**Lists of related options** where users need to choose one approach:
- Try PowerShell 7 for the most reliable experience
- Switch to Command Prompt if you're still having issues
- Use WSL Bash for Linux compatibility

**Quick reference items** that users might need to scan quickly when problem-solving.

**Improving Visual Hierarchy** when there's a wall of text - that's a good time to introduce bullet points or numbered lists.

Each bulleted item or numbered list should be a discrete action or piece of information that benefits from being visually separated. This is a key weapon you can employ when going for that artwork experience I mentioned earlier. 

<good_example_of_bullet_points>
## Finding and Configuring Terminal Settings

You can access Cline's terminal settings by clicking the settings icon in the Cline sidebar, then navigating to the Terminal section. These settings control how Cline interacts with your system's terminal.

- The **Default Terminal Profile** setting determines which shell Cline uses for executing commands. If you're experiencing issues, this is usually the first thing to change. I personally keep this set to `bash` on all my systems because it's the most reliable option, even though I use `zsh` for my regular terminal work.

- **Shell Integration Timeout** controls how long Cline waits for the terminal to become ready. The default is 4 seconds, but if you have a heavy shell configuration (lots of plugins, slow startup scripts), you might need to increase this to 10 or even 15 seconds. I've found that WSL environments and SSH connections often need longer timeouts.

- The **Enable Aggressive Terminal Reuse** setting determines whether Cline tries to reuse existing terminals even when they're not in the correct directory. When this causes problems (commands running in the wrong directory, virtual environment issues), disabling it creates more terminal instances but ensures each command runs in a clean state.

- **Terminal Output Line Limit** sets how many lines of output Cline will read from commands. The default of 500 lines works for most cases, but you might need to adjust it. For verbose build outputs, increase it. For commands with progress bars that spam thousands of lines, decrease it to around 100 to avoid consuming too many tokens.
</good_example_of_bullet_points>

## Write Like a Human, Not an AI

Keep it short and direct. If you can say something in fewer words, do it. Long explanations often confuse more than they help.

Use normal sentence lengths. Mix short and long sentences naturally, like you would when talking to someone. Avoid those meandering compound sentences that go on forever.

Cut the corporate speak. Instead of "utilize," say "use." Instead of "in order to," just say "to." Write like you're explaining something to a colleague, not drafting a press release.

Don't over-explain obvious things. If users are reading documentation, they probably understand basic concepts. Respect their intelligence.

## Never use em dashes or emojis

That's pretty much it. Never use them. Only AI writes with em dashes or emojis.

# Anthropomorphizing Cline

When referring to Cline, always call him a "him" not an "it". 

Bad example:
- When Cline can’t execute commands or read their output, you lose access to one of its most powerful capabilities.

Good Example:

- When Cline can’t execute commands or read their output, you lose access to one of his most powerful capabilities.

# Using "I" when sharing your workflow

Adding a personal touch goes a long way. There are great examples in the docs currently where I use "I" to share how I personally use cline, from dev to dev. It's a great technique.

# Crosslinking relevant documentation pages

Make sure you crosslink when you're done writing the docs. If there are relevant docs, just link to them.

# Brevity is the soul of wit

Don't ramble if you don't need to. Use bullet points and numbered lists. Keep things easy to read.

<bad_example>

When Cline can't execute commands or read their output, you lose access to one of his most powerful capabilities. Terminal integration problems are frustrating, but they're usually fixable with a few simple changes.

## The Most Common Problem: Shell Integration Issues

If you're seeing "Shell integration unavailable" or Cline isn't getting command output, the issue is almost always your shell configuration. Complex shell setups with custom prompts, plugins, and fancy configurations can interfere with VSCode's terminal integration.

**Switch to bash first.** This fixes the problem 90% of the time. Navigate to Cline Settings → Terminal → Default Terminal Profile and select "bash" from the dropdown. Restart VSCode after making this change.

Still having issues? Try increasing the shell integration timeout. Go to Cline Settings → Terminal → Shell Integration Timeout and change it from 4 seconds to 10 seconds. Heavy shell configurations need more time to initialize properly.

If commands are running in the wrong directories or you're seeing weird behavior, disable aggressive terminal reuse. In Cline Settings → Terminal, uncheck "Enable aggressive terminal reuse." This creates more terminal instances but ensures each command runs in a clean environment.


</bad_exaxmple>

The first part is total filler, useless to any serious developer. You can tell it's written by a non technical person that doesn't value clean, straightforward information. 

<good_example>
## Shell Integration Issues

If you're seeing "Shell integration unavailable" or Cline can't read command output, your shell configuration is interfering with VSCode's terminal integration.

**Switch to bash first.** Go to Cline Settings → Terminal → Default Terminal Profile and select "bash." This fixes 90% of problems.

Still broken? Try these:
- Increase shell integration timeout to 10 seconds in Cline Settings → Terminal
- Disable "aggressive terminal reuse" if commands run in wrong directories
- Restart VSCode after making changes
</good_example>

The good version cuts straight to the problem and solution. No hand-holding, no emotional language about frustration, just the facts: what's wrong, how to fix it, what to try next. Respects that developers want information, not sympathy.RetryClaude can make mistakes. Please double-check responses.

ALWAYS consider your audience. And your audience is devs who don't want their time wasted. Give them the info. I cannot stress this enough. Use bullet points and numbered lists. Prose is good, but every word should actually mean something to the dev reading it.

# Lastly, before you start writing docs

1. Internalize these guidelines. I mean it. 

2. Read `docs/docs.json` and get an understanding of the structure of the docs. This will come in handly at the end when you're doing a final pass so you can cross link to docs where relevant.

3. Read some good examples that I personally wrote and am proud of:

- docs/features/slash-commands/workflows.mdx
- docs/features/slash-commands/new-task.mdx
- docs/features/at-mentions/overview.mdx
- docs/features/drag-and-drop.mdx

4. If the user specifies any other instructions make sure you follow them.


--- .github/pull_request_template.md ---
<!--
Thank you for contributing to Cline!

⚠️ Important: Before submitting this PR, please ensure you have:
- For feature requests: Created a discussion in our Feature Requests discussions board https://github.com/cline/cline/discussions/categories/feature-requests and received approval from core maintainers before implementation
- For all changes: Link the associated issue/discussion in the "Related Issue" section below

Limited exceptions:
Small bug fixes, typo corrections, minor wording improvements, or simple type fixes that don't change functionality may be submitted directly without prior discussion.

Why this requirement?
We deeply appreciate all community contributions - they are essential to Cline's success! To ensure the best use of everyone's time and maintain project direction, we use our Feature Requests discussions board to gauge community interest and validate feature ideas before implementation begins. This helps us focus development efforts on features that will benefit the most users.
-->

### Related Issue

<!-- Replace XXXX with the issue number that this PR addresses -->
**Issue:** #XXXX

### Description

<!-- 
Help reviewers understand your changes by making this PR readable and well-organized:

- What problem does this PR solve?
- Why were these changes introduced and what purpose do they serve?
- For larger changes, provide context about your approach and reasoning

Small PRs may need minimal description, but larger changes benefit from explaining where you're coming from. Much of this context can be in the linked issue above, so feel free to reference it rather than repeating everything here.
-->

### Test Procedure

<!-- 
Please walk us through your testing approach and thought process. This helps reviewers understand that you've thoroughly considered the impact of your changes:

- How did you test this change?
- What could potentially break and how did you verify it doesn't?
- What existing functionality might be affected and how did you check it still works?
- Why are you confident this is ready for merge?

We're not looking for exhaustive documentation - just evidence that you've thought through the implications of your changes and tested accordingly.
-->

### Type of Change

<!-- Put an 'x' in all boxes that apply -->

-   [ ] 🐛 Bug fix (non-breaking change which fixes an issue)
-   [ ] ✨ New feature (non-breaking change which adds functionality)
-   [ ] 💥 Breaking change (fix or feature that would cause existing functionality to not work as expected)
-   [ ] ♻️ Refactor Changes
-   [ ] 💅 Cosmetic Changes
-   [ ] 📚 Documentation update
-   [ ] 🏃 Workflow Changes

### Pre-flight Checklist

<!-- Put an 'x' in all boxes that apply -->

-   [ ] Changes are limited to a single feature, bugfix or chore (split larger changes into separate PRs)
-   [ ] Tests are passing (`npm test`) and code is formatted and linted (`npm run format && npm run lint`)
-   [ ] I have created a changeset using `npm run changeset` (required for user-facing changes)
-   [ ] I have reviewed [contributor guidelines](https://github.com/cline/cline/blob/main/CONTRIBUTING.md)

### Screenshots

<!-- 
Help reviewers quickly understand your changes:

- **UI Changes**: Please include screenshots showing before/after states
- **Complex Workflows**: Consider uploading a screen recording (video) if your changes involve multiple steps or state transitions
- **Backend Changes**: Not required, but feel free to include terminal output or other evidence that demonstrates functionality

This helps reviewers see what you've built without having to pull down and test your branch first.
-->

### Additional Notes

<!-- Add any additional notes for reviewers -->


## Links discovered
- [contributor guidelines](https://github.com/cline/cline/blob/main/CONTRIBUTING.md)

--- .github/scripts/tests/coverage_check_test.py ---
#!/usr/bin/env python3
"""
Tests for coverage_check script.
"""

import os
import sys
import unittest
import subprocess
import tempfile
from unittest.mock import patch, MagicMock, call, mock_open

# Add parent directory to path so we can import coverage modules
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from coverage_check import extract_coverage, compare_coverage, set_verbose, generate_comment, post_comment, set_github_output
from coverage_check.util import log, file_exists, get_file_size, list_directory


class TestCoverage(unittest.TestCase):
    # Class variables to store coverage files
    temp_dir = None
    extension_coverage_file = None
    webview_coverage_file = None
    
    @classmethod
    def setUpClass(cls):
        """Set up test environment once for all tests."""
        # Create temporary directory for test files
        cls.temp_dir = tempfile.TemporaryDirectory()
        cls.extension_coverage_file = os.path.join(cls.temp_dir.name, 'extension_coverage.txt')
        cls.webview_coverage_file = os.path.join(cls.temp_dir.name, 'webview_coverage.txt')
        
        # Run actual tests to generate coverage reports
        cls.generate_coverage_reports()
        
        # Verify files exist and are not empty
        assert os.path.exists(cls.extension_coverage_file), \
            f"Extension coverage file {cls.extension_coverage_file} does not exist"
        assert os.path.getsize(cls.extension_coverage_file) > 0, \
            f"Extension coverage file {cls.extension_coverage_file} is empty"
        assert os.path.exists(cls.webview_coverage_file), \
            f"Webview coverage file {cls.webview_coverage_file} does not exist"
        assert os.path.getsize(cls.webview_coverage_file) > 0, \
            f"Webview coverage file {cls.webview_coverage_file} is empty"

    @classmethod
    def tearDownClass(cls):
        """Clean up test environment after all tests."""
        if cls.temp_dir:
            cls.temp_dir.cleanup()

    @classmethod
    def generate_coverage_reports(cls):
        """Generate real coverage reports by running tests."""
        log("Generating coverage reports (this may take a while)...")
        
        # Run extension tests with coverage
        try:
            # Get absolute paths
            root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..'))
            webview_dir = os.path.join(root_dir, 'webview-ui')
            
            # Use xvfb-run on Linux
            if sys.platform.startswith('linux'):
                cmd = f"cd {root_dir} && xvfb-run -a npm run test:coverage > {cls.extension_coverage_file} 2>&1"
            else:
                cmd = f"cd {root_dir} && npm run test:coverage > {cls.extension_coverage_file} 2>&1"
            
            log("Running extension tests...")
            log(f"Command: {cmd}")
            result = subprocess.run(cmd, shell=True, check=False, capture_output=True, text=True)
            log(f"Extension tests exit code: {result.returncode}")
            
            # Run webview tests with coverage
            log("Running webview tests...")
            cmd = f"cd {webview_dir} && npm run test:coverage > {cls.webview_coverage_file} 2>&1"
            log(f"Command: {cmd}")
            result = subprocess.run(cmd, shell=True, check=False, capture_output=True, text=True)
            log(f"Webview tests exit code: {result.returncode}")
            
            # Verify files were created
            if file_exists(cls.extension_coverage_file):
                ext_size = get_file_size(cls.extension_coverage_file)
                log(f"Extension coverage file created: {cls.extension_coverage_file} (size: {ext_size} bytes)")
            else:
                log(f"WARNING: Extension coverage file was not created: {cls.extension_coverage_file}")
                
            if file_exists(cls.webview_coverage_file):
                web_size = get_file_size(cls.webview_coverage_file)
                log(f"Webview coverage file created: {cls.webview_coverage_file} (size: {web_size} bytes)")
            else:
                log(f"WARNING: Webview coverage file was not created: {cls.webview_coverage_file}")
            
            log("Coverage reports generation completed.")
        except Exception as e:
            log(f"Error generating coverage reports: {e}")
            import traceback
            log(traceback.format_exc())
            
            # Create empty files if tests fail
            log("Creating fallback coverage files...")
            with open(cls.extension_coverage_file, 'w') as f:
                f.write("No coverage data available")
            with open(cls.webview_coverage_file, 'w') as f:
                f.write("No coverage data available")

    def test_extract_coverage(self):
        """Test extract_coverage function with both extension and webview coverage."""
        # Check if verbose mode is enabled
        if '-v' in sys.argv or '--verbose' in sys.argv:
            set_verbose(True)
        
        # Verify files exist before testing
        self.assertTrue(file_exists(self.extension_coverage_file), 
                       f"Extension coverage file does not exist: {self.extension_coverage_file}")
        self.assertTrue(file_exists(self.webview_coverage_file), 
                       f"Webview coverage file does not exist: {self.webview_coverage_file}")
        
        # Log file sizes
        ext_size = get_file_size(self.extension_coverage_file)
        web_size = get_file_size(self.webview_coverage_file)
        log(f"Extension coverage file size: {ext_size} bytes")
        log(f"Webview coverage file size: {web_size} bytes")
        
        # Test extension coverage
        log("Testing extension coverage extraction...")
        ext_coverage_pct = extract_coverage(self.extension_coverage_file, 'extension')
        
        # Check that coverage percentage is a float
        self.assertIsInstance(ext_coverage_pct, float)
        
        # Check that coverage percentage is between 0 and 100
        self.assertGreaterEqual(ext_coverage_pct, 0)
        self.assertLessEqual(ext_coverage_pct, 100)
        
        # Log coverage percentage for debugging
        log(f"Extension coverage: {ext_coverage_pct}%")
        
        # Test webview coverage
        log("Testing webview coverage extraction...")
        web_coverage_pct = extract_coverage(self.webview_coverage_file, 'webview')
        
        # Convert to float if it's an integer
        if isinstance(web_coverage_pct, int):
            web_coverage_pct = float(web_coverage_pct)
        
        # Check that coverage percentage is a float
        self.assertIsInstance(web_coverage_pct, float)
        
        # Check that coverage percentage is between 0 and 100
        self.assertGreaterEqual(web_coverage_pct, 0)
        self.assertLessEqual(web_coverage_pct, 100)
        
        # Log coverage percentage for debugging
        log(f"Webview coverage: {web_coverage_pct}%")

    def test_compare_coverage(self):
        """Test compare_coverage function."""
        # Test with coverage increase
        decreased, diff = compare_coverage(80, 90)
        self.assertFalse(decreased)
        self.assertEqual(diff, 10)
        
        # Test with coverage decrease
        decreased, diff = compare_coverage(90, 80)
        self.assertTrue(decreased)
        self.assertEqual(diff, 10)
        
        # Test with no change
        decreased, diff = compare_coverage(80, 80)
        self.assertFalse(decreased)
        self.assertEqual(diff, 0)

    def test_generate_comment(self):
        """Test generate_comment function."""
        comment = generate_comment(
            80, 90, 'false', 10,
            70, 75, 'false', 5
        )
        
        # Check that comment contains expected sections
        self.assertIn('Coverage Report', comment)
        self.assertIn('Extension Coverage', comment)
        self.assertIn('Webview Coverage', comment)
        self.assertIn('Overall Assessment', comment)
        
        # Check that comment contains coverage percentages
        self.assertIn('Base branch: 80%', comment)
        self.assertIn('PR branch: 90%', comment)
        self.assertIn('Base branch: 70%', comment)
        self.assertIn('PR branch: 75%', comment)
        
        # Check that comment contains correct assessment
        self.assertIn('Coverage increased or remained the same', comment)
        self.assertIn('Test coverage has been maintained or improved', comment)

    @patch('coverage_check.requests.get')
    @patch('coverage_check.requests.post')
    @patch('coverage_check.requests.patch')
    def test_post_comment_new(self, mock_patch, mock_post, mock_get):
        """Test post_comment function when creating a new comment."""
        # Create a temporary comment file
        comment_file = os.path.join(self.temp_dir.name, 'comment.md')
        with open(comment_file, 'w') as f:
            f.write('<!-- COVERAGE_REPORT -->\nTest comment')
        
        # Mock the API responses
        mock_get.return_value = MagicMock(status_code=200, json=lambda: [])
        mock_post.return_value = MagicMock(status_code=201)
        
        # Test post_comment function
        post_comment(comment_file, '123', 'owner/repo', 'token')
        
        # Check that the correct API calls were made
        mock_get.assert_called_once()
        mock_post.assert_called_once()
        mock_patch.assert_not_called()

    @patch('coverage_check.requests.get')
    @patch('coverage_check.requests.post')
    @patch('coverage_check.requests.patch')
    def test_post_comment_update(self, mock_patch, mock_post, mock_get):
        """Test post_comment function when updating an existing comment."""
        # Create a temporary comment file
        comment_file = os.path.join(self.temp_dir.name, 'comment.md')
        with open(comment_file, 'w') as f:
            f.write('<!-- COVERAGE_REPORT -->\nTest comment')
        
        # Mock the API responses
        mock_get.return_value = MagicMock(
            status_code=200, 
            json=lambda: [{'id': 456, 'body': '<!-- COVERAGE_REPORT -->\nOld comment'}]
        )
        mock_patch.return_value = MagicMock(status_code=200)
        
        # Test post_comment function
        post_comment(comment_file, '123', 'owner/repo', 'token')
        
        # Check that the correct API calls were made
        mock_get.assert_called_once()
        mock_patch.assert_called_once()
        mock_post.assert_not_called()

    def test_set_github_output(self):
        """Test set_github_output function."""
        # Capture stdout
        with patch('sys.stdout', new=MagicMock()) as mock_stdout:
            # Mock environment without GITHUB_OUTPUT
            with patch.dict('os.environ', {}, clear=True):
                set_github_output('test_name', 'test_value')
                
                # Check that the correct output was printed to stdout
                mock_stdout.assert_has_calls([
                    # GitHub Actions output format (deprecated method)
                    call.write('::set-output name=test_name::test_value\n'),
                    call.flush(),
                    # Human readable format
                    call.write('test_name: test_value\n'),
                    call.flush()
                ], any_order=False)
                
                # Reset mock for next test
                mock_stdout.reset_mock()
            
            # Test with GITHUB_OUTPUT environment variable
            with patch.dict('os.environ', {'GITHUB_OUTPUT': '/tmp/github_output'}), \
                 patch('builtins.open', mock_open()) as mock_file:
                set_github_output('test_name', 'test_value')
                
                # Check that file was written to
                mock_file.assert_called_once_with('/tmp/github_output', 'a')
                mock_file().write.assert_called_once_with('test_name=test_value\n')
                
                # Check that human readable output was printed
                mock_stdout.assert_has_calls([
                    call.write('test_name: test_value\n'),
                    call.flush()
                ], any_order=False)


if __name__ == '__main__':
    unittest.main()


--- .github/scripts/coverage_check/extraction.py ---
"""
Coverage extraction module.
This module handles extracting coverage percentages from coverage report files.
"""

import os
import re
import sys
import shlex
import subprocess
import traceback
from .util import log, file_exists, get_file_size, list_directory, is_safe_command, run_command

# Global verbose flag
verbose = False

def set_verbose(value):
    """Set the global verbose flag."""
    global verbose
    verbose = value

def print_debug_output(content, coverage_type):
    """
    Print debug information about the coverage output.
    
    Args:
        content: The content of the coverage file
        coverage_type: Type of coverage report (extension or webview)
    """
    if not verbose:
        return

    # Extract and print only the coverage summary section
    if coverage_type == "extension":
        # Look for the coverage summary section
        summary_match = re.search(r'=============================== Coverage summary ===============================\n(.*?)\n=+', content, re.DOTALL)
        if summary_match:
            sys.stdout.write("\n##[group]EXTENSION COVERAGE SUMMARY\n")
            sys.stdout.write("=============================== Coverage summary ===============================\n")
            sys.stdout.write(summary_match.group(1) + "\n")
            sys.stdout.write("================================================================================\n")
            sys.stdout.write("##[endgroup]\n")
            sys.stdout.flush()
        else:
            sys.stdout.write("\n##[warning]No coverage summary found in extension coverage file\n")
            sys.stdout.flush()
    else:  # webview
        # Look for the coverage table - specifically the "All files" row
        table_match = re.search(r'% Coverage report from v8.*?-+\|.*?\n.*?\n(All files.*?)(?:\n[^\n]*\|)', content, re.DOTALL)
        if table_match:
            sys.stdout.write("\n##[group]WEBVIEW COVERAGE SUMMARY\n")
            sys.stdout.write("% Coverage report from v8\n")
            sys.stdout.write("-------------------|---------|----------|---------|---------|-------------------\n")
            sys.stdout.write("File               | % Stmts | % Branch | % Funcs | % Lines | Uncovered Line #s \n")
            sys.stdout.write("-------------------|---------|----------|---------|---------|-------------------\n")
            sys.stdout.write(table_match.group(1) + "\n")
            sys.stdout.write("-------------------|---------|----------|---------|---------|-------------------\n")
            sys.stdout.write("##[endgroup]\n")
            sys.stdout.flush()
        else:
            sys.stdout.write("\n##[warning]No coverage table found in webview coverage file\n")
            sys.stdout.flush()

def extract_coverage(file_path, coverage_type="extension"):
    """
    Extract coverage percentage from a coverage report file.
    
    Args:
        file_path: Path to the coverage report file
        coverage_type: Type of coverage report (extension or webview)
        
    Returns:
        Coverage percentage as a float
    """
    
    # Always print file path for debugging
    log(f"Checking coverage file: {file_path}")
    
    # Check if file exists and get its size
    if not file_exists(file_path):
        sys.stdout.write(f"\n##[error]File {file_path} does not exist\n")
        sys.stdout.flush()
        log(f"Error: File {file_path} does not exist")
        
        # Check if the directory exists
        dir_path = os.path.dirname(file_path)
        if not os.path.exists(dir_path):
            sys.stdout.write(f"\n##[error]Directory {dir_path} does not exist\n")
            sys.stdout.flush()
            log(f"Error: Directory {dir_path} does not exist")
        else:
            # List directory contents for debugging
            log(f"Directory {dir_path} exists, listing contents:")
            try:
                dir_contents = list_directory(dir_path)
                for name, size in dir_contents:
                    log(f"  {name} - {size}")
                    sys.stdout.write(f"  {name} - {size}\n")
                sys.stdout.flush()
            except Exception as e:
                log(f"Error listing directory: {e}")
        
        return 0.0
    
    file_size = get_file_size(file_path)
    log(f"File size: {file_size} bytes")
    sys.stdout.write(f"\n##[info]Coverage file {file_path} exists, size: {file_size} bytes\n")
    sys.stdout.flush()
    
    if file_size == 0:
        sys.stdout.write(f"\n##[warning]File {file_path} is empty\n")
        sys.stdout.flush()
        log(f"Warning: File {file_path} is empty")
        return 0.0
    
    # List directory contents for debugging
    dir_path = os.path.dirname(file_path)
    log(f"Directory contents of {dir_path}:")
    try:
        dir_contents = list_directory(dir_path)
        for name, size in dir_contents:
            log(f"  {name} - {size}")
    except Exception as e:
        log(f"Error listing directory: {e}")
    
    with open(file_path, 'r') as f:
        content = f.read()
    
    # Print debug information if verbose
    print_debug_output(content, coverage_type)
    
    # Extract coverage percentage based on coverage type
    if coverage_type == "extension":
        # Extract the percentage from the "Lines" row in the coverage summary
        # Pattern: Lines : xx.xx% ( xxxxxxx/xxxxxxx )
        lines_match = re.search(r'Lines\s*:\s*(\d+\.\d+)%', content)
        if lines_match:
            coverage_pct = float(lines_match.group(1))
            if verbose:
                sys.stdout.write(f"Pattern matched (Lines percentage): {coverage_pct}\n")
                sys.stdout.flush()
            return coverage_pct
        else:
            # No coverage data found, log full content for debugging
            log("No coverage data found. Full file content:")
            log("=== Full file content ===")
            log(content)
            log("=== End file content ===")
    else:  # webview
        # Extract the percentage from the "% Lines" column in the "All files" row
        # Pattern: All files | xx.xx | xx.xx | xx.xx | xx.xx |
        all_files_match = re.search(r'All files\s+\|\s+\d+\.\d+\s+\|\s+\d+\.\d+\s+\|\s+\d+\.\d+\s+\|\s+(\d+\.\d+)', content)
        if all_files_match:
            coverage_pct = float(all_files_match.group(1))
            if verbose:
                sys.stdout.write(f"Pattern matched (All files % Lines): {coverage_pct}\n")
                sys.stdout.flush()
            return coverage_pct
        else:
            # No coverage data found, log full content for debugging
            log("No coverage data found. Full file content:")
            log("=== Full file content ===")
            log(content)
            log("=== End file content ===")
    
    # If no match found, return 0.0
    return 0.0

def compare_coverage(base_cov, pr_cov):
    """
    Compare coverage percentages between base and PR branches.
    
    Args:
        base_cov: Base branch coverage percentage
        pr_cov: PR branch coverage percentage
        
    Returns:
        Tuple of (decreased, diff)
    """
    try:
        base_cov = float(base_cov)
        pr_cov = float(pr_cov)
    except ValueError:
        sys.stdout.write(f"Error: Invalid coverage values - base: {base_cov}, PR: {pr_cov}\n")
        sys.stdout.flush()
        return False, 0
    
    diff = pr_cov - base_cov
    decreased = diff < 0
    
    return decreased, abs(diff)

def run_coverage(command, output_file, coverage_type="extension"):
    """
    Run a coverage command and extract the coverage percentage.
    
    Args:
        command: Command to run
        output_file: File to save the output to
        coverage_type: Type of coverage report (extension or webview)
        
    Returns:
        Coverage percentage as a float
    
    Raises:
        SystemExit: If the output file is not created or is empty
    """
    
    try:
        # Run the command and capture output
        if not is_safe_command(command):
            error_msg = f"ERROR: Unsafe command detected: {command}"
            log(error_msg)
            sys.stdout.write(f"\n##[error]{error_msg}\n")
            sys.stdout.flush()
            sys.exit(1)

        # Run command using safe execution from util
        returncode, stdout, stderr = run_command(command)
        
        # Log command result
        log(f"Command exit code: {returncode}")
        log(f"Command stdout length: {len(stdout)} bytes")
        log(f"Command stderr length: {len(stderr)} bytes")
        
        # Save output to file
        log(f"Saving command output to {output_file}")
        with open(output_file, 'w') as f:
            f.write(stdout)
            if stderr:
                f.write("\n\n=== STDERR ===\n")
                f.write(stderr)
        
        # Verify file was created and has content
        if not file_exists(output_file):
            error_msg = f"ERROR: Output file {output_file} was not created"
            log(error_msg)
            sys.stdout.write(f"\n##[error]{error_msg}\n")
            sys.stdout.flush()
            sys.exit(1)  # Exit with error code to fail the workflow
            
        file_size = get_file_size(output_file)
        if file_size == 0:
            error_msg = f"ERROR: Output file {output_file} is empty"
            log(error_msg)
            sys.stdout.write(f"\n##[error]{error_msg}\n")
            sys.stdout.flush()
            sys.exit(1)  # Exit with error code to fail the workflow
            
        log(f"Output file size: {file_size} bytes")
        
        # Extract coverage percentage
        coverage_pct = extract_coverage(output_file, coverage_type)
        
        log(f"{coverage_type.capitalize()} coverage: {coverage_pct}%")
        return coverage_pct
    
    except Exception as e:
        error_msg = f"Error running coverage command: {e}"
        log(error_msg)
        sys.stdout.write(f"\n##[error]{error_msg}\n")
        sys.stdout.flush()
        # Print stack trace for debugging
        log(traceback.format_exc())
        sys.exit(1)  # Exit with error code to fail the workflow


--- .github/scripts/coverage_check/__init__.py ---
"""
Coverage utility package for GitHub Actions workflows.
This package handles extracting coverage percentages, comparing them, and generating PR comments.
"""

# Import external dependencies
import requests

# Import main function for CLI usage
from .__main__ import main

# Import functions from extraction module
from .extraction import extract_coverage, compare_coverage, run_coverage, set_verbose

# Import functions from github_api module
from .github_api import generate_comment, post_comment, set_github_output

# Import functions from workflow module
from .workflow import process_coverage_workflow


--- .github/scripts/coverage_check/__main__.py ---
"""
Main module.
This module provides the CLI interface for the coverage utility script.
"""

import sys
import argparse

from .extraction import extract_coverage, compare_coverage, run_coverage, set_verbose
from .github_api import generate_comment, post_comment, set_github_output
from .workflow import process_coverage_workflow
from .util import log

def setup_verbose_mode(args):
    """
    Set up verbose mode based on command line arguments.
    
    Args:
        args: Parsed command line arguments
    """
    if getattr(args, 'verbose', False):
        set_verbose(True)
        log("Verbose mode enabled")

def main():
    # Create parent parser with common arguments
    parent_parser = argparse.ArgumentParser(add_help=False)
    parent_parser.add_argument('-v', '--verbose', action='store_true', help='Enable verbose output')

    # Create main parser that inherits common arguments
    parser = argparse.ArgumentParser(description='Coverage utility script for GitHub Actions workflows', parents=[parent_parser])
    subparsers = parser.add_subparsers(dest='command', help='Command to run')
    
    # extract-coverage command - used directly in workflow
    extract_parser = subparsers.add_parser('extract-coverage', help='Extract coverage percentage from a file', parents=[parent_parser])
    extract_parser.add_argument('file_path', help='Path to the coverage report file')
    extract_parser.add_argument('--type', choices=['extension', 'webview'], default='extension',
                               help='Type of coverage report')
    extract_parser.add_argument('--github-output', action='store_true', help='Output in GitHub Actions format')
    
    # compare-coverage command - used by process-workflow
    compare_parser = subparsers.add_parser('compare-coverage', help='Compare coverage percentages', parents=[parent_parser])
    compare_parser.add_argument('base_cov', help='Base branch coverage percentage')
    compare_parser.add_argument('pr_cov', help='PR branch coverage percentage')
    compare_parser.add_argument('--output-prefix', default='', help='Prefix for GitHub Actions output variables')
    compare_parser.add_argument('--github-output', action='store_true', help='Output in GitHub Actions format')
    
    # generate-comment command - used by process-workflow
    comment_parser = subparsers.add_parser('generate-comment', help='Generate PR comment with coverage comparison', parents=[parent_parser])
    comment_parser.add_argument('base_ext_cov', help='Base branch extension coverage')
    comment_parser.add_argument('pr_ext_cov', help='PR branch extension coverage')
    comment_parser.add_argument('ext_decreased', help='Whether extension coverage decreased (true/false)')
    comment_parser.add_argument('ext_diff', help='Extension coverage difference')
    comment_parser.add_argument('base_web_cov', help='Base branch webview coverage')
    comment_parser.add_argument('pr_web_cov', help='PR branch webview coverage')
    comment_parser.add_argument('web_decreased', help='Whether webview coverage decreased (true/false)')
    comment_parser.add_argument('web_diff', help='Webview coverage difference')
    
    # post-comment command - used by process-workflow
    post_parser = subparsers.add_parser('post-comment', help='Post a comment to a GitHub PR', parents=[parent_parser])
    post_parser.add_argument('comment_path', help='Path to the file containing the comment text')
    post_parser.add_argument('pr_number', help='PR number')
    post_parser.add_argument('repo', help='Repository in the format "owner/repo"')
    post_parser.add_argument('--token', help='GitHub token')
    
    # run-coverage command - used by process-workflow
    run_parser = subparsers.add_parser('run-coverage', help='Run a coverage command and extract the coverage percentage', parents=[parent_parser])
    run_parser.add_argument('coverage_cmd', help='Command to run')
    run_parser.add_argument('output_file', help='File to save the output to')
    run_parser.add_argument('--type', choices=['extension', 'webview'], default='extension',
                           help='Type of coverage report')
    run_parser.add_argument('--github-output', action='store_true', help='Output in GitHub Actions format')
    
    # process-workflow command - used directly in workflow
    workflow_parser = subparsers.add_parser('process-workflow', help='Process the entire coverage workflow', parents=[parent_parser])
    workflow_parser.add_argument('--base-branch', required=True, help='Base branch name')
    workflow_parser.add_argument('--pr-number', help='PR number')
    workflow_parser.add_argument('--repo', help='Repository in the format "owner/repo"')
    workflow_parser.add_argument('--token', help='GitHub token')
    
    # set-github-output command - used by process-workflow
    output_parser = subparsers.add_parser('set-github-output', help='Set GitHub Actions output variable', parents=[parent_parser])
    output_parser.add_argument('name', help='Output variable name')
    output_parser.add_argument('value', help='Output variable value')
    
    args = parser.parse_args()
    
    # Set up verbose mode
    setup_verbose_mode(args)
    
    if args.command == 'extract-coverage':
        log(f"Extracting coverage from file: {args.file_path} (type: {args.type})")
        coverage_pct = extract_coverage(args.file_path, args.type)
        if args.github_output:
            set_github_output(f"{args.type}_coverage", coverage_pct)
        else:
            log(f"Coverage: {coverage_pct}%")
        
    elif args.command == 'compare-coverage':
        log(f"Comparing coverage: base={args.base_cov}%, PR={args.pr_cov}%")
        decreased, diff = compare_coverage(args.base_cov, args.pr_cov)
        if args.github_output:
            prefix = args.output_prefix
            set_github_output(f"{prefix}decreased", str(decreased).lower())
            set_github_output(f"{prefix}diff", diff)
            log(f"Coverage difference: {diff}%")
            log(f"Coverage decreased: {decreased}")
        else:
            log(f"decreased={str(decreased).lower()}")
            log(f"diff={diff}")
        
    elif args.command == 'generate-comment':
        log("Generating coverage comparison comment")
        comment = generate_comment(
            args.base_ext_cov, args.pr_ext_cov, args.ext_decreased, args.ext_diff,
            args.base_web_cov, args.pr_web_cov, args.web_decreased, args.web_diff
        )
        # Output the comment to stdout
        log(comment)
        
    elif args.command == 'post-comment':
        log(f"Posting comment from {args.comment_path} to PR #{args.pr_number} in {args.repo}")
        post_comment(args.comment_path, args.pr_number, args.repo, args.token)
        
    elif args.command == 'run-coverage':
        log(f"Running coverage command: {args.coverage_cmd}")
        log(f"Output file: {args.output_file}")
        log(f"Coverage type: {args.type}")
        coverage_pct = run_coverage(args.coverage_cmd, args.output_file, args.type)
        if args.github_output:
            set_github_output(f"{args.type}_coverage", coverage_pct)
        else:
            log(f"Coverage: {coverage_pct}%")
        
    elif args.command == 'process-workflow':
        log("Processing coverage workflow")
        log(f"Base branch: {args.base_branch}")
        if args.pr_number:
            log(f"PR number: {args.pr_number}")
        if args.repo:
            log(f"Repository: {args.repo}")
        process_coverage_workflow(args)
        
    elif args.command == 'set-github-output':
        log(f"Setting GitHub output: {args.name}={args.value}")
        set_github_output(args.name, args.value)
    
    else:
        log("No command specified")
        parser.print_help()
        sys.exit(1)

if __name__ == "__main__":
    main()


--- .github/scripts/coverage_check/util.py ---
"""
Utility module.
This module provides utility functions used across the coverage check scripts.
"""

import os
import sys
import re
import shlex
import subprocess
import traceback
from typing import List, Tuple, Dict, Any, Optional, Union

# List of allowed commands and their arguments
ALLOWED_COMMANDS = {
    'xvfb-run': ['-a'],
    'npm': ['run', 'test:coverage', 'ci', 'install', '--no-save', '@vitest/coverage-v8', 'check-types', 'lint', 'format', 'compile'],
    'cd': ['webview-ui'],
    'python': ['-m', 'coverage_check'],
    'git': ['fetch', 'checkout', 'origin'],
}

def is_safe_command(command: Union[str, List[str]]) -> bool:
    """
    Check if a command is safe to execute.
    
    Args:
        command: Command to check (string or list)
        
    Returns:
        True if command is safe, False otherwise
    """
    # Convert string command to list
    if isinstance(command, str):
        try:
            cmd_parts = shlex.split(command)
        except ValueError:
            return False
    else:
        cmd_parts = command

    if not cmd_parts:
        return False

    # Get base command
    base_cmd = os.path.basename(cmd_parts[0])
    
    # Check if command is in allowed list
    if base_cmd not in ALLOWED_COMMANDS:
        return False
        
    # For each argument, check for suspicious patterns
    for arg in cmd_parts[1:]:
        # Check for shell metacharacters
        if re.search(r'[;&|`$]', arg):
            return False
        # Check for path traversal
        if '..' in arg and not (base_cmd == 'npm' and arg.startswith('@')):
            return False
            
    return True

def log(message: str) -> None:
    """
    Write a message to stdout and flush.
    
    Args:
        message: The message to write
    """
    sys.stdout.write(f"{message}\n")
    sys.stdout.flush()

def file_exists(file_path: str) -> bool:
    """
    Check if a file exists.
    
    Args:
        file_path: Path to the file
        
    Returns:
        True if the file exists, False otherwise
    """
    return os.path.exists(file_path) and os.path.isfile(file_path)

def get_file_size(file_path: str) -> int:
    """
    Get the size of a file in bytes.
    
    Args:
        file_path: Path to the file
        
    Returns:
        Size of the file in bytes, or 0 if the file doesn't exist
    """
    if file_exists(file_path):
        return os.path.getsize(file_path)
    return 0

def list_directory(dir_path: str) -> List[Tuple[str, Union[int, str]]]:
    """
    List the contents of a directory.
    
    Args:
        dir_path: Path to the directory
        
    Returns:
        List of (name, size) tuples for each file/directory in the directory
    """
    if not os.path.exists(dir_path) or not os.path.isdir(dir_path):
        return []
    
    contents = []
    for item in os.listdir(dir_path):
        item_path = os.path.join(dir_path, item)
        if os.path.isfile(item_path):
            contents.append((item, os.path.getsize(item_path)))
        else:
            contents.append((item, "DIR"))
    
    return contents

def read_file_content(file_path: str, default: str = "") -> str:
    """
    Read file content with error handling.
    
    Args:
        file_path: Path to the file
        default: Default value to return if file cannot be read
        
    Returns:
        File content or default value
    """
    if not file_exists(file_path):
        log(f"File does not exist: {file_path}")
        return default
    
    try:
        with open(file_path, 'r') as f:
            return f.read()
    except Exception as e:
        log(f"Error reading file {file_path}: {e}")
        return default

def write_file_content(file_path: str, content: str) -> bool:
    """
    Write content to file with error handling.
    
    Args:
        file_path: Path to the file
        content: Content to write
        
    Returns:
        True if successful, False otherwise
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        with open(file_path, 'w') as f:
            f.write(content)
        return True
    except Exception as e:
        log(f"Error writing to file {file_path}: {e}")
        return False

def run_command(command: Union[str, List[str]], capture_output: bool = True) -> Tuple[int, str, str]:
    """
    Run a command and return the result.
    
    Args:
        command: Command to run (string or list)
        capture_output: Whether to capture stdout/stderr
        
    Returns:
        Tuple of (returncode, stdout, stderr)
    """
    if not is_safe_command(command):
        error_msg = f"Unsafe command detected: {command}"
        log(error_msg)
        return 1, "", error_msg
        
    log(f"Running command: {command}")
    try:
        # Convert string command to list
        if isinstance(command, str):
            cmd_list = shlex.split(command)
        else:
            cmd_list = command
            
        result = subprocess.run(
            cmd_list,
            shell=False,  # Never use shell=True for security
            capture_output=capture_output,
            text=True
        )
        log(f"Command exit code: {result.returncode}")
        return result.returncode, result.stdout, result.stderr
    except Exception as e:
        log(f"Error running command: {e}")
        log(traceback.format_exc())
        return 1, "", str(e)

def find_pattern(content: str, pattern: str, group: int = 0, 
                default: Optional[str] = None) -> Optional[str]:
    """
    Find a pattern in content and return the specified group.
    
    Args:
        content: Text content to search
        pattern: Regex pattern to search for
        group: Group number to return (default: 0 for entire match)
        default: Default value to return if pattern not found
        
    Returns:
        Matched text or default value
    """
    match = re.search(pattern, content, re.DOTALL)
    if match:
        return match.group(group)
    return default

def get_env_var(name: str, default: Optional[str] = None) -> Optional[str]:
    """
    Get environment variable with default value.
    
    Args:
        name: Environment variable name
        default: Default value if not set
        
    Returns:
        Environment variable value or default
    """
    return os.environ.get(name, default)

def format_exception(e: Exception) -> str:
    """
    Format an exception with traceback for logging.
    
    Args:
        e: Exception to format
        
    Returns:
        Formatted exception string
    """
    return f"{type(e).__name__}: {str(e)}\n{traceback.format_exc()}"


--- .github/scripts/coverage_check/workflow.py ---
"""
Workflow module.
This module handles the main workflow logic for running coverage tests and processing results.
"""

import os
import re
import sys
import subprocess
import traceback

from .extraction import run_coverage, compare_coverage, extract_coverage
from .github_api import generate_comment, post_comment, set_github_output
from .util import log, file_exists, get_file_size, list_directory, run_command

def is_valid_branch_name(branch_name: str) -> bool:
    """
    Validate a git branch name.
    
    Args:
        branch_name: Branch name to validate
        
    Returns:
        True if valid, False otherwise
    """
    # Check for common branch name patterns
    if not re.match(r'^[a-zA-Z0-9_\-./]+$', branch_name):
        return False
    
    # Check for path traversal
    if '..' in branch_name:
        return False
        
    # Check for shell metacharacters
    if re.search(r'[;&|`$]', branch_name):
        return False
        
    return True

def checkout_branch(branch_name: str) -> None:
    """
    Checkout a branch for testing.
    
    Args:
        branch_name: Branch name to checkout
        
    Raises:
        RuntimeError: If branch checkout fails
        ValueError: If branch name is invalid
    """
    if not is_valid_branch_name(branch_name):
        raise ValueError(f"Invalid branch name: {branch_name}")
        
    log(f"=== Checking out branch: {branch_name} ===")
    
    # Fetch the branch
    returncode, stdout, stderr = run_command(['git', 'fetch', 'origin', branch_name])
    if returncode != 0:
        log(f"ERROR: Failed to fetch branch {branch_name}")
        log(f"Error details: {stderr}")
        raise RuntimeError(f"Git fetch failed: {stderr}")
    
    # Checkout the branch
    returncode, stdout, stderr = run_command(['git', 'checkout', branch_name])
    if returncode != 0:
        log(f"ERROR: Failed to checkout branch {branch_name}")
        log(f"Error details: {stderr}")
        raise RuntimeError(f"Git checkout failed: {stderr}")
    
    log(f"Successfully checked out branch: {branch_name}")

def extract_extension_coverage_from_file(file_path):
    """Extract extension coverage from file when run_coverage returns 0."""
    if not file_exists(file_path):
        log(f"File {file_path} does not exist, cannot extract extension coverage")
        return 0.0
    
    file_size = get_file_size(file_path)
    if file_size == 0:
        log(f"File {file_path} is empty, cannot extract extension coverage")
        return 0.0
        
    log(f"Extension coverage is 0.0, trying to read from file directly: {file_path} (size: {file_size} bytes)")
    with open(file_path, 'r') as f:
        content = f.read()
        # Extract the percentage from the "Lines" row in the coverage summary
        # Pattern: Lines : xx.xx% ( xxxxxxx/xxxxxxx )
        lines_match = re.search(r'Lines\s*:\s*(\d+\.\d+)%', content)
        if lines_match:
            coverage = float(lines_match.group(1))
            log(f"Found extension coverage in file: {coverage}%")
            return coverage
    return 0.0

def extract_webview_coverage_from_file(file_path):
    """Extract webview coverage from file when run_coverage returns 0."""
    if not file_exists(file_path):
        log(f"File {file_path} does not exist, cannot extract webview coverage")
        return 0.0
    
    file_size = get_file_size(file_path)
    if file_size == 0:
        log(f"File {file_path} is empty, cannot extract webview coverage")
        return 0.0
        
    log(f"Webview coverage is 0.0, trying to read from file directly: {file_path} (size: {file_size} bytes)")
    with open(file_path, 'r') as f:
        content = f.read()
        # Extract the percentage from the "% Lines" column in the "All files" row
        # Pattern: All files | xx.xx | xx.xx | xx.xx | xx.xx |
        all_files_match = re.search(r'All files\s+\|\s+\d+\.\d+\s+\|\s+\d+\.\d+\s+\|\s+\d+\.\d+\s+\|\s+(\d+\.\d+)', content)
        if all_files_match:
            coverage = float(all_files_match.group(1))
            log(f"Found webview coverage in file: {coverage}%")
            return coverage
    return 0.0

def run_extension_coverage(branch_name=None):
    """Run extension coverage tests and extract results."""
    prefix = 'base_' if branch_name else ''
    file_path = f"{prefix}extension_coverage.txt"
    
    # Run coverage tests
    ext_cov = run_coverage(
        ["xvfb-run", "-a", "npm", "run", "test:coverage"], 
        file_path, 
        "extension"
    )
    
    # If coverage is 0.0, try to extract from file directly
    if ext_cov == 0.0:
        ext_cov = extract_extension_coverage_from_file(file_path)
        
    return ext_cov

def run_webview_coverage(branch_name=None):
    """Run webview coverage tests and extract results."""
    prefix = 'base_' if branch_name else ''
    file_path = f"{prefix}webview_coverage.txt"
    
    # Save current directory
    original_dir = os.getcwd()
    
    try:
        # Change to webview-ui directory
        os.chdir('webview-ui')
        
        # Install coverage dependency
        returncode, stdout, stderr = run_command(["npm", "install", "--no-save", "@vitest/coverage-v8"])
        if returncode != 0:
            log(f"Failed to install coverage dependency: {stderr}")
            return 0.0
        
        # Run coverage tests from webview-ui directory
        web_cov = run_coverage(
            ["npm", "run", "test:coverage"],
            os.path.join('..', file_path),
            "webview"
        )
    finally:
        # Always change back to original directory
        os.chdir(original_dir)
    
    # If coverage is 0.0, try to extract from file directly
    if web_cov == 0.0:
        web_cov = extract_webview_coverage_from_file(file_path)
        
    return web_cov

def run_branch_coverage(branch_name=None):
    """
    Run coverage tests for a branch.
    
    Args:
        branch_name: Name of the branch to checkout before running tests (optional)
        
    Returns:
        Tuple of (extension_coverage, webview_coverage)
    """
    # Checkout branch if specified
    if branch_name:
        checkout_branch(branch_name)
    
    # Run coverage tests
    log(f"=== Running coverage tests{' for ' + branch_name if branch_name else ''} ===")
    
    # Run extension and webview coverage
    ext_cov = run_extension_coverage(branch_name)
    web_cov = run_webview_coverage(branch_name)
    
    return ext_cov, web_cov

def find_potential_coverage_files():
    """Find potential coverage files in the current directory and webview-ui."""
    log("Searching for potential coverage files...")
    
    # Find files in current directory
    current_dir_files = list_directory('.')
    for name, size in current_dir_files:
        if 'coverage' in name.lower() and size != "DIR":
            log(f"Found potential coverage file: {name} (size: {size} bytes)")
    
    # Find files in webview-ui directory
    if os.path.exists('webview-ui') and os.path.isdir('webview-ui'):
        webview_files = list_directory('webview-ui')
        for name, size in webview_files:
            if 'coverage' in name.lower() and size != "DIR":
                log(f"Found potential webview coverage file: webview-ui/{name} (size: {size} bytes)")
    else:
        log("webview-ui directory not found")

def generate_warnings(base_ext_cov, pr_ext_cov, ext_decreased, ext_diff, 
                     base_web_cov, pr_web_cov, web_decreased, web_diff):
    """Generate warnings for coverage decreases."""
    if not (ext_decreased or web_decreased):
        return []
        
    warnings = [
        "Test coverage has decreased in this PR",
        f"Extension coverage: {base_ext_cov}% -> {pr_ext_cov}% (Diff: {ext_diff}%)",
        f"Webview coverage: {base_web_cov}% -> {pr_web_cov}% (Diff: {web_diff}%)"
    ]
    
    # Additional warning for significant decrease (more than 1%)
    if ext_decreased and ext_diff > 1.0:
        warnings.append(f"Extension coverage decreased by more than 1% ({ext_diff}%). Consider adding tests to cover your changes.")
    
    if web_decreased and web_diff > 1.0:
        warnings.append(f"Webview coverage decreased by more than 1% ({web_diff}%). Consider adding tests to cover your changes.")
        
    return warnings

def output_warnings(warnings):
    """Output warnings to GitHub step summary and console."""
    if not warnings:
        return
        
    # Get the GitHub step summary file path from environment variable
    github_step_summary = os.environ.get('GITHUB_STEP_SUMMARY')
    
    # Write to GitHub step summary if available
    if github_step_summary:
        with open(github_step_summary, 'a') as f:
            f.write("## Coverage Warnings\n\n")
            for warning in warnings:
                f.write(f"⚠️ {warning}\n\n")
    
    # Also output to console with ::warning:: syntax for backward compatibility
    for warning in warnings:
        log(f"::warning::{warning}")

def output_github_results(pr_ext_cov, pr_web_cov, base_ext_cov, base_web_cov, 
                         ext_decreased, ext_diff, web_decreased, web_diff):
    """Output results for GitHub Actions."""
    set_github_output("pr_extension_coverage", pr_ext_cov)
    set_github_output("pr_webview_coverage", pr_web_cov)
    set_github_output("base_extension_coverage", base_ext_cov)
    set_github_output("base_webview_coverage", base_web_cov)
    set_github_output("extension_decreased", str(ext_decreased).lower())
    set_github_output("extension_diff", ext_diff)
    set_github_output("webview_decreased", str(web_decreased).lower())
    set_github_output("webview_diff", web_diff)

def extract_pr_coverage_from_artifacts():
    """
    Extract PR branch coverage from artifact files.
    
    Returns:
        Tuple of (extension_coverage, webview_coverage)
        
    Raises:
        SystemExit: If the coverage files don't exist
    """
    log("=== Extracting PR branch coverage from artifacts ===")
    
    # Check if the coverage files exist
    ext_file_path = "extension_coverage.txt"
    web_file_path = "webview-ui/webview_coverage.txt"
    
    # Extract extension coverage
    log(f"Extracting extension coverage from {ext_file_path}")
    if not file_exists(ext_file_path):
        error_msg = f"ERROR: PR extension coverage file {ext_file_path} not found"
        log(error_msg)
        
        # List directory contents for debugging
        log("Current directory contents:")
        try:
            dir_contents = list_directory('.')
            for name, size in dir_contents:
                log(f"  {name} - {size}\n")
        except Exception as e:
            log(f"Error listing directory: {e}")
        
        sys.exit(1)  # Exit with error code to fail the workflow
    
    ext_cov = extract_extension_coverage_from_file(ext_file_path)
    log(f"PR extension coverage from artifact: {ext_cov}%")
    
    # Extract webview coverage
    log(f"Extracting webview coverage from {web_file_path}")
    if not file_exists(web_file_path):
        error_msg = f"ERROR: PR webview coverage file {web_file_path} not found"
        log(error_msg)
        
        # Check if the webview-ui directory exists
        if not os.path.exists('webview-ui'):
            log("ERROR: webview-ui directory not found")
        else:
            # List webview-ui directory contents for debugging
            log("webview-ui directory contents:")
            try:
                dir_contents = list_directory('webview-ui')
                for name, size in dir_contents:
                    log(f"  {name} - {size}")
            except Exception as e:
                log(f"Error listing directory: {e}")
        
        sys.exit(1)  # Exit with error code to fail the workflow
    
    web_cov = extract_webview_coverage_from_file(web_file_path)
    log(f"PR webview coverage from artifact: {web_cov}%")
    
    return ext_cov, web_cov

def process_coverage_workflow(args):
    """
    Process the entire coverage workflow.
    
    Args:
        args: Command line arguments
    """
    # Initialize all variables at the start
    pr_ext_cov = 0.0
    pr_web_cov = 0.0
    base_ext_cov = 0.0
    base_web_cov = 0.0
    ext_decreased = False
    ext_diff = 0.0
    web_decreased = False
    web_diff = 0.0
    
    try:
        # Validate branch name
        if not is_valid_branch_name(args.base_branch):
            raise ValueError(f"Invalid base branch name: {args.base_branch}")
        
        # Check if we're running in GitHub Actions
        is_github_actions = 'GITHUB_ACTIONS' in os.environ
        if is_github_actions:
            log("Running in GitHub Actions environment")
        
        # Extract PR branch coverage from artifacts (from test job)
        pr_ext_cov, pr_web_cov = extract_pr_coverage_from_artifacts()
        
        # Verify PR coverage values
        if pr_ext_cov == 0.0:
            log("WARNING: PR extension coverage is 0.0, this may indicate an issue with the coverage report")
            find_potential_coverage_files()
        
        if pr_web_cov == 0.0:
            log("WARNING: PR webview coverage is 0.0, this may indicate an issue with the coverage report")
            find_potential_coverage_files()
        
        # Run base branch coverage
        log(f"=== Running base branch coverage for {args.base_branch} ===")
        base_ext_cov, base_web_cov = run_branch_coverage(args.base_branch)
        
        # Verify base coverage values
        if base_ext_cov == 0.0:
            log("WARNING: Base extension coverage is 0.0, this may indicate an issue with the coverage report")
        
        if base_web_cov == 0.0:
            log("WARNING: Base webview coverage is 0.0, this may indicate an issue with the coverage report")
        
        # Compare coverage
        log("=== Comparing extension coverage ===")
        ext_decreased, ext_diff = compare_coverage(base_ext_cov, pr_ext_cov)
        
        log("=== Comparing webview coverage ===")
        web_decreased, web_diff = compare_coverage(base_web_cov, pr_web_cov)
        
        # Print summary of coverage values
        log("\n=== Coverage Summary ===")
        log(f"PR extension coverage: {pr_ext_cov}%")
        log(f"Base extension coverage: {base_ext_cov}%")
        log(f"Extension coverage change: {'+' if not ext_decreased else '-'}{ext_diff}%")
        log(f"PR webview coverage: {pr_web_cov}%")
        log(f"Base webview coverage: {base_web_cov}%")
        log(f"Webview coverage change: {'+' if not web_decreased else '-'}{web_diff}%")
        
        # Generate and output warnings
        warnings = generate_warnings(
            base_ext_cov, pr_ext_cov, ext_decreased, ext_diff,
            base_web_cov, pr_web_cov, web_decreased, web_diff
        )
        output_warnings(warnings)
        
        # Generate comment
        log("=== Generating comment ===")
        comment = generate_comment(
            base_ext_cov, pr_ext_cov, str(ext_decreased).lower(), ext_diff,
            base_web_cov, pr_web_cov, str(web_decreased).lower(), web_diff
        )
        
        # Save comment to file
        with open("coverage_comment.md", "w") as f:
            f.write(comment)
        
        # Post comment if PR number is provided
        if args.pr_number:
            log(f"=== Posting comment to PR #{args.pr_number} ===")
            post_comment("coverage_comment.md", args.pr_number, args.repo, args.token)
        
        # Output results for GitHub Actions
        output_github_results(
            pr_ext_cov, pr_web_cov, base_ext_cov, base_web_cov,
            ext_decreased, ext_diff, web_decreased, web_diff
        )
        
    except Exception as e:
        log(f"ERROR in process_coverage_workflow: {e}")
        traceback.print_exc()
        
        # Try to output results even if there was an error
        try:
            output_github_results(
                pr_ext_cov, pr_web_cov, base_ext_cov, base_web_cov,
                ext_decreased, ext_diff, web_decreased, web_diff
            )
        except Exception as e2:
            log(f"ERROR outputting GitHub results: {e2}")


--- cli/README.md ---
# Cline

<p align="center">
  <img src="https://github.com/user-attachments/assets/7123f9d1-afeb-48d5-93fa-e750dec0ebba" width="70%" />
</p>

<div align="center">
<table>
<tbody>
<td align="center">
<a href="https://www.npmjs.com/package/cline" target="_blank"><strong>NPM</strong></a>
</td>
<td align="center">
<a href="https://discord.gg/cline" target="_blank"><strong>Discord</strong></a>
</td>
<td align="center">
<a href="https://www.reddit.com/r/cline/" target="_blank"><strong>r/cline</strong></a>
</td>
<td align="center">
<a href="https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop" target="_blank"><strong>Feature Requests</strong></a>
</td>
<td align="center">
<a href="https://docs.cline.bot/getting-started/for-new-coders" target="_blank"><strong>Getting Started</strong></a>
</td>
</tbody>
</table>
</div>

Meet Cline, an AI assistant that lives in your terminal.

Cline can handle complex software development tasks step-by-step. With tools that let him create & edit files, explore large projects, use the browser, and execute terminal commands (after you grant permission), he can assist you in ways that go beyond code completion or tech support.

```bash
npm i -g cline

# cd into your project and run:
cline
```

> Move your mouse around under the Cline icon for a surprise!

---

<img align="right" width="340" src="https://github.com/user-attachments/assets/ceb74224-08aa-4b8b-a3e7-b438ac3d160a">

### Use any API and Model

Cline supports API providers like ChatGPT, Anthropic, OpenAI, Google Gemini, AWS Bedrock, Azure, GCP Vertex, Cerebras, Groq, and Moonshot. You can also configure any OpenAI compatible API, or use a local model through LM Studio/Ollama. If you're using a Cline Account, you'll always have access to the newest models as soon as they're available.

<!-- Transparent pixel to create line break after floating image -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="370" src="https://github.com/user-attachments/assets/cad091f6-6c0f-4e4b-97ea-a1ff67e39b9b">

### Stay in Control with Human-in-the-Loop

Cline asks for your approval before running commands, editing files, or taking any action. Review each step and approve or reject as you go—or enable auto-approve to let Cline work autonomously to completion.

<!-- Transparent pixel to create line break after floating image -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/cad091f6-6c0f-4e4b-97ea-a1ff67e39b9b"><br>

<img align="right" width="400" src="https://github.com/user-attachments/assets/4f264a0c-3802-49a7-8e5e-13d97beb659e">

### Plan & Act Modes

Toggle to Plan Mode to discuss implementation and architecture with Cline. He'll ask clarifying questions, explore your codebase, and present a plan for you to align on. Once you're satisfied, switch to Act Mode and let Cline execute the plan.

<!-- Transparent pixel to create line break after floating image -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/4f264a0c-3802-49a7-8e5e-13d97beb659e"><br>


## Enterprise

Get the same Cline experience with enterprise-grade controls: SSO (SAML/OIDC), global policies and configuration, observability with audit trails, private networking (VPC/private link), and self-hosted or on-prem deployments, and enterprise support. Learn more at our [enterprise page](https://cline.bot/enterprise) or [talk to us](https://cline.bot/contact-sales).

## License

[Apache 2.0 © 2026 Cline Bot Inc.](./LICENSE)


## Links discovered
- [enterprise page](https://cline.bot/enterprise)
- [talk to us](https://cline.bot/contact-sales)
- [Apache 2.0 © 2026 Cline Bot Inc.](https://github.com/cline/cline/blob/main/cli/LICENSE.md)
- [<strong>NPM</strong>](https://www.npmjs.com/package/cline)
- [<strong>Discord</strong>](https://discord.gg/cline)
- [<strong>r/cline</strong>](https://www.reddit.com/r/cline/)
- [<strong>Feature Requests</strong>](https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop)
- [<strong>Getting Started</strong>](https://docs.cline.bot/getting-started/for-new-coders)

--- cli/DEVELOPMENT.md ---
# Cline CLI

The official CLI for Cline. Run Cline tasks directly from the terminal with the same underlying functionality as the VS Code extension.

## Features

- **Reuses Core Codebase**: Shares the same Controller, Task, and API handling as the VS Code extension
- **Terminal Output**: Displays Cline messages directly in your terminal with colored output
- **Task History**: Access your task history from the command line
- **Configurable**: Use custom configuration directories and working directories
- **Image Support**: Attach images to your prompts using file paths or inline references

## Prerequisites

- Node.js 20.x or later
- npm or yarn
- The parent Cline project dependencies installed

## Installation

From the repository root:

```bash
# Install all dependencies first
npm run install:all

# Ensure protos are generated
npm run protos

# Build and link the CLI globally
npm run cli:link
```

## Usage

### Interactive Mode (Default)

When you run `cline` without any command, it launches an interactive welcome prompt:

```bash
# Launch interactive mode
cline

# Or run a task directly
cline "Create a hello world function in Python"

# With options
cline -v --thinking "Analyze this codebase"
```

### Commands

#### `task` (alias: `t`)

Run a new task with a prompt.

```bash
cline task "Create a hello world function in Python"
cline t "Create a hello world function"
```

**Options:**

| Option | Description |
|--------|-------------|
| `-a, --act` | Run in act mode |
| `-p, --plan` | Run in plan mode |
| `-y, --yolo` | Enable yolo mode (auto-approve actions) |
| `-m, --model <model>` | Model to use for the task |
| `-i, --images <paths...>` | Image file paths to include with the task |
| `-v, --verbose` | Show verbose output including reasoning |
| `-c, --cwd <path>` | Working directory for the task |
| `--config <path>` | Path to Cline configuration directory |
| `-t, --thinking` | Enable extended thinking (1024 token budget) |

**Examples:**

```bash
# Run in plan mode with verbose output
cline task -p -v "Design a REST API"

# Use a specific model with yolo mode
cline task -m claude-sonnet-4-5-20250929 -y "Refactor this function"

# Include images with your prompt
cline task -i screenshot.png diagram.jpg "Fix the UI based on these images"

# Or use inline image references in the prompt
cline task "Fix the layout shown in @./screenshot.png"

# Enable extended thinking for complex tasks
cline task -t "Architect a microservices system"

# Specify working directory
cline task -c /path/to/project "Add unit tests"
```

#### `history` (alias: `h`)

List task history with pagination support.

```bash
cline history
cline h
```

**Options:**

| Option | Description |
|--------|-------------|
| `-n, --limit <number>` | Number of tasks to show (default: 10) |
| `-p, --page <number>` | Page number, 1-based (default: 1) |
| `--config <path>` | Path to Cline configuration directory |

**Examples:**

```bash
# Show last 10 tasks (default)
cline history

# Show 20 tasks
cline history -n 20

# Show page 2 with 5 tasks per page
cline history -n 5 -p 2
```

#### `config`

Show current configuration including global and workspace state.

```bash
cline config
```

**Options:**

| Option | Description |
|--------|-------------|
| `--config <path>` | Path to Cline configuration directory |

#### `auth`

Authenticate a provider and configure what model is used.

```bash
cline auth
```

**Options:**

| Option | Description |
|--------|-------------|
| `-p, --provider <id>` | Provider ID for quick setup (e.g., openai-native, anthropic) |
| `-k, --apikey <key>` | API key for the provider |
| `-m, --modelid <id>` | Model ID to configure (e.g., gpt-4o, claude-sonnet-4-5-20250929) |
| `-b, --baseurl <url>` | Base URL (optional, only for openai provider) |
| `-v, --verbose` | Show verbose output |
| `-c, --cwd <path>` | Working directory for the task |
| `--config <path>` | Path to Cline configuration directory |

**Examples:**

```bash
# Interactive authentication
cline auth

# Quick setup with provider and API key
cline auth -p anthropic -k sk-ant-xxxxx

# Full quick setup with model
cline auth -p openai-native -k sk-xxxxx -m gpt-4o

# OpenAI-compatible provider with custom base URL
cline auth -p openai -k your-api-key -b https://api.example.com/v1
```

### Global Options

These options are available for the default command (running a task directly):

| Option | Description |
|--------|-------------|
| `-v, --verbose` | Show verbose output |
| `-c, --cwd <path>` | Working directory |
| `--config <path>` | Configuration directory |
| `--thinking` | Enable extended thinking (1024 token budget) |

## Development

### Quick Start

```bash
# 1. Install all dependencies (root, webview-ui, cli)
npm run install:all

# 2. Build and link globally so you can run `cline` from anywhere
npm run cli:link

# 3. Test it
cline --help
```

### Scripts

Run these from the repository root:

| Script | Description |
|--------|-------------|
| `npm run install:all` | Install deps for root, webview-ui, and cli |
| `npm run cli:build` | Generate protos and build CLI |
| `npm run cli:build:production` | Production build (minified) |
| `npm run cli:link` | Build and `npm link` so you can run `cline` from anywhere |
| `npm run cli:unlink` | Remove the global `cline` symlink |
| `npm run cli:dev` | Link + watch mode for development |
| `npm run cli:watch` | Watch mode only (no initial build) |
| `npm run cli:test` | Run CLI tests |

### Development Workflow

1. Run `npm run cli:dev` - this links the CLI globally and starts watch mode
2. Make changes to files in `cli/src/`
3. The build automatically rebuilds on save
4. Test your changes by running `cline` in another terminal
5. When done, run `npm run cli:unlink` to clean up

### Proto Generation

The CLI uses proto-generated types for message passing (same as the VS Code extension). If you modify any `.proto` files, run:

```bash
npm run protos
```

This generates TypeScript types in `src/generated/` that both the CLI and extension use.

## Publish

#### 1. Publish to npm
```bash
npm publish
```

#### 2. Update the Homebrew formula
```bash
npm run update-brew-formula
```

#### 3. Test the formula locally
```bash
# Create a local tap
brew tap-new cline/local
cp ./cli/cline.rb "$(brew --repository)/Library/Taps/cline/homebrew-local/Formula/cline.rb"

# Build from Source
brew install --build-from-source cline/local/cline

# Install from your local tap
brew install cline/local/cline

# Clean up when done
brew untap cline/local
```

#### 4. If using a tap, commit and push
```bash
git add cline.rb
git commit -m "Update cline to v2.0.0"
git push
```

## Architecture

### How It Works

The CLI directly imports and reuses the core Cline TypeScript codebase (the same code that powers the VS Code extension). This means feature parity is easy to maintain - when core gets updated, the CLI automatically benefits.

```
┌─────────────────────────────────────────────────────────┐
│                     CLI (cli/)                          │
│  - React Ink terminal UI                                │
│  - Command parsing (commander)                          │
│  - Terminal-specific adapters                           │
└─────────────────────────────────────────────────────────┘
                          │
                          │ direct imports
                          ▼
┌─────────────────────────────────────────────────────────┐
│                  Core (src/core/)                       │
│  - Controller: task lifecycle, state management         │
│  - Task: AI API calls, tool execution                   │
│  - StateManager: persistent storage                     │
│  - Proto types: message definitions                     │
└─────────────────────────────────────────────────────────┘
```

Unlike a client-server architecture, the CLI runs everything in a single Node.js process. The "host bridge" pattern provides terminal-appropriate implementations for things the VS Code extension would handle differently (clipboard, file dialogs, etc.).

### Key Files

| File | Purpose |
|------|---------|
| `src/index.ts` | Entry point, command definitions |
| `src/components/App.tsx` | Main React Ink app |
| `src/components/ChatView.tsx` | Task conversation UI |
| `src/controllers/CliWebviewProvider.ts` | Bridges core messages to terminal output |
| `src/vscode-context.ts` | Mock VS Code extension context for core compatibility |
| `src/vscode-shim.ts` | Shims for VS Code APIs that core depends on |
| `src/constants/colors.ts` | Terminal color definitions |

### React Ink

The CLI uses [React Ink](https://github.com/vadimdemedes/ink) for its terminal UI. This lets us build the interface with React components that render to the terminal. Key patterns:

- Components in `src/components/` render terminal UI
- Hooks in `src/hooks/` manage terminal-specific state (size, scrolling)
- The `useStateSubscriber` hook subscribes to core state changes

## Configuration

The CLI stores its data in `~/.cline/data/` by default:

- `globalState.json`: Global settings and state
- `secrets.json`: API keys and secrets
- `workspace/`: Workspace-specific state
- `tasks/`: Task history and conversation data

Override with the `--config` option or `CLINE_DIR` environment variable.

## Troubleshooting

### Build Errors

If you encounter build errors:

```bash
# Make sure all deps are installed
npm run install:all

# Regenerate proto types
npm run protos

# Then rebuild
npm run cli:build
```

### "command not found: cline"

The CLI isn't linked globally. Run:

```bash
npm run cli:link
```

### Changes Not Reflected

If your code changes aren't showing up:

1. Make sure watch mode is running (`npm run cli:dev`)
2. Check for TypeScript errors in the watch output
3. Try unlinking and relinking: `npm run cli:unlink && npm run cli:link`

### Import Errors from Core

The CLI imports from `@core/`, `@shared/`, etc. These paths are defined in the root `tsconfig.json`. If you see import errors, make sure you're building from the repo root, not from inside `cli/`.

## Links discovered
- [React Ink](https://github.com/vadimdemedes/ink)

--- cli/vitest.config.ts ---
import path from "path"
import { defineConfig } from "vitest/config"

export default defineConfig({
	test: {
		globals: true,
		environment: "node",
		include: ["src/**/*.test.{ts,tsx}", "tests/**/*.test.{ts,tsx}"],
		coverage: {
			reporter: ["text", "json", "html"],
			exclude: ["node_modules/", "dist/"],
		},
	},
	resolve: {
		alias: {
			vscode: path.resolve(__dirname, "src/vscode-shim.ts"),
			// Match tsconfig paths - baseUrl is parent directory
			"@": path.resolve(__dirname, "../src"),
			"@api": path.resolve(__dirname, "../src/core/api"),
			"@core": path.resolve(__dirname, "../src/core"),
			"@generated": path.resolve(__dirname, "../src/generated"),
			"@hosts": path.resolve(__dirname, "../src/hosts"),
			"@integrations": path.resolve(__dirname, "../src/integrations"),
			"@packages": path.resolve(__dirname, "../src/packages"),
			"@services": path.resolve(__dirname, "../src/services"),
			"@shared": path.resolve(__dirname, "../src/shared"),
			"@utils": path.resolve(__dirname, "../src/utils"),
		},
	},
})


--- cli/man/cline.1.md ---
---
title: CLINE
section: 1
header: User Commands
footer: Cline CLI 2.0
date: January 2026
---

# NAME

cline - AI coding assistant in your terminal

# SYNOPSIS

**cline** [*prompt*] [*options*]

**cline** *command* [*options*] [*arguments*]

# DESCRIPTION

**cline** is a command-line interface for the Cline AI coding assistant. It provides the same powerful AI capabilities as the VS Code extension, directly in your terminal.

Cline is an autonomous AI agent that can read, write, and execute code across your projects. He can create and edit files, run terminal commands, use a headless browser, and more—all while asking for your approval before taking actions.

The CLI supports both interactive mode (with a rich terminal UI) and plain text mode (for piped input and scripted workflows).

# MODES OF OPERATION

**Interactive Mode** :   When you run **cline** without arguments, it launches an interactive welcome prompt with a rich terminal UI. You can type your task, view conversation history, and interact with Cline in real-time.

**Task Mode** :   Run **cline "prompt"** or **cline task "prompt"** to immediately start a task. If stdin is a TTY, you'll see the interactive UI. If stdin is piped or output is redirected, the CLI automatically switches to plain text mode.

**Plain Text Mode** :   Activated automatically when stdin is piped, output is redirected, or **\--json**/**\--yolo** flags are used. Outputs clean text without the Ink UI, suitable for scripting and CI/CD pipelines.

# AGENT BEHAVIOR

Cline operates in two primary modes:

**ACT MODE** :   Cline actively uses tools to accomplish tasks. He can read files, write code, execute commands, use a headless browser, and more. This is the default mode for task execution.

**PLAN MODE** :   Cline gathers information and creates a detailed plan before implementation. He explores the codebase, asks clarifying questions, and presents a strategy for user approval before switching to ACT MODE.

# COMMANDS

## task (alias: t)

Run a new task with a prompt.

**cline task** *prompt* [*options*]

**cline t** *prompt* [*options*] :   Create and run a new task. Options:

**-a**, **\--act** :   Run in act mode (default)

**-p**, **\--plan** :   Run in plan mode

**-y**, **\--yolo** :   Enable yolo/yes mode (auto-approve all actions, output in plain mode, exit process automatically when task complete)

**-m**, **\--model** *model* :   Model to use for the task

**-i**, **\--images** *paths...* :   Image file paths to include with the task

**-v**, **\--verbose** :   Show verbose output including reasoning

**-c**, **\--cwd** *path* :   Working directory for the task

**\--config** *path* :   Path to Cline configuration directory

**\--thinking** :   Enable extended thinking (1024 token budget)

**\--json** :   Output messages as JSON instead of styled text

**-T**, **\--taskId** *id* :   Resume an existing task by ID. The prompt argument becomes an optional follow-up message.

## history (alias: h)

List task history with pagination.

**cline history** [*options*]

**cline h** [*options*] :   Display previous tasks. Options:

**-n**, **\--limit** *number* :   Number of tasks to show (default: 10)

**-p**, **\--page** *number* :   Page number, 1-based (default: 1)

**\--config** *path* :   Path to Cline configuration directory

## config

Show current configuration.

**cline config** [*options*] :   Display global and workspace state. Options:

**\--config** *path* :   Path to Cline configuration directory

## auth

Authenticate a provider and configure the model.

**cline auth** [*options*] :   Launch interactive authentication wizard, or use quick setup flags. Options:

**-p**, **\--provider** *id* :   Provider ID for quick setup (e.g., openai-native, anthropic, openrouter)

**-k**, **\--apikey** *key* :   API key for the provider

**-m**, **\--modelid** *id* :   Model ID to configure (e.g., gpt-4o, claude-sonnet-4-5-20250929)

**-b**, **\--baseurl** *url* :   Base URL (optional, for OpenAI-compatible providers)

**-v**, **\--verbose** :   Show verbose output

**-c**, **\--cwd** *path* :   Working directory

**\--config** *path* :   Path to Cline configuration directory

## update

Check for updates and install if available.

**cline update** [*options*] :   Check npm for newer versions. Options:

**-v**, **\--verbose** :   Show verbose output

## version

Show the CLI version number.

**cline version**

## dev

Developer tools and utilities.

**cline dev log** :   Open the log file for debugging.

# DEFAULT COMMAND OPTIONS

When running **cline** with just a prompt (no subcommand), these options are available:

**-a**, **\--act** :   Run in act mode (default)

**-p**, **\--plan** :   Run in plan mode

**-y**, **\--yolo** :   Enable yolo mode (auto-approve all actions). Also forces plain text output mode.

**-m**, **\--model** *model* :   Model to use for the task

**-v**, **\--verbose** :   Show verbose output

**-c**, **\--cwd** *path* :   Working directory

**\--config** *path* :   Configuration directory

**\--thinking** :   Enable extended thinking (1024 token budget)

**\--json** :   Output messages as JSON instead of styled text. Forces plain text mode.

**-T**, **\--taskId** *id* :   Resume an existing task by ID instead of starting a new one. The prompt becomes an optional follow-up message.

# JSON OUTPUT FORMAT

When using **\--json**, each message is output as a JSON object with these fields:

**Required fields:**

- **type**: "ask" or "say"
- **text**: message text
- **ts**: Unix epoch timestamp in milliseconds

**Optional fields:**

- **reasoning**: reasoning text
- **say**: say subtype (when type is "say")
- **ask**: ask subtype (when type is "ask")
- **partial**: streaming flag
- **images**: list of image URIs
- **files**: list of file paths

# EXAMPLES

## Basic Usage

```bash
# Launch interactive mode
cline

# Run a task directly
cline "Create a hello world function in Python"

# Run with verbose output and extended thinking
cline -v --thinking "Analyze this codebase architecture"
```

## Mode Selection

```bash
# Run in plan mode (gather info before acting)
cline -p "Design a REST API for user management"

# Run in act mode with auto-approval (yolo)
cline -y "Fix the typo in README.md"
```

## Using Specific Models

```bash
# Use a specific model
cline -m claude-sonnet-4-5-20250929 "Refactor this function"

# Quick auth setup with model
cline auth -p anthropic -k sk-ant-xxxxx -m claude-sonnet-4-5-20250929
```

## Including Images

```bash
# Include images with explicit flag
cline task -i screenshot.png diagram.jpg "Fix the UI based on these images"

# Or use inline image references in the prompt
cline "Fix the layout shown in @./screenshot.png"
```

## Piped Input

```bash
# Pipe file contents to Cline
cat README.md | cline "Summarize this document"

# Pipe with additional prompt
echo "function add(a, b) { return a + b }" | cline "Add TypeScript types to this"

# Combine piped input with a prompt
git diff | cline "Review these changes and suggest improvements"
```

## Scripting and Automation

```bash
# JSON output for parsing
cline --json "What files are in this directory?" | jq '.text'

# Yolo mode for automated workflows (auto-approves all actions), forces plain text output
cline -y "Run the test suite and fix any failures"
```

## Task History

```bash
# List recent tasks
cline history

# Show more tasks with pagination
cline history -n 20 -p 2
```

## Resuming Tasks

```bash
# Resume a task by ID (get IDs from cline history)
cline -T abc123def

# Resume a task with a follow-up message
cline -T abc123def "Now add unit tests for the changes"

# Resume in plan mode to review before continuing
cline -T abc123def -p "What's left to do?"

# Resume with yolo mode for automated continuation
cline -T abc123def -y "Continue with the implementation"
```

## Authentication

```bash
# Interactive authentication wizard
cline auth

# Quick setup for Anthropic
cline auth -p anthropic -k sk-ant-api-xxxxx

# Quick setup for OpenAI
cline auth -p openai-native -k sk-xxxxx -m gpt-4o

# OpenAI-compatible provider with custom base URL
cline auth -p openai -k your-api-key -b https://api.example.com/v1
```

# ENVIRONMENT

**CLINE_DIR** :   Override the default configuration directory. When set, Cline stores all data in this directory instead of `~/.cline/data/`.

**CLINE_COMMAND_PERMISSIONS** :   JSON configuration for restricting which shell commands Cline can execute. When set, commands are validated against allow/deny patternks before execution. When not set, all commands are allowed.

Format: `{"allow": ["pattern1", "pattern2"], "deny": ["pattern3"], "allowRedirects": true}`

**Fields:**

- **allow** (array of strings): Glob patterns for allowed commands. If specified, only matching commands are permitted. Uses `*` to match any characters and `?` to match a single character. Setting allow on anything will deny all others.
- **deny** (array of strings): Glob patterns for denied commands. Deny rules take precedence over allow rules.
- **allowRedirects** (boolean): Whether to allow shell redirects (`>`, `>>`, `<`, etc.). Defaults to false.

**Rule evaluation:**

1. Check for dangerous characters (backticks outside single quotes, unquoted newlines)
2. Parse command into segments split by operators (`&&`, `||`, `|`, `;`)
3. If redirects detected and `allowRedirects` is not true, command is denied
4. Each segment is validated against deny rules first, then allow rules
5. Subshell contents (`$(...)` and `(...)`) are recursively validated
6. All segments must pass for the command to be allowed

**Examples:**

```bash
# Allow only npm and git commands.
export CLINE_COMMAND_PERMISSIONS='{"allow": ["npm *", "git *"]}'

# Allow development commands but deny dangerous ones. Deny not strictly required here since allow is set.
export CLINE_COMMAND_PERMISSIONS='{"allow": ["npm *", "git *", "node *"], "deny": ["rm -rf *", "sudo *"]}'

# Allow file operations with redirects
export CLINE_COMMAND_PERMISSIONS='{"allow": ["cat *", "echo *"], "allowRedirects": true}'
```


# CONFIGURATION FILES

```
~/.cline/
├── data/                    # Default configuration directory
│   ├── globalState.json     # Global settings and state
│   ├── secrets.json         # API keys and secrets (stored securely)
│   ├── workspace/           # Workspace-specific state
│   └── tasks/               # Task history and conversation data
└── log/                     # Log files for debugging
```

View logs with `cline dev log`.


# BUGS

Report bugs at: <https://github.com/cline/cline/issues>

For real-time help, join the Discord community at: <https://discord.gg/cline>

# SEE ALSO

Full documentation: <https://docs.cline.bot>

VS Code extension: <https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev>

# AUTHORS

Cline is developed by Cline Bot Inc. and the open source community.

# COPYRIGHT

Copyright © 2025 Cline Bot Inc. Licensed under the Apache License 2.0.


--- cli/src/index.ts ---
/**
 * Cline CLI - TypeScript implementation with React Ink
 */

import { exit } from "node:process"
import type { ApiProvider } from "@shared/api"
import { Command } from "commander"
import { render } from "ink"
import React from "react"
import { ClineEndpoint } from "@/config"
import { Controller } from "@/core/controller"
import { StateManager } from "@/core/storage/StateManager"
import { AuthHandler } from "@/hosts/external/AuthHandler"
import { HostProvider } from "@/hosts/host-provider"
import { FileEditProvider } from "@/integrations/editor/FileEditProvider"
import { openAiCodexOAuthManager } from "@/integrations/openai-codex/oauth"
import { StandaloneTerminalManager } from "@/integrations/terminal/standalone/StandaloneTerminalManager"
import { ErrorService } from "@/services/error/ErrorService"
import { telemetryService } from "@/services/telemetry"
import { PostHogClientProvider } from "@/services/telemetry/providers/posthog/PostHogClientProvider"
import { HistoryItem } from "@/shared/HistoryItem"
import { Logger } from "@/shared/services/Logger"
import { Session } from "@/shared/services/Session"
import { getProviderModelIdKey, ProviderToApiKeyMap } from "@/shared/storage"
import { isOpenaiReasoningEffort, OPENAI_REASONING_EFFORT_OPTIONS, type OpenaiReasoningEffort } from "@/shared/storage/types"
import { version as CLI_VERSION } from "../package.json"
import { runAcpMode } from "./acp/index.js"
import { App } from "./components/App"
import { checkRawModeSupport } from "./context/StdinContext"
import { createCliHostBridgeProvider } from "./controllers"
import { CliCommentReviewController } from "./controllers/CliCommentReviewController"
import { CliWebviewProvider } from "./controllers/CliWebviewProvider"
import { restoreConsole } from "./utils/console"
import { printInfo, printWarning } from "./utils/display"
import { selectOutputMode } from "./utils/mode-selection"
import { parseImagesFromInput, processImagePaths } from "./utils/parser"
import { CLINE_CLI_DIR, getCliBinaryPath } from "./utils/path"
import { readStdinIfPiped } from "./utils/piped"
import { runPlainTextTask } from "./utils/plain-text-task"
import { applyProviderConfig } from "./utils/provider-config"
import { getValidCliProviders, isValidCliProvider } from "./utils/providers"
import { autoUpdateOnStartup, checkForUpdates } from "./utils/update"
import { initializeCliContext } from "./vscode-context"
import { CLI_LOG_FILE, shutdownEvent, window } from "./vscode-shim"

/**
 * Common options shared between runTask and resumeTask
 */
interface TaskOptions {
	act?: boolean
	plan?: boolean
	model?: string
	verbose?: boolean
	cwd?: string
	config?: string
	thinking?: boolean | string
	reasoningEffort?: string
	maxConsecutiveMistakes?: string
	yolo?: boolean
	doubleCheckCompletion?: boolean
	timeout?: string
	json?: boolean
	stdinWasPiped?: boolean
}

let telemetryDisposed = false

async function disposeTelemetryServices(): Promise<void> {
	if (telemetryDisposed) {
		return
	}

	telemetryDisposed = true
	await Promise.allSettled([telemetryService.dispose(), PostHogClientProvider.getInstance().dispose()])
}

/**
 * Restore yoloModeToggled to its original value from before this CLI session.
 * This ensures the --yolo flag is session-only and doesn't leak into future runs.
 * Must be called before flushPendingState so the restored value gets persisted.
 */
function restoreYoloState(): void {
	if (savedYoloModeToggled !== null) {
		try {
			StateManager.get().setGlobalState("yoloModeToggled", savedYoloModeToggled)
			savedYoloModeToggled = null
		} catch {
			// StateManager may not be initialized (e.g., early exit before init)
		}
	}
}

async function disposeCliContext(ctx: CliContext): Promise<void> {
	restoreYoloState()
	await ctx.controller.stateManager.flushPendingState()
	await ctx.controller.dispose()
	await ErrorService.get().dispose()
	await disposeTelemetryServices()
}

function setModeScopedState(currentMode: "act" | "plan", setter: (mode: "act" | "plan") => void): void {
	const stateManager = StateManager.get()
	setter(currentMode)

	const separateModels = stateManager.getGlobalSettingsKey("planActSeparateModelsSetting") ?? false
	if (!separateModels) {
		const otherMode: "act" | "plan" = currentMode === "act" ? "plan" : "act"
		setter(otherMode)
	}
}

function normalizeReasoningEffort(value?: string): OpenaiReasoningEffort | undefined {
	if (value === undefined) {
		return undefined
	}

	const normalized = value.toLowerCase()
	if (isOpenaiReasoningEffort(normalized)) {
		return normalized
	}

	printWarning(
		`Invalid --reasoning-effort '${value}'. Using 'medium'. Valid values: ${OPENAI_REASONING_EFFORT_OPTIONS.join(", ")}.`,
	)
	return "medium"
}

function normalizeMaxConsecutiveMistakes(value?: string): number | undefined {
	if (value === undefined) {
		return undefined
	}

	const parsed = Number.parseInt(value, 10)
	if (Number.isNaN(parsed) || parsed < 1) {
		printWarning(`Invalid --max-consecutive-mistakes value '${value}'. Expected integer >= 1.`)
		return undefined
	}

	return parsed
}

/**
 * Apply task-related options (mode, model, thinking, yolo) to StateManager.
 * Shared between runTask and resumeTask to avoid duplication.
 */
function applyTaskOptions(options: TaskOptions): void {
	// Apply mode flag
	if (options.plan) {
		StateManager.get().setGlobalState("mode", "plan")
		telemetryService.captureHostEvent("mode_flag", "plan")
	} else if (options.act) {
		StateManager.get().setGlobalState("mode", "act")
		telemetryService.captureHostEvent("mode_flag", "act")
	}

	// Apply model override if specified
	if (options.model) {
		const selectedMode = (StateManager.get().getGlobalSettingsKey("mode") || "act") as "act" | "plan"
		const providerKey = selectedMode === "act" ? "actModeApiProvider" : "planModeApiProvider"
		const currentProvider = StateManager.get().getGlobalSettingsKey(providerKey) as ApiProvider
		const modelKey = getProviderModelIdKey(currentProvider, selectedMode)
		if (modelKey) {
			StateManager.get().setGlobalState(modelKey, options.model)
		}
		telemetryService.captureHostEvent("model_flag", options.model)
	}

	// Set thinking budget based on --thinking flag (boolean or number)
	let thinkingBudget = 0
	if (options.thinking) {
		if (typeof options.thinking === "string") {
			const parsed = Number.parseInt(options.thinking, 10)
			if (Number.isNaN(parsed) || parsed < 0) {
				printWarning(`Invalid --thinking value '${options.thinking}'. Using default 1024.`)
				thinkingBudget = 1024
			} else {
				thinkingBudget = parsed
			}
		} else {
			thinkingBudget = 1024
		}
	}
	const currentMode = (StateManager.get().getGlobalSettingsKey("mode") || "act") as "act" | "plan"
	setModeScopedState(currentMode, (mode) => {
		const thinkingKey = mode === "act" ? "actModeThinkingBudgetTokens" : "planModeThinkingBudgetTokens"
		StateManager.get().setGlobalState(thinkingKey, thinkingBudget)
	})
	if (options.thinking) {
		telemetryService.captureHostEvent("thinking_flag", "true")
	}

	const reasoningEffort = normalizeReasoningEffort(options.reasoningEffort)
	if (reasoningEffort !== undefined) {
		setModeScopedState(currentMode, (mode) => {
			const reasoningKey = mode === "act" ? "actModeReasoningEffort" : "planModeReasoningEffort"
			StateManager.get().setGlobalState(reasoningKey, reasoningEffort)
		})
		telemetryService.captureHostEvent("reasoning_effort_flag", reasoningEffort)
	}

	const maxConsecutiveMistakes = normalizeMaxConsecutiveMistakes(options.maxConsecutiveMistakes)
	if (maxConsecutiveMistakes !== undefined) {
		StateManager.get().setGlobalState("maxConsecutiveMistakes", maxConsecutiveMistakes)
		telemetryService.captureHostEvent("max_consecutive_mistakes_flag", String(maxConsecutiveMistakes))
	}

	// Override yolo mode only if --yolo flag is explicitly passed.
	// The original value is saved in initializeCli and restored on exit.
	if (options.yolo) {
		const state = StateManager.get()
		savedYoloModeToggled = state.getGlobalSettingsKey("yoloModeToggled") ?? false
		state.setGlobalState("yoloModeToggled", true)
		telemetryService.captureHostEvent("yolo_flag", "true")
	}

	// Set double-check completion based on flag
	if (options.doubleCheckCompletion) {
		StateManager.get().setGlobalState("doubleCheckCompletionEnabled", true)
		telemetryService.captureHostEvent("double_check_completion_flag", "true")
	}
}

/**
 * Get mode selection result using the extracted, testable selectOutputMode function.
 * This wrapper provides the current process TTY state.
 */
function getModeSelection(options: TaskOptions) {
	return selectOutputMode({
		stdoutIsTTY: process.stdout.isTTY === true,
		stdinIsTTY: process.stdin.isTTY === true,
		stdinWasPiped: options.stdinWasPiped ?? false,
		json: options.json,
		yolo: options.yolo,
	})
}

/**
 * Determine if plain text mode should be used based on options and environment.
 */
function shouldUsePlainTextMode(options: TaskOptions): boolean {
	return getModeSelection(options).usePlainTextMode
}

/**
 * Get the reason for using plain text mode (for telemetry).
 */
function getPlainTextModeReason(options: TaskOptions): string {
	return getModeSelection(options).reason
}

/**
 * Run a task in plain text mode (no Ink UI).
 * Handles auth check, task execution, cleanup, and exit.
 */
async function runTaskInPlainTextMode(
	ctx: CliContext,
	options: TaskOptions,
	taskConfig: {
		prompt?: string
		taskId?: string
		imageDataUrls?: string[]
	},
): Promise<never> {
	// Set flag so shutdown handler knows not to clear Ink UI lines
	isPlainTextMode = true

	// Check if auth is configured before attempting to run the task
	// In plain text mode we can't show the interactive auth flow
	const hasAuth = await isAuthConfigured()
	if (!hasAuth) {
		printWarning("Not authenticated. Please run 'cline auth' first to configure your API credentials.")
		await disposeCliContext(ctx)
		exit(1)
	}

	const reason = getPlainTextModeReason(options)
	telemetryService.captureHostEvent("plain_text_mode", reason)

	// Plain text mode: no Ink rendering, just clean text output
	const success = await runPlainTextTask({
		controller: ctx.controller,
		prompt: taskConfig.prompt,
		taskId: taskConfig.taskId,
		imageDataUrls: taskConfig.imageDataUrls,
		verbose: options.verbose,
		jsonOutput: options.json,
		timeoutSeconds: options.timeout ? Number.parseInt(options.timeout, 10) : undefined,
	})

	// Cleanup
	await disposeCliContext(ctx)

	// Ensure stdout is fully drained before exiting - critical for piping
	await drainStdout()
	exit(success ? 0 : 1)
}

/**
 * Create the standard cleanup function for Ink apps.
 */
function createInkCleanup(ctx: CliContext, onTaskError?: () => boolean): () => Promise<void> {
	return async () => {
		await disposeCliContext(ctx)
		if (onTaskError?.()) {
			printWarning("Task ended with errors.")
			exit(1)
		}
		exit(0)
	}
}

// Track active context for graceful shutdown
let activeContext: CliContext | null = null
let isShuttingDown = false
// Track if we're in plain text mode (no Ink UI) - set by runTask when piped stdin detected
let isPlainTextMode = false
// Track the original yoloModeToggled value from before this CLI session so we can restore it on exit.
// The --yolo flag should only affect the current invocation, not persist across runs.
let savedYoloModeToggled: boolean | null = null

/**
 * Wait for stdout to fully drain before exiting.
 * Critical for piping - ensures data is flushed to the next command in the pipe.
 */
async function drainStdout(): Promise<void> {
	return new Promise<void>((resolve) => {
		// Check if stdout needs draining
		if (process.stdout.writableNeedDrain) {
			process.stdout.once("drain", resolve)
		} else {
			// Give a small delay to ensure any pending writes complete
			setImmediate(resolve)
		}
	})
}

function setupSignalHandlers() {
	const shutdown = async (signal: string) => {
		if (isShuttingDown) {
			// Force exit on second signal
			process.exit(1)
		}
		isShuttingDown = true

		// Notify components to hide UI before shutdown
		shutdownEvent.fire()

		// Only clear Ink UI lines if we're not in plain text mode
		// In plain text mode, there's no Ink UI to clear and the ANSI codes
		// would corrupt the streaming output
		if (!isPlainTextMode) {
			// Clear several lines to remove the input field and footer from display
			// Move cursor up and clear lines (input box + footer rows)
			const linesToClear = 8 // Input box (3 lines with border) + footer (4-5 lines)
			process.stdout.write(`\x1b[${linesToClear}A\x1b[J`)
		}

		printWarning(`${signal} received, shutting down...`)

		try {
			// Restore yolo state before any cleanup - this is idempotent and safe
			// even if disposeCliContext also calls it (restoreYoloState checks savedYoloModeToggled !== null)
			restoreYoloState()

			if (activeContext) {
				const task = activeContext.controller.task
				if (task) {
					task.abortTask()
				}
				await disposeCliContext(activeContext)
			} else {
				// Best-effort flush of restored yolo state when no active context
				try {
					await StateManager.get().flushPendingState()
				} catch {
					// StateManager may not be initialized yet
				}
				await ErrorService.get().dispose()
				await disposeTelemetryServices()
			}
		} catch {
			// Best effort cleanup
		}

		process.exit(0)
	}

	process.on("SIGINT", () => shutdown("SIGINT"))
	process.on("SIGTERM", () => shutdown("SIGTERM"))

	// Suppress known abort errors from unhandled rejections
	// These occur when task is cancelled and async operations throw "Cline instance aborted"
	process.on("unhandledRejection", (reason: unknown) => {
		const message = reason instanceof Error ? reason.message : String(reason)
		// Silently ignore abort-related errors - they're expected during task cancellation
		if (message.includes("aborted") || message.includes("abort")) {
			Logger.info("Suppressed unhandled rejection due to abort:", message)
			return
		}
		// For other unhandled rejections, log to file via Logger (if available)
		// This won't show in terminal but will be in log files for debugging
		Logger.error("Unhandled rejection:", reason)
	})
}

setupSignalHandlers()

interface CliContext {
	extensionContext: any
	dataDir: string
	extensionDir: string
	workspacePath: string
	controller: Controller
}

interface InitOptions {
	config?: string
	cwd?: string
	verbose?: boolean
	enableAuth?: boolean
}

/**
 * Initialize all CLI infrastructure and return context needed for commands
 */
async function initializeCli(options: InitOptions): Promise<CliContext> {
	const workspacePath = options.cwd || process.cwd()
	const { extensionContext, DATA_DIR, EXTENSION_DIR } = initializeCliContext({
		clineDir: options.config,
		workspaceDir: workspacePath,
	})

	// Set up output channel and Logger early so ClineEndpoint.initialize logs are captured
	const outputChannel = window.createOutputChannel("Cline CLI")
	const logToChannel = (message: string) => outputChannel.appendLine(message)

	// Configure the shared Logging class early to capture all initialization logs
	Logger.subscribe(logToChannel)

	await ClineEndpoint.initialize(EXTENSION_DIR)

	// Auto-update check (after endpoints initialized, so we can detect bundled configs)
	autoUpdateOnStartup(CLI_VERSION)

	// Initialize/reset session tracking for this CLI run
	Session.reset()

	if (options.enableAuth) {
		AuthHandler.getInstance().setEnabled(true)
	}

	outputChannel.appendLine(
		`Cline CLI initialized. Data dir: ${DATA_DIR}, Extension dir: ${EXTENSION_DIR}, Log dir: ${CLINE_CLI_DIR.log}`,
	)

	HostProvider.initialize(
		() => new CliWebviewProvider(extensionContext as any),
		() => new FileEditProvider(),
		() => new CliCommentReviewController(),
		() => new StandaloneTerminalManager(),
		createCliHostBridgeProvider(workspacePath),
		logToChannel,
		async (path: string) => (options.enableAuth ? AuthHandler.getInstance().getCallbackUrl(path) : ""),
		getCliBinaryPath,
		EXTENSION_DIR,
		DATA_DIR,
	)

	await StateManager.initialize(extensionContext as any)

	await ErrorService.initialize()

	// Initialize OpenAI Codex OAuth manager with extension context for secrets storage
	openAiCodexOAuthManager.initialize(extensionContext)

	const webview = HostProvider.get().createWebviewProvider() as CliWebviewProvider
	const controller = webview.controller

	await telemetryService.captureExtensionActivated()
	await telemetryService.captureHostEvent("cline_cli", "initialized")

	const ctx = { extensionContext, dataDir: DATA_DIR, extensionDir: EXTENSION_DIR, workspacePath, controller }
	activeContext = ctx
	return ctx
}

/**
 * Run an Ink app with proper cleanup handling
 */
async function runInkApp(element: React.ReactElement, cleanup: () => Promise<void>): Promise<void> {
	// Clear terminal for clean UI - robot will render at row 1
	process.stdout.write("\x1b[2J\x1b[3J\x1b[H")

	// Note: incrementalRendering is disabled because it causes UI glitches on terminal resize.
	// Ink's incremental rendering tries to erase N lines based on previous output height,
	// but when the terminal shrinks, this leaves artifacts. Gemini CLI only enables
	// incrementalRendering when alternateBuffer is also enabled (which we don't use).
	const { waitUntilExit, unmount } = render(element, { exitOnCtrlC: true })

	try {
		await waitUntilExit()
	} finally {
		try {
			unmount()
		} catch {
			// Already unmounted
		}
		restoreConsole()
		await cleanup()
	}
}

/**
 * Run a task with the given prompt - uses welcome view for consistent behavior
 */
async function runTask(prompt: string, options: TaskOptions & { images?: string[] }, existingContext?: CliContext) {
	const ctx = existingContext || (await initializeCli({ ...options, enableAuth: true }))

	// Parse images from the prompt text (e.g., @/path/to/image.png)
	const { prompt: cleanPrompt, imagePaths: parsedImagePaths } = parseImagesFromInput(prompt)

	// Combine parsed image paths with explicit --images option
	const allImagePaths = [...(options.images || []), ...parsedImagePaths]
	// Convert image file paths to base64 data URLs
	const imageDataUrls = await processImagePaths(allImagePaths)

	// Use clean prompt (with image refs removed)
	const taskPrompt = cleanPrompt || prompt

	// Task without prompt starts in interactive mode
	telemetryService.captureHostEvent("task_command", prompt ? "task" : "interactive")

	// Apply shared task options (mode, model, thinking, yolo)
	applyTaskOptions(options)
	await StateManager.get().flushPendingState()

	// Use plain text mode when output is redirected, stdin was piped, JSON mode is enabled, or --yolo flag is used
	if (shouldUsePlainTextMode(options)) {
		return runTaskInPlainTextMode(ctx, options, {
			prompt: taskPrompt,
			imageDataUrls: imageDataUrls.length > 0 ? imageDataUrls : undefined,
		})
	}

	// Interactive mode: Render the welcome view with optional initial prompt/images
	// If prompt provided (cline task "prompt"), ChatView will auto-submit
	// If no prompt (cline interactive), user will type it in
	let taskError = false

	await runInkApp(
		React.createElement(App, {
			view: "welcome",
			verbose: options.verbose,
			controller: ctx.controller,
			isRawModeSupported: checkRawModeSupport(),
			initialPrompt: taskPrompt || undefined,
			initialImages: imageDataUrls.length > 0 ? imageDataUrls : undefined,
			onError: () => {
				taskError = true
			},
			onWelcomeExit: () => {
				// User pressed Esc; Ink exits and cleanup handles process exit.
			},
		}),
		createInkCleanup(ctx, () => taskError),
	)
}

/**
 * List task history
 */
async function listHistory(options: { config?: string; limit?: number; page?: number }) {
	const ctx = await initializeCli(options)

	const taskHistory = StateManager.get().getGlobalStateKey("taskHistory") || []
	// Sort by timestamp (newest first) before pagination
	const sortedHistory = [...taskHistory].sort((a: any, b: any) => (b.ts || 0) - (a.ts || 0))
	const limit = typeof options.limit === "string" ? Number.parseInt(options.limit, 10) : options.limit || 10
	const initialPage = typeof options.page === "string" ? Number.parseInt(options.page, 10) : options.page || 1
	const totalCount = sortedHistory.length
	const totalPages = Math.ceil(totalCount / limit)

	telemetryService.captureHostEvent("history_command", "executed")

	if (sortedHistory.length === 0) {
		printInfo("No task history found.")
		await disposeCliContext(ctx)
		exit(0)
	}

	await runInkApp(
		React.createElement(App, {
			view: "history",
			historyItems: [],
			historyAllItems: sortedHistory,
			controller: ctx.controller,
			historyPagination: { page: initialPage, totalPages, totalCount, limit },
			isRawModeSupported: checkRawModeSupport(),
		}),
		async () => {
			await disposeCliContext(ctx)
			exit(0)
		},
	)
}

/**
 * Show current configuration
 */
async function showConfig(options: { config?: string }) {
	const ctx = await initializeCli(options)
	const stateManager = StateManager.get()

	// Dynamically import the wrapper to avoid circular dependencies
	const { ConfigViewWrapper } = await import("./components/ConfigViewWrapper")

	telemetryService.captureHostEvent("config_command", "executed")

	await runInkApp(
		React.createElement(ConfigViewWrapper, {
			controller: ctx.controller,
			dataDir: ctx.dataDir,
			globalState: stateManager.getAllGlobalStateEntries(),
			workspaceState: stateManager.getAllWorkspaceStateEntries(),
			hooksEnabled: true,
			skillsEnabled: true,
			isRawModeSupported: checkRawModeSupport(),
		}),
		async () => {
			await disposeCliContext(ctx)
			exit(0)
		},
	)
}

/**
 * Run authentication flow
 */
/**
 * Perform quick auth setup without UI - validates and saves configuration directly
 */
async function performQuickAuthSetup(
	ctx: CliContext,
	options: { provider: string; apikey: string; modelid: string; baseurl?: string },
): Promise<{ success: boolean; error?: string }> {
	const { provider, apikey, modelid, baseurl } = options

	const normalizedProvider = provider.toLowerCase().trim()

	if (!isValidCliProvider(normalizedProvider)) {
		const validProviders = getValidCliProviders()
		return { success: false, error: `Invalid provider '${provider}'. Supported providers: ${validProviders.join(", ")}` }
	}

	if (normalizedProvider === "bedrock") {
		return {
			success: false,
			error: "Bedrock provider is not supported for quick setup due to complex authentication requirements. Please use interactive setup.",
		}
	}

	if (baseurl && !["openai", "openai-native"].includes(normalizedProvider)) {
		return { success: false, error: "Base URL is only supported for OpenAI and OpenAI-compatible providers" }
	}

	// Save configuration using shared utility
	await applyProviderConfig({
		providerId: normalizedProvider,
		apiKey: apikey,
		modelId: modelid,
		baseUrl: baseurl,
		controller: ctx.controller,
	})

	// Mark onboarding as complete
	StateManager.get().setGlobalState("welcomeViewCompleted", true)
	await StateManager.get().flushPendingState()

	return { success: true }
}

async function runAuth(options: {
	provider?: string
	apikey?: string
	modelid?: string
	baseurl?: string
	verbose?: boolean
	cwd?: string
	config?: string
}) {
	const ctx = await initializeCli({ ...options, enableAuth: true })

	const hasQuickSetupFlags = options.provider && options.apikey && options.modelid

	telemetryService.captureHostEvent("auth_command", hasQuickSetupFlags ? "quick_setup" : "interactive")

	// Quick setup mode - no UI, just save configuration and exit
	if (hasQuickSetupFlags) {
		const result = await performQuickAuthSetup(ctx, {
			provider: options.provider!,
			apikey: options.apikey!,
			modelid: options.modelid!,
			baseurl: options.baseurl,
		})

		if (!result.success) {
			printWarning(result.error || "Quick setup failed")
			await telemetryService.captureHostEvent("auth", "error")
			await disposeCliContext(ctx)
			exit(1)
		}

		await telemetryService.captureHostEvent("auth", "completed")
		await disposeCliContext(ctx)
		exit(0)
	}

	// Interactive mode - show Ink UI
	let authError = false

	await runInkApp(
		React.createElement(App, {
			view: "auth",
			controller: ctx.controller,
			isRawModeSupported: checkRawModeSupport(),
			onComplete: () => {
				telemetryService.captureHostEvent("auth", "completed")
			},
			onError: () => {
				telemetryService.captureHostEvent("auth", "error")
				authError = true
			},
		}),
		async () => {
			await disposeCliContext(ctx)
			exit(authError ? 1 : 0)
		},
	)
}

// Setup CLI commands
const program = new Command()

program.name("cline").description("Cline CLI - AI coding assistant in your terminal").version(CLI_VERSION)

// Enable positional options to avoid conflicts between root and subcommand options with the same name
program.enablePositionalOptions()

program
	.command("task")
	.alias("t")
	.description("Run a new task")
	.argument("<prompt>", "The task prompt")
	.option("-a, --act", "Run in act mode")
	.option("-p, --plan", "Run in plan mode")
	.option("-y, --yolo", "Enable yes/yolo mode (auto-approve actions)")
	.option("-t, --timeout <seconds>", "Timeout in seconds for yes/yolo mode (default: 600)")
	.option("-m, --model <model>", "Model to use for the task")
	.option("-v, --verbose", "Show verbose output")
	.option("-c, --cwd <path>", "Working directory for the task")
	.option("--config <path>", "Path to Cline configuration directory")
	.option("--thinking [tokens]", "Enable extended thinking (default: 1024 tokens)")
	.option("--reasoning-effort <effort>", "Reasoning effort: none|low|medium|high|xhigh")
	.option("--max-consecutive-mistakes <count>", "Maximum consecutive mistakes before halting in yolo mode")
	.option("--json", "Output messages as JSON instead of styled text")
	.option("--double-check-completion", "Reject first completion attempt to force re-verification")
	.option("-T, --taskId <id>", "Resume an existing task by ID")
	.action((prompt, options) => {
		if (options.taskId) {
			return resumeTask(options.taskId, { ...options, initialPrompt: prompt })
		}
		return runTask(prompt, options)
	})

program
	.command("history")
	.alias("h")
	.description("List task history")
	.option("-n, --limit <number>", "Number of tasks to show", "10")
	.option("-p, --page <number>", "Page number (1-based)", "1")
	.option("--config <path>", "Path to Cline configuration directory")
	.action(listHistory)

program
	.command("config")
	.description("Show current configuration")
	.option("--config <path>", "Path to Cline configuration directory")
	.action(showConfig)

program
	.command("auth")
	.description("Authenticate a provider and configure what model is used")
	.option("-p, --provider <id>", "Provider ID for quick setup (e.g., openai-native, anthropic, moonshot)")
	.option("-k, --apikey <key>", "API key for the provider")
	.option("-m, --modelid <id>", "Model ID to configure (e.g., gpt-4o, claude-sonnet-4-5-20250929, kimi-k2.5)")
	.option("-b, --baseurl <url>", "Base URL (optional, only for openai provider)")
	.option("-v, --verbose", "Show verbose output")
	.option("-c, --cwd <path>", "Working directory for the task")
	.option("--config <path>", "Path to Cline configuration directory")
	.action(runAuth)

program
	.command("version")
	.description("Show Cline CLI version number")
	.action(() => printInfo(`Cline CLI version: ${CLI_VERSION}`))

program
	.command("update")
	.description("Check for updates and install if available")
	.option("-v, --verbose", "Show verbose output")
	.action(() => checkForUpdates(CLI_VERSION))

// Dev command with subcommands
const devCommand = program.command("dev").description("Developer tools and utilities")

devCommand
	.command("log")
	.description("Open the log file")
	.action(async () => {
		const { openExternal } = await import("@/utils/env")
		await openExternal(CLI_LOG_FILE)
	})

/**
 * Check if the user has completed onboarding (has any provider configured).
 *
 * Uses `welcomeViewCompleted` as the single source of truth, matching the VS Code extension's approach.
 * If `welcomeViewCompleted` is undefined (first run), checks if ANY provider has credentials
 * and sets the flag accordingly.
 */
async function isAuthConfigured(): Promise<boolean> {
	const stateManager = StateManager.get()

	// Check welcomeViewCompleted first - this is the single source of truth
	const welcomeViewCompleted = stateManager.getGlobalStateKey("welcomeViewCompleted")
	if (welcomeViewCompleted !== undefined) {
		return welcomeViewCompleted
	}

	// welcomeViewCompleted is undefined - run migration logic to check if ANY provider has credentials
	// This mirrors the extension's migrateWelcomeViewCompleted behavior
	const hasAnyAuth = await checkAnyProviderConfigured()

	// Set welcomeViewCompleted based on what we found
	stateManager.setGlobalState("welcomeViewCompleted", hasAnyAuth)
	await stateManager.flushPendingState()

	return hasAnyAuth
}

/**
 * Check if ANY provider has valid credentials configured.
 * Used for migration when welcomeViewCompleted is undefined.
 */
async function checkAnyProviderConfigured(): Promise<boolean> {
	const stateManager = StateManager.get()
	const config = stateManager.getApiConfiguration() as Record<string, unknown>

	// Check Cline account (stored as "cline:clineAccountId" in secrets, loaded into config)
	if (config["clineApiKey"] || config["cline:clineAccountId"]) return true

	// Check OpenAI Codex OAuth (stored in SECRETS_KEYS, loaded into config)
	if (config["openai-codex-oauth-credentials"]) return true

	// Check all BYO provider API keys (loaded into config from secrets)
	for (const [provider, keyField] of Object.entries(ProviderToApiKeyMap)) {
		// Skip cline - already checked above with the correct key
		if (provider === "cline") continue

		const fields = Array.isArray(keyField) ? keyField : [keyField]
		for (const field of fields) {
			if (config[field]) return true
		}
	}

	// Check provider-specific settings that indicate configuration
	// (for providers that don't require API keys like Bedrock with IAM, Ollama, LM Studio)
	if (config.awsRegion) return true
	if (config.vertexProjectId) return true
	if (config.ollamaBaseUrl) return true
	if (config.lmStudioBaseUrl) return true

	return false
}

/**
 * Validate that a task exists in history
 * @returns The task history item if found, null otherwise
 */
function findTaskInHistory(taskId: string): HistoryItem | null {
	const taskHistory = StateManager.get().getGlobalStateKey("taskHistory") || []
	return taskHistory.find((item) => item.id === taskId) || null
}

/**
 * Resume an existing task by ID
 * Loads the task and optionally prefills the input with a prompt
 */
async function resumeTask(taskId: string, options: TaskOptions & { initialPrompt?: string }) {
	const ctx = await initializeCli({ ...options, enableAuth: true })

	// Validate task exists
	const historyItem = findTaskInHistory(taskId)
	if (!historyItem) {
		printWarning(`Task not found: ${taskId}`)
		printInfo("Use 'cline history' to see available tasks.")
		await disposeCliContext(ctx)
		exit(1)
	}

	telemetryService.captureHostEvent("resume_task_command", options.initialPrompt ? "with_prompt" : "interactive")

	// Apply shared task options (mode, model, thinking, yolo)
	applyTaskOptions(options)
	await StateManager.get().flushPendingState()

	// Use plain text mode for non-interactive scenarios
	if (shouldUsePlainTextMode(options)) {
		return runTaskInPlainTextMode(ctx, options, {
			prompt: options.initialPrompt,
			taskId: taskId,
		})
	}

	// Interactive mode: render the task view with the existing task
	let taskError = false

	await runInkApp(
		React.createElement(App, {
			view: "task",
			taskId: taskId,
			verbose: options.verbose,
			controller: ctx.controller,
			isRawModeSupported: checkRawModeSupport(),
			initialPrompt: options.initialPrompt || undefined,
			onError: () => {
				taskError = true
			},
			onWelcomeExit: () => {
				// User pressed Esc; Ink exits and cleanup handles process exit.
			},
		}),
		createInkCleanup(ctx, () => taskError),
	)
}

/**
 * Show welcome prompt and wait for user input
 * If auth is not configured, show auth flow first
 */
async function showWelcome(options: { verbose?: boolean; cwd?: string; config?: string; thinking?: boolean }) {
	const ctx = await initializeCli({ ...options, enableAuth: true })

	// Check if auth is configured
	const hasAuth = await isAuthConfigured()

	let hadError = false

	await runInkApp(
		React.createElement(App, {
			// Start with auth view if not configured, otherwise welcome
			view: hasAuth ? "welcome" : "auth",
			verbose: options.verbose,
			controller: ctx.controller,
			isRawModeSupported: checkRawModeSupport(),
			onWelcomeExit: () => {
				// User pressed Esc; Ink exits and cleanup handles process exit.
			},
			onError: () => {
				hadError = true
			},
		}),
		async () => {
			await disposeCliContext(ctx)
			exit(hadError ? 1 : 0)
		},
	)
}

// Interactive mode (default when no command given)
program
	.argument("[prompt]", "Task prompt (starts task immediately)")
	.option("-a, --act", "Run in act mode")
	.option("-p, --plan", "Run in plan mode")
	.option("-y, --yolo", "Enable yolo mode (auto-approve actions)")
	.option("-t, --timeout <seconds>", "Timeout in seconds for yolo mode (default: 600)")
	.option("-m, --model <model>", "Model to use for the task")
	.option("-v, --verbose", "Show verbose output")
	.option("-c, --cwd <path>", "Working directory")
	.option("--config <path>", "Configuration directory")
	.option("--thinking [tokens]", "Enable extended thinking (default: 1024 tokens)")
	.option("--reasoning-effort <effort>", "Reasoning effort: none|low|medium|high|xhigh")
	.option("--max-consecutive-mistakes <count>", "Maximum consecutive mistakes before halting in yolo mode")
	.option("--json", "Output messages as JSON instead of styled text")
	.option("--double-check-completion", "Reject first completion attempt to force re-verification")
	.option("--acp", "Run in ACP (Agent Client Protocol) mode for editor integration")
	.option("-T, --taskId <id>", "Resume an existing task by ID")
	.action(async (prompt, options) => {
		// Check for ACP mode first - this takes precedence over everything else
		if (options.acp) {
			await runAcpMode({
				config: options.config,
				cwd: options.cwd,
				verbose: options.verbose,
			})
			return
		}

		// Always check for piped stdin content
		const stdinInput = await readStdinIfPiped()

		// Track whether stdin was actually piped (even if empty) vs not piped (null)
		// stdinInput === null means stdin wasn't piped (TTY or not FIFO/file)
		// stdinInput === "" means stdin was piped but empty
		// stdinInput has content means stdin was piped with data
		const stdinWasPiped = stdinInput !== null

		// Error if stdin was piped but empty AND no prompt was provided
		// This handles:
		// - `echo "" | cline` -> error (empty stdin, no prompt)
		// - `cline "prompt"` in GitHub Actions -> OK (empty stdin ignored, has prompt)
		// - `cat file | cline "explain"` -> OK (has stdin AND prompt)
		if (stdinInput === "" && !prompt) {
			printWarning("Empty input received from stdin. Please provide content to process.")
			exit(1)
		}

		// If no prompt argument, check if input is piped via stdin
		let effectivePrompt = prompt
		if (stdinInput) {
			if (effectivePrompt) {
				// Prepend stdin content to the prompt
				effectivePrompt = `${stdinInput}\n\n${effectivePrompt}`
			} else {
				effectivePrompt = stdinInput
			}

			telemetryService.captureHostEvent("piped", "detached")

			// Debug: show that we received piped input
			if (options.verbose) {
				process.stderr.write(`[debug] Received ${stdinInput.length} bytes from stdin\n`)
			}
		}

		// Handle --taskId flag to resume an existing task
		if (options.taskId) {
			await resumeTask(options.taskId, {
				...options,
				initialPrompt: effectivePrompt,
				stdinWasPiped,
			})
			return
		}

		if (effectivePrompt) {
			// Pass stdinWasPiped flag so runTask knows to use plain text mode
			await runTask(effectivePrompt, { ...options, stdinWasPiped })
		} else {
			// Show welcome prompt if no prompt given
			await showWelcome(options)
		}
	})

// Parse and run
program.parse()


--- cli/src/index.test.ts ---
import { Command } from "commander"
import { beforeEach, describe, expect, it } from "vitest"

/**
 * Tests for CLI command parsing and structure
 * These tests verify the commander.js command definitions without
 * actually running the commands (which would require full infrastructure)
 */

describe("CLI Commands", () => {
	let program: Command

	beforeEach(() => {
		// Create a fresh program instance for each test
		program = new Command()
		program.name("cline").description("Cline CLI - AI coding assistant").version("0.0.0")
		program.enablePositionalOptions()

		// Define commands matching index.ts
		program
			.command("task")
			.alias("t")
			.description("Run a new task")
			.argument("<prompt>", "The task prompt")
			.option("-a, --act", "Run in act mode")
			.option("-p, --plan", "Run in plan mode")
			.option("-y, --yolo", "Enable yolo mode")
			.option("-m, --model <model>", "Model to use")
			.option("-i, --images <paths...>", "Image file paths")
			.option("-v, --verbose", "Show verbose output")
			.option("-c, --cwd <path>", "Working directory")
			.option("--config <path>", "Configuration directory")
			.option("--thinking [tokens]", "Enable extended thinking")
			.option("--reasoning-effort <effort>", "Reasoning effort")
			.option("--max-consecutive-mistakes <count>", "Maximum consecutive mistakes")
			.action(() => {})

		program
			.command("history")
			.alias("h")
			.description("List task history")
			.option("-n, --limit <number>", "Number of tasks to show", "10")
			.option("-p, --page <number>", "Page number", "1")
			.option("--config <path>", "Configuration directory")
			.action(() => {})

		program
			.command("config")
			.description("Show current configuration")
			.option("--config <path>", "Configuration directory")
			.action(() => {})

		program
			.command("auth")
			.description("Authenticate a provider")
			.option("-p, --provider <id>", "Provider ID")
			.option("-k, --apikey <key>", "API key")
			.option("-m, --modelid <id>", "Model ID")
			.option("-b, --baseurl <url>", "Base URL")
			.option("-v, --verbose", "Verbose output")
			.option("-c, --cwd <path>", "Working directory")
			.option("--config <path>", "Configuration directory")
			.action(() => {})

		// Default command for interactive mode
		program
			.argument("[prompt]", "Task prompt")
			.option("-i, --images <paths...>", "Image file paths")
			.option("-v, --verbose", "Verbose output")
			.option("-c, --cwd <path>", "Working directory")
			.option("--config <path>", "Configuration directory")
			.option("--thinking [tokens]", "Enable extended thinking")
			.option("--reasoning-effort <effort>", "Reasoning effort")
			.option("--max-consecutive-mistakes <count>", "Maximum consecutive mistakes")
			.action(() => {})
	})

	describe("task command", () => {
		it("should parse task command with prompt", () => {
			const args = ["node", "cli", "task", "write hello world"]
			program.parse(args)
			// Command should be parsed without error
		})

		it("should parse task alias", () => {
			const args = ["node", "cli", "t", "write hello world"]
			program.parse(args)
		})

		it("should parse --act flag", () => {
			const taskCmd = program.commands.find((c) => c.name() === "task")!
			const args = ["test prompt", "--act"]
			taskCmd.parse(args, { from: "user" })
			expect(taskCmd.opts().act).toBe(true)
		})

		it("should parse --plan flag", () => {
			const taskCmd = program.commands.find((c) => c.name() === "task")!
			const args = ["test prompt", "--plan"]
			taskCmd.parse(args, { from: "user" })
			expect(taskCmd.opts().plan).toBe(true)
		})

		it("should parse --yolo flag", () => {
			const taskCmd = program.commands.find((c) => c.name() === "task")!
			const args = ["test prompt", "--yolo"]
			taskCmd.parse(args, { from: "user" })
			expect(taskCmd.opts().yolo).toBe(true)
		})

		it("should parse --model option", () => {
			const taskCmd = program.commands.find((c) => c.name() === "task")!
			const args = ["test prompt", "--model", "claude-sonnet-4-20250514"]
			taskCmd.parse(args, { from: "user" })
			expect(taskCmd.opts().model).toBe("claude-sonnet-4-20250514")
		})

		it("should parse --images option with multiple paths", () => {
			const taskCmd = program.commands.find((c) => c.name() === "task")!
			const args = ["test prompt", "--images", "/path/to/img1.png", "/path/to/img2.jpg"]
			taskCmd.parse(args, { from: "user" })
			expect(taskCmd.opts().images).toEqual(["/path/to/img1.png", "/path/to/img2.jpg"])
		})

		it("should parse --verbose flag", () => {
			const taskCmd = program.commands.find((c) => c.name() === "task")!
			const args = ["test prompt", "--verbose"]
			taskCmd.parse(args, { from: "user" })
			expect(taskCmd.opts().verbose).toBe(true)
		})

		it("should parse --cwd option", () => {
			const taskCmd = program.commands.find((c) => c.name() === "task")!
			const args = ["test prompt", "--cwd", "/some/path"]
			taskCmd.parse(args, { from: "user" })
			expect(taskCmd.opts().cwd).toBe("/some/path")
		})

		it("should parse --config option", () => {
			const taskCmd = program.commands.find((c) => c.name() === "task")!
			const args = ["test prompt", "--config", "/custom/config"]
			taskCmd.parse(args, { from: "user" })
			expect(taskCmd.opts().config).toBe("/custom/config")
		})

		it("should parse --thinking flag", () => {
			const taskCmd = program.commands.find((c) => c.name() === "task")!
			const args = ["test prompt", "--thinking"]
			taskCmd.parse(args, { from: "user" })
			expect(taskCmd.opts().thinking).toBe(true)
		})

		it("should parse --thinking with token budget", () => {
			const taskCmd = program.commands.find((c) => c.name() === "task")!
			const args = ["test prompt", "--thinking", "8000"]
			taskCmd.parse(args, { from: "user" })
			expect(taskCmd.opts().thinking).toBe("8000")
		})

		it("should parse --reasoning-effort option", () => {
			const taskCmd = program.commands.find((c) => c.name() === "task")!
			const args = ["test prompt", "--reasoning-effort", "high"]
			taskCmd.parse(args, { from: "user" })
			expect(taskCmd.opts().reasoningEffort).toBe("high")
		})

		it("should parse --max-consecutive-mistakes option", () => {
			const taskCmd = program.commands.find((c) => c.name() === "task")!
			const args = ["test prompt", "--max-consecutive-mistakes", "999"]
			taskCmd.parse(args, { from: "user" })
			expect(taskCmd.opts().maxConsecutiveMistakes).toBe("999")
		})

		it("should parse short flags", () => {
			const taskCmd = program.commands.find((c) => c.name() === "task")!
			const args = ["test prompt", "-a", "-v", "-m", "gpt-4"]
			taskCmd.parse(args, { from: "user" })
			expect(taskCmd.opts().act).toBe(true)
			expect(taskCmd.opts().verbose).toBe(true)
			expect(taskCmd.opts().model).toBe("gpt-4")
		})
	})

	describe("history command", () => {
		it("should have default limit of 10", () => {
			const historyCmd = program.commands.find((c) => c.name() === "history")!
			historyCmd.parse([], { from: "user" })
			expect(historyCmd.opts().limit).toBe("10")
		})

		it("should have default page of 1", () => {
			const historyCmd = program.commands.find((c) => c.name() === "history")!
			historyCmd.parse([], { from: "user" })
			expect(historyCmd.opts().page).toBe("1")
		})

		it("should parse --limit option", () => {
			const historyCmd = program.commands.find((c) => c.name() === "history")!
			const args = ["--limit", "20"]
			historyCmd.parse(args, { from: "user" })
			expect(historyCmd.opts().limit).toBe("20")
		})

		it("should parse --page option", () => {
			const historyCmd = program.commands.find((c) => c.name() === "history")!
			const args = ["--page", "3"]
			historyCmd.parse(args, { from: "user" })
			expect(historyCmd.opts().page).toBe("3")
		})

		it("should parse history alias", () => {
			const args = ["node", "cli", "h"]
			program.parse(args)
			// Alias should work
		})

		it("should parse short flags", () => {
			const historyCmd = program.commands.find((c) => c.name() === "history")!
			const args = ["-n", "5", "-p", "2"]
			historyCmd.parse(args, { from: "user" })
			expect(historyCmd.opts().limit).toBe("5")
			expect(historyCmd.opts().page).toBe("2")
		})
	})

	describe("config command", () => {
		it("should parse config command", () => {
			const args = ["node", "cli", "config"]
			program.parse(args)
		})

		it("should parse --config option", () => {
			const configCmd = program.commands.find((c) => c.name() === "config")!
			const args = ["--config", "/custom/path"]
			configCmd.parse(args, { from: "user" })
			expect(configCmd.opts().config).toBe("/custom/path")
		})
	})

	describe("auth command", () => {
		it("should parse auth command", () => {
			const args = ["node", "cli", "auth"]
			program.parse(args)
		})

		it("should parse --provider option", () => {
			const authCmd = program.commands.find((c) => c.name() === "auth")!
			const args = ["--provider", "openai"]
			authCmd.parse(args, { from: "user" })
			expect(authCmd.opts().provider).toBe("openai")
		})

		it("should parse --apikey option", () => {
			const authCmd = program.commands.find((c) => c.name() === "auth")!
			const args = ["--apikey", "sk-test-key"]
			authCmd.parse(args, { from: "user" })
			expect(authCmd.opts().apikey).toBe("sk-test-key")
		})

		it("should parse --modelid option", () => {
			const authCmd = program.commands.find((c) => c.name() === "auth")!
			const args = ["--modelid", "gpt-4"]
			authCmd.parse(args, { from: "user" })
			expect(authCmd.opts().modelid).toBe("gpt-4")
		})

		it("should parse --baseurl option", () => {
			const authCmd = program.commands.find((c) => c.name() === "auth")!
			const args = ["--baseurl", "https://api.example.com"]
			authCmd.parse(args, { from: "user" })
			expect(authCmd.opts().baseurl).toBe("https://api.example.com")
		})

		it("should parse short flags", () => {
			const authCmd = program.commands.find((c) => c.name() === "auth")!
			const args = ["-p", "anthropic", "-k", "key123", "-m", "claude-sonnet-4-20250514"]
			authCmd.parse(args, { from: "user" })
			expect(authCmd.opts().provider).toBe("anthropic")
			expect(authCmd.opts().apikey).toBe("key123")
			expect(authCmd.opts().modelid).toBe("claude-sonnet-4-20250514")
		})
	})

	describe("default command (interactive mode)", () => {
		it("should parse optional prompt argument", () => {
			const args = ["node", "cli", "do something"]
			program.parse(args)
		})

		it("should parse without prompt (interactive mode)", () => {
			const args = ["node", "cli"]
			program.parse(args)
		})

		it("should parse --images option", () => {
			program.parse(["node", "cli", "--images", "img.png"])
			expect(program.opts().images).toEqual(["img.png"])
		})

		it("should parse --verbose flag", () => {
			program.parse(["node", "cli", "--verbose"])
			expect(program.opts().verbose).toBe(true)
		})

		it("should parse --thinking flag", () => {
			program.parse(["node", "cli", "--thinking"])
			expect(program.opts().thinking).toBe(true)
		})

		it("should parse --thinking with token budget", () => {
			program.parse(["node", "cli", "--thinking", "4096"])
			expect(program.opts().thinking).toBe("4096")
		})

		it("should parse --reasoning-effort option", () => {
			program.parse(["node", "cli", "--reasoning-effort", "medium"])
			expect(program.opts().reasoningEffort).toBe("medium")
		})

		it("should parse --max-consecutive-mistakes option", () => {
			program.parse(["node", "cli", "--max-consecutive-mistakes", "7"])
			expect(program.opts().maxConsecutiveMistakes).toBe("7")
		})
	})

	describe("command structure", () => {
		it("should have all expected commands", () => {
			const commandNames = program.commands.map((c) => c.name())
			expect(commandNames).toContain("task")
			expect(commandNames).toContain("history")
			expect(commandNames).toContain("config")
			expect(commandNames).toContain("auth")
		})

		it("should have correct aliases", () => {
			const taskCmd = program.commands.find((c) => c.name() === "task")!
			const historyCmd = program.commands.find((c) => c.name() === "history")!
			expect(taskCmd.aliases()).toContain("t")
			expect(historyCmd.aliases()).toContain("h")
		})

		it("should have descriptions for all commands", () => {
			for (const cmd of program.commands) {
				expect(cmd.description()).toBeTruthy()
			}
		})
	})
})

describe("getProviderModelIdKey", () => {
	// Test the provider model ID key mapping logic
	const providerKeyMap: Record<string, string> = {
		openrouter: "OpenRouterModelId",
		cline: "OpenRouterModelId",
		openai: "OpenAiModelId",
		ollama: "OllamaModelId",
		lmstudio: "LmStudioModelId",
		litellm: "LiteLlmModelId",
		requesty: "RequestyModelId",
		together: "TogetherModelId",
		fireworks: "FireworksModelId",
		sapaicore: "SapAiCoreModelId",
		groq: "GroqModelId",
		baseten: "BasetenModelId",
		huggingface: "HuggingFaceModelId",
	}

	function getProviderModelIdKey(provider: string, mode: "act" | "plan"): string | null {
		const prefix = mode === "act" ? "actMode" : "planMode"
		const keySuffix = providerKeyMap[provider]
		if (keySuffix) {
			return `${prefix}${keySuffix}`
		}
		return null
	}

	it("should return correct key for openrouter in act mode", () => {
		expect(getProviderModelIdKey("openrouter", "act")).toBe("actModeOpenRouterModelId")
	})

	it("should return correct key for openrouter in plan mode", () => {
		expect(getProviderModelIdKey("openrouter", "plan")).toBe("planModeOpenRouterModelId")
	})

	it("should return same key for cline as openrouter", () => {
		expect(getProviderModelIdKey("cline", "act")).toBe("actModeOpenRouterModelId")
	})

	it("should return correct key for openai", () => {
		expect(getProviderModelIdKey("openai", "act")).toBe("actModeOpenAiModelId")
	})

	it("should return correct key for ollama", () => {
		expect(getProviderModelIdKey("ollama", "act")).toBe("actModeOllamaModelId")
	})

	it("should return null for anthropic (uses generic key)", () => {
		expect(getProviderModelIdKey("anthropic", "act")).toBeNull()
	})

	it("should return null for gemini (uses generic key)", () => {
		expect(getProviderModelIdKey("gemini", "act")).toBeNull()
	})

	it("should return null for bedrock (uses generic key)", () => {
		expect(getProviderModelIdKey("bedrock", "act")).toBeNull()
	})

	it("should return null for unknown providers", () => {
		expect(getProviderModelIdKey("unknown-provider", "act")).toBeNull()
	})
})


--- cli/src/stub-devtools.js ---
// Stub for react-devtools-core - not needed in CLI
module.exports = {}


--- cli/src/vscode-context.ts ---
/**
 * VSCode context stub for CLI mode
 * Provides mock implementations of VSCode extension context
 */

import { mkdirSync } from "node:fs"
import { fileURLToPath } from "node:url"
import os from "os"
import path from "path"
import { ExtensionRegistryInfo } from "@/registry"
import { ClineExtensionContext } from "@/shared/cline"
import { ClineFileStorage } from "@/shared/storage"
import { EnvironmentVariableCollection, ExtensionKind, ExtensionMode, readJson, URI } from "./vscode-shim"

// ES module equivalent of __dirname
const __filename = fileURLToPath(import.meta.url)
const __dirname = path.dirname(__filename)

const SETTINGS_SUBFOLDER = "data"

/**
 * CLI-specific state overrides.
 * These values are always returned regardless of what's stored,
 * and writes to these keys are silently ignored.
 */
const CLI_STATE_OVERRIDES: Record<string, any> = {
	// CLI always uses background execution, not VSCode terminal
	vscodeTerminalExecutionMode: "backgroundExec",
	backgroundEditEnabled: true,
	multiRootEnabled: false,
	enableCheckpointsSetting: false,
	browserSettings: {
		disableToolUse: true,
	},
}

/**
 * File-based Memento store with optional key overrides.
 * Implements VSCode's Memento interface using SyncJsonFileStorage.
 */
class MementoStore extends ClineFileStorage {
	private overrides: Record<string, any>

	constructor(filePath: string, overrides: Record<string, any> = {}) {
		super(filePath, "MementoStore")
		this.overrides = overrides
	}

	// VSCode Memento interface - override base class get() with overload support
	override get<T>(key: string): T | undefined
	override get<T>(key: string, defaultValue: T): T
	override get<T>(key: string, defaultValue?: T): T | undefined {
		if (key in this.overrides) {
			return this.overrides[key] as T
		}
		const value = super.get<T>(key)
		return value !== undefined ? value : defaultValue
	}

	override async update(key: string, value: any): Promise<void> {
		if (key in this.overrides) {
			return
		}
		this.set(key, value)
	}

	setKeysForSync(_keys: readonly string[]): void {
		// No-op for CLI
	}
}

/**
 * File-based secret storage implementing VSCode's SecretStorage interface.
 * Uses sync storage internally but exposes async API for VSCode compatibility.
 */
class SecretStore {
	private storage: ClineFileStorage<string>
	private onDidChangeEmitter = {
		event: () => ({ dispose: () => {} }),
		fire: (_e: any) => {},
		dispose: () => {},
	}

	onDidChange = this.onDidChangeEmitter.event

	constructor(filePath: string) {
		this.storage = new ClineFileStorage<string>(filePath, "SecretStore")
	}

	get(key: string): Promise<string | undefined> {
		return Promise.resolve(this.storage.get(key))
	}

	store(key: string, value: string): Promise<void> {
		this.storage.set(key, value)
		return Promise.resolve()
	}

	delete(key: string): Promise<void> {
		this.storage.delete(key)
		return Promise.resolve()
	}
}

export interface CliContextConfig {
	clineDir?: string
	/** The workspace directory being worked in (for hashing into storage path) */
	workspaceDir?: string
}

/**
 * Create a short hash of a string for use in directory names
 */
function hashString(str: string): string {
	let hash = 0
	for (let i = 0; i < str.length; i++) {
		const char = str.charCodeAt(i)
		hash = (hash << 5) - hash + char
		hash = hash & hash // Convert to 32bit integer
	}
	return Math.abs(hash).toString(16).substring(0, 8)
}

export interface CliContextResult {
	extensionContext: ClineExtensionContext
	DATA_DIR: string
	EXTENSION_DIR: string
	WORKSPACE_STORAGE_DIR: string
}

/**
 * Initialize the VSCode-like context for CLI mode
 */
export function initializeCliContext(config: CliContextConfig = {}): CliContextResult {
	const CLINE_DIR = config.clineDir || process.env.CLINE_DIR || path.join(os.homedir(), ".cline")
	const DATA_DIR = path.join(CLINE_DIR, SETTINGS_SUBFOLDER)

	// Workspace storage should always be under ~/.cline/data/workspaces/<hash>/
	// where hash is derived from the workspace path to keep workspaces isolated
	const workspacePath = config.workspaceDir || process.cwd()
	const workspaceHash = hashString(workspacePath)
	const WORKSPACE_STORAGE_DIR = process.env.WORKSPACE_STORAGE_DIR || path.join(DATA_DIR, "workspaces", workspaceHash)

	// Ensure directories exist
	mkdirSync(DATA_DIR, { recursive: true })
	mkdirSync(WORKSPACE_STORAGE_DIR, { recursive: true })

	// For CLI, extension dir is the package root (one level up from dist/)
	const EXTENSION_DIR = path.resolve(__dirname, "..")
	const EXTENSION_MODE = process.env.IS_DEV === "true" ? ExtensionMode.Development : ExtensionMode.Production

	const extension: ClineExtensionContext["extension"] = {
		id: ExtensionRegistryInfo.id,
		isActive: true,
		extensionPath: EXTENSION_DIR,
		extensionUri: URI.file(EXTENSION_DIR),
		packageJSON: readJson(path.join(EXTENSION_DIR, "package.json")),
		exports: undefined,
		activate: async () => {},
		extensionKind: ExtensionKind.UI,
	}

	const extensionContext: ClineExtensionContext = {
		extension: extension,
		extensionMode: EXTENSION_MODE,

		// Set up KV stores (globalState has CLI-specific overrides)
		globalState: new MementoStore(path.join(DATA_DIR, "globalState.json"), CLI_STATE_OVERRIDES),
		secrets: new SecretStore(path.join(DATA_DIR, "secrets.json")),

		// Set up URIs
		storageUri: URI.file(WORKSPACE_STORAGE_DIR),
		storagePath: WORKSPACE_STORAGE_DIR,
		globalStorageUri: URI.file(DATA_DIR),
		globalStoragePath: DATA_DIR,

		// Logs
		logUri: URI.file(DATA_DIR),
		logPath: DATA_DIR,

		extensionUri: URI.file(EXTENSION_DIR),
		extensionPath: EXTENSION_DIR,
		asAbsolutePath: (relPath: string) => path.join(EXTENSION_DIR, relPath),

		subscriptions: [],

		environmentVariableCollection: new EnvironmentVariableCollection() as any,

		// Workspace state
		workspaceState: new MementoStore(path.join(WORKSPACE_STORAGE_DIR, "workspaceState.json")),
	}

	return {
		extensionContext,
		DATA_DIR,
		EXTENSION_DIR,
		WORKSPACE_STORAGE_DIR,
	}
}


--- cli/src/vscode-shim.ts ---
/**
 * VSCode namespace shim for CLI mode
 * Provides minimal stubs for VSCode types and enums used by the codebase
 */

import { existsSync, readFileSync } from "node:fs"
import path from "node:path"
import pino, { type Logger } from "pino"
import { printError, printInfo, printWarning } from "./utils/display"
import { CLINE_CLI_DIR } from "./utils/path"

export { URI } from "vscode-uri"
export { ClineFileStorage } from "@/shared/storage"

export const CLI_LOG_FILE = path.join(CLINE_CLI_DIR.log, "cline-cli.1.log")

/**
 * Safely read and parse a JSON file, returning a default value on failure
 */
export function readJson<T = any>(filePath: string, defaultValue: T = {} as T): T {
	try {
		if (existsSync(filePath)) {
			return JSON.parse(readFileSync(filePath, "utf8"))
		}
	} catch {
		// Return default if file doesn't exist or is invalid
	}
	return defaultValue
}

/**
 * Mock environment variable collection for non-VSCode environments
 */
export class EnvironmentVariableCollection {
	private variables = new Map<string, { value: string; type: string }>()
	persistent = true
	description = "CLI Environment Variables"

	entries() {
		return this.variables.entries()
	}

	replace(variable: string, value: string) {
		this.variables.set(variable, { value, type: "replace" })
	}

	append(variable: string, value: string) {
		this.variables.set(variable, { value, type: "append" })
	}

	prepend(variable: string, value: string) {
		this.variables.set(variable, { value, type: "prepend" })
	}

	get(variable: string) {
		return this.variables.get(variable)
	}

	forEach(callback: (variable: string, mutator: { value: string; type: string }, collection: this) => void) {
		this.variables.forEach((mutator, variable) => callback(variable, mutator, this))
	}

	delete(variable: string) {
		return this.variables.delete(variable)
	}

	clear() {
		this.variables.clear()
	}

	getScoped(_scope: unknown) {
		return this
	}
}

// ============================================================================
// VSCode enums
// ============================================================================

export enum ExtensionMode {
	Production = 1,
	Development = 2,
	Test = 3,
}

export enum ExtensionKind {
	UI = 1,
	Workspace = 2,
}

export enum DiagnosticSeverity {
	Error = 0,
	Warning = 1,
	Information = 2,
	Hint = 3,
}

export enum EndOfLine {
	LF = 1,
	CRLF = 2,
}

const outputChannelLoggers = new Map<string, Logger>()

function getOutputChannelLogger(channelName: string): Logger {
	let logger = outputChannelLoggers.get(channelName)
	if (!logger) {
		const transport = pino.transport({
			target: "pino-roll",
			options: {
				name: channelName,
				file: CLI_LOG_FILE.replace(".1", ""),
				mkdir: true,
				frequency: "daily",
				limit: { count: 5 },
			},
		})
		logger = pino({ timestamp: pino.stdTimeFunctions.isoTime }, transport)
		outputChannelLoggers.set(channelName, logger)
	}
	return logger
}

export class Position {
	constructor(
		public readonly line: number,
		public readonly character: number,
	) {}

	compareTo(other: Position): number {
		return this.line - other.line || this.character - other.character
	}

	isAfter(other: Position): boolean {
		return this.compareTo(other) > 0
	}

	isAfterOrEqual(other: Position): boolean {
		return this.compareTo(other) >= 0
	}

	isBefore(other: Position): boolean {
		return this.compareTo(other) < 0
	}

	isBeforeOrEqual(other: Position): boolean {
		return this.compareTo(other) <= 0
	}

	isEqual(other: Position): boolean {
		return this.compareTo(other) === 0
	}

	translate(lineDelta = 0, characterDelta = 0): Position {
		return new Position(this.line + lineDelta, this.character + characterDelta)
	}

	with(line?: number, character?: number): Position {
		return new Position(line ?? this.line, character ?? this.character)
	}
}

export class Range {
	public readonly start: Position
	public readonly end: Position

	constructor(start: Position, end: Position)
	constructor(startLine: number, startCharacter: number, endLine: number, endCharacter: number)
	constructor(
		startOrStartLine: Position | number,
		endOrStartCharacter: Position | number,
		endLine?: number,
		endCharacter?: number,
	) {
		if (typeof startOrStartLine === "number") {
			this.start = new Position(startOrStartLine, endOrStartCharacter as number)
			this.end = new Position(endLine!, endCharacter!)
		} else {
			this.start = startOrStartLine
			this.end = endOrStartCharacter as Position
		}
	}

	get isEmpty(): boolean {
		return this.start.isEqual(this.end)
	}

	get isSingleLine(): boolean {
		return this.start.line === this.end.line
	}

	contains(positionOrRange: Position | Range): boolean {
		if (positionOrRange instanceof Range) {
			return this.contains(positionOrRange.start) && this.contains(positionOrRange.end)
		}
		return positionOrRange.isAfterOrEqual(this.start) && positionOrRange.isBeforeOrEqual(this.end)
	}

	isEqual(other: Range): boolean {
		return this.start.isEqual(other.start) && this.end.isEqual(other.end)
	}

	intersection(range: Range): Range | undefined {
		const start = this.start.isAfter(range.start) ? this.start : range.start
		const end = this.end.isBefore(range.end) ? this.end : range.end
		return start.isAfter(end) ? undefined : new Range(start, end)
	}

	union(other: Range): Range {
		const start = this.start.isBefore(other.start) ? this.start : other.start
		const end = this.end.isAfter(other.end) ? this.end : other.end
		return new Range(start, end)
	}

	with(start?: Position, end?: Position): Range {
		return new Range(start ?? this.start, end ?? this.end)
	}
}

export class Selection extends Range {
	public readonly anchor: Position
	public readonly active: Position

	constructor(anchor: Position, active: Position)
	constructor(anchorLine: number, anchorCharacter: number, activeLine: number, activeCharacter: number)
	constructor(
		anchorOrAnchorLine: Position | number,
		activeOrAnchorCharacter: Position | number,
		activeLine?: number,
		activeCharacter?: number,
	) {
		const anchor =
			typeof anchorOrAnchorLine === "number"
				? new Position(anchorOrAnchorLine, activeOrAnchorCharacter as number)
				: anchorOrAnchorLine
		const active =
			typeof anchorOrAnchorLine === "number"
				? new Position(activeLine!, activeCharacter!)
				: (activeOrAnchorCharacter as Position)
		const isForward = anchor.isBefore(active)
		super(isForward ? anchor : active, isForward ? active : anchor)
		this.anchor = anchor
		this.active = active
	}

	get isReversed(): boolean {
		return this.anchor.isAfter(this.active)
	}
}

export interface CancellationToken {
	isCancellationRequested: boolean
	onCancellationRequested: any
}

export class EventEmitter<T> {
	private listeners: Array<(e: T) => void> = []

	event = (listener: (e: T) => void) => {
		this.listeners.push(listener)
		return {
			dispose: () => {
				const idx = this.listeners.indexOf(listener)
				if (idx >= 0) this.listeners.splice(idx, 1)
			},
		}
	}

	fire(data: T): void {
		this.listeners.forEach((listener) => listener(data))
	}

	dispose(): void {
		this.listeners.length = 0
	}
}

export class Disposable {
	constructor(private callOnDispose: () => void) {}

	static from(...disposables: { dispose(): any }[]): Disposable {
		return new Disposable(() => disposables.forEach((d) => d.dispose()))
	}

	dispose(): void {
		this.callOnDispose()
	}
}

const noop = () => {}
const noopAsync = async () => {}
const noopDisposable = { dispose: noop }

export const workspace = {
	workspaceFolders: undefined as any[] | undefined,
	getWorkspaceFolder: (_uri: any) => undefined,
	onDidChangeWorkspaceFolders: () => noopDisposable,
	fs: {
		readFile: async (_uri: any): Promise<Uint8Array> => new Uint8Array(),
		writeFile: noopAsync,
		delete: noopAsync,
		stat: async (_uri: any) => ({ type: 1, size: 0 }),
		readDirectory: async (_uri: any): Promise<any[]> => [],
		createDirectory: noopAsync,
	},
}

export const window = {
	showInformationMessage: async (message: string) => {
		printInfo(`[INFO] ${message}`)
	},
	showWarningMessage: async (message: string) => {
		printWarning(`[WARN] ${message}`)
	},
	showErrorMessage: async (message: string) => {
		printError(`[ERROR] ${message}`)
	},
	createOutputChannel: (name: string) => {
		const logger = getOutputChannelLogger(name)
		const log = (text: string) => logger.info({ channel: name }, text)
		return { appendLine: log, append: log, clear: noop, show: noop, hide: noop, dispose: noop }
	},
	terminals: [] as any[],
	activeTerminal: undefined as any,
	createTerminal: (_options?: any) => ({
		name: "CLI Terminal",
		processId: Promise.resolve(process.pid),
		sendText: (text: string) => printInfo(`[${new Date().toISOString()}] [Terminal] ${text}`),
		show: noop,
		hide: noop,
		dispose: noop,
	}),
}

export type ExtensionContext = any
export type Memento = any
export type SecretStorage = any
// biome-ignore lint/correctness/noUnusedVariables: placeholder
export type Extension<T> = any

// ============================================================================
// Shutdown event for graceful cleanup
// ============================================================================

/**
 * Event emitter for app shutdown notification.
 * Components can listen to this to clean up UI before process exit.
 */
export const shutdownEvent = new EventEmitter<void>()


--- cli/src/acp/AcpAgent.ts ---
/**
 * AcpAgent - Thin wrapper that bridges stdio connection to ClineAgent.
 *
 * This class wraps the ClineAgent and connects it to an ACP AgentSideConnection
 * for stdio-based communication. It:
 * - Wires up the permission handler to call connection.requestPermission()
 * - Subscribes to ClineAgent session events and forwards them to connection.sessionUpdate()
 * - Delegates all acp.Agent methods to the internal ClineAgent
 *
 * For programmatic usage without stdio, use ClineAgent directly.
 *
 * @module acp
 */

import type * as acp from "@agentclientprotocol/sdk"
import { Logger } from "@/shared/services/Logger.js"
import { ClineAgent } from "../agent/ClineAgent.js"
import type { AcpAgentOptions, SessionUpdateType } from "../agent/types.js"

/**
 * ACP Agent wrapper that bridges stdio connection to ClineAgent.
 *
 * This is the class used by runAcpMode() for stdio-based ACP communication.
 * It creates an internal ClineAgent and wires up the connection for:
 * - Permission requests (via connection.requestPermission)
 * - Session updates (via connection.sessionUpdate)
 */
export class AcpAgent implements acp.Agent {
	private readonly connection: acp.AgentSideConnection
	private readonly clineAgent: ClineAgent

	/** Track which sessions we've subscribed to for event forwarding */
	private readonly subscribedSessions: Set<string> = new Set()

	constructor(connection: acp.AgentSideConnection, options: AcpAgentOptions) {
		this.connection = connection

		// Create the internal ClineAgent
		this.clineAgent = new ClineAgent(options)

		// Wire up the permission handler to use the connection
		this.clineAgent.setPermissionHandler(async (request, resolve) => {
			try {
				Logger.debug("[AcpAgent] Forwarding permission request to connection")
				const response = await this.connection.requestPermission({
					sessionId: this.getCurrentSessionId() ?? "",
					toolCall: request.toolCall,
					options: request.options,
				})
				resolve(response)
			} catch (error) {
				Logger.debug("[AcpAgent] Error requesting permission:", error)
				resolve({ outcome: "rejected" as unknown as acp.RequestPermissionOutcome })
			}
		})
	}

	/**
	 * Get the current active session ID from the ClineAgent.
	 */
	private getCurrentSessionId(): string | undefined {
		// Find the session that's currently processing
		for (const [sessionId, session] of this.clineAgent.sessions) {
			if (session.controller?.task) {
				return sessionId
			}
		}
		// Fall back to the first session if none is actively processing
		const firstSession = this.clineAgent.sessions.keys().next()
		return firstSession.done ? undefined : firstSession.value
	}

	/**
	 * Subscribe to session events and forward them to the connection.
	 */
	private subscribeToSessionEvents(sessionId: string): void {
		if (this.subscribedSessions.has(sessionId)) {
			return
		}

		const emitter = this.clineAgent.emitterForSession(sessionId)

		// Forward session update by adding the sessionUpdate discriminator
		const forwardSessionUpdate = <K extends SessionUpdateType>(eventName: K) => {
			emitter.on(eventName, (payload: Record<string, unknown>) => {
				const update = {
					sessionUpdate: eventName,
					...payload,
				} as acp.SessionUpdate
				this.connection.sessionUpdate({ sessionId, update }).catch((error) => {
					Logger.error(`[AcpAgent] Error forwarding ${eventName}:`, error)
				})
			})
		}

		// Forward all standard session updates
		forwardSessionUpdate("agent_message_chunk")
		forwardSessionUpdate("agent_thought_chunk")
		forwardSessionUpdate("tool_call")
		forwardSessionUpdate("tool_call_update")
		forwardSessionUpdate("available_commands_update")
		forwardSessionUpdate("plan")
		forwardSessionUpdate("current_mode_update")
		forwardSessionUpdate("user_message_chunk")
		forwardSessionUpdate("config_option_update")
		forwardSessionUpdate("session_info_update")

		// Handle errors specially (not part of ACP SessionUpdate)
		emitter.on("error", (error) => {
			Logger.error("[AcpAgent] Session error:", error)
		})

		this.subscribedSessions.add(sessionId)
	}

	// ============================================================
	// acp.Agent Interface Implementation - Delegate to ClineAgent
	// ============================================================

	async initialize(params: acp.InitializeRequest): Promise<acp.InitializeResponse> {
		return await this.clineAgent.initialize(params, this.connection)
	}

	async newSession(params: acp.NewSessionRequest): Promise<acp.NewSessionResponse> {
		const response = await this.clineAgent.newSession(params)
		// Subscribe to events for this new session
		this.subscribeToSessionEvents(response.sessionId)
		return response
	}

	async prompt(params: acp.PromptRequest): Promise<acp.PromptResponse> {
		// Ensure we're subscribed to this session's events
		this.subscribeToSessionEvents(params.sessionId)
		return this.clineAgent.prompt(params)
	}

	async cancel(params: acp.CancelNotification): Promise<void> {
		return this.clineAgent.cancel(params)
	}

	async setSessionMode(params: acp.SetSessionModeRequest): Promise<acp.SetSessionModeResponse> {
		return this.clineAgent.setSessionMode(params)
	}

	async unstable_setSessionModel(params: acp.SetSessionModelRequest): Promise<acp.SetSessionModelResponse> {
		return this.clineAgent.unstable_setSessionModel(params)
	}

	async authenticate(params: acp.AuthenticateRequest): Promise<acp.AuthenticateResponse> {
		return this.clineAgent.authenticate(params)
	}

	async shutdown(): Promise<void> {
		this.subscribedSessions.clear()
		return this.clineAgent.shutdown()
	}
}


--- evals/README.md ---
# Cline Evaluation Framework

A layered testing system for measuring Cline's performance at different levels.

## Directory Structure

```
evals/
├── smoke-tests/           # Quick provider validation (minutes)
│   ├── run-smoke-tests.ts
│   └── scenarios/         # 5 curated test scenarios
│
├── e2e/                   # Full E2E with cline-bench (hours)
│   └── run-cline-bench.ts
│
├── cline-bench/           # Real-world tasks (git submodule)
│   └── tasks/             # 12 production bug fixes
│
├── analysis/              # Metrics and reporting framework
│   ├── src/
│   │   ├── metrics.ts     # pass@k, pass^k calculations
│   │   ├── classifier.ts  # Failure pattern matching
│   │   └── reporters/     # Markdown, JSON output
│   └── patterns/
│       └── cline-failures.yaml
│
└── baselines/             # Performance baselines for regression detection
```

## Test Layers

### Layer 1: Contract Tests (Unit)

Location: `src/core/api/transform/__tests__/`

Tests API transform logic without LLM calls:
- Thinking trace preservation
- Tool call parsing (XML, native formats)
- Provider format conversions

```bash
npm run test:unit -- --grep "Thinking\|Tool Call"
```

### Layer 2: Smoke Tests (Minutes)

Location: `evals/smoke-tests/`

Quick validation across providers with real LLM calls:
- 5 curated scenarios
- 3 trials per test for pass@k metrics
- Runs via cline CLI with `-s` flags

```bash
# Set API key (Cline provider)
export CLINE_API_KEY=sk-...

# Run smoke tests
npm run eval:smoke

# Run specific scenario
npm run eval:smoke -- --scenario 01-create-file

# Run with specific model (overrides per-scenario models)
npm run eval:smoke -- --model anthropic/claude-sonnet-4.5
```

### Layer 3: E2E Tests (Hours)

Location: `evals/e2e/` + `evals/cline-bench/`

Full agent tests on production-grade tasks:
- 12 real-world coding problems
- Docker/Daytona execution via Harbor
- Nightly CI runs

```bash
# Prerequisites: Python 3.13, Harbor, Docker
npm run eval:e2e

# Specific task
npm run eval:e2e -- --tasks discord

# Different provider
npm run eval:e2e -- --provider openai --model gpt-4o
```

## Metrics

The framework calculates:

| Metric | Formula | Interpretation |
|--------|---------|----------------|
| **pass@k** | P(≥1 of k passes) | Solution finding capability |
| **pass^k** | P(all k pass) | Reliability |
| **Flakiness** | Entropy of pass rate | Consistency |

With 3 trials:
- All pass → `pass` (reliable)
- All fail → `fail` (broken)
- Mixed → `flaky` (needs investigation)

## CI Integration

- **PR Gate**: Contract tests + smoke tests (fast, ~3min)
- **Nightly**: E2E tests with cline-bench (not yet implemented, see TODO)

## Quick Start

```bash
# Run all fast tests
npm run test:unit
npm run eval:smoke

# Run E2E (requires setup)
cd evals/cline-bench
# Follow README.md for Harbor setup
npm run eval:e2e
```

## Adding Tests

### Smoke Test Scenario

1. Create `evals/smoke-tests/scenarios/<name>/config.json`
2. Add optional `template/` directory with starting files
3. Run to verify: `npm run eval:smoke -- --scenario <name>`

### Contract Test

1. Add to `src/core/api/transform/__tests__/`
2. Run: `npm run test:unit -- --grep "YourTest"`

### E2E Task

Contribute to [cline/cline-bench](https://github.com/cline/cline-bench)

## Resources

- [cline-bench tasks](evals/cline-bench/README.md)
- [Smoke test scenarios](evals/smoke-tests/README.md)

## TODO

- [ ] **Nightly E2E CI**: Add scheduled workflow for cline-bench tests
  - Requires: Docker runner, Harbor setup, ~1-2 hour timeout
  - Should run on schedule (e.g., nightly) not per-PR
  - Separate secrets for E2E environment
- [ ] **Native tool calling smoke tests**: Add CLI support for `native_tool_call_enabled` setting to test Claude 4 with native tools


## Links discovered
- [cline/cline-bench](https://github.com/cline/cline-bench)
- [cline-bench tasks](https://github.com/cline/cline/blob/main/evals/evals/cline-bench/README.md)
- [Smoke test scenarios](https://github.com/cline/cline/blob/main/evals/evals/smoke-tests/README.md)

--- evals/e2e/README.md ---
# E2E Agent Tests

Full end-to-end tests using real-world tasks from cline-bench.

## Overview

These tests run Cline against production-grade coding problems derived from actual user sessions. Each task:
- Starts with a broken codebase in Docker
- Gives Cline the task description
- Verifies the fix with pytest

## Prerequisites

1. **Python 3.13 with uv**
   ```bash
   # macOS
   brew install python@3.13
   pip install uv
   ```

2. **Harbor** (benchmark execution framework)
   ```bash
   uv tool install harbor
   ```

3. **Docker** (for local execution)
   ```bash
   # Verify Docker is running
   docker info
   ```

4. **API Keys**
   ```bash
   export ANTHROPIC_API_KEY=sk-ant-...
   # or
   export API_KEY=sk-ant-...  # Generic fallback
   ```

## Running Locally

```bash
# Run all tasks with default settings (Anthropic, Docker)
npx tsx evals/e2e/run-cline-bench.ts

# Run specific task
npx tsx evals/e2e/run-cline-bench.ts --tasks discord

# Use different provider/model
npx tsx evals/e2e/run-cline-bench.ts --provider openai --model gpt-4o

# Run on Daytona cloud (faster, parallel)
export DAYTONA_API_KEY=dtn_...
npx tsx evals/e2e/run-cline-bench.ts --env daytona

# Output to JSON
npx tsx evals/e2e/run-cline-bench.ts --output results.json
```

## CLI Options

| Option | Default | Description |
|--------|---------|-------------|
| `--env` | `docker` | Execution environment: `docker` or `daytona` |
| `--provider` | `anthropic` | Provider: `anthropic`, `openai`, `openrouter`, `gemini` |
| `--model` | `claude-sonnet-4-20250514` | Model ID |
| `--tasks` | `all` | Task filter pattern |
| `--trials` | `1` | Number of trials per task |
| `--output` | - | Write JSON results to file |

## Tasks

Current tasks from cline-bench (12 total):

1. **every-plugin-api-migration** - Migrate API calls in plugin
2. **police-sync-segfault** - Fix segmentation fault
3. **intercept-axios-error-handling** - Fix Axios error handling
4. **telegram-plugin-refactor** - Refactor Telegram plugin
5. **discord-trivia-approval-keyerror** - Fix KeyError in Discord bot
6. **terraform-azurerm-deployment-stacks** - Terraform provider fix
7. **orpc-client-migration** - Client migration task
8. **v-edit-workspace-tests** - Fix workspace tests
9. **healthchain-prefetch-removal** - Remove prefetch logic
10. **aenet-pytorch-pbc-neighborlist** - PyTorch PBC fix
11. **suave-http-data-bleeding** - Fix HTTP data bleeding
12. **filmarchiver** - Film archiver fixes

## CI Integration

These tests run nightly (not on every PR) due to:
- Long execution time (20-30 min per task)
- API costs (~$1-5 per run depending on model)
- Docker/Daytona infrastructure requirements

See `.github/workflows/nightly-evals.yml` for CI configuration.

## Results

Results are written to `evals/cline-bench/jobs/` directory by Harbor:

```
jobs/
└── 2025-01-25__10-00-00/
    ├── result.json              # Aggregate results
    └── <task-id>__<hash>/
        ├── result.json          # Trial result
        ├── agent/cline.txt      # Conversation log
        └── verifier/reward.txt  # 1 (pass) or 0 (fail)
```

## Troubleshooting

### "Harbor not found"

```bash
source .venv/bin/activate  # If using venv
uv tool install harbor
```

### "Docker not available"

```bash
# Start Docker daemon
docker info  # Should show Docker info
```

### Task timeouts

Some tasks (Qt WASM, Android) can take 20-30 minutes. If running locally, ensure Docker has sufficient resources (8GB+ RAM).


--- evals/smoke-tests/README.md ---
# Smoke Tests

Curated smoke tests that verify Cline works correctly with LLM providers.

## Purpose

These tests catch regressions in:
- Tool execution (read, write, edit files)
- Provider response parsing
- Tool chaining (multiple operations)
- Basic code generation

## Quick Start

```bash
# One-time auth setup
cline auth

# Build CLI from source (after code changes)
npm run eval:smoke:build

# Run tests (3 trials by default)
npm run eval:smoke:run

# Or build + run in one command
npm run eval:smoke
```

## Commands

| Command | What it does |
|---------|--------------|
| `npm run eval:smoke:build` | Build/install CLI from source |
| `npm run eval:smoke:run` | Run tests (uses installed CLI) |
| `npm run eval:smoke` | Build + run (3 trials) |
| `npm run eval:smoke:ci` | Build + run (1 trial, for CI) |

## Options

```bash
# Run specific scenario
npm run eval:smoke:run -- --scenario 01-create-file

# Run with fewer trials (faster)
npm run eval:smoke:run -- --trials 1

# Run with specific model (overrides any per-scenario models)
npm run eval:smoke:run -- --model claude-sonnet-4-5-20250929
```

## Authentication

### Interactive (recommended for local dev)

```bash
cline auth
```

### With API key (for automation)

```bash
cline auth -p cline -k "$CLINE_API_KEY" -m anthropic/claude-sonnet-4.5
```

## Scenarios

| ID | Name | What it tests |
|----|------|---------------|
| 01-create-file | Create a simple file | `write_to_file` |
| 02-edit-file | Edit existing file | `replace_in_file` |
| 03-read-summarize | Read and summarize | `read_file` |
| 04-multi-file | Create multiple files | Multiple tool calls |
| 05-typescript-function | Generate TypeScript | Code generation |
| 06-apply-patch | Edit file (GPT-5) | `apply_patch` tool, native tool calling |
| 07-edit-gemini | Edit file (Gemini) | Gemini model variant |

### Per-Scenario Models

Scenarios can specify their own model(s) via the `models` field in `config.json`. This is useful for testing model-specific code paths like `apply_patch` (GPT-5 only).

If you pass `--model`, it overrides any per-scenario `models` list.

Examples:
```bash
# Run apply_patch scenario with its default model (GPT-5)
npm run eval:smoke:run -- --scenario 06-apply-patch

# Force that scenario to use a specific model
npm run eval:smoke:run -- --scenario 06-apply-patch --model openai/gpt-4o
```

## Metrics

- **pass@k**: Probability at least 1 of k trials succeeds
- **pass^k**: Probability ALL k trials succeed (reliability)

Shows `pass@1` when trials < 3, `pass@3` otherwise.

## Adding New Scenarios

1. Create directory: `scenarios/<name>/`
2. Add `config.json`:
   ```json
   {
     "name": "Human-readable name",
     "description": "What this tests",
     "prompt": "The task prompt for Cline",
     "expectedFiles": ["file1.txt"],
     "expectedContent": [
       { "file": "file1.txt", "contains": "expected text" }
     ],
     "timeout": 60
   }
   ```
3. (Optional) Add `template/` directory with starting files

## CI Integration

Smoke tests run automatically via `.github/workflows/cline-evals-regression.yml`.

### Triggers

- Push to `main` (when core code changes)
- Pull requests
- Manual dispatch

### Architecture

```
Build Job (1x)          Test Jobs (5x parallel)       Summarize
─────────────────       ─────────────────────────     ──────────
compile-cli             Download artifact             Merge results
compile-standalone  →   Install CLI               →   Post summary
Upload artifact         Configure auth
                        Run single scenario
```

### Required Secrets

- `CLINE_API_KEY` - Cline API key

### Viewing Results

- Actions tab → "Smoke Tests" workflow
- View "Summary" for quick results
- Download "smoke-test-results" artifact for details

## TODO

- [ ] **Native tool calling tests**: Add CLI support for `native_tool_call_enabled` setting, then create a scenario that tests Claude 4 with native tool calling enabled (currently only GPT-5 models automatically use native tools via the Responses API)


--- evals/e2e/run-cline-bench.ts ---
#!/usr/bin/env npx tsx
/**
 * cline-bench Runner
 *
 * Runs real-world tasks from cline-bench using Harbor framework.
 * Designed for nightly CI execution.
 *
 * Prerequisites:
 *   - Python 3.13 with uv
 *   - Harbor installed (`uv tool install harbor`)
 *   - Docker (for local) or DAYTONA_API_KEY (for cloud)
 *
 * Usage:
 *   npx tsx evals/e2e/run-cline-bench.ts [options]
 *
 * Options:
 *   --env <docker|daytona>  Execution environment (default: docker)
 *   --provider <name>       Provider to use (default: anthropic)
 *   --model <id>            Model ID (default: claude-sonnet-4-20250514)
 *   --tasks <pattern>       Task filter pattern (default: all)
 *   --trials <n>            Number of trials per task (default: 1)
 *   --output <file>         Write results to JSON file
 */

import { execSync, spawnSync } from "child_process"
import * as fs from "fs"
import * as path from "path"

interface RunOptions {
	env: "docker" | "daytona"
	provider: string
	model: string
	tasks: string
	trials: number
	outputFile?: string
}

// Provider configurations for Harbor model format
const PROVIDER_MODEL_PREFIX: Record<string, string> = {
	anthropic: "anthropic",
	openrouter: "openrouter",
	openai: "openai-native",
	gemini: "gemini", // Needs different handling
}

const PROVIDER_API_KEY_ENV: Record<string, string> = {
	anthropic: "ANTHROPIC_API_KEY",
	openrouter: "OPENROUTER_API_KEY",
	openai: "OPENAI_API_KEY",
	gemini: "GEMINI_API_KEY",
}

function checkPrerequisites(): { ok: boolean; error?: string } {
	// Check Python
	try {
		const pythonVersion = execSync("python3 --version", { encoding: "utf-8" })
		if (!pythonVersion.includes("3.13")) {
			console.warn(`Warning: Python 3.13 recommended, found: ${pythonVersion.trim()}`)
		}
	} catch {
		return { ok: false, error: "Python 3 not found" }
	}

	// Check Harbor
	try {
		execSync("which harbor", { encoding: "utf-8" })
	} catch {
		return { ok: false, error: "Harbor not found. Install with: uv tool install harbor" }
	}

	// Check Docker (for local env)
	try {
		execSync("docker info > /dev/null 2>&1")
	} catch {
		console.warn("Warning: Docker not available. Use --env daytona for cloud execution.")
	}

	return { ok: true }
}

function getTaskList(clineBenchDir: string, filter?: string): string[] {
	const tasksDir = path.join(clineBenchDir, "tasks")
	if (!fs.existsSync(tasksDir)) {
		throw new Error(`Tasks directory not found: ${tasksDir}`)
	}

	let tasks = fs
		.readdirSync(tasksDir, { withFileTypes: true })
		.filter((d) => d.isDirectory())
		.map((d) => d.name)

	if (filter && filter !== "all") {
		tasks = tasks.filter((t) => t.includes(filter))
	}

	return tasks
}

interface TaskResult {
	taskId: string
	passed: boolean
	duration_sec: number
	error?: string
}

function runHarborTask(clineBenchDir: string, taskId: string, options: RunOptions): TaskResult {
	const startTime = Date.now()

	// Build Harbor model string
	const modelPrefix = PROVIDER_MODEL_PREFIX[options.provider] || options.provider
	const harborModel = `${modelPrefix}:${options.model}`

	// Set up environment
	const apiKeyEnv = PROVIDER_API_KEY_ENV[options.provider]
	const apiKey = process.env[apiKeyEnv] || process.env.API_KEY

	if (!apiKey) {
		return {
			taskId,
			passed: false,
			duration_sec: 0,
			error: `Missing API key: ${apiKeyEnv} or API_KEY`,
		}
	}

	const harborEnv = {
		...process.env,
		API_KEY: apiKey,
	}

	// Build Harbor command
	const harborArgs = ["run", "-p", `tasks/${taskId}`, "-a", "cline-cli", "-m", harborModel, "--env", options.env]

	console.log(`  Running: harbor ${harborArgs.join(" ")}`)

	try {
		const result = spawnSync("harbor", harborArgs, {
			cwd: clineBenchDir,
			env: harborEnv,
			stdio: ["inherit", "pipe", "pipe"],
			timeout: 30 * 60 * 1000, // 30 minutes
		})

		const duration_sec = (Date.now() - startTime) / 1000

		if (result.status !== 0) {
			return {
				taskId,
				passed: false,
				duration_sec,
				error: result.stderr?.toString() || `Exit code: ${result.status}`,
			}
		}

		// Check if task passed by looking at the latest job results
		// Harbor writes results to jobs/ directory
		const jobsDir = path.join(clineBenchDir, "jobs")
		if (fs.existsSync(jobsDir)) {
			const latestJob = fs
				.readdirSync(jobsDir)
				.filter((d) => d.startsWith("2"))
				.sort()
				.pop()

			if (latestJob) {
				const jobDir = path.join(jobsDir, latestJob)
				const trialDirs = fs.readdirSync(jobDir).filter((d) => d.includes(taskId.substring(0, 10)))

				for (const trialDir of trialDirs) {
					const rewardFile = path.join(jobDir, trialDir, "verifier", "reward.txt")
					if (fs.existsSync(rewardFile)) {
						const reward = fs.readFileSync(rewardFile, "utf-8").trim()
						return {
							taskId,
							passed: reward === "1",
							duration_sec,
						}
					}
				}
			}
		}

		// Couldn't determine result from files
		return {
			taskId,
			passed: false,
			duration_sec,
			error: "Could not determine task result",
		}
	} catch (error: any) {
		return {
			taskId,
			passed: false,
			duration_sec: (Date.now() - startTime) / 1000,
			error: error.message || String(error),
		}
	}
}

interface BenchmarkReport {
	timestamp: string
	provider: string
	model: string
	environment: string
	trialsPerTask: number
	results: TaskResult[]
	summary: {
		total: number
		passed: number
		failed: number
		passRate: number
	}
}

async function main() {
	const args = process.argv.slice(2)

	// Parse arguments
	const options: RunOptions = {
		env: "docker",
		provider: "anthropic",
		model: "claude-sonnet-4-20250514",
		tasks: "all",
		trials: 1,
	}

	for (let i = 0; i < args.length; i++) {
		if (args[i] === "--env" && args[i + 1]) {
			options.env = args[++i] as "docker" | "daytona"
		} else if (args[i] === "--provider" && args[i + 1]) {
			options.provider = args[++i]
		} else if (args[i] === "--model" && args[i + 1]) {
			options.model = args[++i]
		} else if (args[i] === "--tasks" && args[i + 1]) {
			options.tasks = args[++i]
		} else if (args[i] === "--trials" && args[i + 1]) {
			options.trials = parseInt(args[++i], 10)
		} else if (args[i] === "--output" && args[i + 1]) {
			options.outputFile = args[++i]
		}
	}

	// Check prerequisites
	const prereq = checkPrerequisites()
	if (!prereq.ok) {
		console.error(`Prerequisite check failed: ${prereq.error}`)
		process.exit(1)
	}

	// Find cline-bench directory
	const clineBenchDir = path.join(__dirname, "..", "cline-bench")
	if (!fs.existsSync(clineBenchDir)) {
		console.error(`cline-bench not found at: ${clineBenchDir}`)
		console.error("Ensure the submodule is initialized: git submodule update --init")
		process.exit(1)
	}

	// Get task list
	const tasks = getTaskList(clineBenchDir, options.tasks)
	if (tasks.length === 0) {
		console.error("No tasks found matching filter:", options.tasks)
		process.exit(1)
	}

	console.log(`cline-bench E2E Runner`)
	console.log(`======================`)
	console.log(`Provider: ${options.provider}`)
	console.log(`Model: ${options.model}`)
	console.log(`Environment: ${options.env}`)
	console.log(`Tasks: ${tasks.length}`)
	console.log(`Trials per task: ${options.trials}`)
	console.log("")

	const results: TaskResult[] = []

	// Run tasks
	for (const taskId of tasks) {
		console.log(`\n[${taskId}]`)

		for (let trial = 0; trial < options.trials; trial++) {
			if (options.trials > 1) {
				console.log(`  Trial ${trial + 1}/${options.trials}`)
			}

			const result = runHarborTask(clineBenchDir, taskId, options)
			results.push(result)

			console.log(`  Result: ${result.passed ? "✓ PASS" : `✗ FAIL: ${result.error || "unknown"}`}`)
			console.log(`  Duration: ${result.duration_sec.toFixed(1)}s`)
		}
	}

	// Generate report
	const passed = results.filter((r) => r.passed).length
	const report: BenchmarkReport = {
		timestamp: new Date().toISOString(),
		provider: options.provider,
		model: options.model,
		environment: options.env,
		trialsPerTask: options.trials,
		results,
		summary: {
			total: results.length,
			passed,
			failed: results.length - passed,
			passRate: results.length > 0 ? passed / results.length : 0,
		},
	}

	// Output
	if (options.outputFile) {
		fs.writeFileSync(options.outputFile, JSON.stringify(report, null, 2))
		console.log(`\nResults written to: ${options.outputFile}`)
	}

	// Summary
	console.log("\n" + "=".repeat(60))
	console.log("SUMMARY")
	console.log("=".repeat(60))
	console.log(`Total: ${report.summary.total}`)
	console.log(`Passed: ${report.summary.passed}`)
	console.log(`Failed: ${report.summary.failed}`)
	console.log(`Pass Rate: ${(report.summary.passRate * 100).toFixed(1)}%`)

	// Exit with error if any failures
	if (report.summary.failed > 0) {
		process.exit(1)
	}
}

main().catch((err) => {
	console.error(err)
	process.exit(1)
})


--- evals/smoke-tests/run-smoke-tests.ts ---
#!/usr/bin/env npx tsx
/**
 * Smoke Test Runner for Cline
 *
 * Runs curated smoke tests against configured providers to verify:
 * - Basic tool execution works
 * - Provider responses are correctly parsed
 * - Thinking traces are preserved
 *
 * Usage:
 *   npx tsx evals/smoke-tests/run-smoke-tests.ts [options]
 *
 * Options:
 *   --provider <name>  Run tests for a specific provider (default: all configured)
 *   --trials <n>       Number of trials per test (default: 3)
 *   --scenario <name>  Run a specific scenario (default: all)
 *   --output <file>    Write JSON results to file
 */

import { execSync, spawn } from "child_process"
import * as fs from "fs"
import * as path from "path"
import { MetricsCalculator } from "../analysis/src/metrics"

// Default provider and model for smoke tests
// These ensure deterministic behavior regardless of local config
const DEFAULT_PROVIDER = "cline"
const DEFAULT_MODEL = "anthropic/claude-sonnet-4.5"

// Models to test - can be overridden with --model flag
const MODELS: string[] = [DEFAULT_MODEL]

// Check if cline CLI is available
function checkClineCli(): boolean {
	try {
		execSync("which cline", { encoding: "utf-8", timeout: 5000 })
		return true
	} catch {
		return false
	}
}

// Use user's existing Cline config (already has auth configured)
// For CI, this would be set up by the auth step before tests run
const CLINE_CONFIG_DIR = path.join(process.env.HOME || "", ".cline")

// Configure authentication using CLINE_API_KEY environment variable
// Returns success if auth is configured, error message otherwise
function configureAuth(): { ok: boolean; error?: string } {
	const apiKey = process.env.CLINE_API_KEY
	if (!apiKey) {
		return {
			ok: false,
			error: "CLINE_API_KEY environment variable not set",
		}
	}

	// Ensure config directory exists
	fs.mkdirSync(CLINE_CONFIG_DIR, { recursive: true })

	try {
		// Run quick auth setup (non-interactive when all flags provided)
		execSync(`cline auth --config "${CLINE_CONFIG_DIR}" -p ${DEFAULT_PROVIDER} -k "${apiKey}" -m "${DEFAULT_MODEL}"`, {
			encoding: "utf-8",
			timeout: 10000,
			stdio: "pipe",
		})
		return { ok: true }
	} catch (err: any) {
		return {
			ok: false,
			error: err.message || "Auth command failed",
		}
	}
}

// Smoke test scenario definition
interface SmokeScenario {
	id: string
	name: string
	description: string
	prompt: string
	workdir: string // Relative to scenario directory
	expectedFiles?: string[] // Files that should exist after
	expectedContent?: { file: string; contains: string }[] // Content checks
	timeout: number // Seconds
	models?: string[] // Optional: override default models for this scenario
}

// Load scenarios from disk
function loadScenarios(scenariosDir: string): SmokeScenario[] {
	const scenarios: SmokeScenario[] = []

	for (const entry of fs.readdirSync(scenariosDir, { withFileTypes: true })) {
		if (entry.isDirectory()) {
			const configPath = path.join(scenariosDir, entry.name, "config.json")
			if (fs.existsSync(configPath)) {
				const config = JSON.parse(fs.readFileSync(configPath, "utf-8"))
				scenarios.push({
					...config,
					id: entry.name,
					workdir: path.join(scenariosDir, entry.name, "workspace"),
				})
			}
		}
	}

	return scenarios
}

// Run a single trial
interface TrialResult {
	passed: boolean
	error?: string
	durationMs: number
	stdout: string
	stderr: string
}

async function runTrial(scenario: SmokeScenario, modelId: string, trialWorkdir: string): Promise<TrialResult> {
	const startTime = Date.now()

	// Ensure workspace exists and is clean
	if (fs.existsSync(trialWorkdir)) {
		fs.rmSync(trialWorkdir, { recursive: true })
	}
	fs.mkdirSync(trialWorkdir, { recursive: true })

	// Copy any template files from scenario
	const templateDir = path.join(path.dirname(scenario.workdir), "template")
	if (fs.existsSync(templateDir)) {
		fs.cpSync(templateDir, trialWorkdir, { recursive: true })
	}

	// Build CLI command with explicit model setting for determinism
	// Provider is configured via `cline auth` before running tests
	const args = [
		"--config",
		CLINE_CONFIG_DIR, // Use shared config directory for auth
		"-y", // YOLO mode - auto-approve all actions, exits after completion
		"-t",
		String(scenario.timeout), // CLI timeout (matches our timeout)
		"-m",
		modelId, // Model to use (overrides configured default)
		scenario.prompt,
	]

	try {
		// Run cline CLI
		const result = await runClineWithTimeout(args, trialWorkdir, scenario.timeout * 1000)

		if (!result.success) {
			return {
				passed: false,
				error: result.error || "CLI execution failed",
				durationMs: Date.now() - startTime,
				stdout: result.stdout,
				stderr: result.stderr,
			}
		}

		// Verify expected files
		if (scenario.expectedFiles) {
			for (const file of scenario.expectedFiles) {
				const filePath = path.join(trialWorkdir, file)
				if (!fs.existsSync(filePath)) {
					return {
						passed: false,
						error: `Expected file not found: ${file}`,
						durationMs: Date.now() - startTime,
						stdout: result.stdout,
						stderr: result.stderr,
					}
				}
			}
		}

		// Verify expected content
		if (scenario.expectedContent) {
			for (const check of scenario.expectedContent) {
				const filePath = path.join(trialWorkdir, check.file)
				if (!fs.existsSync(filePath)) {
					return {
						passed: false,
						error: `File not found for content check: ${check.file}`,
						durationMs: Date.now() - startTime,
						stdout: result.stdout,
						stderr: result.stderr,
					}
				}
				const content = fs.readFileSync(filePath, "utf-8")
				if (!content.includes(check.contains)) {
					return {
						passed: false,
						error: `Expected content not found in ${check.file}: "${check.contains}"`,
						durationMs: Date.now() - startTime,
						stdout: result.stdout,
						stderr: result.stderr,
					}
				}
			}
		}

		return {
			passed: true,
			durationMs: Date.now() - startTime,
			stdout: result.stdout,
			stderr: result.stderr,
		}
	} catch (error: any) {
		return {
			passed: false,
			error: error.message || String(error),
			durationMs: Date.now() - startTime,
			stdout: "",
			stderr: "",
		}
	}
}

// Run cline CLI with timeout
interface ClineResult {
	success: boolean
	error?: string
	stdout: string
	stderr: string
}

function runClineWithTimeout(args: string[], cwd: string, timeoutMs: number): Promise<ClineResult> {
	return new Promise((resolve) => {
		let stdout = ""
		let stderr = ""

		const proc = spawn("cline", args, {
			cwd,
			env: { ...process.env },
			stdio: ["ignore", "pipe", "pipe"], // stdin: ignore, stdout/stderr: pipe
		})

		const timeout = setTimeout(() => {
			proc.kill("SIGKILL")
			resolve({
				success: false,
				error: "Timeout exceeded",
				stdout,
				stderr,
			})
		}, timeoutMs)

		proc.stdout?.on("data", (data) => {
			stdout += data.toString()
		})

		proc.stderr?.on("data", (data) => {
			stderr += data.toString()
		})

		proc.on("error", (err) => {
			clearTimeout(timeout)
			resolve({
				success: false,
				error: err.message,
				stdout,
				stderr,
			})
		})

		proc.on("close", (code) => {
			clearTimeout(timeout)
			let error: string | undefined
			if (code !== 0) {
				// Include last line of stderr for context
				const lastStderr = stderr.trim().split("\n").slice(-3).join(" | ")
				error = `Exit code: ${code}${lastStderr ? ` - ${lastStderr}` : ""}`
			}
			resolve({
				success: code === 0,
				error,
				stdout,
				stderr,
			})
		})
	})
}

// Result types
interface ScenarioResult {
	scenarioId: string
	scenarioName: string
	model: string
	modelId: string
	trials: TrialResult[]
	metrics: {
		passAt1: number
		passAt3: number
		passCaret3: number
		flakinessScore: number
	}
	status: "pass" | "fail" | "flaky"
}

interface SmokeTestReport {
	timestamp: string
	provider: string
	models: string[]
	scenarios: string[]
	trialsPerTest: number
	results: ScenarioResult[]
	summary: {
		total: number
		passed: number
		failed: number
		flaky: number
		passAt1Overall: number
		passAt3Overall: number
	}
}

// Main execution
async function main() {
	const args = process.argv.slice(2)

	// Parse arguments
	let selectedModel: string | undefined
	let trials = 3
	let selectedScenario: string | undefined
	let outputFile: string | undefined
	let parallel = false
	let parallelLimit = 4

	for (let i = 0; i < args.length; i++) {
		if (args[i] === "--model" && args[i + 1]) {
			selectedModel = args[++i]
		} else if (args[i] === "--trials" && args[i + 1]) {
			trials = parseInt(args[++i], 10)
		} else if (args[i] === "--scenario" && args[i + 1]) {
			selectedScenario = args[++i]
		} else if (args[i] === "--output" && args[i + 1]) {
			outputFile = args[++i]
		} else if (args[i] === "--parallel") {
			parallel = true
			if (args[i + 1] && !args[i + 1].startsWith("--")) {
				parallelLimit = parseInt(args[++i], 10)
			}
		}
	}

	// Check cline CLI is available
	if (!checkClineCli()) {
		console.error("ERROR: cline CLI not found in PATH")
		console.error("")
		console.error("For local development:")
		console.error("  cd cli && npm install && npm run build && npm link")
		console.error("")
		console.error("For CI:")
		console.error("  Ensure CLI build and 'npm link' steps completed")
		process.exit(1)
	}

	// Configure authentication if CLINE_API_KEY is set
	// Otherwise use existing auth from ~/.cline
	if (process.env.CLINE_API_KEY) {
		console.log("Configuring authentication from CLINE_API_KEY...")
		const authResult = configureAuth()
		if (!authResult.ok) {
			console.error("")
			console.error("ERROR: Authentication failed")
			console.error(`  ${authResult.error}`)
			console.error("")
			process.exit(1)
		}
		console.log("Authentication configured")
	} else {
		console.log("Using existing authentication from ~/.cline")
	}
	console.log("")

	// Load scenarios
	const scenariosDir = path.join(__dirname, "scenarios")
	let scenarios = loadScenarios(scenariosDir)

	if (scenarios.length === 0) {
		console.error("No scenarios found in", scenariosDir)
		process.exit(1)
	}

	if (selectedScenario) {
		scenarios = scenarios.filter((s) => s.id === selectedScenario)
		if (scenarios.length === 0) {
			console.error(`Scenario not found: ${selectedScenario}`)
			process.exit(1)
		}
	}

	// Filter models
	let models = MODELS
	if (selectedModel) {
		models = [selectedModel]
	}

	// Create results directory with timestamp
	const timestamp = new Date().toISOString().replace(/[:.]/g, "-")
	const resultsBaseDir = path.join(__dirname, "results")
	const resultsDir = path.join(resultsBaseDir, timestamp)
	fs.mkdirSync(resultsDir, { recursive: true })

	// Models are now always explicit
	const resolvedModels = models

	console.log(`Running ${scenarios.length} scenarios × ${models.length} models × ${trials} trials`)
	console.log(`Provider: ${DEFAULT_PROVIDER}`)
	console.log(`Models: ${resolvedModels.join(", ")}`)
	console.log(`Scenarios: ${scenarios.map((s) => s.id).join(", ")}`)
	console.log(`Results: ${resultsDir}`)
	console.log(`Parallel: ${parallel ? `yes (limit: ${parallelLimit})` : "no"}`)
	console.log("")

	const metricsCalc = new MetricsCalculator()
	const results: ScenarioResult[] = []

	// Build list of all scenario+model combinations
	interface TestJob {
		scenario: Scenario
		modelId: string
	}
	const jobs: TestJob[] = []
	for (const scenario of scenarios) {
		const scenarioModels = selectedModel ? [selectedModel] : scenario.models || models
		for (const modelId of scenarioModels) {
			jobs.push({ scenario, modelId })
		}
	}

	// Run a single job
	async function runJob(job: TestJob): Promise<ScenarioResult> {
		const { scenario, modelId } = job
		const logDir = path.join(resultsDir, scenario.id, modelId)
		fs.mkdirSync(logDir, { recursive: true })

		const trialResults: TrialResult[] = []
		const trialWorkdirs: string[] = []

		for (let t = 0; t < trials; t++) {
			const trialWorkdir = path.join(logDir, `workspace-trial-${t + 1}`)
			trialWorkdirs.push(trialWorkdir)
			const result = await runTrial(scenario, modelId, trialWorkdir)
			trialResults.push(result)
		}

		trialResults.forEach((result, t) => {
			const trialNum = t + 1
			const logContent =
				`# Trial ${trialNum}\n` +
				`Status: ${result.passed ? "PASS" : "FAIL"}\n` +
				`Duration: ${result.durationMs}ms\n` +
				(result.error ? `Error: ${result.error}\n` : "") +
				`\n## STDOUT\n${result.stdout || "(empty)"}\n` +
				`\n## STDERR\n${result.stderr || "(empty)"}\n`
			fs.writeFileSync(path.join(logDir, `trial-${trialNum}.log`), logContent)
		})

		const trialBools = trialResults.map((t) => t.passed)
		const metrics = metricsCalc.calculateTaskMetrics(trialBools)
		const status = metricsCalc.getTaskStatus(trialBools)

		return {
			scenarioId: scenario.id,
			scenarioName: scenario.name,
			model: modelId,
			modelId: modelId,
			trials: trialResults,
			metrics,
			status,
		}
	}

	if (parallel) {
		// Run jobs in parallel with concurrency limit
		console.log(`Running ${jobs.length} jobs in parallel...`)
		const executing: Promise<void>[] = []

		for (const job of jobs) {
			const p = runJob(job).then((result) => {
				results.push(result)
				const passMetric = trials >= 3 ? result.metrics.passAt3 : result.metrics.passAt1
				const icon = result.status === "pass" ? "✓" : result.status === "flaky" ? "~" : "✗"
				console.log(
					`  ${icon} [${result.scenarioId}] ${result.model}: ${result.status.toUpperCase()} (${(passMetric * 100).toFixed(0)}%)`,
				)
			})
			executing.push(p as unknown as Promise<void>)

			if (executing.length >= parallelLimit) {
				await Promise.race(executing)
				// Remove settled promises
				for (let i = executing.length - 1; i >= 0; i--) {
					const settled = await Promise.race([executing[i].then(() => true).catch(() => true), Promise.resolve(false)])
					if (settled) executing.splice(i, 1)
				}
			}
		}
		await Promise.all(executing)
	} else {
		// Sequential execution
		for (const job of jobs) {
			console.log(`\n[${job.scenario.id}] ${job.scenario.name} (${job.modelId})`)
			const logDir = path.join(resultsDir, job.scenario.id, job.modelId)
			fs.mkdirSync(logDir, { recursive: true })

			const trialResults: TrialResult[] = []
			const trialWorkdirs: string[] = []

			for (let t = 0; t < trials; t++) {
				const trialWorkdir = path.join(logDir, `workspace-trial-${t + 1}`)
				trialWorkdirs.push(trialWorkdir)
				process.stdout.write(`  Trial ${t + 1}/${trials}... `)
				const result = await runTrial(job.scenario, job.modelId, trialWorkdir)
				trialResults.push(result)
				console.log(result.passed ? "✓ PASS" : `✗ FAIL: ${result.error}`)
			}

			trialResults.forEach((result, t) => {
				const trialNum = t + 1
				const logContent =
					`# Trial ${trialNum}\n` +
					`Status: ${result.passed ? "PASS" : "FAIL"}\n` +
					`Duration: ${result.durationMs}ms\n` +
					(result.error ? `Error: ${result.error}\n` : "") +
					`\n## STDOUT\n${result.stdout || "(empty)"}\n` +
					`\n## STDERR\n${result.stderr || "(empty)"}\n`
				fs.writeFileSync(path.join(logDir, `trial-${trialNum}.log`), logContent)
			})

			const trialBools = trialResults.map((t) => t.passed)
			const metrics = metricsCalc.calculateTaskMetrics(trialBools)
			const status = metricsCalc.getTaskStatus(trialBools)

			results.push({
				scenarioId: job.scenario.id,
				scenarioName: job.scenario.name,
				model: job.modelId,
				modelId: job.modelId,
				trials: trialResults,
				metrics,
				status,
			})

			// Display pass@k where k = actual trials (pass@3 is meaningless with fewer trials)
			const passMetric = trials >= 3 ? metrics.passAt3 : metrics.passAt1
			const passLabel = trials >= 3 ? "pass@3" : "pass@1"
			console.log(`  Result: ${status.toUpperCase()} | ${passLabel}: ${(passMetric * 100).toFixed(0)}%`)
		}
	}

	// Generate report
	const report: SmokeTestReport = {
		timestamp: new Date().toISOString(),
		provider: DEFAULT_PROVIDER,
		models: resolvedModels,
		scenarios: scenarios.map((s) => s.id),
		trialsPerTest: trials,
		results,
		summary: {
			total: results.length,
			passed: results.filter((r) => r.status === "pass").length,
			failed: results.filter((r) => r.status === "fail").length,
			flaky: results.filter((r) => r.status === "flaky").length,
			passAt1Overall: results.length > 0 ? results.reduce((sum, r) => sum + r.metrics.passAt1, 0) / results.length : 0,
			passAt3Overall: results.length > 0 ? results.reduce((sum, r) => sum + r.metrics.passAt3, 0) / results.length : 0,
		},
	}

	// Save report.json
	fs.writeFileSync(path.join(resultsDir, "report.json"), JSON.stringify(report, null, 2))

	// Generate summary.md for CI job summary
	const summaryMd = generateSummaryMarkdown(report)
	fs.writeFileSync(path.join(resultsDir, "summary.md"), summaryMd)

	// Create/update 'latest' symlink
	const latestLink = path.join(resultsBaseDir, "latest")
	try {
		if (fs.existsSync(latestLink)) {
			fs.unlinkSync(latestLink)
		}
		fs.symlinkSync(timestamp, latestLink)
	} catch {
		// Symlinks may fail on some systems, ignore
	}

	// Also write to custom output file if specified
	if (outputFile) {
		fs.writeFileSync(outputFile, JSON.stringify(report, null, 2))
		console.log(`\nResults also written to: ${outputFile}`)
	}

	// Summary
	console.log("\n" + "=".repeat(60))
	console.log("SUMMARY")
	console.log("=".repeat(60))
	console.log(`Total: ${report.summary.total}`)
	console.log(`Passed: ${report.summary.passed}`)
	console.log(`Failed: ${report.summary.failed}`)
	console.log(`Flaky: ${report.summary.flaky}`)
	const passLabel = report.trialsPerTest >= 3 ? "pass@3" : "pass@1"
	const passOverall = report.trialsPerTest >= 3 ? report.summary.passAt3Overall : report.summary.passAt1Overall
	console.log(`Overall ${passLabel}: ${(passOverall * 100).toFixed(1)}%`)
	console.log(`\nFull results: ${resultsDir}`)
	console.log(`Latest link: ${latestLink}`)

	// Exit with error if any failures
	if (report.summary.failed > 0) {
		process.exit(1)
	}
}

// Generate markdown summary for CI
function generateSummaryMarkdown(report: SmokeTestReport): string {
	const lines: string[] = []
	lines.push("## Smoke Test Results")
	lines.push("")
	lines.push(`**Date:** ${report.timestamp}`)

	// Show unique model IDs from results
	const modelIds = [...new Set(report.results.map((r) => r.modelId))]
	lines.push(`**Models:** ${modelIds.join(", ")}`)
	lines.push(`**Trials per test:** ${report.trialsPerTest}`)
	lines.push("")
	lines.push("### Summary")
	lines.push("")
	lines.push(`| Metric | Value |`)
	lines.push(`|--------|-------|`)
	lines.push(`| Total | ${report.summary.total} |`)
	lines.push(`| Passed | ${report.summary.passed} |`)
	lines.push(`| Failed | ${report.summary.failed} |`)
	lines.push(`| Flaky | ${report.summary.flaky} |`)
	const mdPassLabel = report.trialsPerTest >= 3 ? "pass@3" : "pass@1"
	const mdPassOverall = report.trialsPerTest >= 3 ? report.summary.passAt3Overall : report.summary.passAt1Overall
	lines.push(`| Overall ${mdPassLabel} | ${(mdPassOverall * 100).toFixed(1)}% |`)
	lines.push("")

	// Results table
	lines.push("### Results by Scenario")
	lines.push("")
	lines.push(`| Scenario | Model | Status | ${mdPassLabel} |`)
	lines.push("|----------|-------|--------|--------|")
	for (const r of report.results) {
		const statusEmoji = r.status === "pass" ? "✅" : r.status === "flaky" ? "⚠️" : "❌"
		const rPassMetric = report.trialsPerTest >= 3 ? r.metrics.passAt3 : r.metrics.passAt1
		lines.push(
			`| ${r.scenarioId} | ${r.modelId} | ${statusEmoji} ${r.status.toUpperCase()} | ${(rPassMetric * 100).toFixed(0)}% |`,
		)
	}
	lines.push("")

	// Failed/flaky details
	const problemResults = report.results.filter((r) => r.status !== "pass")
	if (problemResults.length > 0) {
		lines.push("### Failed/Flaky Details")
		lines.push("")
		for (const r of problemResults) {
			lines.push(`#### ${r.scenarioId} (${r.modelId})`)
			lines.push("")
			r.trials.forEach((t, i) => {
				if (!t.passed) {
					lines.push(`- Trial ${i + 1}: ${t.error}`)
				}
			})
			lines.push("")
		}
	}

	return lines.join("\n")
}

main().catch((err) => {
	console.error(err)
	process.exit(1)
})


--- evals/analysis/src/cli.ts ---
#!/usr/bin/env node

/**
 * Cline Analysis Framework CLI
 *
 * Commands:
 * - analyze: Parse Harbor job output and generate reports
 * - compare: Compare baseline vs current results for regression detection
 */

import chalk from "chalk"
import { Command } from "commander"
import * as fs from "fs"
import { HarborParser } from "./parsers"
import { JsonReporter, MarkdownReporter } from "./reporters"
import type { AnalysisOutputV1, ComparisonResult } from "./schemas"

const program = new Command()

program.name("cline-analysis").description("Analysis framework for Cline evaluations").version("1.0.0")

// Analyze command
program
	.command("analyze <job-dir>")
	.description("Parse Harbor job output and generate analysis report")
	.option("-f, --format <format>", "Output format: markdown, json, or minimal", "markdown")
	.option("-o, --output <file>", "Write report to file (default: stdout)")
	.option("--no-color", "Disable colored output")
	.action(async (jobDir: string, options: any) => {
		try {
			// Validate job directory
			if (!fs.existsSync(jobDir)) {
				console.error(chalk.red(`Error: Job directory not found: ${jobDir}`))
				process.exit(1)
			}

			console.error(chalk.blue(`Analyzing Harbor job: ${jobDir}`))

			// Parse Harbor output
			const parser = new HarborParser()
			const analysis = parser.parseJob(jobDir)

			// Generate report
			let report: string

			if (options.format === "json") {
				const jsonReporter = new JsonReporter()
				report = jsonReporter.generate(analysis, true)
			} else if (options.format === "minimal") {
				const jsonReporter = new JsonReporter()
				report = jsonReporter.generateMinimal(analysis)
			} else {
				const markdownReporter = new MarkdownReporter()
				report = markdownReporter.generate(analysis, options.color)
			}

			// Output report
			if (options.output) {
				fs.writeFileSync(options.output, report)
				console.error(chalk.green(`✓ Report written to: ${options.output}`))

				// Also write full JSON for future reference
				if (options.format === "markdown") {
					const jsonPath = options.output.replace(/\.md$/, ".json")
					const jsonReporter = new JsonReporter()
					fs.writeFileSync(jsonPath, jsonReporter.generate(analysis, true))
					console.error(chalk.gray(`  (Full JSON saved to: ${jsonPath})`))
				}
			} else {
				console.log(report)
			}

			// Summary on stderr
			const markdownReporter = new MarkdownReporter()
			const summary = markdownReporter.generateCompactSummary(analysis)
			console.error("\n" + chalk.bold("Summary:"))
			console.error(summary)
		} catch (error) {
			console.error(chalk.red("Error during analysis:"))
			console.error(error)
			process.exit(1)
		}
	})

// Compare command
program
	.command("compare <baseline> <current>")
	.description("Compare baseline and current analysis results")
	.option("-t, --threshold <number>", "Regression threshold (percentage points)", "10")
	.option("--no-color", "Disable colored output")
	.action(async (baselinePath: string, currentPath: string, options: any) => {
		try {
			// Load both analysis outputs
			if (!fs.existsSync(baselinePath)) {
				console.error(chalk.red(`Error: Baseline file not found: ${baselinePath}`))
				process.exit(1)
			}

			if (!fs.existsSync(currentPath)) {
				console.error(chalk.red(`Error: Current file not found: ${currentPath}`))
				process.exit(1)
			}

			const baseline: AnalysisOutputV1 = JSON.parse(fs.readFileSync(baselinePath, "utf-8"))
			const current: AnalysisOutputV1 = JSON.parse(fs.readFileSync(currentPath, "utf-8"))

			const threshold = parseFloat(options.threshold)

			// Compare results
			const comparison = compareAnalyses(baseline, current, threshold)

			// Display comparison
			displayComparison(comparison, options.color)

			// Exit with error if regression detected
			if (comparison.regression_detected) {
				console.error(chalk.red("\n✗ Regression detected! See details above."))
				process.exit(1)
			} else {
				console.error(chalk.green("\n✓ No significant regression detected."))
				process.exit(0)
			}
		} catch (error) {
			console.error(chalk.red("Error during comparison:"))
			console.error(error)
			process.exit(1)
		}
	})

program.parse()

/**
 * Compare two analysis outputs for regression detection
 */
function compareAnalyses(baseline: AnalysisOutputV1, current: AnalysisOutputV1, threshold: number): ComparisonResult {
	const delta = {
		pass_at_1: (current.summary.pass_at_1 - baseline.summary.pass_at_1) * 100,
		pass_at_3: (current.summary.pass_at_3 - baseline.summary.pass_at_3) * 100,
		pass_caret_3: (current.summary.pass_caret_3 - baseline.summary.pass_caret_3) * 100,
		cost_usd: current.summary.total_cost_usd - baseline.summary.total_cost_usd,
		duration_sec: current.summary.total_duration_sec - baseline.summary.total_duration_sec,
	}

	// Detect regression (drop in pass rates exceeding threshold)
	const regression_detected = delta.pass_at_1 < -threshold || delta.pass_at_3 < -threshold

	// Find tasks that regressed or improved
	const tasks_regressed: string[] = []
	const tasks_improved: string[] = []

	const baselineTaskMap = new Map(baseline.tasks.map((t) => [t.task_id, t]))

	for (const currentTask of current.tasks) {
		const baselineTask = baselineTaskMap.get(currentTask.task_id)
		if (!baselineTask) {
			continue
		}

		const taskDelta = (currentTask.metrics.pass_at_3 - baselineTask.metrics.pass_at_3) * 100

		if (taskDelta < -threshold) {
			tasks_regressed.push(currentTask.task_name)
		} else if (taskDelta > threshold) {
			tasks_improved.push(currentTask.task_name)
		}
	}

	return {
		baseline: baseline.summary,
		current: current.summary,
		delta,
		regression_detected,
		tasks_regressed,
		tasks_improved,
	}
}

/**
 * Display comparison results with color coding
 */
function displayComparison(comparison: ComparisonResult, useColor: boolean): void {
	const separator = "━".repeat(79)

	console.log(useColor ? chalk.bold(separator) : separator)
	console.log(useColor ? chalk.bold.cyan("Baseline vs Current Comparison") : "Baseline vs Current Comparison")
	console.log(useColor ? chalk.bold(separator) : separator)
	console.log("")

	// Pass rate changes
	console.log(useColor ? chalk.bold("Pass Rate Changes:") : "Pass Rate Changes:")
	console.log(`  pass@1: ${formatDelta(comparison.delta.pass_at_1, useColor)} percentage points`)
	console.log(`  pass@3: ${formatDelta(comparison.delta.pass_at_3, useColor)} percentage points`)
	console.log(`  pass^3: ${formatDelta(comparison.delta.pass_caret_3, useColor)} percentage points`)
	console.log("")

	// Cost and duration changes
	console.log(useColor ? chalk.bold("Resource Changes:") : "Resource Changes:")
	console.log(`  Cost: ${formatDelta(comparison.delta.cost_usd, useColor, true)} USD`)
	console.log(`  Duration: ${formatDelta(comparison.delta.duration_sec, useColor)} seconds`)
	console.log("")

	// Tasks regressed
	if (comparison.tasks_regressed.length > 0) {
		console.log(useColor ? chalk.bold.red("Tasks Regressed:") : "Tasks Regressed:")
		for (const task of comparison.tasks_regressed) {
			console.log(`  • ${task}`)
		}
		console.log("")
	}

	// Tasks improved
	if (comparison.tasks_improved.length > 0) {
		console.log(useColor ? chalk.bold.green("Tasks Improved:") : "Tasks Improved:")
		for (const task of comparison.tasks_improved) {
			console.log(`  • ${task}`)
		}
		console.log("")
	}
}

/**
 * Format delta value with color coding
 */
function formatDelta(value: number, useColor: boolean, invertSign = false): string {
	const sign = invertSign ? -Math.sign(value) : Math.sign(value)
	const absValue = Math.abs(value).toFixed(2)
	const signStr = sign > 0 ? "+" : sign < 0 ? "-" : " "

	if (!useColor) {
		return `${signStr}${absValue}`
	}

	if (sign > 0) {
		return chalk.green(`${signStr}${absValue}`)
	}
	if (sign < 0) {
		return chalk.red(`${signStr}${absValue}`)
	}
	return chalk.gray(`${signStr}${absValue}`)
}


--- evals/analysis/src/metrics.ts ---
/**
 * Metrics calculation for nondeterministic AI testing
 *
 * Implements:
 * - pass@k: P(at least 1 of k trials passes) - solution finding capability
 * - pass^k: P(all k trials pass) - reliability measure
 * - Flakiness score: Entropy-based variance measurement
 *
 * References:
 * - HumanEval paper: https://arxiv.org/abs/2107.03374
 * - pass@k methodology: https://github.com/openai/human-eval
 */

export class MetricsCalculator {
	/**
	 * Calculate pass@k: Probability that at least 1 of k trials succeeds
	 *
	 * Formula: 1 - C(n-c, k) / C(n, k)
	 * where n = total trials, c = number of passes, k = sample size
	 *
	 * Interpretation: "Can this model solve the problem?"
	 *
	 * @param trials Array of boolean trial results (true = pass, false = fail)
	 * @param k Number of trials to sample
	 * @returns Probability [0, 1]
	 */
	passAtK(trials: boolean[], k: number): number {
		const n = trials.length
		const c = trials.filter(Boolean).length

		if (n < k) {
			throw new Error(`Cannot calculate pass@${k} with only ${n} trials`)
		}

		// If we have at least k passes, probability is 100%
		if (c >= k) {
			return 1.0
		}

		// Calculate: 1 - C(n-c, k) / C(n, k)
		const numerator = this.binomial(n - c, k)
		const denominator = this.binomial(n, k)

		return 1 - numerator / denominator
	}

	/**
	 * Calculate pass^k: Probability that ALL k trials succeed
	 *
	 * Formula: C(c, k) / C(n, k)
	 * where n = total trials, c = number of passes, k = sample size
	 *
	 * Interpretation: "Can I rely on this model?" (reliability metric)
	 *
	 * @param trials Array of boolean trial results
	 * @param k Number of trials that must all pass
	 * @returns Probability [0, 1]
	 */
	passCaretK(trials: boolean[], k: number): number {
		const n = trials.length
		const c = trials.filter(Boolean).length

		if (n < k) {
			throw new Error(`Cannot calculate pass^${k} with only ${n} trials`)
		}

		// If we have fewer than k passes, probability is 0%
		if (c < k) {
			return 0.0
		}

		// Calculate: C(c, k) / C(n, k)
		const numerator = this.binomial(c, k)
		const denominator = this.binomial(n, k)

		return numerator / denominator
	}

	/**
	 * Calculate flakiness score: Entropy-based measure of variance
	 *
	 * Formula: -p*log2(p) - (1-p)*log2(1-p)
	 * where p = pass rate
	 *
	 * Returns:
	 * - 0.0: No variance (all pass or all fail)
	 * - 1.0: Maximum variance (50% pass rate)
	 *
	 * Interpretation: How unpredictable/inconsistent is this task?
	 *
	 * @param trials Array of boolean trial results
	 * @returns Flakiness score [0, 1]
	 */
	flakinessScore(trials: boolean[]): number {
		const passRate = trials.filter(Boolean).length / trials.length

		// No variance if all pass or all fail
		if (passRate === 0 || passRate === 1) {
			return 0
		}

		// Binary entropy
		const entropy = -passRate * Math.log2(passRate) - (1 - passRate) * Math.log2(1 - passRate)

		return entropy // Already in [0, 1] range
	}

	/**
	 * Binomial coefficient C(n, k) = n! / (k! * (n-k)!)
	 *
	 * Uses iterative calculation to avoid factorial overflow
	 *
	 * @param n Total items
	 * @param k Items to choose
	 * @returns Number of ways to choose k items from n
	 */
	private binomial(n: number, k: number): number {
		if (k > n) {
			return 0
		}
		if (k === 0 || k === n) {
			return 1
		}

		// Optimize by using smaller k
		if (k > n - k) {
			k = n - k
		}

		let result = 1
		for (let i = 1; i <= k; i++) {
			result *= n - i + 1
			result /= i
		}

		return result
	}

	/**
	 * Calculate all metrics for a task's trials
	 *
	 * @param trials Array of boolean trial results
	 * @returns Object with pass@1, pass@3, pass^3, and flakiness scores
	 */
	calculateTaskMetrics(trials: boolean[]): {
		passAt1: number
		passAt3: number
		passCaret3: number
		flakinessScore: number
	} {
		if (trials.length === 0) {
			throw new Error("Cannot calculate metrics with no trials")
		}

		// Calculate pass@k and pass^k for available trials
		const passAt1 = trials.length >= 1 ? this.passAtK(trials, 1) : 0
		const passAt3 = trials.length >= 3 ? this.passAtK(trials, 3) : 0
		const passCaret3 = trials.length >= 3 ? this.passCaretK(trials, 3) : 0

		return {
			passAt1,
			passAt3,
			passCaret3,
			flakinessScore: this.flakinessScore(trials),
		}
	}

	/**
	 * Determine task status based on trial results
	 *
	 * @param trials Array of boolean trial results
	 * @returns "pass" | "fail" | "flaky"
	 */
	getTaskStatus(trials: boolean[]): "pass" | "fail" | "flaky" {
		const passCount = trials.filter(Boolean).length
		const totalCount = trials.length

		if (passCount === totalCount) {
			return "pass"
		}
		if (passCount === 0) {
			return "fail"
		}
		return "flaky"
	}
}


--- evals/analysis/src/schemas/analysis-output.ts ---
/**
 * Type definitions for Cline Analysis Framework output
 * These schemas define the structured JSON output from our analysis tools
 */

/**
 * Versioned analysis output schema (V1)
 * Breaking changes should increment the version number
 */
export interface AnalysisOutputV1 {
	schema_version: "1.0"
	metadata: AnalysisMetadata
	summary: AnalysisSummary
	tasks: TaskResultV1[]
	failures: FailureAnalysis
}

export interface AnalysisMetadata {
	generated_at: string // ISO 8601 timestamp
	analysis_version: string // Package version (from package.json)
	job_id: string // Job directory name (e.g., "2025-01-25__10-30-00")
	model: string // Model used for the run
	agent: string // Agent name (e.g., "cline-cli")
	environment: string // "docker" | "daytona"
}

export interface AnalysisSummary {
	total_tasks: number
	total_trials: number
	pass_at_1: number // Probability at least 1 of k=1 succeeds
	pass_at_3: number // Probability at least 1 of k=3 succeeds
	pass_caret_3: number // Probability ALL k=3 succeed (reliability)
	total_cost_usd: number
	total_duration_sec: number
	flaky_task_count: number // Tasks with variance across trials
}

export interface TaskResultV1 {
	task_id: string
	task_name: string
	trials: TrialResultV1[]
	metrics: TaskMetrics
	status: "pass" | "fail" | "flaky"
	total_cost_usd: number
	avg_duration_sec: number
}

export interface TrialResultV1 {
	trial_index: number // 0-indexed trial number
	trial_hash: string // Hash from trial directory name
	passed: boolean
	duration_sec: number
	cost_usd: number
	tokens_in?: number
	tokens_out?: number
	failures: FailureInfo[] // Classified failure patterns
}

export interface TaskMetrics {
	pass_at_1: number // P(at least 1 of 1 succeeds)
	pass_at_3: number // P(at least 1 of 3 succeeds)
	pass_caret_3: number // P(all 3 succeed) - reliability metric
	flakiness_score: number // Entropy-based variance measure (0-1)
}

export interface FailureInfo {
	name: string // Pattern name (e.g., "gemini_signature")
	category: FailureCategory
	excerpt: string // Log excerpt showing the failure
	issue_url?: string // GitHub issue link if applicable
}

export type FailureCategory =
	| "provider_bug" // Cline integration bugs (Gemini #7974, Claude #7998)
	| "transient" // Rate limits, network timeouts, service unavailable
	| "harness" // Test harness or verification script failure
	| "environment" // Docker/Daytona setup failure
	| "policy" // Model safety/content policy refusal
	| "auth" // Invalid API credentials
	| "task_failure" // Model couldn't solve the task

export interface FailureAnalysis {
	by_category: Record<FailureCategory, number>
	by_pattern: FailurePatternSummary[]
}

export interface FailurePatternSummary {
	name: string
	count: number
	issue_url?: string
	examples: FailureExample[]
}

export interface FailureExample {
	task_id: string
	trial_index: number
	excerpt: string
}

/**
 * Tool Precision Test Result (for replace_in_file benchmarks)
 */
export interface ToolPrecisionResult {
	schema_version: "1.0"
	benchmark: "tool-precision/replace-in-file"
	total_cases: number
	passed: number
	failed: number
	pass_rate: number
	avg_latency_ms: number
	known_failures: string[]
	timestamp: string
}

/**
 * Coding Exercises Result (for small task benchmarks)
 */
export interface CodingExercisesResult {
	schema_version: "1.0"
	benchmark: "coding-exercises"
	total_exercises: number
	passed: number
	failed: number
	pass_at_1: number
	avg_duration_sec: number
	known_failures: string[]
	timestamp: string
}

/**
 * Comparison result between two analysis outputs
 */
export interface ComparisonResult {
	baseline: AnalysisSummary
	current: AnalysisSummary
	delta: {
		pass_at_1: number // Percentage point change
		pass_at_3: number
		pass_caret_3: number
		cost_usd: number
		duration_sec: number
	}
	regression_detected: boolean
	tasks_regressed: string[] // Task IDs that got worse
	tasks_improved: string[] // Task IDs that got better
}


--- evals/analysis/src/parsers/harbor.ts ---
/**
 * Parser for Harbor framework job output
 *
 * Parses jobs/ directory structure created by Harbor to extract:
 * - Trial results (pass/fail, duration, cost, tokens)
 * - Task groupings and metrics
 * - Failure classifications
 */

import * as fs from "fs"
import * as path from "path"
import { FailureClassifier } from "../classifier"
import { MetricsCalculator } from "../metrics"
import type {
	AnalysisMetadata,
	AnalysisOutputV1,
	AnalysisSummary,
	FailureAnalysis,
	TaskResultV1,
	TrialResultV1,
} from "../schemas"

export interface HarborParserOptions {
	patternsPath?: string
}

export class HarborParser {
	private classifier: FailureClassifier
	private metrics: MetricsCalculator

	constructor(options: HarborParserOptions = {}) {
		this.classifier = new FailureClassifier(options.patternsPath)
		this.metrics = new MetricsCalculator()
	}

	/**
	 * Parse a complete Harbor job directory
	 *
	 * @param jobDir Path to job directory (e.g., jobs/2025-01-25__10-30-00/)
	 * @returns Structured analysis output with schema version 1.0
	 */
	parseJob(jobDir: string): AnalysisOutputV1 {
		const configPath = path.join(jobDir, "config.json")
		const resultPath = path.join(jobDir, "result.json")

		if (!fs.existsSync(configPath) || !fs.existsSync(resultPath)) {
			throw new Error(`Invalid Harbor job directory: ${jobDir}`)
		}

		const config = JSON.parse(fs.readFileSync(configPath, "utf-8"))
		const result = JSON.parse(fs.readFileSync(resultPath, "utf-8"))

		// Find all trial directories
		const trialDirs = this.findTrialDirectories(jobDir)
		const trials = trialDirs.map((dir) => this.parseTrialDirectory(dir))

		// Group trials by task ID
		const taskResults = this.groupTrialsByTask(trials)

		// Calculate aggregate metrics
		const summary = this.calculateSummary(taskResults)

		// Analyze failures
		const failures = this.analyzeFailures(taskResults)

		const metadata: AnalysisMetadata = {
			generated_at: new Date().toISOString(),
			analysis_version: "1.0.0", // TODO: Get from package.json
			job_id: path.basename(jobDir),
			model: config.model,
			agent: config.agent || "cline-cli",
			environment: config.environment || "docker",
		}

		return {
			schema_version: "1.0",
			metadata,
			summary,
			tasks: taskResults,
			failures,
		}
	}

	/**
	 * Find all trial directories in a job
	 */
	private findTrialDirectories(jobDir: string): string[] {
		const entries = fs.readdirSync(jobDir, { withFileTypes: true })

		return entries
			.filter((entry) => entry.isDirectory())
			.filter((entry) => {
				// Trial dirs have format: 01k7a12s...disco__fhSEuhr
				const configExists = fs.existsSync(path.join(jobDir, entry.name, "config.json"))
				return configExists
			})
			.map((entry) => path.join(jobDir, entry.name))
	}

	/**
	 * Parse a single trial directory
	 */
	private parseTrialDirectory(trialDir: string): ParsedTrial {
		const configPath = path.join(trialDir, "config.json")
		const resultPath = path.join(trialDir, "result.json")
		const rewardPath = path.join(trialDir, "verifier", "reward.txt")
		const logsPath = path.join(trialDir, "agent", "cline.txt")
		const testOutputPath = path.join(trialDir, "verifier", "test-stdout.txt")

		const config = JSON.parse(fs.readFileSync(configPath, "utf-8"))
		const result = JSON.parse(fs.readFileSync(resultPath, "utf-8"))
		const reward = fs.readFileSync(rewardPath, "utf-8").trim()
		const logs = fs.existsSync(logsPath) ? fs.readFileSync(logsPath, "utf-8") : ""
		const testOutput = fs.existsSync(testOutputPath) ? fs.readFileSync(testOutputPath, "utf-8") : ""

		const passed = reward === "1"
		const failures = passed ? [] : this.classifier.classify(logs)

		return {
			taskId: config.task_id,
			trialHash: path.basename(trialDir).split("__")[1] || "",
			passed,
			duration: result.duration_sec || 0,
			cost: result.cost_usd || 0,
			tokensIn: result.tokens_in,
			tokensOut: result.tokens_out,
			logs,
			testOutput,
			failures,
		}
	}

	/**
	 * Group trials by task ID and calculate metrics
	 */
	private groupTrialsByTask(trials: ParsedTrial[]): TaskResultV1[] {
		const taskMap = new Map<string, ParsedTrial[]>()

		// Group trials by task ID
		for (const trial of trials) {
			const existing = taskMap.get(trial.taskId) || []
			existing.push(trial)
			taskMap.set(trial.taskId, existing)
		}

		// Convert to TaskResultV1 format
		const taskResults: TaskResultV1[] = []

		for (const [taskId, taskTrials] of taskMap.entries()) {
			const trialResults: TrialResultV1[] = taskTrials.map((trial, index) => ({
				trial_index: index,
				trial_hash: trial.trialHash,
				passed: trial.passed,
				duration_sec: trial.duration,
				cost_usd: trial.cost,
				tokens_in: trial.tokensIn,
				tokens_out: trial.tokensOut,
				failures: trial.failures,
			}))

			const passResults = taskTrials.map((t) => t.passed)
			const metrics = this.metrics.calculateTaskMetrics(passResults)
			const status = this.metrics.getTaskStatus(passResults)

			const totalCost = taskTrials.reduce((sum, t) => sum + t.cost, 0)
			const avgDuration = taskTrials.reduce((sum, t) => sum + t.duration, 0) / taskTrials.length

			// Extract readable task name from ID
			const taskName = this.extractTaskName(taskId)

			taskResults.push({
				task_id: taskId,
				task_name: taskName,
				trials: trialResults,
				metrics,
				status,
				total_cost_usd: totalCost,
				avg_duration_sec: avgDuration,
			})
		}

		return taskResults.sort((a, b) => a.task_name.localeCompare(b.task_name))
	}

	/**
	 * Extract human-readable task name from task ID
	 * Example: 01k7a12sd1nk15j08e6x0x7v9e-discord-trivia-approval-keyerror → discord-trivia
	 */
	private extractTaskName(taskId: string): string {
		const parts = taskId.split("-")
		if (parts.length > 1) {
			// Remove the ID prefix and get first 2-3 meaningful words
			const words = parts.slice(1, 4)
			return words.join("-")
		}
		return taskId
	}

	/**
	 * Calculate aggregate summary metrics
	 */
	private calculateSummary(taskResults: TaskResultV1[]): AnalysisSummary {
		const totalTasks = taskResults.length
		const totalTrials = taskResults.reduce((sum, task) => sum + task.trials.length, 0)

		// Calculate overall pass@k metrics
		const allTrials = taskResults.flatMap((task) => task.trials.map((t) => t.passed))
		let passAt1 = 0
		let passAt3 = 0
		let passCaret3 = 0

		if (allTrials.length >= 1) {
			passAt1 = this.metrics.passAtK(allTrials, 1)
		}
		if (allTrials.length >= 3) {
			passAt3 = this.metrics.passAtK(allTrials, 3)
			passCaret3 = this.metrics.passCaretK(allTrials, 3)
		}

		const totalCost = taskResults.reduce((sum, task) => sum + task.total_cost_usd, 0)
		const totalDuration = taskResults.reduce((sum, task) => sum + task.avg_duration_sec * task.trials.length, 0)

		const flakyTaskCount = taskResults.filter((task) => task.status === "flaky").length

		return {
			total_tasks: totalTasks,
			total_trials: totalTrials,
			pass_at_1: passAt1,
			pass_at_3: passAt3,
			pass_caret_3: passCaret3,
			total_cost_usd: totalCost,
			total_duration_sec: totalDuration,
			flaky_task_count: flakyTaskCount,
		}
	}

	/**
	 * Analyze failure patterns across all tasks
	 */
	private analyzeFailures(taskResults: TaskResultV1[]): FailureAnalysis {
		const categoryCount = new Map<string, number>()
		const patternCount = new Map<string, { count: number; issue_url?: string; examples: any[] }>()

		for (const task of taskResults) {
			for (const trial of task.trials) {
				if (!trial.passed) {
					for (const failure of trial.failures) {
						// Count by category
						categoryCount.set(failure.category, (categoryCount.get(failure.category) || 0) + 1)

						// Count by pattern
						const existing = patternCount.get(failure.name) || {
							count: 0,
							issue_url: failure.issue_url,
							examples: [],
						}
						existing.count++

						// Add example if not too many
						if (existing.examples.length < 3) {
							existing.examples.push({
								task_id: task.task_id,
								trial_index: trial.trial_index,
								excerpt: failure.excerpt,
							})
						}

						patternCount.set(failure.name, existing)
					}
				}
			}
		}

		const byCategory: Record<string, number> = {}
		for (const [category, count] of categoryCount.entries()) {
			byCategory[category] = count
		}

		const byPattern = Array.from(patternCount.entries()).map(([name, data]) => ({
			name,
			count: data.count,
			issue_url: data.issue_url,
			examples: data.examples,
		}))

		return { by_category: byCategory as any, by_pattern: byPattern }
	}
}

interface ParsedTrial {
	taskId: string
	trialHash: string
	passed: boolean
	duration: number
	cost: number
	tokensIn?: number
	tokensOut?: number
	logs: string
	testOutput: string
	failures: any[]
}


--- evals/analysis/src/schemas/harbor-output.ts ---
/**
 * Type definitions for Harbor framework output structure
 * These schemas document Harbor's jobs/ directory format (read-only, we validate against this)
 *
 * Harbor is the execution framework used by cline-bench.
 * See: https://harborframework.com
 */

export interface HarborTrialConfig {
	task_id: string
	model: string
	agent: string
	environment?: string
	retries?: number
}

export interface HarborTrialResult {
	reward: 0 | 1 // Binary pass/fail from verifier
	duration_sec: number
	cost_usd: number
	tokens_in?: number
	tokens_out?: number
	timestamp?: string
}

export interface HarborTrialFiles {
	agent: {
		"cline.txt": string // Full conversation log
		"setup/stdout.txt": string
		"setup/stderr.txt": string
		"setup/return-code.txt": string
		[key: string]: string // command-N/ directories with stdout, stderr, return-code.txt
	}
	verifier: {
		"reward.txt": "0" | "1"
		"test-stdout.txt": string
		"test-stderr.txt": string
	}
}

/**
 * Structure of a single trial directory
 * Example: jobs/2025-01-25__10-30-00/01k7a12s...disco__fhSEuhr/
 */
export interface HarborTrialDirectory {
	"config.json": HarborTrialConfig
	"result.json": HarborTrialResult
	agent: HarborTrialFiles["agent"]
	verifier: HarborTrialFiles["verifier"]
}

/**
 * Structure of a job's config.json
 */
export interface HarborJobConfig {
	model: string
	agent: string
	tasks: string[] // Task IDs
	trials_per_task: number
	environment: string // "docker" | "daytona"
	created_at?: string
}

/**
 * Structure of a job's result.json (aggregate)
 */
export interface HarborJobResult {
	total_tasks: number
	passed_tasks: number
	failed_tasks: number
	total_cost_usd: number
	total_duration_sec: number
	started_at?: string
	completed_at?: string
}

/**
 * Complete job directory structure
 * Example: jobs/2025-01-25__10-30-00/
 */
export interface HarborJobDirectory {
	"config.json": HarborJobConfig
	"result.json": HarborJobResult
	trials: Record<string, HarborTrialDirectory> // trial-hash -> trial data
}

/**
 * Parsed trial result from Harbor output
 */
export interface ParsedHarborTrial {
	taskId: string
	trialHash: string
	passed: boolean
	duration: number
	cost: number
	tokensIn?: number
	tokensOut?: number
	logs: string // Full cline.txt content
	testOutput: string // Test verification output
	errors: string[] // Setup or command errors
}

/**
 * Utility type for extracting trial directory paths
 */
export interface HarborTrialPath {
	jobDir: string
	trialDir: string
	taskId: string
	trialHash: string
}


--- locales/ar-sa/README.md ---
<div align="center"><sub>
العربية | <a href="https://github.com/cline/cline/blob/main/locales/es/README.md" target="_blank">الإسبانية</a> | <a href="https://github.com/cline/cline/blob/main/locales/de/README.md" target="_blank">الألمانية</a> | <a href="https://github.com/cline/cline/blob/main/locales/ja/README.md" target="_blank">اليابانية</a> | <a href="https://github.com/cline/cline/blob/main/locales/zh-cn/README.md" target="_blank">الصينية المبسطة</a> | <a href="https://github.com/cline/cline/blob/main/locales/zh-tw/README.md" target="_blank">الصينية التقليدية</a> | <a href="https://github.com/cline/cline/blob/main/locales/pt-BR/README.md" target="_blank">البرتغالية</a>
</sub></div>

# Cline

<p align="center">
  <img src="https://media.githubusercontent.com/media/cline/cline/main/assets/docs/demo.gif" width="100%" />
</p>

<div align="center">
<table>
<tbody>
<td align="center">
<a href="https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev" target="_blank"><strong>تنزيل من متجر VS</strong></a>
</td>
<td align="center">
<a href="https://discord.gg/cline" target="_blank"><strong>Discord</strong></a>
</td>
<td align="center">
<a href="https://www.reddit.com/r/cline/" target="_blank"><strong>r/cline</strong></a>
</td>
<td align="center">
<a href="https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop" target="_blank"><strong>طلبات الميزات</strong></a>
</td>
<td align="center">
<a href="https://docs.cline.bot/getting-started/getting-started-new-coders" target="_blank"><strong>البدء</strong></a>
</td>
</tbody>
</table>
</div>

التقى Cline، مساعد الذكاء الاصطناعي الذي يمكنه استخدام **سطر الأوامر** و **محرر النصوص** الخاص بك.

بفضل [قدرات Claude 4 Sonnet على التعليمات البرمجية الوكيلة](https://www.anthropic.com/claude/sonnet)، يمكن لـ Cline التعامل مع مهام تطوير البرامج المعقدة خطوة بخطوة. مع الأدوات التي تسمح له بإنشاء وتعديل الملفات، واستكشاف المشاريع الكبيرة، واستخدام المتصفح، وتنفيذ أوامر الطرفية (بعد منحك الإذن)، يمكنه مساعدتك بطرق تتجاوز إكمال الكود أو الدعم الفني. يمكن لـ Cline أيضًا استخدام بروتوكول سياق النموذج (MCP) لإنشاء أدوات جديدة وتوسيع قدراته الخاصة. في حين تعمل النصوص البرمجية الآلية المستقلة تقليديًا في بيئات محاصرة، توفر هذه الإضافة واجهة رسومية لموافقة المستخدم على كل تغيير في الملف وأمر طرفية، مما يوفر طريقة آمنة وسهلة الاستخدام لاستكشاف إمكانات الذكاء الاصطناعي الوكيل.

1. أدخل مهمتك وأضف الصور لتحويل المحاكاة إلى تطبيقات وظيفية أو إصلاح الأخطاء مع لقطات الشاشة.
2. يبدأ Cline بتحليل هيكل الملفات الخاصة بك وشجرة التعريف المصدرية، وإجراء عمليات بحث regex، وقراءة الملفات ذات الصلة للاطلاع على المشاريع الحالية. من خلال إدارة المعلومات التي يتم إضافتها إلى السياق بعناية، يمكن لـ Cline تقديم مساعدة قيمة حتى للمشاريع الكبيرة والمعقدة دون إرهاق نافذة السياق.
3. بمجرد حصول Cline على المعلومات التي يحتاجها، يمكنه:
    - إنشاء وتعديل الملفات + مراقبة أخطاء Linter/Compiler أثناء السير، مما يسمح له بإصلاح المشكلات مثل الواردات المفقودة وأخطاء البناء النحوي بمفرده.
    - تنفيذ الأوامر مباشرة في الطرفية الخاصة بك ومراقبة إخراجها أثناء العمل، مما يسمح له على سبيل المثال بالاستجابة لمشكلات خادم التطوير بعد تعديل ملف.
    - بالنسبة لمهام تطوير الويب، يمكن لـ Cline إطلاق الموقع في متصفح بلا رأس، والنقر، وكتابة النص، والتمرير، والتقاط لقطات الشاشة + سجلات وحدة التحكم، مما يسمح له بإصلاح أخطاء وقت التشغيل والأخطاء البصرية.
4. عند اكتمال المهمة، سيقدم Cline النتيجة لك مع أمر طرفية مثل `open -a "Google Chrome" index.html`، والذي تقوم بتشغيله بنقرة زر.

> [!TIP]
> استخدم اختصار `CMD/CTRL + Shift + P` لفتح لوحة الأوامر واكتب "Cline: Open In New Tab" لفتح الإضافة كعلامة تبويب في محرر النصوص الخاص بك. يتيح لك هذا استخدام Cline جنبًا إلى جنب مع مستكشف الملفات الخاص بك، ورؤية كيف يغير مساحة العمل الخاصة بك بوضوح أكبر.

---

<img align="right" width="340" src="https://github.com/user-attachments/assets/3cf21e04-7ce9-4d22-a7b9-ba2c595e88a4">

### استخدم أي واجهة برمجة تطبيقات ونموذج

يدعم Cline مقدمي واجهات برمجة التطبيقات مثل OpenRouter و Anthropic و OpenAI و Google Gemini و AWS Bedrock و Azure و GCP Vertex. يمكنك أيضًا تكوين أي واجهة برمجة تطبيقات متوافقة مع OpenAI، أو استخدام نموذج محلي من خلال LM Studio/Ollama. إذا كنت تستخدم OpenRouter، فستقوم الإضافة بجلب قائمة النماذج الأحدث الخاصة بهم، مما يسمح لك باستخدام أحدث النماذج بمجرد توفرها.

تتتبع الإضافة أيضًا إجمالي الرموز والاستخدام الخاص بواجهة برمجة التطبيقات لدورة المهمة بأكملها وطلبات فردية، مما يبقيك على اطلاع بالإنفاق في كل خطوة.

<!-- Transparent pixel to create line break after floating image -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="370" src="https://github.com/user-attachments/assets/81be79a8-1fdb-4028-9129-5fe055e01e76">

### تشغيل الأوامر في الطرفية

بفضل [تحديثات تكامل الشل الجديدة في VSCode v1.93](https://code.visualstudio.com/updates/v1_93#_terminal-shell-integration-api)، يمكن لـ Cline تنفيذ الأوامر مباشرة في الطرفية الخاصة بك وتلقي الإخراج. يسمح له هذا بأداء مجموعة واسعة من المهام، من تثبيت الحزم وتشغيل سكربتات البناء إلى نشر التطبيقات، وإدارة قواعد البيانات، وتنفيذ الاختبارات، وذلك بالتكيف مع بيئة التطوير الخاصة بك وسلسلة الأدوات للقيام بالعمل على النحو الصحيح.

بالنسبة للعمليات الطويلة المدى مثل خوادم التطوير، استخدم زر "المتابعة أثناء التشغيل" للسماح لـ Cline بالاستمرار في المهمة بينما يعمل الأمر في الخلفية. أثناء عمل Cline، سيتم إخباره بأي إخراج طرفية جديد على الطريق، مما يسمح له بالاستجابة للمشكلات التي قد تنشأ، مثل أخطاء وقت الإنشاء عند تعديل الملفات.

<!-- Transparent pixel to create line break after floating image -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="400" src="https://github.com/user-attachments/assets/c5977833-d9b8-491e-90f9-05f9cd38c588">

### إنشاء وتعديل الملفات

يمكن لـ Cline إنشاء وتعديل الملفات مباشرة في محرر النصوص الخاص بك، وعرض الاختلافات. يمكنك تعديل أو إلغاء تغييرات Cline مباشرة في محرر الاختلافات، أو تقديم ملاحظات في الدردشة حتى تكون راضيًا عن النتيجة. يراقب Cline أيضًا أخطاء Linter/Compiler (الواردات المفقودة، أخطاء البناء النحوي، إلخ) حتى يتمكن من إصلاح المشكلات التي تنشأ أثناء السير بمفرده.

يتم تسجيل جميع التغييرات التي أجراها Cline في جدول زمني للملف، مما يوفر طريقة سهلة لتتبع وإلغاء التعديلات إذا لزم الأمر.

<!-- Transparent pixel to create line break after floating image -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="370" src="https://github.com/user-attachments/assets/bc2e85ba-dfeb-4fe6-9942-7cfc4703cbe5">

### استخدم المتصفح

مع قدرة [استخدام الكمبيوتر](https://www.anthropic.com/news/3-5-models-and-computer-use) الجديدة لـ Claude 4 Sonnet، يمكن لـ Cline إطلاق متصفح، والنقر على العناصر، وكتابة النص، والتمرير، والتقاط لقطات الشاشة وسجلات وحدة التحكم في كل خطوة. يسمح له هذا بالتصحيح التفاعلي، واختبار نهاية إلى نهاية، وحتى الاستخدام العام للويب! يمنحه هذا الاستقلالية لإصلاح الأخطاء البصرية وأخطاء وقت التشغيل دون الحاجة إلى نسخ ولصق سجلات الأخطاء بنفسك.

حاول طلب من Cline "اختبار التطبيق"، وشاهده يشغل أمرًا مثل `npm run dev`، ويطلق خادم التطوير المحلي في متصفح، ويجري سلسلة من الاختبارات للتأكد من أن كل شيء يعمل. [شاهد عرضًا توضيحيًا هنا.](https://x.com/sdrzn/status/1850880547825823989)

<!-- Transparent pixel to create line break after floating image -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="350" src="https://github.com/user-attachments/assets/ac0efa14-5c1f-4c26-a42d-9d7c56f5fadd">

### "إضافة أداة التي..."

شكراً لـ [بروتوكول سياق النموذج](https://github.com/modelcontextprotocol)، يمكن لـ Cline توسيع قدراته من خلال الأدوات المخصصة. بينما يمكنك استخدام [الخوادم التي أنشأها المجتمع](https://github.com/modelcontextprotocol/servers)، يمكن لـ Cline بدلاً من ذلك إنشاء أدوات وتثبيتها مصممة خصيصًا لتناسب سير عملك. ما عليك سوى أن تطلب من Cline "إضافة أداة"، وسيتولى كل شيء، من إنشاء خادم MCP جديد إلى تثبيته في الامتداد. تصبح هذه الأدوات المخصصة بعد ذلك جزءًا من مجموعة أدوات Cline، جاهزة للاستخدام في المهام المستقبلية.

- **"أضف أداة تجلب تذاكر Jira"**: استرجع تذاكر AC وقم بتشغيل Cline  
- **"أضف أداة تدير AWS EC2s"**: تحقق من مقاييس الخادم وقم بتوسيع أو تقليص عدد الحالات  
- **"أضف أداة تجلب أحدث حوادث PagerDuty"**: استرجع التفاصيل واطلب من Cline إصلاح الأخطاء  

<!-- بكسل شفاف لإنشاء فاصل سطر بعد الصورة العائمة -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="360" src="https://github.com/user-attachments/assets/7fdf41e6-281a-4b4b-ac19-020b838b6970">

### إضافة السياق

**`@url`**: الصق رابط URL ليقوم الامتداد بجلبه وتحويله إلى Markdown، مفيد عندما تريد تزويد Cline بأحدث الوثائق  

**`@problems`**: أضف أخطاء وتحذيرات بيئة العمل ('لوحة المشكلات') ليتمكن Cline من إصلاحها  

**`@file`**: يضيف محتويات ملف حتى لا تضطر إلى إهدار طلبات API بالموافقة على قراءة الملف (+ البحث في الملفات)  

**`@folder`**: يضيف جميع ملفات المجلد دفعة واحدة لتسريع سير العمل بشكل أكبر  

<!-- بكسل شفاف لإنشاء فاصل سطر بعد الصورة العائمة -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="350" src="https://github.com/user-attachments/assets/140c8606-d3bf-41b9-9a1f-4dbf0d4c90cb">

### نقاط التحقق: المقارنة والاستعادة

أثناء عمل Cline على مهمة، يأخذ الامتداد لقطة من بيئة العمل في كل خطوة. يمكنك استخدام زر "Compare" لرؤية الفرق بين اللقطة وبيئة العمل الحالية، وزر "Restore" للعودة إلى تلك النقطة.

على سبيل المثال، عند العمل مع خادم ويب محلي، يمكنك استخدام "استعادة بيئة العمل فقط" لاختبار إصدارات مختلفة من تطبيقك بسرعة، ثم استخدام "استعادة المهمة وبيئة العمل" عندما تجد الإصدار الذي تريد المتابعة منه. يتيح لك ذلك استكشاف أساليب مختلفة بأمان دون فقدان التقدم.

<!-- بكسل شفاف لإنشاء فاصل سطر بعد الصورة العائمة -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

## المساهمة

للمساهمة في المشروع، ابدأ بـ [دليل المساهمة](CONTRIBUTING.md) لتعلم الأساسيات. يمكنك أيضًا الانضمام إلى [خادم Discord](https://discord.gg/cline) للدردشة مع المساهمين الآخرين في قناة `#contributors`. إذا كنت تبحث عن عمل بدوام كامل، تحقق من الوظائف المتاحة على [صفحة التوظيف](https://cline.bot/join-us)!  

<details>
<summary>تعليمات التطوير المحلي</summary>

1. استنساخ المستودع _(يتطلب [git-lfs](https://git-lfs.com/))_:
    ```bash
    git clone https://github.com/cline/cline.git
    ```
2. افتح المشروع في VSCode:
    ```bash
    code cline
    ```
3. قم بتثبيت التبعيات اللازمة للامتداد وواجهة الويب:
    ```bash
    npm run install:all
    ```
4. قم بالتشغيل بالضغط على `F5` (أو من `Run` -> `Start Debugging`) لفتح نافذة VSCode جديدة مع تحميل الامتداد. (قد تحتاج إلى تثبيت [إضافة esbuild problem matchers](https://marketplace.visualstudio.com/items?itemName=connor4312.esbuild-problem-matchers) إذا واجهت مشكلات في بناء المشروع.)

</details>

<details>
<summary>إنشاء طلب سحب (Pull Request)</summary>

1. قبل إنشاء PR، قم بإنشاء إدخال للتغييرات:
    ```bash
    npm run changeset
    ```
   سيطلب منك تحديد:
   - نوع التغيير (رئيسي، ثانوي، إصلاح)
     - `رئيسي` → تغييرات غير متوافقة (1.0.0 → 2.0.0)
     - `ثانوي` → ميزات جديدة (1.0.0 → 1.1.0)
     - `إصلاح` → إصلاحات للأخطاء (1.0.0 → 1.0.1)
   - وصف التغييرات التي قمت بها  

2. قم بحفظ التغييرات وملف `.changeset` الذي تم إنشاؤه  

3. ادفع فرعك وأنشئ PR على GitHub. سيقوم CI بـ:
   - تشغيل الاختبارات والفحوصات  
   - سيقوم Changesetbot بإنشاء تعليق يوضح تأثير الإصدار  
   - عند الدمج مع الفرع الرئيسي، سيقوم Changesetbot بإنشاء PR لحزم الإصدار  
   - عند دمج PR لحزم الإصدار، سيتم نشر إصدار جديد  

</details>

## الرخصة

[Apache 2.0 © 2025 Cline Bot Inc.](./LICENSE)

## Links discovered
- [قدرات Claude 4 Sonnet على التعليمات البرمجية الوكيلة](https://www.anthropic.com/claude/sonnet)
- [تحديثات تكامل الشل الجديدة في VSCode v1.93](https://code.visualstudio.com/updates/v1_93#_terminal-shell-integration-api)
- [استخدام الكمبيوتر](https://www.anthropic.com/news/3-5-models-and-computer-use)
- [شاهد عرضًا توضيحيًا هنا.](https://x.com/sdrzn/status/1850880547825823989)
- [بروتوكول سياق النموذج](https://github.com/modelcontextprotocol)
- [الخوادم التي أنشأها المجتمع](https://github.com/modelcontextprotocol/servers)
- [دليل المساهمة](https://github.com/cline/cline/blob/main/locales/ar-sa/CONTRIBUTING.md)
- [خادم Discord](https://discord.gg/cline)
- [صفحة التوظيف](https://cline.bot/join-us)
- [git-lfs](https://git-lfs.com/)
- [إضافة esbuild problem matchers](https://marketplace.visualstudio.com/items?itemName=connor4312.esbuild-problem-matchers)
- [Apache 2.0 © 2025 Cline Bot Inc.](https://github.com/cline/cline/blob/main/locales/ar-sa/LICENSE.md)
- [الإسبانية](https://github.com/cline/cline/blob/main/locales/es/README.md)
- [الألمانية](https://github.com/cline/cline/blob/main/locales/de/README.md)
- [اليابانية](https://github.com/cline/cline/blob/main/locales/ja/README.md)
- [الصينية المبسطة](https://github.com/cline/cline/blob/main/locales/zh-cn/README.md)
- [الصينية التقليدية](https://github.com/cline/cline/blob/main/locales/zh-tw/README.md)
- [البرتغالية](https://github.com/cline/cline/blob/main/locales/pt-BR/README.md)
- [<strong>تنزيل من متجر VS</strong>](https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev)
- [<strong>Discord</strong>](https://discord.gg/cline)
- [<strong>r/cline</strong>](https://www.reddit.com/r/cline/)
- [<strong>طلبات الميزات</strong>](https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop)
- [<strong>البدء</strong>](https://docs.cline.bot/getting-started/getting-started-new-coders)

--- locales/de/README.md ---
# Cline

<p align="center">
        <img src="https://media.githubusercontent.com/media/cline/cline/main/assets/docs/demo.gif" width="100%" />
</p>

<div align="center">
<table>
<tbody>
<td align="center">
<a href="https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev" target="_blank"><strong>Im VS Marketplace herunterladen</strong></a>
</td>
<td align="center">
<a href="https://discord.gg/cline" target="_blank"><strong>Discord</strong></a>
</td>
<td align="center">
<a href="https://www.reddit.com/r/cline/" target="_blank"><strong>r/cline</strong></a>
</td>
<td align="center">
<a href="https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop" target="_blank"><strong>Feature Requests</strong></a>
</td>
<td align="center">
<a href="https://cline.bot/join-us" target="_blank"><strong>Wir stellen ein!</strong></a>
</td>
</tbody>
</table>
</div>

Lernen Sie Cline kennen, einen KI-Assistenten, der Ihre **CLI** u**N**d **E**ditor nutzen kann.

Dank der [agentischen Codierungsfähigkeiten von Claude 4 Sonnet](https://www.anthropic.com/claude/sonnet) kann Cline komplexe Softwareentwicklungsaufgaben Schritt für Schritt bewältigen. Mit Werkzeugen, die ihm das Erstellen und Bearbeiten von Dateien, das Erkunden großer Projekte, die Nutzung des Browsers und das Ausführen von Terminalbefehlen (nach Ihrer Genehmigung) ermöglichen, kann er Ihnen auf eine Weise helfen, die über die Codevervollständigung oder technischen Support hinausgeht. Cline kann sogar das Model Context Protocol (MCP) verwenden, um neue Werkzeuge zu erstellen und seine eigenen Fähigkeiten zu erweitern. Während autonome KI-Skripte traditionell in sandboxed Umgebungen laufen, bietet diese Erweiterung eine Mensch-in-der-Schleife-GUI, um jede Dateiänderung und jeden Terminalbefehl zu genehmigen, was eine sichere und zugängliche Möglichkeit bietet, das Potenzial agentischer KI zu erkunden.

1. Geben Sie Ihre Aufgabe ein und fügen Sie Bilder hinzu, um Mockups in funktionale Apps zu konvertieren oder Fehler mit Screenshots zu beheben.
2. Cline beginnt mit der Analyse Ihrer Dateistruktur und Quellcode-ASTs, führt Regex-Suchen durch und liest relevante Dateien, um sich in bestehenden Projekten zurechtzufinden. Durch sorgfältiges Management der hinzugefügten Informationen kann Cline wertvolle Unterstützung auch bei großen, komplexen Projekten bieten, ohne das Kontextfenster zu überladen.
3. Sobald Cline die benötigten Informationen hat, kann er:
                - Dateien erstellen und bearbeiten sowie Linter-/Compiler-Fehler überwachen, um proaktiv Probleme wie fehlende Importe und Syntaxfehler selbst zu beheben.
                - Befehle direkt in Ihrem Terminal ausführen und deren Ausgabe überwachen, sodass er z.B. auf Dev-Server-Probleme reagieren kann, nachdem er eine Datei bearbeitet hat.
                - Für Webentwicklungsaufgaben kann Cline die Website in einem Headless-Browser starten, klicken, tippen, scrollen und Screenshots sowie Konsolenprotokolle erfassen, sodass er Laufzeitfehler und visuelle Fehler beheben kann.
4. Wenn eine Aufgabe abgeschlossen ist, präsentiert Cline das Ergebnis mit einem Terminalbefehl wie `open -a "Google Chrome" index.html`, den Sie mit einem Klick ausführen können.

> [!TIPP]
> Verwenden Sie die Tastenkombination `CMD/CTRL + Shift + P`, um die Befehls-Palette zu öffnen und geben Sie "Cline: Open In New Tab" ein, um die Erweiterung als Tab in Ihrem Editor zu öffnen. So können Sie Cline neben Ihrem Dateiexplorer verwenden und sehen, wie er Ihren Arbeitsbereich verändert.

---

<img align="right" width="340" src="https://github.com/user-attachments/assets/3cf21e04-7ce9-4d22-a7b9-ba2c595e88a4">

### Verwenden Sie jede API und jedes Modell

Cline unterstützt API-Anbieter wie OpenRouter, Anthropic, OpenAI, Google Gemini, AWS Bedrock, Azure und GCP Vertex. Sie können auch jede OpenAI-kompatible API konfigurieren oder ein lokales Modell über LM Studio/Ollama verwenden. Wenn Sie OpenRouter verwenden, ruft die Erweiterung deren neueste Modellliste ab, sodass Sie die neuesten Modelle sofort verwenden können, sobald sie verfügbar sind.

Die Erweiterung verfolgt auch die gesamten Token- und API-Nutzungskosten für den gesamten Aufgabenzyklus und einzelne Anfragen, sodass Sie bei jedem Schritt über die Ausgaben informiert sind.

<!-- Transparenter Pixel, um einen Zeilenumbruch nach dem schwebenden Bild zu erzeugen -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="370" src="https://github.com/user-attachments/assets/81be79a8-1fdb-4028-9129-5fe055e01e76">

### Befehle im Terminal ausführen

Dank der neuen [Shell-Integrations-Updates in VSCode v1.93](https://code.visualstudio.com/updates/v1_93#_terminal-shell-integration-api) kann Cline Befehle direkt in Ihrem Terminal ausführen und die Ausgabe empfangen. Dies ermöglicht ihm eine Vielzahl von Aufgaben, von der Installation von Paketen und dem Ausführen von Build-Skripten bis hin zur Bereitstellung von Anwendungen, Verwaltung von Datenbanken und Ausführung von Tests, während er sich an Ihre Entwicklungsumgebung und Toolchain anpasst, um die Aufgabe richtig zu erledigen.

Für lang laufende Prozesse wie Dev-Server verwenden Sie die Schaltfläche "Während des Laufens fortfahren", um Cline die Fortsetzung der Aufgabe zu ermöglichen, während der Befehl im Hintergrund läuft. Während Cline arbeitet, wird er über neue Terminalausgaben benachrichtigt, sodass er auf auftretende Probleme reagieren kann, wie z.B. Kompilierungsfehler beim Bearbeiten von Dateien.

<!-- Transparenter Pixel, um einen Zeilenumbruch nach dem schwebenden Bild zu erzeugen -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="400" src="https://github.com/user-attachments/assets/c5977833-d9b8-491e-90f9-05f9cd38c588">

### Dateien erstellen und bearbeiten

Cline kann Dateien direkt in Ihrem Editor erstellen und bearbeiten und Ihnen eine Diff-Ansicht der Änderungen präsentieren. Sie können die Änderungen von Cline direkt im Diff-Ansichts-Editor bearbeiten oder rückgängig machen oder Feedback im Chat geben, bis Sie mit dem Ergebnis zufrieden sind. Cline überwacht auch Linter-/Compiler-Fehler (fehlende Importe, Syntaxfehler usw.), sodass er auftretende Probleme selbst beheben kann.

Alle von Cline vorgenommenen Änderungen werden in der Timeline Ihrer Datei aufgezeichnet, was eine einfache Möglichkeit bietet, Änderungen nachzuverfolgen und bei Bedarf rückgängig zu machen.

<!-- Transparenter Pixel, um einen Zeilenumbruch nach dem schwebenden Bild zu erzeugen -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="370" src="https://github.com/user-attachments/assets/bc2e85ba-dfeb-4fe6-9942-7cfc4703cbe5">

### Den Browser verwenden

Mit der neuen [Computer Use](https://www.anthropic.com/news/3-5-models-and-computer-use) Fähigkeit von Claude 4 Sonnet kann Cline einen Browser starten, Elemente anklicken, Text eingeben und scrollen, dabei Screenshots und Konsolenprotokolle bei jedem Schritt erfassen. Dies ermöglicht interaktives Debugging, End-to-End-Tests und sogar allgemeine Webnutzung! Dies gibt ihm die Autonomie, visuelle Fehler und Laufzeitprobleme zu beheben, ohne dass Sie selbst Fehlerprotokolle kopieren und einfügen müssen.

Versuchen Sie, Cline zu bitten, "die App zu testen", und sehen Sie zu, wie er einen Befehl wie `npm run dev` ausführt, Ihren lokal laufenden Dev-Server in einem Browser startet und eine Reihe von Tests durchführt, um zu bestätigen, dass alles funktioniert. [Sehen Sie sich hier eine Demo an.](https://x.com/sdrzn/status/1850880547825823989)

<!-- Transparenter Pixel, um einen Zeilenumbruch nach dem schwebenden Bild zu erzeugen -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="350" src="https://github.com/user-attachments/assets/ac0efa14-5c1f-4c26-a42d-9d7c56f5fadd">

### "ein Werkzeug hinzufügen, das..."

Dank des [Model Context Protocol](https://github.com/modelcontextprotocol) kann Cline seine Fähigkeiten durch benutzerdefinierte Werkzeuge erweitern. Während Sie [community-made servers](https://github.com/modelcontextprotocol/servers) verwenden können, kann Cline stattdessen Werkzeuge erstellen und installieren, die speziell auf Ihren Workflow zugeschnitten sind. Bitten Sie Cline einfach, "ein Werkzeug hinzuzufügen", und er erledigt alles, von der Erstellung eines neuen MCP-Servers bis zur Installation in der Erweiterung. Diese benutzerdefinierten Werkzeuge werden dann Teil von Clines Toolkit und sind bereit, in zukünftigen Aufgaben verwendet zu werden.

-   "ein Werkzeug hinzufügen, das Jira-Tickets abruft": Abrufen von Ticket-ACs und Cline zur Arbeit bringen
-   "ein Werkzeug hinzufügen, das AWS EC2s verwaltet": Überprüfen von Servermetriken und Skalieren von Instanzen
-   "ein Werkzeug hinzufügen, das die neuesten PagerDuty-Vorfälle abruft": Abrufen von Details und Cline bitten, Fehler zu beheben

<!-- Transparenter Pixel, um einen Zeilenumbruch nach dem schwebenden Bild zu erzeugen -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="360" src="https://github.com/user-attachments/assets/7fdf41e6-281a-4b4b-ac19-020b838b6970">

### Kontext hinzufügen

**`@url`:** Fügen Sie eine URL ein, damit die Erweiterung sie abruft und in Markdown konvertiert, nützlich, wenn Sie Cline die neuesten Dokumente geben möchten

**`@problems`:** Fügen Sie Arbeitsbereichsfehler und -warnungen (Panel 'Probleme') hinzu, die Cline beheben soll

**`@file`:** Fügt den Inhalt einer Datei hinzu, sodass Sie keine API-Anfragen verschwenden müssen, um das Lesen der Datei zu genehmigen (+ zum Suchen von Dateien tippen)

**`@folder`:** Fügt die Dateien eines Ordners auf einmal hinzu, um Ihren Workflow noch weiter zu beschleunigen

<!-- Transparenter Pixel, um einen Zeilenumbruch nach dem schwebenden Bild zu erzeugen -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="350" src="https://github.com/user-attachments/assets/140c8606-d3bf-41b9-9a1f-4dbf0d4c90cb">

### Checkpoints: Vergleichen und Wiederherstellen

Während Cline eine Aufgabe bearbeitet, erstellt die Erweiterung bei jedem Schritt einen Schnappschuss Ihres Arbeitsbereichs. Sie können die Schaltfläche 'Vergleichen' verwenden, um einen Diff zwischen dem Schnappschuss und Ihrem aktuellen Arbeitsbereich zu sehen, und die Schaltfläche 'Wiederherstellen', um zu diesem Punkt zurückzukehren.

Wenn Sie beispielsweise mit einem lokalen Webserver arbeiten, können Sie 'Nur Arbeitsbereich wiederherstellen' verwenden, um schnell verschiedene Versionen Ihrer App zu testen, und 'Aufgabe und Arbeitsbereich wiederherstellen', wenn Sie die Version gefunden haben, von der aus Sie weiterentwickeln möchten. Dies ermöglicht es Ihnen, sicher verschiedene Ansätze zu erkunden, ohne Fortschritte zu verlieren.

<!-- Transparenter Pixel, um einen Zeilenumbruch nach dem schwebenden Bild zu erzeugen -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

## Beitrag leisten

Um zum Projekt beizutragen, beginnen Sie mit unserem [Beitragsleitfaden](CONTRIBUTING.md), um die Grundlagen zu lernen. Sie können auch unserem [Discord](https://discord.gg/cline) beitreten, um im Kanal `#contributors` mit anderen Mitwirkenden zu chatten. Wenn Sie auf der Suche nach einer Vollzeitstelle sind, schauen Sie sich unsere offenen Stellen auf unserer [Karriereseite](https://cline.bot/join-us) an!

<details>
<summary>Lokale Entwicklungsanweisungen</summary>

1. Klonen Sie das Repository _(Erfordert [git-lfs](https://git-lfs.com/))_:
                ```bash
                git clone https://github.com/cline/cline.git
                ```
2. Öffnen Sie das Projekt in VSCode:
                ```bash
                code cline
                ```
3. Installieren Sie die notwendigen Abhängigkeiten für die Erweiterung und das Webview-GUI:
                ```bash
                npm run install:all
                ```
4. Starten Sie durch Drücken von `F5` (oder `Run`->`Start Debugging`), um ein neues VSCode-Fenster mit der geladenen Erweiterung zu öffnen. (Möglicherweise müssen Sie die [esbuild problem matchers extension](https://marketplace.visualstudio.com/items?itemName=connor4312.esbuild-problem-matchers) installieren, wenn Sie auf Probleme beim Erstellen des Projekts stoßen.)

</details>

## Lizenz

[Apache 2.0 © 2025 Cline Bot Inc.](./LICENSE)



## Links discovered
- [agentischen Codierungsfähigkeiten von Claude 4 Sonnet](https://www.anthropic.com/claude/sonnet)
- [Shell-Integrations-Updates in VSCode v1.93](https://code.visualstudio.com/updates/v1_93#_terminal-shell-integration-api)
- [Computer Use](https://www.anthropic.com/news/3-5-models-and-computer-use)
- [Sehen Sie sich hier eine Demo an.](https://x.com/sdrzn/status/1850880547825823989)
- [Model Context Protocol](https://github.com/modelcontextprotocol)
- [community-made servers](https://github.com/modelcontextprotocol/servers)
- [Beitragsleitfaden](https://github.com/cline/cline/blob/main/locales/de/CONTRIBUTING.md)
- [Discord](https://discord.gg/cline)
- [Karriereseite](https://cline.bot/join-us)
- [git-lfs](https://git-lfs.com/)
- [esbuild problem matchers extension](https://marketplace.visualstudio.com/items?itemName=connor4312.esbuild-problem-matchers)
- [Apache 2.0 © 2025 Cline Bot Inc.](https://github.com/cline/cline/blob/main/locales/de/LICENSE.md)
- [<strong>Im VS Marketplace herunterladen</strong>](https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev)
- [<strong>Discord</strong>](https://discord.gg/cline)
- [<strong>r/cline</strong>](https://www.reddit.com/r/cline/)
- [<strong>Feature Requests</strong>](https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop)
- [<strong>Wir stellen ein!</strong>](https://cline.bot/join-us)

--- locales/es/README.md ---
# Cline

<p align="center">
        <img src="https://media.githubusercontent.com/media/cline/cline/main/assets/docs/demo.gif" width="100%" />
</p>

<div align="center">
<table>
<tbody>
<td align="center">
<a href="https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev" target="_blank"><strong>Descargar en VS Marketplace</strong></a>
</td>
<td align="center">
<a href="https://discord.gg/cline" target="_blank"><strong>Discord</strong></a>
</td>
<td align="center">
<a href="https://www.reddit.com/r/cline/" target="_blank"><strong>r/cline</strong></a>
</td>
<td align="center">
<a href="https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop" target="_blank"><strong>Solicitudes de Funciones</strong></a>
</td>
<td align="center">
<a href="https://cline.bot/join-us" target="_blank"><strong>Estamos Contratando!</strong></a>
</td>
</tbody>
</table>
</div>

Conozca a Cline, un asistente de IA que puede usar su **CLI** y **E**ditor.

Gracias a las [habilidades de codificación agencial de Claude 4 Sonnet](https://www.anthropic.com/claude/sonnet), Cline puede abordar tareas complejas de desarrollo de software paso a paso. Con herramientas que le permiten crear y editar archivos, explorar grandes proyectos, usar el navegador y ejecutar comandos de terminal (con su aprobación), puede ayudarle de una manera que va más allá de la autocompletación de código o el soporte técnico. Cline incluso puede usar el Model Context Protocol (MCP) para crear nuevas herramientas y expandir sus propias capacidades. Mientras que los scripts de IA autónomos tradicionalmente se ejecutan en entornos aislados, esta extensión ofrece una GUI con un humano en el bucle para aprobar cada cambio de archivo y comando de terminal, proporcionando una forma segura y accesible de explorar el potencial de la IA agencial.

1. Ingrese su tarea y agregue imágenes para convertir maquetas en aplicaciones funcionales o solucionar errores con capturas de pantalla.
2. Cline comenzará analizando su estructura de archivos y ASTs de código fuente, realizando búsquedas Regex y leyendo archivos relevantes para orientarse en proyectos existentes. Al gestionar cuidadosamente la información agregada, Cline puede proporcionar asistencia valiosa incluso en proyectos grandes y complejos sin sobrecargar la ventana de contexto.
3. Una vez que Cline tenga la información necesaria, puede:
                - Crear y editar archivos + monitorear errores de Linter/Compilador, para que pueda solucionar proactivamente problemas como importaciones faltantes y errores de sintaxis.
                - Ejecutar comandos directamente en su terminal y monitorear su salida, para que pueda responder a problemas del servidor de desarrollo después de editar un archivo.
                - Para tareas de desarrollo web, Cline puede iniciar el sitio web en un navegador sin cabeza, hacer clic, escribir, desplazarse y capturar capturas de pantalla + registros de consola, para que pueda solucionar errores de tiempo de ejecución y errores visuales.
4. Cuando una tarea esté completa, Cline le presentará el resultado con un comando de terminal como `open -a "Google Chrome" index.html`, que puede ejecutar con un clic en un botón.

> [!TIP]
> Use el atajo de teclado `CMD/CTRL + Shift + P` para abrir la paleta de comandos y escriba "Cline: Open In New Tab" para abrir la extensión como una pestaña en su editor. De esta manera, puede usar Cline junto a su explorador de archivos y ver más claramente cómo cambia su espacio de trabajo.

---

<img align="right" width="340" src="https://github.com/user-attachments/assets/3cf21e04-7ce9-4d22-a7b9-ba2c595e88a4">

### Use cualquier API y modelo

Cline admite proveedores de API como OpenRouter, Anthropic, OpenAI, Google Gemini, AWS Bedrock, Azure y GCP Vertex. También puede configurar cualquier API compatible con OpenAI o usar un modelo local a través de LM Studio/Ollama. Si usa OpenRouter, la extensión recupera su lista de modelos más reciente, para que pueda usar los modelos más nuevos tan pronto como estén disponibles.

La extensión también rastrea el uso total de tokens y costos de API para todo el ciclo de tareas y solicitudes individuales, para que esté informado sobre los gastos en cada paso.

<!-- Pixel transparente para crear un salto de línea después de la imagen flotante -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="370" src="https://github.com/user-attachments/assets/81be79a8-1fdb-4028-9129-5fe055e01e76">

### Ejecutar comandos en el terminal

Gracias a las nuevas [actualizaciones de integración de Shell en VSCode v1.93](https://code.visualstudio.com/updates/v1_93#_terminal-shell-integration-api), Cline puede ejecutar comandos directamente en su terminal y recibir la salida. Esto le permite realizar una variedad de tareas, desde la instalación de paquetes y la ejecución de scripts de compilación hasta la implementación de aplicaciones, la gestión de bases de datos y la ejecución de pruebas, adaptándose a su entorno de desarrollo y cadena de herramientas para hacer el trabajo correctamente.

Para procesos de larga duración como servidores de desarrollo, use el botón "Continuar mientras se ejecuta" para permitir que Cline continúe con la tarea mientras el comando se ejecuta en segundo plano. Mientras Cline trabaja, será notificado sobre nuevas salidas del terminal, para que pueda responder a problemas que puedan surgir, como errores de compilación al editar archivos.

<!-- Pixel transparente para crear un salto de línea después de la imagen flotante -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="400" src="https://github.com/user-attachments/assets/c5977833-d9b8-491e-90f9-05f9cd38c588">

### Crear y editar archivos

Cline puede crear y editar archivos directamente en su editor y presentarle una vista de diferencias de los cambios. Puede editar o deshacer los cambios de Cline directamente en el editor de vista de diferencias o proporcionar comentarios en el chat hasta que esté satisfecho con el resultado. Cline también monitorea errores de Linter/Compilador (importaciones faltantes, errores de sintaxis, etc.), para que pueda solucionar problemas que surjan en el camino.

Todos los cambios realizados por Cline se registran en la línea de tiempo de su archivo, proporcionando una forma sencilla de rastrear cambios y deshacerlos si es necesario.

<!-- Pixel transparente para crear un salto de línea después de la imagen flotante -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="370" src="https://github.com/user-attachments/assets/bc2e85ba-dfeb-4fe6-9942-7cfc4703cbe5">

### Usar el navegador

Con la nueva [habilidad de uso de computadora](https://www.anthropic.com/news/3-5-models-and-computer-use) de Claude 4 Sonnet, Cline puede iniciar un navegador, hacer clic en elementos, escribir texto y desplazarse, capturando capturas de pantalla y registros de consola. Esto permite la depuración interactiva, pruebas de extremo a extremo e incluso el uso general de la web. Esto le da la autonomía para solucionar errores visuales y problemas de tiempo de ejecución sin que tenga que copiar y pegar registros de errores.

Intente pedirle a Cline que "pruebe la aplicación" y observe cómo ejecuta un comando como `npm run dev`, inicia su servidor de desarrollo local en un navegador y realiza una serie de pruebas para confirmar que todo funciona. [Vea una demostración aquí.](https://x.com/sdrzn/status/1850880547825823989)

<!-- Pixel transparente para crear un salto de línea después de la imagen flotante -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="350" src="https://github.com/user-attachments/assets/ac0efa14-5c1f-4c26-a42d-9d7c56f5fadd">

### "agregar una herramienta que..."

Gracias al [Model Context Protocol](https://github.com/modelcontextprotocol), Cline puede expandir sus habilidades mediante herramientas personalizadas. Mientras que puede usar [servidores creados por la comunidad](https://github.com/modelcontextprotocol/servers), Cline puede en su lugar crear e instalar herramientas adaptadas a su flujo de trabajo específico. Simplemente pida a Cline que "agregue una herramienta" y él se encargará de todo, desde la creación de un nuevo servidor MCP hasta la instalación en la extensión. Estas herramientas personalizadas se convierten en parte del conjunto de herramientas de Cline y están listas para ser utilizadas en tareas futuras.

-   "agregar una herramienta que recupere tickets de Jira": Recuperar ACs de tickets y poner a Cline a trabajar
-   "agregar una herramienta que gestione AWS EC2s": Verificar métricas del servidor y escalar instancias hacia arriba o hacia abajo
-   "agregar una herramienta que recupere los últimos incidentes de PagerDuty": Recuperar detalles y pedir a Cline que solucione errores

<!-- Pixel transparente para crear un salto de línea después de la imagen flotante -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="360" src="https://github.com/user-attachments/assets/7fdf41e6-281a-4b4b-ac19-020b838b6970">

### Agregar contexto

**`@url`:** Inserte una URL para que la extensión la recupere y convierta en Markdown, útil cuando desee proporcionar a Cline los documentos más recientes

**`@problems`:** Agregue errores y advertencias del espacio de trabajo (panel 'Problemas') que Cline debe solucionar

**`@file`:** Agregue el contenido de un archivo para que no tenga que desperdiciar solicitudes de API para aprobar la lectura del archivo (+ para buscar archivos)

**`@folder`:** Agregue los archivos de una carpeta a la vez para acelerar aún más su flujo de trabajo

<!-- Pixel transparente para crear un salto de línea después de la imagen flotante -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="350" src="https://github.com/user-attachments/assets/140c8606-d3bf-41b9-9a1f-4dbf0d4c90cb">

### Puntos de control: Comparar y Restaurar

Mientras Cline trabaja en una tarea, la extensión crea una instantánea de su espacio de trabajo en cada paso. Puede usar el botón 'Comparar' para ver una diferencia entre la instantánea y su espacio de trabajo actual, y el botón 'Restaurar' para volver a ese punto.

Por ejemplo, si está trabajando con un servidor web local, puede usar 'Restaurar solo espacio de trabajo' para probar rápidamente diferentes versiones de su aplicación, y luego 'Restaurar tarea y espacio de trabajo' cuando encuentre la versión desde la que desea continuar trabajando. Esto le permite explorar diferentes enfoques de manera segura sin perder progreso.

<!-- Pixel transparente para crear un salto de línea después de la imagen flotante -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

## Contribuir

Para contribuir al proyecto, comience con nuestra [guía de contribución](CONTRIBUTING.md) para aprender los conceptos básicos. También puede unirse a nuestro [Discord](https://discord.gg/cline) para chatear con otros colaboradores en el canal `#contributors`. Si está buscando un trabajo a tiempo completo, consulte nuestras vacantes en nuestra [página de carreras](https://cline.bot/join-us).

<details>
<summary>Instrucciones de desarrollo local</summary>

1. Clone el repositorio _(Requiere [git-lfs](https://git-lfs.com/))_:
                ```bash
                git clone https://github.com/cline/cline.git
                ```
2. Abra el proyecto en VSCode:
                ```bash
                code cline
                ```
3. Instale las dependencias necesarias para la extensión y la GUI de Webview:
                ```bash
                npm run install:all
                ```
4. Inicie presionando `F5` (o `Run`->`Start Debugging`) para abrir una nueva ventana de VSCode con la extensión cargada. (Es posible que deba instalar la [extensión de emparejadores de problemas de esbuild](https://marketplace.visualstudio.com/items?itemName=connor4312.esbuild-problem-matchers) si encuentra problemas al compilar el proyecto.)

</details>

## Licencia

[Apache 2.0 © 2025 Cline Bot Inc.](./LICENSE)


## Links discovered
- [habilidades de codificación agencial de Claude 4 Sonnet](https://www.anthropic.com/claude/sonnet)
- [actualizaciones de integración de Shell en VSCode v1.93](https://code.visualstudio.com/updates/v1_93#_terminal-shell-integration-api)
- [habilidad de uso de computadora](https://www.anthropic.com/news/3-5-models-and-computer-use)
- [Vea una demostración aquí.](https://x.com/sdrzn/status/1850880547825823989)
- [Model Context Protocol](https://github.com/modelcontextprotocol)
- [servidores creados por la comunidad](https://github.com/modelcontextprotocol/servers)
- [guía de contribución](https://github.com/cline/cline/blob/main/locales/es/CONTRIBUTING.md)
- [Discord](https://discord.gg/cline)
- [página de carreras](https://cline.bot/join-us)
- [git-lfs](https://git-lfs.com/)
- [extensión de emparejadores de problemas de esbuild](https://marketplace.visualstudio.com/items?itemName=connor4312.esbuild-problem-matchers)
- [Apache 2.0 © 2025 Cline Bot Inc.](https://github.com/cline/cline/blob/main/locales/es/LICENSE.md)
- [<strong>Descargar en VS Marketplace</strong>](https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev)
- [<strong>Discord</strong>](https://discord.gg/cline)
- [<strong>r/cline</strong>](https://www.reddit.com/r/cline/)
- [<strong>Solicitudes de Funciones</strong>](https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop)
- [<strong>Estamos Contratando!</strong>](https://cline.bot/join-us)

--- locales/ja/README.md ---
# Cline

<p align="center">
    <img src="https://media.githubusercontent.com/media/cline/cline/main/assets/docs/demo.gif" width="100%" />
</p>

<div align="center">
<table>
<tbody>
<td align="center">
<a href="https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev" target="_blank"><strong>VS Marketplaceでダウンロード</strong></a>
</td>
<td align="center">
<a href="https://discord.gg/cline" target="_blank"><strong>Discord</strong></a>
</td>
<td align="center">
<a href="https://www.reddit.com/r/cline/" target="_blank"><strong>r/cline</strong></a>
</td>
<td align="center">
<a href="https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop" target="_blank"><strong>機能リクエスト</strong></a>
</td>
<td align="center">
<a href="https://cline.bot/join-us" target="_blank"><strong>採用情報</strong></a>
</td>
</tbody>
</table>
</div>

Clineは、**CLI**と**エディター**を使用できるAIアシスタントです。

[Claude 4 Sonnetのエージェント的コーディング機能](https://www.anthropic.com/claude/sonnet)のおかげで、Clineは複雑なソフトウェア開発タスクをステップバイステップで処理できます。ファイルの作成と編集、大規模プロジェクトの探索、ブラウザの使用、ターミナルコマンドの実行（許可後）などのツールを使用して、コード補完や技術サポートを超えた支援を提供します。Clineは、Model Context Protocol (MCP)を使用して新しいツールを作成し、自身の機能を拡張することもできます。自律的なAIスクリプトは通常サンドボックス環境で実行されますが、この拡張機能はファイル変更やターミナルコマンドを承認するための人間インターフェースを提供し、エージェント的AIの可能性を安全かつアクセスしやすい方法で探求できます。

1. タスクを入力し、モックアップを機能するアプリに変換したり、スクリーンショットでバグを修正したりします。
2. Clineは、ファイル構造とソースコードASTの分析、正規表現検索の実行、関連ファイルの読み取りから始め、既存プロジェクトに精通します。コンテキストに追加される情報を慎重に管理することで、大規模で複雑なプロジェクトでもコンテキストウィンドウを圧倒することなく貴重な支援を提供できます。
3. Clineが必要な情報を取得すると、次のことができます：
        - ファイルの作成と編集 + リンター/コンパイラーエラーの監視を行い、欠落したインポートや構文エラーなどの問題を自動的に修正します。
        - ターミナルでコマンドを直接実行し、作業中に出力を監視します。これにより、ファイル編集後の開発サーバーの問題に対応できます。
        - ウェブ開発タスクでは、ヘッドレスブラウザでサイトを起動し、クリック、入力、スクロール、スクリーンショットとコンソールログのキャプチャを行い、ランタイムエラーや視覚的なバグを修正します。
4. タスクが完了すると、Clineは`open -a "Google Chrome" index.html`のようなターミナルコマンドを提示し、ボタンをクリックして実行できます。

> [!TIP]
> `CMD/CTRL + Shift + P`ショートカットを使用してコマンドパレットを開き、「Cline: Open In New Tab」と入力して、エディターのタブとして拡張機能を開きます。これにより、ファイルエクスプローラーと並行してClineを使用し、ワークスペースの変更をより明確に確認できます。

---

<img align="right" width="340" src="https://github.com/user-attachments/assets/3cf21e04-7ce9-4d22-a7b9-ba2c595e88a4">

### どのAPIやモデルでも使用可能

Clineは、OpenRouter、Anthropic、OpenAI、Google Gemini、AWS Bedrock、Azure、GCP VertexなどのAPIプロバイダーをサポートしています。また、OpenAI互換のAPIを設定したり、LM Studio/Ollamaを通じてローカルモデルを使用することもできます。OpenRouterを使用している場合、拡張機能は最新のモデルリストを取得し、最新のモデルをすぐに使用できるようにします。

拡張機能は、タスクループ全体と個々のリクエストのトークン総数とAPI使用コストを追跡し、各ステップで支出を把握できます。

<!-- 透明なピクセルで浮動画像の後に改行を作成 -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="370" src="https://github.com/user-attachments/assets/81be79a8-1fdb-4028-9129-5fe055e01e76">

### ターミナルでコマンドを実行

VSCode v1.93の新しい[シェル統合アップデート](https://code.visualstudio.com/updates/v1_93#_terminal-shell-integration-api)のおかげで、Clineはターミナルでコマンドを直接実行し、出力を受け取ることができます。これにより、パッケージのインストールやビルドスクリプトの実行からアプリケーションのデプロイ、データベースの管理、テストの実行まで、幅広いタスクを実行できます。Clineは、開発環境とツールチェーンに適応して、タスクを正確に実行します。

開発サーバーのような長時間実行されるプロセスの場合、「実行中に続行」ボタンを使用して、コマンドがバックグラウンドで実行されている間にClineがタスクを続行できるようにします。Clineが作業を進める中で、新しいターミナル出力が通知され、ファイル編集時のコンパイルエラーなどの問題に対応できます。

<!-- 透明なピクセルで浮動画像の後に改行を作成 -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="400" src="https://github.com/user-attachments/assets/c5977833-d9b8-491e-90f9-05f9cd38c588">

### ファイルの作成と編集

Clineはエディター内でファイルを作成および編集し、変更の差分ビューを提示します。差分ビューエディターでClineの変更を直接編集または元に戻すことができ、チャットでフィードバックを提供して満足するまで調整できます。Clineはリンター/コンパイラーエラー（欠落したインポート、構文エラーなど）も監視し、発生した問題を自動的に修正します。

Clineによるすべての変更はファイルのタイムラインに記録され、必要に応じて変更を追跡および元に戻す簡単な方法を提供します。

<!-- 透明なピクセルで浮動画像の後に改行を作成 -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="370" src="https://github.com/user-attachments/assets/bc2e85ba-dfeb-4fe6-9942-7cfc4703cbe5">

### ブラウザの使用

Claude 4 Sonnetの新しい[コンピュータ使用](https://www.anthropic.com/news/3-5-models-and-computer-use)機能により、Clineはブラウザを起動し、要素をクリック、テキストを入力、スクロールし、各ステップでスクリーンショットとコンソールログをキャプチャできます。これにより、インタラクティブなデバッグ、エンドツーエンドテスト、さらには一般的なウェブ使用が可能になります。これにより、エラーログを手動でコピー＆ペーストすることなく、視覚的なバグやランタイムの問題を自律的に修正できます。

Clineに「アプリをテストして」と頼んでみてください。彼は`npm run dev`のようなコマンドを実行し、ローカルで実行中の開発サーバーをブラウザで起動し、一連のテストを実行してすべてが正常に動作することを確認します。[デモはこちら。](https://x.com/sdrzn/status/1850880547825823989)

<!-- 透明なピクセルで浮動画像の後に改行を作成 -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="350" src="https://github.com/user-attachments/assets/ac0efa14-5c1f-4c26-a42d-9d7c56f5fadd">

### 「ツールを追加して...」

[Model Context Protocol](https://github.com/modelcontextprotocol)のおかげで、Clineはカスタムツールを通じて機能を拡張できます。[コミュニティ製サーバー](https://github.com/modelcontextprotocol/servers)を使用することもできますが、Clineは代わりに特定のワークフローに合わせたツールを作成してインストールできます。「ツールを追加して」と頼むだけで、Clineは新しいMCPサーバーの作成から拡張機能へのインストールまでをすべて処理します。これらのカスタムツールはClineのツールキットの一部となり、将来のタスクで使用できるようになります。

- 「Jiraチケットを取得するツールを追加して」：チケットACを取得し、Clineに作業を依頼
- 「AWS EC2を管理するツールを追加して」：サーバーメトリクスを確認し、インスタンスをスケールアップまたはダウン
- 「最新のPagerDutyインシデントを取得するツールを追加して」：詳細を取得し、Clineにバグ修正を依頼

<!-- 透明なピクセルで浮動画像の後に改行を作成 -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="360" src="https://github.com/user-attachments/assets/7fdf41e6-281a-4b4b-ac19-020b838b6970">

### コンテキストを追加

**`@url`：** 最新のドキュメントをClineに提供したい場合に、URLを貼り付けて拡張機能が取得し、Markdownに変換します。

**`@problems`：** Clineが修正するためのワークスペースエラーと警告（「問題」パネル）を追加します。

**`@file`：** ファイルの内容を追加し、読み取りファイルを承認するAPIリクエストを節約します（+ファイルを検索して入力）。

**`@folder`：** フォルダーのファイルを一度に追加して、ワークフローをさらにスピードアップします。

<!-- 透明なピクセルで浮動画像の後に改行を作成 -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="350" src="https://github.com/user-attachments/assets/140c8606-d3bf-41b9-9a1f-4dbf0d4c90cb">

### チェックポイント：比較と復元

Clineがタスクを進める中で、拡張機能は各ステップでワークスペースのスナップショットを撮ります。「比較」ボタンを使用してスナップショットと現在のワークスペースの差分を確認し、「復元」ボタンを使用してそのポイントにロールバックできます。

たとえば、ローカルウェブサーバーで作業している場合、「ワークスペースのみを復元」を使用して異なるバージョンのアプリを迅速にテストし、「タスクとワークスペースを復元」を使用して続行したいバージョンを見つけたときに使用します。これにより、進行状況を失うことなく異なるアプローチを安全に探求できます。

<!-- 透明なピクセルで浮動画像の後に改行を作成 -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

## 貢献

プロジェクトに貢献するには、[貢献ガイド](CONTRIBUTING.md)から基本を学び始めてください。また、[Discord](https://discord.gg/cline)に参加して、`#contributors`チャンネルで他の貢献者とチャットすることもできます。フルタイムの仕事を探している場合は、[採用ページ](https://cline.bot/join-us)でオープンポジションを確認してください。

<details>
<summary>ローカル開発の手順</summary>

1. リポジトリをクローンします _(Requires [git-lfs](https://git-lfs.com/))_：
        ```bash
        git clone https://github.com/cline/cline.git
        ```
2. プロジェクトをVSCodeで開きます：
        ```bash
        code cline
        ```
3. 拡張機能とwebview-guiの必要な依存関係をインストールします：
        ```bash
        npm run install:all
        ```
4. `F5`を押して（または`Run`->`Start Debugging`）、拡張機能が読み込まれた新しいVSCodeウィンドウを開きます。（プロジェクトのビルドに問題がある場合は、[esbuild problem matchers extension](https://marketplace.visualstudio.com/items?itemName=connor4312.esbuild-problem-matchers)をインストールする必要があるかもしれません。）

</details>

## ライセンス

[Apache 2.0 © 2025 Cline Bot Inc.](./LICENSE)


## Links discovered
- [Claude 4 Sonnetのエージェント的コーディング機能](https://www.anthropic.com/claude/sonnet)
- [シェル統合アップデート](https://code.visualstudio.com/updates/v1_93#_terminal-shell-integration-api)
- [コンピュータ使用](https://www.anthropic.com/news/3-5-models-and-computer-use)
- [デモはこちら。](https://x.com/sdrzn/status/1850880547825823989)
- [Model Context Protocol](https://github.com/modelcontextprotocol)
- [コミュニティ製サーバー](https://github.com/modelcontextprotocol/servers)
- [貢献ガイド](https://github.com/cline/cline/blob/main/locales/ja/CONTRIBUTING.md)
- [Discord](https://discord.gg/cline)
- [採用ページ](https://cline.bot/join-us)
- [git-lfs](https://git-lfs.com/)
- [esbuild problem matchers extension](https://marketplace.visualstudio.com/items?itemName=connor4312.esbuild-problem-matchers)
- [Apache 2.0 © 2025 Cline Bot Inc.](https://github.com/cline/cline/blob/main/locales/ja/LICENSE.md)
- [<strong>VS Marketplaceでダウンロード</strong>](https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev)
- [<strong>Discord</strong>](https://discord.gg/cline)
- [<strong>r/cline</strong>](https://www.reddit.com/r/cline/)
- [<strong>機能リクエスト</strong>](https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop)
- [<strong>採用情報</strong>](https://cline.bot/join-us)

--- locales/ko/README.md ---
# Cline

<p align="center">
    <img src="https://media.githubusercontent.com/media/cline/cline/main/assets/docs/demo.gif" width="100%" />
</p>

<div align="center">
<table>
<tbody>
<td align="center">
<a href="https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev" target="_blank"><strong>VS Marketplace에서 다운로드</strong></a>
</td>
<td align="center">
<a href="https://discord.gg/cline" target="_blank"><strong>Discord</strong></a>
</td>
<td align="center">
<a href="https://www.reddit.com/r/cline/" target="_blank"><strong>r/cline</strong></a>
</td>
<td align="center">
<a href="https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop" target="_blank"><strong>기능 요청</strong></a>
</td>
<td align="center">
<a href="https://cline.bot/join-us" target="_blank"><strong>채용 정보</strong></a>
</td>
</tbody>
</table>
</div>

Cline을 만나보세요, **CLI** 및 **에디터**를 활용할 수 있는 AI 어시스턴트입니다.

[Claude 4 Sonnet의 에이전트형 코딩 기능](https://www.anthropic.com/claude/sonnet) 덕분에, Cline은 복잡한 소프트웨어 개발 작업을 단계별로 처리할 수 있습니다. 파일 생성과 편집, 대규모 프로젝트 탐색, 브라우저 사용, 터미널 명령 실행(권한 허가 필요) 등의 도구를 사용하여 단순 코드 완성이나 기술 지원을 넘어서는 도움을 제공합니다. Cline은 Model Context Protocol(MCP)를 사용하여 새로운 도구를 만들고 자신의 기능을 확장할 수도 있습니다. 자율적인 AI 스크립트는 일반적으로 샌드박스 환경에서 실행되지만, 이 확장 프로그램은 모든 파일 변경 및 터미널 명령을 승인할 수 있는 사람이 개입가능한 GUI를 제공하여, 에이전트형 AI의 잠재력을 보다 안전하고 쉽게 탐색할 수 있도록 합니다.

1. 작업을 입력하고, 목업을 기능하는 앱으로 변환하거나 스크린샷으로 버그를 수정합니다.
2. Cline은 파일 구조와 소스코드 AST의 분석, 정규식 검색 실행, 관련 파일 읽기부터 시작하여 기존 프로젝트를 파악합니다. 또한, 어떤 정보를 컨텍스트에 추가할지를 신중하게 관리하여, 대규모 복잡한 프로젝트에서도 컨텍스트 윈도우를 과부하시키지 않으면서도 효과적인 지원을 제공합니다.
3. Cline이 필요한 정보를 얻은 후 다음과 같은 작업을 할 수 있습니다:
    - 파일 생성과 편집 + 린터/컴파일러 오류 모니터링을 수행하여 누락된 임포트나 구문 오류 등의 문제를 자동으로 수정합니다.
    - 터미널에서 명령을 직접 실행하고 작업 중에 출력을 모니터링합니다. 이를 통해 파일 편집 후 개발 서버의 문제에 대응할 수 있습니다.
    - 웹 개발 작업에서는 헤드리스 브라우저로 사이트를 실행하고, 클릭, 입력, 스크롤, 스크린샷과 콘솔 로그 캡처를 수행하여 런타임 오류나 시각적 버그를 수정합니다.
4. 작업이 완료되면 Cline은 `open -a "Google Chrome" index.html`과 같은 터미널 명령을 제공하여 버튼 클릭 한 번으로 결과를 확인할 수 있도록 합니다.

> [!TIP]
> `CMD/CTRL + Shift + P` 단축키를 사용하여 명령 팔레트를 열고 "Cline: Open In New Tab"을 입력하여 에디터의 탭으로 확장 프로그램을 엽니다. 이를 통해 파일 탐색기와 병행하여 Cline을 사용하고 워크스페이스의 변경을 더 명확하게 확인할 수 있습니다.

---

<img align="right" width="340" src="https://github.com/user-attachments/assets/3cf21e04-7ce9-4d22-a7b9-ba2c595e88a4">

### 어떤 API나 모델이든 사용 가능

Cline은 OpenRouter, Anthropic, OpenAI, Google Gemini, AWS Bedrock, Azure, GCP Vertex 등의 API 제공자를 지원합니다. 또한 OpenAI 호환 API를 설정하거나 LM Studio/Ollama를 통해 로컬 모델을 사용할 수도 있습니다. OpenRouter를 사용하는 경우, 확장 프로그램에서 최신 모델 목록을 가져와 바로 최신 모델을 사용할 수 있게 합니다.

또한, Cline은 전체 작업 루프와 개별 요청별로 토큰 사용량과 API 비용을 추적하여, 진행 중인 작업의 비용을 실시간으로 확인할 수 있도록 도와줍니다.

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="370" src="https://github.com/user-attachments/assets/81be79a8-1fdb-4028-9129-5fe055e01e76">

### 터미널에서 명령 실행

VSCode v1.93의 새로운 [셸 통합 업데이트](https://code.visualstudio.com/updates/v1_93#_terminal-shell-integration-api) 덕분에, Cline은 터미널에서 명령을 직접 실행하고 출력을 받을 수 있습니다. 이를 통해 패키지 설치나 빌드 스크립트 실행부터 애플리케이션 배포, 데이터베이스 관리, 테스트 실행까지 광범위한 작업을 수행할 수 있습니다. Cline은 개발 환경과 도구 체인에 맞추어 정확하게 작업을 실행합니다.

개발 서버와 같은 오래 실행되는 프로세스의 경우, "실행 중 계속"(Proceed While Running) 버튼을 사용하여 명령이 백그라운드에서 실행되는 동안 Cline이 작업을 계속할 수 있게 합니다. 작업이 진행되는 동안 Cline은 새로운 터미널 출력을 실시간으로 확인하여, 파일 편집 시 발생하는 컴파일 오류와 같은 문제에 즉시 대응할 수 있습니다.

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="400" src="https://github.com/user-attachments/assets/c5977833-d9b8-491e-90f9-05f9cd38c588">

### 파일 생성과 편집

Cline은 에디터 내에서 파일을 생성 및 편집하고 변경의 Diff 뷰로 표시합니다. Diff 뷰 에디터에서 Cline의 변경을 직접 편집하거나 되돌릴 수 있으며, 채팅에서 피드백을 제공하여 만족할 때까지 개선 요청할 수 있습니다. Cline은 린터/컴파일러 오류(누락된 임포트, 구문 오류 등)도 모니터링하고 발생한 문제를 자동으로 수정합니다.

Cline에 의한 모든 변경은 파일의 타임라인에 기록되어 필요할 때 변경을 추적하고 되돌릴 수 있는 간단한 방법을 제공합니다.


<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="370" src="https://github.com/user-attachments/assets/bc2e85ba-dfeb-4fe6-9942-7cfc4703cbe5">

### 브라우저 사용

Claude 4 Sonnet의 새로운 [컴퓨터 사용](https://www.anthropic.com/news/3-5-models-and-computer-use) 기능으로 인해, Cline은 브라우저를 실행하고 요소를 클릭하고 텍스트를 입력하고 스크롤하며 각 단계에서 스크린샷과 콘솔 로그를 캡처할 수 있습니다. 이를 통해 인터랙티브한 디버깅, 엔드투엔드 테스트, 심지어 일반적인 웹 탐색까지 가능해집니다. 이로 인해 오류 로그를 수동으로 복사 & 붙여넣기 할 필요 없이 시각적 버그나 런타임 문제를 자율적으로 수정할 수 있습니다.

Cline에게 "앱을 테스트해줘"라고 요청하면, `npm run dev`와 같은 명령을 실행하고 로컬에서 실행 중인 개발 서버를 브라우저에서 실행하여 일련의 테스트를 수행하고 모든 것이 정상적으로 작동하는지 확인합니다. [데모는 여기를 참조하세요.](https://x.com/sdrzn/status/1850880547825823989)

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="350" src="https://github.com/user-attachments/assets/ac0efa14-5c1f-4c26-a42d-9d7c56f5fadd">

### "도구를 추가 해주세요."

Cline은 [Model Context Protocol](https://github.com/modelcontextprotocol)을 활용하여 커스텀 도구를 생성하고 기능을 확장할 수 있습니다. 기존의 [커뮤니티 서버](https://github.com/modelcontextprotocol/servers)를 사용할 수도 있지만, Cline은 사용자의 워크플로우에 최적화된 도구를 직접 제작하고 설치할 수도 있습니다. "~ 도구를 추가해주세요."라고 요청만 하면, Cline은 새로운 MCP 서버 생성부터 확장 프로그램 내 설치까지 모두 자동으로 처리합니다. 이러한 커스텀 도구는 Cline의 툴키트의 일부가 되어 향후 작업에서 사용할 수 있게 됩니다.

- "Jira 티켓을 가져오는 도구를 추가해주세요": 티켓 AC를 가져와 Cline에게 작업을 요청
- "AWS EC2를 관리하는 도구를 추가해주세요": 서버 메트릭을 확인하고 인스턴스를 확장 또는 축소
- "최신 PagerDuty 인시던트를 가져오는 도구를 추가해주세요": 최신 장애 정보를 가져와 Cline에게 버그 수정 요청

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="360" src="https://github.com/user-attachments/assets/7fdf41e6-281a-4b4b-ac19-020b838b6970">

### 컨텍스트 추가

**`@url`：** URL을 붙여넣으면 확장이 해당 페이지를 가져와 Markdown으로 변환합니다. 최신 문서를 Cline에게 제공할 때 유용합니다.

**`@problems`：** Cline이 수정할 워크스페이스 오류와 경고(Problems' panel)를 추가합니다.

**`@file`：** 파일의 내용을 추가하여, 파일을 읽는 데 API 요청을 허비하지 않고도 Cline이 접근할 수 있도록 합니다. (+ 파일 검색 가능)

**`@folder`：** 폴더 내 모든 파일을 한 번에 추가하여 워크플로우를 더욱 빠르게 진행할 수 있습니다.

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="350" src="https://github.com/user-attachments/assets/140c8606-d3bf-41b9-9a1f-4dbf0d4c90cb">

### 체크포인트: 비교 및 복원

Cline이 작업을 진행하는 동안 확장 프로그램은 각 단계에서 워크스페이스의 스냅샷을 저장합니다. “Compare” 버튼을 사용하여 스냅샷과 현재 워크스페이스의 차이를 확인하고, “Restore” 버튼을 사용하여 해당 시점으로 롤백할 수 있습니다.

예를 들어, 로컬 웹 서버에서 작업 중일 때 “Restore Workspace Only”을 사용하여 서로 다른 버전의 앱을 신속하게 테스트하고, “Restore Task and Workspace”을 사용하여 계속 진행할 버전을 찾을 수 있습니다. 이를 통해 진행 상황을 잃지 않고 안전하게 다양한 접근 방식을 실험할 수 있습니다.

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

## 기여

프로젝트에 기여하려면, [기여 가이드](CONTRIBUTING.md)에서 기본 사항을 익히세요. 또한, [Discord](https://discord.gg/cline)에 참여하여 `#contributors` 채널에서 다른 기여자들과 이야기할 수 있습니다. 풀타임 직업을 찾고 있다면, [채용 페이지](https://cline.bot/join-us)에서 열려있는 포지션을 확인하세요.

<details>
<summary>로컬 개발 방법</summary>

1. 리포지토리를 클론합니다 _(Requires [git-lfs](https://git-lfs.com/))_：
        ```bash
        git clone https://github.com/cline/cline.git
        ```
2. 프로젝트를 VSCode에서 엽니다：
        ```bash
        code cline
        ```
3. 확장 프로그램과 webview-gui의 필요한 의존성을 설치합니다：
        ```bash
        npm run install:all
        ```
4. `F5`를 눌러(또는 `Run`->`Start Debugging`), 확장 프로그램이 로드된 새로운 VSCode 창을 엽니다. (프로젝트 빌드에 문제가 있는 경우, [esbuild problem matchers extension](https://marketplace.visualstudio.com/items?itemName=connor4312.esbuild-problem-matchers)을 설치해야 할 수도 있습니다.)

</details>

<details>
<summary>Pull Request 생성 방법</summary>

1. PR을 만들기 전, 변경 사항을 기록하는 changeset 항목을 생성:
    ```bash
    npm run changeset
    ```
   이후 프롬프트에서 다음 정보를 입력하세요:
   - 변경 유형 (major, minor, patch)
     - `major` → 호환되지 않는 변경 (1.0.0 → 2.0.0)
     - `minor` → 새로운 기능 추가 (1.0.0 → 1.1.0)
     - `patch` → 버그 수정 (1.0.0 → 1.0.1)
   - 변경 사항 설명 입력

2. 변경 사항과 생성된 `.changeset` 파일을 커밋 후 브랜치를 푸시하고 GitHub에서 PR을 생성하세요.

3. 브랜치를 푸시하고 GitHub에서 PR을 생성하세요. CI가 다음과 같은 작업을 수행합니다:
   - 테스트 및 코드 검증 실행
   - Changesetbot이 버전 변경 영향을 보여주는 코멘트를 생성
   - 브랜치가 메인에 머지되면, Changesetbot이 버전 패키지 PR을 생성
   - 버전 패키지 PR이 머지되면, 새로운 릴리즈가 게시됨

</details>

## 라이센스

[Apache 2.0 © 2025 Cline Bot Inc.](/LICENSE)


## Links discovered
- [Claude 4 Sonnet의 에이전트형 코딩 기능](https://www.anthropic.com/claude/sonnet)
- [셸 통합 업데이트](https://code.visualstudio.com/updates/v1_93#_terminal-shell-integration-api)
- [컴퓨터 사용](https://www.anthropic.com/news/3-5-models-and-computer-use)
- [데모는 여기를 참조하세요.](https://x.com/sdrzn/status/1850880547825823989)
- [Model Context Protocol](https://github.com/modelcontextprotocol)
- [커뮤니티 서버](https://github.com/modelcontextprotocol/servers)
- [기여 가이드](https://github.com/cline/cline/blob/main/locales/ko/CONTRIBUTING.md)
- [Discord](https://discord.gg/cline)
- [채용 페이지](https://cline.bot/join-us)
- [git-lfs](https://git-lfs.com/)
- [esbuild problem matchers extension](https://marketplace.visualstudio.com/items?itemName=connor4312.esbuild-problem-matchers)
- [Apache 2.0 © 2025 Cline Bot Inc.](https://github.com/cline/cline/blob/main/LICENSE.md)
- [<strong>VS Marketplace에서 다운로드</strong>](https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev)
- [<strong>Discord</strong>](https://discord.gg/cline)
- [<strong>r/cline</strong>](https://www.reddit.com/r/cline/)
- [<strong>기능 요청</strong>](https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop)
- [<strong>채용 정보</strong>](https://cline.bot/join-us)

--- locales/pt-BR/README.md ---
# Cline

<p align="center">
        <img src="https://media.githubusercontent.com/media/cline/cline/main/assets/docs/demo.gif" width="100%" />
</p>

<div align="center">
<table>
<tbody>
<td align="center">
<a href="https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev" target="_blank"><strong>Baixar no VS Marketplace</strong></a>
</td>
<td align="center">
<a href="https://discord.gg/cline" target="_blank"><strong>Discord</strong></a>
</td>
<td align="center">
<a href="https://www.reddit.com/r/cline/" target="_blank"><strong>r/cline</strong></a>
</td>
<td align="center">
<a href="https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop" target="_blank"><strong>Solicitação de Funcionalidades</strong></a>
</td>
<td align="center">
<a href="https://cline.bot/join-us" target="_blank"><strong>Estamos Contratando!</strong></a>
</td>
</tbody>
</table>
</div>

Conheça o Cline: um assistente de IA que pode usar seu **CLI** e **Editor**.

Graças às [habilidades avançadas do Claude 4 Sonnet](https://www.anthropic.com/claude/sonnet), o Cline pode lidar com tarefas complexas de desenvolvimento de software passo a passo. Com ferramentas que permitem criar e editar arquivos, explorar grandes projetos, usar o navegador e executar comandos no terminal (com sua aprovação), ele pode ajudar você de maneiras que vão além da inclusão de código ou suporte técnico. O Cline pode é capaz inclusive de usar o Model Context Protocol (MCP) para criar novas ferramentas e expandir seus próprios recursos. Embora os scripts de IA autônomas tradicionalmente sejam executados em ambientes isolados, esta extensão oferece uma GUI com um humano no circuito para aprovar cada alteração de arquivo e comando de terminal, fornecendo uma maneira segura e acessível de explorar todo o potencial da IA.

1. Insira sua tarefa e adicione imagens para transformar mockups em aplicativos funcionais ou corrigir erros através de capturas de tela.

2. O Cline começará analisando a estrutura do seu arquivo e os ASTs do código-fonte, fazendo pesquisas com Regex e lendo arquivos relevantes para se orientar em projetos existentes. Ao gerenciar cuidadosamente as informações agregadas, o Cline pode fornecer assistência valiosa mesmo em projetos grandes e complexos, sem sobrecarregar a janela de contexto.
3. Assim que ele tiver as informações necessárias, o Cline poderá:
                - Criar e editar arquivos + monitorar erros de Linter/Compilador, para que você possa corrigir proativamente problemas como importações ausentes e erros de sintaxe.
                - Executar comandos diretamente no terminal e monitorar o resultado, para que você possa responder a problemas do servidor de desenvolvimento após editar um arquivo.
                - Para tarefas de desenvolvimento web, o Cline pode iniciar o site em um navegador headless, clicar, digitar, fazer scroll e capturar capturas de tela + registros de console, para que você possa corrigir erros em tempo de execução e erros visuais.

> [!TIP]
> Use o atalho de teclado `CMD/CTRL + Shift + P` para abrir a lista de comandos possiveis e digite "Cline: Abrir em nova aba" para abrir a extensão como uma aba no seu editor. Dessa forma, você pode usar o Cline junto com seu explorador de arquivos e ver mais claramente como seu espaço de trabalho muda.

---

<img align="right" width="340" src="https://github.com/user-attachments/assets/3cf21e04-7ce9-4d22-a7b9-ba2c595e88a4">

### Use qualquer API ou modelo

O Cline oferece suporte a provedores de API como OpenRouter, Anthropic, OpenAI, Google Gemini, AWS Bedrock, Azure e GCP Vertex. Você também pode configurar qualquer API compatível com OpenAI ou usar um modelo local via LM Studio/Ollama. Se você usar o OpenRouter, a extensão recuperará sua lista de modelos mais recentes, para que você possa usar os modelos mais novos assim que estiverem disponíveis.

A extensão também rastreia o uso total de tokens e os custos da API para todo o ciclo de tarefas e solicitações individuais, para que você seja informado sobre as despesas em cada etapa.

<!-- Pixel transparente para criar uma quebra de linha após a imagem flutuante -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="370" src="https://github.com/user-attachments/assets/81be79a8-1fdb-4028-9129-5fe055e01e76">

### Executar comandos no terminal

Graças às novas [atualizações de integração do Shell no VSCode v1.93](https://code.visualstudio.com/updates/v1_93#_terminal-shell-integration-api), o Cline pode executar comandos diretamente no seu terminal e receber o resultado. Isso permite que você execute uma variedade de tarefas, desde instalar pacotes e executar build scripts para fazer deploy de aplicações, gerenciar bancos de dados e executar testes, adaptando-se ao seu ambiente de desenvolvimento e ferramentas para fazer o trabalho corretamente.

Para processos de longa duração, como servidores de desenvolvimento, use o botão "Continuar durante a execução" para permitir que o Cline continue a tarefa enquanto o comando é executado em segundo plano. Enquanto Cline trabalha, você será notificado sobre novas saídas do terminal, para que possa responder a problemas que possam surgir, como erros de compilação ao editar arquivos.

<!-- Pixel transparente para criar uma quebra de linha após a imagem flutuante -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="400" src="https://github.com/user-attachments/assets/c5977833-d9b8-491e-90f9-05f9cd38c588">

### Criar e editar arquivos

Cline pode criar e editar arquivos diretamente no seu editor, apresentando um diff com as alterações. Você pode editar ou reverter as alterações do Cline diretamente no editor de diff ou fornecer feedback no chat até ficar satisfeito com o resultado. Cline também monitora erros de linter/compilador (importações ausentes, erros de sintaxe, etc.) para que possa corrigir problemas que surgem ao longo do caminho por conta própria.

Todas as alterações feitas pelo Cline são registradas na Linha do tempo do arquivo, fornecendo uma maneira fácil de rastrear e reverter modificações, caso seja necessário.

<!-- Pixel transparente para criar uma quebra de linha após a imagem flutuante -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="370" src="https://github.com/user-attachments/assets/bc2e85ba-dfeb-4fe6-9942-7cfc4703cbe5">

### Uso do navegador

Com a nova habilidade de [uso de computador](https://www.anthropic.com/news/3-5-models-and-computer-use) do Claude Sonnet 4, Cline pode abrir um navegador, clicar em elementos, digitar texto e rolar, capturando a tela e logs de console. Isso permite depurar de maneira interativa, testes end-to-end e até mesmo uso geral da web. Isso lhe dá autonomia para solucionar erros visuais e problemas em tempo de execução sem precisar copiar e colar logs dos erros.

Tente pedir a Cline para "testar o aplicativo" e observe enquanto o Cline executa um comando como `npm run dev`, inicia seu servidor de desenvolvimento local em um navegador e executa uma série de testes para confirmar se tudo funciona. [Veja uma demonstração aqui.](https://x.com/sdrzn/status/1850880547825823989)

<!-- Pixel transparente para crear un salto de línea después de la imagen flotante -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="350" src="https://github.com/user-attachments/assets/ac0efa14-5c1f-4c26-a42d-9d7c56f5fadd">

### "adicione uma ferramenta que..."

Graças ao [Model Context Protocol](https://github.com/modelcontextprotocol), o Cline pode expandir seus recursos por meio de ferramentas personalizadas. Embora você possa usar [servidores criados pela comunidade](https://github.com/modelcontextprotocol/servers), Cline pode criar e instalar ferramentas especificamente para seu fluxo de trabalho. Basta pedir ao Cline para "adicionar uma ferramenta" e ele cuidará de tudo, desde a criação de um novo servidor MCP até a instalação na extensão. Essas ferramentas personalizadas se tornam parte do conjunto de ferramentas da Cline e estão prontas para serem usadas em tarefas futuras.

- "adicione uma ferramenta que recupere tickets do Jira": Recupere ACs de tickets e coloque Cline para trabalhar
- "adicione uma ferramenta que gerencie AWS EC2s": verifique as métricas do servidor e aumente ou diminua as instâncias
- "adicione uma ferramenta para recuperar os últimos incidentes do PagerDuty": Recupere detalhes e peça ao Cline para corrigir erros

<!-- Pixel transparente para crear un salto de línea después de la imagen flotante -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="360" src="https://github.com/user-attachments/assets/7fdf41e6-281a-4b4b-ac19-020b838b6970">

### Adicione contexto

**`@url`:** Insira uma URL para a extensão recuperar e converter para Markdown, que é útil quando você deseja fornecer ao Cline documentos mais recentes

**`@problems`:** Adicionar erros e avisos do espaço de trabalho (painel 'Problemas') que o Cline deve corrigir

**`@file`:** Adicione o conteúdo de um arquivo para que você não precise desperdiçar solicitações de API para aprovar a leitura do arquivo (+ para pesquisar arquivos)

**`@folder`:** Adicione arquivos de uma pasta por vez para acelerar ainda mais seu fluxo de trabalho

<!-- Pixel transparente para criar uma quebra de linha após a imagem flutuante -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="350" src="https://github.com/user-attachments/assets/140c8606-d3bf-41b9-9a1f-4dbf0d4c90cb">

### Checkpoints: Comparar e Restaurar

Enquanto Cline trabalha em uma tarefa, a extensão cria um instantâneo de seu espaço de trabalho em cada etapa. Você pode usar o botão "Comparar" para ver a diferença entre o instantâneo e seu espaço de trabalho atual, e o botão "Restaurar" para retornar a esse ponto.

Por exemplo, se estiver trabalhando com um servidor web local, você pode usar 'Restaurar somente o espaço de trabalho' para testar rapidamente diferentes versões do seu aplicativo e, em seguida, 'Restaurar tarefa e espaço de trabalho' quando encontrar a versão na qual deseja continuar trabalhando. Isso permite que você explore diferentes abordagens com segurança sem perder o progresso.

<!-- Pixel transparente para crear un salto de línea después de la imagen flotante -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

## Contribuições

Para contribuir com o projeto, comece com nosso [Guia de Contribuição](CONTRIBUTING.md) para aprender o básico. Você também pode entrar no nosso [Discord](https://discord.gg/cline) para bater papo com outros colaboradores no canal `#contributors`. Se você está procurando um emprego de período integral, confira nossas vagas em aberto na nossa [página de carreiras](https://cline.bot/join-us).

<details>
<summary>Instruções para desenvolvimento local</summary>

1. Clone o repositório _(Necessário [git-lfs](https://git-lfs.com/))_:
                ```bash
                git clone https://github.com/cline/cline.git
                ```
2. Abra o projeto no VSCode:
                ```bash
                code cline
                ```
3. Instale as dependências necessárias para a extensão e webview-gui:
                ```bash
                npm run install:all
                ```
4. Inicie pressionando `F5` (ou `Executar`->`Iniciar Depuração`) para abrir uma nova janela do VSCode com a extensão carregada. (Pode ser necessário instalar a [extensão esbuild problem matchers](https://marketplace.visualstudio.com/items?itemName=connor4312.esbuild-problem-matchers) se você encontrar problemas ao compilar seu projeto.)

</details>

## Licença

[Apache 2.0 © 2025 Cline Bot Inc.](./LICENSE)


## Links discovered
- [habilidades avançadas do Claude 4 Sonnet](https://www.anthropic.com/claude/sonnet)
- [atualizações de integração do Shell no VSCode v1.93](https://code.visualstudio.com/updates/v1_93#_terminal-shell-integration-api)
- [uso de computador](https://www.anthropic.com/news/3-5-models-and-computer-use)
- [Veja uma demonstração aqui.](https://x.com/sdrzn/status/1850880547825823989)
- [Model Context Protocol](https://github.com/modelcontextprotocol)
- [servidores criados pela comunidade](https://github.com/modelcontextprotocol/servers)
- [Guia de Contribuição](https://github.com/cline/cline/blob/main/locales/pt-BR/CONTRIBUTING.md)
- [Discord](https://discord.gg/cline)
- [página de carreiras](https://cline.bot/join-us)
- [git-lfs](https://git-lfs.com/)
- [extensão esbuild problem matchers](https://marketplace.visualstudio.com/items?itemName=connor4312.esbuild-problem-matchers)
- [Apache 2.0 © 2025 Cline Bot Inc.](https://github.com/cline/cline/blob/main/locales/pt-BR/LICENSE.md)
- [<strong>Baixar no VS Marketplace</strong>](https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev)
- [<strong>Discord</strong>](https://discord.gg/cline)
- [<strong>r/cline</strong>](https://www.reddit.com/r/cline/)
- [<strong>Solicitação de Funcionalidades</strong>](https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop)
- [<strong>Estamos Contratando!</strong>](https://cline.bot/join-us)

--- locales/zh-cn/README.md ---
# Cline

<p align="center">
    <img src="https://media.githubusercontent.com/media/cline/cline/main/assets/docs/demo.gif" width="100%" />
</p>

<div align="center">
<table>
<tbody>
<td align="center">
<a href="https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev" target="_blank"><strong>在 VS Marketplace 下载</strong></a>
</td>
<td align="center">
<a href="https://discord.gg/cline" target="_blank"><strong>Discord</strong></a>
</td>
<td align="center">
<a href="https://www.reddit.com/r/cline/" target="_blank"><strong>r/cline</strong></a>
</td>
<td align="center">
<a href="https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop" target="_blank"><strong>功能请求</strong></a>
</td>
<td align="center">
<a href="https://docs.cline.bot/getting-started/for-new-coders" target="_blank"><strong>新手上路</strong></a>
</td>
</tbody>
</table>
</div>

认识 Cline —— 一个可以使用你的 **终端** 和 **编辑器** 的 AI 助手。

得益于 [Claude 4 Sonnet 的代理式编码能力](https://www.anthropic.com/claude/sonnet)，Cline 能够逐步处理复杂的软件开发任务。借助于一系列工具，他可以创建和编辑文件、浏览大型项目、使用浏览器，并在你授权后执行终端命令，从而在代码补全或技术支持之外提供更深入的帮助。Cline 甚至还能使用 Model Context Protocol（MCP）来创建新工具，并扩展自身的能力。虽然传统的自动化 AI 脚本通常运行在沙盒环境中，但这个扩展提供了一个人类参与审核的图形界面（GUI），用于审批每一次文件变更和终端命令，从而为探索代理式 AI 的潜力提供了一种安全且易于使用的方式。

1. 输入你的任务，并添加图片，以将界面原型（mockup）转换为功能应用，或通过截图修复 bug。
2. Cline 会从分析你的文件结构和源代码的抽象语法树（AST）开始，同时执行正则搜索并读取相关文件，以便尽快熟悉项目上下文。通过精细地管理上下文中引入的信息，即使面对大型复杂项目，Cline 也能在不超出上下文窗口限制的前提下提供有效协助。
3. 一旦获取了所需信息，Cline 能够：
   - 创建和编辑文件，并在过程中监控 linter 或编译器错误，主动修复诸如缺少导入、语法错误等问题。
   - 直接在你的终端中执行命令，并在运行过程中监控输出，例如在修改文件后自动响应开发服务器问题。
   - 针对 Web 开发任务，Cline 可以在无头浏览器中打开网站，进行点击、输入、滚动操作，并采集截图与控制台日志，从而修复运行时错误和界面问题。
4. 当任务完成后，Cline 会通过类似 `open -a "Google Chrome" index.html` 的终端命令将结果展示给你，你只需点击按钮即可执行。

> [!TIP]
> 使用 `CMD/CTRL + Shift + P` 快捷键打开命令面板并输入 "Cline: Open In New Tab" 将扩展作为标签在编辑器中打开。这让你可以与文件资源管理器并排使用 Cline，更清楚地看到他如何改变你的工作空间。

---

<img align="right" width="340" src="https://github.com/user-attachments/assets/3cf21e04-7ce9-4d22-a7b9-ba2c595e88a4">

### 使用任何 API 和模型

Cline 支持 OpenRouter、Anthropic、OpenAI、Google Gemini、AWS Bedrock、Azure 和 GCP Vertex 等 API 提供商。你还可以配置任何兼容 OpenAI 的 API，或通过 LM Studio/Ollama 使用本地模型。如果你使用 OpenRouter，扩展会获取他们的最新模型列表，让你在新模型可用时立即使用。

此外，该扩展还会记录整个任务流程中以及每次请求的总 token 数和 API 使用费用，确保你在每一步都能清楚了解花费情况。

<!-- 透明像素以在浮动图像后创建换行 -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="370" src="https://github.com/user-attachments/assets/81be79a8-1fdb-4028-9129-5fe055e01e76">

### 在终端中运行命令

感谢 VSCode v1.93 中的新 [终端 shell 集成更新](https://code.visualstudio.com/updates/v1_93#_terminal-shell-integration-api)，Cline 可以直接在你的终端中执行命令并接收输出。这使他能够执行广泛的任务，从安装包和运行构建脚本到部署应用程序、管理数据库和执行测试，同时适应你的开发环境和工具链以正确完成工作。

对于长时间运行的进程如开发服务器，使用“在运行时继续”按钮让 Cline 在命令后台运行时继续任务。当 Cline 工作时，他会在过程中收到任何新的终端输出通知，让他对可能出现的问题做出反应，例如编辑文件时的编译时错误。

<!-- 透明像素以在浮动图像后创建换行 -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="400" src="https://github.com/user-attachments/assets/c5977833-d9b8-491e-90f9-05f9cd38c588">

### 创建和编辑文件

Cline 可以直接在你的编辑器中创建和编辑文件，向你展示更改的差异视图。你可以直接在差异视图编辑器中编辑或恢复 Cline 的更改，或在聊天中提供反馈，直到你对结果满意。Cline 还会监控 linter/编译器错误（缺少导入、语法错误等），以便他在过程中自行修复出现的问题。

Cline 所做的所有更改都会记录在你的文件时间轴中，提供了一种简单的方法来跟踪和恢复修改（如果需要）。

<!-- 透明像素以在浮动图像后创建换行 -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="370" src="https://github.com/user-attachments/assets/bc2e85ba-dfeb-4fe6-9942-7cfc4703cbe5">

### 使用浏览器

借助 Claude 4 Sonnet 的新 [计算机使用](https://www.anthropic.com/news/3-5-models-and-computer-use) 功能，Cline 可以启动浏览器，点击元素，输入文本和滚动，在每一步捕获截图和控制台日志。这允许进行交互式调试、端到端测试，甚至是一般的网页使用！这使他能够自主修复视觉错误和运行时问题，而无需你亲自操作和复制粘贴错误日志。

试试让 Cline “测试应用程序”，看看他如何运行 `npm run dev` 命令，在浏览器中启动你本地运行的开发服务器，并执行一系列测试以确认一切正常。[在这里查看演示。](https://x.com/sdrzn/status/1850880547825823989)

<!-- 透明像素以在浮动图像后创建换行 -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="350" src="https://github.com/user-attachments/assets/ac0efa14-5c1f-4c26-a42d-9d7c56f5fadd">

### “添加一个工具……”

感谢 [Model Context Protocol](https://github.com/modelcontextprotocol)，Cline 可以通过自定义工具扩展他的能力。虽然你可以使用 [社区制作的服务器](https://github.com/modelcontextprotocol/servers)，但 Cline 可以创建和安装适合你特定工作流程的工具。只需让 Cline “添加一个工具”，他将处理所有事情，从创建新的 MCP 服务器到将其安装到扩展中。这些自定义工具将成为 Cline 工具包的一部分，准备在未来的任务中使用。

- “添加一个获取 Jira 工单的工具”：检索工单 AC 并让 Cline 开始工作
- “添加一个管理 AWS EC2 的工具”：检查服务器指标并上下扩展实例
- “添加一个获取最新 PagerDuty 事件的工具”：获取详细信息并让 Cline 修复错误

<!-- 透明像素以在浮动图像后创建换行 -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="360" src="https://github.com/user-attachments/assets/7fdf41e6-281a-4b4b-ac19-020b838b6970">

### 添加上下文

**`@url`：** 粘贴一个 URL 以供扩展获取并转换为 markdown，当你想给 Cline 提供最新文档时非常有用

**`@problems`：** 添加工作区错误和警告（“问题”面板）以供 Cline 修复

**`@file`：** 添加文件内容，这样你就不必浪费 API 请求批准读取文件（+ 输入以搜索文件）

**`@folder`：** 一次添加文件夹的文件，以进一步加快你的工作流程

<!-- 透明像素以在浮动图像后创建换行 -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="350" src="https://github.com/user-attachments/assets/140c8606-d3bf-41b9-9a1f-4dbf0d4c90cb">

### 检查点：比较和恢复

当 Cline 完成任务时，扩展会在每一步拍摄你的工作区快照。你可以使用“比较”按钮查看快照和当前工作区之间的差异，并使用“恢复”按钮回滚到该点。

例如，当使用本地 Web 服务器时，你可以使用“仅恢复工作区”快速测试应用程序的不同版本，然后在找到要继续构建的版本时使用“恢复任务和工作区”。这让你可以安全地探索不同的方法而不会丢失进度。

<!-- 透明像素以在浮动图像后创建换行 -->

<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

## 贡献

要为项目做出贡献，请从我们的 [贡献指南](CONTRIBUTING.md) 开始，了解基础知识。你还可以加入我们的 [Discord](https://discord.gg/cline) 在 `#contributors` 频道与其他贡献者聊天。如果你正在寻找全职工作，请查看我们在 [招聘页面](https://cline.bot/join-us) 上的开放职位！

<details>
<summary>本地开发说明</summary>

1. 克隆仓库 _(需要 [git-lfs](https://git-lfs.com/))_：
        ```bash
        git clone https://github.com/cline/cline.git
        ```
2. 在 VSCode 中打开项目：
        ```bash
        code cline
        ```
3. 安装扩展和 webview-gui 的必要依赖：
        ```bash
        npm run install:all
        ```
4. 按 `F5`（或 `运行`->`开始调试`）启动以打开一个加载了扩展的新 VSCode 窗口。（如果你在构建项目时遇到问题，可能需要安装 [esbuild problem matchers 扩展](https://marketplace.visualstudio.com/items?itemName=connor4312.esbuild-problem-matchers)）

</details>

## 许可证

[Apache 2.0 © 2025 Cline Bot Inc.](./LICENSE)



## Links discovered
- [Claude 4 Sonnet 的代理式编码能力](https://www.anthropic.com/claude/sonnet)
- [终端 shell 集成更新](https://code.visualstudio.com/updates/v1_93#_terminal-shell-integration-api)
- [计算机使用](https://www.anthropic.com/news/3-5-models-and-computer-use)
- [在这里查看演示。](https://x.com/sdrzn/status/1850880547825823989)
- [Model Context Protocol](https://github.com/modelcontextprotocol)
- [社区制作的服务器](https://github.com/modelcontextprotocol/servers)
- [贡献指南](https://github.com/cline/cline/blob/main/locales/zh-cn/CONTRIBUTING.md)
- [Discord](https://discord.gg/cline)
- [招聘页面](https://cline.bot/join-us)
- [git-lfs](https://git-lfs.com/)
- [esbuild problem matchers 扩展](https://marketplace.visualstudio.com/items?itemName=connor4312.esbuild-problem-matchers)
- [Apache 2.0 © 2025 Cline Bot Inc.](https://github.com/cline/cline/blob/main/locales/zh-cn/LICENSE.md)
- [<strong>在 VS Marketplace 下载</strong>](https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev)
- [<strong>Discord</strong>](https://discord.gg/cline)
- [<strong>r/cline</strong>](https://www.reddit.com/r/cline/)
- [<strong>功能请求</strong>](https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop)
- [<strong>新手上路</strong>](https://docs.cline.bot/getting-started/for-new-coders)

--- locales/zh-tw/README.md ---
<div align="center"><sub>
<a href="https://github.com/cline/cline/blob/main/README.md" target="_blank">English</a> | <a href="https://github.com/cline/cline/blob/main/locales/es/README.md" target="_blank">Español</a> | <a href="https://github.com/cline/cline/blob/main/locales/de/README.md" target="_blank">Deutsch</a> | <a href="https://github.com/cline/cline/blob/main/locales/ja/README.md" target="_blank">日本語</a> | <a href="https://github.com/cline/cline/blob/main/locales/zh-cn/README.md" target="_blank">简体中文</a> | 繁體中文 | <a href="https://github.com/cline/cline/blob/main/locales/ko/README.md" target="_blank">한국어</a>
</sub></div>

# Cline

<p align="center">
    <img src="https://media.githubusercontent.com/media/cline/cline/main/assets/docs/demo.gif" width="100%" />
</p>

<div align="center">
<table>
<tbody>
<td align="center">
<a href="https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev" target="_blank"><strong>從 VS Marketplace 下載</strong></a>
</td>
<td align="center">
<a href="https://discord.gg/cline" target="_blank"><strong>Discord</strong></a>
</td>
<td align="center">
<a href="https://www.reddit.com/r/cline/" target="_blank"><strong>r/cline</strong></a>
</td>
<td align="center">
<a href="https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop" target="_blank"><strong>功能建議</strong></a>
</td>
<td align="center">
<a href="https://docs.cline.bot/getting-started/getting-started-new-coders" target="_blank"><strong>新手上路</strong></a>
</td>
</tbody>
</table>
</div>

認識 Cline，一個可以使用您的**命令列介面** (CLI) 和**程式編輯器** (Editor) 的 AI 助理。

感謝 [Claude 4 Sonnet 的代理式程式設計能力](https://www.anthropic.com/claude/sonnet)，Cline 能夠逐步處理複雜的軟體開發任務。透過能讓他建立和編輯檔案、探索大型專案、使用瀏覽器，以及執行終端機指令（在您授權後）的工具，從而在程式碼補全或技術支援之外提供更深入的協助。Cline 甚至能使用模型上下文協定（Model Context Protocol，MCP）來建立新工具並擴展自己的功能。雖然自主 AI 腳本傳統上會在沙箱環境中執行，但這個擴充套件提供了人機互動的圖形介面，讓您可以核准每個檔案變更和終端機指令，提供一個安全且容易使用的方式來探索代理式 AI 的潛力。

1. 輸入您的任務，並可以加入圖片來將設計稿轉換成功能性應用程式，或使用截圖來修正錯誤。
2. Cline 會先分析您的檔案結構和程式碼 AST、執行正規表達式搜尋，並讀取相關檔案，以便在現有專案中快速掌握狀況。透過仔細管理加入上下文的資訊，Cline 可以在不超過上下文視窗的情況下，為大型且複雜的專案提供有價值的協助。
3. 一旦 Cline 取得所需資訊後，他可以：
    - 建立和編輯檔案，並在過程中監控程式碼檢查工具/編譯器的錯誤，讓他能主動修正缺少的匯入語句和語法錯誤等問題。
    - 直接在您的終端機中執行指令並監控其輸出，讓他能夠在編輯檔案後回應開發伺服器的問題。
    - 對於網頁開發任務，Cline 可以在無頭瀏覽器中啟動網站、點選、輸入、捲動並擷取螢幕截圖和主控台記錄，讓他能修正執行時錯誤和視覺問題。
4. 當任務完成時，Cline 會以終端機指令（如 `open -a "Google Chrome" index.html`）向您呈現結果，您只需點選按鈕即可執行。

> [!TIP]
> 使用 `CMD/CTRL + Shift + P` 快速鍵開啟命令選擇區，輸入「Cline: Open In New Tab」即可在編輯器中以分頁方式開啟擴充套件。這讓您可以同時檢視檔案總管，並更清楚地看到 Cline 如何變更您的工作區。

---

<img align="right" width="340" src="https://github.com/user-attachments/assets/3cf21e04-7ce9-4d22-a7b9-ba2c595e88a4">

### 使用任何 API 和模型

Cline 支援 OpenRouter、Anthropic、OpenAI、Google Gemini、AWS Bedrock、Azure 和 GCP Vertex 等 API 提供者。您也可以設定任何與 OpenAI 相容的 API，或透過 LM Studio/Ollama 使用本機模型。若您使用 OpenRouter，此擴充套件會擷取他們最新的模型列表，讓您能在新模型推出時立即使用。

此擴充套件也會追蹤整個任務迴圈和個別請求的 token 總數和 API 使用成本，讓您隨時掌握費用支出。

<!-- 透明像素用於浮動圖片後的換行 -->
<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="370" src="https://github.com/user-attachments/assets/81be79a8-1fdb-4028-9129-5fe055e01e76">

### 在終端機中執行指令

感謝 [VSCode v1.93 的終端機整合更新](https://code.visualstudio.com/updates/v1_93#_terminal-shell-integration-api)，Cline 可以直接在您的終端機中執行指令並接收輸出。這讓他能執行各種任務，從安裝套件和執行建置腳本到部署應用程式、管理資料庫和執行測試，同時適應您的開發環境和工具鏈，以正確完成工作。

對於開發伺服器等長時間執行的程序，使用「繼續執行中的程序」按鈕讓 Cline 在指令於背景執行時繼續任務。當 Cline 工作時，他會收到任何新的終端機輸出通知，讓他能回應可能出現的問題，例如編輯檔案時的編譯錯誤。

<!-- 透明像素用於浮動圖片後的換行 -->
<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="400" src="https://github.com/user-attachments/assets/c5977833-d9b8-491e-90f9-05f9cd38c588">

### 建立和編輯檔案

Cline 可以直接在您的編輯器中建立和編輯檔案，並顯示變更的差異檢視。您可以直接在差異檢視編輯器中編輯或還原 Cline 的變更，或在聊天中提供意見回饋，直到您滿意結果為止。Cline 也會監控程式碼檢查工具/編譯器的錯誤（缺少的匯入語句、語法錯誤等），讓他能自行修正過程中出現的問題。

所有 Cline 做的變更都會記錄在您檔案的時間軸中，提供簡單的方式來追蹤和還原修改。

<!-- 透明像素用於浮動圖片後的換行 -->
<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="370" src="https://github.com/user-attachments/assets/bc2e85ba-dfeb-4fe6-9942-7cfc4703cbe5">

### 使用瀏覽器

透過 Claude 4 Sonnet 的新[電腦使用](https://www.anthropic.com/news/3-5-models-and-computer-use)功能，Cline 可以啟動瀏覽器、點選元素、輸入文字和捲動，在每個步驟擷取螢幕截圖和主控台記錄。這讓互動式除錯、端對端測試，甚至一般網頁使用成為可能！這讓他能獨立修正視覺問題和執行時錯誤，而不需要您手動複製錯誤記錄。

試著請 Cline 「測試應用程式」，觀察他如何執行 `npm run dev`、在瀏覽器中啟動您的本機開發伺服器，並執行一系列測試來確認一切正常運作。[點此觀看示範](https://x.com/sdrzn/status/1850880547825823989)。

<!-- 透明像素用於浮動圖片後的換行 -->
<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="350" src="https://github.com/user-attachments/assets/ac0efa14-5c1f-4c26-a42d-9d7c56f5fadd">

### 「新增一個工具來...」

感謝[模型上下文協定](https://github.com/modelcontextprotocol)，Cline 可以透過自訂工具擴展他的功能。雖然您可以使用[社群製作的伺服器](https://github.com/modelcontextprotocol/servers)，但 Cline 可以改為建立專門為您的工作流程量身打造的工具。只要請 Cline 「新增工具」，他就會處理所有事情，從建立新的 MCP 伺服器到將其安裝到擴充套件中。這些自訂工具就會成為 Cline 工具箱的一部分，隨時可用於未來的任務。

- 「新增一個擷取 Jira 工單的工具」：取得工單驗收條件並讓 Cline 開始工作
- 「新增一個管理 AWS EC2 的工具」：檢查伺服器指標並調整執行個體規模
- 「新增一個擷取最新 PagerDuty 事件的工具」：取得詳細資訊並請 Cline 修復錯誤

<!-- 透明像素用於浮動圖片後的換行 -->
<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="left" width="360" src="https://github.com/user-attachments/assets/7fdf41e6-281a-4b4b-ac19-020b838b6970">

### 新增上下文

**`@url`**：貼上網址讓擴充套件擷取並轉換為 Markdown，當您想給 Cline 最新文件時很有用

**`@problems`**：新增工作區的錯誤和警告（「問題」面板）給 Cline 修正

**`@file`**：新增檔案內容，讓您不必浪費 API 請求來核准讀取檔案（+ 輸入以搜尋檔案）

**`@folder`**：一次新增整個資料夾的檔案，讓您的工作流程更快速

<!-- 透明像素用於浮動圖片後的換行 -->
<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

<img align="right" width="350" src="https://github.com/user-attachments/assets/140c8606-d3bf-41b9-9a1f-4dbf0d4c90cb">

### 檢查點：比較和還原

當 Cline 處理任務時，擴充套件會在每個步驟擷取您工作區的快照。您可以使用「比較」按鈕檢視快照與目前工作區的差異，並使用「還原」按鈕回到該時間點。

例如，在使用本機網頁伺服器時，您可以使用「僅還原工作區」來快速測試應用程式的不同版本，然後在找到想要繼續開發的版本時使用「還原任務和工作區」。這讓您能安全地探索不同方法而不會失去進度。

<!-- 透明像素用於浮動圖片後的換行 -->
<img width="2000" height="0" src="https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929"><br>

## 貢獻

要為專案貢獻，請先閱讀我們的[貢獻指南](CONTRIBUTING.md)來了解基礎知識。您也可以加入我們的 [Discord](https://discord.gg/cline)，在 `#contributors` 頻道與其他貢獻者交流。如果您在尋找全職工作，請檢視我們[職涯頁面](https://cline.bot/join-us)上的職缺！

<details>
<summary>本機開發說明</summary>

1. 複製程式碼庫（需要 [git-lfs](https://git-lfs.com/)）：

    ```bash
    git clone https://github.com/cline/cline.git
    ```

2. 在 VSCode 中開啟專案：

    ```bash
    code cline
    ```

3. 安裝擴充套件和網頁介面所需的相依套件：

    ```bash
    npm run install:all
    ```

4. 按下 `F5`（或選擇「執行」->「開始除錯」）來啟動並開啟一個已載入擴充套件的新 VSCode 視窗。（如果建置專案時遇到問題，您可能需要安裝 [esbuild problem matchers 擴充套件](https://marketplace.visualstudio.com/items?itemName=connor4312.esbuild-problem-matchers)）

</details>

<details>
<summary>建立 Pull Request</summary>

1. 在建立 PR 前，產生一個 changeset 項目：

    ```bash
    npm run changeset
    ```

   這會提示您填寫：
   - 變更類型（major、minor、patch）
     - `major` → 重大變更（1.0.0 → 2.0.0）
     - `minor` → 新功能（1.0.0 → 1.1.0）
     - `patch` → 錯誤修正（1.0.0 → 1.0.1）
   - 您的變更說明

2. 提交您的變更和產生的 `.changeset` 檔案

3. 推送您的分支並在 GitHub 上建立 PR。我們的 CI 會：
   - 執行測試和檢查
   - Changesetbot 會建立一個顯示版本影響的評論
   - 當合併到 main 時，changesetbot 會建立一個 Version Packages PR
   - 當 Version Packages PR 合併時，就會發布新版本

</details>

## 授權條款

[Apache 2.0 © 2025 Cline Bot Inc.](./LICENSE)


## Links discovered
- [Claude 4 Sonnet 的代理式程式設計能力](https://www.anthropic.com/claude/sonnet)
- [VSCode v1.93 的終端機整合更新](https://code.visualstudio.com/updates/v1_93#_terminal-shell-integration-api)
- [電腦使用](https://www.anthropic.com/news/3-5-models-and-computer-use)
- [點此觀看示範](https://x.com/sdrzn/status/1850880547825823989)
- [模型上下文協定](https://github.com/modelcontextprotocol)
- [社群製作的伺服器](https://github.com/modelcontextprotocol/servers)
- [貢獻指南](https://github.com/cline/cline/blob/main/locales/zh-tw/CONTRIBUTING.md)
- [Discord](https://discord.gg/cline)
- [職涯頁面](https://cline.bot/join-us)
- [git-lfs](https://git-lfs.com/)
- [esbuild problem matchers 擴充套件](https://marketplace.visualstudio.com/items?itemName=connor4312.esbuild-problem-matchers)
- [Apache 2.0 © 2025 Cline Bot Inc.](https://github.com/cline/cline/blob/main/locales/zh-tw/LICENSE.md)
- [English](https://github.com/cline/cline/blob/main/README.md)
- [Español](https://github.com/cline/cline/blob/main/locales/es/README.md)
- [Deutsch](https://github.com/cline/cline/blob/main/locales/de/README.md)
- [日本語](https://github.com/cline/cline/blob/main/locales/ja/README.md)
- [简体中文](https://github.com/cline/cline/blob/main/locales/zh-cn/README.md)
- [한국어](https://github.com/cline/cline/blob/main/locales/ko/README.md)
- [<strong>從 VS Marketplace 下載</strong>](https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev)
- [<strong>Discord</strong>](https://discord.gg/cline)
- [<strong>r/cline</strong>](https://www.reddit.com/r/cline/)
- [<strong>功能建議</strong>](https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop)
- [<strong>新手上路</strong>](https://docs.cline.bot/getting-started/getting-started-new-coders)

--- locales/ar-sa/CODE_OF_CONDUCT.md ---
# ميثاق المساهمين

## تعهدنا

نحن المساهمون والقائمون على هذا المشروع، نتعهد بتوفير بيئة مفتوحة ومرحبة، ونجعل المشاركة في مشروعنا ومجتمعنا تجربة خالية من التحرش للجميع، بغض النظر عن العمر، أو حجم الجسم، أو الإعاقة، أو العرق، أو الخصائص الجنسية، أو الهوية الجنسية والتعبير عنها، أو مستوى الخبرة، أو التعليم، أو الوضع الاجتماعي والاقتصادي، أو الجنسية، أو المظهر الشخصي، أو الدين، أو الهوية الجنسية والتوجه الجنسي.

## معاييرنا

أمثلة على السلوك الذي يساهم في خلق بيئة إيجابية تشمل:

- استخدام لغة ترحيبية وشاملة
- احترام وجهات النظر والخبرات المختلفة
- تقبل النقد البناء برحابة صدر
- التركيز على ما هو الأفضل للمجتمع
- إظهار التعاطف تجاه أعضاء المجتمع الآخرين

أمثلة على السلوك غير المقبول من قبل المشاركين تشمل:

- استخدام لغة أو صور جنسية والاهتمام الجنسي غير المرغوب فيه أو التحرش الجنسي
- التصيد، والتعليقات المهينة/المسيئة، والهجمات الشخصية أو السياسية
- التحرش العلني أو الخاص
- نشر معلومات الآخرين الخاصة، مثل العنوان الفعلي أو الإلكتروني، دون إذن صريح
- أي سلوك آخر يمكن اعتباره غير لائق في بيئة مهنية

## مسؤولياتنا

يتحمل القائمون على المشروع مسؤولية توضيح معايير السلوك المقبول، ومن المتوقع أن يتخذوا إجراءات تصحيحية مناسبة وعادلة استجابة لأي حالات سلوك غير مقبول.

يحق للقائمين على المشروع إزالة أو تعديل أو رفض التعليقات والالتزامات والتعليمات البرمجية وتعديلات wiki والمشكلات والمساهمات الأخرى التي لا تتماشى مع مدونة قواعد السلوك هذه، أو حظر أي مساهم بشكل مؤقت أو دائم بسبب سلوكيات أخرى يعتبرونها غير لائقة أو مهددة أو مسيئة أو ضارة، كما أنهم يتحملون مسؤولية ذلك.

## النطاق

تنطبق مدونة قواعد السلوك هذه داخل مساحات المشروع وفي الأماكن العامة عندما يمثل الفرد المشروع أو مجتمعه. تتضمن أمثلة تمثيل مشروع أو مجتمع استخدام عنوان بريد إلكتروني رسمي للمشروع، أو النشر عبر حساب رسمي على وسائل التواصل الاجتماعي، أو العمل كممثل معين في حدث عبر الإنترنت أو خارجه. يمكن للقائمين على المشروع تحديد وتوضيح تمثيل المشروع بشكل أكبر.

## التنفيذ

يمكن الإبلاغ عن حالات السلوك المسيء أو التحرش أو السلوك غير المقبول عن طريق الاتصال بفريق المشروع على hi@cline.bot. ستتم مراجعة جميع الشكاوى والتحقيق فيها وستؤدي إلى استجابة تعتبر ضرورية ومناسبة للظروف. يلتزم فريق المشروع بالحفاظ على السرية فيما يتعلق بالمبلغ عن الحادث. يمكن نشر مزيد من التفاصيل حول سياسات التنفيذ المحددة بشكل منفصل.

قد يواجه القائمون على المشروع الذين لا يتبعون أو يفرضون مدونة قواعد السلوك بحسن نية تداعيات مؤقتة أو دائمة على النحو الذي يحدده الأعضاء الآخرون في قيادة المشروع.

## الإسناد

تم اقتباس مدونة قواعد السلوك هذه من [تعهد المساهم][homepage]، الإصدار 1.4، متاح على https://www.contributor-covenant.org/version/1/4/code-of-conduct.html

[homepage]: https://www.contributor-covenant.org

للحصول على إجابات للأسئلة الشائعة حول مدونة قواعد السلوك هذه، راجع https://www.contributor-covenant.org/faq

--- locales/de/CODE_OF_CONDUCT.md ---
# Verhaltenskodex für Mitwirkende

## Unser Versprechen

Im Interesse der Förderung einer offenen und einladenden Umgebung verpflichten wir uns als
Mitwirkende und Betreuer, die Teilnahme an unserem Projekt und unserer
Gemeinschaft zu einer belästigungsfreien Erfahrung für alle zu machen, unabhängig von Alter, Körpergröße,
Behinderung, ethnischer Zugehörigkeit, sexuellen Merkmalen, Geschlechtsidentität und -ausdruck,
Erfahrungsniveau, Bildung, sozioökonomischem Status, Nationalität, persönlichem Erscheinungsbild,
Rasse, Religion oder sexueller Identität und Orientierung.

## Unsere Standards

Beispiele für Verhaltensweisen, die dazu beitragen, eine positive Umgebung zu schaffen, sind:

-   Verwendung einer einladenden und inklusiven Sprache
-   Respekt gegenüber unterschiedlichen Standpunkten und Erfahrungen
-   Konstruktive Annahme von Kritik
-   Fokussierung auf das, was das Beste für die Gemeinschaft ist
-   Empathie gegenüber anderen Mitgliedern der Gemeinschaft zeigen

Beispiele für inakzeptables Verhalten von Teilnehmern sind:

-   Die Verwendung von sexualisierter Sprache oder Bildern und unerwünschte sexuelle Aufmerksamkeit oder Annäherungen
-   Trollen, beleidigende/abwertende Kommentare und persönliche oder politische Angriffe
-   Öffentliche oder private Belästigung
-   Veröffentlichen von privaten Informationen anderer, wie eine physische oder elektronische Adresse,
    ohne ausdrückliche Erlaubnis
-   Andere Verhaltensweisen, die in einem professionellen Umfeld als unangemessen angesehen werden könnten

## Unsere Verantwortlichkeiten

Die Projektbetreuer sind dafür verantwortlich, die Standards für akzeptables Verhalten zu klären
und es wird erwartet, dass sie angemessene und faire Korrekturmaßnahmen als Reaktion auf
jedes Beispiel für inakzeptables Verhalten ergreifen.

Die Projektbetreuer haben das Recht und die Verantwortung, Kommentare, Commits, Code, Wiki-Änderungen, Issues und andere Beiträge zu entfernen, zu bearbeiten oder abzulehnen, die nicht mit diesem Verhaltenskodex übereinstimmen, oder jeden Mitwirkenden vorübergehend oder dauerhaft zu


--- scripts/build-tests.js ---
#!/usr/bin/env node
const { execSync } = require("child_process")
const esbuild = require("esbuild")

const watch = process.argv.includes("--watch")

/**
 * @type {import('esbuild').Plugin}
 */
const esbuildProblemMatcherPlugin = {
	name: "esbuild-problem-matcher",

	setup(build) {
		build.onStart(() => {
			console.log("[watch] build started")
		})
		build.onEnd((result) => {
			result.errors.forEach(({ text, location }) => {
				console.error(`✘ [ERROR] ${text}`)
				console.error(`    ${location.file}:${location.line}:${location.column}:`)
			})
			console.log("[watch] build finished")
		})
	},
}

const srcConfig = {
	bundle: true,
	minify: false,
	sourcemap: true,
	sourcesContent: true,
	logLevel: "silent",
	entryPoints: ["src/packages/**/*.ts"],
	outdir: "out/packages",
	format: "cjs",
	platform: "node",
	define: {
		"process.env.IS_TEST": "true",
	},
	external: ["vscode"],
	plugins: [esbuildProblemMatcherPlugin],
}

async function main() {
	const srcCtx = await esbuild.context(srcConfig)

	if (watch) {
		await srcCtx.watch()
	} else {
		await srcCtx.rebuild()

		await srcCtx.dispose()
	}
}

execSync("tsc -p ./tsconfig.test.json --outDir out", { encoding: "utf-8" })

main().catch((e) => {
	console.error(e)
	process.exit(1)
})


--- scripts/generate-stubs.js ---
const fs = require("fs")
const path = require("path")
const { Project, SyntaxKind } = require("ts-morph")

function traverse(container, output, prefix = "") {
	for (const node of container.getStatements()) {
		const kind = node.getKind()

		if (kind === SyntaxKind.ModuleDeclaration) {
			const name = node.getName().replace(/^['"]|['"]$/g, "")
			var fullPrefix
			if (prefix) {
				fullPrefix = `${prefix}.${name}`
			} else {
				fullPrefix = name
			}
			output.push(`${fullPrefix} = {};`)
			const body = node.getBody()
			if (body && body.getKind() === SyntaxKind.ModuleBlock) {
				traverse(body, output, fullPrefix)
			}
		} else if (kind === SyntaxKind.FunctionDeclaration) {
			const name = node.getName()
			const params = node.getParameters().map((p, i) => sanitizeParam(p.getName(), i))
			const typeNode = node.getReturnTypeNode()
			const returnType = typeNode ? typeNode.getText() : ""
			const ret = mapReturn(returnType)
			output.push(
				`${prefix}.${name} = function(${params.join(", ")}) { console.log('Called stubbed function: ${prefix}.${name}');  ${ret} };`,
			)
		} else if (kind === SyntaxKind.EnumDeclaration) {
			const name = node.getName()
			const members = node.getMembers().map((m) => m.getName())
			output.push(`${prefix}.${name} = { ${members.map((m) => `${m}: 0`).join(", ")} };`)
		} else if (kind === SyntaxKind.VariableStatement) {
			for (const decl of node.getDeclarations()) {
				const name = decl.getName()
				output.push(`${prefix}.${name} = createStub("${prefix}.${name}");`)
			}
		} else if (kind === SyntaxKind.ClassDeclaration) {
			const name = node.getName()
			output.push(
				`${prefix}.${name} = class { constructor(...args) {
  console.log('Constructed stubbed class: new ${prefix}.${name}(', args, ')');
  return createStub(${prefix}.${name});
}};`,
			)
		} else if (kind === SyntaxKind.TypeAliasDeclaration || kind === SyntaxKind.InterfaceDeclaration) {
			//console.log("Skipping", SyntaxKind[kind], node.getName())
			// Skip interfaces and type aliases because they are only used at compile time by typescript.
		} else {
			console.log("Can't handle: ", SyntaxKind[kind])
		}
	}
}

function mapReturn(typeStr) {
	if (!typeStr) {
		return ""
	}
	if (typeStr.includes("void")) {
		return ""
	}
	if (typeStr.includes("string")) {
		return `return '';`
	}
	if (typeStr.includes("number")) {
		return `return 0;`
	}
	if (typeStr.includes("boolean")) {
		return `return false;`
	}
	if (typeStr.includes("[]")) {
		return `return [];`
	}
	if (typeStr.includes("Thenable")) {
		return `return Promise.resolve(null);`
	}
	return `return createStub("unknown");`
}

function sanitizeParam(name, index) {
	return name || `arg${index}`
}

async function main() {
	const inputPath = "node_modules/@types/vscode/index.d.ts"
	const outputPath = "standalone/runtime-files/vscode/vscode-stubs.js"

	const project = new Project()
	const sourceFile = project.addSourceFileAtPath(inputPath)

	const output = []
	output.push("// GENERATED CODE -- DO NOT EDIT!")
	output.push('console.log("Loading stubs...");')
	output.push('const { createStub } = require("./stub-utils")')
	traverse(sourceFile, output)
	output.push("module.exports = vscode;")
	output.push('console.log("Finished loading stubs");')

	fs.mkdirSync(path.dirname(outputPath), { recursive: true })
	fs.writeFileSync(outputPath, output.join("\n"))

	console.log(`Wrote vscode SDK stubs to ${outputPath}`)
}

main().catch((err) => {
	console.error(err)
	process.exit(1)
})


--- scripts/interactive-playwright.ts ---
#!/usr/bin/env npx tsx

/**
 * Interactive Playwright launcher for the Cline VS Code extension.
 *
 * Overview:
 *  - Starts the mock Cline API server (from the e2e test fixtures).
 *  - Downloads a stable build of VS Code (via @vscode/test-electron).
 *  - Creates a temporary VS Code user profile directory.
 *  - Installs and links the Cline extension (from dist/e2e.vsix and the dev path).
 *  - Opens a test workspace and automatically reveals the Cline sidebar.
 *  - Records **all gRPC calls** during the session for later inspection.
 *  - Keeps VS Code running for manual interactive testing until the window is closed or Ctrl+C is pressed.
 *  - Cleans up all resources (mock server, temp profile, Electron process) on exit.
 *
 * Usage:
 *   1. (Optional) Build and install the e2e extension:
 *        npm run test:e2e:build
 *
 *   2. From the repo root, start the interactive session:
 *        npm run test:e2e:ui
 *
 *   3. VS Code will launch with the Cline extension loaded and gRPC recording enabled.
 *
 *   4. Interact with the extension manually.
 *
 *   5. Close the VS Code window or press Ctrl+C to end the session and trigger cleanup.
 */

import { downloadAndUnzipVSCode, SilentReporter } from "@vscode/test-electron"
import { mkdtempSync } from "fs"
import os from "os"
import path from "path"
import { _electron } from "playwright"
import { ClineApiServerMock } from "../src/test/e2e/fixtures/server"
import { E2ETestHelper } from "../src/test/e2e/utils/helpers"

async function main() {
	await ClineApiServerMock.startGlobalServer()

	const userDataDir = mkdtempSync(path.join(os.tmpdir(), "vsce-interactive"))
	const executablePath = await downloadAndUnzipVSCode("stable", undefined, new SilentReporter())

	// launch VSCode
	const app = await _electron.launch({
		executablePath,
		env: {
			...process.env,
			TEMP_PROFILE: "true",
			E2E_TEST: "true",
			CLINE_ENVIRONMENT: "local",
			GRPC_RECORDER_ENABLED: "true",
			GRPC_RECORDER_TESTS_FILTERS_ENABLED: "true",
		},
		args: [
			"--no-sandbox",
			"--disable-updates",
			"--disable-workspace-trust",
			"--disable-extensions",
			"--skip-welcome",
			"--skip-release-notes",
			`--user-data-dir=${userDataDir}`,
			`--install-extension=${path.join(E2ETestHelper.CODEBASE_ROOT_DIR, "dist", "e2e.vsix")}`,
			`--extensionDevelopmentPath=${E2ETestHelper.CODEBASE_ROOT_DIR}`,
			path.join(E2ETestHelper.E2E_TESTS_DIR, "fixtures", "workspace"),
		],
	})

	const page = await app.firstWindow()

	await E2ETestHelper.openClineSidebar(page)

	console.log("VSCode with Cline extension is now running!")
	console.log(`Temporary data directory on: ${userDataDir}`)
	console.log("You can manually interact with the extension.")
	console.log("Press Ctrl+C to close when done.")

	async function teardown() {
		console.log("Cleaning up resources...")
		try {
			await app?.close()
			await ClineApiServerMock.stopGlobalServer?.()
			await E2ETestHelper.rmForRetries(userDataDir, { recursive: true })
		} catch (e) {
			console.log(`We could teardown interactive playwright properly, error:${e}`)
		}
		console.log("Finished cleaning up resources...")
	}

	process.on("SIGINT", async () => {
		await teardown()
		process.exit(0)
	})

	process.on("SIGTERM", async () => {
		await teardown()
		process.exit(0)
	})

	const win = await app.firstWindow()
	win.on("close", async () => {
		console.log("VS Code window closed.")
		await teardown()
		process.exit(0)
	})
	process.stdin.resume()
}

main().catch((err) => {
	console.error("Failed to start:", err)
	process.exit(1)
})


--- scripts/report-issue.js ---
const { execSync } = require("child_process")
const readline = require("readline")
const os = require("os")

const rl = readline.createInterface({
	input: process.stdin,
	output: process.stdout,
})

const ask = (question) => new Promise((resolve) => rl.question(`\n${question}`, resolve))

const getClineVersion = () => {
	try {
		const extensions = execSync("code --list-extensions --show-versions").toString()
		const clineMatch = extensions.match(/claude-dev@(\d+\.\d+\.\d+)/)
		return clineMatch ? clineMatch[1] : "Not installed"
	} catch (_err) {
		return "Error getting version"
	}
}

const collectSystemInfo = () => {
	let cpuInfo = "N/A"
	let memoryInfo = "N/A"
	try {
		if (process.platform === "darwin") {
			cpuInfo = execSync("sysctl -n machdep.cpu.brand_string").toString().trim()
			memoryInfo = execSync("sysctl -n hw.memsize").toString().trim()
			memoryInfo = `${Math.round(parseInt(memoryInfo) / 1e9)} GB RAM`
		} else {
			// Linux specific commands
			cpuInfo = execSync("lscpu").toString().split("\n").slice(0, 5).join("\n")
			memoryInfo = execSync("free -h").toString()
		}
	} catch (_err) {
		// Fallback for unsupported systems
		cpuInfo = Array.from(new Set(os.cpus().map((c) => c.model))).join("\n")
		memoryInfo = `${Math.round(os.totalmem() / 1e9)} GB RAM`
	}

	return {
		cpuInfo,
		memoryInfo,
		os: `${os.arch()}; ${os.version()}`,
		nodeVersion: execSync("node -v").toString().trim(),
		npmVersion: execSync("npm -v").toString().trim(),
		clineVersion: getClineVersion(),
	}
}

const checkGitHubAuth = async () => {
	try {
		execSync("gh auth status", { stdio: "ignore" })
		return true
	} catch (_err) {
		console.log("\nGitHub authentication required.")
		console.log("\nPlease run the following command in your terminal to authenticate:")
		console.log("\n  gh auth login\n")
		console.log("After authenticating, run this script again.")
		return false
	}
}

const createIssueUrl = (systemInfo, issueTitle) => {
	return (
		`https://github.com/cline/cline/issues/new?template=bug_report.yml` +
		`&title=${issueTitle}` +
		`&operating-system=${systemInfo.os}` +
		`&cline-version=${systemInfo.clineVersion}` +
		`&system-info=${
			`Node: ${systemInfo.nodeVersion}\n` +
			`npm: ${systemInfo.npmVersion}\n` +
			`CPU Info: ${systemInfo.cpuInfo}\n` +
			`Free RAM: ${systemInfo.memoryInfo}`
		}`
	)
}

const openUrl = (url) => {
	try {
		switch (process.platform) {
			case "darwin":
				execSync(`open "${url}"`)
				break
			case "win32":
				execSync(`start "" "${url}"`)
				break
			case "linux":
				execSync(`xdg-open "${url}"`)
				break
			default:
				console.log("\nPlease open this URL in your browser:")
				console.log(url)
		}
	} catch (_err) {
		console.log("\nFailed to open URL automatically. Please open this URL in your browser:")
		console.log(url)
	}
}

const submitIssue = async (issueTitle, systemInfo) => {
	try {
		const issueUrl = createIssueUrl(systemInfo, issueTitle)
		console.log("\nOpening GitHub issue creation page in your browser...")
		openUrl(issueUrl)
	} catch (err) {
		console.error("\nFailed to create issue URL:", err.message)
	}
}

async function main() {
	const consent = await ask("Do you consent to collect system data and submit a GitHub issue? (y/n): ")
	if (consent.trim().toLowerCase() !== "y") {
		console.log("\nAborted.")
		rl.close()
		return
	}

	console.log("Collecting system data...")
	const systemInfo = collectSystemInfo()

	const isAuthenticated = await checkGitHubAuth()
	if (!isAuthenticated) {
		rl.close()
		return
	}

	const issueTitle = await ask("Enter the title for your issue: ")

	await submitIssue(issueTitle, systemInfo)
	rl.close()
}

main().catch((err) => {
	console.error("\nAn error occurred:", err)
	rl.close()
})


--- scripts/test-hostbridge-server.ts ---
#!/usr/bin/env npx tsx
import * as grpc from "@grpc/grpc-js"
import { ReflectionService } from "@grpc/reflection"
import * as health from "grpc-health-check"
import * as os from "os"
import { type DiffServiceServer, DiffServiceService } from "../src/generated/grpc-js/host/diff"
import { type EnvServiceServer, EnvServiceService } from "../src/generated/grpc-js/host/env"
import { type TestingServiceServer, TestingServiceService } from "../src/generated/grpc-js/host/testing"
import { type WindowServiceServer, WindowServiceService } from "../src/generated/grpc-js/host/window"
import { type WorkspaceServiceServer, WorkspaceServiceService } from "../src/generated/grpc-js/host/workspace"
import { getPackageDefinition } from "./proto-utils.mjs"

export async function startTestHostBridgeServer() {
	const server = new grpc.Server()

	// Set up health check
	const healthImpl = new health.HealthImplementation({ "": "SERVING" })
	healthImpl.addToServer(server)

	// Add host bridge services using the mock implementations
	server.addService(WorkspaceServiceService, createMockService<WorkspaceServiceServer>("WorkspaceService"))
	server.addService(WindowServiceService, createMockService<WindowServiceServer>("WindowService"))
	server.addService(EnvServiceService, createMockService<EnvServiceServer>("EnvService"))
	server.addService(DiffServiceService, createMockService<DiffServiceServer>("DiffService"))
	server.addService(TestingServiceService, createMockService<TestingServiceServer>("TestingService"))

	// Load package definition for reflection service
	const packageDefinition = await getPackageDefinition()
	// Filter service names to only include host services
	const hostBridgeServiceNames = Object.keys(packageDefinition).filter(
		(name) => name.startsWith("host.") || name.startsWith("grpc.health"),
	)
	const reflection = new ReflectionService(packageDefinition, {
		services: hostBridgeServiceNames,
	})
	reflection.addToServer(server)

	const bindAddress = process.env.HOST_BRIDGE_ADDRESS || `127.0.0.1:26041`

	server.bindAsync(bindAddress, grpc.ServerCredentials.createInsecure(), (err) => {
		if (err) {
			console.error(`Failed to bind test host bridge server to ${bindAddress}:`, err)
			process.exit(1)
		}
		server.start()
		console.log(`Test HostBridge gRPC server listening on ${bindAddress}`)
	})
}

/**
 * Creates a mock gRPC service implementation using Proxy
 * @param serviceName Name of the service for logging
 * @returns A proxy that implements the service interface
 */
function createMockService<T extends grpc.UntypedServiceImplementation>(serviceName: string): T {
	const handler: ProxyHandler<T> = {
		get(_target, prop) {
			// Return a function that handles the gRPC call
			return (call: any, callback: any) => {
				console.log(`Hostbridge: ${serviceName}.${String(prop)} called with:`, call.request)

				// Special cases that need specific return values
				switch (prop) {
					case "getWorkspacePaths":
						const workspaceDir = process.env.TEST_HOSTBRIDGE_WORKSPACE_DIR || "/test-workspace"
						callback(null, {
							paths: [workspaceDir],
						})
						return

					case "getMachineId":
						callback(null, {
							value: "fake-machine-id-" + os.hostname(),
						})
						return

					case "getTelemetrySettings":
						callback(null, {
							isEnabled: 2, // Setting.DISABLED
							errorLevel: "all",
						})
						return

					case "clipboardReadText":
						callback(null, {
							value: "",
						})
						return

					case "getWebviewHtml":
						callback(null, {
							html: "<html><body>Fake Webview</body></html>",
						})
						return

					case "showTextDocument":
						callback(null, {
							document_path: call.request?.path || "",
							view_column: 1,
							is_active: true,
						})
						return

					case "openDiff":
						callback(null, {
							diff_id: "fake-diff-" + Date.now(),
						})
						return

					case "getDocumentText":
						callback(null, {
							content: "",
						})
						return

					case "getOpenTabs":
					case "getVisibleTabs":
					case "showOpenDialogue":
						callback(null, {
							paths: [],
						})
						return

					case "getDiagnostics":
						callback(null, {
							file_diagnostics: [],
						})
						return

					// For streaming methods (like subscribeToTelemetrySettings)
					case "subscribeToTelemetrySettings":
						// Just end the stream immediately
						call.end()
						return
				}

				// Default: return empty object for all other methods
				callback(null, {})
			}
		},
	}

	return new Proxy({} as T, handler)
}

if (require.main === module) {
	startTestHostBridgeServer().catch((err) => {
		console.error("Failed to start test host bridge server:", err)
		process.exit(1)
	})
}


--- scripts/testing-platform-orchestrator.ts ---
#!/usr/bin/env npx tsx
/**
 * Test Orchestrator
 *
 * Automates server lifecycle for running spec files against the standalone server.
 *
 * Prerequisites:
 *   Build standalone first: `npm run compile-standalone`
 *
 * Usage:
 *   - Single file:   `npm run test:tp-orchestrator path/to/spec.json`
 *   - All specs dir: `npm run test:tp-orchestrator tests/specs`
 *
 * Flags:
 *   --server-logs        Show server logs (hidden by default)
 *   --count=<number>     Repeat execution N times (default: 1)
 *   --fix     			  Automatically update spec files with actual responses
 *   --coverage     	  Generate integration test coverage information
 *
 */

import { ChildProcess, spawn } from "child_process"
import fs from "fs"
import minimist from "minimist"
import net from "net"
import path from "path"
import kill from "tree-kill"

let showServerLogs = false
let fix = false
let coverage = false
const WAIT_SERVER_DEFAULT_TIMEOUT = 15000
const usedPorts = new Set<number>()

/**
 * Find an available TCP port within the given range [min, max].
 *
 * - Ports are allocated sequentially (starting at `min`) rather than randomly,
 *   which avoids accidental reuse when running hundreds of tests in a row.
 * - Each successfully allocated port is tracked in `usedPorts` to guarantee
 *   it is never handed out again within the lifetime of this orchestrator.
 * - Before returning, the function binds a temporary server to the port to
 *   verify that the OS really considers it available, then immediately closes it.
 *
 * This approach makes the orchestrator much more robust on CI (e.g. GitHub Actions),
 * where a just-terminated server may leave its socket in TIME_WAIT and cause
 * flakiness if the same port is reallocated too soon.
 */
async function getAvailablePort(min = 20000, max = 49151): Promise<number> {
	return new Promise((resolve, _) => {
		const tryPort = (candidate?: number) => {
			const port = candidate ?? Math.floor(Math.random() * (max - min + 1)) + min
			if (usedPorts.has(port)) {
				// already allocated in this run
				return tryPort()
			}
			const server = net.createServer()
			server.once("error", () => tryPort())
			server.once("listening", () => {
				server.close(() => {
					usedPorts.add(port) // mark reserved
					resolve(port)
				})
			})
			server.listen(port, "127.0.0.1")
		}
		tryPort()
	})
}

// Poll until a given TCP port on a host is accepting connections.
async function waitForPort(port: number, host = "127.0.0.1", timeout = 10000): Promise<void> {
	const start = Date.now()
	const waitForPortSleepMs = 100
	while (Date.now() - start < timeout) {
		await new Promise((res) => setTimeout(res, waitForPortSleepMs))
		try {
			await new Promise<void>((resolve, reject) => {
				const socket = net.connect(port, host, () => {
					socket.destroy()
					resolve()
				})
				socket.on("error", reject)
			})
			return
		} catch {
			// try again
		}
	}
	throw new Error(`Timeout waiting for ${host}:${port}`)
}

async function startServer(): Promise<{ server: ChildProcess; grpcPort: string }> {
	const grpcPort = (await getAvailablePort()).toString()
	const hostbridgePort = (await getAvailablePort()).toString()

	const server = spawn("npx", ["tsx", "scripts/test-standalone-core-api-server.ts"], {
		stdio: showServerLogs ? "inherit" : "pipe",
		env: {
			...process.env,
			PROTOBUS_PORT: grpcPort,
			HOSTBRIDGE_PORT: hostbridgePort,
			USE_C8: coverage ? "true" : "false",
		},
	})

	// Wait for either the server to become ready or fail on spawn error
	await Promise.race([
		waitForPort(Number(grpcPort), "127.0.0.1", WAIT_SERVER_DEFAULT_TIMEOUT),
		new Promise((_, reject) => server.once("error", reject)),
	])

	return { server, grpcPort }
}

function stopServer(server: ChildProcess): Promise<void> {
	return new Promise((resolve) => {
		if (!server.pid) return resolve()

		kill(server.pid, "SIGINT", (err) => {
			if (err) console.warn("Failed to kill server process:", err)
			server.once("exit", () => resolve())
		})
	})
}

function runTestingPlatform(specFile: string, grpcPort: string): Promise<void> {
	return new Promise((resolve, reject) => {
		const testProcess = spawn("npx", ["ts-node", "index.ts", specFile, ...(fix ? ["--fix"] : [])], {
			cwd: path.join(process.cwd(), "testing-platform"),
			stdio: "inherit",
			env: {
				...process.env,
				STANDALONE_GRPC_SERVER_PORT: grpcPort,
			},
		})

		testProcess.once("error", reject)
		testProcess.once("exit", (code) => {
			code === 0 ? resolve() : reject(new Error(`Exit code ${code}`))
		})
	})
}

async function runSpec(specFile: string): Promise<void> {
	const { server, grpcPort } = await startServer()
	try {
		await runTestingPlatform(specFile, grpcPort)
		console.log(`✅ ${path.basename(specFile)} passed`)
	} finally {
		await stopServer(server)
	}
}

function collectSpecFiles(inputPath: string): string[] {
	const fullPath = path.resolve(inputPath)
	if (!fs.existsSync(fullPath)) throw new Error(`Path does not exist: ${fullPath}`)

	const stat = fs.statSync(fullPath)
	if (stat.isDirectory()) {
		return fs
			.readdirSync(fullPath)
			.filter((f) => f.endsWith(".json"))
			.map((f) => path.join(fullPath, f))
	}
	if (fullPath.endsWith(".json")) return [fullPath]
	throw new Error("Spec path must be a JSON file or a folder containing JSON files")
}

async function runAll(inputPath: string, count: number) {
	const specFiles = collectSpecFiles(inputPath)
	if (specFiles.length === 0) {
		console.warn(`⚠️ No spec files found in ${inputPath}`)
		return
	}

	let success = 0
	let failure = 0
	const totalStart = Date.now()

	for (let i = 0; i < count; i++) {
		console.log(`\n🔁 Run #${i + 1} of ${count}`)
		for (const specFile of specFiles) {
			try {
				await runSpec(specFile)
				success++
			} catch (err) {
				console.error(`❌ run #${i + 1}: ${path.basename(specFile)} failed:`, (err as Error).message)
				failure++
			}
		}

		if (failure > 0) process.exitCode = 1
	}

	console.log(`✅ Passed: ${success}`)
	if (failure > 0) console.log(`❌ Failed: ${failure}`)
	console.log(`📋 Total specs: ${specFiles.length} Total runs: ${specFiles.length * count}`)
	console.log(`🏁 All runs completed in ${((Date.now() - totalStart) / 1000).toFixed(2)}s`)
}

async function main() {
	const args = minimist(process.argv.slice(2), { default: { count: 1 } })
	const inputPath = args._[0]
	const count = Number(args.count)
	showServerLogs = Boolean(args["server-logs"])
	fix = Boolean(args["fix"])
	coverage = Boolean(args["coverage"])

	if (!inputPath) {
		console.error(
			"Usage: npx tsx scripts/testing-platform-orchestrator.ts <spec-file-or-folder> [--count=N] [--server-logs] [--fix] [--coverage]",
		)
		process.exit(1)
	}

	await runAll(inputPath, count)
}

if (require.main === module) {
	main().catch((err) => {
		console.error("❌ Fatal error:", err)
		process.exit(1)
	})
}


--- src/core/README.md ---
# Core Architecture

Extension entry point (extension.ts) -> webview -> controller -> task

```tree
core/
├── webview/      # Manages webview lifecycle
├── controller/   # Handles webview messages and task management
├── task/         # Executes API requests and tool operations
└── ...           # Additional components to help with context, parsing user/assistant messages, etc.
```


--- src/exports/README.md ---
# Cline API

The Cline extension exposes an API that can be used by other extensions. To use this API in your extension:

1. Copy `src/extension-api/cline.d.ts` to your extension's source directory.
2. Include `cline.d.ts` in your extension's compilation.
3. Get access to the API with the following code:

    ```ts
    const clineExtension = vscode.extensions.getExtension<ClineAPI>("saoudrizwan.claude-dev")

    if (!clineExtension?.isActive) {
    	throw new Error("Cline extension is not activated")
    }

    const cline = clineExtension.exports

    if (cline) {
    	// Now you can use the API

    	// Start a new task with an initial message
    	await cline.startNewTask("Hello, Cline! Let's make a new project...")

    	// Start a new task with an initial message and images
    	await cline.startNewTask("Use this design language", ["data:image/webp;base64,..."])

    	// Send a message to the current task
    	await cline.sendMessage("Can you fix the @problems?")

    	// Simulate pressing the primary button in the chat interface (e.g. 'Save' or 'Proceed While Running')
    	await cline.pressPrimaryButton()

    	// Simulate pressing the secondary button in the chat interface (e.g. 'Reject')
    	await cline.pressSecondaryButton()
    } else {
    	console.error("Cline API is not available")
    }
    ```

    **Note:** To ensure that the `saoudrizwan.claude-dev` extension is activated before your extension, add it to the `extensionDependencies` in your `package.json`:

    ```json
    "extensionDependencies": [
        "saoudrizwan.claude-dev"
    ]
    ```

For detailed information on the available methods and their usage, refer to the `cline.d.ts` file.


--- src/test/e2e/README.md ---
# E2E Tests

This directory contains the end-to-end tests for the Cline VS Code extension using Playwright. These tests simulate user interactions with the extension in a real VS Code environment.

## Test Structure

The E2E test suite consists of several key components:

### Test Files

- **`auth.test.ts`** - Tests API key setup, provider selection, and navigation to settings
- **`chat.test.ts`** - Tests chat functionality including message sending, mode switching (Plan/Act), slash commands, and @ mentions
- **`diff.test.ts`** - Tests the diff editor functionality for file modifications
- **`editor.test.ts`** - Tests code actions, editor panel integration, and code selection features

### Test Infrastructure

- **`utils/helpers.ts`** - Core test utilities and fixtures including:
  - `e2e` - Main test fixture for single-root workspace tests
  - `e2eMultiRoot` - Test fixture for multi-root workspace tests
  - `E2ETestHelper` - Helper class with utilities for VS Code interaction
- **`utils/common.ts`** - Common utility functions for UI interactions
- **`utils/global.setup.ts`** - Global test setup and cleanup
- **`utils/build.mjs`** - Build script for test environment preparation

### Test Fixtures

- **`fixtures/workspace/`** - Single-root workspace test files (HTML, TypeScript, etc.)
- **`fixtures/workspace_2/`** - Additional workspace with Python provider files
- **`fixtures/multiroots.code-workspace`** - Multi-root workspace configuration
- **`fixtures/server/`** - Mock API server for testing Cline's backend interactions

## Running Tests

### Basic Test Execution

To build the test environment and run all E2E tests:

```bash
npm run test:e2e
```

To run all E2E tests without re-building the test environment (e.g. only test files were updated):

```bash
npm run e2e
```

### Debug Mode

To run E2E tests in debug mode with Playwright's interactive debugger:

```bash
npm run test:e2e -- --debug
# Or only run the tests without re-building
npm run e2e -- --debug
```

In debug mode, Playwright will:
- Open a browser window showing the VS Code instance
- Pause execution at the beginning of each test
- Allow you to step through test actions
- Provide a console for inspecting elements and state

### Additional Options

Run specific test files:
```bash
npm run e2e -- auth.test.ts
```

Run tests with specific tags or patterns:
```bash
npm run e2e -- --grep "Chat"
```

Run tests in headed mode (visible browser):
```bash
npm run e2e -- --headed
```

## Writing Tests

### Basic Test Structure

Use the `e2e` fixture for single-root workspace tests:

```typescript
import { expect } from "@playwright/test"
import { e2e } from "./utils/helpers"

e2e("Test description", async ({ sidebar, helper, page }) => {
  // Sign in to Cline
  await helper.signin(sidebar)
  
  // Test interactions
  const inputbox = sidebar.getByTestId("chat-input")
  await inputbox.fill("Hello, Cline!")
  await sidebar.getByTestId("send-button").click()
  
  // Assertions
  await expect(sidebar.getByText("API Request...")).toBeVisible()
})
```

For multi-root workspace tests, use `e2eMultiRoot`:

```typescript
import { e2eMultiRoot } from "./utils/helpers"

e2eMultiRoot("[Multi-roots] Test description", async ({ sidebar, helper }) => {
  // Test implementation
})
```

### Available Fixtures

The test fixtures provide the following objects:

- **`sidebar`** - Playwright Frame object for the Cline extension's sidebar
- **`helper`** - E2ETestHelper instance with utility methods
- **`page`** - Playwright Page object for the main VS Code window
- **`app`** - ElectronApplication instance for VS Code
- **`server`** - Mock API server for backend testing

### Common Patterns

#### Authentication
```typescript
// Sign in with test API key
await helper.signin(sidebar)
```

#### Chat Interactions
```typescript
const inputbox = sidebar.getByTestId("chat-input")
await inputbox.fill("Your message")
await sidebar.getByTestId("send-button").click()
```

#### Mode Switching
```typescript
const actButton = sidebar.getByRole("switch", { name: "Act" })
const planButton = sidebar.getByRole("switch", { name: "Plan" })
await actButton.click() // Switch to Plan mode
```

#### File Operations
```typescript
// Open file explorer and select code
await openTab(page, "Explorer ")
await page.getByRole("treeitem", { name: "index.html" }).locator("a").click()
await addSelectedCodeToClineWebview(page)
```

#### Settings Navigation
```typescript
await sidebar.getByText("settings").click()
await sidebar.getByTestId("tab-api-config").click()
```

### Using the Recorder with Debug Mode

The `--debug` flag enables Playwright's interactive debugging features:

1. **Start debugging session:**
   ```bash
   npm run test:e2e -- --debug
   ```

2. **Playwright will open:**
   - A VS Code window with Cline extension loaded
   - Playwright Inspector for step-by-step debugging
   - Browser developer tools for element inspection

3. **Recording interactions:**
   - Use the "Record" button in Playwright Inspector
   - Interact with the VS Code interface
   - Playwright generates test code automatically
   - Copy the generated code into your test files

4. **Debugging existing tests:**
   - Set breakpoints in your test code
   - Use the "Step over" button to execute line by line
   - Inspect element selectors and page state
   - Modify selectors and retry actions

### Test Environment

The test environment includes:

- **VS Code Configuration:**
  - Disabled updates, workspace trust, and welcome screens
  - Extension development mode with Cline loaded
  - Temporary user data and extensions directories

- **Mock API Server:**
  - Runs on `http://localhost:7777`
  - Provides mock responses for Cline API calls
  - Supports authentication, chat completions, and user management

- **Test Workspaces:**
  - Single-root workspace with HTML, TypeScript, and README files
  - Multi-root workspace with Python provider examples
  - Configurable through fixtures

### Best Practices

1. **Use semantic selectors:**
   ```typescript
   // Good - uses test IDs
   sidebar.getByTestId("chat-input")
   
   // Good - uses roles and accessible names
   sidebar.getByRole("button", { name: "Send" })
   
   // Avoid - brittle CSS selectors
   sidebar.locator(".chat-input-class")
   ```

2. **Wait for elements:**
   ```typescript
   await expect(sidebar.getByText("Loading...")).toBeVisible()
   await expect(sidebar.getByText("Complete")).toBeVisible()
   ```

3. **Clean up state:**
   ```typescript
   // Use helper functions for common cleanup
   await cleanChatView(page)
   ```

4. **Handle async operations:**
   ```typescript
   // Wait for API responses
   await expect(sidebar.getByText("API Request...")).toBeVisible()
   await expect(sidebar.getByText("Response received")).toBeVisible()
   ```

5. **Test both success and error cases:**
   ```typescript
   // Test successful flow
   await helper.signin(sidebar)
   
   // Test error handling
   await expect(sidebar.getByText("API Request Failed")).toBeVisible()
   ```

### Debugging Tips

- Use `page.pause()` to pause execution and inspect the current state
- Add `console.log()` statements to track test progress
- Use `--headed` flag to see the browser window during test execution
- Check video recordings in `test-results/` for failed tests
- Use browser developer tools to inspect element selectors

### Environment Variables

- `CLINE_E2E_TESTS_VERBOSE=true` - Enable verbose logging
- `CI=true` - Adjusts timeouts and reporting for CI environments
- `GRPC_RECORDER_ENABLED=true` - Enable gRPC recording for debugging


--- src/core/prompts/system-prompt/README.md ---
# System Prompt Architecture

## Overview

The system prompt architecture provides a modular, composable system for building AI assistant prompts. It supports multiple model variants, dynamic component composition, flexible tool configuration, and template-based prompt generation.

## Developer

To generate snapshots for each variants added to the unit test in [src/core/prompts/system-prompt/__tests__/integration.test.ts](./__tests__/integration.test.ts):

```sh
npm run test:unit
```

## Directory Structure

```
src/core/prompts/system-prompt/
├── registry/
│   ├── ClineToolSet.ts            # Tool set management & registry
│   ├── PromptRegistry.ts          # Singleton registry for loading/managing prompts
│   ├── PromptBuilder.ts           # Builds final prompts with template resolution
│   └── utils.ts                   # Model family detection utilities
├── components/                    # Reusable prompt components
│   ├── agent_role.ts             # Agent role and identity section
│   ├── system_info.ts            # System information section
│   ├── mcp.ts                    # MCP servers section  
│   ├── todo.ts                   # Todo management section
│   ├── user_instructions.ts      # User custom instructions
│   ├── tool_use.ts               # Tool usage instructions
│   ├── editing_files.ts          # File editing guidelines
│   ├── capabilities.ts           # Agent capabilities section
│   ├── rules.ts                  # Behavioral rules section
│   ├── objective.ts              # Task objective section
│   ├── act_vs_plan.ts            # Action vs planning mode
│   ├── feedback.ts               # Feedback and improvement section
│   └── index.ts                  # Component registry
├── templates/                    # Template engine and placeholders
│   ├── TemplateEngine.ts         # {{placeholder}} resolution engine
│   └── placeholders.ts           # Standard placeholder definitions
├── tools/                        # Individual tool definitions
│   ├── spec.ts                   # Tool specification interface
│   ├── register.ts               # Tool registration system
│   ├── index.ts                  # Tool exports
│   └── [tool-name].ts            # Individual tool implementations
├── variants/                     # Model-specific prompt variants
│   ├── generic/
│   │   ├── config.ts             # Generic fallback configuration
│   │   └── template.ts           # Base prompt template
│   ├── next-gen/
│   │   ├── config.ts             # Next-gen model configuration
│   │   └── template.ts           # Advanced model template
│   ├── xs/
│   │   ├── config.ts             # Small model configuration
│   │   └── template.ts           # Optimized template
│   └── index.ts                  # Variant registry exports
├── types.ts                      # Core type definitions
└── README.md                     # This documentation
```

## Core Components

### 1. PromptRegistry (Singleton)

The `PromptRegistry` is the central manager for all prompt variants and components. It provides a singleton interface for loading and accessing prompts.

```typescript
class PromptRegistry {
  private static instance: PromptRegistry;
  private variants: Map<string, PromptVariant> = new Map();
  private components: ComponentRegistry = {};
  private loaded: boolean = false;

  static getInstance(): PromptRegistry {
    if (!this.instance) {
      this.instance = new PromptRegistry();
    }
    return this.instance;
  }

  // Load all prompts and components on initialization
  async load(): Promise<void> {
    if (this.loaded) return;
    
    await Promise.all([
      this.loadVariants(),    // Load from variants/ directory
      this.loadComponents()   // Load from components/ directory
    ]);
    
    this.loaded = true;
  }

	/**
	 * Get prompt by model ID with fallback to generic
	 */
	async get(context: SystemPromptContext): Promise<string> {
		await this.load()

		// Try model family fallback (e.g., "claude-4" -> "claude")
		const modelFamily = this.getModelFamily(context.providerInfo)
		const variant = this.variants.get(modelFamily ?? ModelFamily.GENERIC)

		if (!variant) {
			throw new Error(
				`No prompt variant found for model '${context.providerInfo.model.id}' and no generic fallback available`,
			)
		}

		const builder = new PromptBuilder(variant, context, this.components)
		return await builder.build()
	}

  // Get specific version of a prompt
  async getVersion(modelId: string, version: number, context: SystemPromptContext, isNextGenModelFamily?: boolean): Promise<string> {
    // Supports next-gen model family prioritization
  }

  // Get prompt by tag/label
  async getByTag(modelId: string, tag?: string, label?: string, context?: SystemPromptContext, isNextGenModelFamily?: boolean): Promise<string> {
    // Supports tag and label-based retrieval with next-gen prioritization
  }
}
```

### 2. PromptVariant Structure

The `PromptVariant` interface defines the configuration for each model-specific prompt variant:

```typescript
interface PromptVariant {
  id: string;                                    // Model family ID (e.g., "next-gen", "generic")
  version: number;                               // Version number
  family: ModelFamily;                           // Model family enum
  tags: string[];                                // ["production", "beta", "experimental"]
  labels: { [key: string]: number };             // {"staging": 2, "prod": 1}
  description: string;                           // Brief description of the variant

  // Prompt configuration
  config: PromptConfig;                          // Model-specific config
  baseTemplate: string;                          // Main prompt template with placeholders
  componentOrder: SystemPromptSection[];        // Ordered list of components to include
  componentOverrides: { [K in SystemPromptSection]?: ConfigOverride }; // Component customizations
  placeholders: { [key: string]: string };      // Default placeholder values

  // Tool configuration
  tools?: ClineDefaultTool[];                    // Ordered list of tools to include
  toolOverrides?: { [K in ClineDefaultTool]?: ConfigOverride }; // Tool-specific customizations
}

interface PromptConfig {
  modelName?: string;
  temperature?: number;
  maxTokens?: number;
  tools?: ClineToolSpec[];
  [key: string]: any;                            // Additional arbitrary config
}

interface ConfigOverride {
  template?: string;                             // Custom template for the component/tool
  enabled?: boolean;                             // Whether the component/tool is enabled
  order?: number;                                // Override the order
}
```

### 3. PromptBuilder

The `PromptBuilder` orchestrates the construction of the final prompt by combining templates, components, and placeholders:

```typescript
class PromptBuilder {
  private templateEngine: TemplateEngine;

  constructor(
    private variant: PromptVariant,
    private context: SystemPromptContext,
    private components: ComponentRegistry
  ) {
    this.templateEngine = new TemplateEngine();
  }

  async build(): Promise<string> {
    // 1. Build all components in specified order
    const componentSections = await this.buildComponents();
    
    // 2. Prepare all placeholder values
    const placeholderValues = this.preparePlaceholders(componentSections);
    
    // 3. Resolve template placeholders
    const prompt = this.templateEngine.resolve(this.variant.baseTemplate, placeholderValues);
    
    // 4. Apply final post-processing
    return this.postProcess(prompt);
  }

  private async buildComponents(): Promise<Record<string, string>> {
    const sections: Record<string, string> = {};
    
    // Process components sequentially to maintain order
    for (const componentId of this.variant.componentOrder) {
      const componentFn = this.components[componentId];
      if (!componentFn) {
        console.warn(`Warning: Component '${componentId}' not found`);
        continue;
      }

      try {
        const result = await componentFn(this.variant, this.context);
        if (result?.trim()) {
          sections[componentId] = result;
        }
      } catch (error) {
        console.warn(`Warning: Failed to build component '${componentId}':`, error);
      }
    }

    return sections;
  }

  private preparePlaceholders(componentSections: Record<string, string>): Record<string, unknown> {
    const placeholders: Record<string, unknown> = {};

    // Add variant placeholders
    Object.assign(placeholders, this.variant.placeholders);

    // Add standard system placeholders
    placeholders[STANDARD_PLACEHOLDERS.CWD] = this.context.cwd || process.cwd();
    placeholders[STANDARD_PLACEHOLDERS.SUPPORTS_BROWSER] = this.context.supportsBrowserUse || false;
    placeholders[STANDARD_PLACEHOLDERS.MODEL_FAMILY] = this.variant.family;
    placeholders[STANDARD_PLACEHOLDERS.CURRENT_DATE] = new Date().toISOString().split("T")[0];

    // Add all component sections
    Object.assign(placeholders, componentSections);

    // Add runtime placeholders with highest priority
    const runtimePlaceholders = (this.context as any).runtimePlaceholders;
    if (runtimePlaceholders) {
      Object.assign(placeholders, runtimePlaceholders);
    }

    return placeholders;
  }

  private postProcess(prompt: string): string {
    if (!prompt) return "";

    // Combine multiple regex operations for better performance
    return prompt
      .replace(/\n\s*\n\s*\n/g, "\n\n")     // Remove multiple consecutive empty lines
      .trim()                                // Remove leading/trailing whitespace
      .replace(/====+\s*$/, "")             // Remove trailing ==== after trim
      .replace(/\n====+\s*\n+\s*====+\n/g, "\n====\n") // Remove empty sections between separators
      .replace(/====\n([^\n])/g, "====\n\n$1")          // Ensure proper section separation
      .replace(/([^\n])\n====/g, "$1\n\n====");
  }
}
```

### 4. Template System

The template system uses `{{PLACEHOLDER}}` syntax for dynamic content injection:

```typescript
class TemplateEngine {
  resolve(template: string, placeholders: Record<string, unknown>): string {
    return template.replace(/\{\{([^}]+)\}\}/g, (match, key) => {
      const trimmedKey = key.trim();
      
      // Support nested object access using dot notation
      const value = this.getNestedValue(placeholders, trimmedKey);
      
      if (value !== undefined && value !== null) {
        return typeof value === "string" ? value : JSON.stringify(value);
      }
      
      // Keep placeholder if not found (allows for partial resolution)
      return match;
    });
  }

  extractPlaceholders(template: string): string[] {
    const placeholders: string[] = [];
    const regex = /\{\{([^}]+)\}\}/g;
    let match: RegExpExecArray | null = null;

    match = regex.exec(template);
    while (match !== null) {
      const placeholder = match[1].trim();
      if (!placeholders.includes(placeholder)) {
        placeholders.push(placeholder);
      }
      match = regex.exec(template);
    }

    return placeholders;
  }
}
```

**Base Template Example:**
```markdown
You are Cline, a highly skilled software engineer...

====

{{TOOL_USE_SECTION}}

====

{{MCP_SECTION}}

====

{{USER_INSTRUCTIONS_SECTION}}

====

{{SYSTEM_INFO_SECTION}}

====

{{TODO_SECTION}}
```

### 5. Component System

Components are reusable functions that generate specific sections of the prompt:

```typescript
type ComponentFunction = (
  variant: PromptVariant, 
  context: SystemPromptContext
) => Promise<string | undefined>;

// Example component
export async function getSystemInfo(
  variant: PromptVariant,
  context: SystemPromptContext,
): Promise<string> {
  const info = await getSystemEnv();

  // Support component overrides
  const template = variant.componentOverrides?.SYSTEM_INFO_SECTION?.template || `
Operating System: {{os}}
Default Shell: {{shell}}
Home Directory: {{homeDir}}
Current Working Directory: {{workingDir}}
  `;

  return new TemplateEngine().resolve(template, {
    os: info.os,
    shell: info.shell,
    homeDir: info.homeDir,
    workingDir: info.workingDir
  });
}
```

### 6. Tool System

Tools are managed through the `ClineToolSet` and can be configured per variant:

```typescript
class ClineToolSet {
  private static variants: Map<ModelFamily, Set<ClineToolSet>> = new Map();

  static register(config: ClineToolSpec): ClineToolSet {
    return new ClineToolSet(config.id, config);
  }

  static getTools(variant: ModelFamily): ClineToolSet[] {
    const toolsSet = ClineToolSet.variants.get(variant) || new Set();
    const defaultSet = ClineToolSet.variants.get(ModelFamily.GENERIC) || new Set();
    return toolsSet ? Array.from(toolsSet) : Array.from(defaultSet);
  }
}

// Tool generation in PromptBuilder
public static async getToolsPrompts(variant: PromptVariant, context: SystemPromptContext) {
  const tools = ClineToolSet.getTools(variant.family);
  
  // Filter and sort tools based on variant configuration
  const enabledTools = tools.filter((tool) => 
    !tool.config.contextRequirements || tool.config.contextRequirements(context)
  );

  let sortedEnabledTools = enabledTools;
  if (variant?.tools?.length) {
    const toolOrderMap = new Map(variant.tools.map((id, index) => [id, index]));
    sortedEnabledTools = enabledTools.sort((a, b) => {
      const orderA = toolOrderMap.get(a.config.id);
      const orderB = toolOrderMap.get(b.config.id);
      
      if (orderA !== undefined && orderB !== undefined) {
        return orderA - orderB;
      }
      if (orderA !== undefined) return -1;
      if (orderB !== undefined) return 1;
      return a.config.id.localeCompare(b.config.id);
    });
  }

  const ids = sortedEnabledTools.map((tool) => tool.config.id);
  return Promise.all(sortedEnabledTools.map((tool) => PromptBuilder.tool(tool.config, ids)));
}
```

## Configuration Examples

### Basic Variant Configuration (Using Builder Pattern)

```typescript
// variants/generic/config.ts
import { ModelFamily } from "@/shared/prompts";
import { ClineDefaultTool } from "@/shared/tools";
import { SystemPromptSection } from "../../templates/placeholders";
import { validateVariant } from "../../validation/VariantValidator";
import { createVariant } from "../builder";
import { baseTemplate } from "./template";

// Type-safe variant configuration using the builder pattern
export const config = createVariant(ModelFamily.GENERIC)
  .description("The fallback prompt for generic use cases and models.")
  .version(1)
  .tags("fallback", "stable")
  .labels({
    stable: 1,
    fallback: 1,
  })
  .template(baseTemplate)
  .components(
    SystemPromptSection.AGENT_ROLE,
    SystemPromptSection.TOOL_USE,
    SystemPromptSection.MCP,
    SystemPromptSection.EDITING_FILES,
    SystemPromptSection.ACT_VS_PLAN,
    SystemPromptSection.TODO,
    SystemPromptSection.CAPABILITIES,
    SystemPromptSection.RULES,
    SystemPromptSection.SYSTEM_INFO,
    SystemPromptSection.OBJECTIVE,
    SystemPromptSection.USER_INSTRUCTIONS,
  )
  .tools(
    ClineDefaultTool.BASH,
    ClineDefaultTool.FILE_READ,
    ClineDefaultTool.FILE_NEW,
    ClineDefaultTool.FILE_EDIT,
    ClineDefaultTool.SEARCH,
    ClineDefaultTool.LIST_FILES,
    ClineDefaultTool.LIST_CODE_DEF,
    ClineDefaultTool.BROWSER,
    ClineDefaultTool.MCP_USE,
    ClineDefaultTool.MCP_ACCESS,
    ClineDefaultTool.ASK,
    ClineDefaultTool.ATTEMPT,
    ClineDefaultTool.NEW_TASK,
    ClineDefaultTool.PLAN_MODE,
    ClineDefaultTool.MCP_DOCS,
    ClineDefaultTool.TODO,
  )
  .placeholders({
    MODEL_FAMILY: "generic",
  })
  .config({})
  .build();

// Compile-time validation
const validationResult = validateVariant({ ...config, id: "generic" }, { strict: true });
if (!validationResult.isValid) {
  console.error("Generic variant configuration validation failed:", validationResult.errors);
  throw new Error(`Invalid generic variant configuration: ${validationResult.errors.join(", ")}`);
}

// Export type information for better IDE support
export type GenericVariantConfig = typeof config;
```

### Advanced Variant with Overrides (Using Builder Pattern)

```typescript
// variants/next-gen/config.ts
import { ModelFamily } from "@/shared/prompts";
import { ClineDefaultTool } from "@/shared/tools";
import { SystemPromptSection } from "../../templates/placeholders";
import { validateVariant } from "../../validation/VariantValidator";
import { createVariant } from "../builder";
import { baseTemplate, rules_template } from "./template";

// Type-safe variant configuration using the builder pattern
export const config = createVariant(ModelFamily.NEXT_GEN)
  .description("Prompt tailored to newer frontier models with smarter agentic capabilities.")
  .version(1)
  .tags("next-gen", "advanced", "production")
  .labels({
    stable: 1,
    production: 1,
    advanced: 1,
  })
  .template(baseTemplate)
  .components(
    SystemPromptSection.AGENT_ROLE,
    SystemPromptSection.TOOL_USE,
    SystemPromptSection.MCP,
    SystemPromptSection.EDITING_FILES,
    SystemPromptSection.ACT_VS_PLAN,
    SystemPromptSection.TODO,
    SystemPromptSection.CAPABILITIES,
    SystemPromptSection.FEEDBACK,  // Additional component for next-gen
    SystemPromptSection.RULES,
    SystemPromptSection.SYSTEM_INFO,
    SystemPromptSection.OBJECTIVE,
    SystemPromptSection.USER_INSTRUCTIONS,
  )
  .tools(
    ClineDefaultTool.BASH,
    ClineDefaultTool.FILE_READ,
    ClineDefaultTool.FILE_NEW,
    ClineDefaultTool.FILE_EDIT,
    ClineDefaultTool.SEARCH,
    ClineDefaultTool.LIST_FILES,
    ClineDefaultTool.LIST_CODE_DEF,
    ClineDefaultTool.BROWSER,
    ClineDefaultTool.WEB_FETCH,  // Additional tool for next-gen
    ClineDefaultTool.MCP_USE,
    ClineDefaultTool.MCP_ACCESS,
    ClineDefaultTool.ASK,
    ClineDefaultTool.ATTEMPT,
    ClineDefaultTool.NEW_TASK,
    ClineDefaultTool.PLAN_MODE,
    ClineDefaultTool.MCP_DOCS,
    ClineDefaultTool.TODO,
  )
  .placeholders({
    MODEL_FAMILY: ModelFamily.NEXT_GEN,
  })
  .config({})
  // Override the RULES component with custom template
  .overrideComponent(SystemPromptSection.RULES, {
    template: rules_template,
  })
  .build();

// Compile-time validation
const validationResult = validateVariant({ ...config, id: "next-gen" }, { strict: true });
if (!validationResult.isValid) {
  console.error("Next-gen variant configuration validation failed:", validationResult.errors);
  throw new Error(`Invalid next-gen variant configuration: ${validationResult.errors.join(", ")}`);
}

// Export type information for better IDE support
export type NextGenVariantConfig = typeof config;
```

### Compact Variant with Component Overrides

```typescript
// variants/xs/config.ts
import { ModelFamily } from "@/shared/prompts";
import { ClineDefaultTool } from "@/shared/tools";
import { SystemPromptSection } from "../../templates/placeholders";
import { validateVariant } from "../../validation/VariantValidator";
import { createVariant } from "../builder";
import { xsComponentOverrides } from "./overrides";
import { baseTemplate } from "./template";

// Type-safe variant configuration using the builder pattern
export const config = createVariant(ModelFamily.XS)
  .description("Prompt for models with a small context window.")
  .version(1)
  .tags("local", "xs", "compact")
  .labels({
    stable: 1,
    production: 1,
    advanced: 1,
  })
  .template(baseTemplate)
  .components(
    SystemPromptSection.AGENT_ROLE,
    SystemPromptSection.RULES,
    SystemPromptSection.ACT_VS_PLAN,
    SystemPromptSection.CAPABILITIES,
    SystemPromptSection.EDITING_FILES,
    SystemPromptSection.OBJECTIVE,
    SystemPromptSection.SYSTEM_INFO,
    SystemPromptSection.USER_INSTRUCTIONS,
  )
  .tools(
    ClineDefaultTool.BASH,
    ClineDefaultTool.FILE_READ,
    ClineDefaultTool.FILE_NEW,
    ClineDefaultTool.FILE_EDIT,
    ClineDefaultTool.SEARCH,
    ClineDefaultTool.LIST_FILES,
    ClineDefaultTool.ASK,
    ClineDefaultTool.ATTEMPT,
    ClineDefaultTool.NEW_TASK,
    ClineDefaultTool.PLAN_MODE,
    ClineDefaultTool.MCP_USE,
    ClineDefaultTool.MCP_ACCESS,
    ClineDefaultTool.MCP_DOCS,
  )
  .placeholders({
    MODEL_FAMILY: ModelFamily.XS,
  })
  .config({})
  .build();

// Apply component overrides after building the base configuration
// This is necessary because the builder pattern doesn't support bulk overrides
Object.assign(config.componentOverrides, xsComponentOverrides);

// Compile-time validation
const validationResult = validateVariant({ ...config, id: "xs" }, { strict: true });
if (!validationResult.isValid) {
  console.error("XS variant configuration validation failed:", validationResult.errors);
  throw new Error(`Invalid XS variant configuration: ${validationResult.errors.join(", ")}`);
}

// Export type information for better IDE support
export type XsVariantConfig = typeof config;
```

### VariantBuilder API Reference

The `VariantBuilder` class provides a fluent, type-safe API for creating variant configurations:

```typescript
import { createVariant } from "../VariantBuilder";

const config = createVariant(ModelFamily.GENERIC)
  .description("Brief description of this variant")  // Required
  .version(1)                                        // Required, defaults to 1
  .tags("tag1", "tag2", "tag3")                     // Optional, can be chained
  .labels({ stable: 1, production: 1 })             // Optional
  .template(baseTemplate)                           // Required
  .components(                                      // Required, type-safe component selection
    SystemPromptSection.AGENT_ROLE,
    SystemPromptSection.TOOL_USE,
    // ... more components
  )
  .tools(                                          // Optional, type-safe tool selection
    ClineDefaultTool.BASH,
    ClineDefaultTool.FILE_READ,
    // ... more tools
  )
  .placeholders({                                  // Optional
    MODEL_FAMILY: "generic",
    CUSTOM_PLACEHOLDER: "value",
  })
  .config({                                        // Optional, model-specific config
    temperature: 0.7,
    maxTokens: 4096,
  })
  .overrideComponent(SystemPromptSection.RULES, {  // Optional, component overrides
    template: customRulesTemplate,
  })
  .overrideTool(ClineDefaultTool.BASH, {          // Optional, tool overrides
    enabled: false,
  })
  .build();                                       // Returns Omit<PromptVariant, "id">
```

## Usage Examples

### Basic Usage

```typescript
// Initialize registry (done once at startup)
const registry = PromptRegistry.getInstance();
await registry.load();

// Get prompt for specific model
const prompt = await registry.get("claude-3-5-sonnet-20241022", context);

// Get prompt for next-gen model (automatically detects model family)
const prompt = await registry.get("claude-4-20250101", context);
```

### Version and Tag-based Retrieval

```typescript
// Get specific version
const prompt = await registry.getVersion("next-gen", 2, context);

// Get by tag/label with next-gen prioritization
const prompt = await registry.getByTag("claude-4", "production", undefined, context, true);

// Get by label
const prompt = await registry.getByTag("generic", undefined, "stable", context);
```

### Runtime Placeholder Resolution

```typescript
// Add runtime placeholders to context
context.runtimePlaceholders = {
  "USER_NAME": "John",
  "PROJECT_TYPE": "React",
  "CUSTOM_INSTRUCTION": "Focus on TypeScript best practices"
};

const prompt = await registry.get("next-gen", context);
```

## Model Family Detection

The system automatically detects model families based on model IDs:

```typescript
function getModelFamily(modelId: string): ModelFamily {
  // Check for next-gen models first
  if (isNextGenModel(modelId)) {
    return ModelFamily.NEXT_GEN;
  }
  
  if (modelId.includes("qwen")) {
    return ModelFamily.XS;
  }
  
  // Default fallback
  return ModelFamily.GENERIC;
}

function isNextGenModel(modelId: string): boolean {
  return (
    isClaude4ModelFamily(mockApiHandlerModel) ||
    isGemini2dot5ModelFamily(mockApiHandlerModel) ||
    isGrok4ModelFamily(mockApiHandlerModel) ||
    isGPT5ModelFamily(mockApiHandlerModel)
  );
}
```

## Available Components

The system includes the following built-in components:

- `AGENT_ROLE_SECTION`: Agent identity and role definition
- `TOOL_USE_SECTION`: Tool usage instructions and available tools
- `MCP_SECTION`: MCP server information and capabilities
- `EDITING_FILES_SECTION`: File editing guidelines and best practices
- `ACT_VS_PLAN_SECTION`: Action vs planning mode instructions
- `TODO_SECTION`: Todo management and task tracking
- `CAPABILITIES_SECTION`: Agent capabilities and limitations
- `FEEDBACK_SECTION`: Feedback and improvement instructions (next-gen only)
- `RULES_SECTION`: Behavioral rules and constraints
- `SYSTEM_INFO_SECTION`: System environment information
- `OBJECTIVE_SECTION`: Current task objective
- `USER_INSTRUCTIONS_SECTION`: User-provided custom instructions

## Available Tools

The system supports the following tools (mapped to `ClineDefaultTool` enum):

- `BASH`: Execute shell commands
- `FILE_READ`: Read file contents
- `FILE_NEW`: Create new files
- `FILE_EDIT`: Edit existing files
- `SEARCH`: Search through files
- `LIST_FILES`: List directory contents
- `LIST_CODE_DEF`: List code definitions
- `BROWSER`: Browser automation (conditional)
- `WEB_FETCH`: Web content fetching (next-gen only)
- `MCP_USE`: Use MCP tools
- `MCP_ACCESS`: Access MCP resources
- `ASK`: Ask follow-up questions
- `ATTEMPT`: Attempt task completion
- `NEW_TASK`: Create new tasks
- `PLAN_MODE`: Plan mode responses
- `MCP_DOCS`: Load MCP documentation
- `TODO`: Todo management

## Adding New Tools

### Tool Structure and Anatomy

Each tool in Cline follows a specific structure with variants for different model families. Here's the anatomy of a tool:

```typescript
// src/core/prompts/system-prompt/tools/my_new_tool.ts
import { ModelFamily } from "@/shared/prompts"
import { ClineDefaultTool } from "@/shared/tools"
import type { ClineToolSpec } from "../spec"

const id = ClineDefaultTool.MY_NEW_TOOL // Add to enum first

const generic: ClineToolSpec = {
	variant: ModelFamily.GENERIC,
	id,
	name: "my_new_tool",
	description: "Description of what this tool does and when to use it",
	parameters: [
		{
			name: "required_param",
			required: true,
			instruction: "Description of this parameter and how to use it",
			usage: "Example value or placeholder text",
		},
		{
			name: "optional_param",
			required: false,
			instruction: "Description of optional parameter",
			usage: "Optional example (optional)",
			dependencies: [ClineDefaultTool.SOME_OTHER_TOOL], // Only show if dependency exists
		},
	],
}

// Create variants for different model families if needed
const nextGen = { ...generic, variant: ModelFamily.NEXT_GEN }
const gpt = { ...generic, variant: ModelFamily.GPT }
const gemini = { ...generic, variant: ModelFamily.GEMINI }

export const my_new_tool_variants = [generic, nextGen, gpt, gemini]
```

### Step-by-Step Instructions for Adding a New Tool

#### 1. Add Tool ID to Enum

First, add your tool ID to the `ClineDefaultTool` enum:

```typescript
// src/shared/tools.ts
export enum ClineDefaultTool {
	// ... existing tools
	MY_NEW_TOOL = "my_new_tool",
}
```

#### 2. Create Tool Specification File

Create a new file in `src/core/prompts/system-prompt/tools/` following the naming convention `{tool_name}.ts`:

```typescript
// src/core/prompts/system-prompt/tools/my_new_tool.ts
import { ModelFamily } from "@/shared/prompts"
import { ClineDefaultTool } from "@/shared/tools"
import type { ClineToolSpec } from "../spec"

const id = ClineDefaultTool.MY_NEW_TOOL

const generic: ClineToolSpec = {
	variant: ModelFamily.GENERIC,
	id,
	name: "my_new_tool",
	description: "Comprehensive description of the tool's purpose, when to use it, and what it accomplishes. Be specific about use cases and limitations.",
	parameters: [
		{
			name: "input_parameter",
			required: true,
			instruction: "Clear instruction on what this parameter expects and how to format it",
			usage: "Example input here",
		},
		{
			name: "options",
			required: false,
			instruction: "Optional configuration or settings for the tool",
			usage: "Configuration options (optional)",
		},
	],
}

// Export variants array - this is crucial for registration
export const my_new_tool_variants = [generic]
```

#### 3. Export Tool from Index

Add your tool export to the tools index file:

```typescript
// src/core/prompts/system-prompt/tools/index.ts
export * from "./my_new_tool"
```

#### 4. Register Tool in Init File

Add your tool to the registration function:

```typescript
// src/core/prompts/system-prompt/tools/init.ts
import { my_new_tool_variants } from "./my_new_tool"

export function registerClineToolSets(): void {
	const allToolVariants = [
		// ... existing tool variants
		...my_new_tool_variants,
	]

	allToolVariants.forEach((v) => {
		ClineToolSet.register(v)
	})
}
```

#### 5. Implement Tool Handler (Backend)

Create the actual tool implementation in the appropriate handler:

```typescript
// In your tool handler class (e.g., ClineProvider)
async handleMyNewTool(args: { input_parameter: string; options?: string }) {
	// Implement your tool logic here
	const result = await performToolOperation(args.input_parameter, args.options)
	
	return {
		type: "tool_result" as const,
		content: result,
	}
}
```

### Advanced Tool Configuration

#### Context-Aware Tools

Tools can be conditionally enabled based on context:

```typescript
const contextAwareTool: ClineToolSpec = {
	variant: ModelFamily.GENERIC,
	id: ClineDefaultTool.CONTEXT_TOOL,
	name: "context_tool",
	description: "Tool that only appears in certain contexts",
	contextRequirements: (context: SystemPromptContext) => {
		// Only show this tool if browser support is available
		return context.supportsBrowserUse === true
	},
	parameters: [
		// ... parameters
	],
}
```

#### Model-Specific Variants

Create different tool behaviors for different model families:

```typescript
const claude: ClineToolSpec = {
	variant: ModelFamily.GENERIC,
	id: ClineDefaultTool.MODEL_SPECIFIC_TOOL,
	name: "model_specific_tool",
	description: "Tool optimized for Claude models with detailed instructions",
	parameters: [
		{
			name: "detailed_input",
			required: true,
			instruction: "Provide comprehensive details as Claude handles complex instructions well",
			usage: "Detailed input with context and examples",
		},
	],
}

const gpt: ClineToolSpec = {
	...claude,
	variant: ModelFamily.GPT,
	description: "Tool optimized for GPT models with concise instructions",
	parameters: [
		{
			name: "detailed_input",
			required: true,
			instruction: "Provide concise, structured input",
			usage: "Brief, structured input",
		},
	],
}

export const model_specific_tool_variants = [claude, gpt]
```

#### Parameter Dependencies

Tools can have parameters that only appear when other tools are available:

```typescript
const dependentTool: ClineToolSpec = {
	variant: ModelFamily.GENERIC,
	id: ClineDefaultTool.DEPENDENT_TOOL,
	name: "dependent_tool",
	description: "Tool with conditional parameters",
	parameters: [
		{
			name: "always_present",
			required: true,
			instruction: "This parameter is always available",
			usage: "Standard input",
		},
		{
			name: "conditional_param",
			required: false,
			instruction: "This parameter only appears if TODO tool is available",
			usage: "Conditional input (optional)",
			dependencies: [ClineDefaultTool.TODO],
		},
	],
}
```

### Best Practices

#### 1. Tool Naming Conventions
- Use snake_case for tool IDs and file names
- Use descriptive names that clearly indicate the tool's purpose
- Prefix with action verb when appropriate (e.g., `create_file`, `search_code`)

#### 2. Parameter Design
- Always provide clear, actionable instructions
- Include usage examples that show expected format
- Mark parameters as required/optional appropriately
- Use dependencies to avoid cluttering the prompt with irrelevant parameters

#### 3. Description Guidelines
- Be specific about when and why to use the tool
- Include limitations and constraints
- Mention any prerequisites or setup requirements
- Provide context about expected outcomes

#### 4. Model Variant Strategy
- Start with a GENERIC variant that works across all models
- Create specific variants only when models need different instructions
- Keep variant differences minimal and focused on instruction style
- Test across different model families to ensure compatibility

#### 5. Error Handling
- Design tools to fail gracefully
- Provide meaningful error messages
- Consider edge cases in parameter validation
- Document expected error scenarios

### Testing Your New Tool

#### 1. Unit Tests
Create unit tests for your tool specification:

```typescript
// src/core/prompts/system-prompt/tools/__tests__/my_new_tool.test.ts
import { my_new_tool_variants } from "../my_new_tool"
import { ModelFamily } from "@/shared/prompts"

describe("my_new_tool", () => {
	it("should have correct structure", () => {
		const generic = my_new_tool_variants.find(v => v.variant === ModelFamily.GENERIC)
		expect(generic).toBeDefined()
		expect(generic?.name).toBe("my_new_tool")
		expect(generic?.parameters).toHaveLength(2)
	})
})
```

#### 2. Integration Tests
Add your tool to the integration test suite:

```typescript
// src/core/prompts/system-prompt/__tests__/integration.test.ts
// The test will automatically pick up your tool if properly registered
```

#### 3. Manual Testing
1. Run the unit tests: `npm run test:unit`
2. Start the application and verify your tool appears in the system prompt
3. Test tool execution with various parameter combinations
4. Verify tool works across different model families

### Complete Example: File Analyzer Tool

Here's a complete example of adding a new "analyze_file" tool:

```typescript
// 1. Add to src/shared/tools.ts
export enum ClineDefaultTool {
	// ... existing tools
	ANALYZE_FILE = "analyze_file",
}

// 2. Create src/core/prompts/system-prompt/tools/analyze_file.ts
import { ModelFamily } from "@/shared/prompts"
import { ClineDefaultTool } from "@/shared/tools"
import type { ClineToolSpec } from "../spec"

const id = ClineDefaultTool.ANALYZE_FILE

const generic: ClineToolSpec = {
	variant: ModelFamily.GENERIC,
	id,
	name: "analyze_file",
	description: "Analyze a file's structure, dependencies, and potential issues. Use this when you need to understand a file's architecture, identify problems, or assess code quality before making changes.",
	parameters: [
		{
			name: "file_path",
			required: true,
			instruction: "The path to the file you want to analyze (relative to current working directory)",
			usage: "src/components/MyComponent.tsx",
		},
		{
			name: "analysis_type",
			required: false,
			instruction: "Type of analysis to perform: 'structure', 'dependencies', 'quality', or 'all'",
			usage: "all (optional)",
		},
		{
			name: "include_suggestions",
			required: false,
			instruction: "Whether to include improvement suggestions in the analysis",
			usage: "true (optional)",
		},
	],
}

const nextGen: ClineToolSpec = {
	...generic,
	variant: ModelFamily.NEXT_GEN,
	description: "Perform comprehensive file analysis including structure, dependencies, code quality, and improvement suggestions. Ideal for code review and refactoring planning.",
}

export const analyze_file_variants = [generic, nextGen]

// 3. Add to src/core/prompts/system-prompt/tools/index.ts
export * from "./analyze_file"

// 4. Add to src/core/prompts/system-prompt/tools/init.ts
import { analyze_file_variants } from "./analyze_file"

export function registerClineToolSets(): void {
	const allToolVariants = [
		// ... existing variants
		...analyze_file_variants,
	]
	// ... rest of function
}
```

This comprehensive guide should help developers understand both the architecture and practical steps needed to extend Cline with new tools.

## Key Features

- **Modular Components**: Reusable across different model variants  
- **Template System**: `{{placeholder}}` support with runtime resolution  
- **Versioning**: Full version control with tags and labels  
- **Model Family Detection**: Automatic model family detection and fallback  
- **Flexible Tool Configuration**: Per-variant tool selection and customization  
- **Component Overrides**: Custom templates for specific components  
- **Runtime Placeholders**: Dynamic value injection at build time  
- **Performance Optimized**: Efficient component building and template resolution  
- **Error Handling**: Graceful degradation when components fail  
- **Conditional Logic**: Context-aware tool and component inclusion

## Links discovered
- [src/core/prompts/system-prompt/__tests__/integration.test.ts](https://github.com/cline/cline/blob/main/src/core/prompts/system-prompt/__tests__/integration.test.ts)

--- src/core/hooks/__tests__/fixtures/README.md ---
# Hook Test Fixtures

This directory contains pre-written hook scripts for testing the Cline hooks system.

## Directory Structure

```
fixtures/
├── hooks/
│   ├── pretooluse/       # PreToolUse hook fixtures
│   │   ├── success/      # Returns success immediately
│   │   ├── blocking/     # Blocks tool execution
│   │   ├── context-injection/  # Adds context with type prefix
│   │   └── error/        # Exits with error code
│   ├── posttooluse/      # PostToolUse hook fixtures
│   │   ├── success/      # Returns success immediately
│   │   └── error/        # Exits with error code
│   └── template/         # Template for new hooks
└── inputs/               # Sample input data (future)
```

## Using Fixtures in Tests

### With loadFixture()

The `loadFixture()` helper function copies a fixture to your test environment:

```typescript
import { loadFixture } from '../test-utils'

it("should work with real hook", async () => {
  const { getEnv } = setupHookTests()
  
  await loadFixture("hooks/pretooluse/success", getEnv().tempDir)
  
  const factory = new HookFactory()
  const runner = await factory.create("PreToolUse")
  const result = await runner.run(buildPreToolUseInput({ toolName: "test_tool" }))
  
  result.cancel.should.be.false()
})
```

### Direct File Copy

For more control, you can also manually copy fixture files.

## Available Fixtures

### PreToolUse Hooks

#### `hooks/pretooluse/success`
- **Returns**: `{ cancel: false, contextModification: "PreToolUse hook executed successfully", errorMessage: "" }`
- **Use for**: Testing happy path scenarios

#### `hooks/pretooluse/blocking`
- **Returns**: `{ cancel: true, contextModification: "", errorMessage: "Tool execution blocked by hook" }`
- **Use for**: Testing tool execution blocking

#### `hooks/pretooluse/context-injection`
- **Returns**: `{ cancel: false, contextModification: "WORKSPACE_RULES: Tool [toolName] requires review", errorMessage: "" }`
- **Use for**: Testing context injection with type prefixes
- **Note**: Dynamically includes tool name from input

#### `hooks/pretooluse/error`
- **Behavior**: Prints error to stderr and exits with code 1
- **Use for**: Testing error handling

### PostToolUse Hooks

#### `hooks/posttooluse/success`
- **Returns**: `{ cancel: false, contextModification: "PostToolUse hook executed successfully", errorMessage: "" }`
- **Use for**: Testing PostToolUse execution

#### `hooks/posttooluse/error`
- **Behavior**: Prints error to stderr and exits with code 1
- **Use for**: Testing error handling in PostToolUse

### UserPromptSubmit Hooks

#### `hooks/userpromptsubmit/success`
- **Returns**: `{ cancel: false, contextModification: "Prompt approved", errorMessage: "" }`
- **Use for**: Testing successful prompt submission

#### `hooks/userpromptsubmit/blocking`
- **Returns**: `{ cancel: true, contextModification: "", errorMessage: "Prompt violates policy" }`
- **Use for**: Testing prompt submission blocking

#### `hooks/userpromptsubmit/context-injection`
- **Returns**: `{ cancel: false, contextModification: "CONTEXT_INJECTION: User is in plan mode", errorMessage: "" }`
- **Use for**: Testing context injection into task request

#### `hooks/userpromptsubmit/multiline`
- **Returns**: `{ cancel: false, contextModification: "Line count: N", errorMessage: "" }`
- **Use for**: Testing multiline prompt handling
- **Note**: Dynamically counts newlines in the prompt

#### `hooks/userpromptsubmit/large-prompt`
- **Returns**: `{ cancel: false, contextModification: "Prompt size: N", errorMessage: "" }`
- **Use for**: Testing large prompt handling
- **Note**: Dynamically reports prompt character count

#### `hooks/userpromptsubmit/special-chars`
- **Returns**: `{ cancel: false, contextModification: "Special chars preserved" | "Missing special chars", errorMessage: "" }`
- **Use for**: Testing special character preservation
- **Note**: Checks for @, #, and $ characters

#### `hooks/userpromptsubmit/empty-prompt`
- **Returns**: `{ cancel: false, contextModification: "Prompt length: 0", errorMessage: "" }`
- **Use for**: Testing empty prompt handling
- **Note**: Safely handles undefined or empty prompts

#### `hooks/userpromptsubmit/malformed-json`
- **Behavior**: Outputs invalid JSON ("not valid json")
- **Use for**: Testing malformed JSON error handling

#### `hooks/userpromptsubmit/error`
- **Behavior**: Prints error to stderr and exits with code 1
- **Use for**: Testing error handling in UserPromptSubmit

### TaskStart Hooks

#### `hooks/taskstart/success`
- **Returns**: `{ cancel: false, contextModification: "TaskStart hook executed successfully", errorMessage: "" }`
- **Use for**: Testing TaskStart hook success path, allowing task to proceed

#### `hooks/taskstart/blocking`
- **Returns**: `{ cancel: true, contextModification: "", errorMessage: "Task execution blocked by hook" }`
- **Use for**: Testing task blocking at start (e.g., policy enforcement)

#### `hooks/taskstart/error`
- **Behavior**: Prints error to stderr and exits with code 1
- **Use for**: Testing error handling in TaskStart hooks

## Platform Considerations

These fixtures are designed for the embedded shell architecture (similar to git hooks). They work uniformly across all platforms once the embedded shell is implemented.

### Current Status
- **Linux/macOS**: Fully functional - executable scripts with shebangs
- **Windows**: Pending embedded shell implementation

### Creating New Fixtures

1. Create a new directory under the appropriate hook type
2. Add the hook script with shebang `#!/usr/bin/env node`
3. Make executable: `chmod +x HookName`
4. Update this README with the new fixture

### Example: Creating a new fixture

```bash
# Create directory
mkdir -p src/core/hooks/__tests__/fixtures/hooks/pretooluse/my-new-scenario

# Create hook script
cat > src/core/hooks/__tests__/fixtures/hooks/pretooluse/my-new-scenario/PreToolUse << 'EOF'
#!/usr/bin/env node
const input = JSON.parse(require('fs').readFileSync(0, 'utf-8'));
console.log(JSON.stringify({
  cancel: false,
  contextModification: "My custom context",
  errorMessage: ""
}));
EOF

# Make executable
chmod +x src/core/hooks/__tests__/fixtures/hooks/pretooluse/my-new-scenario/PreToolUse
```

## Maintenance

- Keep fixtures simple and focused on one scenario
- Fixtures are Node.js scripts that work across platforms
- Update this README when adding new fixtures
- Remove obsolete fixtures and update references
