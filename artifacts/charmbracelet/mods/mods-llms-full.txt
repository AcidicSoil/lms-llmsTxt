# llms-full (private-aware)
> Built from GitHub files and website pages. Large files may be truncated.

--- examples.md ---
# Mods Examples

### Improve Your Code

Piping source code to Mods and giving it an instruction on what to do with it
gives you a lot of options for refactoring, enhancing or debugging code.

`mods -f "what are your thoughts on improving this code?" < main.go | glow`

<p><img src="https://github.com/charmbracelet/mods/assets/25087/738fe969-1c9f-4849-af8a-cde38156ce92" width="900" alt="a GIF of mods offering code refactoring suggestions"></p>

### Come Up With Product Features

Mods can also come up with entirely new features based on source code (or a
README file).

`mods -f "come up with 10 new features for this tool." < main.go | glow`

<p><img src="https://github.com/charmbracelet/mods/assets/25087/025de860-798a-4ab2-b1cf-a0b32dbdbe4d" width="900" alt="a GIF of mods suggesting feature improvements"></p>

### Help Write Docs

Mods can quickly give you a first draft for new documentation.

`mods "write a new section to this readme for a feature that sends you a free rabbit if you hit r" < README.md | glow`

<p><img src="https://github.com/charmbracelet/mods/assets/25087/c26a17a9-c772-40cc-b3f1-9189ac682730" width="900" alt="a GIF of mods contributing to a product README"></p>

### Organize Your Videos

The file system can be an amazing source of input for Mods. If you have music
or video files, Mods can parse the output of `ls` and offer really good
editorialization of your content.

`ls ~/vids | mods -f "organize these by decade and summarize each" | glow`

<p><img src="https://github.com/charmbracelet/mods/assets/25087/8204d06a-8cf1-401d-802f-2b94345dec5d" width="900" alt="a GIF of mods oraganizing and summarizing video from a shell ls statement"></p>

### Make Recommendations

Mods is really good at generating recommendations based on what you have as
well, both for similar content but also content in an entirely different media
(like getting music recommendations based on movies you have).

`ls ~/vids | mods -f "recommend me 10 shows based on these, make them obscure" | glow`

`ls ~/vids | mods -f "recommend me 10 albums based on these shows, do not include any soundtrack music or music from the show" | glow`

<p><img src="https://github.com/charmbracelet/mods/assets/25087/48159b19-5cae-413b-9677-dce8c6dfb6b8" width="900" alt="a GIF of mods generating television show recommendations based on a file listing from a directory of videos"></p>

### Read Your Fortune

It's easy to let your downloads folder grow into a chaotic never-ending pit of
files, but with Mods you can use that to your advantage!

`ls ~/Downloads | mods -f "tell my fortune based on these files" | glow`

<p><img src="https://github.com/charmbracelet/mods/assets/25087/da2206a8-799f-4c92-b75e-bac66c56ea88" width="900" alt="a GIF of mods generating a fortune from the contents of a downloads directory"></p>

### Understand APIs

Mods can parse and understand the output of an API call with `curl` and convert
it to something human readable.

`curl "https://api.open-meteo.com/v1/forecast?latitude=29.00&longitude=-90.00&current_weather=true&hourly=temperature_2m,relativehumidity_2m,windspeed_10m" 2>/dev/null | mods -f "summarize this weather data for a human." | glow`

<p><img src="https://github.com/charmbracelet/mods/assets/25087/3af13876-46a3-4bab-986e-50d9f54d2921" width="900" alt="a GIF of mods summarizing the weather from JSON API output"></p>

### Read The Comments (so you don't have to)

Just like with APIs, Mods can read through raw HTML and summarize the contents.

`curl "https://news.ycombinator.com/item?id=30048332" 2>/dev/null | mods -f "what are the authors of these comments saying?" | glow`

<p><img src="https://github.com/charmbracelet/mods/assets/25087/e4d94ef8-43aa-45ea-9be5-fe13e53d5203" width="900" alt="a GIF of mods summarizing the comments on hacker news"></p>


--- README.md ---
# Mods

<p>
    <img src="https://github.com/charmbracelet/mods/assets/25087/5442bf46-b908-47af-bf4e-60f7c38951c4" width="630" alt="Mods product art and type treatment"/>
    <br>
    <a href="https://github.com/charmbracelet/mods/releases"><img src="https://img.shields.io/github/release/charmbracelet/mods.svg" alt="Latest Release"></a>
    <a href="https://github.com/charmbracelet/mods/actions"><img src="https://github.com/charmbracelet/mods/workflows/build/badge.svg" alt="Build Status"></a>
</p>

AI for the command line, built for pipelines.

<p><img src="https://vhs.charm.sh/vhs-5Uyj0U6Hlqi1LVIIRyYKM5.gif" width="900" alt="a GIF of mods running"></p>

Large Language Models (LLM) based AI is useful to ingest command output and
format results in Markdown, JSON, and other text based formats. Mods is a
tool to add a sprinkle of AI in your command line and make your pipelines
artificially intelligent.

It works great with LLMs running locally through [LocalAI]. You can also use
[OpenAI], [Cohere], [Groq], or [Azure OpenAI].

[LocalAI]: https://github.com/go-skynet/LocalAI
[OpenAI]: https://platform.openai.com/account/api-keys
[Cohere]: https://dashboard.cohere.com/api-keys
[Groq]: https://console.groq.com/keys
[Azure OpenAI]: https://azure.microsoft.com/en-us/products/cognitive-services/openai-service

### Installation

Use a package manager:

```bash
# macOS or Linux
brew install charmbracelet/tap/mods

# Windows (with Winget)
winget install charmbracelet.mods

# Arch Linux (btw)
yay -S mods

# Nix
nix-shell -p mods
```

<details>
<summary>Debian/Ubuntu</summary>

```bash
sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://repo.charm.sh/apt/gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/charm.gpg
echo "deb [signed-by=/etc/apt/keyrings/charm.gpg] https://repo.charm.sh/apt/ * *" | sudo tee /etc/apt/sources.list.d/charm.list
sudo apt update && sudo apt install mods
```

</details>

<details>
<summary>Fedora/RHEL</summary>

```bash
echo '[charm]
name=Charm
baseurl=https://repo.charm.sh/yum/
enabled=1
gpgcheck=1
gpgkey=https://repo.charm.sh/yum/gpg.key' | sudo tee /etc/yum.repos.d/charm.repo
sudo yum install mods
```

</details>

### FreeBSD

Install [from ports](https://www.freshports.org/sysutils/mods/):
```sh
cd /usr/ports/sysutils/mods
make install
```

Or, download it:

- [Packages][releases] are available in Debian and RPM formats
- [Binaries][releases] are available for Linux, macOS, and Windows

[releases]: https://github.com/charmbracelet/mods/releases

Or, just install it with `go`:

```sh
go install github.com/charmbracelet/mods@latest
```

<details>
<summary>Shell Completions</summary>

All the packages and archives come with pre-generated completion files for Bash,
ZSH, Fish, and PowerShell.

If you built it from source, you can generate them with:

```bash
mods completion bash -h
mods completion zsh -h
mods completion fish -h
mods completion powershell -h
```

If you use a package (like Homebrew, Debs, etc), the completions should be set
up automatically, given your shell is configured properly.

</details>

## What Can It Do?

Mods works by reading standard in and prefacing it with a prompt supplied in
the `mods` arguments. It sends the input text to an LLM and prints out the
result, optionally asking the LLM to format the response as Markdown. This
gives you a way to "question" the output of a command. Mods will also work on
standard in or an argument supplied prompt individually.

Be sure to check out the [examples](examples.md) and a list of all the
[features](features.md).

Mods works with OpenAI compatible endpoints. By default, Mods is configured to
support OpenAI's official API and a LocalAI installation running on port 8080.
You can configure additional endpoints in your settings file by running
`mods --settings`.

## Saved Conversations

Conversations are saved locally by default. Each conversation has a SHA-1
identifier and a title (like `git`!).

<p>
  <img src="https://vhs.charm.sh/vhs-6MMscpZwgzohYYMfTrHErF.gif" width="900" alt="a GIF listing and showing saved conversations.">
</p>

Check the [`./features.md`](./features.md) for more details.

## Usage

- `-m`, `--model`: Specify Large Language Model to use
- `-M`, `--ask-model`: Ask which model to use via interactive prompt
- `-f`, `--format`: Ask the LLM to format the response in a given format
- `--format-as`: Specify the format for the output (used with `--format`)
- `-P`, `--prompt` Include the prompt from the arguments and stdin, truncate stdin to specified number of lines
- `-p`, `--prompt-args`: Include the prompt from the arguments in the response
- `-q`, `--quiet`: Only output errors to standard err
- `-r`, `--raw`: Print raw response without syntax highlighting
- `--settings`: Open settings
- `-x`, `--http-proxy`: Use HTTP proxy to connect to the API endpoints
- `--max-retries`: Maximum number of retries
- `--max-tokens`: Specify maximum tokens with which to respond
- `--no-limit`: Do not limit the response tokens
- `--role`: Specify the role to use (See [custom roles](#custom-roles))
- `--word-wrap`: Wrap output at width (defaults to 80)
- `--reset-settings`: Restore settings to default
- `--theme`: Theme to use in the forms; valid choices are: `charm`, `catppuccin`, `dracula`, and `base16`
- `--status-text`: Text to show while generating

#### Conversations

- `-t`, `--title`: Set the title for the conversation.
- `-l`, `--list`: List saved conversations.
- `-c`, `--continue`: Continue from last response or specific title or SHA-1.
- `-C`, `--continue-last`: Continue the last conversation.
- `-s`, `--show`: Show saved conversation for the given title or SHA-1
- `-S`, `--show-last`: Show previous conversation
- `--delete-older-than=<duration>`: Deletes conversations older than given duration (`10d`, `1mo`).
- `--delete`: Deletes the saved conversations for the given titles or SHA-1s
- `--no-cache`: Do not save conversations

#### MCP

- `--mcp-list`: List all available MCP servers
- `--mcp-list-tools`: List all available tools from enabled MCP servers
- `--mcp-disable`: Disable specific MCP servers

#### Advanced

- `--fanciness`: Level of fanciness
- `--temp`: Sampling temperature
- `--topp`: Top P value
- `--topk`: Top K value

## Custom Roles

Roles allow you to set system prompts. Here is an example of a `shell` role:

```yaml
roles:
  shell:
    - you are a shell expert
    - you do not explain anything
    - you simply output one liners to solve the problems you're asked
    - you do not provide any explanation whatsoever, ONLY the command
```

Then, use the custom role in `mods`:

```sh
mods --role shell list files in the current directory
```

## Setup

### Open AI

Mods uses GPT-4 by default. It will fall back to GPT-3.5 Turbo.

Set the `OPENAI_API_KEY` environment variable. If you don't have one yet, you
can grab it the [OpenAI website](https://platform.openai.com/account/api-keys).

Alternatively, set the [`AZURE_OPENAI_KEY`] environment variable to use Azure
OpenAI. Grab a key from [Azure](https://azure.microsoft.com/en-us/products/cognitive-services/openai-service).

### Cohere

Cohere provides enterprise optimized models.

Set the `COHERE_API_KEY` environment variable. If you don't have one yet, you can
get it from the [Cohere dashboard](https://dashboard.cohere.com/api-keys).

### Local AI

Local AI allows you to run models locally. Mods works with the GPT4ALL-J model
as setup in [this tutorial](https://github.com/go-skynet/LocalAI#example-use-gpt4all-j-model).

### Groq

Groq provides models powered by their LPU inference engine.

Set the `GROQ_API_KEY` environment variable. If you don't have one yet, you can
get it from the [Groq console](https://console.groq.com/keys).

### Gemini

Mods supports using Gemini models from Google.

Set the `GOOGLE_API_KEY` enviroment variable. If you don't have one yet,
you can get it from the [Google AI Studio](https://aistudio.google.com/apikey).

## Contributing

See [contributing][contribute].

[contribute]: https://github.com/charmbracelet/mods/contribute

## Whatcha Think?

We’d love to hear your thoughts on this project. Feel free to drop us a note.

- [Twitter](https://twitter.com/charmcli)
- [The Fediverse](https://mastodon.social/@charmcli)
- [Discord](https://charm.sh/chat)

## License

[MIT](https://github.com/charmbracelet/mods/raw/main/LICENSE)

---

Part of [Charm](https://charm.sh).

<a href="https://charm.sh/"><img alt="The Charm logo" width="400" src="https://stuff.charm.sh/charm-badge.jpg" /></a>

<!--prettier-ignore-->
Charm热爱开源 • Charm loves open source


## Links discovered
- [from ports](https://www.freshports.org/sysutils/mods/)
- [examples](https://github.com/charmbracelet/mods/blob/main/examples.md)
- [features](https://github.com/charmbracelet/mods/blob/main/features.md)
- [`./features.md`](https://github.com/charmbracelet/mods/blob/main/features.md)
- [OpenAI website](https://platform.openai.com/account/api-keys)
- [Azure](https://azure.microsoft.com/en-us/products/cognitive-services/openai-service)
- [Cohere dashboard](https://dashboard.cohere.com/api-keys)
- [this tutorial](https://github.com/go-skynet/LocalAI#example-use-gpt4all-j-model)
- [Groq console](https://console.groq.com/keys)
- [Google AI Studio](https://aistudio.google.com/apikey)
- [Twitter](https://twitter.com/charmcli)
- [The Fediverse](https://mastodon.social/@charmcli)
- [Discord](https://charm.sh/chat)
- [MIT](https://github.com/charmbracelet/mods/raw/main/LICENSE)
- [Charm](https://charm.sh)
- [<img src="https://img.shields.io/github/release/charmbracelet/mods.svg" alt="Latest Release">](https://github.com/charmbracelet/mods/releases)
- [<img src="https://github.com/charmbracelet/mods/workflows/build/badge.svg" alt="Build Status">](https://github.com/charmbracelet/mods/actions)
- [<img alt="The Charm logo" width="400" src="https://stuff.charm.sh/charm-badge.jpg" />](https://charm.sh/)

--- features.md ---
# Mods Features

## Regular usage

By default:

- all messages go to `STDERR`
- all prompts are saved with the first line of the prompt as the title
- glamour is used by default if `STDOUT` is a TTY

### Basic

The most basic usage is:

```bash
mods 'first 2 primes'
```

### Pipe from

You can also pipe to it, in which case `STDIN` will not be a TTY:

```bash
echo 'as json' | mods 'first 2 primes'
```

In this case, `mods` should read `STDIN` and append it to the prompt.

### Pipe to

You may also pipe the output to another program, in which case `STDOUT` will not
be a TTY:

```bash
echo 'as json' | mods 'first 2 primes' | jq .
```

In this case, the "Generating" animation will go to `STDERR`, but the response
will be streamed to `STDOUT`.

### Custom title

You can set a custom title:

```bash
mods --title='title' 'first 2 primes'
```

### Continue latest

You can continue the latest conversation and save it with a new title using
`--continue=title`:

```bash
mods 'first 2 primes'
mods --continue='primes as json' 'format as json'
```

### Untitled continue latest

```bash
mods 'first 2 primes'
mods --continue-last 'format as json'
```

### Continue from specific conversation, save with a new title

```bash
mods --title='naturals' 'first 5 natural numbers'
mods --continue='naturals' --title='naturals.json' 'format as json'
```

### Conversation branching

You can use the `--continue` and `--title` to branch out conversations, for
instance:

```bash
mods --title='naturals' 'first 5 natural numbers'
mods --continue='naturals' --title='naturals.json' 'format as json'
mods --continue='naturals' --title='naturals.yaml' 'format as yaml'
```

With this you'll end up with 3 conversations: `naturals`, `naturals.json`, and
`naturals.yaml`.

## List conversations

You can list your previous conversations with:

```bash
mods --list
# or
mods -l
```

## Show a previous conversation

You can also show a previous conversation by ID or title, e.g.:

```bash
mods --show='naturals'
mods -s='a2e2'
```

For titles, the match should be exact.
For IDs, only the first 4 chars are needed. If it matches multiple
conversations, you can add more chars until it matches a single one again.

## Delete a conversation

You can also delete conversations by title or ID, same as `--show`, different
flag:

```bash
mods --delete='naturals' --delete='a2e2'
```

Keep in mind that these operations are not reversible.
You can repeat the delete flag to delete multiple conversations at once.
