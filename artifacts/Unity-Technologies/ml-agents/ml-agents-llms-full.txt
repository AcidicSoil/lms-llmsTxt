# llms-full (private-aware)
> Built from GitHub files and website pages. Large files may be truncated.

--- docs/Getting-Started.md ---
# Getting Started Guide

This guide walks through the end-to-end process of opening one of our
[example environments](Learning-Environment-Examples.md) in Unity, training an
Agent in it, and embedding the trained model into the Unity environment. After
reading this tutorial, you should be able to train any of the example
environments. If you are not familiar with the
[Unity Engine](https://unity3d.com/unity), view our
[Background: Unity](Background-Unity.md) page for helpful pointers.
Additionally, if you're not familiar with machine learning, view our
[Background: Machine Learning](Background-Machine-Learning.md) page for a brief
overview and helpful pointers.

![3D Balance Ball](images/balance.png)

For this guide, we'll use the **3D Balance Ball** environment which contains a
number of agent cubes and balls (which are all copies of each other). Each agent
cube tries to keep its ball from falling by rotating either horizontally or
vertically. In this environment, an agent cube is an **Agent** that receives a
reward for every step that it balances the ball. An agent is also penalized with
a negative reward for dropping the ball. The goal of the training process is to
have the agents learn to balance the ball on their head.

Let's get started!

## Installation

If you haven't already, follow the [installation instructions](Installation.md).
Afterwards, open the Unity Project that contains all the example environments:

1. Open the Package Manager Window by navigating to `Window -> Package Manager`
   in the menu.
1. Navigate to the ML-Agents Package and click on it.
1. Find the `3D Ball` sample and click `Import`.
1. In the **Project** window, go to the
   `Assets/ML-Agents/Examples/3DBall/Scenes` folder and open the `3DBall` scene
   file.

## Understanding a Unity Environment

An agent is an autonomous actor that observes and interacts with an
_environment_. In the context of Unity, an environment is a scene containing one
or more Agent objects, and, of course, the other entities that an agent
interacts with.

![Unity Editor](images/mlagents-3DBallHierarchy.png)

**Note:** In Unity, the base object of everything in a scene is the
_GameObject_. The GameObject is essentially a container for everything else,
including behaviors, graphics, physics, etc. To see the components that make up
a GameObject, select the GameObject in the Scene window, and open the Inspector
window. The Inspector shows every component on a GameObject.

The first thing you may notice after opening the 3D Balance Ball scene is that
it contains not one, but several agent cubes. Each agent cube in the scene is an
independent agent, but they all share the same Behavior. 3D Balance Ball does
this to speed up training since all twelve agents contribute to training in
parallel.

### Agent

The Agent is the actor that observes and takes actions in the environment. In
the 3D Balance Ball environment, the Agent components are placed on the twelve
"Agent" GameObjects. The base Agent object has a few properties that affect its
behavior:

- **Behavior Parameters** — Every Agent must have a Behavior. The Behavior
  determines how an Agent makes decisions.
- **Max Step** — Defines how many simulation steps can occur before the Agent's
  episode ends. In 3D Balance Ball, an Agent restarts after 5000 steps.

#### Behavior Parameters : Vector Observation Space

Before making a decision, an agent collects its observation about its state in
the world. The vector observation is a vector of floating point numbers which
contain relevant information for the agent to make decisions.

The Behavior Parameters of the 3D Balance Ball example uses a `Space Size` of 8.
This means that the feature vector containing the Agent's observations contains
eight elements: the `x` and `z` components of the agent cube's rotation and the
`x`, `y`, and `z` components of the ball's relative position and velocity.

#### Behavior Parameters : Actions

An Agent is given instructions in the form of actions.
ML-Agents Toolkit classifies actions into two types: continuous and discrete.
The 3D Balance Ball example is programmed to use continuous actions, which
are a vector of floating-point numbers that can vary continuously. More specifically,
it uses a `Space Size` of 2 to control the amount of `x` and `z` rotations to apply to
itself to keep the ball balanced on its head.

## Running a pre-trained model

We include pre-trained models for our agents (`.onnx` files) and we use the
[Inference Engine](Inference-Engine.md) to run these models inside
Unity. In this section, we will use the pre-trained model for the 3D Ball
example.

1. In the **Project** window, go to the
   `Assets/ML-Agents/Examples/3DBall/Prefabs` folder. Expand `3DBall` and click
   on the `Agent` prefab. You should see the `Agent` prefab in the **Inspector**
   window.

   **Note**: The platforms in the `3DBall` scene were created using the `3DBall`
   prefab. Instead of updating all 12 platforms individually, you can update the
   `3DBall` prefab instead.

   ![Platform Prefab](images/platform_prefab.png)

1. In the **Project** window, drag the **3DBall** Model located in
   `Assets/ML-Agents/Examples/3DBall/TFModels` into the `Model` property under
   `Behavior Parameters (Script)` component in the Agent GameObject
   **Inspector** window.

   ![3dball learning brain](images/3dball_learning_brain.png)

1. You should notice that each `Agent` under each `3DBall` in the **Hierarchy**
   windows now contains **3DBall** as `Model` on the `Behavior Parameters`.
   **Note** : You can modify multiple game objects in a scene by selecting them
   all at once using the search bar in the Scene Hierarchy.
1. Set the **Inference Device** to use for this model as `CPU`.
1. Click the **Play** button in the Unity Editor and you will see the platforms
   balance the balls using the pre-trained model.

## Training a new model with Reinforcement Learning

While we provide pre-trained models for the agents in this environment, any
environment you make yourself will require training agents from scratch to
generate a new model file. In this section we will demonstrate how to use the
reinforcement learning algorithms that are part of the ML-Agents Python package
to accomplish this. We have provided a convenient command `mlagents-learn` which
accepts arguments used to configure both training and inference phases.

### Training the environment

1. Open a command or terminal window.
1. Navigate to the folder where you cloned the `ml-agents` repository. **Note**:
   If you followed the default [installation](Installation.md), then you should
   be able to run `mlagents-learn` from any directory.
1. Run `mlagents-learn config/ppo/3DBall.yaml --run-id=first3DBallRun`.
   - `config/ppo/3DBall.yaml` is the path to a default training
     configuration file that we provide. The `config/ppo` folder includes training configuration
     files for all our example environments, including 3DBall.
   - `run-id` is a unique name for this training session.
1. When the message _"Start training by pressing the Play button in the Unity
   Editor"_ is displayed on the screen, you can press the **Play** button in
   Unity to start training in the Editor.

If `mlagents-learn` runs correctly and starts training, you should see something
like this:

```console
INFO:mlagents_envs:
'Ball3DAcademy' started successfully!
Unity Academy name: Ball3DAcademy

INFO:mlagents_envs:Connected new brain:
Unity brain name: 3DBallLearning
        Number of Visual Observations (per agent): 0
        Vector Observation space size (per agent): 8
        Number of stacked Vector Observation: 1
INFO:mlagents_envs:Hyperparameters for the PPO Trainer of brain 3DBallLearning:
        batch_size:          64
        beta:                0.001
        buffer_size:         12000
        epsilon:             0.2
        gamma:               0.995
        hidden_units:        128
        lambd:               0.99
        learning_rate:       0.0003
        max_steps:           5.0e4
        normalize:           True
        num_epoch:           3
        num_layers:          2
        time_horizon:        1000
        sequence_length:     64
        summary_freq:        1000
        use_recurrent:       False
        memory_size:         256
        use_curiosity:       False
        curiosity_strength:  0.01
        curiosity_enc_size:  128
        output_path: ./results/first3DBallRun/3DBallLearning
INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 1000. Mean Reward: 1.242. Std of Reward: 0.746. Training.
INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 2000. Mean Reward: 1.319. Std of Reward: 0.693. Training.
INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 3000. Mean Reward: 1.804. Std of Reward: 1.056. Training.
INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 4000. Mean Reward: 2.151. Std of Reward: 1.432. Training.
INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 5000. Mean Reward: 3.175. Std of Reward: 2.250. Training.
INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 6000. Mean Reward: 4.898. Std of Reward: 4.019. Training.
INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 7000. Mean Reward: 6.716. Std of Reward: 5.125. Training.
INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 8000. Mean Reward: 12.124. Std of Reward: 11.929. Training.
INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 9000. Mean Reward: 18.151. Std of Reward: 16.871. Training.
INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 10000. Mean Reward: 27.284. Std of Reward: 28.667. Training.
```

Note how the `Mean Reward` value printed to the screen increases as training
progresses. This is a positive sign that training is succeeding.

**Note**: You can train using an executable rather than the Editor. To do so,
follow the instructions in
[Using an Executable](Learning-Environment-Executable.md).

### Observing Training Progress

Once you start training using `mlagents-learn` in the way described in the
previous section, the `ml-agents` directory will contain a `results`
directory. In order to observe the training process in more detail, you can use
TensorBoard. From the command line run:

```sh
tensorboard --logdir results
```

Then navigate to `localhost:6006` in your browser to view the TensorBoard
summary statistics as shown below. For the purposes of this section, the most
important statistic is `Environment/Cumulative Reward` which should increase
throughout training, eventually converging close to `100` which is the maximum
reward the agent can accumulate.

![Example TensorBoard Run](images/mlagents-TensorBoard.png)

## Embedding the model into the Unity Environment

Once the training process completes, and the training process saves the model
(denoted by the `Saved Model` message) you can add it to the Unity project and
use it with compatible Agents (the Agents that generated the model). **Note:**
Do not just close the Unity Window once the `Saved Model` message appears.
Either wait for the training process to close the window or press `Ctrl+C` at
the command-line prompt. If you close the window manually, the `.onnx` file
containing the trained model is not exported into the ml-agents folder.

If you've quit the training early using `Ctrl+C` and want to resume training,
run the same command again, appending the `--resume` flag:

```sh
mlagents-learn config/ppo/3DBall.yaml --run-id=first3DBallRun --resume
```

Your trained model will be at `results/<run-identifier>/<behavior_name>.onnx` where
`<behavior_name>` is the name of the `Behavior Name` of the agents corresponding
to the model. This file corresponds to your model's latest checkpoint. You can
now embed this trained model into your Agents by following the steps below,
which is similar to the steps described [above](#running-a-pre-trained-model).

1. Move your model file into
   `Project/Assets/ML-Agents/Examples/3DBall/TFModels/`.
1. Open the Unity Editor, and select the **3DBall** scene as described above.
1. Select the **3DBall** prefab Agent object.
1. Drag the `<behavior_name>.onnx` file from the Project window of the Editor to
   the **Model** placeholder in the **Ball3DAgent** inspector window.
1. Press the **Play** button at the top of the Editor.

## Next Steps

- For more information on the ML-Agents Toolkit, in addition to helpful
  background, check out the [ML-Agents Toolkit Overview](ML-Agents-Overview.md)
  page.
- For a "Hello World" introduction to creating your own Learning Environment,
  check out the
  [Making a New Learning Environment](Learning-Environment-Create-New.md) page.
- For an overview on the more complex example environments that are provided in
  this toolkit, check out the
  [Example Environments](Learning-Environment-Examples.md) page.
- For more information on the various training options available, check out the
  [Training ML-Agents](Training-ML-Agents.md) page.


## Links discovered
- [example environments](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Examples.md)
- [Unity Engine](https://unity3d.com/unity)
- [Background: Unity](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Background-Unity.md)
- [Background: Machine Learning](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Background-Machine-Learning.md)
- [3D Balance Ball](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/images/balance.png)
- [installation instructions](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Installation.md)
- [Unity Editor](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/images/mlagents-3DBallHierarchy.png)
- [Inference Engine](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Inference-Engine.md)
- [Platform Prefab](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/images/platform_prefab.png)
- [3dball learning brain](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/images/3dball_learning_brain.png)
- [installation](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Installation.md)
- [Using an Executable](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Executable.md)
- [Example TensorBoard Run](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/images/mlagents-TensorBoard.png)
- [ML-Agents Toolkit Overview](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/ML-Agents-Overview.md)
- [Making a New Learning Environment](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Create-New.md)
- [Example Environments](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Examples.md)
- [Training ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Training-ML-Agents.md)

--- docs/Installation.md ---
# Installation

The ML-Agents Toolkit contains several components:

- Unity package ([`com.unity.ml-agents`](../com.unity.ml-agents/)) contains the
  Unity C# SDK that will be integrated into your Unity project. This package contains
  a sample to help you get started with ML-Agents, including advanced features like
  custom sensors, input system integration, and physics-based components.
- Two Python packages:
  - [`mlagents`](../ml-agents/) contains the machine learning algorithms that
    enables you to train behaviors in your Unity scene. Most users of ML-Agents
    will only need to directly install `mlagents`.
  - [`mlagents_envs`](../ml-agents-envs/) contains a set of Python APIs to interact with
    a Unity scene. It is a foundational layer that facilitates data messaging
    between Unity scene and the Python machine learning algorithms.
    Consequently, `mlagents` depends on `mlagents_envs`.
- Unity [Project](https://github.com/Unity-Technologies/ml-agents/tree/main/Project/) that contains several
  [example environments](Learning-Environment-Examples.md) that highlight the
  various features of the toolkit to help you get started.

Consequently, to install and use the ML-Agents Toolkit you will need to:

- Install Unity (6000.0 or later)
- Install Python (>= 3.10.1, <=3.10.12) - we recommend using 3.10.12
- Clone this repository (Recommended for the latest version and bug fixes)
  - __Note:__ If you do not clone the repository, then you will not be
  able to access the example environments and training configurations.
  Additionally, the [Getting Started Guide](Getting-Started.md) assumes that you have cloned the
  repository.
- Install the `com.unity.ml-agents` Unity package
- Install the `mlagents-envs`
- Install the `mlagents` Python package

### Install **Unity 6000.0** or Later

[Download](https://unity3d.com/get-unity/download) and install Unity. We
strongly recommend that you install Unity through the Unity Hub as it will
enable you to manage multiple Unity versions.

### Install **Python 3.10.12**

We recommend [installing](https://www.python.org/downloads/) Python 3.10.12.
If you are using Windows, please install the x86-64 version and not x86.
If your Python environment doesn't include `pip3`, see these
[instructions](https://packaging.python.org/guides/installing-using-linux-tools/#installing-pip-setuptools-wheel-with-linux-package-managers)
on installing it. We also recommend using [conda](https://docs.conda.io/en/latest/) or [mamba](https://github.com/mamba-org/mamba) to manage your python virtual environments.

#### Conda python setup

Once conda has been installed in your system, open a terminal and execute the following commands to setup a python 3.10.12 virtual environment
and activate it.

```shell
conda create -n mlagents python=3.10.12 && conda activate mlagents
```

### Clone the ML-Agents Toolkit Repository (Recommended)

Now that you have installed Unity and Python, you can now install the Unity and
Python packages. You do not need to clone the repository to install those
packages, but you may choose to clone the repository if you'd like download our
example environments and training configurations to experiment with them (some
of our tutorials / guides assume you have access to our example environments).

**NOTE:** There are samples shipped with the Unity Package.  You only need to clone
the repository if you would like to explore more examples.

```sh
git clone --branch release_22 https://github.com/Unity-Technologies/ml-agents.git
```

The `--branch release_22` option will switch to the tag of the latest stable
release. Omitting that will get the `develop` branch which is potentially unstable.
However, if you find that a release branch does not work, the recommendation is to use
the `develop` branch as it may have potential fixes for bugs and dependency issues.

(Optional to get bleeding edge)

```sh
git clone https://github.com/Unity-Technologies/ml-agents.git
```

#### Advanced: Local Installation for Development

You will need to clone the repository if you plan to modify or extend the
ML-Agents Toolkit for your purposes. If you plan to contribute those changes
back, make sure to clone the `develop` branch (by omitting `--branch release_22`
from the command above). See our
[Contributions Guidelines](../com.unity.ml-agents/CONTRIBUTING.md) for more
information on contributing to the ML-Agents Toolkit.

### Install the `com.unity.ml-agents` Unity package

The Unity ML-Agents C# SDK is a Unity Package. You can install the
`com.unity.ml-agents` package
[directly from the Package Manager registry](https://docs.unity3d.com/Manual/upm-ui-install.html).
Please make sure you enable 'Preview Packages' in the 'Advanced' dropdown in
order to find the latest Preview release of the package.

**NOTE:** If you do not see the ML-Agents package listed in the Package Manager
please follow the [advanced installation instructions](#advanced-local-installation-for-development) below.

#### Advanced: Local Installation for Development

You can [add the local](https://docs.unity3d.com/Manual/upm-ui-local.html)
`com.unity.ml-agents` package (from the repository that you just cloned) to your
project by:

1. navigating to the menu `Window` -> `Package Manager`.
1. In the package manager window click on the `+` button on the top left of the packages list).
1. Select `Add package from disk...`
1. Navigate into the `com.unity.ml-agents` folder.
1. Select the `package.json` file.

<p align="center">
  <img src="../images/unity_package_manager_window.png"
       alt="Unity Package Manager Window"
       height="150"
       border="10" />
  <img src="../images/unity_package_json.png"
     alt="package.json"
     height="150"
     border="10" />
</p>

If you are going to follow the examples from our documentation, you can open the
`Project` folder in Unity and start tinkering immediately.

### Install the `mlagents` Python package

Installing the `mlagents` Python package involves installing other Python
packages that `mlagents` depends on. So you may run into installation issues if
your machine has older versions of any of those dependencies already installed.
Consequently, our supported path for installing `mlagents` is to leverage Python
Virtual Environments. Virtual Environments provide a mechanism for isolating the
dependencies for each project and are supported on Mac / Windows / Linux. We
offer a dedicated [guide on Virtual Environments](Using-Virtual-Environment.md).

#### (Windows) Installing PyTorch

On Windows, you'll have to install the PyTorch package separately prior to
installing ML-Agents in order to make sure the cuda-enabled version is used,
rather than the CPU-only version. Activate your virtual environment and run from
the command line:

```sh
pip3 install torch~=2.2.1 --index-url https://download.pytorch.org/whl/cu121
```

Note that on Windows, you may also need Microsoft's
[Visual C++ Redistributable](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)
if you don't have it already. See the [PyTorch installation guide](https://pytorch.org/get-started/locally/)
for more installation options and versions.

#### Installing `mlagents`

To install the `mlagents` Python package, activate your virtual environment and
run from the command line:

```sh
cd /path/to/ml-agents
python -m pip install ./ml-agents-envs
python -m pip install ./ml-agents
```

Note that this will install `mlagents` from the cloned repository, _not_ from the PyPi
repository. If you installed this correctly, you should be able to run
`mlagents-learn --help`, after which you will see the command
line parameters you can use with `mlagents-learn`.

**NOTE:** Since ML-Agents development has slowed, PyPi releases will be less frequent. However, you can install from PyPi by executing
the following command:

```shell
python -m pip install mlagents==1.1.0
```

which will install the latest version of ML-Agents and associated dependencies available on PyPi. Note, you need to have the matching version of
the Unity packages with the particular release of the python packages. You can find the release history [here](https://github.com/Unity-Technologies/ml-agents/releases)

By installing the `mlagents` package, the dependencies listed in the
[setup.py file](../ml-agents/setup.py) are also installed. These include
[PyTorch](Background-PyTorch.md).

#### Advanced: Local Installation for Development

If you intend to make modifications to `mlagents` or `mlagents_envs`, you should
install the packages from the cloned repository rather than from PyPi. To do
this, you will need to install `mlagents` and `mlagents_envs` separately. From
the repository's root directory, run:

```sh
pip3 install torch -f https://download.pytorch.org/whl/torch_stable.html
pip3 install -e ./ml-agents-envs
pip3 install -e ./ml-agents
```

Running pip with the `-e` flag will let you make changes to the Python files
directly and have those reflected when you run `mlagents-learn`. It is important
to install these packages in this order as the `mlagents` package depends on
`mlagents_envs`, and installing it in the other order will download
`mlagents_envs` from PyPi.

## Next Steps

The [Getting Started](Getting-Started.md) guide contains several short tutorials
on setting up the ML-Agents Toolkit within Unity, running a pre-trained model,
in addition to building and training environments.

## Help

If you run into any problems regarding ML-Agents, refer to our [FAQ](FAQ.md) and
our [Limitations](Limitations.md) pages. If you can't find anything please
[submit an issue](https://github.com/Unity-Technologies/ml-agents/issues) and
make sure to cite relevant information on OS, Python version, and exact error
message (whenever possible).


## Links discovered
- [`com.unity.ml-agents`](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents)
- [`mlagents`](https://github.com/Unity-Technologies/ml-agents/blob/develop/ml-agents.md)
- [`mlagents_envs`](https://github.com/Unity-Technologies/ml-agents/blob/develop/ml-agents-envs.md)
- [Project](https://github.com/Unity-Technologies/ml-agents/tree/main/Project/)
- [example environments](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Examples.md)
- [Getting Started Guide](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Getting-Started.md)
- [Download](https://unity3d.com/get-unity/download)
- [installing](https://www.python.org/downloads/)
- [instructions](https://packaging.python.org/guides/installing-using-linux-tools/#installing-pip-setuptools-wheel-with-linux-package-managers)
- [conda](https://docs.conda.io/en/latest/)
- [mamba](https://github.com/mamba-org/mamba)
- [Contributions Guidelines](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/CONTRIBUTING.md)
- [directly from the Package Manager registry](https://docs.unity3d.com/Manual/upm-ui-install.html)
- [add the local](https://docs.unity3d.com/Manual/upm-ui-local.html)
- [guide on Virtual Environments](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Using-Virtual-Environment.md)
- [Visual C++ Redistributable](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)
- [PyTorch installation guide](https://pytorch.org/get-started/locally/)
- [here](https://github.com/Unity-Technologies/ml-agents/releases)
- [setup.py file](https://github.com/Unity-Technologies/ml-agents/blob/develop/ml-agents/setup.py)
- [PyTorch](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Background-PyTorch.md)
- [Getting Started](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Getting-Started.md)
- [FAQ](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/FAQ.md)
- [Limitations](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Limitations.md)
- [submit an issue](https://github.com/Unity-Technologies/ml-agents/issues)

--- docs/Installation-Anaconda-Windows.md ---
# Installing ML-Agents Toolkit for Windows (Deprecated)

:warning: **Note:** We no longer use this guide ourselves and so it may not work
correctly. We've decided to keep it up just in case it is helpful to you.

The ML-Agents Toolkit supports Windows 10. While it might be possible to run the
ML-Agents Toolkit using other versions of Windows, it has not been tested on
other versions. Furthermore, the ML-Agents Toolkit has not been tested on a
Windows VM such as Bootcamp or Parallels.

To use the ML-Agents Toolkit, you install Python and the required Python
packages as outlined below. This guide also covers how set up GPU-based training
(for advanced users). GPU-based training is not currently required for the
ML-Agents Toolkit. However, training on a GPU might be required by future
versions and features.

## Step 1: Install Python via Anaconda

[Download](https://www.anaconda.com/download/#windows) and install Anaconda for
Windows. By using Anaconda, you can manage separate environments for different
distributions of Python. Python 3.7.2 or higher is required as we no longer
support Python 2. In this guide, we are using Python version 3.7 and Anaconda
version 5.1
([64-bit](https://repo.continuum.io/archive/Anaconda3-5.1.0-Windows-x86_64.exe)
or [32-bit](https://repo.continuum.io/archive/Anaconda3-5.1.0-Windows-x86.exe)
direct links).

<p align="center">
  <img src="images/anaconda_install.PNG"
       alt="Anaconda Install"
       width="500" border="10" />
</p>

We recommend the default _advanced installation options_. However, select the
options appropriate for your specific situation.

<p align="center">
  <img src="images/anaconda_default.PNG" alt="Anaconda Install" width="500" border="10" />
</p>

After installation, you must open **Anaconda Navigator** to finish the setup.
From the Windows search bar, type _anaconda navigator_. You can close Anaconda
Navigator after it opens.

If environment variables were not created, you will see error "conda is not
recognized as internal or external command" when you type `conda` into the
command line. To solve this you will need to set the environment variable
correctly.

Type `environment variables` in the search bar (this can be reached by hitting
the Windows key or the bottom left Windows button). You should see an option
called **Edit the system environment variables**.

<p align="center">
  <img src="images/edit_env_var.png"
       alt="edit env variables"
       width="250" border="10" />
</p>

From here, click the **Environment Variables** button. Double click "Path" under
**System variable** to edit the "Path" variable, click **New** to add the
following new paths.

```console
%UserProfile%\Anaconda3\Scripts
%UserProfile%\Anaconda3\Scripts\conda.exe
%UserProfile%\Anaconda3
%UserProfile%\Anaconda3\python.exe
```

## Step 2: Setup and Activate a New Conda Environment

You will create a new [Conda environment](https://conda.io/docs/) to be used
with the ML-Agents Toolkit. This means that all the packages that you install
are localized to just this environment. It will not affect any other
installation of Python or other environments. Whenever you want to run
ML-Agents, you will need activate this Conda environment.

To create a new Conda environment, open a new Anaconda Prompt (_Anaconda Prompt_
in the search bar) and type in the following command:

```sh
conda create -n ml-agents python=3.7
```

You may be asked to install new packages. Type `y` and press enter _(make sure
you are connected to the Internet)_. You must install these required packages.
The new Conda environment is called ml-agents and uses Python version 3.7.

<p align="center">
  <img src="images/conda_new.PNG" alt="Anaconda Install" width="500" border="10" />
</p>

To use this environment, you must activate it. _(To use this environment In the
future, you can run the same command)_. In the same Anaconda Prompt, type in the
following command:

```sh
activate ml-agents
```

You should see `(ml-agents)` prepended on the last line.

Next, install `tensorflow`. Install this package using `pip` - which is a
package management system used to install Python packages. Latest versions of
TensorFlow won't work, so you will need to make sure that you install version
1.7.1. In the same Anaconda Prompt, type in the following command _(make sure
you are connected to the Internet)_:

```sh
pip install tensorflow==1.7.1
```

## Step 3: Install Required Python Packages

The ML-Agents Toolkit depends on a number of Python packages. Use `pip` to
install these Python dependencies.

If you haven't already, clone the ML-Agents Toolkit Github repository to your
local computer. You can do this using Git
([download here](https://git-scm.com/download/win)) and running the following
commands in an Anaconda Prompt _(if you open a new prompt, be sure to activate
the ml-agents Conda environment by typing `activate ml-agents`)_:

```sh
git clone --branch release_22 https://github.com/Unity-Technologies/ml-agents.git
```

The `--branch release_22` option will switch to the tag of the latest stable
release. Omitting that will get the `main` branch which is potentially
unstable.

If you don't want to use Git, you can find download links on the
[releases page](https://github.com/Unity-Technologies/ml-agents/releases).

The `com.unity.ml-agents` subdirectory contains the core code to add to your
projects. The `Project` subdirectory contains many
[example environments](Learning-Environment-Examples.md) to help you get
started.

The `ml-agents` subdirectory contains a Python package which provides deep
reinforcement learning trainers to use with Unity environments.

The `ml-agents-envs` subdirectory contains a Python API to interface with Unity,
which the `ml-agents` package depends on.

The `gym-unity` subdirectory contains a package to interface with OpenAI Gym.

Keep in mind where the files were downloaded, as you will need the trainer
config files in this directory when running `mlagents-learn`. Make sure you are
connected to the Internet and then type in the Anaconda Prompt:

```console
python -m pip install mlagents==1.1.0
```

This will complete the installation of all the required Python packages to run
the ML-Agents Toolkit.

Sometimes on Windows, when you use pip to install certain Python packages, the
pip will get stuck when trying to read the cache of the package. If you see
this, you can try:

```console
python -m pip install mlagents==1.1.0 --no-cache-dir
```

This `--no-cache-dir` tells the pip to disable the cache.

### Installing for Development

If you intend to make modifications to `ml-agents` or `ml-agents-envs`, you
should install the packages from the cloned repo rather than from PyPi. To do
this, you will need to install `ml-agents` and `ml-agents-envs` separately.

In our example, the files are located in `C:\Downloads`. After you have either
cloned or downloaded the files, from the Anaconda Prompt, change to the
ml-agents subdirectory inside the ml-agents directory:

```console
cd C:\Downloads\ml-agents
```

From the repo's main directory, now run:

```console
cd ml-agents-envs
pip install -e .
cd ..
cd ml-agents
pip install -e .
```

Running pip with the `-e` flag will let you make changes to the Python files
directly and have those reflected when you run `mlagents-learn`. It is important
to install these packages in this order as the `mlagents` package depends on
`mlagents_envs`, and installing it in the other order will download
`mlagents_envs` from PyPi.

## (Optional) Step 4: GPU Training using The ML-Agents Toolkit

GPU is not required for the ML-Agents Toolkit and won't speed up the PPO
algorithm a lot during training(but something in the future will benefit from
GPU). This is a guide for advanced users who want to train using GPUs.
Additionally, you will need to check if your GPU is CUDA compatible. Please
check Nvidia's page [here](https://developer.nvidia.com/cuda-gpus).

Currently for the ML-Agents Toolkit, only CUDA v9.0 and cuDNN v7.0.5 is
supported.

### Install Nvidia CUDA toolkit

[Download](https://developer.nvidia.com/cuda-toolkit-archive) and install the
CUDA toolkit 9.0 from Nvidia's archive. The toolkit includes GPU-accelerated
libraries, debugging and optimization tools, a C/C++ (Step Visual Studio 2017)
compiler and a runtime library and is needed to run the ML-Agents Toolkit. In
this guide, we are using version
[9.0.176](https://developer.nvidia.com/compute/cuda/9.0/Prod/network_installers/cuda_9.0.176_win10_network-exe)).

Before installing, please make sure you **close any running instances of Unity
or Visual Studio**.

Run the installer and select the Express option. Note the directory where you
installed the CUDA toolkit. In this guide, we installed in the directory
`C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0`

### Install Nvidia cuDNN library

[Download](https://developer.nvidia.com/cudnn) and install the cuDNN library
from Nvidia. cuDNN is a GPU-accelerated library of primitives for deep neural
networks. Before you can download, you will need to sign up for free to the
Nvidia Developer Program.

<p align="center">
  <img src="images/cuDNN_membership_required.png"
       alt="cuDNN membership required"
       width="500" border="10" />
</p>

Once you've signed up, go back to the cuDNN
[downloads page](https://developer.nvidia.com/cudnn). You may or may not be
asked to fill out a short survey. When you get to the list cuDNN releases,
**make sure you are downloading the right version for the CUDA toolkit you
installed in Step 1.** In this guide, we are using version 7.0.5 for CUDA
toolkit version 9.0
([direct link](https://developer.nvidia.com/compute/machine-learning/cudnn/secure/v7.0.5/prod/9.0_20171129/cudnn-9.0-windows10-x64-v7)).

After you have downloaded the cuDNN files, you will need to extract the files
into the CUDA toolkit directory. In the cuDNN zip file, there are three folders
called `bin`, `include`, and `lib`.

<p align="center">
  <img src="images/cudnn_zip_files.PNG"
       alt="cuDNN zip files"
       width="500" border="10" />
</p>

Copy these three folders into the CUDA toolkit directory. The CUDA toolkit
directory is located at
`C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0`

<p align="center">
  <img src="images/cuda_toolkit_directory.PNG"
       alt="cuda toolkit directory"
       width="500" border="10" />
</p>

### Set Environment Variables

You will need to add one environment variable and two path variables.

To set the environment variable, type `environment variables` in the search bar
(this can be reached by hitting the Windows key or the bottom left Windows
button). You should see an option called **Edit the system environment
variables**.

<p align="center">
  <img src="images/edit_env_var.png"
       alt="edit env variables"
       width="250" border="10" />
</p>

From here, click the **Environment Variables** button. Click **New** to add a
new system variable _(make sure you do this under **System variables** and not
User variables_.

<p align="center">
  <img src="images/new_system_variable.PNG"
       alt="new system variable"
       width="500" border="10" />
</p>

For **Variable Name**, enter `CUDA_HOME`. For the variable value, put the
directory location for the CUDA toolkit. In this guide, the directory location
is `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0`. Press **OK** once.

<p align="center">
  <img src="images/system_variable_name_value.PNG"
       alt="system variable names and values"
       width="500" border="10" />
</p>

To set the two path variables, inside the same **Environment Variables** window
and under the second box called **System Variables**, find a variable called
`Path` and click **Edit**. You will add two directories to the list. For this
guide, the two entries would look like:

```console
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\lib\x64
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\extras\CUPTI\libx64
```

Make sure to replace the relevant directory location with the one you have
installed. _Please note that case sensitivity matters_.

<p align="center">
    <img src="images/path_variables.PNG"
        alt="Path variables"
        width="500" border="10" />
</p>

### Install TensorFlow GPU

Next, install `tensorflow-gpu` using `pip`. You'll need version 1.7.1. In an
Anaconda Prompt with the Conda environment ml-agents activated, type in the
following command to uninstall TensorFlow for cpu and install TensorFlow for gpu
_(make sure you are connected to the Internet)_:

```sh
pip uninstall tensorflow
pip install tensorflow-gpu==1.7.1
```

Lastly, you should test to see if everything installed properly and that
TensorFlow can identify your GPU. In the same Anaconda Prompt, open Python in
the Prompt by calling:

```sh
python
```

And then type the following commands:

```python
import tensorflow as tf

sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
```

You should see something similar to:

```console
Found device 0 with properties ...
```

## Acknowledgments

We would like to thank
[Jason Weimann](https://unity3d.college/2017/10/25/machine-learning-in-unity3d-setting-up-the-environment-tensorflow-for-agentml-on-windows-10/)
and
[Nitish S. Mutha](http://blog.nitishmutha.com/tensorflow/2017/01/22/TensorFlow-with-gpu-for-windows.html)
for writing the original articles which were used to create this guide.


## Links discovered
- [Download](https://www.anaconda.com/download/#windows)
- [64-bit](https://repo.continuum.io/archive/Anaconda3-5.1.0-Windows-x86_64.exe)
- [32-bit](https://repo.continuum.io/archive/Anaconda3-5.1.0-Windows-x86.exe)
- [Conda environment](https://conda.io/docs/)
- [download here](https://git-scm.com/download/win)
- [releases page](https://github.com/Unity-Technologies/ml-agents/releases)
- [example environments](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Examples.md)
- [here](https://developer.nvidia.com/cuda-gpus)
- [Download](https://developer.nvidia.com/cuda-toolkit-archive)
- [9.0.176](https://developer.nvidia.com/compute/cuda/9.0/Prod/network_installers/cuda_9.0.176_win10_network-exe)
- [Download](https://developer.nvidia.com/cudnn)
- [downloads page](https://developer.nvidia.com/cudnn)
- [direct link](https://developer.nvidia.com/compute/machine-learning/cudnn/secure/v7.0.5/prod/9.0_20171129/cudnn-9.0-windows10-x64-v7)
- [Jason Weimann](https://unity3d.college/2017/10/25/machine-learning-in-unity3d-setting-up-the-environment-tensorflow-for-agentml-on-windows-10/)
- [Nitish S. Mutha](http://blog.nitishmutha.com/tensorflow/2017/01/22/TensorFlow-with-gpu-for-windows.html)

--- docs/ML-Agents-Overview.md ---
# ML-Agents Toolkit Overview

**Table of Contents**

- [Running Example: Training NPC Behaviors](#running-example-training-npc-behaviors)
- [Key Components](#key-components)
- [Training Modes](#training-modes)
  - [Built-in Training and Inference](#built-in-training-and-inference)
    - [Cross-Platform Inference](#cross-platform-inference)
  - [Custom Training and Inference](#custom-training-and-inference)
- [Flexible Training Scenarios](#flexible-training-scenarios)
- [Training Methods: Environment-agnostic](#training-methods-environment-agnostic)
  - [A Quick Note on Reward Signals](#a-quick-note-on-reward-signals)
  - [Deep Reinforcement Learning](#deep-reinforcement-learning)
    - [Curiosity for Sparse-reward Environments](#curiosity-for-sparse-reward-environments)
    - [RND for Sparse-reward Environments](#rnd-for-sparse-reward-environments)
  - [Imitation Learning](#imitation-learning)
    - [GAIL (Generative Adversarial Imitation Learning)](#gail-generative-adversarial-imitation-learning)
    - [Behavioral Cloning (BC)](#behavioral-cloning-bc)
    - [Recording Demonstrations](#recording-demonstrations)
  - [Summary](#summary)
- [Training Methods: Environment-specific](#training-methods-environment-specific)
  - [Training in Competitive Multi-Agent Environments with Self-Play](#training-in-competitive-multi-agent-environments-with-self-play)
  - [Training in Cooperative Multi-Agent Environments with MA-POCA](#training-in-cooperative-multi-agent-environments-with-ma-poca)
  - [Solving Complex Tasks using Curriculum Learning](#solving-complex-tasks-using-curriculum-learning)
  - [Training Robust Agents using Environment Parameter Randomization](#training-robust-agents-using-environment-parameter-randomization)
- [Model Types](#model-types)
  - [Learning from Vector Observations](#learning-from-vector-observations)
  - [Learning from Cameras using Convolutional Neural Networks](#learning-from-cameras-using-convolutional-neural-networks)
  - [Learning from Variable Length Observations using Attention](#learning-from-variable-length-observations-using-attention)
  - [Memory-enhanced Agents using Recurrent Neural Networks](#memory-enhanced-agents-using-recurrent-neural-networks)
- [Additional Features](#additional-features)
- [Summary and Next Steps](#summary-and-next-steps)

**The Unity Machine Learning Agents Toolkit** (ML-Agents Toolkit) is an
open-source project that enables games and simulations to serve as environments
for training intelligent agents. Agents can be trained using reinforcement
learning, imitation learning, neuroevolution, or other machine learning methods
through a simple-to-use Python API. We also provide implementations (based on
PyTorch) of state-of-the-art algorithms to enable game developers and
hobbyists to easily train intelligent agents for 2D, 3D and VR/AR games. These
trained agents can be used for multiple purposes, including controlling NPC
behavior (in a variety of settings such as multi-agent and adversarial),
automated testing of game builds and evaluating different game design decisions
pre-release. The ML-Agents Toolkit is mutually beneficial for both game
developers and AI researchers as it provides a central platform where advances
in AI can be evaluated on Unity’s rich environments and then made accessible to
the wider research and game developer communities.

Depending on your background (i.e. researcher, game developer, hobbyist), you
may have very different questions on your mind at the moment. To make your
transition to the ML-Agents Toolkit easier, we provide several background pages
that include overviews and helpful resources on the
[Unity Engine](Background-Unity.md),
[machine learning](Background-Machine-Learning.md) and
[PyTorch](Background-PyTorch.md). We **strongly** recommend browsing the
relevant background pages if you're not familiar with a Unity scene, basic
machine learning concepts or have not previously heard of PyTorch.

The remainder of this page contains a deep dive into ML-Agents, its key
components, different training modes and scenarios. By the end of it, you should
have a good sense of _what_ the ML-Agents Toolkit allows you to do. The
subsequent documentation pages provide examples of _how_ to use ML-Agents. To
get started, watch this
[demo video of ML-Agents in action](https://www.youtube.com/watch?v=fiQsmdwEGT8&feature=youtu.be).

## Running Example: Training NPC Behaviors

To help explain the material and terminology in this page, we'll use a
hypothetical, running example throughout. We will explore the problem of
training the behavior of a non-playable character (NPC) in a game. (An NPC is a
game character that is never controlled by a human player and its behavior is
pre-defined by the game developer.) More specifically, let's assume we're
building a multi-player, war-themed game in which players control the soldiers.
In this game, we have a single NPC who serves as a medic, finding and reviving
wounded players. Lastly, let us assume that there are two teams, each with five
players and one NPC medic.

The behavior of a medic is quite complex. It first needs to avoid getting
injured, which requires detecting when it is in danger and moving to a safe
location. Second, it needs to be aware of which of its team members are injured
and require assistance. In the case of multiple injuries, it needs to assess the
degree of injury and decide who to help first. Lastly, a good medic will always
place itself in a position where it can quickly help its team members. Factoring
in all of these traits means that at every instance, the medic needs to measure
several attributes of the environment (e.g. position of team members, position
of enemies, which of its team members are injured and to what degree) and then
decide on an action (e.g. hide from enemy fire, move to help one of its
members). Given the large number of settings of the environment and the large
number of actions that the medic can take, defining and implementing such
complex behaviors by hand is challenging and prone to errors.

With ML-Agents, it is possible to _train_ the behaviors of such NPCs (called
**Agents**) using a variety of methods. The basic idea is quite simple. We need
to define three entities at every moment of the game (called **environment**):

- **Observations** - what the medic perceives about the environment.
  Observations can be numeric and/or visual. Numeric observations measure
  attributes of the environment from the point of view of the agent. For our
  medic this would be attributes of the battlefield that are visible to it. For
  most interesting environments, an agent will require several continuous
  numeric observations. Visual observations, on the other hand, are images
  generated from the cameras attached to the agent and represent what the agent
  is seeing at that point in time. It is common to confuse an agent's
  observation with the environment (or game) **state**. The environment state
  represents information about the entire scene containing all the game
  characters. The agents observation, however, only contains information that
  the agent is aware of and is typically a subset of the environment state. For
  example, the medic observation cannot include information about an enemy in
  hiding that the medic is unaware of.
- **Actions** - what actions the medic can take. Similar to observations,
  actions can either be continuous or discrete depending on the complexity of
  the environment and agent. In the case of the medic, if the environment is a
  simple grid world where only their location matters, then a discrete action
  taking on one of four values (north, south, east, west) suffices. However, if
  the environment is more complex and the medic can move freely then using two
  continuous actions (one for direction and another for speed) is more
  appropriate.
- **Reward signals** - a scalar value indicating how well the medic is doing.
  Note that the reward signal need not be provided at every moment, but only
  when the medic performs an action that is good or bad. For example, it can
  receive a large negative reward if it dies, a modest positive reward whenever
  it revives a wounded team member, and a modest negative reward when a wounded
  team member dies due to lack of assistance. Note that the reward signal is how
  the objectives of the task are communicated to the agent, so they need to be
  set up in a manner where maximizing reward generates the desired optimal
  behavior.

After defining these three entities (the building blocks of a **reinforcement
learning task**), we can now _train_ the medic's behavior. This is achieved by
simulating the environment for many trials where the medic, over time, learns
what is the optimal action to take for every observation it measures by
maximizing its future reward. The key is that by learning the actions that
maximize its reward, the medic is learning the behaviors that make it a good
medic (i.e. one who saves the most number of lives). In **reinforcement
learning** terminology, the behavior that is learned is called a **policy**,
which is essentially a (optimal) mapping from observations to actions. Note that
the process of learning a policy through running simulations is called the
**training phase**, while playing the game with an NPC that is using its learned
policy is called the **inference phase**.

The ML-Agents Toolkit provides all the necessary tools for using Unity as the
simulation engine for learning the policies of different objects in a Unity
environment. In the next few sections, we discuss how the ML-Agents Toolkit
achieves this and what features it provides.

## Key Components

The ML-Agents Toolkit contains five high-level components:

- **Learning Environment** - which contains the Unity scene and all the game
  characters. The Unity scene provides the environment in which agents observe,
  act, and learn. How you set up the Unity scene to serve as a learning
  environment really depends on your goal. You may be trying to solve a specific
  reinforcement learning problem of limited scope, in which case you can use the
  same scene for both training and for testing trained agents. Or, you may be
  training agents to operate in a complex game or simulation. In this case, it
  might be more efficient and practical to create a purpose-built training
  scene. The ML-Agents Toolkit includes an ML-Agents Unity SDK
  (`com.unity.ml-agents` package) that enables you to transform any Unity scene
  into a learning environment by defining the agents and their behaviors.
- **Python Low-Level API** - which contains a low-level Python interface for
  interacting and manipulating a learning environment. Note that, unlike the
  Learning Environment, the Python API is not part of Unity, but lives outside
  and communicates with Unity through the Communicator. This API is contained in
  a dedicated `mlagents_envs` Python package and is used by the Python training
  process to communicate with and control the Academy during training. However,
  it can be used for other purposes as well. For example, you could use the API
  to use Unity as the simulation engine for your own machine learning
  algorithms. See [Python API](Python-LLAPI.md) for more information.
- **External Communicator** - which connects the Learning Environment with the
  Python Low-Level API. It lives within the Learning Environment.
- **Python Trainers** which contains all the machine learning algorithms that
  enable training agents. The algorithms are implemented in Python and are part
  of their own `mlagents` Python package. The package exposes a single
  command-line utility `mlagents-learn` that supports all the training methods
  and options outlined in this document. The Python Trainers interface solely
  with the Python Low-Level API.
- **Gym Wrapper** (not pictured). A common way in which machine learning
  researchers interact with simulation environments is via a wrapper provided by
  OpenAI called [gym](https://github.com/openai/gym). We provide a gym wrapper
  in the `ml-agents-envs` package and [instructions](Python-Gym-API.md) for using
  it with existing machine learning algorithms which utilize gym.
- **PettingZoo Wrapper** (not pictured) PettingZoo is python API for
  interacting with multi-agent simulation environments that provides a
  gym-like interface. We provide a PettingZoo wrapper for Unity ML-Agents
  environments in the `ml-agents-envs` package and
  [instructions](Python-PettingZoo-API.md) for using it with machine learning
  algorithms.

<p align="center">
  <img src="../images/learning_environment_basic.png"
       alt="Simplified ML-Agents Scene Block Diagram"
       width="600"
       border="10" />
</p>

_Simplified block diagram of ML-Agents._

The Learning Environment contains two Unity Components that help organize the
Unity scene:

- **Agents** - which is attached to a Unity GameObject (any character within a
  scene) and handles generating its observations, performing the actions it
  receives and assigning a reward (positive / negative) when appropriate. Each
  Agent is linked to a Behavior.
- **Behavior** - defines specific attributes of the agent such as the number of
  actions that agent can take. Each Behavior is uniquely identified by a
  `Behavior Name` field. A Behavior can be thought as a function that receives
  observations and rewards from the Agent and returns actions. A Behavior can be
  of one of three types: Learning, Heuristic or Inference. A Learning Behavior
  is one that is not, yet, defined but about to be trained. A Heuristic Behavior
  is one that is defined by a hard-coded set of rules implemented in code. An
  Inference Behavior is one that includes a trained Neural Network file. In
  essence, after a Learning Behavior is trained, it becomes an Inference
  Behavior.

Every Learning Environment will always have one Agent for every character in the
scene. While each Agent must be linked to a Behavior, it is possible for Agents
that have similar observations and actions to have the same Behavior. In our
sample game, we have two teams each with their own medic. Thus we will have two
Agents in our Learning Environment, one for each medic, but both of these medics
can have the same Behavior. This does not mean that at each instance they will
have identical observation and action _values_.

<p align="center">
  <img src="../images/learning_environment_example.png"
       alt="Example ML-Agents Scene Block Diagram"
       width="700"
       border="10" />
</p>

_Example block diagram of ML-Agents Toolkit for our sample game._

Note that in a single environment, there can be multiple Agents and multiple
Behaviors at the same time. For example, if we expanded our game to include tank
driver NPCs, then the Agent attached to those characters cannot share its
Behavior with the Agent linked to the medics (medics and drivers have different
actions). The Learning Environment through the Academy (not represented in the
diagram) ensures that all the Agents are in sync in addition to controlling
environment-wide settings.

Lastly, it is possible to exchange data between Unity and Python outside of the
machine learning loop through _Side Channels_. One example of using _Side
Channels_ is to exchange data with Python about _Environment Parameters_. The
following diagram illustrates the above.

<p align="center">
  <img src="../images/learning_environment_full.png"
       alt="More Complete Example ML-Agents Scene Block Diagram"
       border="10" />
</p>

## Training Modes

Given the flexibility of ML-Agents, there are a few ways in which training and
inference can proceed.

### Built-in Training and Inference

As mentioned previously, the ML-Agents Toolkit ships with several
implementations of state-of-the-art algorithms for training intelligent agents.
More specifically, during training, all the medics in the scene send their
observations to the Python API through the External Communicator. The Python API
processes these observations and sends back actions for each medic to take.
During training these actions are mostly exploratory to help the Python API
learn the best policy for each medic. Once training concludes, the learned
policy for each medic can be exported as a model file. Then during the inference
phase, the medics still continue to generate their observations, but instead of
being sent to the Python API, they will be fed into their (internal, embedded)
model to generate the _optimal_ action for each medic to take at every point in
time.

The [Getting Started Guide](Getting-Started.md) tutorial covers this training
mode with the **3D Balance Ball** sample environment.

#### Cross-Platform Inference

It is important to note that the ML-Agents Toolkit leverages the
[Inference Engine](Inference-Engine.md) to run the models within a
Unity scene such that an agent can take the _optimal_ action at each step. Given
that Inference Engine supports all Unity runtime platforms, this
means that any model you train with the ML-Agents Toolkit can be embedded into
your Unity application that runs on any platform. See our
[dedicated blog post](https://blogs.unity3d.com/2019/03/01/unity-ml-agents-toolkit-v0-7-a-leap-towards-cross-platform-inference/)
for additional information.

### Custom Training and Inference

In the previous mode, the Agents were used for training to generate a PyTorch
model that the Agents can later use. However, any user of the ML-Agents Toolkit
can leverage their own algorithms for training. In this case, the behaviors of
all the Agents in the scene will be controlled within Python. You can even turn
your environment into a [gym.](Python-Gym-API.md)

We do not currently have a tutorial highlighting this mode, but you can learn
more about the Python API [here](Python-LLAPI.md).

## Flexible Training Scenarios

While the discussion so-far has mostly focused on training a single agent, with
ML-Agents, several training scenarios are possible. We are excited to see what
kinds of novel and fun environments the community creates. For those new to
training intelligent agents, below are a few examples that can serve as
inspiration:

- Single-Agent. A single agent, with its own reward signal. The traditional way
  of training an agent. An example is any single-player game, such as Chicken.
- Simultaneous Single-Agent. Multiple independent agents with independent reward
  signals with same `Behavior Parameters`. A parallelized version of the
  traditional training scenario, which can speed-up and stabilize the training
  process. Helpful when you have multiple versions of the same character in an
  environment who should learn similar behaviors. An example might be training a
  dozen robot-arms to each open a door simultaneously.
- Adversarial Self-Play. Two interacting agents with inverse reward signals. In
  two-player games, adversarial self-play can allow an agent to become
  increasingly more skilled, while always having the perfectly matched opponent:
  itself. This was the strategy employed when training AlphaGo, and more
  recently used by OpenAI to train a human-beating 1-vs-1 Dota 2 agent.
- Cooperative Multi-Agent. Multiple interacting agents with a shared reward
  signal with same or different `Behavior Parameters`. In this scenario, all
  agents must work together to accomplish a task that cannot be done alone.
  Examples include environments where each agent only has access to partial
  information, which needs to be shared in order to accomplish the task or
  collaboratively solve a puzzle.
- Competitive Multi-Agent. Multiple interacting agents with inverse reward
  signals with same or different `Behavior Parameters`. In this scenario, agents
  must compete with one another to either win a competition, or obtain some
  limited set of resources. All team sports fall into this scenario.
- Ecosystem. Multiple interacting agents with independent reward signals with
  same or different `Behavior Parameters`. This scenario can be thought of as
  creating a small world in which animals with different goals all interact,
  such as a savanna in which there might be zebras, elephants and giraffes, or
  an autonomous driving simulation within an urban environment.

## Training Methods: Environment-agnostic

The remaining sections overview the various state-of-the-art machine learning
algorithms that are part of the ML-Agents Toolkit. If you aren't studying
machine and reinforcement learning as a subject and just want to train agents to
accomplish tasks, you can treat these algorithms as _black boxes_. There are a
few training-related parameters to adjust inside Unity as well as on the Python
training side, but you do not need in-depth knowledge of the algorithms
themselves to successfully create and train agents. Step-by-step procedures for
running the training process are provided in the
[Training ML-Agents](Training-ML-Agents.md) page.

This section specifically focuses on the training methods that are available
regardless of the specifics of your learning environment.

#### A Quick Note on Reward Signals

In this section we introduce the concepts of _intrinsic_ and _extrinsic_
rewards, which helps explain some of the training methods.

In reinforcement learning, the end goal for the Agent is to discover a behavior
(a Policy) that maximizes a reward. You will need to provide the agent one or
more reward signals to use during training. Typically, a reward is defined by
your environment, and corresponds to reaching some goal. These are what we refer
to as _extrinsic_ rewards, as they are defined external of the learning
algorithm.

Rewards, however, can be defined outside of the environment as well, to
encourage the agent to behave in certain ways, or to aid the learning of the
true extrinsic reward. We refer to these rewards as _intrinsic_ reward signals.
The total reward that the agent will learn to maximize can be a mix of extrinsic
and intrinsic reward signals.

The ML-Agents Toolkit allows reward signals to be defined in a modular way, and
we provide four reward signals that can the mixed and matched to help shape
your agent's behavior:

- `extrinsic`: represents the rewards defined in your environment, and is
  enabled by default
- `gail`: represents an intrinsic reward signal that is defined by GAIL (see
  below)
- `curiosity`: represents an intrinsic reward signal that encourages exploration
  in sparse-reward environments that is defined by the Curiosity module (see
  below).
- `rnd`: represents an intrinsic reward signal that encourages exploration
  in sparse-reward environments that is defined by the Curiosity module (see
  below).

### Deep Reinforcement Learning

ML-Agents provide an implementation of two reinforcement learning algorithms:

- [Proximal Policy Optimization (PPO)](https://openai.com/research/openai-baselines-ppo)
- [Soft Actor-Critic (SAC)](https://bair.berkeley.edu/blog/2018/12/14/sac/)

The default algorithm is PPO. This is a method that has been shown to be more
general purpose and stable than many other RL algorithms.

In contrast with PPO, SAC is _off-policy_, which means it can learn from
experiences collected at any time during the past. As experiences are collected,
they are placed in an experience replay buffer and randomly drawn during
training. This makes SAC significantly more sample-efficient, often requiring
5-10 times less samples to learn the same task as PPO. However, SAC tends to
require more model updates. SAC is a good choice for heavier or slower
environments (about 0.1 seconds per step or more). SAC is also a "maximum
entropy" algorithm, and enables exploration in an intrinsic way. Read more about
maximum entropy RL
[here](https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/).

#### Curiosity for Sparse-reward Environments

In environments where the agent receives rare or infrequent rewards (i.e.
sparse-reward), an agent may never receive a reward signal on which to bootstrap
its training process. This is a scenario where the use of an intrinsic reward
signals can be valuable. Curiosity is one such signal which can help the agent
explore when extrinsic rewards are sparse.

The `curiosity` Reward Signal enables the Intrinsic Curiosity Module. This is an
implementation of the approach described in
[Curiosity-driven Exploration by Self-supervised Prediction](https://pathak22.github.io/noreward-rl/)
by Pathak, et al. It trains two networks:

- an inverse model, which takes the current and next observation of the agent,
  encodes them, and uses the encoding to predict the action that was taken
  between the observations
- a forward model, which takes the encoded current observation and action, and
  predicts the next encoded observation.

The loss of the forward model (the difference between the predicted and actual
encoded observations) is used as the intrinsic reward, so the more surprised the
model is, the larger the reward will be.

For more information, see our dedicated
[blog post on the Curiosity module](https://blogs.unity3d.com/2018/06/26/solving-sparse-reward-tasks-with-curiosity/).

#### RND for Sparse-reward Environments

Similarly to Curiosity, Random Network Distillation (RND) is useful in sparse or rare
reward environments as it helps the Agent explore. The RND Module is implemented following
the paper [Exploration by Random Network Distillation](https://arxiv.org/abs/1810.12894).
RND uses two networks:

 - The first is a network with fixed random weights that takes observations as inputs and
 generates an encoding
 - The second is a network with similar architecture that is trained to predict the
 outputs of the first network and uses the observations the Agent collects as training data.

The loss (the squared difference between the predicted and actual encoded observations)
of the trained model is used as intrinsic reward. The more an Agent visits a state, the
more accurate the predictions and the lower the rewards which encourages the Agent to
explore new states with higher prediction errors.

### Imitation Learning

It is often more intuitive to simply demonstrate the behavior we want an agent
to perform, rather than attempting to have it learn via trial-and-error methods.
For example, instead of indirectly training a medic with the help of a reward
function, we can give the medic real world examples of observations from the
game and actions from a game controller to guide the medic's behavior. Imitation
Learning uses pairs of observations and actions from a demonstration to learn a
policy. See this [video demo](https://youtu.be/kpb8ZkMBFYs) of imitation
learning .

Imitation learning can either be used alone or in conjunction with reinforcement
learning. If used alone it can provide a mechanism for learning a specific type
of behavior (i.e. a specific style of solving the task). If used in conjunction
with reinforcement learning it can dramatically reduce the time the agent takes
to solve the environment. This can be especially pronounced in sparse-reward
environments. For instance, on the
[Pyramids environment](Learning-Environment-Examples.md#pyramids), using 6
episodes of demonstrations can reduce training steps by more than 4 times. See
Behavioral Cloning + GAIL + Curiosity + RL below.

<p align="center">
  <img src="../images/mlagents-ImitationAndRL.png"
       alt="Using Demonstrations with Reinforcement Learning"
       width="700" border="0" />
</p>

The ML-Agents Toolkit provides a way to learn directly from demonstrations, as
well as use them to help speed up reward-based training (RL). We include two
algorithms called Behavioral Cloning (BC) and Generative Adversarial Imitation
Learning (GAIL). In most scenarios, you can combine these two features:

- If you want to help your agents learn (especially with environments that have
  sparse rewards) using pre-recorded demonstrations, you can generally enable
  both GAIL and Behavioral Cloning at low strengths in addition to having an
  extrinsic reward. An example of this is provided for the PushBlock example
  environment in `config/imitation/PushBlock.yaml`.
- If you want to train purely from demonstrations with GAIL and BC _without_ an
  extrinsic reward signal, please see the CrawlerStatic example environment under
  in `config/imitation/CrawlerStatic.yaml`.

***Note:*** GAIL introduces a [_survivor bias_](https://arxiv.org/pdf/1809.02925.pdf)
to the learning process. That is, by giving positive rewards based on similarity
to the expert, the agent is incentivized to remain alive for as long as possible.
This can directly conflict with goal-oriented tasks like our PushBlock or Pyramids
example environments where an agent must reach a goal state thus ending the
episode as quickly as possible. In these cases, we strongly recommend that you
use a low strength GAIL reward signal and a sparse extrinsic signal when
the agent achieves the task. This way, the GAIL reward signal will guide the
agent until it discovers the extrinsic signal and will not overpower it. If the
agent appears to be ignoring the extrinsic reward signal, you should reduce
the strength of GAIL.

#### GAIL (Generative Adversarial Imitation Learning)

GAIL, or
[Generative Adversarial Imitation Learning](https://arxiv.org/abs/1606.03476),
uses an adversarial approach to reward your Agent for behaving similar to a set
of demonstrations. GAIL can be used with or without environment rewards, and
works well when there are a limited number of demonstrations. In this framework,
a second neural network, the discriminator, is taught to distinguish whether an
observation/action is from a demonstration or produced by the agent. This
discriminator can then examine a new observation/action and provide it a reward
based on how close it believes this new observation/action is to the provided
demonstrations.

At each training step, the agent tries to learn how to maximize this reward.
Then, the discriminator is trained to better distinguish between demonstrations
and agent state/actions. In this way, while the agent gets better and better at
mimicking the demonstrations, the discriminator keeps getting stricter and
stricter and the agent must try harder to "fool" it.

This approach learns a _policy_ that produces states and actions similar to the
demonstrations, requiring fewer demonstrations than direct cloning of the
actions. In addition to learning purely from demonstrations, the GAIL reward
signal can be mixed with an extrinsic reward signal to guide the learning
process.

#### Behavioral Cloning (BC)

BC trains the Agent's policy to exactly mimic the actions shown in a set of
demonstrations. The BC feature can be enabled on the PPO or SAC trainers. As BC
cannot generalize past the examples shown in the demonstrations, BC tends to
work best when there exists demonstrations for nearly all of the states that the
agent can experience, or in conjunction with GAIL and/or an extrinsic reward.

#### Recording Demonstrations

Demonstrations of agent behavior can be recorded from the Unity Editor or build,
and saved as assets. These demonstrations contain information on the
observations, actions, and rewards for a given agent during the recording
session. They can be managed in the Editor, as well as used for training with BC
and GAIL. See the
[Designing Agents](Learning-Environment-Design-Agents.md#recording-demonstrations)
page for more information on how to record demonstrations for your agent.

### Summary

To summarize, we provide 3 training methods: BC, GAIL and RL (PPO or SAC) that
can be used independently or together:

- BC can be used on its own or as a pre-training step before GAIL and/or RL
- GAIL can be used with or without extrinsic rewards
- RL can be used on its own (either PPO or SAC) or in conjunction with BC and/or
  GAIL.

Leveraging either BC or GAIL requires recording demonstrations to be provided as
input to the training algorithms.

## Training Methods: Environment-specific

In addition to the three environment-agnostic training methods introduced in the
previous section, the ML-Agents Toolkit provides additional methods that can aid
in training behaviors for specific types of environments.

### Training in Competitive Multi-Agent Environments with Self-Play

ML-Agents provides the functionality to train both symmetric and asymmetric
adversarial games with
[Self-Play](https://openai.com/research/competitive-self-play). A symmetric game is
one in which opposing agents are equal in form, function and objective. Examples
of symmetric games are our Tennis and Soccer example environments. In
reinforcement learning, this means both agents have the same observation and
actions and learn from the same reward function and so _they can share the
same policy_. In asymmetric games, this is not the case. An example of an
asymmetric games are Hide and Seek. Agents in these types of games do not always
have the same observation or actions and so sharing policy networks is not
necessarily ideal.

With self-play, an agent learns in adversarial games by competing against fixed,
past versions of its opponent (which could be itself as in symmetric games) to
provide a more stable, stationary learning environment. This is compared to
competing against the current, best opponent in every episode, which is
constantly changing (because it's learning).

Self-play can be used with our implementations of both Proximal Policy
Optimization (PPO) and Soft Actor-Critic (SAC). However, from the perspective of
an individual agent, these scenarios appear to have non-stationary dynamics
because the opponent is often changing. This can cause significant issues in the
experience replay mechanism used by SAC. Thus, we recommend that users use PPO.
For further reading on this issue in particular, see the paper
[Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/1702.08887.pdf).

See our
[Designing Agents](Learning-Environment-Design-Agents.md#defining-teams-for-multi-agent-scenarios)
page for more information on setting up teams in your Unity scene. Also, read
our
[blog post on self-play](https://blogs.unity3d.com/2020/02/28/training-intelligent-adversaries-using-self-play-with-ml-agents/)
for additional information. Additionally, check [ELO Rating System](ELO-Rating-System.md) the method we use to calculate
the relative skill level between two players.

### Training In Cooperative Multi-Agent Environments with MA-POCA

![PushBlock with Agents Working Together](images/cooperative_pushblock.png)

ML-Agents provides the functionality for training cooperative behaviors - i.e.,
groups of agents working towards a common goal, where the success of the individual
is linked to the success of the whole group. In such a scenario, agents typically receive
rewards as a group. For instance, if a team of agents wins a game against an opposing
team, everyone is rewarded - even agents who did not directly contribute to the win. This
makes learning what to do as an individual difficult - you may get a win
for doing nothing, and a loss for doing your best.

In ML-Agents, we provide MA-POCA (MultiAgent POsthumous Credit Assignment), which
is a novel multi-agent trainer that trains a _centralized critic_, a neural network
that acts as a "coach" for a whole group of agents. You can then give rewards to the team
as a whole, and the agents will learn how best to contribute to achieving that reward.
Agents can _also_ be given rewards individually, and the team will work together to help the
individual achieve those goals. During an episode, agents can be added or removed from the group,
such as when agents spawn or die in a game. If agents are removed mid-episode (e.g., if teammates die
or are removed from the game), they will still learn whether their actions contributed
to the team winning later, enabling agents to take group-beneficial actions even if
they result in the individual being removed from the game (i.e., self-sacrifice).
MA-POCA can also be combined with self-play to train teams of agents to play against each other.

To learn more about enabling cooperative behaviors for agents in an ML-Agents environment,
check out [this page](Learning-Environment-Design-Agents.md#groups-for-cooperative-scenarios).

To learn more about MA-POCA, please see our paper
[On the Use and Misuse of Absorbing States in Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2111.05992.pdf).
For further reading, MA-POCA builds on previous work in multi-agent cooperative learning
([Lowe et al.](https://arxiv.org/abs/1706.02275), [Foerster et al.](https://arxiv.org/pdf/1705.08926.pdf),
among others) to enable the above use-cases.

### Solving Complex Tasks using Curriculum Learning

Curriculum learning is a way of training a machine learning model where more
difficult aspects of a problem are gradually introduced in such a way that the
model is always optimally challenged. This idea has been around for a long time,
and it is how we humans typically learn. If you imagine any childhood primary
school education, there is an ordering of classes and topics. Arithmetic is
taught before algebra, for example. Likewise, algebra is taught before calculus.
The skills and knowledge learned in the earlier subjects provide a scaffolding
for later lessons. The same principle can be applied to machine learning, where
training on easier tasks can provide a scaffolding for harder tasks in the
future.

Imagine training the medic to scale a wall to arrive at a wounded team
member. The starting point when training a medic to accomplish this task will be
a random policy. That starting policy will have the medic running in circles,
and will likely never, or very rarely scale the wall properly to revive their
team member (and achieve the reward). If we start with a simpler task, such as
moving toward an unobstructed team member, then the medic can easily learn to
accomplish the task. From there, we can slowly add to the difficulty of the task
by increasing the size of the wall until the medic can complete the initially
near-impossible task of scaling the wall. We have included an environment to
demonstrate this with ML-Agents, called
[Wall Jump](Learning-Environment-Examples.md#wall-jump).

![Wall](images/curriculum.png)

_Demonstration of a hypothetical curriculum training scenario in which a
progressively taller wall obstructs the path to the goal._

_[**Note**: The example provided above is for instructional purposes, and was
based on an early version of the
[Wall Jump example environment](Learning-Environment-Examples.md). As such, it
is not possible to directly replicate the results here using that environment.]_

The ML-Agents Toolkit supports modifying custom environment parameters during
the training process to aid in learning. This allows elements of the environment
related to difficulty or complexity to be dynamically adjusted based on training
progress. The [Training ML-Agents](Training-ML-Agents.md#curriculum-learning)
page has more information on defining training curriculums.

### Training Robust Agents using Environment Parameter Randomization

An agent trained on a specific environment, may be unable to generalize to any
tweaks or variations in the environment (in machine learning this is referred to
as overfitting). This becomes problematic in cases where environments are
instantiated with varying objects or properties. One mechanism to alleviate this
and train more robust agents that can generalize to unseen variations of the
environment is to expose them to these variations during training. Similar to
Curriculum Learning, where environments become more difficult as the agent
learns, the ML-Agents Toolkit provides a way to randomly sample parameters of
the environment during training. We refer to this approach as **Environment
Parameter Randomization**. For those familiar with Reinforcement Learning
research, this approach is based on the concept of
[Domain Randomization](https://arxiv.org/abs/1703.06907). By using
[parameter randomization during training](Training-ML-Agents.md#environment-parameter-randomization),
the agent can be better suited to adapt (with higher performance) to future
unseen variations of the environment.

|      Ball scale of 0.5       |      Ball scale of 4       |
| :--------------------------: | :------------------------: |
| ![](images/3dball_small.png) | ![](images/3dball_big.png) |

_Example of variations of the 3D Ball environment. The environment parameters
are `gravity`, `ball_mass` and `ball_scale`._

## Model Types

Regardless of the training method deployed, there are a few model types that
users can train using the ML-Agents Toolkit. This is due to the flexibility in
defining agent observations, which include vector, ray cast and visual
observations. You can learn more about how to instrument an agent's observation
in the [Designing Agents](Learning-Environment-Design-Agents.md) guide.

### Learning from Vector Observations

Whether an agent's observations are ray cast or vector, the ML-Agents Toolkit
provides a fully connected neural network model to learn from those
observations. At training time you can configure different aspects of this model
such as the number of hidden units and number of layers.

### Learning from Cameras using Convolutional Neural Networks

Unlike other platforms, where the agent’s observation might be limited to a
single vector or image, the ML-Agents Toolkit allows multiple cameras to be used
for observations per agent. This enables agents to learn to integrate
information from multiple visual streams. This can be helpful in several
scenarios such as training a self-driving car which requires multiple cameras
with different viewpoints, or a navigational agent which might need to integrate
aerial and first-person visuals. You can learn more about adding visual
observations to an agent
[here](Learning-Environment-Design-Agents.md#multiple-visual-observations).

When visual observations are utilized, the ML-Agents Toolkit leverages
convolutional neural networks (CNN) to learn from the input images. We offer
three network architectures:

- a simple encoder which consists of two convolutional layers
- the implementation proposed by
  [Mnih et al.](https://www.nature.com/articles/nature14236), consisting of
  three convolutional layers,
- the [IMPALA Resnet](https://arxiv.org/abs/1802.01561) consisting of three
  stacked layers, each with two residual blocks, making a much larger network
  than the other two.

The choice of the architecture depends on the visual complexity of the scene and
the available computational resources.

### Learning from Variable Length Observations using Attention

Using the ML-Agents Toolkit, it is possible to have agents learn from a
varying number of inputs. To do so, each agent can keep track of a buffer
of vector observations. At each step, the agent will go through all the
elements in the buffer and extract information but the elements
in the buffer can change at every step.
This can be useful in scenarios in which the agents must keep track of
a varying number of elements throughout the episode. For example in a game
where an agent must learn to avoid projectiles, but the projectiles can vary in
numbers.

![Variable Length Observations Illustrated](images/variable-length-observation-illustrated.png)

You can learn more about variable length observations
[here](Learning-Environment-Design-Agents.md#variable-length-observations).
When variable length observations are utilized, the ML-Agents Toolkit
leverages attention networks to learn from a varying number of entities.
Agents using attention will ignore entities that are deemed not relevant
and pay special attention to entities relevant to the current situation
based on context.

### Memory-enhanced Agents using Recurrent Neural Networks

Have you ever entered a room to get something and immediately forgot what you
were looking for? Don't let that happen to your agents.

![Inspector](images/ml-agents-LSTM.png)

In some scenarios, agents must learn to remember the past in order to take the
best decision. When an agent only has partial observability of the environment,
keeping track of past observations can help the agent learn. Deciding what the
agents should remember in order to solve a task is not easy to do by hand, but
our training algorithms can learn to keep track of what is important to remember
with [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory).

## Additional Features

Beyond the flexible training scenarios available, the ML-Agents Toolkit includes
additional features which improve the flexibility and interpretability of the
training process.

- **Concurrent Unity Instances** - We enable developers to run concurrent,
  parallel instances of the Unity executable during training. For certain
  scenarios, this should speed up training. Check out our dedicated page on
  [creating a Unity executable](Learning-Environment-Executable.md) and the
  [Training ML-Agents](Training-ML-Agents.md#training-using-concurrent-unity-instances)
  page for instructions on how to set the number of concurrent instances.
- **Recording Statistics from Unity** - We enable developers to
  [record statistics](Learning-Environment-Design.md#recording-statistics) from
  within their Unity environments. These statistics are aggregated and generated
  during the training process.
- **Custom Side Channels** - We enable developers to
  [create custom side channels](Custom-SideChannels.md) to manage data transfer
  between Unity and Python that is unique to their training workflow and/or
  environment.
- **Custom Samplers** - We enable developers to
  [create custom sampling methods](Training-ML-Agents.md#defining-a-new-sampler-type)
  for Environment Parameter Randomization. This enables users to customize this
  training method for their particular environment.

## Summary and Next Steps

To briefly summarize: The ML-Agents Toolkit enables games and simulations built
in Unity to serve as the platform for training intelligent agents. It is
designed to enable a large variety of training modes and scenarios and comes
packed with several features to enable researchers and developers to leverage
(and enhance) machine learning within Unity.

In terms of next steps:

- For a walkthrough of running ML-Agents with a simple scene, check out the
  [Getting Started](Getting-Started.md) guide.
- For a "Hello World" introduction to creating your own Learning Environment,
  check out the
  [Making a New Learning Environment](Learning-Environment-Create-New.md) page.
- For an overview on the more complex example environments that are provided in
  this toolkit, check out the
  [Example Environments](Learning-Environment-Examples.md) page.
- For more information on the various training options available, check out the
  [Training ML-Agents](Training-ML-Agents.md) page.


## Links discovered
- [Unity Engine](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Background-Unity.md)
- [machine learning](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Background-Machine-Learning.md)
- [PyTorch](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Background-PyTorch.md)
- [demo video of ML-Agents in action](https://www.youtube.com/watch?v=fiQsmdwEGT8&feature=youtu.be)
- [Python API](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Python-LLAPI.md)
- [gym](https://github.com/openai/gym)
- [instructions](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Python-Gym-API.md)
- [instructions](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Python-PettingZoo-API.md)
- [Getting Started Guide](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Getting-Started.md)
- [Inference Engine](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Inference-Engine.md)
- [dedicated blog post](https://blogs.unity3d.com/2019/03/01/unity-ml-agents-toolkit-v0-7-a-leap-towards-cross-platform-inference/)
- [gym.](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Python-Gym-API.md)
- [here](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Python-LLAPI.md)
- [Training ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Training-ML-Agents.md)
- [Proximal Policy Optimization (PPO)](https://openai.com/research/openai-baselines-ppo)
- [Soft Actor-Critic (SAC)](https://bair.berkeley.edu/blog/2018/12/14/sac/)
- [here](https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/)
- [Curiosity-driven Exploration by Self-supervised Prediction](https://pathak22.github.io/noreward-rl/)
- [blog post on the Curiosity module](https://blogs.unity3d.com/2018/06/26/solving-sparse-reward-tasks-with-curiosity/)
- [Exploration by Random Network Distillation](https://arxiv.org/abs/1810.12894)
- [video demo](https://youtu.be/kpb8ZkMBFYs)
- [Pyramids environment](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Examples.md#pyramids)
- [_survivor bias_](https://arxiv.org/pdf/1809.02925.pdf)
- [Generative Adversarial Imitation Learning](https://arxiv.org/abs/1606.03476)
- [Designing Agents](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Design-Agents.md#recording-demonstrations)
- [Self-Play](https://openai.com/research/competitive-self-play)
- [Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/1702.08887.pdf)
- [Designing Agents](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Design-Agents.md#defining-teams-for-multi-agent-scenarios)
- [blog post on self-play](https://blogs.unity3d.com/2020/02/28/training-intelligent-adversaries-using-self-play-with-ml-agents/)
- [ELO Rating System](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/ELO-Rating-System.md)
- [PushBlock with Agents Working Together](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/images/cooperative_pushblock.png)
- [this page](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Design-Agents.md#groups-for-cooperative-scenarios)
- [On the Use and Misuse of Absorbing States in Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2111.05992.pdf)
- [Lowe et al.](https://arxiv.org/abs/1706.02275)
- [Foerster et al.](https://arxiv.org/pdf/1705.08926.pdf)
- [Wall Jump](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Examples.md#wall-jump)
- [Wall](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/images/curriculum.png)
- [**Note**: The example provided above is for instructional purposes, and was
based on an early version of the
[Wall Jump example environment](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Examples.md)
- [Training ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Training-ML-Agents.md#curriculum-learning)
- [Domain Randomization](https://arxiv.org/abs/1703.06907)
- [parameter randomization during training](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Training-ML-Agents.md#environment-parameter-randomization)
- [Designing Agents](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Design-Agents.md)
- [here](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Design-Agents.md#multiple-visual-observations)
- [Mnih et al.](https://www.nature.com/articles/nature14236)
- [IMPALA Resnet](https://arxiv.org/abs/1802.01561)
- [Variable Length Observations Illustrated](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/images/variable-length-observation-illustrated.png)
- [here](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Design-Agents.md#variable-length-observations)
- [Inspector](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/images/ml-agents-LSTM.png)
- [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory)
- [creating a Unity executable](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Executable.md)
- [Training ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Training-ML-Agents.md#training-using-concurrent-unity-instances)
- [record statistics](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Design.md#recording-statistics)
- [create custom side channels](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Custom-SideChannels.md)
- [create custom sampling methods](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Training-ML-Agents.md#defining-a-new-sampler-type)
- [Getting Started](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Getting-Started.md)
- [Making a New Learning Environment](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Create-New.md)
- [Example Environments](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Examples.md)

--- docs/Readme.md ---
# Unity ML-Agents Toolkit

[![docs badge](https://img.shields.io/badge/docs-reference-blue.svg)](https://github.com/Unity-Technologies/ml-agents/tree/release_22_docs/docs/)

[![license badge](https://img.shields.io/badge/license-Apache--2.0-green.svg)](../LICENSE.md)

([latest release](https://github.com/Unity-Technologies/ml-agents/releases/tag/latest_release))
([all releases](https://github.com/Unity-Technologies/ml-agents/releases))

**The Unity Machine Learning Agents Toolkit** (ML-Agents) is an open-source
project that enables games and simulations to serve as environments for
training intelligent agents. We provide implementations (based on PyTorch)
of state-of-the-art algorithms to enable game developers and hobbyists to easily
train intelligent agents for 2D, 3D and VR/AR games. Researchers can also use the
provided simple-to-use Python API to train Agents using reinforcement learning,
imitation learning, neuroevolution, or any other methods. These trained agents can be
used for multiple purposes, including controlling NPC behavior (in a variety of
settings such as multi-agent and adversarial), automated testing of game builds
and evaluating different game design decisions pre-release. The ML-Agents
Toolkit is mutually beneficial for both game developers and AI researchers as it
provides a central platform where advances in AI can be evaluated on Unity’s
rich environments and then made accessible to the wider research and game
developer communities.

## Features
- 17+ [example Unity environments](Learning-Environment-Examples.md)
- Support for multiple environment configurations and training scenarios
- Flexible Unity SDK that can be integrated into your game or custom Unity scene
- Support for training single-agent, multi-agent cooperative, and multi-agent
  competitive scenarios via several Deep Reinforcement Learning algorithms (PPO, SAC, MA-POCA, self-play).
- Support for learning from demonstrations through two Imitation Learning algorithms (BC and GAIL).
- Quickly and easily add your own [custom training algorithm](Python-Custom-Trainer-Plugin.md) and/or components.
- Easily definable Curriculum Learning scenarios for complex tasks
- Train robust agents using environment randomization
- Flexible agent control with On Demand Decision Making
- Train using multiple concurrent Unity environment instances
- Utilizes the [Inference Engine](Inference-Engine.md) to
  provide native cross-platform support
- Unity environment [control from Python](Python-LLAPI.md)
- Wrap Unity learning environments as a [gym](Python-Gym-API.md) environment
- Wrap Unity learning environments as a [PettingZoo](Python-PettingZoo-API.md) environment

See our [ML-Agents Overview](ML-Agents-Overview.md) page for detailed
descriptions of all these features. Or go straight to our [web docs](https://unity-technologies.github.io/ml-agents/).
## Releases & Documentation

**Our latest, stable release is `Release 22`. Click
[here](Getting-Started.md)
to get started with the latest release of ML-Agents.**

**You can also check out our new [web docs](https://unity-technologies.github.io/ml-agents/)!**

The table below lists all our releases, including our `main` branch which is
under active development and may be unstable. A few helpful guidelines:

- The [Versioning page](Versioning.md) overviews how we manage our GitHub
  releases and the versioning process for each of the ML-Agents components.
- The [Releases page](https://github.com/Unity-Technologies/ml-agents/releases)
  contains details of the changes between releases.
- The [Migration page](Migrating.md) contains details on how to upgrade
  from earlier releases of the ML-Agents Toolkit.
- The **Documentation** links in the table below include installation and usage
  instructions specific to each release. Remember to always use the
  documentation that corresponds to the release version you're using.
- The `com.unity.ml-agents` package is [verified](https://docs.unity3d.com/2020.1/Documentation/Manual/pack-safe.html)
  for Unity 2020.1 and later. Verified packages releases are numbered 1.0.x.

|        **Version**         | **Release Date** | **Source** | **Documentation** | **Download** | **Python Package** | **Unity Package** |
|:--------------------------:|:------:|:-------------:|:-------:|:------------:|:------------:|:------------:|
| **Release 22** | **October 5, 2024** | **[source](https://github.com/Unity-Technologies/ml-agents/tree/release_22)** | **[docs](https://unity-technologies.github.io/ml-agents/)** | **[download](https://github.com/Unity-Technologies/ml-agents/archive/release_22.zip)** | **[1.1.0](https://pypi.org/project/mlagents/1.1.0/)** | **[3.0.0](https://docs.unity3d.com/Packages/com.unity.ml-agents@3.0/manual/index.html)** |
| **develop (unstable)** | -- | [source](https://github.com/Unity-Technologies/ml-agents/tree/develop) | [docs](https://unity-technologies.github.io/ml-agents/) | [download](https://github.com/Unity-Technologies/ml-agents/archive/develop.zip) | -- | -- |



If you are a researcher interested in a discussion of Unity as an AI platform,
see a pre-print of our
[reference paper on Unity and the ML-Agents Toolkit](https://arxiv.org/abs/1809.02627).

If you use Unity or the ML-Agents Toolkit to conduct research, we ask that you
cite the following paper as a reference:

```
@article{juliani2020,
  title={Unity: A general platform for intelligent agents},
  author={Juliani, Arthur and Berges, Vincent-Pierre and Teng, Ervin and Cohen, Andrew and Harper, Jonathan and Elion, Chris and Goy, Chris and Gao, Yuan and Henry, Hunter and Mattar, Marwan and Lange, Danny},
  journal={arXiv preprint arXiv:1809.02627},
  url={https://arxiv.org/pdf/1809.02627.pdf},
  year={2020}
}
```

Additionally, if you use the MA-POCA trainer in your research, we ask that you
cite the following paper as a reference:

```
@article{cohen2022,
  title={On the Use and Misuse of Absorbing States in Multi-agent Reinforcement Learning},
  author={Cohen, Andrew and Teng, Ervin and Berges, Vincent-Pierre and Dong, Ruo-Ping and Henry, Hunter and Mattar, Marwan and Zook, Alexander and Ganguly, Sujoy},
  journal={RL in Games Workshop AAAI 2022},
  url={http://aaai-rlg.mlanctot.info/papers/AAAI22-RLG_paper_32.pdf},
  year={2022}
}
```



## Additional Resources

We have a Unity Learn course,
[ML-Agents: Hummingbirds](https://learn.unity.com/course/ml-agents-hummingbirds),
that provides a gentle introduction to Unity and the ML-Agents Toolkit.

We've also partnered with
[CodeMonkeyUnity](https://www.youtube.com/c/CodeMonkeyUnity) to create a
[series of tutorial videos](https://www.youtube.com/playlist?list=PLzDRvYVwl53vehwiN_odYJkPBzcqFw110)
on how to implement and use the ML-Agents Toolkit.

We have also published a series of blog posts that are relevant for ML-Agents:

- (July 12, 2021)
  [ML-Agents plays Dodgeball](https://blog.unity.com/technology/ml-agents-plays-dodgeball)
- (May 5, 2021)
  [ML-Agents v2.0 release: Now supports training complex cooperative behaviors](https://blogs.unity3d.com/2021/05/05/ml-agents-v2-0-release-now-supports-training-complex-cooperative-behaviors/)
- (December 28, 2020)
  [Happy holidays from the Unity ML-Agents team!](https://blogs.unity3d.com/2020/12/28/happy-holidays-from-the-unity-ml-agents-team/)
- (November 20, 2020)
  [How Eidos-Montréal created Grid Sensors to improve observations for training agents](https://blogs.unity3d.com/2020/11/20/how-eidos-montreal-created-grid-sensors-to-improve-observations-for-training-agents/)
- (November 11, 2020)
  [2020 AI@Unity interns shoutout](https://blogs.unity3d.com/2020/11/11/2020-aiunity-interns-shoutout/)
- (May 12, 2020)
  [Announcing ML-Agents Unity Package v1.0!](https://blogs.unity3d.com/2020/05/12/announcing-ml-agents-unity-package-v1-0/)
- (February 28, 2020)
  [Training intelligent adversaries using self-play with ML-Agents](https://blogs.unity3d.com/2020/02/28/training-intelligent-adversaries-using-self-play-with-ml-agents/)
- (November 11, 2019)
  [Training your agents 7 times faster with ML-Agents](https://blogs.unity3d.com/2019/11/11/training-your-agents-7-times-faster-with-ml-agents/)
- (October 21, 2019)
  [The AI@Unity interns help shape the world](https://blogs.unity3d.com/2019/10/21/the-aiunity-interns-help-shape-the-world/)
- (April 15, 2019)
  [Unity ML-Agents Toolkit v0.8: Faster training on real games](https://blogs.unity3d.com/2019/04/15/unity-ml-agents-toolkit-v0-8-faster-training-on-real-games/)
- (March 1, 2019)
  [Unity ML-Agents Toolkit v0.7: A leap towards cross-platform inference](https://blogs.unity3d.com/2019/03/01/unity-ml-agents-toolkit-v0-7-a-leap-towards-cross-platform-inference/)
- (December 17, 2018)
  [ML-Agents Toolkit v0.6: Improved usability of Brains and Imitation Learning](https://blogs.unity3d.com/2018/12/17/ml-agents-toolkit-v0-6-improved-usability-of-brains-and-imitation-learning/)
- (October 2, 2018)
  [Puppo, The Corgi: Cuteness Overload with the Unity ML-Agents Toolkit](https://blogs.unity3d.com/2018/10/02/puppo-the-corgi-cuteness-overload-with-the-unity-ml-agents-toolkit/)
- (September 11, 2018)
  [ML-Agents Toolkit v0.5, new resources for AI researchers available now](https://blogs.unity3d.com/2018/09/11/ml-agents-toolkit-v0-5-new-resources-for-ai-researchers-available-now/)
- (June 26, 2018)
  [Solving sparse-reward tasks with Curiosity](https://blogs.unity3d.com/2018/06/26/solving-sparse-reward-tasks-with-curiosity/)
- (June 19, 2018)
  [Unity ML-Agents Toolkit v0.4 and Udacity Deep Reinforcement Learning Nanodegree](https://blogs.unity3d.com/2018/06/19/unity-ml-agents-toolkit-v0-4-and-udacity-deep-reinforcement-learning-nanodegree/)
- (May 24, 2018)
  [Imitation Learning in Unity: The Workflow](https://blogs.unity3d.com/2018/05/24/imitation-learning-in-unity-the-workflow/)
- (March 15, 2018)
  [ML-Agents Toolkit v0.3 Beta released: Imitation Learning, feedback-driven features, and more](https://blogs.unity3d.com/2018/03/15/ml-agents-v0-3-beta-released-imitation-learning-feedback-driven-features-and-more/)
- (December 11, 2017)
  [Using Machine Learning Agents in a real game: a beginner’s guide](https://blogs.unity3d.com/2017/12/11/using-machine-learning-agents-in-a-real-game-a-beginners-guide/)
- (December 8, 2017)
  [Introducing ML-Agents Toolkit v0.2: Curriculum Learning, new environments, and more](https://blogs.unity3d.com/2017/12/08/introducing-ml-agents-v0-2-curriculum-learning-new-environments-and-more/)
- (September 19, 2017)
  [Introducing: Unity Machine Learning Agents Toolkit](https://blogs.unity3d.com/2017/09/19/introducing-unity-machine-learning-agents/)
- Overviewing reinforcement learning concepts
  ([multi-armed bandit](https://blogs.unity3d.com/2017/06/26/unity-ai-themed-blog-entries/)
  and
  [Q-learning](https://blogs.unity3d.com/2017/08/22/unity-ai-reinforcement-learning-with-q-learning/))

### More from Unity

- [Unity Inference Engine](https://unity.com/products/sentis)
- [Introducing Unity Muse and Sentis](https://blog.unity.com/engine-platform/introducing-unity-muse-and-unity-sentis-ai)

## Community and Feedback

The ML-Agents Toolkit is an open-source project and we encourage and welcome
contributions. If you wish to contribute, be sure to review our
[contribution guidelines](CONTRIBUTING.md) and
[code of conduct](CODE_OF_CONDUCT.md).

For problems with the installation and setup of the ML-Agents Toolkit, or
discussions about how to best setup or train your agents, please create a new
thread on the
[Unity ML-Agents forum](https://forum.unity.com/forums/ml-agents.453/) and make
sure to include as much detail as possible. If you run into any other problems
using the ML-Agents Toolkit or have a specific feature request, please
[submit a GitHub issue](https://github.com/Unity-Technologies/ml-agents/issues).

Please tell us which samples you would like to see shipped with the ML-Agents Unity
package by replying to
[this forum thread](https://forum.unity.com/threads/feedback-wanted-shipping-sample-s-with-the-ml-agents-package.1073468/).


Your opinion matters a great deal to us. Only by hearing your thoughts on the
Unity ML-Agents Toolkit can we continue to improve and grow. Please take a few
minutes to
[let us know about it](https://unitysoftware.co1.qualtrics.com/jfe/form/SV_55pQKCZ578t0kbc).

For any other questions or feedback, connect directly with the ML-Agents team at
ml-agents@unity3d.com.

## Privacy

In order to improve the developer experience for Unity ML-Agents Toolkit, we have added in-editor analytics.
Please refer to "Information that is passively collected by Unity" in the
[Unity Privacy Policy](https://unity3d.com/legal/privacy-policy).


## Links discovered
- [![docs badge](https://img.shields.io/badge/docs-reference-blue.svg)
- [![license badge](https://img.shields.io/badge/license-Apache--2.0-green.svg)
- [latest release](https://github.com/Unity-Technologies/ml-agents/releases/tag/latest_release)
- [all releases](https://github.com/Unity-Technologies/ml-agents/releases)
- [example Unity environments](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Examples.md)
- [custom training algorithm](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Python-Custom-Trainer-Plugin.md)
- [Inference Engine](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Inference-Engine.md)
- [control from Python](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Python-LLAPI.md)
- [gym](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Python-Gym-API.md)
- [PettingZoo](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Python-PettingZoo-API.md)
- [ML-Agents Overview](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/ML-Agents-Overview.md)
- [web docs](https://unity-technologies.github.io/ml-agents/)
- [here](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Getting-Started.md)
- [Versioning page](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Versioning.md)
- [Releases page](https://github.com/Unity-Technologies/ml-agents/releases)
- [Migration page](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Migrating.md)
- [verified](https://docs.unity3d.com/2020.1/Documentation/Manual/pack-safe.html)
- [source](https://github.com/Unity-Technologies/ml-agents/tree/release_22)
- [docs](https://unity-technologies.github.io/ml-agents/)
- [download](https://github.com/Unity-Technologies/ml-agents/archive/release_22.zip)
- [1.1.0](https://pypi.org/project/mlagents/1.1.0/)
- [3.0.0](https://docs.unity3d.com/Packages/com.unity.ml-agents@3.0/manual/index.html)
- [source](https://github.com/Unity-Technologies/ml-agents/tree/develop)
- [download](https://github.com/Unity-Technologies/ml-agents/archive/develop.zip)
- [reference paper on Unity and the ML-Agents Toolkit](https://arxiv.org/abs/1809.02627)
- [ML-Agents: Hummingbirds](https://learn.unity.com/course/ml-agents-hummingbirds)
- [CodeMonkeyUnity](https://www.youtube.com/c/CodeMonkeyUnity)
- [series of tutorial videos](https://www.youtube.com/playlist?list=PLzDRvYVwl53vehwiN_odYJkPBzcqFw110)
- [ML-Agents plays Dodgeball](https://blog.unity.com/technology/ml-agents-plays-dodgeball)
- [ML-Agents v2.0 release: Now supports training complex cooperative behaviors](https://blogs.unity3d.com/2021/05/05/ml-agents-v2-0-release-now-supports-training-complex-cooperative-behaviors/)
- [Happy holidays from the Unity ML-Agents team!](https://blogs.unity3d.com/2020/12/28/happy-holidays-from-the-unity-ml-agents-team/)
- [How Eidos-Montréal created Grid Sensors to improve observations for training agents](https://blogs.unity3d.com/2020/11/20/how-eidos-montreal-created-grid-sensors-to-improve-observations-for-training-agents/)
- [2020 AI@Unity interns shoutout](https://blogs.unity3d.com/2020/11/11/2020-aiunity-interns-shoutout/)
- [Announcing ML-Agents Unity Package v1.0!](https://blogs.unity3d.com/2020/05/12/announcing-ml-agents-unity-package-v1-0/)
- [Training intelligent adversaries using self-play with ML-Agents](https://blogs.unity3d.com/2020/02/28/training-intelligent-adversaries-using-self-play-with-ml-agents/)
- [Training your agents 7 times faster with ML-Agents](https://blogs.unity3d.com/2019/11/11/training-your-agents-7-times-faster-with-ml-agents/)
- [The AI@Unity interns help shape the world](https://blogs.unity3d.com/2019/10/21/the-aiunity-interns-help-shape-the-world/)
- [Unity ML-Agents Toolkit v0.8: Faster training on real games](https://blogs.unity3d.com/2019/04/15/unity-ml-agents-toolkit-v0-8-faster-training-on-real-games/)
- [Unity ML-Agents Toolkit v0.7: A leap towards cross-platform inference](https://blogs.unity3d.com/2019/03/01/unity-ml-agents-toolkit-v0-7-a-leap-towards-cross-platform-inference/)
- [ML-Agents Toolkit v0.6: Improved usability of Brains and Imitation Learning](https://blogs.unity3d.com/2018/12/17/ml-agents-toolkit-v0-6-improved-usability-of-brains-and-imitation-learning/)
- [Puppo, The Corgi: Cuteness Overload with the Unity ML-Agents Toolkit](https://blogs.unity3d.com/2018/10/02/puppo-the-corgi-cuteness-overload-with-the-unity-ml-agents-toolkit/)
- [ML-Agents Toolkit v0.5, new resources for AI researchers available now](https://blogs.unity3d.com/2018/09/11/ml-agents-toolkit-v0-5-new-resources-for-ai-researchers-available-now/)
- [Solving sparse-reward tasks with Curiosity](https://blogs.unity3d.com/2018/06/26/solving-sparse-reward-tasks-with-curiosity/)
- [Unity ML-Agents Toolkit v0.4 and Udacity Deep Reinforcement Learning Nanodegree](https://blogs.unity3d.com/2018/06/19/unity-ml-agents-toolkit-v0-4-and-udacity-deep-reinforcement-learning-nanodegree/)
- [Imitation Learning in Unity: The Workflow](https://blogs.unity3d.com/2018/05/24/imitation-learning-in-unity-the-workflow/)
- [ML-Agents Toolkit v0.3 Beta released: Imitation Learning, feedback-driven features, and more](https://blogs.unity3d.com/2018/03/15/ml-agents-v0-3-beta-released-imitation-learning-feedback-driven-features-and-more/)
- [Using Machine Learning Agents in a real game: a beginner’s guide](https://blogs.unity3d.com/2017/12/11/using-machine-learning-agents-in-a-real-game-a-beginners-guide/)
- [Introducing ML-Agents Toolkit v0.2: Curriculum Learning, new environments, and more](https://blogs.unity3d.com/2017/12/08/introducing-ml-agents-v0-2-curriculum-learning-new-environments-and-more/)
- [Introducing: Unity Machine Learning Agents Toolkit](https://blogs.unity3d.com/2017/09/19/introducing-unity-machine-learning-agents/)
- [multi-armed bandit](https://blogs.unity3d.com/2017/06/26/unity-ai-themed-blog-entries/)
- [Q-learning](https://blogs.unity3d.com/2017/08/22/unity-ai-reinforcement-learning-with-q-learning/)
- [Unity Inference Engine](https://unity.com/products/sentis)
- [Introducing Unity Muse and Sentis](https://blog.unity.com/engine-platform/introducing-unity-muse-and-unity-sentis-ai)
- [contribution guidelines](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/CONTRIBUTING.md)
- [code of conduct](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/CODE_OF_CONDUCT.md)
- [Unity ML-Agents forum](https://forum.unity.com/forums/ml-agents.453/)
- [submit a GitHub issue](https://github.com/Unity-Technologies/ml-agents/issues)
- [this forum thread](https://forum.unity.com/threads/feedback-wanted-shipping-sample-s-with-the-ml-agents-package.1073468/)
- [let us know about it](https://unitysoftware.co1.qualtrics.com/jfe/form/SV_55pQKCZ578t0kbc)
- [Unity Privacy Policy](https://unity3d.com/legal/privacy-policy)

--- com.unity.ml-agents/Documentation~/Installation.md ---
# Install the ML-Agents Toolkit
Set up your system to use the ML-Agents Toolkit to train and run machine-learning agents in Unity projects.

This process includes installing Unity, configuring Python, and installing the ML-Agents packages. Follow the steps in order to ensure compatibility between Unity and the ML-Agents components.



##  Install Unity

Install Unity 6000.0 or later to use the ML-Agents Toolkit.

To install Unity, follow these steps:

1. [Download Unity](https://unity3d.com/get-unity/download).
2. Use **Unity Hub** to manage installations and versions.
   Unity Hub makes it easier to manage multiple Unity versions and associated projects.
3. Verify that the Unity Editor version is 6000.0 or later.

## Install Python 3.10.12 using Conda

Use Conda or Mamba to install and manage your Python environment. This ensures that ML-Agents dependencies are isolated and version-controlled.

To install Python, follow these steps:

1. Install [Conda](https://docs.conda.io/en/latest/) or [Mamba](https://github.com/mamba-org/mamba).
2. Open a terminal and create a new Conda environment with Python 3.10.12:

   ```shell
   conda create -n mlagents python=3.10.12 && conda activate mlagents

3. On **Windows**, install PyTorch separately to ensure CUDA support:

```shell
pip3 install torch~=2.2.1 --index-url https://download.pytorch.org/whl/cu121
```
If prompted, install Microsoft Visual C++ Redistributable. For more installation options and versions, refer to the [PyTorch installation guide](https://pytorch.org/get-started/locally/).


## Install ML-Agents
You can install ML-Agents in two ways:

* [Package installation](#install-ml-agents-package-installation): Recommended for most users who want to use ML-Agents without modifying the source code or using the example environments.
* [Advanced installation](#install-ml-agents-advanced-installation): For contributors, developers extending ML-Agents, or users who want access to the example environments.

### Install ML-Agents (Package installation)

Use this method if you don’t plan to modify the toolkit or need the example environments.

#### Install the ML-Agents Unity package

To install the package, follow these steps:

1. In Unity, open **Window** > **Package Manager**.
2. Select **+** > **Add package by name**.
3. Enter `com.unity.ml-agents`.
4. Enable **Preview Packages** under the **Advanced** drop-down list if the package doesn’t appear.

If the package isn’t listed, follow the [Advanced Installation](#install-ml-agents-advanced-installation) method instead.



#### Install the ML-Agents Python package

Install the ML-Agents Python package to enable communication between Unity and your machine learning training environment.

Using a Python virtual environment helps isolate project dependencies and prevent version conflicts across your system. Virtual environments are supported on macOS, Windows, and Linux. For more information, refer to [Using Virtual Environments](Using-Virtual-Environment.md).

1. Before installing ML-Agents, activate the Conda environment you created.


2. Install the ML-Agents Python package from the Python Package Index (PyPI):

```shell
python -m pip install mlagents==1.1.0
```
Make sure to install a Python package version that matches your Unity ML-Agents package version. For information on compatible versions, refer to the [ML-Agents release history](https://github.com/Unity-Technologies/ml-agents/releases).

3. If you encounter an error while building the `grpcio` wheel, install it separately before reinstalling `mlagents`:

```shell
conda install "grpcio=1.48.2" -c conda-forge
```
This step resolves dependency conflicts that can occur with older versions of `grpcio`.

4. When the installation completes successfully, all the required Python dependencies listed in the [setup.py file](https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/ml-agents/setup.py), including [PyTorch](Background-PyTorch.md) are automatically configured.


### Install ML-Agents (Advanced Installation)

Use the advanced installation method if you plan to modify or extend the ML-Agents Toolkit, or if you want to download and use the example environments included in the repository.

#### Clone the ML-Agents repository

Clone the ML-Agents repository to access the source code, sample environments, and development branches.

To clone the latest stable release, run:

```sh
git clone --branch release_23 https://github.com/Unity-Technologies/ml-agents.git
```

The `--branch release_23` flag checks out the latest stable release.
If you omit this option, the `develop` branch is cloned instead, which may contain experimental or unstable changes.
If the release branch does not work as expected, switch to the develop branch. It may include fixes for dependency or compatibility issues.
To clone the bleeding-edge development version (optional), run:
```sh
git clone https://github.com/Unity-Technologies/ml-agents.git
```
If you plan to contribute your changes, clone the develop branch (omit the `--branch` flag) and refer to the [Contribution Guidelines](CONTRIBUTING.md) for details.


#### Add the ML-Agents Unity package

After cloning the repository, add the `com.unity.ml-agents` Unity package to your project.

To add the local package, follow these steps:

1. In the Unity Editor, go to **Window** > **Package Manager**.
2. In the **Package Manager** window, select **+**.
3. Select **Add package from disk**.
4. Navigate to the cloned repository and open the `com.unity.ml-agents` folder.
5. Select the `package.json` file.

Unity adds the ML-Agents package to your project.

If you plan to use the example environments provided in the repository, open the **Project** folder in Unity to explore and experiment with them.


<p align="center"> <img src="images/unity_package_manager_window.png" alt="Unity Package Manager Window" height="150" border="10" /> <img src="images/unity_package_json.png" alt="package.json" height="150" border="10" /> </p>


#### Install the ML-Agents Python package

Install the Python packages from the cloned repository to enable training and environment communication.

1. From the root of the cloned repository, activate your virtual environment and run:

```sh
cd /path/to/ml-agents
python -m pip install ./ml-agents-envs
python -m pip install ./ml-agents
```

This installs the ML-Agents packages directly from the cloned source, _not_ from PyPi.

2. To confirm a successful installation, run:
`mlagents-learn --help`

If the command lists available parameters, your setup is complete.

3. If you plan to modify the ML-Agents source code or contribute changes, install the packages in editable mode.
Editable installs let you make live changes to the Python files and test them immediately.

From the repository’s root directory, run:

```sh
pip3 install torch -f https://download.pytorch.org/whl/torch_stable.html
pip3 install -e ./ml-agents-envs
pip3 install -e ./ml-agents
```

Note:

Install the packages in this order. The `mlagents` package depends on `mlagents_envs`.
Installing them in the other order will download `mlagents_envs` from PyPi, which can cause version mismatches.


## Links discovered
- [Download Unity](https://unity3d.com/get-unity/download)
- [Conda](https://docs.conda.io/en/latest/)
- [Mamba](https://github.com/mamba-org/mamba)
- [PyTorch installation guide](https://pytorch.org/get-started/locally/)
- [Using Virtual Environments](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Using-Virtual-Environment.md)
- [ML-Agents release history](https://github.com/Unity-Technologies/ml-agents/releases)
- [setup.py file](https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/ml-agents/setup.py)
- [PyTorch](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Background-PyTorch.md)
- [Contribution Guidelines](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/CONTRIBUTING.md)

--- com.unity.ml-agents/Documentation~/ML-Agents-Overview.md ---
# ML-Agents Theory

Depending on your background (i.e. researcher, game developer, hobbyist), you may have very different questions on your mind at the moment. To make your transition to the ML-Agents Toolkit easier, we provide several background pages that include overviews and helpful resources on the [Unity Engine](Background-Unity.md), [machine learning](Background-Machine-Learning.md) and [PyTorch](Background-PyTorch.md). We **strongly** recommend browsing the relevant background pages if you're not familiar with a Unity scene, basic machine learning concepts or have not previously heard of PyTorch.

The remainder of this page contains a deep dive into ML-Agents, its key components, different training modes and scenarios. By the end of it, you should have a good sense of _what_ the ML-Agents Toolkit allows you to do. The subsequent documentation pages provide examples of _how_ to use ML-Agents. To get started, watch this [demo video of ML-Agents in action](https://www.youtube.com/watch?v=fiQsmdwEGT8&feature=youtu.be).

## Running Example: Training NPC Behaviors

To help explain the material and terminology in this page, we'll use a hypothetical, running example throughout. We will explore the problem of training the behavior of a non-playable character (NPC) in a game. (An NPC is a game character that is never controlled by a human player and its behavior is pre-defined by the game developer.) More specifically, let's assume we're building a multi-player, war-themed game in which players control the soldiers. In this game, we have a single NPC who serves as a medic, finding and reviving wounded players. Lastly, let us assume that there are two teams, each with five players and one NPC medic.

The behavior of a medic is quite complex. It first needs to avoid getting injured, which requires detecting when it is in danger and moving to a safe location. Second, it needs to be aware of which of its team members are injured and require assistance. In the case of multiple injuries, it needs to assess the degree of injury and decide who to help first. Lastly, a good medic will always place itself in a position where it can quickly help its team members. Factoring in all of these traits means that at every instance, the medic needs to measure several attributes of the environment (e.g. position of team members, position of enemies, which of its team members are injured and to what degree) and then decide on an action (e.g. hide from enemy fire, move to help one of its members). Given the large number of settings of the environment and the large number of actions that the medic can take, defining and implementing such complex behaviors by hand is challenging and prone to errors.

With ML-Agents, it is possible to _train_ the behaviors of such NPCs (called **Agents**) using a variety of methods. The basic idea is quite simple. We need to define three entities at every moment of the game (called **environment**):

- **Observations** - what the medic perceives about the environment. Observations can be numeric and/or visual. Numeric observations measure attributes of the environment from the point of view of the agent. For our medic this would be attributes of the battlefield that are visible to it. For most interesting environments, an agent will require several continuous numeric observations. Visual observations, on the other hand, are images generated from the cameras attached to the agent and represent what the agent is seeing at that point in time. It is common to confuse an agent's observation with the environment (or game) **state**. The environment state represents information about the entire scene containing all the game characters. The agents observation, however, only contains information that the agent is aware of and is typically a subset of the environment state. For example, the medic observation cannot include information about an enemy in hiding that the medic is unaware of.
- **Actions** - what actions the medic can take. Similar to observations, actions can either be continuous or discrete depending on the complexity of the environment and agent. In the case of the medic, if the environment is a simple grid world where only their location matters, then a discrete action taking on one of four values (north, south, east, west) suffices. However, if the environment is more complex and the medic can move freely then using two continuous actions (one for direction and another for speed) is more appropriate.
- **Reward signals** - a scalar value indicating how well the medic is doing. Note that the reward signal need not be provided at every moment, but only when the medic performs an action that is good or bad. For example, it can receive a large negative reward if it dies, a modest positive reward whenever it revives a wounded team member, and a modest negative reward when a wounded team member dies due to lack of assistance. Note that the reward signal is how the objectives of the task are communicated to the agent, so they need to be set up in a manner where maximizing reward generates the desired optimal behavior.

After defining these three entities (the building blocks of a **reinforcement learning task**), we can now _train_ the medic's behavior. This is achieved by simulating the environment for many trials where the medic, over time, learns what is the optimal action to take for every observation it measures by maximizing its future reward. The key is that by learning the actions that maximize its reward, the medic is learning the behaviors that make it a good medic (i.e. one who saves the most number of lives). In **reinforcement learning** terminology, the behavior that is learned is called a **policy**, which is essentially a (optimal) mapping from observations to actions. Note that the process of learning a policy through running simulations is called the **training phase**, while playing the game with an NPC that is using its learned policy is called the **inference phase**.

The ML-Agents Toolkit provides all the necessary tools for using Unity as the simulation engine for learning the policies of different objects in a Unity environment. In the next few sections, we discuss how the ML-Agents Toolkit achieves this and what features it provides.

## Key Components

The ML-Agents Toolkit contains five high-level components:

- **Learning Environment** - which contains the Unity scene and all the game characters. The Unity scene provides the environment in which agents observe, act, and learn. How you set up the Unity scene to serve as a learning environment really depends on your goal. You may be trying to solve a specific reinforcement learning problem of limited scope, in which case you can use the same scene for both training and for testing trained agents. Or, you may be training agents to operate in a complex game or simulation. In this case, it might be more efficient and practical to create a purpose-built training scene. The ML-Agents Toolkit includes an ML-Agents Unity SDK (`com.unity.ml-agents` package) that enables you to transform any Unity scene into a learning environment by defining the agents and their behaviors.
- **Python Low-Level API** - which contains a low-level Python interface for interacting and manipulating a learning environment. Note that, unlike the Learning Environment, the Python API is not part of Unity, but lives outside and communicates with Unity through the Communicator. This API is contained in a dedicated `mlagents_envs` Python package and is used by the Python training process to communicate with and control the Academy during training. However, it can be used for other purposes as well. For example, you could use the API to use Unity as the simulation engine for your own machine learning algorithms. See [Python API](Python-LLAPI.md) for more information.
- **External Communicator** - which connects the Learning Environment with the Python Low-Level API. It lives within the Learning Environment.
- **Python Trainers** which contains all the machine learning algorithms that enable training agents. The algorithms are implemented in Python and are part of their own `mlagents` Python package. The package exposes a single command-line utility `mlagents-learn` that supports all the training methods and options outlined in this document. The Python Trainers interface solely with the Python Low-Level API.
- **Gym Wrapper** (not pictured). A common way in which machine learning researchers interact with simulation environments is via a wrapper provided by OpenAI called [gym](https://github.com/openai/gym). We provide a gym wrapper in the `ml-agents-envs` package and [instructions](Python-Gym-API.md) for using it with existing machine learning algorithms which utilize gym.
- **PettingZoo Wrapper** (not pictured) PettingZoo is python API for interacting with multi-agent simulation environments that provides a gym-like interface. We provide a PettingZoo wrapper for Unity ML-Agents environments in the `ml-agents-envs` package and [instructions](Python-PettingZoo-API.md) for using it with machine learning algorithms.

<p align="center"> <img src="images/learning_environment_basic.png" alt="Simplified ML-Agents Scene Block Diagram" width="600" border="10" /> </p>

_Simplified block diagram of ML-Agents._

The Learning Environment contains two Unity Components that help organize the Unity scene:

- **Agents** - which is attached to a Unity GameObject (any character within a scene) and handles generating its observations, performing the actions it receives and assigning a reward (positive / negative) when appropriate. Each Agent is linked to a Behavior.
- **Behavior** - defines specific attributes of the agent such as the number of actions that agent can take. Each Behavior is uniquely identified by a `Behavior Name` field. A Behavior can be thought as a function that receives observations and rewards from the Agent and returns actions. A Behavior can be of one of three types: Learning, Heuristic or Inference. A Learning Behavior is one that is not, yet, defined but about to be trained. A Heuristic Behavior is one that is defined by a hard-coded set of rules implemented in code. An Inference Behavior is one that includes a trained Neural Network file. In essence, after a Learning Behavior is trained, it becomes an Inference Behavior.

Every Learning Environment will always have one Agent for every character in the scene. While each Agent must be linked to a Behavior, it is possible for Agents that have similar observations and actions to have the same Behavior. In our sample game, we have two teams each with their own medic. Thus we will have two Agents in our Learning Environment, one for each medic, but both of these medics can have the same Behavior. This does not mean that at each instance they will have identical observation and action _values_.

<p align="center"> <img src="images/learning_environment_example.png" alt="Example ML-Agents Scene Block Diagram" width="700" border="10" /> </p>

_Example block diagram of ML-Agents Toolkit for our sample game._

Note that in a single environment, there can be multiple Agents and multiple Behaviors at the same time. For example, if we expanded our game to include tank driver NPCs, then the Agent attached to those characters cannot share its Behavior with the Agent linked to the medics (medics and drivers have different actions). The Learning Environment through the Academy (not represented in the diagram) ensures that all the Agents are in sync in addition to controlling environment-wide settings.

Lastly, it is possible to exchange data between Unity and Python outside of the machine learning loop through _Side Channels_. One example of using _Side Channels_ is to exchange data with Python about _Environment Parameters_. The following diagram illustrates the above.

<p align="center"> <img src="images/learning_environment_full.png" alt="More Complete Example ML-Agents Scene Block Diagram" border="10" /> </p>

## Training Modes

Given the flexibility of ML-Agents, there are a few ways in which training and inference can proceed.

### Built-in Training and Inference

As mentioned previously, the ML-Agents Toolkit ships with several implementations of state-of-the-art algorithms for training intelligent agents. More specifically, during training, all the medics in the scene send their observations to the Python API through the External Communicator. The Python API processes these observations and sends back actions for each medic to take. During training these actions are mostly exploratory to help the Python API learn the best policy for each medic. Once training concludes, the learned policy for each medic can be exported as a model file. Then during the inference phase, the medics still continue to generate their observations, but instead of being sent to the Python API, they will be fed into their (internal, embedded) model to generate the _optimal_ action for each medic to take at every point in time.

The [Running an Example](Sample.md) guide covers this training mode with the **3D Balance Ball** sample environment.

#### Cross-Platform Inference

It is important to note that the ML-Agents Toolkit leverages the [Inference Engine](Inference-Engine.md) to run the models within a Unity scene such that an agent can take the _optimal_ action at each step. Given that Inference Engine supports all Unity runtime platforms, this means that any model you train with the ML-Agents Toolkit can be embedded into your Unity application that runs on any platform.

### Custom Training and Inference

In the previous mode, the Agents were used for training to generate a PyTorch model that the Agents can later use. However, any user of the ML-Agents Toolkit can leverage their own algorithms for training. In this case, the behaviors of all the Agents in the scene will be controlled within Python. You can even turn your environment into a [gym.](Python-Gym-API.md)

We do not currently have a tutorial highlighting this mode, but you can learn more about the Python API [here](Python-LLAPI.md).

## Flexible Training Scenarios

While the discussion so-far has mostly focused on training a single agent, with ML-Agents, several training scenarios are possible. We are excited to see what kinds of novel and fun environments the community creates. For those new to training intelligent agents, below are a few examples that can serve as inspiration:

- Single-Agent. A single agent, with its own reward signal. The traditional way of training an agent. An example is any single-player game, such as Chicken.
- Simultaneous Single-Agent. Multiple independent agents with independent reward signals with same `Behavior Parameters`. A parallelized version of the traditional training scenario, which can speed-up and stabilize the training process. Helpful when you have multiple versions of the same character in an environment who should learn similar behaviors. An example might be training a dozen robot-arms to each open a door simultaneously.
- Adversarial Self-Play. Two interacting agents with inverse reward signals. In two-player games, adversarial self-play can allow an agent to become increasingly more skilled, while always having the perfectly matched opponent: itself. This was the strategy employed when training AlphaGo, and more recently used by OpenAI to train a human-beating 1-vs-1 Dota 2 agent.
- Cooperative Multi-Agent. Multiple interacting agents with a shared reward signal with same or different `Behavior Parameters`. In this scenario, all agents must work together to accomplish a task that cannot be done alone. Examples include environments where each agent only has access to partial information, which needs to be shared in order to accomplish the task or collaboratively solve a puzzle.
- Competitive Multi-Agent. Multiple interacting agents with inverse reward signals with same or different `Behavior Parameters`. In this scenario, agents must compete with one another to either win a competition, or obtain some limited set of resources. All team sports fall into this scenario.
- Ecosystem. Multiple interacting agents with independent reward signals with same or different `Behavior Parameters`. This scenario can be thought of as creating a small world in which animals with different goals all interact, such as a savanna in which there might be zebras, elephants and giraffes, or an autonomous driving simulation within an urban environment.

## Training Methods: Environment-agnostic

The remaining sections overview the various state-of-the-art machine learning algorithms that are part of the ML-Agents Toolkit. If you aren't studying machine and reinforcement learning as a subject and just want to train agents to accomplish tasks, you can treat these algorithms as _black boxes_. There are a few training-related parameters to adjust inside Unity as well as on the Python training side, but you do not need in-depth knowledge of the algorithms themselves to successfully create and train agents. Step-by-step procedures for running the training process are provided in the [Training ML-Agents](Training-ML-Agents.md) page.

This section specifically focuses on the training methods that are available regardless of the specifics of your learning environment.

#### A Quick Note on Reward Signals

In this section we introduce the concepts of _intrinsic_ and _extrinsic_ rewards, which helps explain some of the training methods.

In reinforcement learning, the end goal for the Agent is to discover a behavior (a Policy) that maximizes a reward. You will need to provide the agent one or more reward signals to use during training. Typically, a reward is defined by your environment, and corresponds to reaching some goal. These are what we refer to as _extrinsic_ rewards, as they are defined external of the learning algorithm.

Rewards, however, can be defined outside of the environment as well, to encourage the agent to behave in certain ways, or to aid the learning of the true extrinsic reward. We refer to these rewards as _intrinsic_ reward signals. The total reward that the agent will learn to maximize can be a mix of extrinsic and intrinsic reward signals.

The ML-Agents Toolkit allows reward signals to be defined in a modular way, and we provide four reward signals that can the mixed and matched to help shape your agent's behavior:

- `extrinsic`: represents the rewards defined in your environment, and is enabled by default
- `gail`: represents an intrinsic reward signal that is defined by GAIL (see below)
- `curiosity`: represents an intrinsic reward signal that encourages exploration in sparse-reward environments that is defined by the Curiosity module (see below).
- `rnd`: represents an intrinsic reward signal that encourages exploration in sparse-reward environments that is defined by the Curiosity module (see below).

### Deep Reinforcement Learning

ML-Agents provide an implementation of two reinforcement learning algorithms:

- [Proximal Policy Optimization (PPO)](https://openai.com/research/openai-baselines-ppo)
- [Soft Actor-Critic (SAC)](https://bair.berkeley.edu/blog/2018/12/14/sac/)

The default algorithm is PPO. This is a method that has been shown to be more general purpose and stable than many other RL algorithms.

In contrast with PPO, SAC is _off-policy_, which means it can learn from experiences collected at any time during the past. As experiences are collected, they are placed in an experience replay buffer and randomly drawn during training. This makes SAC significantly more sample-efficient, often requiring
5-10 times less samples to learn the same task as PPO. However, SAC tends to
require more model updates. SAC is a good choice for heavier or slower environments (about 0.1 seconds per step or more). SAC is also a "maximum entropy" algorithm, and enables exploration in an intrinsic way. Read more about maximum entropy RL [here](https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/).

#### Curiosity for Sparse-reward Environments

In environments where the agent receives rare or infrequent rewards (i.e. sparse-reward), an agent may never receive a reward signal on which to bootstrap its training process. This is a scenario where the use of an intrinsic reward signals can be valuable. Curiosity is one such signal which can help the agent explore when extrinsic rewards are sparse.

The `curiosity` Reward Signal enables the Intrinsic Curiosity Module. This is an implementation of the approach described in [Curiosity-driven Exploration by Self-supervised Prediction](https://pathak22.github.io/noreward-rl/) by Pathak, et al. It trains two networks:

- an inverse model, which takes the current and next observation of the agent, encodes them, and uses the encoding to predict the action that was taken between the observations
- a forward model, which takes the encoded current observation and action, and predicts the next encoded observation.

The loss of the forward model (the difference between the predicted and actual encoded observations) is used as the intrinsic reward, so the more surprised the model is, the larger the reward will be.

For more information, see our dedicated [blog post on the Curiosity module](https://blogs.unity3d.com/2018/06/26/solving-sparse-reward-tasks-with-curiosity/).

#### RND for Sparse-reward Environments

Similarly to Curiosity, Random Network Distillation (RND) is useful in sparse or rare reward environments as it helps the Agent explore. The RND Module is implemented following the paper [Exploration by Random Network Distillation](https://arxiv.org/abs/1810.12894). RND uses two networks:

 - The first is a network with fixed random weights that takes observations as inputs and generates an encoding
 - The second is a network with similar architecture that is trained to predict the outputs of the first network and uses the observations the Agent collects as training data.

The loss (the squared difference between the predicted and actual encoded observations) of the trained model is used as intrinsic reward. The more an Agent visits a state, the more accurate the predictions and the lower the rewards which encourages the Agent to explore new states with higher prediction errors.

### Imitation Learning

It is often more intuitive to simply demonstrate the behavior we want an agent to perform, rather than attempting to have it learn via trial-and-error methods. For example, instead of indirectly training a medic with the help of a reward function, we can give the medic real world examples of observations from the game and actions from a game controller to guide the medic's behavior. Imitation Learning uses pairs of observations and actions from a demonstration to learn a policy. See this [video demo](https://youtu.be/kpb8ZkMBFYs) of imitation learning .

Imitation learning can either be used alone or in conjunction with reinforcement learning. If used alone it can provide a mechanism for learning a specific type of behavior (i.e. a specific style of solving the task). If used in conjunction with reinforcement learning it can dramatically reduce the time the agent takes to solve the environment. This can be especially pronounced in sparse-reward environments. For instance, on the [Pyramids environment](Learning-Environment-Examples.md#pyramids), using 6 episodes of demonstrations can reduce training steps by more than 4 times. See Behavioral Cloning + GAIL + Curiosity + RL below.

<p align="center"> <img src="images/mlagents-ImitationAndRL.png" alt="Using Demonstrations with Reinforcement Learning" width="700" border="0" /> </p>

The ML-Agents Toolkit provides a way to learn directly from demonstrations, as well as use them to help speed up reward-based training (RL). We include two algorithms called Behavioral Cloning (BC) and Generative Adversarial Imitation Learning (GAIL). In most scenarios, you can combine these two features:

- If you want to help your agents learn (especially with environments that have sparse rewards) using pre-recorded demonstrations, you can generally enable both GAIL and Behavioral Cloning at low strengths in addition to having an extrinsic reward. An example of this is provided for the PushBlock example environment in `config/imitation/PushBlock.yaml`.
- If you want to train purely from demonstrations with GAIL and BC _without_ an extrinsic reward signal, please see the CrawlerStatic example environment under in `config/imitation/CrawlerStatic.yaml`.

***Note:*** GAIL introduces a [_survivor bias_](https://arxiv.org/pdf/1809.02925.pdf) to the learning process. That is, by giving positive rewards based on similarity to the expert, the agent is incentivized to remain alive for as long as possible. This can directly conflict with goal-oriented tasks like our PushBlock or Pyramids example environments where an agent must reach a goal state thus ending the episode as quickly as possible. In these cases, we strongly recommend that you use a low strength GAIL reward signal and a sparse extrinsic signal when the agent achieves the task. This way, the GAIL reward signal will guide the agent until it discovers the extrinsic signal and will not overpower it. If the agent appears to be ignoring the extrinsic reward signal, you should reduce the strength of GAIL.

#### GAIL (Generative Adversarial Imitation Learning)

GAIL, or [Generative Adversarial Imitation Learning](https://arxiv.org/abs/1606.03476), uses an adversarial approach to reward your Agent for behaving similar to a set of demonstrations. GAIL can be used with or without environment rewards, and works well when there are a limited number of demonstrations. In this framework, a second neural network, the discriminator, is taught to distinguish whether an observation/action is from a demonstration or produced by the agent. This discriminator can then examine a new observation/action and provide it a reward based on how close it believes this new observation/action is to the provided demonstrations.

At each training step, the agent tries to learn how to maximize this reward. Then, the discriminator is trained to better distinguish between demonstrations and agent state/actions. In this way, while the agent gets better and better at mimicking the demonstrations, the discriminator keeps getting stricter and stricter and the agent must try harder to "fool" it.

This approach learns a _policy_ that produces states and actions similar to the demonstrations, requiring fewer demonstrations than direct cloning of the actions. In addition to learning purely from demonstrations, the GAIL reward signal can be mixed with an extrinsic reward signal to guide the learning process.

#### Behavioral Cloning (BC)

BC trains the Agent's policy to exactly mimic the actions shown in a set of demonstrations. The BC feature can be enabled on the PPO or SAC trainers. As BC cannot generalize past the examples shown in the demonstrations, BC tends to work best when there exists demonstrations for nearly all of the states that the agent can experience, or in conjunction with GAIL and/or an extrinsic reward.

#### Recording Demonstrations

Demonstrations of agent behavior can be recorded from the Unity Editor or build, and saved as assets. These demonstrations contain information on the observations, actions, and rewards for a given agent during the recording session. They can be managed in the Editor, as well as used for training with BC and GAIL. See the [Designing Agents](Learning-Environment-Design-Agents.md#recording-demonstrations) page for more information on how to record demonstrations for your agent.

### Summary

To summarize, we provide 3 training methods: BC, GAIL and RL (PPO or SAC) that can be used independently or together:

- BC can be used on its own or as a pre-training step before GAIL and/or RL
- GAIL can be used with or without extrinsic rewards
- RL can be used on its own (either PPO or SAC) or in conjunction with BC and/or GAIL.

Leveraging either BC or GAIL requires recording demonstrations to be provided as input to the training algorithms.

## Training Methods: Environment-specific

In addition to the three environment-agnostic training methods introduced in the previous section, the ML-Agents Toolkit provides additional methods that can aid in training behaviors for specific types of environments.

### Training in Competitive Multi-Agent Environments with Self-Play

ML-Agents provides the functionality to train both symmetric and asymmetric adversarial games with [Self-Play](https://openai.com/research/competitive-self-play). A symmetric game is one in which opposing agents are equal in form, function and objective. Examples of symmetric games are our Tennis and Soccer example environments. In reinforcement learning, this means both agents have the same observation and actions and learn from the same reward function and so _they can share the same policy_. In asymmetric games, this is not the case. An example of an asymmetric games are Hide and Seek. Agents in these types of games do not always have the same observation or actions and so sharing policy networks is not necessarily ideal.

With self-play, an agent learns in adversarial games by competing against fixed, past versions of its opponent (which could be itself as in symmetric games) to provide a more stable, stationary learning environment. This is compared to competing against the current, best opponent in every episode, which is constantly changing (because it's learning).

Self-play can be used with our implementations of both Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC). However, from the perspective of an individual agent, these scenarios appear to have non-stationary dynamics because the opponent is often changing. This can cause significant issues in the experience replay mechanism used by SAC. Thus, we recommend that users use PPO. For further reading on this issue in particular, see the paper [Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/1702.08887.pdf).

See our [Designing Agents](Learning-Environment-Design-Agents.md#defining-multi-agent-scenarios) page for more information on setting up teams in your Unity scene. Also, read our [blog post on self-play](https://blogs.unity3d.com/2020/02/28/training-intelligent-adversaries-using-self-play-with-ml-agents/) for additional information. Additionally, check [ELO Rating System](ELO-Rating-System.md) the method we use to calculate the relative skill level between two players.

### Training In Cooperative Multi-Agent Environments with MA-POCA

![PushBlock with Agents Working Together](images/cooperative_pushblock.png)

ML-Agents provides the functionality for training cooperative behaviors - i.e., groups of agents working towards a common goal, where the success of the individual is linked to the success of the whole group. In such a scenario, agents typically receive rewards as a group. For instance, if a team of agents wins a game against an opposing team, everyone is rewarded - even agents who did not directly contribute to the win. This makes learning what to do as an individual difficult - you may get a win for doing nothing, and a loss for doing your best.

In ML-Agents, we provide MA-POCA (MultiAgent POsthumous Credit Assignment), which is a novel multi-agent trainer that trains a _centralized critic_, a neural network that acts as a "coach" for a whole group of agents. You can then give rewards to the team as a whole, and the agents will learn how best to contribute to achieving that reward. Agents can _also_ be given rewards individually, and the team will work together to help the individual achieve those goals. During an episode, agents can be added or removed from the group, such as when agents spawn or die in a game. If agents are removed mid-episode (e.g., if teammates die or are removed from the game), they will still learn whether their actions contributed to the team winning later, enabling agents to take group-beneficial actions even if they result in the individual being removed from the game (i.e., self-sacrifice). MA-POCA can also be combined with self-play to train teams of agents to play against each other.

To learn more about enabling cooperative behaviors for agents in an ML-Agents environment, check out [this page](Learning-Environment-Design-Agents.md#groups-for-cooperative-scenarios).

To learn more about MA-POCA, please see our paper [On the Use and Misuse of Absorbing States in Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2111.05992.pdf). For further reading, MA-POCA builds on previous work in multi-agent cooperative learning ([Lowe et al.](https://arxiv.org/abs/1706.02275), [Foerster et al.](https://arxiv.org/pdf/1705.08926.pdf), among others) to enable the above use-cases.

### Solving Complex Tasks using Curriculum Learning

Curriculum learning is a way of training a machine learning model where more difficult aspects of a problem are gradually introduced in such a way that the model is always optimally challenged. This idea has been around for a long time, and it is how we humans typically learn. If you imagine any childhood primary school education, there is an ordering of classes and topics. Arithmetic is taught before algebra, for example. Likewise, algebra is taught before calculus. The skills and knowledge learned in the earlier subjects provide a scaffolding for later lessons. The same principle can be applied to machine learning, where training on easier tasks can provide a scaffolding for harder tasks in the future.

Imagine training the medic to scale a wall to arrive at a wounded team member. The starting point when training a medic to accomplish this task will be a random policy. That starting policy will have the medic running in circles, and will likely never, or very rarely scale the wall properly to revive their team member (and achieve the reward). If we start with a simpler task, such as moving toward an unobstructed team member, then the medic can easily learn to accomplish the task. From there, we can slowly add to the difficulty of the task by increasing the size of the wall until the medic can complete the initially near-impossible task of scaling the wall. We have included an environment to demonstrate this with ML-Agents, called [Wall Jump](Learning-Environment-Examples.md#wall-jump).

![Wall](images/curriculum.png)

_Demonstration of a hypothetical curriculum training scenario in which a progressively taller wall obstructs the path to the goal._

_[**Note**: The example provided above is for instructional purposes, and was based on an early version of the [Wall Jump example environment](Learning-Environment-Examples.md). As such, it is not possible to directly replicate the results here using that environment.]_

The ML-Agents Toolkit supports modifying custom environment parameters during the training process to aid in learning. This allows elements of the environment related to difficulty or complexity to be dynamically adjusted based on training progress. The [Training ML-Agents](Training-ML-Agents.md#curriculum) page has more information on defining training curriculums.

### Training Robust Agents using Environment Parameter Randomization

An agent trained on a specific environment, may be unable to generalize to any tweaks or variations in the environment (in machine learning this is referred to as overfitting). This becomes problematic in cases where environments are instantiated with varying objects or properties. One mechanism to alleviate this and train more robust agents that can generalize to unseen variations of the environment is to expose them to these variations during training. Similar to Curriculum Learning, where environments become more difficult as the agent learns, the ML-Agents Toolkit provides a way to randomly sample parameters of the environment during training. We refer to this approach as **Environment Parameter Randomization**. For those familiar with Reinforcement Learning research, this approach is based on the concept of [Domain Randomization](https://arxiv.org/abs/1703.06907). By using [parameter randomization during training](Training-ML-Agents.md#environment-parameter-randomization), the agent can be better suited to adapt (with higher performance) to future unseen variations of the environment.

|      Ball scale of 0.5       |      Ball scale of 4       |
| :--------------------------: | :------------------------: |
| ![](images/3dball_small.png) | ![](images/3dball_big.png) |

_Example of variations of the 3D Ball environment. The environment parameters are `gravity`, `ball_mass` and `ball_scale`._

## Model Types

Regardless of the training method deployed, there are a few model types that users can train using the ML-Agents Toolkit. This is due to the flexibility in defining agent observations, which include vector, ray cast and visual observations. You can learn more about how to instrument an agent's observation in the [Designing Agents](Learning-Environment-Design-Agents.md) guide.

### Learning from Vector Observations

Whether an agent's observations are ray cast or vector, the ML-Agents Toolkit provides a fully connected neural network model to learn from those observations. At training time you can configure different aspects of this model such as the number of hidden units and number of layers.

### Learning from Cameras using Convolutional Neural Networks

Unlike other platforms, where the agent’s observation might be limited to a single vector or image, the ML-Agents Toolkit allows multiple cameras to be used for observations per agent. This enables agents to learn to integrate information from multiple visual streams. This can be helpful in several scenarios such as training a self-driving car which requires multiple cameras with different viewpoints, or a navigational agent which might need to integrate aerial and first-person visuals. You can learn more about adding visual observations to an agent [here](Learning-Environment-Design-Agents.md#visual-observations).

When visual observations are utilized, the ML-Agents Toolkit leverages convolutional neural networks (CNN) to learn from the input images. We offer three network architectures:

- a simple encoder which consists of two convolutional layers
- the implementation proposed by [Mnih et al.](https://www.nature.com/articles/nature14236), consisting of three convolutional layers,
- the [IMPALA Resnet](https://arxiv.org/abs/1802.01561) consisting of three stacked layers, each with two residual blocks, making a much larger network than the other two.

The choice of the architecture depends on the visual complexity of the scene and the available computational resources.

### Learning from Variable Length Observations using Attention

Using the ML-Agents Toolkit, it is possible to have agents learn from a varying number of inputs. To do so, each agent can keep track of a buffer of vector observations. At each step, the agent will go through all the elements in the buffer and extract information but the elements in the buffer can change at every step. This can be useful in scenarios in which the agents must keep track of a varying number of elements throughout the episode. For example in a game where an agent must learn to avoid projectiles, but the projectiles can vary in numbers.

![Variable Length Observations Illustrated](images/variable-length-observation-illustrated.png)

You can learn more about variable length observations [here](Learning-Environment-Design-Agents.md#variable-length-observations). When variable length observations are utilized, the ML-Agents Toolkit leverages attention networks to learn from a varying number of entities. Agents using attention will ignore entities that are deemed not relevant and pay special attention to entities relevant to the current situation based on context.

### Memory-enhanced Agents using Recurrent Neural Networks

Have you ever entered a room to get something and immediately forgot what you were looking for? Don't let that happen to your agents.

![Inspector](images/ml-agents-LSTM.png)

In some scenarios, agents must learn to remember the past in order to take the best decision. When an agent only has partial observability of the environment, keeping track of past observations can help the agent learn. Deciding what the agents should remember in order to solve a task is not easy to do by hand, but our training algorithms can learn to keep track of what is important to remember with [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory).

## Additional Features

Beyond the flexible training scenarios available, the ML-Agents Toolkit includes additional features which improve the flexibility and interpretability of the training process.

- **Concurrent Unity Instances** - We enable developers to run concurrent, parallel instances of the Unity executable during training. For certain scenarios, this should speed up training. Check out our dedicated page on [creating a Unity executable](Learning-Environment-Executable.md) and the [Training ML-Agents](Training-ML-Agents.md#training-using-concurrent-unity-instances) page for instructions on how to set the number of concurrent instances.
- **Recording Statistics from Unity** - We enable developers to [record statistics](Learning-Environment-Design.md#recording-statistics) from within their Unity environments. These statistics are aggregated and generated during the training process.
- **Custom Side Channels** - We enable developers to [create custom side channels](Custom-SideChannels.md) to manage data transfer between Unity and Python that is unique to their training workflow and/or environment.


## Links discovered
- [Unity Engine](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Background-Unity.md)
- [machine learning](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Background-Machine-Learning.md)
- [PyTorch](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Background-PyTorch.md)
- [demo video of ML-Agents in action](https://www.youtube.com/watch?v=fiQsmdwEGT8&feature=youtu.be)
- [Python API](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Python-LLAPI.md)
- [gym](https://github.com/openai/gym)
- [instructions](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Python-Gym-API.md)
- [instructions](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Python-PettingZoo-API.md)
- [Running an Example](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Sample.md)
- [Inference Engine](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Inference-Engine.md)
- [gym.](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Python-Gym-API.md)
- [here](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Python-LLAPI.md)
- [Training ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Training-ML-Agents.md)
- [Proximal Policy Optimization (PPO)](https://openai.com/research/openai-baselines-ppo)
- [Soft Actor-Critic (SAC)](https://bair.berkeley.edu/blog/2018/12/14/sac/)
- [here](https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/)
- [Curiosity-driven Exploration by Self-supervised Prediction](https://pathak22.github.io/noreward-rl/)
- [blog post on the Curiosity module](https://blogs.unity3d.com/2018/06/26/solving-sparse-reward-tasks-with-curiosity/)
- [Exploration by Random Network Distillation](https://arxiv.org/abs/1810.12894)
- [video demo](https://youtu.be/kpb8ZkMBFYs)
- [Pyramids environment](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Examples.md#pyramids)
- [_survivor bias_](https://arxiv.org/pdf/1809.02925.pdf)
- [Generative Adversarial Imitation Learning](https://arxiv.org/abs/1606.03476)
- [Designing Agents](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Design-Agents.md#recording-demonstrations)
- [Self-Play](https://openai.com/research/competitive-self-play)
- [Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/1702.08887.pdf)
- [Designing Agents](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Design-Agents.md#defining-multi-agent-scenarios)
- [blog post on self-play](https://blogs.unity3d.com/2020/02/28/training-intelligent-adversaries-using-self-play-with-ml-agents/)
- [ELO Rating System](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/ELO-Rating-System.md)
- [PushBlock with Agents Working Together](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/cooperative_pushblock.png)
- [this page](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Design-Agents.md#groups-for-cooperative-scenarios)
- [On the Use and Misuse of Absorbing States in Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2111.05992.pdf)
- [Lowe et al.](https://arxiv.org/abs/1706.02275)
- [Foerster et al.](https://arxiv.org/pdf/1705.08926.pdf)
- [Wall Jump](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Examples.md#wall-jump)
- [Wall](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/curriculum.png)
- [**Note**: The example provided above is for instructional purposes, and was based on an early version of the [Wall Jump example environment](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Examples.md)
- [Training ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Training-ML-Agents.md#curriculum)
- [Domain Randomization](https://arxiv.org/abs/1703.06907)
- [parameter randomization during training](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Training-ML-Agents.md#environment-parameter-randomization)
- [Designing Agents](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Design-Agents.md)
- [here](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Design-Agents.md#visual-observations)
- [Mnih et al.](https://www.nature.com/articles/nature14236)
- [IMPALA Resnet](https://arxiv.org/abs/1802.01561)
- [Variable Length Observations Illustrated](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/variable-length-observation-illustrated.png)
- [here](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Design-Agents.md#variable-length-observations)
- [Inspector](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/ml-agents-LSTM.png)
- [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory)
- [creating a Unity executable](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Executable.md)
- [Training ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Training-ML-Agents.md#training-using-concurrent-unity-instances)
- [record statistics](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Design.md#recording-statistics)
- [create custom side channels](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Custom-SideChannels.md)

--- docs/doxygen/Readme.md ---
# Doxygen files

To generate the API reference as HTML files, run:

    doxygen dox-ml-agents.conf


--- localized_docs/KR/README.md ---
<img src="docs/images/image-banner.png" align="middle" width="3000"/>

# 유니티 ML-Agents 툴킷

[![docs badge](https://img.shields.io/badge/docs-reference-blue.svg)](https://github.com/Unity-Technologies/ml-agents/tree/release_12_docs/docs/)

[![license badge](https://img.shields.io/badge/license-Apache--2.0-green.svg)](LICENSE)

([latest release](https://github.com/Unity-Technologies/ml-agents/releases/tag/latest_release))
([all releases](https://github.com/Unity-Technologies/ml-agents/releases))

**유니티 기계학습 에이전트 툴킷** (ML-Agents) 은 게임 컨텐츠 및 게임을 포함한 다양한 시뮬레이션에서 활용하기 위한 지능형 에이전트를 훈련시키는 환경을 제공하는 오픈 소스 프로젝트입니다.
ML-Agents는 게임 개발자 들이 2D, 3D 및 가상현실/증강현실 게임에서 지능형 에이전트를 쉽게 교육할 수 있도록 최신 알고리즘의 구현(PyTorch 기반)을 제공합니다.
간단한 파이썬 API를 사용하여 강화 학습, 모방 학습, 신경 진화 등 다른 방법을 활용하여 에이전트를 교육할 수 있습니다.
학습된 에이전트는 NPC 행동 제어(다중 에이전트 및 적대적 에이전트와 같은 다양한 설정), 게임 빌드 테스트 자동화, 그리고 출시 전 게임 설계(밸런스) 검증 등을 포함한 다양한 용도로 활용할 수 있습니다.
ML-Agents 툴킷은 유니티의 자유로운 환경에서 인공지능 에이전트를 개발하기 위한 기반을 제공하며, 이틀 통해 연구자 및 게임 개발자 등 광범위한 커뮤니티에 접근할 수 있기 때문에 게임 개발자와 인공지능 연구원 모두에게 상호 이익이 됩니다.

## 특징

- 15+ [유니티 환경 예제](docs/Learning-Environment-Examples.md)
- 다양한 환경 구성 및 교육 시나리오 지원
- 게임이나 커스텀 유니티 씬에 통합될 수 있는 유연한 유니티 SDK
- Proximal Policy Optimization (PPO) 와 Soft Actor-Critic (SAC) 의 두 가지 심층 강화 학습 알고리즘을 이용한 훈련
- Behavioral Cloning 이나 Generative Adversarial Imitation Learning 을 통한 모방 학습에 대한 내장 지원
- 적대적(Adversarial) 시나리오에서 에이전트를 교육하기 위한 Self-play 메커니즘
- 복잡한 작업에 대해 쉽게 정의할 수 있는 커리큘럼 학습 시나리오
- 환경 랜덤화를 사용하여 강력한 에이전트 학습
- 온 디맨드 의사 결정을 통한 유연한 에이전트 제어
- 여러 개의 유니티 환경 인스턴스를 동시에 사용하는 학습
- 네이티브 크로스 플랫폼을 지원하기 위해 [유니티 추론(Inference) 엔진](docs/Unity-Inference-Engine.md) 이용
- 유니티 환경 [파이썬에서 제어](docs/Python-API.md)
- [gym](gym-unity/README.md) 과 같은 유니티 학습 환경 제공

이 모든 기능에 대한 자세한 설명은 [ML-Agents 개요](docs/ML-Agents-Overview.md) 페이지를 참조하십시오.

## 릴리즈 & 설명서

**최신의 안정적 릴리즈는 `Release 12` 입니다. 클릭해서 ML-Agents의 최신 릴리스를 시작하세요.** [여기](https://github.com/Unity-Technologies/ml-agents/tree/release_12_docs/docs/Readme.md)

아래 표에는 현재 개발이 진행 중이며 불안정할 수 있는 `master` 브랜치를 포함한 모든 릴리스가 나와 있습니다. 몇 가지 유용한 지침:
- [버전 관리 페이지](docs/Versioning.md) 는 GitHub 릴리즈를 관리하는 방법과 각 ML-Agents 구성 요소에 대한 버전 관리 프로세스를 간략히 설명합니다.
- [릴리즈 페이지](https://github.com/Unity-Technologies/ml-agents/releases) 는 릴리스 간의 변경 사항에 대한 세부 정보가 포함되어 있습니다.
- [마이그레이션(Migration) 페이지](docs/Migrating.md) 는 이전 릴리스의 ML-Agents 툴킷에서 업그레이드하는 방법에 대한 세부 정보가 포함되어 있습니다.
- 아래 표의 **설명서** 링크에는 각 릴리스에 대한 설치 및 사용 지침이 포함되어 있습니다. 사용 중인 릴리스 버전에 해당하는 설명서를 항상 사용해야 합니다.

| **버전** | **릴리즈 날짜** | **소스** | **설명서** | **다운로드** |
|:-------:|:------:|:-------------:|:-------:|:------------:|
| **master (unstable)** | -- | [source](https://github.com/Unity-Technologies/ml-agents/tree/master) | [docs](https://github.com/Unity-Technologies/ml-agents/tree/master/docs/Readme.md) | [download](https://github.com/Unity-Technologies/ml-agents/archive/master.zip) |
| **Release 12** | **December 22, 2020** | **[source](https://github.com/Unity-Technologies/ml-agents/tree/release_12)** | **[docs](https://github.com/Unity-Technologies/ml-agents/tree/release_12_docs/docs/Readme.md)** | **[download](https://github.com/Unity-Technologies/ml-agents/archive/release_12.zip)** |
| **Release 11** | December 21, 2020 | [source](https://github.com/Unity-Technologies/ml-agents/tree/release_11) | [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_11_docs/docs/Readme.md) | [download](https://github.com/Unity-Technologies/ml-agents/archive/release_11.zip) |
| **Release 10** | November 18, 2020 | [source](https://github.com/Unity-Technologies/ml-agents/tree/release_10) | [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_10_docs/docs/Readme.md) | [download](https://github.com/Unity-Technologies/ml-agents/archive/release_10.zip) |
| **Release 9** | November 4, 2020 | [source](https://github.com/Unity-Technologies/ml-agents/tree/release_9) | [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_9_docs/docs/Readme.md) | [download](https://github.com/Unity-Technologies/ml-agents/archive/release_9.zip) |
| **Release 8** | October 14, 2020 | [source](https://github.com/Unity-Technologies/ml-agents/tree/release_8) | [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_8_docs/docs/Readme.md) | [download](https://github.com/Unity-Technologies/ml-agents/archive/release_8.zip) |
| **Release 7** | September 16, 2020 | [source](https://github.com/Unity-Technologies/ml-agents/tree/release_7) | [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_7_docs/docs/Readme.md) | [download](https://github.com/Unity-Technologies/ml-agents/archive/release_7.zip) |
| **Release 6** | August 12, 2020 | [source](https://github.com/Unity-Technologies/ml-agents/tree/release_6) | [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_6_docs/docs/Readme.md) | [download](https://github.com/Unity-Technologies/ml-agents/archive/release_6.zip) |
| **Release 5** | July 31, 2020 | [source](https://github.com/Unity-Technologies/ml-agents/tree/release_5) | [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_5_docs/docs/Readme.md) | [download](https://github.com/Unity-Technologies/ml-agents/archive/release_5.zip) |

## 인용

인공지능 플랫폼으로서의 유니티에 대한 논의에 관심이 있는 연구자라면, 프리프린트를 참조하시오.
[ 및 ML-Agents 툴킷에 대한 참조 문서](https://arxiv.org/abs/1809.02627).

유니티 또는 ML-Agents 툴킷을 사용하여 연구를 수행하는 경우, 다음 논문을 참조 자료로 인용할 것을 요청합니다.
Juliani, A., Berges, V., Teng, E., Cohen, A., Harper, J., Elion, C., Goy, C., Gao, Y., Henry, H., Mattar, M., Lange, D. (2020). Unity: A General Platform for Intelligent Agents. _arXiv preprint [arXiv:1809.02627](https://arxiv.org/abs/1809.02627)._ https://github.com/Unity-Technologies/ml-agents.

## 추가 리소스

유니티 및 ML-Agents 툴킷에 대해 자세히 소개하는 유니티 학습 과정이 있습니다. [ML-Agents: 벌새](https://learn.unity.com/course/ml-agents-hummingbirds)

또한 [CodeMonkeyUnity](https://www.youtube.com/c/CodeMonkeyUnity)와 제휴하여 ML-Agents 툴킷의 구현 및 사용 방법에 대한 [튜토리얼 비디오](https://www.youtube.com/playlist?list=PLzDRvYVwl53vehwiN_odYJkPBzcqFw110)도 제작했습니다.

또한 ML-Agents 관련 블로그 게시물도 게시했습니다.

- (December 28, 2020)
  [Happy holidays from the Unity ML-Agents team!](https://blogs.unity3d.com/2020/12/28/happy-holidays-from-the-unity-ml-agents-team/)
- (November 20, 2020)
  [How Eidos-Montréal created Grid Sensors to improve observations for training agents](https://blogs.unity3d.com/2020/11/20/how-eidos-montreal-created-grid-sensors-to-improve-observations-for-training-agents/)
- (November 11, 2020)
  [2020 AI@Unity interns shoutout](https://blogs.unity3d.com/2020/11/11/2020-aiunity-interns-shoutout/)
- (May 12, 2020)
  [Announcing ML-Agents Unity Package v1.0!](https://blogs.unity3d.com/2020/05/12/announcing-ml-agents-unity-package-v1-0/)
- (February 28, 2020)
  [Training intelligent adversaries using self-play with ML-Agents](https://blogs.unity3d.com/2020/02/28/training-intelligent-adversaries-using-self-play-with-ml-agents/)
- (November 11, 2019)
  [Training your agents 7 times faster with ML-Agents](https://blogs.unity3d.com/2019/11/11/training-your-agents-7-times-faster-with-ml-agents/)
- (October 21, 2019)
  [The AI@Unity interns help shape the world](https://blogs.unity3d.com/2019/10/21/the-aiunity-interns-help-shape-the-world/)
- (April 15, 2019)
  [Unity ML-Agents Toolkit v0.8: Faster training on real games](https://blogs.unity3d.com/2019/04/15/unity-ml-agents-toolkit-v0-8-faster-training-on-real-games/)
- (March 1, 2019)
  [Unity ML-Agents Toolkit v0.7: A leap towards cross-platform inference](https://blogs.unity3d.com/2019/03/01/unity-ml-agents-toolkit-v0-7-a-leap-towards-cross-platform-inference/)
- (December 17, 2018)
  [ML-Agents Toolkit v0.6: Improved usability of Brains and Imitation Learning](https://blogs.unity3d.com/2018/12/17/ml-agents-toolkit-v0-6-improved-usability-of-brains-and-imitation-learning/)
- (October 2, 2018)
  [Puppo, The Corgi: Cuteness Overload with the Unity ML-Agents Toolkit](https://blogs.unity3d.com/2018/10/02/puppo-the-corgi-cuteness-overload-with-the-unity-ml-agents-toolkit/)
- (September 11, 2018)
  [ML-Agents Toolkit v0.5, new resources for AI researchers available now](https://blogs.unity3d.com/2018/09/11/ml-agents-toolkit-v0-5-new-resources-for-ai-researchers-available-now/)
- (June 26, 2018)
  [Solving sparse-reward tasks with Curiosity](https://blogs.unity3d.com/2018/06/26/solving-sparse-reward-tasks-with-curiosity/)
- (June 19, 2018)
  [Unity ML-Agents Toolkit v0.4 and Udacity Deep Reinforcement Learning Nanodegree](https://blogs.unity3d.com/2018/06/19/unity-ml-agents-toolkit-v0-4-and-udacity-deep-reinforcement-learning-nanodegree/)
- (May 24, 2018)
  [Imitation Learning in Unity: The Workflow](https://blogs.unity3d.com/2018/05/24/imitation-learning-in-unity-the-workflow/)
- (March 15, 2018)
  [ML-Agents Toolkit v0.3 Beta released: Imitation Learning, feedback-driven features, and more](https://blogs.unity3d.com/2018/03/15/ml-agents-v0-3-beta-released-imitation-learning-feedback-driven-features-and-more/)
- (December 11, 2017)
  [Using Machine Learning Agents in a real game: a beginner’s guide](https://blogs.unity3d.com/2017/12/11/using-machine-learning-agents-in-a-real-game-a-beginners-guide/)
- (December 8, 2017)
  [Introducing ML-Agents Toolkit v0.2: Curriculum Learning, new environments, and more](https://blogs.unity3d.com/2017/12/08/introducing-ml-agents-v0-2-curriculum-learning-new-environments-and-more/)
- (September 19, 2017)
  [Introducing: Unity Machine Learning Agents Toolkit](https://blogs.unity3d.com/2017/09/19/introducing-unity-machine-learning-agents/)
- Overviewing reinforcement learning concepts
  ([multi-armed bandit](https://blogs.unity3d.com/2017/06/26/unity-ai-themed-blog-entries/)
  and
  [Q-learning](https://blogs.unity3d.com/2017/08/22/unity-ai-reinforcement-learning-with-q-learning/))


## 커뮤니티 그리고 피드백

ML-Agents 툴킷은 오픈소스 프로젝트이며 컨트리뷰션을 환영합니다. 만약 컨트리뷰션을 원하시는 경우
[컨트리뷰션 가이드라인](com.unity.ml-agents/CONTRIBUTING.md) 과 [행동 규칙](CODE_OF_CONDUCT.md) 을 검토해주십시오.

ML-Agents 툴킷 설치 및 설정과 관련된 문제 또는 에이전트를 가장 잘 설정하거나 교육하는 방법에 대한 논의는 [유니티 ML-Agents 포럼](https://forum.unity.com/forums/ml-agents.453/) 에 새 스레드를 작성하십시오. 가능한 많은 세부 정보를 포함해야 합니다.
ML-Agents 툴킷을 사용하여 다른 문제가 발생하거나 특정 기능 요청이 있는 경우 [이슈 제출](https://github.com/Unity-Technologies/ml-agents/issues) 부탁합니다.

여러분의 의견은 저희에게 매우 중요합니다. 유니티 ML-Agents 툴킷에 관련된 여러분의 의견을 통해서 저희는 계속해서 발전하고 성장할 수 있습니다. 단 몇 분만 사용하여 [저희에게 알려주세요](https://github.com/Unity-Technologies/ml-agents/issues/1454).

다른 의견과 피드백은 ML-Agents 팀과 직접 연락부탁드립니다. (ml-agents@unity3d.com)


## 개인정보

Unity ML-Agents 툴킷에 대한 개발자 경험을 개선하기 위해, 우리는 에디터 내부 분석을 추가했습니다.
[유니티 개인 정보 보호 정책](https://unity3d.com/legal/privacy-policy) 의 "Unity가 기본적으로 수집하는 정보"를 참조하시기 바랍니다.


## 라이센스

[Apache License 2.0](LICENSE)


## 한글 번역

유니티 ML-Agents 관련 문서의 한글 번역은 [장현준(Hyeonjun Jang)][https://github.com/JangHyeonJun],  [민규식 (Kyushik Min)]([https://github.com/Kyushik](https://github.com/Kyushik))에 의해 진행되었습니다. 내용상 오류나 오탈자가 있는 경우 각 문서의 번역을 진행한 사람의 이메일을 통해 연락주시면 감사드리겠습니다.

장현준: totok682@naver.com

민규식: kyushikmin@gmail.com

최태혁: chlxogur_@naver.com


## Links discovered
- [![docs badge](https://img.shields.io/badge/docs-reference-blue.svg)
- [![license badge](https://img.shields.io/badge/license-Apache--2.0-green.svg)
- [latest release](https://github.com/Unity-Technologies/ml-agents/releases/tag/latest_release)
- [all releases](https://github.com/Unity-Technologies/ml-agents/releases)
- [유니티 환경 예제](https://github.com/Unity-Technologies/ml-agents/blob/develop/localized_docs/KR/docs/Learning-Environment-Examples.md)
- [유니티 추론(Inference) 엔진](https://github.com/Unity-Technologies/ml-agents/blob/develop/localized_docs/KR/docs/Unity-Inference-Engine.md)
- [파이썬에서 제어](https://github.com/Unity-Technologies/ml-agents/blob/develop/localized_docs/KR/docs/Python-API.md)
- [gym](https://github.com/Unity-Technologies/ml-agents/blob/develop/localized_docs/KR/gym-unity/README.md)
- [ML-Agents 개요](https://github.com/Unity-Technologies/ml-agents/blob/develop/localized_docs/KR/docs/ML-Agents-Overview.md)
- [여기](https://github.com/Unity-Technologies/ml-agents/tree/release_12_docs/docs/Readme.md)
- [버전 관리 페이지](https://github.com/Unity-Technologies/ml-agents/blob/develop/localized_docs/KR/docs/Versioning.md)
- [릴리즈 페이지](https://github.com/Unity-Technologies/ml-agents/releases)
- [마이그레이션(Migration) 페이지](https://github.com/Unity-Technologies/ml-agents/blob/develop/localized_docs/KR/docs/Migrating.md)
- [source](https://github.com/Unity-Technologies/ml-agents/tree/master)
- [docs](https://github.com/Unity-Technologies/ml-agents/tree/master/docs/Readme.md)
- [download](https://github.com/Unity-Technologies/ml-agents/archive/master.zip)
- [source](https://github.com/Unity-Technologies/ml-agents/tree/release_12)
- [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_12_docs/docs/Readme.md)
- [download](https://github.com/Unity-Technologies/ml-agents/archive/release_12.zip)
- [source](https://github.com/Unity-Technologies/ml-agents/tree/release_11)
- [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_11_docs/docs/Readme.md)
- [download](https://github.com/Unity-Technologies/ml-agents/archive/release_11.zip)
- [source](https://github.com/Unity-Technologies/ml-agents/tree/release_10)
- [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_10_docs/docs/Readme.md)
- [download](https://github.com/Unity-Technologies/ml-agents/archive/release_10.zip)
- [source](https://github.com/Unity-Technologies/ml-agents/tree/release_9)
- [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_9_docs/docs/Readme.md)
- [download](https://github.com/Unity-Technologies/ml-agents/archive/release_9.zip)
- [source](https://github.com/Unity-Technologies/ml-agents/tree/release_8)
- [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_8_docs/docs/Readme.md)
- [download](https://github.com/Unity-Technologies/ml-agents/archive/release_8.zip)
- [source](https://github.com/Unity-Technologies/ml-agents/tree/release_7)
- [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_7_docs/docs/Readme.md)
- [download](https://github.com/Unity-Technologies/ml-agents/archive/release_7.zip)
- [source](https://github.com/Unity-Technologies/ml-agents/tree/release_6)
- [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_6_docs/docs/Readme.md)
- [download](https://github.com/Unity-Technologies/ml-agents/archive/release_6.zip)
- [source](https://github.com/Unity-Technologies/ml-agents/tree/release_5)
- [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_5_docs/docs/Readme.md)
- [download](https://github.com/Unity-Technologies/ml-agents/archive/release_5.zip)
- [및 ML-Agents 툴킷에 대한 참조 문서](https://arxiv.org/abs/1809.02627)
- [arXiv:1809.02627](https://arxiv.org/abs/1809.02627)
- [ML-Agents: 벌새](https://learn.unity.com/course/ml-agents-hummingbirds)
- [CodeMonkeyUnity](https://www.youtube.com/c/CodeMonkeyUnity)
- [튜토리얼 비디오](https://www.youtube.com/playlist?list=PLzDRvYVwl53vehwiN_odYJkPBzcqFw110)
- [Happy holidays from the Unity ML-Agents team!](https://blogs.unity3d.com/2020/12/28/happy-holidays-from-the-unity-ml-agents-team/)
- [How Eidos-Montréal created Grid Sensors to improve observations for training agents](https://blogs.unity3d.com/2020/11/20/how-eidos-montreal-created-grid-sensors-to-improve-observations-for-training-agents/)
- [2020 AI@Unity interns shoutout](https://blogs.unity3d.com/2020/11/11/2020-aiunity-interns-shoutout/)
- [Announcing ML-Agents Unity Package v1.0!](https://blogs.unity3d.com/2020/05/12/announcing-ml-agents-unity-package-v1-0/)
- [Training intelligent adversaries using self-play with ML-Agents](https://blogs.unity3d.com/2020/02/28/training-intelligent-adversaries-using-self-play-with-ml-agents/)
- [Training your agents 7 times faster with ML-Agents](https://blogs.unity3d.com/2019/11/11/training-your-agents-7-times-faster-with-ml-agents/)
- [The AI@Unity interns help shape the world](https://blogs.unity3d.com/2019/10/21/the-aiunity-interns-help-shape-the-world/)
- [Unity ML-Agents Toolkit v0.8: Faster training on real games](https://blogs.unity3d.com/2019/04/15/unity-ml-agents-toolkit-v0-8-faster-training-on-real-games/)
- [Unity ML-Agents Toolkit v0.7: A leap towards cross-platform inference](https://blogs.unity3d.com/2019/03/01/unity-ml-agents-toolkit-v0-7-a-leap-towards-cross-platform-inference/)
- [ML-Agents Toolkit v0.6: Improved usability of Brains and Imitation Learning](https://blogs.unity3d.com/2018/12/17/ml-agents-toolkit-v0-6-improved-usability-of-brains-and-imitation-learning/)
- [Puppo, The Corgi: Cuteness Overload with the Unity ML-Agents Toolkit](https://blogs.unity3d.com/2018/10/02/puppo-the-corgi-cuteness-overload-with-the-unity-ml-agents-toolkit/)
- [ML-Agents Toolkit v0.5, new resources for AI researchers available now](https://blogs.unity3d.com/2018/09/11/ml-agents-toolkit-v0-5-new-resources-for-ai-researchers-available-now/)
- [Solving sparse-reward tasks with Curiosity](https://blogs.unity3d.com/2018/06/26/solving-sparse-reward-tasks-with-curiosity/)
- [Unity ML-Agents Toolkit v0.4 and Udacity Deep Reinforcement Learning Nanodegree](https://blogs.unity3d.com/2018/06/19/unity-ml-agents-toolkit-v0-4-and-udacity-deep-reinforcement-learning-nanodegree/)
- [Imitation Learning in Unity: The Workflow](https://blogs.unity3d.com/2018/05/24/imitation-learning-in-unity-the-workflow/)
- [ML-Agents Toolkit v0.3 Beta released: Imitation Learning, feedback-driven features, and more](https://blogs.unity3d.com/2018/03/15/ml-agents-v0-3-beta-released-imitation-learning-feedback-driven-features-and-more/)
- [Using Machine Learning Agents in a real game: a beginner’s guide](https://blogs.unity3d.com/2017/12/11/using-machine-learning-agents-in-a-real-game-a-beginners-guide/)
- [Introducing ML-Agents Toolkit v0.2: Curriculum Learning, new environments, and more](https://blogs.unity3d.com/2017/12/08/introducing-ml-agents-v0-2-curriculum-learning-new-environments-and-more/)
- [Introducing: Unity Machine Learning Agents Toolkit](https://blogs.unity3d.com/2017/09/19/introducing-unity-machine-learning-agents/)
- [multi-armed bandit](https://blogs.unity3d.com/2017/06/26/unity-ai-themed-blog-entries/)
- [Q-learning](https://blogs.unity3d.com/2017/08/22/unity-ai-reinforcement-learning-with-q-learning/)
- [컨트리뷰션 가이드라인](https://github.com/Unity-Technologies/ml-agents/blob/develop/localized_docs/KR/com.unity.ml-agents/CONTRIBUTING.md)
- [행동 규칙](https://github.com/Unity-Technologies/ml-agents/blob/develop/localized_docs/KR/CODE_OF_CONDUCT.md)
- [유니티 ML-Agents 포럼](https://forum.unity.com/forums/ml-agents.453/)
- [이슈 제출](https://github.com/Unity-Technologies/ml-agents/issues)
- [저희에게 알려주세요](https://github.com/Unity-Technologies/ml-agents/issues/1454)
- [유니티 개인 정보 보호 정책](https://unity3d.com/legal/privacy-policy)
- [Apache License 2.0](https://github.com/Unity-Technologies/ml-agents/blob/develop/localized_docs/KR/LICENSE.md)
- [민규식 (Kyushik Min)](https://github.com/Unity-Technologies/ml-agents/blob/develop/localized_docs/KR/[https:/github.com/Kyushik](https:/github.com/Kyushik.md)

--- localized_docs/RU/README.md ---
<img src="https://github.com/Unity-Technologies/ml-agents/blob/main/docs/images/image-banner.png" align="middle" width="3000"/>

# Unity ML-Agents Toolkit Version Release 7

[![docs badge](https://img.shields.io/badge/docs-reference-blue.svg)](https://github.com/Unity-Technologies/ml-agents/tree/release_7_docs/docs/)
[![license badge](https://img.shields.io/badge/license-Apache--2.0-green.svg)](LICENSE)

([latest release](https://github.com/Unity-Technologies/ml-agents/releases/tag/latest_release))
([all releases](https://github.com/Unity-Technologies/ml-agents/releases))

**The Unity Machine Learning Agents Toolkit (ML-Agents)** - open-source проект,
предназначенный для обучения искусственного интеллекта (агента) через взаимодействие со средой, -
игрой или симуляцией, - используя различные методы машинного обучения:
обучение с подкреплением (reinforcement learning), имитационное обучение (imitation learning),
нейроэволюция (neuroevolution) и др. средствами Python API. В проекте реализованы также и современные
алгоритмы (на основе TensorFlow), чтобы дать возможность как разработчикам игр так и любым другим,
кто увлечен темой AI, обучать искусственный интеллект для 2D, 3D и VR/AR игр. Применение таких агентов
бесчисленно: например, вы можете использовать их для управления NPC (опций также много - будь то
обучение действиям в кооперативе или друг против друга), для тестирования различных версий сборок
игры, а также для оценки гейм дизайнерских решений. ML-Agents объединяет разработчиков игр и
исследователей AI, так как предоставляет единую платформу, в рамках которой новые разработки
в сфере искусственного интеллекта могут быть протестированы через движок Unity и, как следствие,
стать доступнее большему количеству и тех, и других.

## Особенности:

- Более [15 примеров на Unity](docs/Learning-Environment-Examples.md).
- Большие возможности по конфигурации среды и тренировочных сценариев.
- Unity SDK, который легко встроить в код вашей игры или в кастомную сцену в Unity
- Два алгоритма глубинного обучения с подкреплением (deep reinforcement learning):
Proximal Policy Optimization (PPO) и Soft Actor-Critic (SAC). Первый алгоритм старается узнать,
какой будет наилучший шаг в конкретной ситуации, тогда как второй - узнать в целом правила
игры/системы/симуляции, их закон и действовать согласно этому усвоенному закону изменения среды.
- Встроенная поддержка для имитационного обучения (Imitation Learning), которое можно сделать
либо через клонирование поведения (Behavioral Cloning), либо через генеративно-состязательное
имитационное обучение (Generative Adversarial Imitation Learning - GAIL), когда одна часть алгоритма
генерирует поведение, а другая определяет, похоже данное поведение на то, которое было дано как исходное,
например, самим пользователем в виде записи его действий. Генерация происходит до тех пор, пока
сгенерированное поведение не будет определено как неотличимое или очень близкое к исходному.
- Возможность для агента игры с самим собой, если агент обучается в контексте сценария “состязание”:
например, игра в футбол, где есть две команды.
- ML-Agents позволяет настроить череду сцен, где каждая новая сцена - это усложнение сцены предыдущей,
например, добавление новой преграды. Не всегда поставленную задачу агент сможет научиться
выполнять, если среда слишком сложная изначально. Дайте ему сначала сценку попроще, когда
он научиться ее проходить, его перенесет на уровень посложнее.
- Обучение агента, устойчивого к изменениям, с помощью возможности случайного генерации элементов сцены
- Гибкий контроль агента: демонстрация выученного поведения только при определенных условиях.
Например, NPC входит в контекст “атака” - атакует так, как научился ранее в рамках обучающего сценария.
- Обучение агента сразу на множестве сцен одновременно. Представьте, как он играет в футбол сразу
на десяти стадионах, набираясь опыта одновременно на них всех. Выглядит это в Unity также,
как и представляется.
- Использование [Sentis](docs/Unity-Inference-Engine.md) для поддержки кроссплатформенности.
- Контроль через [Python API](docs/Python-API.md) сцен.
- Возможность обернуть Unity среду для обучения как [gym](gym-unity/README.md).

Для более детального ознакомления с данными особенностями см. [Обзор ML-Agents] (docs/ML-Agents-Overview.md).

## Релизы и Документация

**Наш последний стабильный релиз - это `7-ой Релиз` (Release 7).
См. [здесь](https://github.com/Unity-Technologies/ml-agents/tree/release_7_docs/docs/Readme.md),
чтобы начать работать с самой последней версий ML-Agents.**

Таблица внизу - список всех наших релизов, включая main ветку, над которой мы ведем активную работу
и которая может быть нестабильной. Полезная информация:

[Управление версиями](docs/Versioning.md) - описание того, как мы работам с GitHub.
[Релизы](https://github.com/Unity-Technologies/ml-agents/releases) - об изменениях между версиями
[Миграция](docs/Migrating.md) - как перейти с более ранней версии ML-Agents на новую.
Ссылки на **документацию** - как установить и начать пользоваться ML-Agents в зависимости от версии.
Всегда используйте только ту документацию, которая относится к той версии, которую вы установили:

| **Version** | **Дата релиза** | **Source** | **Документация** | **Загрузка** |
|:-------:|:------:|:-------------:|:-------:|:------------:|
| **main (unstable)** | -- | [source](https://github.com/Unity-Technologies/ml-agents/tree/main) | [docs](https://github.com/Unity-Technologies/ml-agents/tree/main/docs/Readme.md) | [download](https://github.com/Unity-Technologies/ml-agents/archive/main.zip) |
| **Release 7** | **16 Сентября, 2020** | **[source](https://github.com/Unity-Technologies/ml-agents/tree/release_7)** | **[docs](https://github.com/Unity-Technologies/ml-agents/tree/release_7_docs/docs/Readme.md)** | **[download](https://github.com/Unity-Technologies/ml-agents/archive/release_7.zip)** |
| **Release 6** | 12 Августа, 2020 | [source](https://github.com/Unity-Technologies/ml-agents/tree/release_6) | [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_6_docs/docs/Readme.md) | [download](https://github.com/Unity-Technologies/ml-agents/archive/release_6.zip) |
| **Release 5** | 31 Июля, 2020 | [source](https://github.com/Unity-Technologies/ml-agents/tree/release_5) | [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_5_docs/docs/Readme.md) | [download](https://github.com/Unity-Technologies/ml-agents/archive/release_5.zip) |
| **Release 4** | 15 Июля, 2020 | [source](https://github.com/Unity-Technologies/ml-agents/tree/release_4) | [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_4_docs/docs/Readme.md) | [download](https://github.com/Unity-Technologies/ml-agents/archive/release_4.zip) |
| **Release 3** | 10 Июня, 2020 | [source](https://github.com/Unity-Technologies/ml-agents/tree/release_3) | [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_3_docs/docs/Readme.md) | [download](https://github.com/Unity-Technologies/ml-agents/archive/release_3.zip) |
| **Release 2** | 20 Мая, 2020 | [source](https://github.com/Unity-Technologies/ml-agents/tree/release_2) | [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_2_docs/docs/Readme.md) | [download](https://github.com/Unity-Technologies/ml-agents/archive/release_2.zip) |
| **Release 1** | 30 Апреля, 2020 | [source](https://github.com/Unity-Technologies/ml-agents/tree/release_1) | [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_1_docs/docs/Readme.md) | [download](https://github.com/Unity-Technologies/ml-agents/archive/release_1.zip) |

## Цитирование

Если вас интересует Unity как платформа для изучения AI, см. [нашу работу Unity и ML-Agents](https://arxiv.org/abs/1809.02627).
Если вы используете Unity или ML-Agents для исследовательской работы, пожалуйста, указывайте
в списке используемой литературы следующую работу:
Juliani, A., Berges, V., Teng, E., Cohen, A., Harper, J., Elion, C., Goy,
C., Gao, Y., Henry, H., Mattar, M., Lange, D. (2020). Unity: A General Platform for
Intelligent Agents. _arXiv preprint
[arXiv:1809.02627].(https://arxiv.org/abs/1809.02627)._
https://github.com/Unity-Technologies/ml-agents.

## Дополнительные источники:

Мы опубликовали серию статей на нашем блоге про ML-Agents (**пока без перевода на русский**):

- (12 Мая, 2020)
[Announcing ML-Agents Unity Package v1.0!](https://blogs.unity3d.com/2020/05/12/announcing-ml-agents-unity-package-v1-0/)
- (28 Февраля, 2020)
[Training intelligent adversaries using self-play with ML-Agents](https://blogs.unity3d.com/2020/02/28/training-intelligent-adversaries-using-self-play-with-ml-agents/)
- (11 Ноября, 2019)
[Training your agents 7 times faster with ML-Agents](https://blogs.unity3d.com/2019/11/11/training-your-agents-7-times-faster-with-ml-agents/)
- (21 Октября, 2019)
[The AI@Unity interns help shape the world](https://blogs.unity3d.com/2019/10/21/the-aiunity-interns-help-shape-the-world/)
- (15 Апреля, 2019)
[Unity ML-Agents Toolkit v0.8: Faster training on real games](https://blogs.unity3d.com/2019/04/15/unity-ml-agents-toolkit-v0-8-faster-training-on-real-games/)
- (1 Марта, 2019)
[Unity ML-Agents Toolkit v0.7: A leap towards cross-platform inference](https://blogs.unity3d.com/2019/03/01/unity-ml-agents-toolkit-v0-7-a-leap-towards-cross-platform-inference/)
- (17 Декабря, 2018)
[ML-Agents Toolkit v0.6: Improved usability of Brains and Imitation Learning](https://blogs.unity3d.com/2018/12/17/ml-agents-toolkit-v0-6-improved-usability-of-brains-and-imitation-learning/)
- (2 Октября, 2018)
[Puppo, The Corgi: Cuteness Overload with the Unity ML-Agents Toolkit](https://blogs.unity3d.com/2018/10/02/puppo-the-corgi-cuteness-overload-with-the-unity-ml-agents-toolkit/)
- (11 Сентября, 2018)
[ML-Agents Toolkit v0.5, new resources for AI researchers available now](https://blogs.unity3d.com/2018/09/11/ml-agents-toolkit-v0-5-new-resources-for-ai-researchers-available-now/)
- (26 Июня, 2018)
[Solving sparse-reward tasks with Curiosity](https://blogs.unity3d.com/2018/06/26/solving-sparse-reward-tasks-with-curiosity/)
- (19 Июня, 2018)
[Unity ML-Agents Toolkit v0.4 and Udacity Deep Reinforcement Learning Nanodegree](https://blogs.unity3d.com/2018/06/19/unity-ml-agents-toolkit-v0-4-and-udacity-deep-reinforcement-learning-nanodegree/)
- (24 Мая, 2018)
[Imitation Learning in Unity: The Workflow](https://blogs.unity3d.com/2018/05/24/imitation-learning-in-unity-the-workflow/)
- (15 Марта, 2018)
[ML-Agents Toolkit v0.3 Beta released: Imitation Learning, feedback-driven features, and more](https://blogs.unity3d.com/2018/03/15/ml-agents-v0-3-beta-released-imitation-learning-feedback-driven-features-and-more/)
- (11 Декабря, 2017)
[Using Machine Learning Agents in a real game: a beginner’s guide](https://blogs.unity3d.com/2017/12/11/using-machine-learning-agents-in-a-real-game-a-beginners-guide/)
- (8 Декабря, 2017)
[Introducing ML-Agents Toolkit v0.2: Curriculum Learning, new environments, and more](https://blogs.unity3d.com/2017/12/08/introducing-ml-agents-v0-2-curriculum-learning-new-environments-and-more/)
- (19 Сентября, 2017)
[Introducing: Unity Machine Learning Agents Toolkit](https://blogs.unity3d.com/2017/09/19/introducing-unity-machine-learning-agents/)
- Обзор обучения с подкреплением (
[multi-armed bandit](https://blogs.unity3d.com/2017/06/26/unity-ai-themed-blog-entries/)
и
[Q-learning](https://blogs.unity3d.com/2017/08/22/unity-ai-reinforcement-learning-with-q-learning/))

Дополнительные материалы от других авторов:
- [A Game Developer Learns Machine Learning] (https://mikecann.co.uk/machine-learning/a-game-developer-learns-machine-learning-intent/)
- [Explore Unity Technologies ML-Agents Exclusively on Intel Architecture](https://software.intel.com/en-us/articles/explore-unity-technologies-ml-agents-exclusively-on-intel-architecture)
- [ML-Agents Penguins tutorial](https://learn.unity.com/project/ml-agents-penguins)

## Community and Feedback

ML-Agents Toolkit - open-source проект, поэтому мы рады любой помощи. Если вы хотите нам помочь,
ознакомьтесь, для начала, пожалуйста, для с [гайдом, как сделать это правильно](com.unity.ml-agents/CONTRIBUTING.md),
и [кодексом поведения](CODE_OF_CONDUCT.md).

Если возникли проблемы с установкой и настройкой ML-Agents, если вы хотите обсудить как лучше всего
обучать агентов и пр., пожалуйста, посмотрите возможные решения на [форуме Unity ML-Agents](https://forum.unity.com/forums/ml-agents.453/).
Если вы не найдете нужной вам информации, начните новую тему, дав подробное описания вашей проблемы. Если вы обнаружили
какие-то баги или ошибки во время работы с ML-Agents, пожалуйста, сообщите об этом [здесь](https://github.com/Unity-Technologies/ml-agents/issues).

Нам важно знать ваше мнение. Только на его основе проект Unity ML-Agents и продолжает развиваться.
Пожалуйста, уделите несколько минут и [поделитесь](https://github.com/Unity-Technologies/ml-agents/issues/1454)
с нами тем, что могло бы улучшить наш проект.

По всем остальным вопросам или отзыву, пишите сразу на адрес команды разработчиков ML-Agents - ml-agents@unity3d.com.

## Лицензия

Apache License 2.0


## Links discovered
- [![docs badge](https://img.shields.io/badge/docs-reference-blue.svg)
- [![license badge](https://img.shields.io/badge/license-Apache--2.0-green.svg)
- [latest release](https://github.com/Unity-Technologies/ml-agents/releases/tag/latest_release)
- [all releases](https://github.com/Unity-Technologies/ml-agents/releases)
- [15 примеров на Unity](https://github.com/Unity-Technologies/ml-agents/blob/develop/localized_docs/RU/docs/Learning-Environment-Examples.md)
- [Sentis](https://github.com/Unity-Technologies/ml-agents/blob/develop/localized_docs/RU/docs/Unity-Inference-Engine.md)
- [Python API](https://github.com/Unity-Technologies/ml-agents/blob/develop/localized_docs/RU/docs/Python-API.md)
- [gym](https://github.com/Unity-Technologies/ml-agents/blob/develop/localized_docs/RU/gym-unity/README.md)
- [здесь](https://github.com/Unity-Technologies/ml-agents/tree/release_7_docs/docs/Readme.md)
- [Управление версиями](https://github.com/Unity-Technologies/ml-agents/blob/develop/localized_docs/RU/docs/Versioning.md)
- [Релизы](https://github.com/Unity-Technologies/ml-agents/releases)
- [Миграция](https://github.com/Unity-Technologies/ml-agents/blob/develop/localized_docs/RU/docs/Migrating.md)
- [source](https://github.com/Unity-Technologies/ml-agents/tree/main)
- [docs](https://github.com/Unity-Technologies/ml-agents/tree/main/docs/Readme.md)
- [download](https://github.com/Unity-Technologies/ml-agents/archive/main.zip)
- [source](https://github.com/Unity-Technologies/ml-agents/tree/release_7)
- [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_7_docs/docs/Readme.md)
- [download](https://github.com/Unity-Technologies/ml-agents/archive/release_7.zip)
- [source](https://github.com/Unity-Technologies/ml-agents/tree/release_6)
- [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_6_docs/docs/Readme.md)
- [download](https://github.com/Unity-Technologies/ml-agents/archive/release_6.zip)
- [source](https://github.com/Unity-Technologies/ml-agents/tree/release_5)
- [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_5_docs/docs/Readme.md)
- [download](https://github.com/Unity-Technologies/ml-agents/archive/release_5.zip)
- [source](https://github.com/Unity-Technologies/ml-agents/tree/release_4)
- [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_4_docs/docs/Readme.md)
- [download](https://github.com/Unity-Technologies/ml-agents/archive/release_4.zip)
- [source](https://github.com/Unity-Technologies/ml-agents/tree/release_3)
- [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_3_docs/docs/Readme.md)
- [download](https://github.com/Unity-Technologies/ml-agents/archive/release_3.zip)
- [source](https://github.com/Unity-Technologies/ml-agents/tree/release_2)
- [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_2_docs/docs/Readme.md)
- [download](https://github.com/Unity-Technologies/ml-agents/archive/release_2.zip)
- [source](https://github.com/Unity-Technologies/ml-agents/tree/release_1)
- [docs](https://github.com/Unity-Technologies/ml-agents/tree/release_1_docs/docs/Readme.md)
- [download](https://github.com/Unity-Technologies/ml-agents/archive/release_1.zip)
- [нашу работу Unity и ML-Agents](https://arxiv.org/abs/1809.02627)
- [Announcing ML-Agents Unity Package v1.0!](https://blogs.unity3d.com/2020/05/12/announcing-ml-agents-unity-package-v1-0/)
- [Training intelligent adversaries using self-play with ML-Agents](https://blogs.unity3d.com/2020/02/28/training-intelligent-adversaries-using-self-play-with-ml-agents/)
- [Training your agents 7 times faster with ML-Agents](https://blogs.unity3d.com/2019/11/11/training-your-agents-7-times-faster-with-ml-agents/)
- [The AI@Unity interns help shape the world](https://blogs.unity3d.com/2019/10/21/the-aiunity-interns-help-shape-the-world/)
- [Unity ML-Agents Toolkit v0.8: Faster training on real games](https://blogs.unity3d.com/2019/04/15/unity-ml-agents-toolkit-v0-8-faster-training-on-real-games/)
- [Unity ML-Agents Toolkit v0.7: A leap towards cross-platform inference](https://blogs.unity3d.com/2019/03/01/unity-ml-agents-toolkit-v0-7-a-leap-towards-cross-platform-inference/)
- [ML-Agents Toolkit v0.6: Improved usability of Brains and Imitation Learning](https://blogs.unity3d.com/2018/12/17/ml-agents-toolkit-v0-6-improved-usability-of-brains-and-imitation-learning/)
- [Puppo, The Corgi: Cuteness Overload with the Unity ML-Agents Toolkit](https://blogs.unity3d.com/2018/10/02/puppo-the-corgi-cuteness-overload-with-the-unity-ml-agents-toolkit/)
- [ML-Agents Toolkit v0.5, new resources for AI researchers available now](https://blogs.unity3d.com/2018/09/11/ml-agents-toolkit-v0-5-new-resources-for-ai-researchers-available-now/)
- [Solving sparse-reward tasks with Curiosity](https://blogs.unity3d.com/2018/06/26/solving-sparse-reward-tasks-with-curiosity/)
- [Unity ML-Agents Toolkit v0.4 and Udacity Deep Reinforcement Learning Nanodegree](https://blogs.unity3d.com/2018/06/19/unity-ml-agents-toolkit-v0-4-and-udacity-deep-reinforcement-learning-nanodegree/)
- [Imitation Learning in Unity: The Workflow](https://blogs.unity3d.com/2018/05/24/imitation-learning-in-unity-the-workflow/)
- [ML-Agents Toolkit v0.3 Beta released: Imitation Learning, feedback-driven features, and more](https://blogs.unity3d.com/2018/03/15/ml-agents-v0-3-beta-released-imitation-learning-feedback-driven-features-and-more/)
- [Using Machine Learning Agents in a real game: a beginner’s guide](https://blogs.unity3d.com/2017/12/11/using-machine-learning-agents-in-a-real-game-a-beginners-guide/)
- [Introducing ML-Agents Toolkit v0.2: Curriculum Learning, new environments, and more](https://blogs.unity3d.com/2017/12/08/introducing-ml-agents-v0-2-curriculum-learning-new-environments-and-more/)
- [Introducing: Unity Machine Learning Agents Toolkit](https://blogs.unity3d.com/2017/09/19/introducing-unity-machine-learning-agents/)
- [multi-armed bandit](https://blogs.unity3d.com/2017/06/26/unity-ai-themed-blog-entries/)
- [Q-learning](https://blogs.unity3d.com/2017/08/22/unity-ai-reinforcement-learning-with-q-learning/)
- [Explore Unity Technologies ML-Agents Exclusively on Intel Architecture](https://software.intel.com/en-us/articles/explore-unity-technologies-ml-agents-exclusively-on-intel-architecture)
- [ML-Agents Penguins tutorial](https://learn.unity.com/project/ml-agents-penguins)
- [гайдом, как сделать это правильно](https://github.com/Unity-Technologies/ml-agents/blob/develop/localized_docs/RU/com.unity.ml-agents/CONTRIBUTING.md)
- [кодексом поведения](https://github.com/Unity-Technologies/ml-agents/blob/develop/localized_docs/RU/CODE_OF_CONDUCT.md)
- [форуме Unity ML-Agents](https://forum.unity.com/forums/ml-agents.453/)
- [здесь](https://github.com/Unity-Technologies/ml-agents/issues)
- [поделитесь](https://github.com/Unity-Technologies/ml-agents/issues/1454)

--- ml-agents-plugin-examples/README.md ---
# ML-Agents Plugins

See the [Plugins documentation](../com.unity.ml-agents/Documentation~/Training-Plugins.md) for more information.


## Links discovered
- [Plugins documentation](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Training-Plugins.md)

--- ml-agents-plugin-examples/setup.py ---
from setuptools import setup
from mlagents.plugins import ML_AGENTS_STATS_WRITER

setup(
    name="mlagents_plugin_examples",
    version="0.0.1",
    # Example of how to add your own registration functions that will be called
    # by mlagents-learn.
    #
    # Here, the get_example_stats_writer() function in mlagents_plugin_examples/example_stats_writer.py
    # will get registered with the ML_AGENTS_STATS_WRITER plugin interface.
    entry_points={
        ML_AGENTS_STATS_WRITER: [
            "example=mlagents_plugin_examples.example_stats_writer:get_example_stats_writer"
        ]
    },
)


--- ml-agents-plugin-examples/mlagents_plugin_examples/example_stats_writer.py ---
from typing import Dict, List
from mlagents.trainers.settings import RunOptions
from mlagents.trainers.stats import StatsWriter, StatsSummary


class ExampleStatsWriter(StatsWriter):
    """
    Example implementation of the StatsWriter abstract class.
    This doesn't do anything interesting, just prints the stats that it gets.
    """

    def write_stats(
        self, category: str, values: Dict[str, StatsSummary], step: int
    ) -> None:
        print(f"ExampleStatsWriter category: {category} values: {values}")


def get_example_stats_writer(run_options: RunOptions) -> List[StatsWriter]:
    """
    Registration function. This is referenced in setup.py and will
    be called by mlagents-learn when it starts to determine the
    list of StatsWriters to use.

    It must return a list of StatsWriters.
    """
    print("Creating a new stats writer! This is so exciting!")
    return [ExampleStatsWriter()]


--- ml-agents-plugin-examples/mlagents_plugin_examples/__init__.py ---


--- com.unity.ml-agents/Documentation~/Learning-Environment-Examples.md ---
# Example Learning Environments

<img src="images/example-envs.png" align="middle" width="3000"/>

The Unity ML-Agents Toolkit includes an expanding set of example environments that highlight the various features of the toolkit. These environments can also serve as templates for new environments or as ways to test new ML algorithms. Environments are located in `Project/Assets/ML-Agents/Examples` and summarized below.

For the environments that highlight specific features of the toolkit, we provide the pre-trained model files and the training config file that enables you to train the scene yourself. The environments that are designed to serve as challenges for researchers do not have accompanying pre-trained model files or training configs and are marked as _Optional_ below.

This page only overviews the example environments we provide. To learn more on how to design and build your own environments see our [Making a New Learning Environment](Learning-Environment-Create-New.md) page. If you would like to contribute environments, please see our [contribution guidelines](CONTRIBUTING.md) page.

## Basic

![Basic](images/basic.png)

- Set-up: A linear movement task where the agent must move left or right to rewarding states.
- Goal: Move to the most reward state.
- Agents: The environment contains one agent.
- Agent Reward Function:
  - -0.01 at each step
  - +0.1 for arriving at suboptimal state.
  - +1.0 for arriving at optimal state.
- Behavior Parameters:
  - Vector Observation space: One variable corresponding to current state.
  - Actions: 1 discrete action branch with 3 actions (Move left, do nothing, move right).
  - Visual Observations: None
- Float Properties: None
- Benchmark Mean Reward: 0.93

## 3DBall: 3D Balance Ball

![3D Balance Ball](images/balance.png)

- Set-up: A balance-ball task, where the agent balances the ball on it's head.
- Goal: The agent must balance the ball on it's head for as long as possible.
- Agents: The environment contains 12 agents of the same kind, all using the same Behavior Parameters.
- Agent Reward Function:
  - +0.1 for every step the ball remains on it's head.
  - -1.0 if the ball falls off.
- Behavior Parameters:
  - Vector Observation space: 8 variables corresponding to rotation of the agent cube, and position and velocity of ball.
  - Vector Observation space (Hard Version): 5 variables corresponding to rotation of the agent cube and position of ball.
  - Actions: 2 continuous actions, with one value corresponding to X-rotation, and the other to Z-rotation.
  - Visual Observations: Third-person view from the upper-front of the agent. Use
    `Visual3DBall` scene.
- Float Properties: Three
  - scale: Specifies the scale of the ball in the 3 dimensions (equal across the three dimensions)
    - Default: 1
    - Recommended Minimum: 0.2
    - Recommended Maximum: 5
  - gravity: Magnitude of gravity
    - Default: 9.81
    - Recommended Minimum: 4
    - Recommended Maximum: 105
  - mass: Specifies mass of the ball
    - Default: 1
    - Recommended Minimum: 0.1
    - Recommended Maximum: 20
- Benchmark Mean Reward: 100

## GridWorld

![GridWorld](images/gridworld.png)

- Set-up: A multi-goal version of the grid-world task. Scene contains agent, goal, and obstacles.
- Goal: The agent must navigate the grid to the appropriate goal while avoiding the obstacles.
- Agents: The environment contains nine agents with the same Behavior Parameters.
- Agent Reward Function:
  - -0.01 for every step.
  - +1.0 if the agent navigates to the correct goal (episode ends).
  - -1.0 if the agent navigates to an incorrect goal (episode ends).
- Behavior Parameters:
  - Vector Observation space: None
  - Actions: 1 discrete action branch with 5 actions, corresponding to movement in cardinal directions or not moving. Note that for this environment, [action masking](Learning-Environment-Design-Agents.md#masking-discrete-actions) is turned on by default (this option can be toggled using the `Mask Actions` checkbox within the `trueAgent` GameObject). The trained model file provided was generated with action masking turned on.
  - Visual Observations: One corresponding to top-down view of GridWorld.
  - Goal Signal : A one hot vector corresponding to which color is the correct goal for the Agent
- Float Properties: Three, corresponding to grid size, number of green goals, and number of red goals.
- Benchmark Mean Reward: 0.8

## Push Block

![Push](images/push.png)

- Set-up: A platforming environment where the agent can push a block around.
- Goal: The agent must push the block to the goal.
- Agents: The environment contains one agent.
- Agent Reward Function:
  - -0.0025 for every step.
  - +1.0 if the block touches the goal.
- Behavior Parameters:
  - Vector Observation space: (Continuous) 70 variables corresponding to 14 ray-casts each detecting one of three possible objects (wall, goal, or block).
  - Actions: 1 discrete action branch with 7 actions, corresponding to turn clockwise and counterclockwise, move along four different face directions, or do nothing.
- Float Properties: Four
  - block_scale: Scale of the block along the x and z dimensions
    - Default: 2
    - Recommended Minimum: 0.5
    - Recommended Maximum: 4
  - dynamic_friction: Coefficient of friction for the ground material acting on moving objects
    - Default: 0
    - Recommended Minimum: 0
    - Recommended Maximum: 1
  - static_friction: Coefficient of friction for the ground material acting on stationary objects
    - Default: 0
    - Recommended Minimum: 0
    - Recommended Maximum: 1
  - block_drag: Effect of air resistance on block
    - Default: 0.5
    - Recommended Minimum: 0
    - Recommended Maximum: 2000
- Benchmark Mean Reward: 4.5

## Wall Jump

![Wall](images/wall.png)

- Set-up: A platforming environment where the agent can jump over a wall.
- Goal: The agent must use the block to scale the wall and reach the goal.
- Agents: The environment contains one agent linked to two different Models. The Policy the agent is linked to changes depending on the height of the wall. The change of Policy is done in the WallJumpAgent class.
- Agent Reward Function:
  - -0.0005 for every step.
  - +1.0 if the agent touches the goal.
  - -1.0 if the agent falls off the platform.
- Behavior Parameters:
  - Vector Observation space: Size of 74, corresponding to 14 ray casts each detecting 4 possible objects. plus the global position of the agent and whether or not the agent is grounded.
  - Actions: 4 discrete action branches:
    - Forward Motion (3 possible actions: Forward, Backwards, No Action)
    - Rotation (3 possible actions: Rotate Left, Rotate Right, No Action)
    - Side Motion (3 possible actions: Left, Right, No Action)
    - Jump (2 possible actions: Jump, No Action)
  - Visual Observations: None
- Float Properties: Four
- Benchmark Mean Reward (Big & Small Wall): 0.8

## Crawler

![Crawler](images/crawler.png)

- Set-up: A creature with 4 arms and 4 forearms.
- Goal: The agents must move its body toward the goal direction without falling.
- Agents: The environment contains 10 agents with same Behavior Parameters.
- Agent Reward Function (independent): The reward function is now geometric meaning the reward each step is a product of all the rewards instead of a sum, this helps the agent try to maximize all rewards instead of the easiest rewards.
  - Body velocity matches goal velocity. (normalized between (0,1))
  - Head direction alignment with goal direction. (normalized between (0,1))
- Behavior Parameters:
  - Vector Observation space: 172 variables corresponding to position, rotation, velocity, and angular velocities of each limb plus the acceleration and angular acceleration of the body.
  - Actions: 20 continuous actions, corresponding to target rotations for joints.
  - Visual Observations: None
- Float Properties: None
- Benchmark Mean Reward: 3000

## Worm

![Worm](images/worm.png)

- Set-up: A worm with a head and 3 body segments.
- Goal: The agents must move its body toward the goal direction.
- Agents: The environment contains 10 agents with same Behavior Parameters.
- Agent Reward Function (independent): The reward function is now geometric meaning the reward each step is a product of all the rewards instead of a sum, this helps the agent try to maximize all rewards instead of the easiest rewards.
  - Body velocity matches goal velocity. (normalized between (0,1))
  - Body direction alignment with goal direction. (normalized between (0,1))
- Behavior Parameters:
  - Vector Observation space: 64 variables corresponding to position, rotation, velocity, and angular velocities of each limb plus the acceleration and angular acceleration of the body.
  - Actions: 9 continuous actions, corresponding to target rotations for joints.
  - Visual Observations: None
- Float Properties: None
- Benchmark Mean Reward: 800

## Food Collector

![Collector](images/foodCollector.png)

- Set-up: A multi-agent environment where agents compete to collect food.
- Goal: The agents must learn to collect as many green food spheres as possible while avoiding red spheres.
- Agents: The environment contains 5 agents with same Behavior Parameters.
- Agent Reward Function (independent):
  - +1 for interaction with green spheres
  - -1 for interaction with red spheres
- Behavior Parameters:
  - Vector Observation space: 53 corresponding to velocity of agent (2), whether agent is frozen and/or shot its laser (2), plus grid based perception of objects around agent's forward direction (40 by 40 with 6 different categories).
  - Actions:
    - 3 continuous actions correspond to Forward Motion, Side Motion and Rotation
    - 1 discrete action branch for Laser with 2 possible actions corresponding to Shoot Laser or No Action
  - Visual Observations (Optional): First-person camera per-agent, plus one vector flag representing the frozen state of the agent. This scene uses a combination of vector and visual observations and the training will not succeed without the frozen vector flag. Use `VisualFoodCollector` scene.
- Float Properties: Two
  - laser_length: Length of the laser used by the agent
    - Default: 1
    - Recommended Minimum: 0.2
    - Recommended Maximum: 7
  - agent_scale: Specifies the scale of the agent in the 3 dimensions (equal across the three dimensions)
    - Default: 1
    - Recommended Minimum: 0.5
    - Recommended Maximum: 5
- Benchmark Mean Reward: 10

## Hallway

![Hallway](images/hallway.png)

- Set-up: Environment where the agent needs to find information in a room, remember it, and use it to move to the correct goal.
- Goal: Move to the goal which corresponds to the color of the block in the room.
- Agents: The environment contains one agent.
- Agent Reward Function (independent):
  - +1 For moving to correct goal.
  - -0.1 For moving to incorrect goal.
  - -0.0003 Existential penalty.
- Behavior Parameters:
  - Vector Observation space: 30 corresponding to local ray-casts detecting objects, goals, and walls.
  - Actions: 1 discrete action Branch, with 4 actions corresponding to agent rotation and forward/backward movement.
- Float Properties: None
- Benchmark Mean Reward: 0.7
  - To train this environment, you can enable curiosity by adding the `curiosity` reward signal in `config/ppo/Hallway.yaml`

## Soccer Twos

![SoccerTwos](images/soccer.png)

- Set-up: Environment where four agents compete in a 2 vs 2 toy soccer game.
- Goal:
  - Get the ball into the opponent's goal while preventing the ball from entering own goal.
- Agents: The environment contains two different Multi Agent Groups with two agents in each. Parameters : SoccerTwos.
- Agent Reward Function (dependent):
  - (1 - `accumulated time penalty`) When ball enters opponent's goal `accumulated time penalty` is incremented by (1 / `MaxStep`) every fixed update and is reset to 0 at the beginning of an episode.
  - -1 When ball enters team's goal.
- Behavior Parameters:
  - Vector Observation space: 336 corresponding to 11 ray-casts forward distributed over 120 degrees and 3 ray-casts backward distributed over 90 degrees each detecting 6 possible object types, along with the object's distance. The forward ray-casts contribute 264 state dimensions and backward 72 state dimensions over three observation stacks.
  - Actions: 3 discrete branched actions corresponding to forward, backward, sideways movement, as well as rotation.
  - Visual Observations: None
- Float Properties: Two
  - ball_scale: Specifies the scale of the ball in the 3 dimensions (equal across the three dimensions)
    - Default: 7.5
    - Recommended minimum: 4
    - Recommended maximum: 10
  - gravity: Magnitude of the gravity
    - Default: 9.81
    - Recommended minimum: 6
    - Recommended maximum: 20

## Strikers Vs. Goalie

![StrikersVsGoalie](images/strikersvsgoalie.png)

- Set-up: Environment where two agents compete in a 2 vs 1 soccer variant.
- Goal:
  - Striker: Get the ball into the opponent's goal.
  - Goalie: Keep the ball out of the goal.
- Agents: The environment contains two different Multi Agent Groups. One with two Strikers and the other one Goalie. Behavior Parameters : Striker, Goalie.
- Striker Agent Reward Function (dependent):
  - +1 When ball enters opponent's goal.
  - -0.001 Existential penalty.
- Goalie Agent Reward Function (dependent):
  - -1 When ball enters goal.
  - 0.001 Existential bonus.
- Behavior Parameters:
  - Striker Vector Observation space: 294 corresponding to 11 ray-casts forward distributed over 120 degrees and 3 ray-casts backward distributed over 90 degrees each detecting 5 possible object types, along with the object's distance. The forward ray-casts contribute 231 state dimensions and backward 63 state dimensions over three observation stacks.
  - Striker Actions: 3 discrete branched actions corresponding to forward, backward, sideways movement, as well as rotation.
  - Goalie Vector Observation space: 738 corresponding to 41 ray-casts distributed over 360 degrees each detecting 4 possible object types, along with the object's distance and 3 observation stacks.
  - Goalie Actions: 3 discrete branched actions corresponding to forward, backward, sideways movement, as well as rotation.
  - Visual Observations: None
- Float Properties: Two
  - ball_scale: Specifies the scale of the ball in the 3 dimensions (equal across the three dimensions)
    - Default: 7.5
    - Recommended minimum: 4
    - Recommended maximum: 10
  - gravity: Magnitude of the gravity
    - Default: 9.81
    - Recommended minimum: 6
    - Recommended maximum: 20

## Walker

![Walker](images/walker.png)

- Set-up: Physics-based Humanoid agents with 26 degrees of freedom. These DOFs correspond to articulation of the following body-parts: hips, chest, spine, head, thighs, shins, feet, arms, forearms and hands.
- Goal: The agents must move its body toward the goal direction without falling.
- Agents: The environment contains 10 independent agents with same Behavior Parameters.
- Agent Reward Function (independent): The reward function is now geometric meaning the reward each step is a product of all the rewards instead of a sum, this helps the agent try to maximize all rewards instead of the easiest rewards.
  - Body velocity matches goal velocity. (normalized between (0,1))
  - Head direction alignment with goal direction. (normalized between (0,1))
- Behavior Parameters:
  - Vector Observation space: 243 variables corresponding to position, rotation, velocity, and angular velocities of each limb, along with goal direction.
  - Actions: 39 continuous actions, corresponding to target rotations and strength applicable to the joints.
  - Visual Observations: None
- Float Properties: Four
  - gravity: Magnitude of gravity
    - Default: 9.81
    - Recommended Minimum:
    - Recommended Maximum:
  - hip_mass: Mass of the hip component of the walker
    - Default: 8
    - Recommended Minimum: 7
    - Recommended Maximum: 28
  - chest_mass: Mass of the chest component of the walker
    - Default: 8
    - Recommended Minimum: 3
    - Recommended Maximum: 20
  - spine_mass: Mass of the spine component of the walker
    - Default: 8
    - Recommended Minimum: 3
    - Recommended Maximum: 20
- Benchmark Mean Reward : 2500


## Pyramids

![Pyramids](images/pyramids.png)

- Set-up: Environment where the agent needs to press a button to spawn a pyramid, then navigate to the pyramid, knock it over, and move to the gold brick at the top.
- Goal: Move to the golden brick on top of the spawned pyramid.
- Agents: The environment contains one agent.
- Agent Reward Function (independent):
  - +2 For moving to golden brick (minus 0.001 per step).
- Behavior Parameters:
  - Vector Observation space: 148 corresponding to local ray-casts detecting switch, bricks, golden brick, and walls, plus variable indicating switch state.
  - Actions: 1 discrete action branch, with 4 actions corresponding to agent rotation and forward/backward movement.
- Float Properties: None
- Benchmark Mean Reward: 1.75

## Match 3
![Match 3](images/match3.png)

- Set-up: Simple match-3 game. Matched pieces are removed, and remaining pieces drop down. New pieces are spawned randomly at the top, with a chance of being "special".
- Goal: Maximize score from matching pieces.
- Agents: The environment contains several independent Agents.
- Agent Reward Function (independent):
  - .01 for each normal piece cleared. Special pieces are worth 2x or 3x.
- Behavior Parameters:
  - None
  - Observations and actions are defined with a sensor and actuator respectively.
- Float Properties: None
- Benchmark Mean Reward:
  - 39.5 for visual observations
  - 38.5 for vector observations
  - 34.2 for simple heuristic (pick a random valid move)
  - 37.0 for greedy heuristic (pick the highest-scoring valid move)

## Sorter
![Sorter](images/sorter.png)

 - Set-up: The Agent is in a circular room with numbered tiles. The values of the tiles are random between 1 and 20. The tiles present in the room are randomized at each episode. When the Agent visits a tile, it turns green.
 - Goal: Visit all the tiles in ascending order.
 - Agents: The environment contains a single Agent
 - Agent Reward Function:
  - -.0002 Existential penalty.
  - +1 For visiting the right tile
  - -1 For visiting the wrong tile
 - BehaviorParameters:
  - Vector Observations : 4 : 2 floats for Position and 2 floats for orientation
  - Variable Length Observations : Between 1 and 20 entities (one for each tile) each with 22 observations, the first 20 are one hot encoding of the value of the tile, the 21st and 22nd represent the position of the tile relative to the Agent and the 23rd is `1` if the tile was visited and `0` otherwise.
  - Actions: 3 discrete branched actions corresponding to forward, backward, sideways movement, as well as rotation.
  - Float Properties: One
    - num_tiles: The maximum number of tiles to sample.
      - Default: 2
      - Recommended Minimum: 1
      - Recommended Maximum: 20
  - Benchmark Mean Reward: Depends on the number of tiles.

## Cooperative Push Block
![CoopPushBlock](images/cooperative_pushblock.png)

- Set-up: Similar to Push Block, the agents are in an area with blocks that need to be pushed into a goal. Small blocks can be pushed by one agent and are worth +1 value, medium blocks require two agents to push in and are worth +2, and large blocks require all 3 agents to push and are worth +3.
- Goal: Push all blocks into the goal.
- Agents: The environment contains three Agents in a Multi Agent Group.
- Agent Reward Function:
  - -0.0001 Existential penalty, as a group reward.
  - +1, +2, or +3 for pushing in a block, added as a group reward.
- Behavior Parameters:
  - Observation space: A single Grid Sensor with separate tags for each block size, the goal, the walls, and other agents.
  - Actions: 1 discrete action branch with 7 actions, corresponding to turn clockwise and counterclockwise, move along four different face directions, or do nothing.
- Float Properties: None
- Benchmark Mean Reward: 11 (Group Reward)

## Dungeon Escape
![DungeonEscape](images/dungeon_escape.png)

- Set-up: Agents are trapped in a dungeon with a dragon, and must work together to escape. To retrieve the key, one of the agents must find and slay the dragon, sacrificing itself to do so. The dragon will drop a key for the others to use. The other agents can then pick up this key and unlock the dungeon door. If the agents take too long, the dragon will escape through a portal and the environment resets.
- Goal: Unlock the dungeon door and leave.
- Agents: The environment contains three Agents in a Multi Agent Group and one Dragon, which moves in a predetermined pattern.
- Agent Reward Function:
  - +1 group reward if any agent successfully unlocks the door and leaves the dungeon.
- Behavior Parameters:
  - Observation space: A Ray Perception Sensor with separate tags for the walls, other agents, the door, key, the dragon, and the dragon's portal. A single Vector Observation which indicates whether the agent is holding a key.
  - Actions: 1 discrete action branch with 7 actions, corresponding to turn clockwise and counterclockwise, move along four different face directions, or do nothing.
- Float Properties: None
- Benchmark Mean Reward: 1.0 (Group Reward)


## Links discovered
- [Making a New Learning Environment](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Create-New.md)
- [contribution guidelines](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/CONTRIBUTING.md)
- [Basic](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/basic.png)
- [3D Balance Ball](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/balance.png)
- [GridWorld](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/gridworld.png)
- [action masking](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Design-Agents.md#masking-discrete-actions)
- [Push](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/push.png)
- [Wall](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/wall.png)
- [Crawler](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/crawler.png)
- [Worm](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/worm.png)
- [Collector](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/foodCollector.png)
- [Hallway](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/hallway.png)
- [SoccerTwos](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/soccer.png)
- [StrikersVsGoalie](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/strikersvsgoalie.png)
- [Walker](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/walker.png)
- [Pyramids](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/pyramids.png)
- [Match 3](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/match3.png)
- [Sorter](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/sorter.png)
- [CoopPushBlock](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/cooperative_pushblock.png)
- [DungeonEscape](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/dungeon_escape.png)

--- com.unity.ml-agents/Documentation~/Tutorial-Colab.md ---
# Python Tutorial with Google Colab

Interactive tutorials for using ML-Agents with Google Colab environments.

| **Tutorial**                                                                                                                                                                           | **Description**                                                    |
|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------|
| [Using a UnityEnvironment](https://colab.research.google.com/github/Unity-Technologies/ml-agents/blob/release/4.0.0/colab/Colab_UnityEnvironment_1_Run.ipynb)                          | Learn how to set up and interact with Unity environments in Colab. |
| [Q-Learning with a UnityEnvironment](https://colab.research.google.com/github/Unity-Technologies/ml-agents/blob/release/4.0.0/colab/Colab_UnityEnvironment_2_Train.ipynb)              | Implement Q-Learning algorithms with Unity ML-Agents in Colab.     |
| [Using Side Channels on a UnityEnvironment](https://colab.research.google.com/github/Unity-Technologies/ml-agents/blob/release/4.0.0/colab/Colab_UnityEnvironment_3_SideChannel.ipynb) | Explore side channel communication between Unity and Python.       |


## Links discovered
- [Using a UnityEnvironment](https://colab.research.google.com/github/Unity-Technologies/ml-agents/blob/release/4.0.0/colab/Colab_UnityEnvironment_1_Run.ipynb)
- [Q-Learning with a UnityEnvironment](https://colab.research.google.com/github/Unity-Technologies/ml-agents/blob/release/4.0.0/colab/Colab_UnityEnvironment_2_Train.ipynb)
- [Using Side Channels on a UnityEnvironment](https://colab.research.google.com/github/Unity-Technologies/ml-agents/blob/release/4.0.0/colab/Colab_UnityEnvironment_3_SideChannel.ipynb)

--- com.unity.ml-agents/Documentation~/Tutorial-Custom-Trainer-Plugin.md ---
# Custom Trainer Plugin

## How to write a custom trainer plugin

### Step 1: Write your custom trainer class
Before you start writing your code, make sure to use your favorite environment management tool(e.g. `venv` or `conda`) to create and activate a Python virtual environment. The following command uses `conda`, but other tools work similarly:
```shell
conda create -n trainer-env python=3.10.12
conda activate trainer-env
```

Users of the plug-in system are responsible for implementing the trainer class subject to the API standard. Let us follow an example by implementing a custom trainer named "YourCustomTrainer". You can either extend `OnPolicyTrainer` or `OffPolicyTrainer` classes depending on the training strategies you choose.

Please refer to the internal [PPO implementation](https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/ml-agents/mlagents/trainers/ppo/trainer.py) for a complete code example. We will not provide a workable code in the document. The purpose of the tutorial is to introduce you to the core components and interfaces of our plugin framework. We use code snippets and patterns to demonstrate the control and data flow.

Your custom trainers are responsible for collecting experiences and training the models. Your custom trainer class acts like a coordinator to the policy and optimizer. To start implementing methods in the class, create a policy class objects from method `create_policy`:


```python
def create_policy(
    self, parsed_behavior_id: BehaviorIdentifiers, behavior_spec: BehaviorSpec
) -> TorchPolicy:

    actor_cls: Union[Type[SimpleActor], Type[SharedActorCritic]] = SimpleActor
    actor_kwargs: Dict[str, Any] = {
        "conditional_sigma": False,
        "tanh_squash": False,
    }
    if self.shared_critic:
        reward_signal_configs = self.trainer_settings.reward_signals
        reward_signal_names = [
            key.value for key, _ in reward_signal_configs.items()
        ]
        actor_cls = SharedActorCritic
        actor_kwargs.update({"stream_names": reward_signal_names})

    policy = TorchPolicy(
        self.seed,
        behavior_spec,
        self.trainer_settings.network_settings,
        actor_cls,
        actor_kwargs,
    )
    return policy

```

Depending on whether you use shared or separate network architecture for your policy, we provide `SimpleActor` and `SharedActorCritic` from `mlagents.trainers.torch_entities.networks` that you can choose from. In our example above, we use a `SimpleActor`.

Next, create an optimizer class object from `create_optimizer` method and connect it to the policy object you created above:


```python
def create_optimizer(self) -> TorchOptimizer:
    return TorchPPOOptimizer(  # type: ignore
        cast(TorchPolicy, self.policy), self.trainer_settings  # type: ignore
    )  # type: ignore

```

There are a couple of abstract methods(`_process_trajectory` and `_update_policy`) inherited from `RLTrainer` that you need to implement in your custom trainer class. `_process_trajectory` takes a trajectory and processes it, putting it into the update buffer. Processing involves calculating value and advantage targets for the model updating step. Given input `trajectory: Trajectory`, users are responsible for processing the data in the trajectory and append `agent_buffer_trajectory` to the back of the update buffer by calling `self._append_to_update_buffer(agent_buffer_trajectory)`, whose output will be used in updating the model in `optimizer` class.

A typical `_process_trajectory` function(incomplete) will convert a trajectory object to an agent buffer then get all value estimates from the trajectory by calling `self.optimizer.get_trajectory_value_estimates`. From the returned dictionary of value estimates we extract reward signals keyed by their names:

```python
def _process_trajectory(self, trajectory: Trajectory) -> None:
    super()._process_trajectory(trajectory)
    agent_id = trajectory.agent_id  # All the agents should have the same ID

    agent_buffer_trajectory = trajectory.to_agentbuffer()

    # Get all value estimates
    (
        value_estimates,
        value_next,
        value_memories,
    ) =  self.optimizer.get_trajectory_value_estimates(
        agent_buffer_trajectory,
        trajectory.next_obs,
        trajectory.done_reached and not trajectory.interrupted,
    )

    for name, v in value_estimates.items():
        agent_buffer_trajectory[RewardSignalUtil.value_estimates_key(name)].extend(
            v
        )
        self._stats_reporter.add_stat(
            f"Policy/{self.optimizer.reward_signals[name].name.capitalize()} Value Estimate",
            np.mean(v),
        )

    # Evaluate all reward functions
    self.collected_rewards["environment"][agent_id] += np.sum(
        agent_buffer_trajectory[BufferKey.ENVIRONMENT_REWARDS]
    )
    for name, reward_signal in self.optimizer.reward_signals.items():
        evaluate_result = (
            reward_signal.evaluate(agent_buffer_trajectory) * reward_signal.strength
        )
        agent_buffer_trajectory[RewardSignalUtil.rewards_key(name)].extend(
            evaluate_result
        )
        # Report the reward signals
        self.collected_rewards[name][agent_id] += np.sum(evaluate_result)

    self._append_to_update_buffer(agent_buffer_trajectory)

```

A trajectory will be a list of dictionaries of strings mapped to `Anything`. When calling `forward` on a policy, the argument will include an “experience” dictionary from the last step. The `forward` method will generate an action and the next “experience” dictionary. Examples of fields in the “experience” dictionary include observation, action, reward, done status, group_reward, LSTM memory state, etc.



### Step 2: implement your custom optimizer for the trainer.
We will show you an example we implemented - `class TorchPPOOptimizer(TorchOptimizer)`, which takes a Policy and a Dict of trainer parameters and creates an Optimizer that connects to the policy. Your optimizer should include a value estimator and a loss function in the `update` method.

Before writing your optimizer class, first define setting class `class PPOSettings(OnPolicyHyperparamSettings)` for your custom optimizer:



```python
class PPOSettings(OnPolicyHyperparamSettings):
    beta: float = 5.0e-3
    epsilon: float = 0.2
    lambd: float = 0.95
    num_epoch: int = 3
    shared_critic: bool = False
    learning_rate_schedule: ScheduleType = ScheduleType.LINEAR
    beta_schedule: ScheduleType = ScheduleType.LINEAR
    epsilon_schedule: ScheduleType = ScheduleType.LINEAR

```

You should implement `update` function following interface:


```python
def update(self, batch: AgentBuffer, num_sequences: int) -> Dict[str, float]:

```

In which losses and other metrics are calculated from an `AgentBuffer` that is generated from your trainer class, depending on which model you choose to implement the loss functions will be different. In our case we calculate value loss from critic and trust region policy loss. A typical pattern(incomplete) of the calculations will look like the following:


```python
run_out = self.policy.actor.get_stats(
    current_obs,
    actions,
    masks=act_masks,
    memories=memories,
    sequence_length=self.policy.sequence_length,
)

log_probs = run_out["log_probs"]
entropy = run_out["entropy"]

values, _ = self.critic.critic_pass(
    current_obs,
    memories=value_memories,
    sequence_length=self.policy.sequence_length,
)
policy_loss = ModelUtils.trust_region_policy_loss(
    ModelUtils.list_to_tensor(batch[BufferKey.ADVANTAGES]),
    log_probs,
    old_log_probs,
    loss_masks,
    decay_eps,
)
loss = (
    policy_loss
    + 0.5 * value_loss
    - decay_bet * ModelUtils.masked_mean(entropy, loss_masks)
)

```

Finally update the model and return the a dictionary including calculated losses and updated decay learning rate:


```python
ModelUtils.update_learning_rate(self.optimizer, decay_lr)
self.optimizer.zero_grad()
loss.backward()

self.optimizer.step()
update_stats = {
    "Losses/Policy Loss": torch.abs(policy_loss).item(),
    "Losses/Value Loss": value_loss.item(),
    "Policy/Learning Rate": decay_lr,
    "Policy/Epsilon": decay_eps,
    "Policy/Beta": decay_bet,
}

```

### Step 3: Integrate your custom trainer into the plugin system

By integrating a custom trainer into the plugin system, a user can use their published packages which have their implementations. To do that, you need to add a setup.py file. In the call to setup(), you'll need to add to the entry_points dictionary for each plugin interface that you implement. The form of this is {entry point name}={plugin module}:{plugin function}. For example:



```python
entry_points={
        ML_AGENTS_TRAINER_TYPE: [
            "your_trainer_type=your_package.your_custom_trainer:get_type_and_setting"
        ]
    },
```

Some key elements in the code:

```
ML_AGENTS_TRAINER_TYPE: a string constant for trainer type
your_trainer_type: name your trainer type, used in configuration file
your_package: your pip installable package containing custom trainer implementation
```

Also define `get_type_and_setting` method in `YourCustomTrainer` class:


```python
def get_type_and_setting():
    return {YourCustomTrainer.get_trainer_name(): YourCustomTrainer}, {
        YourCustomTrainer.get_trainer_name(): YourCustomSetting
    }

```

Finally, specify trainer type in the config file:


```python
behaviors:
  3DBall:
    trainer_type: your_trainer_type
...
```

### Step 4: Install your custom trainer and run training:
Before installing your custom trainer package, make sure you have `ml-agents-env` and `ml-agents` installed

```shell
pip3 install -e ./ml-agents-envs && pip3 install -e ./ml-agents
```

Install your custom trainer package(if your package is pip installable):
```shell
pip3 install your_custom_package
```
Or follow our internal implementations:
```shell
pip3 install -e ./ml-agents-trainer-plugin
```

Following the previous installations your package is added as an entrypoint and you can use a config file with new trainers:
```shell
mlagents-learn ml-agents-trainer-plugin/mlagents_trainer_plugin/a2c/a2c_3DBall.yaml --run-id <run-id-name>
--env <env-executable>
```

### Validate your implementations:
Create a clean Python environment with Python 3.10.12 and activate it before you start, if you haven't done so already:
```shell
conda create -n trainer-env python=3.10.12
conda activate trainer-env
```

Make sure you follow previous steps and install all required packages. We are testing internal implementations in this tutorial, but ML-Agents users can run similar validations once they have their own implementations installed:
```shell
pip3 install -e ./ml-agents-envs && pip3 install -e ./ml-agents
pip3 install -e ./ml-agents-trainer-plugin
```
Once your package is added as an `entrypoint`, you can add to the config file the new trainer type. Check if trainer type is specified in the config file `a2c_3DBall.yaml`:
```
trainer_type: a2c
```

Test if custom trainer package is installed by running:
```shell
mlagents-learn ml-agents-trainer-plugin/mlagents_trainer_plugin/a2c/a2c_3DBall.yaml --run-id test-trainer
```

You can also list all trainers installed in the registry. Type `python` in your shell to open a REPL session. Run the python code below, you should be able to see all trainer types currently installed:
```python
>>> import pkg_resources
>>> for entry in pkg_resources.iter_entry_points('mlagents.trainer_type'):
...     print(entry)
...
default = mlagents.plugins.trainer_type:get_default_trainer_types
a2c = mlagents_trainer_plugin.a2c.a2c_trainer:get_type_and_setting
dqn = mlagents_trainer_plugin.dqn.dqn_trainer:get_type_and_setting
```

If it is properly installed, you will see Unity logo and message indicating training will start:
```
[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.
```

If you see the following error message, it could be due to trainer type is wrong or the trainer type specified is not installed:
```shell
mlagents.trainers.exception.TrainerConfigError: Invalid trainer type a2c was found
```



## Links discovered
- [PPO implementation](https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/ml-agents/mlagents/trainers/ppo/trainer.py)

--- ml-agents/mlagents/trainers/demo_loader.py ---
import os
from typing import List, Tuple
import numpy as np
from mlagents.trainers.buffer import AgentBuffer, BufferKey
from mlagents_envs.communicator_objects.agent_info_action_pair_pb2 import (
    AgentInfoActionPairProto,
)
from mlagents.trainers.trajectory import ObsUtil
from mlagents_envs.rpc_utils import behavior_spec_from_proto, steps_from_proto
from mlagents_envs.base_env import BehaviorSpec
from mlagents_envs.communicator_objects.brain_parameters_pb2 import BrainParametersProto
from mlagents_envs.communicator_objects.demonstration_meta_pb2 import (
    DemonstrationMetaProto,
)
from mlagents_envs.timers import timed, hierarchical_timer
from google.protobuf.internal.decoder import _DecodeVarint32  # type: ignore
from google.protobuf.internal.encoder import _EncodeVarint  # type: ignore


INITIAL_POS = 33
SUPPORTED_DEMONSTRATION_VERSIONS = frozenset([0, 1])


@timed
def make_demo_buffer(
    pair_infos: List[AgentInfoActionPairProto],
    behavior_spec: BehaviorSpec,
    sequence_length: int,
) -> AgentBuffer:
    # Create and populate buffer using experiences
    demo_raw_buffer = AgentBuffer()
    demo_processed_buffer = AgentBuffer()
    for idx, current_pair_info in enumerate(pair_infos):
        if idx > len(pair_infos) - 2:
            break
        next_pair_info = pair_infos[idx + 1]
        current_decision_step, current_terminal_step = steps_from_proto(
            [current_pair_info.agent_info], behavior_spec
        )
        next_decision_step, next_terminal_step = steps_from_proto(
            [next_pair_info.agent_info], behavior_spec
        )
        previous_action = (
            np.array(
                pair_infos[idx].action_info.vector_actions_deprecated, dtype=np.float32
            )
            * 0
        )
        if idx > 0:
            previous_action = np.array(
                pair_infos[idx - 1].action_info.vector_actions_deprecated,
                dtype=np.float32,
            )

        next_done = len(next_terminal_step) == 1
        next_reward = 0
        if len(next_terminal_step) == 1:
            next_reward = next_terminal_step.reward[0]
        else:
            next_reward = next_decision_step.reward[0]
        current_obs = None
        if len(current_terminal_step) == 1:
            current_obs = list(current_terminal_step.values())[0].obs
        else:
            current_obs = list(current_decision_step.values())[0].obs

        demo_raw_buffer[BufferKey.DONE].append(next_done)
        demo_raw_buffer[BufferKey.ENVIRONMENT_REWARDS].append(next_reward)
        for i, obs in enumerate(current_obs):
            demo_raw_buffer[ObsUtil.get_name_at(i)].append(obs)
        if (
            len(current_pair_info.action_info.continuous_actions) == 0
            and len(current_pair_info.action_info.discrete_actions) == 0
        ):
            if behavior_spec.action_spec.continuous_size > 0:
                demo_raw_buffer[BufferKey.CONTINUOUS_ACTION].append(
                    current_pair_info.action_info.vector_actions_deprecated
                )
            else:
                demo_raw_buffer[BufferKey.DISCRETE_ACTION].append(
                    current_pair_info.action_info.vector_actions_deprecated
                )
        else:
            if behavior_spec.action_spec.continuous_size > 0:
                demo_raw_buffer[BufferKey.CONTINUOUS_ACTION].append(
                    current_pair_info.action_info.continuous_actions
                )
            if behavior_spec.action_spec.discrete_size > 0:
                demo_raw_buffer[BufferKey.DISCRETE_ACTION].append(
                    current_pair_info.action_info.discrete_actions
                )
        demo_raw_buffer[BufferKey.PREV_ACTION].append(previous_action)
        if next_done:
            demo_raw_buffer.resequence_and_append(
                demo_processed_buffer, batch_size=None, training_length=sequence_length
            )
            demo_raw_buffer.reset_agent()
    demo_raw_buffer.resequence_and_append(
        demo_processed_buffer, batch_size=None, training_length=sequence_length
    )
    return demo_processed_buffer


@timed
def demo_to_buffer(
    file_path: str, sequence_length: int, expected_behavior_spec: BehaviorSpec = None
) -> Tuple[BehaviorSpec, AgentBuffer]:
    """
    Loads demonstration file and uses it to fill training buffer.
    :param file_path: Location of demonstration file (.demo).
    :param sequence_length: Length of trajectories to fill buffer.
    :return:
    """
    behavior_spec, info_action_pair, _ = load_demonstration(file_path)
    demo_buffer = make_demo_buffer(info_action_pair, behavior_spec, sequence_length)
    if expected_behavior_spec:
        # check action dimensions in demonstration match
        if behavior_spec.action_spec != expected_behavior_spec.action_spec:
            raise RuntimeError(
                "The actions {} in demonstration do not match the policy's {}.".format(
                    behavior_spec.action_spec, expected_behavior_spec.action_spec
                )
            )
        # check observations match
        if len(behavior_spec.observation_specs) != len(
            expected_behavior_spec.observation_specs
        ):
            raise RuntimeError(
                "The demonstrations do not have the same number of observations as the policy."
            )
        else:
            for i, (demo_obs, policy_obs) in enumerate(
                zip(
                    behavior_spec.observation_specs,
                    expected_behavior_spec.observation_specs,
                )
            ):
                if demo_obs.shape != policy_obs.shape:
                    raise RuntimeError(
                        f"The shape {demo_obs} for observation {i} in demonstration \
                        do not match the policy's {policy_obs}."
                    )
    return behavior_spec, demo_buffer


def get_demo_files(path: str) -> List[str]:
    """
    Retrieves the demonstration file(s) from a path.
    :param path: Path of demonstration file or directory.
    :return: List of demonstration files

    Raises errors if |path| is invalid.
    """
    if os.path.isfile(path):
        if not path.endswith(".demo"):
            raise ValueError("The path provided is not a '.demo' file.")
        return [path]
    elif os.path.isdir(path):
        paths = [
            os.path.join(path, name)
            for name in os.listdir(path)
            if name.endswith(".demo")
        ]
        if not paths:
            raise ValueError("There are no '.demo' files in the provided directory.")
        return paths
    else:
        raise FileNotFoundError(
            f"The demonstration file or directory {path} does not exist."
        )


@timed
def load_demonstration(
    file_path: str,
) -> Tuple[BehaviorSpec, List[AgentInfoActionPairProto], int]:
    """
    Loads and parses a demonstration file.
    :param file_path: Location of demonstration file (.demo).
    :return: BrainParameter and list of AgentInfoActionPairProto containing demonstration data.
    """

    # First 32 bytes of file dedicated to meta-data.
    file_paths = get_demo_files(file_path)
    behavior_spec = None
    brain_param_proto = None
    info_action_pairs = []
    total_expected = 0
    for _file_path in file_paths:
        with open(_file_path, "rb") as fp:
            with hierarchical_timer("read_file"):
                data = fp.read()
            next_pos, pos, obs_decoded = 0, 0, 0
            while pos < len(data):
                next_pos, pos = _DecodeVarint32(data, pos)
                if obs_decoded == 0:
                    meta_data_proto = DemonstrationMetaProto()
                    meta_data_proto.ParseFromString(data[pos : pos + next_pos])
                    if (
                        meta_data_proto.api_version
                        not in SUPPORTED_DEMONSTRATION_VERSIONS
                    ):
                        raise RuntimeError(
                            f"Can't load Demonstration data from an unsupported version ({meta_data_proto.api_version})"
                        )
                    total_expected += meta_data_proto.number_steps
                    pos = INITIAL_POS
                if obs_decoded == 1:
                    brain_param_proto = BrainParametersProto()
                    brain_param_proto.ParseFromString(data[pos : pos + next_pos])
                    pos += next_pos
                if obs_decoded > 1:
                    agent_info_action = AgentInfoActionPairProto()
                    agent_info_action.ParseFromString(data[pos : pos + next_pos])
                    if behavior_spec is None:
                        behavior_spec = behavior_spec_from_proto(
                            brain_param_proto, agent_info_action.agent_info
                        )
                    info_action_pairs.append(agent_info_action)
                    if len(info_action_pairs) == total_expected:
                        break
                    pos += next_pos
                obs_decoded += 1
    if not behavior_spec:
        raise RuntimeError(
            f"No BrainParameters found in demonstration file at {file_path}."
        )
    return behavior_spec, info_action_pairs, total_expected


def write_delimited(f, message):
    msg_string = message.SerializeToString()
    msg_size = len(msg_string)
    _EncodeVarint(f.write, msg_size)
    f.write(msg_string)


def write_demo(demo_path, meta_data_proto, brain_param_proto, agent_info_protos):
    with open(demo_path, "wb") as f:
        # write metadata
        write_delimited(f, meta_data_proto)
        f.seek(INITIAL_POS)
        write_delimited(f, brain_param_proto)

        for agent in agent_info_protos:
            write_delimited(f, agent)


--- ml-agents-envs/mlagents_envs/communicator_objects/demonstration_meta_pb2.py ---
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: mlagents_envs/communicator_objects/demonstration_meta.proto

import sys
_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode('latin1'))
from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
from google.protobuf import descriptor_pb2
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor.FileDescriptor(
  name='mlagents_envs/communicator_objects/demonstration_meta.proto',
  package='communicator_objects',
  syntax='proto3',
  serialized_pb=_b('\n;mlagents_envs/communicator_objects/demonstration_meta.proto\x12\x14\x63ommunicator_objects\"\x8d\x01\n\x16\x44\x65monstrationMetaProto\x12\x13\n\x0b\x61pi_version\x18\x01 \x01(\x05\x12\x1a\n\x12\x64\x65monstration_name\x18\x02 \x01(\t\x12\x14\n\x0cnumber_steps\x18\x03 \x01(\x05\x12\x17\n\x0fnumber_episodes\x18\x04 \x01(\x05\x12\x13\n\x0bmean_reward\x18\x05 \x01(\x02\x42%\xaa\x02\"Unity.MLAgents.CommunicatorObjectsb\x06proto3')
)




_DEMONSTRATIONMETAPROTO = _descriptor.Descriptor(
  name='DemonstrationMetaProto',
  full_name='communicator_objects.DemonstrationMetaProto',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='api_version', full_name='communicator_objects.DemonstrationMetaProto.api_version', index=0,
      number=1, type=5, cpp_type=1, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='demonstration_name', full_name='communicator_objects.DemonstrationMetaProto.demonstration_name', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='number_steps', full_name='communicator_objects.DemonstrationMetaProto.number_steps', index=2,
      number=3, type=5, cpp_type=1, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='number_episodes', full_name='communicator_objects.DemonstrationMetaProto.number_episodes', index=3,
      number=4, type=5, cpp_type=1, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='mean_reward', full_name='communicator_objects.DemonstrationMetaProto.mean_reward', index=4,
      number=5, type=2, cpp_type=6, label=1,
      has_default_value=False, default_value=float(0),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=86,
  serialized_end=227,
)

DESCRIPTOR.message_types_by_name['DemonstrationMetaProto'] = _DEMONSTRATIONMETAPROTO
_sym_db.RegisterFileDescriptor(DESCRIPTOR)

DemonstrationMetaProto = _reflection.GeneratedProtocolMessageType('DemonstrationMetaProto', (_message.Message,), dict(
  DESCRIPTOR = _DEMONSTRATIONMETAPROTO,
  __module__ = 'mlagents_envs.communicator_objects.demonstration_meta_pb2'
  # @@protoc_insertion_point(class_scope:communicator_objects.DemonstrationMetaProto)
  ))
_sym_db.RegisterMessage(DemonstrationMetaProto)


DESCRIPTOR.has_options = True
DESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b('\252\002\"Unity.MLAgents.CommunicatorObjects'))
# @@protoc_insertion_point(module_scope)


--- ml-agents-plugin-examples/mlagents_plugin_examples/tests/__init__.py ---


--- com.unity.ml-agents/Documentation~/Python-APIs.md ---
# Python APIs

The Python APIs allow you to control and interact with Unity environments from Python scripts. Each API is designed for specific use cases and offers different levels of abstraction and functionality.


| **API**                                                                              | **Description**                                                     |
|--------------------------------------------------------------------------------------|---------------------------------------------------------------------|
| [Python Gym API](Python-Gym-API.md)                                                  | OpenAI Gym-compatible interface for standard RL workflows.          |
| [Python Gym API Documentation](Python-Gym-API-Documentation.md)                      | Detailed documentation for the Python Gym API.                      |
| [Python PettingZoo API](Python-PettingZoo-API.md)                                    | Multi-agent environment interface compatible with PettingZoo.       |
| [Python PettingZoo API Documentation](Python-PettingZoo-API-Documentation.md)        | Detailed documentation for the Python PettingZoo API.               |
| [Python Low-Level API](Python-LLAPI.md)                                              | Direct low-level access for custom training and advanced use cases. |
| [Python Low-Level API Documentation](Python-LLAPI-Documentation.md)                  | Detailed documentation for the Python Low-Level API.                |
| [On/Off Policy Trainer Documentation](Python-On-Off-Policy-Trainer-Documentation.md) | Documentation for on-policy and off-policy training methods.        |
| [Python Optimizer Documentation](Python-Optimizer-Documentation.md)                  | Documentation for optimizers used in training algorithms.           |


## Links discovered
- [Python Gym API](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Python-Gym-API.md)
- [Python Gym API Documentation](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Python-Gym-API-Documentation.md)
- [Python PettingZoo API](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Python-PettingZoo-API.md)
- [Python PettingZoo API Documentation](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Python-PettingZoo-API-Documentation.md)
- [Python Low-Level API](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Python-LLAPI.md)
- [Python Low-Level API Documentation](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Python-LLAPI-Documentation.md)
- [On/Off Policy Trainer Documentation](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Python-On-Off-Policy-Trainer-Documentation.md)
- [Python Optimizer Documentation](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Python-Optimizer-Documentation.md)

--- com.unity.ml-agents/Documentation~/Python-Gym-API.md ---
# Unity ML-Agents Gym Wrapper

A common way in which machine learning researchers interact with simulation environments is via a wrapper provided by OpenAI called `gym`. For more information on the gym interface, see [here](https://github.com/openai/gym).

We provide a gym wrapper and instructions for using it with existing machine learning algorithms which utilize gym. Our wrapper provides interfaces on top of our `UnityEnvironment` class, which is the default way of interfacing with a Unity environment via Python.

## Installation

The gym wrapper is part of the `mlagents_envs` package. Please refer to the [mlagents_envs installation instructions](https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/ml-agents-envs/README.md).


## Using the Gym Wrapper

The gym interface is available from `gym_unity.envs`. To launch an environment from the root of the project repository use:

```python
from mlagents_envs.envs.unity_gym_env import UnityToGymWrapper

env = UnityToGymWrapper(unity_env, uint8_visual, flatten_branched, allow_multiple_obs)
```

- `unity_env` refers to the Unity environment to be wrapped.

- `uint8_visual` refers to whether to output visual observations as `uint8` values (0-255). Many common Gym environments (e.g. Atari) do this. By default, they will be floats (0.0-1.0). Defaults to `False`.

- `flatten_branched` will flatten a branched discrete action space into a Gym Discrete. Otherwise, it will be converted into a MultiDiscrete. Defaults to `False`.

- `allow_multiple_obs` will return a list of observations. The first elements contain the visual observations and the last element contains the array of vector observations. If False the environment returns a single array (containing a single visual observations, if present, otherwise the vector observation). Defaults to `False`.

- `action_space_seed` is the optional seed for action sampling. If non-None, will be used to set the random seed on created gym.Space instances.

The returned environment `env` will function as a gym.

## Limitations

- It is only possible to use an environment with a **single** Agent.
- By default, the first visual observation is provided as the `observation`, if present. Otherwise, vector observations are provided. You can receive all visual and vector observations by using the `allow_multiple_obs=True` option in the gym parameters. If set to `True`, you will receive a list of `observation` instead of only one.
- The `TerminalSteps` or `DecisionSteps` output from the environment can still be accessed from the `info` provided by `env.step(action)`.
- Stacked vector observations are not supported.
- Environment registration for use with `gym.make()` is currently not supported.
- Calling env.render() will not render a new frame of the environment. It will return the latest visual observation if using visual observations.

## Running OpenAI Baselines Algorithms

OpenAI provides a set of open-source maintained and tested Reinforcement Learning algorithms called the [Baselines](https://github.com/openai/baselines).

Using the provided Gym wrapper, it is possible to train ML-Agents environments using these algorithms. This requires the creation of custom training scripts to launch each algorithm. In most cases these scripts can be created by making slight modifications to the ones provided for Atari and Mujoco environments.

These examples were tested with baselines version 0.1.6.

### Example - DQN Baseline

In order to train an agent to play the `GridWorld` environment using the Baselines DQN algorithm, you first need to install the baselines package using pip:

```
pip install git+git://github.com/openai/baselines
```

Next, create a file called `train_unity.py`. Then create an `/envs/` directory and build the environment to that directory. For more information on building Unity environments, see [here](Learning-Environment-Executable.md). Note that because of limitations of the DQN baseline, the environment must have a single visual observation, a single discrete action and a single Agent in the scene. Add the following code to the `train_unity.py` file:

```python
import gym

from baselines import deepq
from baselines import logger

from mlagents_envs.environment import UnityEnvironment
from mlagents_envs.envs.unity_gym_env import UnityToGymWrapper


def main():
  unity_env = UnityEnvironment( < path - to - environment >)
  env = UnityToGymWrapper(unity_env, uint8_visual=True)
  logger.configure('./logs')  # Change to log in a different directory
  act = deepq.learn(
    env,
    "cnn",  # For visual inputs
    lr=2.5e-4,
    total_timesteps=1000000,
    buffer_size=50000,
    exploration_fraction=0.05,
    exploration_final_eps=0.1,
    print_freq=20,
    train_freq=5,
    learning_starts=20000,
    target_network_update_freq=50,
    gamma=0.99,
    prioritized_replay=False,
    checkpoint_freq=1000,
    checkpoint_path='./logs',  # Change to save model in a different directory
    dueling=True
  )
  print("Saving model to unity_model.pkl")
  act.save("unity_model.pkl")


if __name__ == '__main__':
  main()
```

To start the training process, run the following from the directory containing
`train_unity.py`:

```sh
python -m train_unity
```

### Other Algorithms

Other algorithms in the Baselines repository can be run using scripts similar to the examples from the baselines package. In most cases, the primary changes needed to use a Unity environment are to import `UnityToGymWrapper`, and to replace the environment creation code, typically `gym.make()`, with a call to `UnityToGymWrapper(unity_environment)` passing the environment as input.

A typical rule of thumb is that for vision-based environments, modification should be done to Atari training scripts, and for vector observation environments, modification should be done to Mujoco scripts.

Some algorithms will make use of `make_env()` or `make_mujoco_env()` functions. You can define a similar function for Unity environments. An example of such a method using the PPO2 baseline:

```python
from mlagents_envs.environment import UnityEnvironment
from mlagents_envs.envs import UnityToGymWrapper
from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv
from baselines.common.vec_env.dummy_vec_env import DummyVecEnv
from baselines.bench import Monitor
from baselines import logger
import baselines.ppo2.ppo2 as ppo2

import os

try:
  from mpi4py import MPI
except ImportError:
  MPI = None


def make_unity_env(env_directory, num_env, visual, start_index=0):
  """
  Create a wrapped, monitored Unity environment.
  """

  def make_env(rank, use_visual=True):  # pylint: disable=C0111
    def _thunk():
      unity_env = UnityEnvironment(env_directory, base_port=5000 + rank)
      env = UnityToGymWrapper(unity_env, uint8_visual=True)
      env = Monitor(env, logger.get_dir() and os.path.join(logger.get_dir(), str(rank)))
      return env

    return _thunk

  if visual:
    return SubprocVecEnv([make_env(i + start_index) for i in range(num_env)])
  else:
    rank = MPI.COMM_WORLD.Get_rank() if MPI else 0
    return DummyVecEnv([make_env(rank, use_visual=False)])


def main():
  env = make_unity_env( < path - to - environment >, 4, True)
  ppo2.learn(
    network="mlp",
    env=env,
    total_timesteps=100000,
    lr=1e-3,
  )


if __name__ == '__main__':
  main()
```

## Run Google Dopamine Algorithms

Google provides a framework [Dopamine](https://github.com/google/dopamine), and implementations of algorithms, e.g. DQN, Rainbow, and the C51 variant of Rainbow. Using the Gym wrapper, we can run Unity environments using Dopamine.

First, after installing the Gym wrapper, clone the Dopamine repository.

```
git clone https://github.com/google/dopamine
```

Then, follow the appropriate install instructions as specified on [Dopamine's homepage](https://github.com/google/dopamine). Note that the Dopamine guide specifies using a virtualenv. If you choose to do so, make sure your unity_env package is also installed within the same virtualenv as Dopamine.

### Adapting Dopamine's Scripts

First, open `dopamine/atari/run_experiment.py`. Alternatively, copy the entire `atari` folder, and name it something else (e.g. `unity`). If you choose the  copy approach, be sure to change the package names in the import statements in `train.py` to your new directory.

Within `run_experiment.py`, we will need to make changes to which environment is instantiated, just as in the Baselines example. At the top of the file, insert

```python
from mlagents_envs.environment import UnityEnvironment
from mlagents_envs.envs import UnityToGymWrapper
```

to import the Gym Wrapper. Navigate to the `create_atari_environment` method in the same file, and switch to instantiating a Unity environment by replacing the method with the following code.

```python
    game_version = 'v0' if sticky_actions else 'v4'
    full_game_name = '{}NoFrameskip-{}'.format(game_name, game_version)
    unity_env = UnityEnvironment(<path-to-environment>)
    env = UnityToGymWrapper(unity_env, uint8_visual=True)
    return env
```

`<path-to-environment>` is the path to your built Unity executable. For more information on building Unity environments, see [here](Learning-Environment-Executable.md), and note the Limitations section below.

Note that we are not using the preprocessor from Dopamine, as it uses many Atari-specific calls. Furthermore, frame-skipping can be done from within Unity, rather than on the Python side.

### Limitations

Since Dopamine is designed around variants of DQN, it is only compatible with discrete action spaces, and specifically the Discrete Gym space. For environments that use branched discrete action spaces, you can enable the `flatten_branched` parameter in `UnityToGymWrapper`, which treats each combination of branched actions as separate actions.

Furthermore, when building your environments, ensure that your Agent is using visual observations with greyscale enabled, and that the dimensions of the visual observations is 84 by 84 (matches the parameter found in `dqn_agent.py` and `rainbow_agent.py`). Dopamine's agents currently do not automatically adapt to the observation dimensions or number of channels.

### Hyperparameters

The hyperparameters provided by Dopamine are tailored to the Atari games, and you will likely need to adjust them for ML-Agents environments. Here is a sample `dopamine/agents/rainbow/configs/rainbow.gin` file that is known to work with a simple GridWorld.

```python
import dopamine.agents.rainbow.rainbow_agent
import dopamine.unity.run_experiment
import dopamine.replay_memory.prioritized_replay_buffer
import gin.tf.external_configurables

RainbowAgent.num_atoms = 51
RainbowAgent.stack_size = 1
RainbowAgent.vmax = 10.
RainbowAgent.gamma = 0.99
RainbowAgent.update_horizon = 3
RainbowAgent.min_replay_history = 20000  # agent steps
RainbowAgent.update_period = 5
RainbowAgent.target_update_period = 50  # agent steps
RainbowAgent.epsilon_train = 0.1
RainbowAgent.epsilon_eval = 0.01
RainbowAgent.epsilon_decay_period = 50000  # agent steps
RainbowAgent.replay_scheme = 'prioritized'
RainbowAgent.tf_device = '/cpu:0'  # use '/cpu:*' for non-GPU version
RainbowAgent.optimizer = @tf.train.AdamOptimizer()

tf.train.AdamOptimizer.learning_rate = 0.00025
tf.train.AdamOptimizer.epsilon = 0.0003125

Runner.game_name = "Unity" # any name can be used here
Runner.sticky_actions = False
Runner.num_iterations = 200
Runner.training_steps = 10000  # agent steps
Runner.evaluation_steps = 500  # agent steps
Runner.max_steps_per_episode = 27000  # agent steps

WrappedPrioritizedReplayBuffer.replay_capacity = 1000000
WrappedPrioritizedReplayBuffer.batch_size = 32
```

This example assumed you copied `atari` to a separate folder named `unity`. Replace `unity` in `import dopamine.unity.run_experiment` with the folder you copied your `run_experiment.py` and `trainer.py` files to. If you directly modified the existing files, then use `atari` here.

### Starting a Run

You can now run Dopamine as you would normally:

```
python -um dopamine.unity.train \
  --agent_name=rainbow \
  --base_dir=/tmp/dopamine \
  --gin_files='dopamine/agents/rainbow/configs/rainbow.gin'
```

Again, we assume that you've copied `atari` into a separate folder. Remember to replace `unity` with the directory you copied your files into. If you edited the Atari files directly, this should be `atari`.

### Example: GridWorld

As a baseline, here are rewards over time for the three algorithms provided with Dopamine as run on the GridWorld example environment. All Dopamine (DQN, Rainbow, C51) runs were done with the same epsilon, epsilon decay, replay history, training steps, and buffer settings as specified above. Note that the first 20000 steps are used to pre-fill the training buffer, and no learning happens.

We provide results from our PPO implementation and the DQN from Baselines as reference. Note that all runs used the same greyscale GridWorld as Dopamine. For PPO, `num_layers` was set to 2, and all other hyperparameters are the default for GridWorld in `config/ppo/GridWorld.yaml`. For Baselines DQN, the provided hyperparameters in the previous section are used. Note that Baselines implements certain features (e.g. dueling-Q) that are not enabled in Dopamine DQN.


![Dopamine on GridWorld](images/dopamine_gridworld_plot.png)


## Links discovered
- [here](https://github.com/openai/gym)
- [mlagents_envs installation instructions](https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/ml-agents-envs/README.md)
- [Baselines](https://github.com/openai/baselines)
- [here](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Executable.md)
- [Dopamine](https://github.com/google/dopamine)
- [Dopamine's homepage](https://github.com/google/dopamine)
- [Dopamine on GridWorld](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/dopamine_gridworld_plot.png)

--- com.unity.ml-agents/Documentation~/Python-Gym-API-Documentation.md ---
# Python Gym API Documentation

<a name="mlagents_envs.envs.unity_gym_env"></a>
# mlagents\_envs.envs.unity\_gym\_env

<a name="mlagents_envs.envs.unity_gym_env.UnityGymException"></a>
## UnityGymException Objects

```python
class UnityGymException(error.Error)
```

Any error related to the gym wrapper of ml-agents.

<a name="mlagents_envs.envs.unity_gym_env.UnityToGymWrapper"></a>
## UnityToGymWrapper Objects

```python
class UnityToGymWrapper(gym.Env)
```

Provides Gym wrapper for Unity Learning Environments.

<a name="mlagents_envs.envs.unity_gym_env.UnityToGymWrapper.__init__"></a>
#### \_\_init\_\_

```python
 | __init__(unity_env: BaseEnv, uint8_visual: bool = False, flatten_branched: bool = False, allow_multiple_obs: bool = False, action_space_seed: Optional[int] = None)
```

Environment initialization

**Arguments**:

- `unity_env`: The Unity BaseEnv to be wrapped in the gym. Will be closed when the UnityToGymWrapper closes.
- `uint8_visual`: Return visual observations as uint8 (0-255) matrices instead of float (0.0-1.0).
- `flatten_branched`: If True, turn branched discrete action spaces into a Discrete space rather than MultiDiscrete.
- `allow_multiple_obs`: If True, return a list of np.ndarrays as observations with the first elements containing the visual observations and the last element containing the array of vector observations. If False, returns a single np.ndarray containing either only a single visual observation or the array of vector observations.
- `action_space_seed`: If non-None, will be used to set the random seed on created gym.Space instances.

<a name="mlagents_envs.envs.unity_gym_env.UnityToGymWrapper.reset"></a>
#### reset

```python
 | reset() -> Union[List[np.ndarray], np.ndarray]
```

Resets the state of the environment and returns an initial observation. Returns: observation (object/list): the initial observation of the space.

<a name="mlagents_envs.envs.unity_gym_env.UnityToGymWrapper.step"></a>
#### step

```python
 | step(action: List[Any]) -> GymStepResult
```

Run one timestep of the environment's dynamics. When end of episode is reached, you are responsible for calling `reset()` to reset this environment's state. Accepts an action and returns a tuple (observation, reward, done, info).

**Arguments**:

- `action` _object/list_ - an action provided by the environment

**Returns**:

- `observation` _object/list_ - agent's observation of the current environment reward (float/list) : amount of reward returned after previous action
- `done` _boolean/list_ - whether the episode has ended.
- `info` _dict_ - contains auxiliary diagnostic information.

<a name="mlagents_envs.envs.unity_gym_env.UnityToGymWrapper.render"></a>
#### render

```python
 | render(mode="rgb_array")
```

Return the latest visual observations. Note that it will not render a new frame of the environment.

<a name="mlagents_envs.envs.unity_gym_env.UnityToGymWrapper.close"></a>
#### close

```python
 | close() -> None
```

Override _close in your subclass to perform any necessary cleanup. Environments will automatically close() themselves when garbage collected or when the program exits.

<a name="mlagents_envs.envs.unity_gym_env.UnityToGymWrapper.seed"></a>
#### seed

```python
 | seed(seed: Any = None) -> None
```

Sets the seed for this env's random number generator(s). Currently not implemented.

<a name="mlagents_envs.envs.unity_gym_env.ActionFlattener"></a>
## ActionFlattener Objects

```python
class ActionFlattener()
```

Flattens branched discrete action spaces into single-branch discrete action spaces.

<a name="mlagents_envs.envs.unity_gym_env.ActionFlattener.__init__"></a>
#### \_\_init\_\_

```python
 | __init__(branched_action_space)
```

Initialize the flattener.

**Arguments**:

- `branched_action_space`: A List containing the sizes of each branch of the action space, e.g. [2,3,3] for three branches with size 2, 3, and 3 respectively.

<a name="mlagents_envs.envs.unity_gym_env.ActionFlattener.lookup_action"></a>
#### lookup\_action

```python
 | lookup_action(action)
```

Convert a scalar discrete action into a unique set of branched actions.

**Arguments**:

- `action`: A scalar value representing one of the discrete actions.

**Returns**:

The List containing the branched actions.


--- com.unity.ml-agents/Documentation~/Python-LLAPI.md ---
# Unity ML-Agents Python Low Level API

The `mlagents` Python package contains two components: a low level API which allows you to interact directly with a Unity Environment (`mlagents_envs`) and an entry point to train (`mlagents-learn`) which allows you to train agents in Unity Environments using our implementations of reinforcement learning or imitation learning. This document describes how to use the `mlagents_envs` API. For information on using `mlagents-learn`, see [here](Training-ML-Agents.md). For Python Low Level API documentation, see [here](Python-LLAPI-Documentation.md).

The Python Low Level API can be used to interact directly with your Unity learning environment. As such, it can serve as the basis for developing and evaluating new learning algorithms.

## mlagents_envs

The ML-Agents Toolkit Low Level API is a Python API for controlling the simulation loop of an environment or game built with Unity. This API is used by the training algorithms inside the ML-Agent Toolkit, but you can also write your own Python programs using this API.

The key objects in the Python API include:

- **UnityEnvironment** — the main interface between the Unity application and your code. Use UnityEnvironment to start and control a simulation or training session.
- **BehaviorName** - is a string that identifies a behavior in the simulation.
- **AgentId** - is an `int` that serves as unique identifier for Agents in the simulation.
- **DecisionSteps** — contains the data from Agents belonging to the same "Behavior" in the simulation, such as observations and rewards. Only Agents that requested a decision since the last call to `env.step()` are in the DecisionSteps object.
- **TerminalSteps** — contains the data from Agents belonging to the same "Behavior" in the simulation, such as observations and rewards. Only Agents whose episode ended since the last call to `env.step()` are in the TerminalSteps object.
- **BehaviorSpec** — describes the shape of the observation data inside DecisionSteps and TerminalSteps as well as the expected action shapes.

These classes are all defined in the [base_env](https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/ml-agents-envs/mlagents_envs/base_env.py) script.

An Agent "Behavior" is a group of Agents identified by a `BehaviorName` that share the same observations and action types (described in their `BehaviorSpec`). You can think about Agent Behavior as a group of agents that will share the same policy. All Agents with the same behavior have the same goal and reward signals.

To communicate with an Agent in a Unity environment from a Python program, the Agent in the simulation must have `Behavior Parameters` set to communicate. You must set the `Behavior Type` to `Default` and give it a `Behavior Name`.

_Notice: Currently communication between Unity and Python takes place over an open socket without authentication. As such, please make sure that the network where training takes place is secure. This will be addressed in a future release._

## Loading a Unity Environment

Python-side communication happens through `UnityEnvironment` which is located in [`environment.py`](https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/ml-agents-envs/mlagents_envs/environment.py). To load a Unity environment from a built binary file, put the file in the same directory as `envs`. For example, if the filename of your Unity environment is `3DBall`, in python, run:

```python
from mlagents_envs.environment import UnityEnvironment
# This is a non-blocking call that only loads the environment.
env = UnityEnvironment(file_name="3DBall", seed=1, side_channels=[])
# Start interacting with the environment.
env.reset()
behavior_names = env.behavior_specs.keys()
...
```
**NOTE:** Please read [Interacting with a Unity Environment](#interacting-with-a-unity-environment) to read more about how you can interact with the Unity environment from Python.

- `file_name` is the name of the environment binary (located in the root directory of the python project).
- `worker_id` indicates which port to use for communication with the environment. For use in parallel training regimes such as A3C.
- `seed` indicates the seed to use when generating random numbers during the training process. In environments which are stochastic, setting the seed enables reproducible experimentation by ensuring that the environment and trainers utilize the same random seed.
- `side_channels` provides a way to exchange data with the Unity simulation that is not related to the reinforcement learning loop. For example: configurations or properties. More on them in the [Side Channels](Custom-SideChannels.md) doc.

If you want to directly interact with the Editor, you need to use `file_name=None`, then press the **Play** button in the Editor when the message _"Start training by pressing the Play button in the Unity Editor"_ is displayed on the screen

### Interacting with a Unity Environment

#### The BaseEnv interface

A `BaseEnv` has the following methods:

- **Reset : `env.reset()`** Sends a signal to reset the environment. Returns None.
- **Step : `env.step()`** Sends a signal to step the environment. Returns None. Note that a "step" for Python does not correspond to either Unity `Update` nor `FixedUpdate`. When `step()` or `reset()` is called, the Unity simulation will move forward until an Agent in the simulation needs a input from Python to act.
- **Close : `env.close()`** Sends a shutdown signal to the environment and terminates the communication.
- **Behavior Specs : `env.behavior_specs`** Returns a Mapping of `BehaviorName` to `BehaviorSpec` objects (read only). A `BehaviorSpec` contains the observation shapes and the `ActionSpec` (which defines the action shape). Note that the `BehaviorSpec` for a specific group is fixed throughout the simulation. The number of entries in the Mapping can change over time in the simulation if new Agent behaviors are created in the simulation.
- **Get Steps : `env.get_steps(behavior_name: str)`** Returns a tuple `DecisionSteps, TerminalSteps` corresponding to the behavior_name given as input. The `DecisionSteps` contains information about the state of the agents
  **that need an action this step** and have the behavior behavior_name. The `TerminalSteps` contains information about the state of the agents **whose episode ended** and have the behavior behavior_name. Both `DecisionSteps` an `TerminalSteps` contain information such as the observations, the rewards and the agent identifiers. `DecisionSteps` also contains action masks for the next action while `TerminalSteps` contains the reason for termination (did the Agent reach its maximum step and was interrupted). The data is in `np.array` of which the first dimension is always the number of agents note that the number of agents is not guaranteed to remain constant during the simulation and it is not unusual to have either `DecisionSteps` or `TerminalSteps` contain no Agents at all.
- **Set Actions :`env.set_actions(behavior_name: str, action: ActionTuple)`** Sets the actions for a whole agent group. `action` is an `ActionTuple`, which is made up of a 2D `np.array` of `dtype=np.int32` for discrete actions, and `dtype=np.float32` for continuous actions. The first dimension of `np.array`in the tuple is the number of agents that requested a decision since the last call to `env.step()`. The second dimension is the number of discrete or continuous actions for the corresponding array.
- **Set Action for Agent: `env.set_action_for_agent(agent_group: str, agent_id: int, action: ActionTuple)`** Sets the action for a specific Agent in an agent group. `agent_group` is the name of the group the Agent belongs to and `agent_id` is the integer identifier of the Agent. `action` is an `ActionTuple` as described above.
**Note:** If no action is provided for an agent group between two calls to `env.step()` then the default action will be all zeros.

#### DecisionSteps and DecisionStep

`DecisionSteps` (with `s`) contains information about a whole batch of Agents while `DecisionStep` (no `s`) only contains information about a single Agent.

A `DecisionSteps` has the following fields :

- `obs` is a list of numpy arrays observations collected by the group of agent. The first dimension of the array corresponds to the batch size of the group (number of agents requesting a decision since the last call to `env.step()`).
- `reward` is a float vector of length batch size. Corresponds to the rewards collected by each agent since the last simulation step.
- `agent_id` is an int vector of length batch size containing unique identifier for the corresponding Agent. This is used to track Agents across simulation steps.
- `action_mask` is an optional list of two-dimensional arrays of booleans which is only available when using multi-discrete actions. Each array corresponds to an action branch. The first dimension of each array is the batch size and the second contains a mask for each action of the branch. If true, the action is not available for the agent during this simulation step.

It also has the two following methods:

- `len(DecisionSteps)` Returns the number of agents requesting a decision since the last call to `env.step()`.
- `DecisionSteps[agent_id]` Returns a `DecisionStep` for the Agent with the `agent_id` unique identifier.

A `DecisionStep` has the following fields:

- `obs` is a list of numpy arrays observations collected by the agent. (Each array has one less dimension than the arrays in `DecisionSteps`)
- `reward` is a float. Corresponds to the rewards collected by the agent since the last simulation step.
- `agent_id` is an int and an unique identifier for the corresponding Agent.
- `action_mask` is an optional list of one dimensional arrays of booleans which is only available when using multi-discrete actions. Each array corresponds to an action branch. Each array contains a mask for each action of the branch. If true, the action is not available for the agent during this simulation step.

#### TerminalSteps and TerminalStep

Similarly to `DecisionSteps` and `DecisionStep`, `TerminalSteps` (with `s`) contains information about a whole batch of Agents while `TerminalStep` (no `s`) only contains information about a single Agent.

A `TerminalSteps` has the following fields :

- `obs` is a list of numpy arrays observations collected by the group of agent. The first dimension of the array corresponds to the batch size of the group (number of agents requesting a decision since the last call to `env.step()`).
- `reward` is a float vector of length batch size. Corresponds to the rewards collected by each agent since the last simulation step.
- `agent_id` is an int vector of length batch size containing unique identifier for the corresponding Agent. This is used to track Agents across simulation steps.
- `interrupted` is an array of booleans of length batch size. Is true if the associated Agent was interrupted since the last decision step. For example, if the Agent reached the maximum number of steps for the episode.

It also has the two following methods:

- `len(TerminalSteps)` Returns the number of agents requesting a decision since the last call to `env.step()`.
- `TerminalSteps[agent_id]` Returns a `TerminalStep` for the Agent with the `agent_id` unique identifier.

A `TerminalStep` has the following fields:

- `obs` is a list of numpy arrays observations collected by the agent. (Each array has one less dimension than the arrays in `TerminalSteps`)
- `reward` is a float. Corresponds to the rewards collected by the agent since the last simulation step.
- `agent_id` is an int and an unique identifier for the corresponding Agent.
- `interrupted` is a bool. Is true if the Agent was interrupted since the last decision step. For example, if the Agent reached the maximum number of steps for the episode.

#### BehaviorSpec

A `BehaviorSpec` has the following fields :

- `observation_specs` is a List of `ObservationSpec` objects : Each `ObservationSpec` corresponds to an observation's properties: `shape` is a tuple of ints that corresponds to the shape of the observation (without the number of agents dimension). `dimension_property` is a tuple of flags containing extra information about how the data should be processed in the corresponding dimension. `observation_type` is an enum corresponding to what type of observation is generating the data (i.e., default, goal, etc). Note that the `ObservationSpec` have the same ordering as the ordering of observations in the DecisionSteps, DecisionStep, TerminalSteps and TerminalStep.
- `action_spec` is an `ActionSpec` namedtuple that defines the number and types of actions for the Agent.

An `ActionSpec` has the following fields and properties:
- `continuous_size` is the number of floats that constitute the continuous actions.
- `discrete_size` is the number of branches (the number of independent actions) that constitute the multi-discrete actions.
- `discrete_branches` is a Tuple of ints. Each int corresponds to the number of different options for each branch of the action. For example: In a game direction input (no movement, left, right) and jump input (no jump, jump) there will be two branches (direction and jump), the first one with 3 options and the second with 2 options. (`discrete_size = 2` and `discrete_action_branches = (3,2,)`)


### Communicating additional information with the Environment

In addition to the means of communicating between Unity and python described above, we also provide methods for sharing agent-agnostic information. These additional methods are referred to as side channels. ML-Agents includes two ready-made side channels, described below. It is also possible to create custom side channels to communicate any additional data between a Unity environment and Python. Instructions for creating custom side channels can be found [here](Custom-SideChannels.md).

Side channels exist as separate classes which are instantiated, and then passed as list to the `side_channels` argument of the constructor of the `UnityEnvironment` class.

```python
channel = MyChannel()

env = UnityEnvironment(side_channels = [channel])
```

**Note** : A side channel will only send/receive messages when `env.step` or
`env.reset()` is called.

#### EngineConfigurationChannel

The `EngineConfiguration` side channel allows you to modify the time-scale, resolution, and graphics quality of the environment. This can be useful for adjusting the environment to perform better during training, or be more interpretable during inference.

`EngineConfigurationChannel` has two methods :

- `set_configuration_parameters` which takes the following arguments:
  - `width`: Defines the width of the display. (Must be set alongside height)
  - `height`: Defines the height of the display. (Must be set alongside width)
  - `quality_level`: Defines the quality level of the simulation.
  - `time_scale`: Defines the multiplier for the deltatime in the simulation. If set to a higher value, time will pass faster in the simulation but the physics may perform unpredictably.
  - `target_frame_rate`: Instructs simulation to try to render at a specified frame rate.
  - `capture_frame_rate` Instructs the simulation to consider time between updates to always be constant, regardless of the actual frame rate.
- `set_configuration` with argument config which is an `EngineConfig` NamedTuple object.

For example, the following code would adjust the time-scale of the simulation to be 2x realtime.

```python
from mlagents_envs.environment import UnityEnvironment
from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel

channel = EngineConfigurationChannel()

env = UnityEnvironment(side_channels=[channel])

channel.set_configuration_parameters(time_scale = 2.0)

i = env.reset()
...
```

#### EnvironmentParameters

The `EnvironmentParameters` will allow you to get and set pre-defined numerical values in the environment. This can be useful for adjusting environment-specific settings, or for reading non-agent related information from the environment. You can call `get_property` and `set_property` on the side channel to read and write properties.

`EnvironmentParametersChannel` has one methods:

- `set_float_parameter` Sets a float parameter in the Unity Environment.
  - key: The string identifier of the property.
  - value: The float value of the property.

```python
from mlagents_envs.environment import UnityEnvironment
from mlagents_envs.side_channel.environment_parameters_channel import EnvironmentParametersChannel

channel = EnvironmentParametersChannel()

env = UnityEnvironment(side_channels=[channel])

channel.set_float_parameter("parameter_1", 2.0)

i = env.reset()
...
```

Once a property has been modified in Python, you can access it in C# after the next call to `step` as follows:

```csharp
var envParameters = Academy.Instance.EnvironmentParameters;
float property1 = envParameters.GetWithDefault("parameter_1", 0.0f);
```

#### Custom side channels

For information on how to make custom side channels for sending additional data types, see the documentation [here](Custom-SideChannels.md).


## Links discovered
- [here](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Training-ML-Agents.md)
- [here](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Python-LLAPI-Documentation.md)
- [base_env](https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/ml-agents-envs/mlagents_envs/base_env.py)
- [`environment.py`](https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/ml-agents-envs/mlagents_envs/environment.py)
- [Side Channels](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Custom-SideChannels.md)
- [here](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Custom-SideChannels.md)

--- com.unity.ml-agents/Documentation~/Python-LLAPI-Documentation.md ---
# Python Low-Level API Documentation

<a name="mlagents_envs.base_env"></a>
# mlagents\_envs.base\_env

Python Environment API for the ML-Agents Toolkit The aim of this API is to expose Agents evolving in a simulation to perform reinforcement learning on. This API supports multi-agent scenarios and groups similar Agents (same observations, actions spaces and behavior) together. These groups of Agents are identified by their BehaviorName. For performance reasons, the data of each group of agents is processed in a batched manner. Agents are identified by a unique AgentId identifier that allows tracking of Agents across simulation steps. Note that there is no guarantee that the number or order of the Agents in the state will be consistent across simulation steps. A simulation steps corresponds to moving the simulation forward until at least one agent in the simulation sends its observations to Python again. Since Agents can request decisions at different frequencies, a simulation step does not necessarily correspond to a fixed simulation time increment.

<a name="mlagents_envs.base_env.DecisionStep"></a>
## DecisionStep Objects

```python
class DecisionStep(NamedTuple)
```

Contains the data a single Agent collected since the last simulation step.
 - obs is a list of numpy arrays observations collected by the agent.
 - reward is a float. Corresponds to the rewards collected by the agent since the last simulation step.
 - agent_id is an int and an unique identifier for the corresponding Agent.
 - action_mask is an optional list of one dimensional array of booleans. Only available when using multi-discrete actions. Each array corresponds to an action branch. Each array contains a mask for each action of the branch. If true, the action is not available for the agent during this simulation step.

<a name="mlagents_envs.base_env.DecisionSteps"></a>
## DecisionSteps Objects

```python
class DecisionSteps(Mapping)
```

Contains the data a batch of similar Agents collected since the last simulation step. Note that all Agents do not necessarily have new information to send at each simulation step. Therefore, the ordering of agents and the batch size of the DecisionSteps are not fixed across simulation steps.
 - obs is a list of numpy arrays observations collected by the batch of agent. Each obs has one extra dimension compared to DecisionStep: the first dimension of the array corresponds to the batch size of the batch.
 - reward is a float vector of length batch size. Corresponds to the rewards collected by each agent since the last simulation step.
 - agent_id is an int vector of length batch size containing unique identifier for the corresponding Agent. This is used to track Agents across simulation steps.
 - action_mask is an optional list of two-dimensional array of booleans. Only available when using multi-discrete actions. Each array corresponds to an action branch. The first dimension of each array is the batch size and the second contains a mask for each action of the branch. If true, the action is not available for the agent during this simulation step.

<a name="mlagents_envs.base_env.DecisionSteps.agent_id_to_index"></a>
#### agent\_id\_to\_index

```python
 | @property
 | agent_id_to_index() -> Dict[AgentId, int]
```

**Returns**:

A Dict that maps agent_id to the index of those agents in this DecisionSteps.

<a name="mlagents_envs.base_env.DecisionSteps.__getitem__"></a>
#### \_\_getitem\_\_

```python
 | __getitem__(agent_id: AgentId) -> DecisionStep
```

returns the DecisionStep for a specific agent.

**Arguments**:

- `agent_id`: The id of the agent

**Returns**:

The DecisionStep

<a name="mlagents_envs.base_env.DecisionSteps.empty"></a>
#### empty

```python
 | @staticmethod
 | empty(spec: "BehaviorSpec") -> "DecisionSteps"
```

Returns an empty DecisionSteps.

**Arguments**:

- `spec`: The BehaviorSpec for the DecisionSteps

<a name="mlagents_envs.base_env.TerminalStep"></a>
## TerminalStep Objects

```python
class TerminalStep(NamedTuple)
```

Contains the data a single Agent collected when its episode ended.
 - obs is a list of numpy arrays observations collected by the agent.
 - reward is a float. Corresponds to the rewards collected by the agent since the last simulation step.
 - interrupted is a bool. Is true if the Agent was interrupted since the last decision step. For example, if the Agent reached the maximum number of steps for the episode.
 - agent_id is an int and an unique identifier for the corresponding Agent.

<a name="mlagents_envs.base_env.TerminalSteps"></a>
## TerminalSteps Objects

```python
class TerminalSteps(Mapping)
```

Contains the data a batch of Agents collected when their episode terminated. All Agents present in the TerminalSteps have ended their episode.
 - obs is a list of numpy arrays observations collected by the batch of agent. Each obs has one extra dimension compared to DecisionStep: the first dimension of the array corresponds to the batch size of the batch.
 - reward is a float vector of length batch size. Corresponds to the rewards collected by each agent since the last simulation step.
 - interrupted is an array of booleans of length batch size. Is true if the associated Agent was interrupted since the last decision step. For example, if the Agent reached the maximum number of steps for the episode.
 - agent_id is an int vector of length batch size containing unique identifier for the corresponding Agent. This is used to track Agents across simulation steps.

<a name="mlagents_envs.base_env.TerminalSteps.agent_id_to_index"></a>
#### agent\_id\_to\_index

```python
 | @property
 | agent_id_to_index() -> Dict[AgentId, int]
```

**Returns**:

A Dict that maps agent_id to the index of those agents in this TerminalSteps.

<a name="mlagents_envs.base_env.TerminalSteps.__getitem__"></a>
#### \_\_getitem\_\_

```python
 | __getitem__(agent_id: AgentId) -> TerminalStep
```

returns the TerminalStep for a specific agent.

**Arguments**:

- `agent_id`: The id of the agent

**Returns**:

obs, reward, done, agent_id and optional action mask for a specific agent

<a name="mlagents_envs.base_env.TerminalSteps.empty"></a>
#### empty

```python
 | @staticmethod
 | empty(spec: "BehaviorSpec") -> "TerminalSteps"
```

Returns an empty TerminalSteps.

**Arguments**:

- `spec`: The BehaviorSpec for the TerminalSteps

<a name="mlagents_envs.base_env.ActionTuple"></a>
## ActionTuple Objects

```python
class ActionTuple(_ActionTupleBase)
```

An object whose fields correspond to actions of different types. Continuous and discrete actions are numpy arrays of type float32 and int32, respectively and are type checked on construction. Dimensions are of (n_agents, continuous_size) and (n_agents, discrete_size), respectively. Note, this also holds when continuous or discrete size is zero.

<a name="mlagents_envs.base_env.ActionTuple.discrete_dtype"></a>
#### discrete\_dtype

```python
 | @property
 | discrete_dtype() -> np.dtype
```

The dtype of a discrete action.

<a name="mlagents_envs.base_env.ActionSpec"></a>
## ActionSpec Objects

```python
class ActionSpec(NamedTuple)
```

A NamedTuple containing utility functions and information about the action spaces for a group of Agents under the same behavior.
- num_continuous_actions is an int corresponding to the number of floats which constitute the action.
- discrete_branch_sizes is a Tuple of int where each int corresponds to the number of discrete actions available to the agent on an independent action branch.

<a name="mlagents_envs.base_env.ActionSpec.is_discrete"></a>
#### is\_discrete

```python
 | is_discrete() -> bool
```

Returns true if this Behavior uses discrete actions

<a name="mlagents_envs.base_env.ActionSpec.is_continuous"></a>
#### is\_continuous

```python
 | is_continuous() -> bool
```

Returns true if this Behavior uses continuous actions

<a name="mlagents_envs.base_env.ActionSpec.discrete_size"></a>
#### discrete\_size

```python
 | @property
 | discrete_size() -> int
```

Returns an int corresponding to the number of discrete branches.

<a name="mlagents_envs.base_env.ActionSpec.empty_action"></a>
#### empty\_action

```python
 | empty_action(n_agents: int) -> ActionTuple
```

Generates ActionTuple corresponding to an empty action (all zeros) for a number of agents.

**Arguments**:

- `n_agents`: The number of agents that will have actions generated

<a name="mlagents_envs.base_env.ActionSpec.random_action"></a>
#### random\_action

```python
 | random_action(n_agents: int) -> ActionTuple
```

Generates ActionTuple corresponding to a random action (either discrete or continuous) for a number of agents.

**Arguments**:

- `n_agents`: The number of agents that will have actions generated

<a name="mlagents_envs.base_env.ActionSpec.create_continuous"></a>
#### create\_continuous

```python
 | @staticmethod
 | create_continuous(continuous_size: int) -> "ActionSpec"
```

Creates an ActionSpec that is homogenously continuous

<a name="mlagents_envs.base_env.ActionSpec.create_discrete"></a>
#### create\_discrete

```python
 | @staticmethod
 | create_discrete(discrete_branches: Tuple[int]) -> "ActionSpec"
```

Creates an ActionSpec that is homogenously discrete

<a name="mlagents_envs.base_env.ActionSpec.create_hybrid"></a>
#### create\_hybrid

```python
 | @staticmethod
 | create_hybrid(continuous_size: int, discrete_branches: Tuple[int]) -> "ActionSpec"
```

Creates a hybrid ActionSpace

<a name="mlagents_envs.base_env.DimensionProperty"></a>
## DimensionProperty Objects

```python
class DimensionProperty(IntFlag)
```

The dimension property of a dimension of an observation.

<a name="mlagents_envs.base_env.DimensionProperty.UNSPECIFIED"></a>
#### UNSPECIFIED

No properties specified.

<a name="mlagents_envs.base_env.DimensionProperty.NONE"></a>
#### NONE

No Property of the observation in that dimension. Observation can be processed with Fully connected networks.

<a name="mlagents_envs.base_env.DimensionProperty.TRANSLATIONAL_EQUIVARIANCE"></a>
#### TRANSLATIONAL\_EQUIVARIANCE

Means it is suitable to do a convolution in this dimension.

<a name="mlagents_envs.base_env.DimensionProperty.VARIABLE_SIZE"></a>
#### VARIABLE\_SIZE

Means that there can be a variable number of observations in this dimension. The observations are unordered.

<a name="mlagents_envs.base_env.ObservationType"></a>
## ObservationType Objects

```python
class ObservationType(Enum)
```

An Enum which defines the type of information carried in the observation of the agent.

<a name="mlagents_envs.base_env.ObservationType.DEFAULT"></a>
#### DEFAULT

Observation information is generic.

<a name="mlagents_envs.base_env.ObservationType.GOAL_SIGNAL"></a>
#### GOAL\_SIGNAL

Observation contains goal information for current task.

<a name="mlagents_envs.base_env.ObservationSpec"></a>
## ObservationSpec Objects

```python
class ObservationSpec(NamedTuple)
```

A NamedTuple containing information about the observation of Agents.
- shape is a Tuple of int : It corresponds to the shape of an observation's dimensions.
- dimension_property is a Tuple of DimensionProperties flag, one flag for each dimension.
- observation_type is an enum of ObservationType.

<a name="mlagents_envs.base_env.BehaviorSpec"></a>
## BehaviorSpec Objects

```python
class BehaviorSpec(NamedTuple)
```

A NamedTuple containing information about the observation and action spaces for a group of Agents under the same behavior.
- observation_specs is a List of ObservationSpec NamedTuple containing information about the information of the Agent's observations such as their shapes. The order of the ObservationSpec is the same as the order of the observations of an agent.
- action_spec is an ActionSpec NamedTuple.

<a name="mlagents_envs.base_env.BaseEnv"></a>
## BaseEnv Objects

```python
class BaseEnv(ABC)
```

<a name="mlagents_envs.base_env.BaseEnv.step"></a>
#### step

```python
 | @abstractmethod
 | step() -> None
```

Signals the environment that it must move the simulation forward by one step.

<a name="mlagents_envs.base_env.BaseEnv.reset"></a>
#### reset

```python
 | @abstractmethod
 | reset() -> None
```

Signals the environment that it must reset the simulation.

<a name="mlagents_envs.base_env.BaseEnv.close"></a>
#### close

```python
 | @abstractmethod
 | close() -> None
```

Signals the environment that it must close.

<a name="mlagents_envs.base_env.BaseEnv.behavior_specs"></a>
#### behavior\_specs

```python
 | @property
 | @abstractmethod
 | behavior_specs() -> MappingType[str, BehaviorSpec]
```

Returns a Mapping from behavior names to behavior specs. Agents grouped under the same behavior name have the same action and observation specs, and are expected to behave similarly in the environment. Note that new keys can be added to this mapping as new policies are instantiated.

<a name="mlagents_envs.base_env.BaseEnv.set_actions"></a>
#### set\_actions

```python
 | @abstractmethod
 | set_actions(behavior_name: BehaviorName, action: ActionTuple) -> None
```

Sets the action for all of the agents in the simulation for the next step. The Actions must be in the same order as the order received in the DecisionSteps.

**Arguments**:

- `behavior_name`: The name of the behavior the agents are part of
- `action`: ActionTuple tuple of continuous and/or discrete action. Actions are np.arrays with dimensions  (n_agents, continuous_size) and (n_agents, discrete_size), respectively.

<a name="mlagents_envs.base_env.BaseEnv.set_action_for_agent"></a>
#### set\_action\_for\_agent

```python
 | @abstractmethod
 | set_action_for_agent(behavior_name: BehaviorName, agent_id: AgentId, action: ActionTuple) -> None
```

Sets the action for one of the agents in the simulation for the next step.

**Arguments**:

- `behavior_name`: The name of the behavior the agent is part of
- `agent_id`: The id of the agent the action is set for
- `action`: ActionTuple tuple of continuous and/or discrete action. Actions are np.arrays with dimensions  (1, continuous_size) and (1, discrete_size), respectively. Note, this initial dimensions of 1 is because this action is meant for a single agent.

<a name="mlagents_envs.base_env.BaseEnv.get_steps"></a>
#### get\_steps

```python
 | @abstractmethod
 | get_steps(behavior_name: BehaviorName) -> Tuple[DecisionSteps, TerminalSteps]
```

Retrieves the steps of the agents that requested a step in the simulation.

**Arguments**:

- `behavior_name`: The name of the behavior the agents are part of

**Returns**:

A tuple containing :
- A DecisionSteps NamedTuple containing the observations, the rewards, the agent ids and the action masks for the Agents of the specified behavior. These Agents need an action this step.
- A TerminalSteps NamedTuple containing the observations, rewards, agent ids and interrupted flags of the agents that had their episode terminated last step.

<a name="mlagents_envs.environment"></a>
# mlagents\_envs.environment

<a name="mlagents_envs.environment.UnityEnvironment"></a>
## UnityEnvironment Objects

```python
class UnityEnvironment(BaseEnv)
```

<a name="mlagents_envs.environment.UnityEnvironment.__init__"></a>
#### \_\_init\_\_

```python
 | __init__(file_name: Optional[str] = None, worker_id: int = 0, base_port: Optional[int] = None, seed: int = 0, no_graphics: bool = False, no_graphics_monitor: bool = False, timeout_wait: int = 60, additional_args: Optional[List[str]] = None, side_channels: Optional[List[SideChannel]] = None, log_folder: Optional[str] = None, num_areas: int = 1)
```

Starts a new unity environment and establishes a connection with the environment. Notice: Currently communication between Unity and Python takes place over an open socket without authentication. Ensure that the network where training takes place is secure.

:string file_name: Name of Unity environment binary. :int base_port: Baseline port number to connect to Unity environment over. worker_id increments over this. If no environment is specified (i.e. file_name is None), the DEFAULT_EDITOR_PORT will be used. :int worker_id: Offset from base_port. Used for training multiple environments simultaneously. :bool no_graphics: Whether to run the Unity simulator in no-graphics mode :bool no_graphics_monitor: Whether to run the main worker in graphics mode, with the remaining in no-graphics mode :int timeout_wait: Time (in seconds) to wait for connection from environment. :list args: Addition Unity command line arguments :list side_channels: Additional side channel for no-rl communication with Unity :str log_folder: Optional folder to write the Unity Player log file into.  Requires absolute path.

<a name="mlagents_envs.environment.UnityEnvironment.close"></a>
#### close

```python
 | close()
```

Sends a shutdown signal to the unity environment, and closes the socket connection.

<a name="mlagents_envs.registry"></a>
# mlagents\_envs.registry

<a name="mlagents_envs.registry.unity_env_registry"></a>
# mlagents\_envs.registry.unity\_env\_registry

<a name="mlagents_envs.registry.unity_env_registry.UnityEnvRegistry"></a>
## UnityEnvRegistry Objects

```python
class UnityEnvRegistry(Mapping)
```

### UnityEnvRegistry
Provides a library of Unity environments that can be launched without the need of downloading the Unity Editor. The UnityEnvRegistry implements a Map, to access an entry of the Registry, use:
```python
registry = UnityEnvRegistry()
entry = registry[<environment_identifier>]
```
An entry has the following properties :
 * `identifier` : Uniquely identifies this environment
 * `expected_reward` : Corresponds to the reward an agent must obtained for the task to be considered completed.
 * `description` : A human readable description of the environment.

To launch a Unity environment from a registry entry, use the `make` method:
```python
registry = UnityEnvRegistry()
env = registry[<environment_identifier>].make()
```

<a name="mlagents_envs.registry.unity_env_registry.UnityEnvRegistry.register"></a>
#### register

```python
 | register(new_entry: BaseRegistryEntry) -> None
```

Registers a new BaseRegistryEntry to the registry. The BaseRegistryEntry.identifier value will be used as indexing key. If two are more environments are registered under the same key, the most recentry added will replace the others.

<a name="mlagents_envs.registry.unity_env_registry.UnityEnvRegistry.register_from_yaml"></a>
#### register\_from\_yaml

```python
 | register_from_yaml(path_to_yaml: str) -> None
```

Registers the environments listed in a yaml file (either local or remote). Note that the entries are registered lazily: the registration will only happen when an environment is accessed. The yaml file must have the following format :
```yaml
environments:
- <identifier of the first environment>:
    expected_reward: <expected reward of the environment>
    description: | <a multi line description of the environment>
      <continued multi line description>
    linux_url: <The url for the Linux executable zip file>
    darwin_url: <The url for the OSX executable zip file>
    win_url: <The url for the Windows executable zip file>

- <identifier of the second environment>:
    expected_reward: <expected reward of the environment>
    description: | <a multi line description of the environment>
      <continued multi line description>
    linux_url: <The url for the Linux executable zip file>
    darwin_url: <The url for the OSX executable zip file>
    win_url: <The url for the Windows executable zip file>

- ...
```

**Arguments**:

- `path_to_yaml`: A local path or url to the yaml file

<a name="mlagents_envs.registry.unity_env_registry.UnityEnvRegistry.clear"></a>
#### clear

```python
 | clear() -> None
```

Deletes all entries in the registry.

<a name="mlagents_envs.registry.unity_env_registry.UnityEnvRegistry.__getitem__"></a>
#### \_\_getitem\_\_

```python
 | __getitem__(identifier: str) -> BaseRegistryEntry
```

Returns the BaseRegistryEntry with the provided identifier. BaseRegistryEntry can then be used to make a Unity Environment.

**Arguments**:

- `identifier`: The identifier of the BaseRegistryEntry

**Returns**:

The associated BaseRegistryEntry

<a name="mlagents_envs.side_channel"></a>
# mlagents\_envs.side\_channel

<a name="mlagents_envs.side_channel.raw_bytes_channel"></a>
# mlagents\_envs.side\_channel.raw\_bytes\_channel

<a name="mlagents_envs.side_channel.raw_bytes_channel.RawBytesChannel"></a>
## RawBytesChannel Objects

```python
class RawBytesChannel(SideChannel)
```

This is an example of what the SideChannel for raw bytes exchange would look like. Is meant to be used for general research purpose.

<a name="mlagents_envs.side_channel.raw_bytes_channel.RawBytesChannel.on_message_received"></a>
#### on\_message\_received

```python
 | on_message_received(msg: IncomingMessage) -> None
```

Is called by the environment to the side channel. Can be called multiple times per step if multiple messages are meant for that SideChannel.

<a name="mlagents_envs.side_channel.raw_bytes_channel.RawBytesChannel.get_and_clear_received_messages"></a>
#### get\_and\_clear\_received\_messages

```python
 | get_and_clear_received_messages() -> List[bytes]
```

returns a list of bytearray received from the environment.

<a name="mlagents_envs.side_channel.raw_bytes_channel.RawBytesChannel.send_raw_data"></a>
#### send\_raw\_data

```python
 | send_raw_data(data: bytearray) -> None
```

Queues a message to be sent by the environment at the next call to step.

<a name="mlagents_envs.side_channel.outgoing_message"></a>
# mlagents\_envs.side\_channel.outgoing\_message

<a name="mlagents_envs.side_channel.outgoing_message.OutgoingMessage"></a>
## OutgoingMessage Objects

```python
class OutgoingMessage()
```

Utility class for forming the message that is written to a SideChannel. All data is written in little-endian format using the struct module.

<a name="mlagents_envs.side_channel.outgoing_message.OutgoingMessage.__init__"></a>
#### \_\_init\_\_

```python
 | __init__()
```

Create an OutgoingMessage with an empty buffer.

<a name="mlagents_envs.side_channel.outgoing_message.OutgoingMessage.write_bool"></a>
#### write\_bool

```python
 | write_bool(b: bool) -> None
```

Append a boolean value.

<a name="mlagents_envs.side_channel.outgoing_message.OutgoingMessage.write_int32"></a>
#### write\_int32

```python
 | write_int32(i: int) -> None
```

Append an integer value.

<a name="mlagents_envs.side_channel.outgoing_message.OutgoingMessage.write_float32"></a>
#### write\_float32

```python
 | write_float32(f: float) -> None
```

Append a float value. It will be truncated to 32-bit precision.

<a name="mlagents_envs.side_channel.outgoing_message.OutgoingMessage.write_float32_list"></a>
#### write\_float32\_list

```python
 | write_float32_list(float_list: List[float]) -> None
```

Append a list of float values. They will be truncated to 32-bit precision.

<a name="mlagents_envs.side_channel.outgoing_message.OutgoingMessage.write_string"></a>
#### write\_string

```python
 | write_string(s: str) -> None
```

Append a string value. Internally, it will be encoded to ascii, and the encoded length will also be written to the message.

<a name="mlagents_envs.side_channel.outgoing_message.OutgoingMessage.set_raw_bytes"></a>
#### set\_raw\_bytes

```python
 | set_raw_bytes(buffer: bytearray) -> None
```

Set the internal buffer to a new bytearray. This will overwrite any existing data.

**Arguments**:

- `buffer`:

**Returns**:



<a name="mlagents_envs.side_channel.engine_configuration_channel"></a>
# mlagents\_envs.side\_channel.engine\_configuration\_channel

<a name="mlagents_envs.side_channel.engine_configuration_channel.EngineConfigurationChannel"></a>
## EngineConfigurationChannel Objects

```python
class EngineConfigurationChannel(SideChannel)
```

This is the SideChannel for engine configuration exchange. The data in the engine configuration is as follows :
 - int width;
 - int height;
 - int qualityLevel;
 - float timeScale;
 - int targetFrameRate;
 - int captureFrameRate;

<a name="mlagents_envs.side_channel.engine_configuration_channel.EngineConfigurationChannel.on_message_received"></a>
#### on\_message\_received

```python
 | on_message_received(msg: IncomingMessage) -> None
```

Is called by the environment to the side channel. Can be called multiple times per step if multiple messages are meant for that SideChannel. Note that Python should never receive an engine configuration from Unity

<a name="mlagents_envs.side_channel.engine_configuration_channel.EngineConfigurationChannel.set_configuration_parameters"></a>
#### set\_configuration\_parameters

```python
 | set_configuration_parameters(width: Optional[int] = None, height: Optional[int] = None, quality_level: Optional[int] = None, time_scale: Optional[float] = None, target_frame_rate: Optional[int] = None, capture_frame_rate: Optional[int] = None) -> None
```

Sets the engine configuration. Takes as input the configurations of the engine.

**Arguments**:

- `width`: Defines the width of the display. (Must be set alongside height)
- `height`: Defines the height of the display. (Must be set alongside width)
- `quality_level`: Defines the quality level of the simulation.
- `time_scale`: Defines the multiplier for the deltatime in the simulation. If set to a higher value, time will pass faster in the simulation but the physics might break.
- `target_frame_rate`: Instructs simulation to try to render at a specified frame rate.
- `capture_frame_rate`: Instructs the simulation to consider time between updates to always be constant, regardless of the actual frame rate.

<a name="mlagents_envs.side_channel.engine_configuration_channel.EngineConfigurationChannel.set_configuration"></a>
#### set\_configuration

```python
 | set_configuration(config: EngineConfig) -> None
```

Sets the engine configuration. Takes as input an EngineConfig.

<a name="mlagents_envs.side_channel.side_channel_manager"></a>
# mlagents\_envs.side\_channel.side\_channel\_manager

<a name="mlagents_envs.side_channel.side_channel_manager.SideChannelManager"></a>
## SideChannelManager Objects

```python
class SideChannelManager()
```

<a name="mlagents_envs.side_channel.side_channel_manager.SideChannelManager.process_side_channel_message"></a>
#### process\_side\_channel\_message

```python
 | process_side_channel_message(data: bytes) -> None
```

Separates the data received from Python into individual messages for each registered side channel and calls on_message_received on them.

**Arguments**:

- `data`: The packed message sent by Unity

<a name="mlagents_envs.side_channel.side_channel_manager.SideChannelManager.generate_side_channel_messages"></a>
#### generate\_side\_channel\_messages

```python
 | generate_side_channel_messages() -> bytearray
```

Gathers the messages that the registered side channels will send to Unity and combines them into a single message ready to be sent.

<a name="mlagents_envs.side_channel.stats_side_channel"></a>
# mlagents\_envs.side\_channel.stats\_side\_channel

<a name="mlagents_envs.side_channel.stats_side_channel.StatsSideChannel"></a>
## StatsSideChannel Objects

```python
class StatsSideChannel(SideChannel)
```

Side channel that receives (string, float) pairs from the environment, so that they can eventually be passed to a StatsReporter.

<a name="mlagents_envs.side_channel.stats_side_channel.StatsSideChannel.on_message_received"></a>
#### on\_message\_received

```python
 | on_message_received(msg: IncomingMessage) -> None
```

Receive the message from the environment, and save it for later retrieval.

**Arguments**:

- `msg`:

**Returns**:



<a name="mlagents_envs.side_channel.stats_side_channel.StatsSideChannel.get_and_reset_stats"></a>
#### get\_and\_reset\_stats

```python
 | get_and_reset_stats() -> EnvironmentStats
```

Returns the current stats, and resets the internal storage of the stats.

**Returns**:



<a name="mlagents_envs.side_channel.incoming_message"></a>
# mlagents\_envs.side\_channel.incoming\_message

<a name="mlagents_envs.side_channel.incoming_message.IncomingMessage"></a>
## IncomingMessage Objects

```python
class IncomingMessage()
```

Utility class for reading the message written to a SideChannel. Values must be read in the order they were written.

<a name="mlagents_envs.side_channel.incoming_message.IncomingMessage.__init__"></a>
#### \_\_init\_\_

```python
 | __init__(buffer: bytes, offset: int = 0)
```

Create a new IncomingMessage from the bytes.

<a name="mlagents_envs.side_channel.incoming_message.IncomingMessage.read_bool"></a>
#### read\_bool

```python
 | read_bool(default_value: bool = False) -> bool
```

Read a boolean value from the message buffer.

**Arguments**:

- `default_value`: Default value to use if the end of the message is reached.

**Returns**:

The value read from the message, or the default value if the end was reached.

<a name="mlagents_envs.side_channel.incoming_message.IncomingMessage.read_int32"></a>
#### read\_int32

```python
 | read_int32(default_value: int = 0) -> int
```

Read an integer value from the message buffer.

**Arguments**:

- `default_value`: Default value to use if the end of the message is reached.

**Returns**:

The value read from the message, or the default value if the end was reached.

<a name="mlagents_envs.side_channel.incoming_message.IncomingMessage.read_float32"></a>
#### read\_float32

```python
 | read_float32(default_value: float = 0.0) -> float
```

Read a float value from the message buffer.

**Arguments**:

- `default_value`: Default value to use if the end of the message is reached.

**Returns**:

The value read from the message, or the default value if the end was reached.

<a name="mlagents_envs.side_channel.incoming_message.IncomingMessage.read_float32_list"></a>
#### read\_float32\_list

```python
 | read_float32_list(default_value: List[float] = None) -> List[float]
```

Read a list of float values from the message buffer.

**Arguments**:

- `default_value`: Default value to use if the end of the message is reached.

**Returns**:

The value read from the message, or the default value if the end was reached.

<a name="mlagents_envs.side_channel.incoming_message.IncomingMessage.read_string"></a>
#### read\_string

```python
 | read_string(default_value: str = "") -> str
```

Read a string value from the message buffer.

**Arguments**:

- `default_value`: Default value to use if the end of the message is reached.

**Returns**:

The value read from the message, or the default value if the end was reached.

<a name="mlagents_envs.side_channel.incoming_message.IncomingMessage.get_raw_bytes"></a>
#### get\_raw\_bytes

```python
 | get_raw_bytes() -> bytes
```

Get a copy of the internal bytes used by the message.

<a name="mlagents_envs.side_channel.float_properties_channel"></a>
# mlagents\_envs.side\_channel.float\_properties\_channel

<a name="mlagents_envs.side_channel.float_properties_channel.FloatPropertiesChannel"></a>
## FloatPropertiesChannel Objects

```python
class FloatPropertiesChannel(SideChannel)
```

This is the SideChannel for float properties shared with Unity. You can modify the float properties of an environment with the commands set_property, get_property and list_properties.

<a name="mlagents_envs.side_channel.float_properties_channel.FloatPropertiesChannel.on_message_received"></a>
#### on\_message\_received

```python
 | on_message_received(msg: IncomingMessage) -> None
```

Is called by the environment to the side channel. Can be called multiple times per step if multiple messages are meant for that SideChannel.

<a name="mlagents_envs.side_channel.float_properties_channel.FloatPropertiesChannel.set_property"></a>
#### set\_property

```python
 | set_property(key: str, value: float) -> None
```

Sets a property in the Unity Environment.

**Arguments**:

- `key`: The string identifier of the property.
- `value`: The float value of the property.

<a name="mlagents_envs.side_channel.float_properties_channel.FloatPropertiesChannel.get_property"></a>
#### get\_property

```python
 | get_property(key: str) -> Optional[float]
```

Gets a property in the Unity Environment. If the property was not found, will return None.

**Arguments**:

- `key`: The string identifier of the property.

**Returns**:

The float value of the property or None.

<a name="mlagents_envs.side_channel.float_properties_channel.FloatPropertiesChannel.list_properties"></a>
#### list\_properties

```python
 | list_properties() -> List[str]
```

Returns a list of all the string identifiers of the properties currently present in the Unity Environment.

<a name="mlagents_envs.side_channel.float_properties_channel.FloatPropertiesChannel.get_property_dict_copy"></a>
#### get\_property\_dict\_copy

```python
 | get_property_dict_copy() -> Dict[str, float]
```

Returns a copy of the float properties.

**Returns**:



<a name="mlagents_envs.side_channel.environment_parameters_channel"></a>
# mlagents\_envs.side\_channel.environment\_parameters\_channel

<a name="mlagents_envs.side_channel.environment_parameters_channel.EnvironmentParametersChannel"></a>
## EnvironmentParametersChannel Objects

```python
class EnvironmentParametersChannel(SideChannel)
```

This is the SideChannel for sending environment parameters to Unity. You can send parameters to an environment with the command set_float_parameter.

<a name="mlagents_envs.side_channel.environment_parameters_channel.EnvironmentParametersChannel.set_float_parameter"></a>
#### set\_float\_parameter

```python
 | set_float_parameter(key: str, value: float) -> None
```

Sets a float environment parameter in the Unity Environment.

**Arguments**:

- `key`: The string identifier of the parameter.
- `value`: The float value of the parameter.

<a name="mlagents_envs.side_channel.environment_parameters_channel.EnvironmentParametersChannel.set_uniform_sampler_parameters"></a>
#### set\_uniform\_sampler\_parameters

```python
 | set_uniform_sampler_parameters(key: str, min_value: float, max_value: float, seed: int) -> None
```

Sets a uniform environment parameter sampler.

**Arguments**:

- `key`: The string identifier of the parameter.
- `min_value`: The minimum of the sampling distribution.
- `max_value`: The maximum of the sampling distribution.
- `seed`: The random seed to initialize the sampler.

<a name="mlagents_envs.side_channel.environment_parameters_channel.EnvironmentParametersChannel.set_gaussian_sampler_parameters"></a>
#### set\_gaussian\_sampler\_parameters

```python
 | set_gaussian_sampler_parameters(key: str, mean: float, st_dev: float, seed: int) -> None
```

Sets a gaussian environment parameter sampler.

**Arguments**:

- `key`: The string identifier of the parameter.
- `mean`: The mean of the sampling distribution.
- `st_dev`: The standard deviation of the sampling distribution.
- `seed`: The random seed to initialize the sampler.

<a name="mlagents_envs.side_channel.environment_parameters_channel.EnvironmentParametersChannel.set_multirangeuniform_sampler_parameters"></a>
#### set\_multirangeuniform\_sampler\_parameters

```python
 | set_multirangeuniform_sampler_parameters(key: str, intervals: List[Tuple[float, float]], seed: int) -> None
```

Sets a multirangeuniform environment parameter sampler.

**Arguments**:

- `key`: The string identifier of the parameter.
- `intervals`: The lists of min and max that define each uniform distribution.
- `seed`: The random seed to initialize the sampler.

<a name="mlagents_envs.side_channel.side_channel"></a>
# mlagents\_envs.side\_channel.side\_channel

<a name="mlagents_envs.side_channel.side_channel.SideChannel"></a>
## SideChannel Objects

```python
class SideChannel(ABC)
```

The side channel just get access to a bytes buffer that will be shared between C# and Python. For example, We will create a specific side channel for properties that will be a list of string (fixed size) to float number, that can be modified by both C# and Python. All side channels are passed to the Env object at construction.

<a name="mlagents_envs.side_channel.side_channel.SideChannel.queue_message_to_send"></a>
#### queue\_message\_to\_send

```python
 | queue_message_to_send(msg: OutgoingMessage) -> None
```

Queues a message to be sent by the environment at the next call to step.

<a name="mlagents_envs.side_channel.side_channel.SideChannel.on_message_received"></a>
#### on\_message\_received

```python
 | @abstractmethod
 | on_message_received(msg: IncomingMessage) -> None
```

Is called by the environment to the side channel. Can be called multiple times per step if multiple messages are meant for that SideChannel.

<a name="mlagents_envs.side_channel.side_channel.SideChannel.channel_id"></a>
#### channel\_id

```python
 | @property
 | channel_id() -> uuid.UUID
```

**Returns**:

The type of side channel used. Will influence how the data is processed in the environment.


--- com.unity.ml-agents/Documentation~/Python-PettingZoo-API.md ---
# Unity ML-Agents PettingZoo Wrapper

With the increasing interest in multi-agent training with a gym-like API, we provide a PettingZoo Wrapper around the [Petting Zoo API](https://pettingzoo.farama.org/). Our wrapper provides interfaces on top of our `UnityEnvironment` class, which is the default way of interfacing with a Unity environment via Python.

## Installation and Examples

The PettingZoo wrapper is part of the `mlagents_envs` package. Please refer to the [mlagents_envs installation instructions](https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/ml-agents-envs/README.md).

[[Colab] PettingZoo Wrapper Example](https://colab.research.google.com/github/Unity-Technologies/ml-agents/blob/develop-python-api-ga/ml-agents-envs/colabs/Colab_PettingZoo.ipynb)

This colab notebook demonstrates the example usage of the wrapper, including installation, basic usages, and an example with our [Striker vs Goalie environment](Learning-Environment-Examples.md#strikers-vs-goalie) which is a multi-agents environment with multiple different behavior names.

## API interface

This wrapper is compatible with PettingZoo API. Please check out [PettingZoo API page](https://pettingzoo.farama.org/api/aec/) for more details. Here's an example of interacting with wrapped environment:

```python
from mlagents_envs.environment import UnityEnvironment
from mlagents_envs.envs import UnityToPettingZooWrapper

unity_env = UnityEnvironment("StrikersVsGoalie")
env = UnityToPettingZooWrapper(unity_env)
env.reset()
for agent in env.agent_iter():
    observation, reward, done, info = env.last()
    action = policy(observation, agent)
    env.step(action)
```

## Notes
- There is support for both [AEC](https://pettingzoo.farama.org/api/aec/) and [Parallel](https://pettingzoo.farama.org/api/parallel/) PettingZoo APIs.
- The AEC wrapper is compatible with PettingZoo (PZ) API interface but works in a slightly different way under the hood. For the AEC API, Instead of stepping the environment in every `env.step(action)`, the PZ wrapper will store the action, and will only perform environment stepping when all the agents requesting for actions in the current step have been assigned an action. This is for performance, considering that the communication between Unity and python is more efficient when data are sent in batches.
- Since the actions for the AEC wrapper are stored without applying them to the environment until all the actions are queued, some components of the API might behave in unexpected way. For example, a call to `env.reward` should return the instantaneous reward for that particular step, but the true reward would only be available when an actual environment step is performed. It's recommended that you follow the API definition for training (access rewards from `env.last()` instead of `env.reward`) and the underlying mechanism shouldn't affect training results.
- The environments will automatically reset when it's done, so `env.agent_iter(max_step)` will keep going on until the specified max step is reached (default: `2**63`). There is no need to call `env.reset()` except for the very beginning of instantiating an environment.


## Links discovered
- [Petting Zoo API](https://pettingzoo.farama.org/)
- [mlagents_envs installation instructions](https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/ml-agents-envs/README.md)
- [Striker vs Goalie environment](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Examples.md#strikers-vs-goalie)
- [PettingZoo API page](https://pettingzoo.farama.org/api/aec/)
- [AEC](https://pettingzoo.farama.org/api/aec/)
- [Parallel](https://pettingzoo.farama.org/api/parallel/)

--- com.unity.ml-agents/Documentation~/Python-PettingZoo-API-Documentation.md ---
# Python PettingZoo API Documentation

<a name="mlagents_envs.envs.pettingzoo_env_factory"></a>
# mlagents\_envs.envs.pettingzoo\_env\_factory

<a name="mlagents_envs.envs.pettingzoo_env_factory.PettingZooEnvFactory"></a>
## PettingZooEnvFactory Objects

```python
class PettingZooEnvFactory()
```

<a name="mlagents_envs.envs.pettingzoo_env_factory.PettingZooEnvFactory.env"></a>
#### env

```python
 | env(seed: Optional[int] = None, **kwargs: Union[List, int, bool, None]) -> UnityAECEnv
```

Creates the environment with env_id from unity's default_registry and wraps it in a UnityToPettingZooWrapper

**Arguments**:

- `seed`: The seed for the action spaces of the agents.
- `kwargs`: Any argument accepted by `UnityEnvironment`class except file_name

<a name="mlagents_envs.envs.unity_aec_env"></a>
# mlagents\_envs.envs.unity\_aec\_env

<a name="mlagents_envs.envs.unity_aec_env.UnityAECEnv"></a>
## UnityAECEnv Objects

```python
class UnityAECEnv(UnityPettingzooBaseEnv,  AECEnv)
```

Unity AEC (PettingZoo) environment wrapper.

<a name="mlagents_envs.envs.unity_aec_env.UnityAECEnv.__init__"></a>
#### \_\_init\_\_

```python
 | __init__(env: BaseEnv, seed: Optional[int] = None)
```

Initializes a Unity AEC environment wrapper.

**Arguments**:

- `env`: The UnityEnvironment that is being wrapped.
- `seed`: The seed for the action spaces of the agents.

<a name="mlagents_envs.envs.unity_aec_env.UnityAECEnv.step"></a>
#### step

```python
 | step(action: Any) -> None
```

Sets the action of the active agent and get the observation, reward, done and info of the next agent.

**Arguments**:

- `action`: The action for the active agent

<a name="mlagents_envs.envs.unity_aec_env.UnityAECEnv.observe"></a>
#### observe

```python
 | observe(agent_id)
```

Returns the observation an agent currently can make. `last()` calls this function.

<a name="mlagents_envs.envs.unity_aec_env.UnityAECEnv.last"></a>
#### last

```python
 | last(observe=True)
```

returns observation, cumulative reward, done, info for the current agent (specified by self.agent_selection)

<a name="mlagents_envs.envs.unity_parallel_env"></a>
# mlagents\_envs.envs.unity\_parallel\_env

<a name="mlagents_envs.envs.unity_parallel_env.UnityParallelEnv"></a>
## UnityParallelEnv Objects

```python
class UnityParallelEnv(UnityPettingzooBaseEnv,  ParallelEnv)
```

Unity Parallel (PettingZoo) environment wrapper.

<a name="mlagents_envs.envs.unity_parallel_env.UnityParallelEnv.__init__"></a>
#### \_\_init\_\_

```python
 | __init__(env: BaseEnv, seed: Optional[int] = None)
```

Initializes a Unity Parallel environment wrapper.

**Arguments**:

- `env`: The UnityEnvironment that is being wrapped.
- `seed`: The seed for the action spaces of the agents.

<a name="mlagents_envs.envs.unity_parallel_env.UnityParallelEnv.reset"></a>
#### reset

```python
 | reset() -> Dict[str, Any]
```

Resets the environment.

<a name="mlagents_envs.envs.unity_pettingzoo_base_env"></a>
# mlagents\_envs.envs.unity\_pettingzoo\_base\_env

<a name="mlagents_envs.envs.unity_pettingzoo_base_env.UnityPettingzooBaseEnv"></a>
## UnityPettingzooBaseEnv Objects

```python
class UnityPettingzooBaseEnv()
```

Unity Petting Zoo base environment.

<a name="mlagents_envs.envs.unity_pettingzoo_base_env.UnityPettingzooBaseEnv.observation_spaces"></a>
#### observation\_spaces

```python
 | @property
 | observation_spaces() -> Dict[str, spaces.Space]
```

Return the observation spaces of all the agents.

<a name="mlagents_envs.envs.unity_pettingzoo_base_env.UnityPettingzooBaseEnv.observation_space"></a>
#### observation\_space

```python
 | observation_space(agent: str) -> Optional[spaces.Space]
```

The observation space of the current agent.

<a name="mlagents_envs.envs.unity_pettingzoo_base_env.UnityPettingzooBaseEnv.action_spaces"></a>
#### action\_spaces

```python
 | @property
 | action_spaces() -> Dict[str, spaces.Space]
```

Return the action spaces of all the agents.

<a name="mlagents_envs.envs.unity_pettingzoo_base_env.UnityPettingzooBaseEnv.action_space"></a>
#### action\_space

```python
 | action_space(agent: str) -> Optional[spaces.Space]
```

The action space of the current agent.

<a name="mlagents_envs.envs.unity_pettingzoo_base_env.UnityPettingzooBaseEnv.side_channel"></a>
#### side\_channel

```python
 | @property
 | side_channel() -> Dict[str, Any]
```

The side channels of the environment. You can access the side channels of an environment with `env.side_channel[<name-of-channel>]`.

<a name="mlagents_envs.envs.unity_pettingzoo_base_env.UnityPettingzooBaseEnv.reset"></a>
#### reset

```python
 | reset()
```

Resets the environment.

<a name="mlagents_envs.envs.unity_pettingzoo_base_env.UnityPettingzooBaseEnv.seed"></a>
#### seed

```python
 | seed(seed=None)
```

Reseeds the environment (making the resulting environment deterministic).
`reset()` must be called after `seed()`, and before `step()`.

<a name="mlagents_envs.envs.unity_pettingzoo_base_env.UnityPettingzooBaseEnv.render"></a>
#### render

```python
 | render(mode="human")
```

NOT SUPPORTED.

Displays a rendered frame from the environment, if supported. Alternate render modes in the default environments are `'rgb_array'` which returns a numpy array and is supported by all environments outside of classic, and `'ansi'` which returns the strings printed (specific to classic environments).

<a name="mlagents_envs.envs.unity_pettingzoo_base_env.UnityPettingzooBaseEnv.close"></a>
#### close

```python
 | close() -> None
```

Close the environment.


--- com.unity.ml-agents/Documentation~/Reference-Support.md ---
# Reference & Support

The Reference & Support section contains essential documentation for ongoing ML-Agents development.


| **Resource**                               | **Description**                                              |
|--------------------------------------------|--------------------------------------------------------------|
| [FAQ](FAQ.md)                              | Frequently asked questions and common issues with solutions. |
| [Limitations](Limitations.md)              | Known limitations and constraints of the ML-Agents Toolkit.  |
| [Migrating](Migrating.md)                  | Migration guides for updating between ML-Agents versions.    |
| [Versioning](Versioning.md)                | Information about ML-Agents versioning and release notes.    |
| [ML-Agents Glossary](Glossary.md)          | Glossary of terms and concepts used in ML-Agents.            |
| [Contribution guidelines](CONTRIBUTING.md) | How to Contribute to ML-Agents.                              |


## Links discovered
- [FAQ](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/FAQ.md)
- [Limitations](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Limitations.md)
- [Migrating](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Migrating.md)
- [Versioning](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Versioning.md)
- [ML-Agents Glossary](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Glossary.md)
- [Contribution guidelines](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/CONTRIBUTING.md)

--- ml-agents/tests/yamato/scripts/run_llapi.py ---
import argparse

from mlagents_envs.environment import UnityEnvironment
from mlagents_envs.side_channel.engine_configuration_channel import (
    EngineConfigurationChannel,
)


def test_run_environment(env_name):
    """
    Run the low-level API test using the specified environment
    :param env_name: Name of the Unity environment binary to launch
    """
    engine_configuration_channel = EngineConfigurationChannel()
    env = UnityEnvironment(
        file_name=env_name,
        side_channels=[engine_configuration_channel],
        no_graphics=True,
        additional_args=["-logFile", "-"],
    )

    try:
        # Reset the environment
        env.reset()

        # Set the default brain to work with
        group_name = list(env.behavior_specs.keys())[0]
        group_spec = env.behavior_specs[group_name]

        # Set the time scale of the engine
        engine_configuration_channel.set_configuration_parameters(time_scale=3.0)

        # Get the state of the agents
        decision_steps, terminal_steps = env.get_steps(group_name)

        # Examine the number of observations per Agent
        print("Number of observations : ", len(group_spec.observation_specs))

        for obs_spec in group_spec.observation_specs:
            # Make sure the name was set in the ObservationSpec
            assert bool(obs_spec.name) is True, f'obs_spec.name="{obs_spec.name}"'

        # Is there a visual observation ?
        vis_obs = any(
            len(obs_spec.shape) == 3 for obs_spec in group_spec.observation_specs
        )
        print("Is there a visual observation ?", vis_obs)

        # Examine the state space for the first observation for the first agent
        print(f"First Agent observation looks like: \n{decision_steps.obs[0][0]}")

        for _episode in range(10):
            env.reset()
            decision_steps, terminal_steps = env.get_steps(group_name)
            done = False
            episode_rewards = 0
            tracked_agent = -1
            while not done:
                action_tuple = group_spec.action_spec.random_action(len(decision_steps))
                if tracked_agent == -1 and len(decision_steps) >= 1:
                    tracked_agent = decision_steps.agent_id[0]
                env.set_actions(group_name, action_tuple)
                env.step()
                decision_steps, terminal_steps = env.get_steps(group_name)
                done = False
                if tracked_agent in decision_steps:
                    episode_rewards += decision_steps[tracked_agent].reward
                if tracked_agent in terminal_steps:
                    episode_rewards += terminal_steps[tracked_agent].reward
                    done = True
            print(f"Total reward this episode: {episode_rewards}")
    finally:
        env.close()


def test_closing(env_name):
    """
    Run the low-level API and close the environment
    :param env_name: Name of the Unity environment binary to launch
    """
    try:
        env1 = UnityEnvironment(
            file_name=env_name,
            base_port=5006,
            no_graphics=True,
            additional_args=["-logFile", "-"],
        )
        env1.close()
        env1 = UnityEnvironment(
            file_name=env_name,
            base_port=5006,
            no_graphics=True,
            additional_args=["-logFile", "-"],
        )
        env2 = UnityEnvironment(
            file_name=env_name,
            base_port=5007,
            no_graphics=True,
            additional_args=["-logFile", "-"],
        )
        env2.reset()
    finally:
        env1.close()
        env2.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--env", default="artifacts/testPlayer")
    args = parser.parse_args()
    test_run_environment(args.env)
    test_closing(args.env)


--- ml-agents/mlagents/trainers/tests/torch_entities/test_bcmodule.py ---
import os
from typing import Dict, Any
from unittest.mock import MagicMock
import pytest
import mlagents.trainers.tests.mock_brain as mb
from mlagents.trainers.policy.torch_policy import TorchPolicy
from mlagents.trainers.torch_entities.components.bc.module import BCModule
from mlagents.trainers.torch_entities.networks import SimpleActor
from mlagents.trainers.settings import (
    TrainerSettings,
    BehavioralCloningSettings,
    NetworkSettings,
)


def create_bc_module(mock_behavior_specs, bc_settings, use_rnn, tanhresample):
    # model_path = env.external_brain_names[0]
    trainer_config = TrainerSettings()
    trainer_config.network_settings.memory = (
        NetworkSettings.MemorySettings() if use_rnn else None
    )
    actor_kwargs: Dict[str, Any] = {
        "conditional_sigma": False,
        "tanh_squash": tanhresample,
    }
    policy = TorchPolicy(
        0,
        mock_behavior_specs,
        trainer_config.network_settings,
        SimpleActor,
        actor_kwargs,
    )
    bc_module = BCModule(
        policy,
        settings=bc_settings,
        policy_learning_rate=trainer_config.hyperparameters.learning_rate,
        default_batch_size=trainer_config.hyperparameters.batch_size,
        default_num_epoch=3,
    )
    return bc_module


def assert_stats_are_float(stats):
    for _, item in stats.items():
        assert isinstance(item, float)


# Test default values
def test_bcmodule_defaults():
    # See if default values match
    mock_specs = mb.create_mock_3dball_behavior_specs()
    bc_settings = BehavioralCloningSettings(
        demo_path=os.path.dirname(os.path.abspath(__file__)) + "/" + "test.demo"
    )
    bc_module = create_bc_module(mock_specs, bc_settings, False, False)
    assert bc_module.num_epoch == 3
    assert bc_module.batch_size == TrainerSettings().hyperparameters.batch_size
    # Assign strange values and see if it overrides properly
    bc_settings = BehavioralCloningSettings(
        demo_path=os.path.dirname(os.path.abspath(__file__)) + "/" + "test.demo",
        num_epoch=100,
        batch_size=10000,
    )
    bc_module = create_bc_module(mock_specs, bc_settings, False, False)
    assert bc_module.num_epoch == 100
    assert bc_module.batch_size == 10000


# Test with continuous control env and vector actions
@pytest.mark.parametrize("is_sac", [True, False], ids=["sac", "ppo"])
def test_bcmodule_update(is_sac):
    mock_specs = mb.create_mock_3dball_behavior_specs()
    bc_settings = BehavioralCloningSettings(
        demo_path=os.path.dirname(os.path.abspath(__file__)) + "/" + "test.demo"
    )
    bc_module = create_bc_module(mock_specs, bc_settings, False, is_sac)
    stats = bc_module.update()
    assert_stats_are_float(stats)


# Test with constant pretraining learning rate
@pytest.mark.parametrize("is_sac", [True, False], ids=["sac", "ppo"])
def test_bcmodule_constant_lr_update(is_sac):
    mock_specs = mb.create_mock_3dball_behavior_specs()
    bc_settings = BehavioralCloningSettings(
        demo_path=os.path.dirname(os.path.abspath(__file__)) + "/" + "test.demo",
        steps=0,
    )
    bc_module = create_bc_module(mock_specs, bc_settings, False, is_sac)
    stats = bc_module.update()
    assert_stats_are_float(stats)
    old_learning_rate = bc_module.current_lr

    _ = bc_module.update()
    assert old_learning_rate == bc_module.current_lr


# Test with constant pretraining learning rate
@pytest.mark.parametrize("is_sac", [True, False], ids=["sac", "ppo"])
def test_bcmodule_linear_lr_update(is_sac):
    mock_specs = mb.create_mock_3dball_behavior_specs()
    bc_settings = BehavioralCloningSettings(
        demo_path=os.path.dirname(os.path.abspath(__file__)) + "/" + "test.demo",
        steps=100,
    )
    bc_module = create_bc_module(mock_specs, bc_settings, False, is_sac)
    # Should decay by 10/100 * 0.0003 = 0.00003
    bc_module.policy.get_current_step = MagicMock(return_value=10)
    old_learning_rate = bc_module.current_lr
    _ = bc_module.update()
    assert old_learning_rate - 0.00003 == pytest.approx(bc_module.current_lr, abs=0.01)


# Test with RNN
@pytest.mark.parametrize("is_sac", [True, False], ids=["sac", "ppo"])
def test_bcmodule_rnn_update(is_sac):
    mock_specs = mb.create_mock_3dball_behavior_specs()
    bc_settings = BehavioralCloningSettings(
        demo_path=os.path.dirname(os.path.abspath(__file__)) + "/" + "test.demo"
    )
    bc_module = create_bc_module(mock_specs, bc_settings, True, is_sac)
    stats = bc_module.update()
    assert_stats_are_float(stats)


# Test with hybrid control and visual observations
@pytest.mark.parametrize("is_sac", [True, False], ids=["sac", "ppo"])
def test_bcmodule_hybrid_visual_updates(is_sac):
    mock_specs = mb.create_visual_food_collector_specs()
    bc_settings = BehavioralCloningSettings(
        demo_path=os.path.dirname(os.path.abspath(__file__))
        + "/"
        + "testhybridvis.demo"
    )
    bc_module = create_bc_module(mock_specs, bc_settings, False, is_sac)
    stats = bc_module.update()
    assert_stats_are_float(stats)


# Test with hybrid control, visual observations and rnn
@pytest.mark.parametrize("is_sac", [True, False], ids=["sac", "ppo"])
def test_bcmodule_rnn_hybrid_update(is_sac):
    mock_specs = mb.create_visual_food_collector_specs()
    bc_settings = BehavioralCloningSettings(
        demo_path=os.path.dirname(os.path.abspath(__file__))
        + "/"
        + "testhybridvis.demo"
    )
    bc_module = create_bc_module(mock_specs, bc_settings, True, is_sac)
    stats = bc_module.update()
    assert_stats_are_float(stats)


if __name__ == "__main__":
    pytest.main()


--- com.unity.ml-agents/Documentation~/FAQ.md ---
# Frequently Asked Questions

## Installation problems

## Environment Permission Error

If you directly import your Unity environment without building it in the editor, you might need to give it additional permissions to execute it.

If you receive such a permission error on macOS, run:

```sh
chmod -R 755 *.app
```

or on Linux:

```sh
chmod -R 755 *.x86_64
```

On Windows, you can find [instructions](https://docs.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2008-R2-and-2008/cc754344(v=ws.11)).

## Environment Connection Timeout

If you are able to launch the environment from `UnityEnvironment` but then receive a timeout error like this:

```
UnityAgentsException: The Communicator was unable to connect. Please make sure the External process is ready to accept communication with Unity.
```

There may be a number of possible causes:

- _Cause_: There may be no agent in the scene
- _Cause_: On OSX, the firewall may be preventing communication with the environment. _Solution_: Add the built environment binary to the list of exceptions on the firewall by following [instructions](https://support.apple.com/en-us/HT201642).
- _Cause_: An error happened in the Unity Environment preventing communication. _Solution_: Look into the [log files](https://docs.unity3d.com/Manual/LogFiles.html) generated by the Unity Environment to figure what error happened.
- _Cause_: You have assigned `HTTP_PROXY` and `HTTPS_PROXY` values in your environment variables. _Solution_: Remove these values and try again.
- _Cause_: You are running in a headless environment (e.g. remotely connected to a server). _Solution_: Pass `--no-graphics` to `mlagents-learn`, or `no_graphics=True` to `RemoteRegistryEntry.make()` or the `UnityEnvironment` initializer. If you need graphics for visual observations, you will need to set up `xvfb` (or equivalent).

## Communication port {} still in use

If you receive an exception `"Couldn't launch new environment because communication port {} is still in use. "`, you can change the worker number in the Python script when calling

```python
UnityEnvironment(file_name=filename, worker_id=X)
```

## Mean reward : nan

If you receive a message `Mean reward : nan` when attempting to train a model using PPO, this is due to the episodes of the Learning Environment not terminating. In order to address this, set `Max Steps` for the Agents within the Scene Inspector to a value greater than 0. Alternatively, it is possible to manually set `done` conditions for episodes from within scripts for custom episode-terminating events.

## "File name" cannot be opened because the developer cannot be verified.

If you have downloaded the repository using the github website on macOS 10.15 (Catalina) or later, you may see this error when attempting to play scenes in the Unity project. Workarounds include installing the package using the Unity Package Manager (this is the officially supported approach - see [here](Installation.md)), or following the instructions [here](https://support.apple.com/en-us/HT202491) to verify the relevant files on your machine on a file-by-file basis.


## Links discovered
- [instructions](https://docs.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2008-R2-and-2008/cc754344(v=ws.11)
- [instructions](https://support.apple.com/en-us/HT201642)
- [log files](https://docs.unity3d.com/Manual/LogFiles.html)
- [here](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Installation.md)
- [here](https://support.apple.com/en-us/HT202491)

--- com.unity.ml-agents/Documentation~/Learning-Environment-Design.md ---
# Designing a Learning Environment

This page contains general advice on how to design your learning environment, in addition to overviewing aspects of the ML-Agents Unity SDK that pertain to setting up your scene and simulation as opposed to designing your agents within the scene. We have a dedicated page on [Designing Agents](Learning-Environment-Design-Agents.md) which includes how to instrument observations, actions and rewards, define teams for multi-agent scenarios and record agent demonstrations for imitation learning.

## The Simulation and Training Process

Training and simulation proceed in steps orchestrated by the ML-Agents Academy class. The Academy works with Agent objects in the scene to step through the simulation.

During training, the external Python training process communicates with the Academy to run a series of episodes while it collects data and optimizes its neural network model. When training is completed successfully, you can add the trained model file to your Unity project for later use.

The ML-Agents Academy class orchestrates the agent simulation loop as follows:

1. Calls your Academy's `OnEnvironmentReset` delegate.
2. Calls the `OnEpisodeBegin()` function for each Agent in the scene.
3. Gathers information about the scene. This is done by calling the `CollectObservations(VectorSensor sensor)` function for each Agent in the scene, as well as updating their sensor and collecting the resulting observations.
4. Uses each Agent's Policy to decide on the Agent's next action.
5. Calls the `OnActionReceived()` function for each Agent in the scene, passing in the action chosen by the Agent's Policy.
6. Calls the Agent's `OnEpisodeBegin()` function if the Agent has reached its `Max Step` count or has otherwise marked itself as `EndEpisode()`.

To create a training environment, extend the Agent class to implement the above methods whether you need to implement them or not depends on your specific scenario.

## Organizing the Unity Scene

To train and use the ML-Agents Toolkit in a Unity scene, the scene as many Agent subclasses as you need. Agent instances should be attached to the GameObject representing that Agent.

### Academy

The Academy is a singleton which orchestrates Agents and their decision making processes. Only a single Academy exists at a time.

#### Academy resetting

To alter the environment at the start of each episode, add your method to the Academy's OnEnvironmentReset action.

```csharp
public class MySceneBehavior : MonoBehaviour
{
    public void Awake()
    {
        Academy.Instance.OnEnvironmentReset += EnvironmentReset;
    }

    void EnvironmentReset()
    {
        // Reset the scene here
    }
}
```

For example, you might want to reset an Agent to its starting position or move a goal to a random position. An environment resets when the `reset()` method is called on the Python `UnityEnvironment`.

When you reset an environment, consider the factors that should change so that training is generalizable to different conditions. For example, if you were training a maze-solving agent, you would probably want to change the maze itself for each training episode. Otherwise, the agent would probably on learn to solve one, particular maze, not mazes in general.

### Multiple Areas

In many of the example environments, many copies of the training area are instantiated in the scene. This generally speeds up training, allowing the environment to gather many experiences in parallel. This can be achieved simply by instantiating many Agents with the same Behavior Name. If possible, consider designing your scene to support multiple areas.

Check out our example environments to see examples of multiple areas. Additionally, the [Making a New Learning Environment](Learning-Environment-Create-New.md#optional-multiple-training-areas-within-the-same-scene) guide demonstrates this option.

## Environments

When you create a training environment in Unity, you must set up the scene so that it can be controlled by the external training process. Considerations include:

- The training scene must start automatically when your Unity application is launched by the training process.
- The Academy must reset the scene to a valid starting point for each episode of training.
- A training episode must have a definite end — either using `Max Steps` or by each Agent ending its episode manually with `EndEpisode()`.

## Environment Parameters

Curriculum learning and environment parameter randomization are two training methods that control specific parameters in your environment. As such, it is important to ensure that your environment parameters are updated at each step to the correct values. To enable this, we expose a `EnvironmentParameters` C# class that you can use to retrieve the values of the parameters defined in the training configurations for both of those features. Please see our [documentation](Training-ML-Agents.md#environment-parameters) for curriculum learning and environment parameter randomization for details.

We recommend modifying the environment from the Agent's `OnEpisodeBegin()` function by leveraging `Academy.Instance.EnvironmentParameters`. See the WallJump example environment for a sample usage (specifically, [WallJumpAgent.cs](https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/Project/Assets/ML-Agents/Examples/WallJump/Scripts/WallJumpAgent.cs) ).

## Agent

The Agent class represents an actor in the scene that collects observations and carries out actions. The Agent class is typically attached to the GameObject in the scene that otherwise represents the actor — for example, to a player object in a football game or a car object in a vehicle simulation. Every Agent must have appropriate `Behavior Parameters`.

Generally, when creating an Agent, you should extend the Agent class and implement the `CollectObservations(VectorSensor sensor)` and `OnActionReceived()` methods:

- `CollectObservations(VectorSensor sensor)` — Collects the Agent's observation of its environment.
- `OnActionReceived()` — Carries out the action chosen by the Agent's Policy and assigns a reward to the current state.

Your implementations of these functions determine how the Behavior Parameters assigned to this Agent must be set.

You must also determine how an Agent finishes its task or times out. You can manually terminate an Agent episode in your `OnActionReceived()` function when the Agent has finished (or irrevocably failed) its task by calling the `EndEpisode()` function. You can also set the Agent's `Max Steps` property to a positive value and the Agent will consider the episode over after it has taken that many steps. You can use the `Agent.OnEpisodeBegin()` function to prepare the Agent to start again.

See [Agents](Learning-Environment-Design-Agents.md) for detailed information about programming your own Agents.

## Recording Statistics

We offer developers a mechanism to record statistics from within their Unity environments. These statistics are aggregated and generated during the training process. To record statistics, see the `StatsRecorder` C# class.

See the FoodCollector example environment for a sample usage (specifically, [FoodCollectorSettings.cs](https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/Project/Assets/ML-Agents/Examples/FoodCollector/Scripts/FoodCollectorSettings.cs) ).


## Links discovered
- [Designing Agents](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Design-Agents.md)
- [Making a New Learning Environment](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Create-New.md#optional-multiple-training-areas-within-the-same-scene)
- [documentation](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Training-ML-Agents.md#environment-parameters)
- [WallJumpAgent.cs](https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/Project/Assets/ML-Agents/Examples/WallJump/Scripts/WallJumpAgent.cs)
- [Agents](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Design-Agents.md)
- [FoodCollectorSettings.cs](https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/Project/Assets/ML-Agents/Examples/FoodCollector/Scripts/FoodCollectorSettings.cs)

--- com.unity.ml-agents/Documentation~/Learning-Environment-Design-Agents.md ---
# Agents

An agent is an entity that can observe its environment, decide on the best course of action using those observations, and execute those actions within its environment. Agents can be created in Unity by extending the `Agent` class. The most important aspects of creating agents that can successfully learn are the observations the agent collects, and the reward you assign to estimate the value of the agent's current state toward accomplishing its tasks.

An Agent passes its observations to its Policy. The Policy then makes a decision and passes the chosen action back to the agent. Your agent code must execute the action, for example, move the agent in one direction or another. In order to train an agent using reinforcement learning, your agent must calculate a reward value at each action. The reward is used to discover the optimal decision-making policy.

The `Policy` class abstracts out the decision making logic from the Agent itself so that you can use the same Policy in multiple Agents. How a Policy makes its decisions depends on the `Behavior Parameters` associated with the agent. If you set `Behavior Type` to `Heuristic Only`, the Agent will use its `Heuristic()` method to make decisions which can allow you to control the Agent manually or write your own Policy. If the Agent has a `Model` file, its Policy will use the neural network `Model` to take decisions.

When you create an Agent, you should usually extend the base Agent class. This includes implementing the following methods:

- `Agent.OnEpisodeBegin()` — Called at the beginning of an Agent's episode, including at the beginning of the simulation.
- `Agent.CollectObservations(VectorSensor sensor)` — Called every step that the Agent requests a decision. This is one possible way for collecting the Agent's observations of the environment; see [Generating Observations](#generating-observations) below for more options.
- `Agent.OnActionReceived()` — Called every time the Agent receives an action to take. Receives the action chosen by the Agent. It is also common to assign a reward in this method.
- `Agent.Heuristic()` - When the `Behavior Type` is set to `Heuristic Only` in the Behavior Parameters of the Agent, the Agent will use the `Heuristic()` method to generate the actions of the Agent. As such, the `Heuristic()` method writes to the array of floats provided to the Heuristic method as argument. __Note__: Do not create a new float array of action in the `Heuristic()` method, as this will prevent writing floats to the original action array.

As a concrete example, here is how the Ball3DAgent class implements these methods:

- `Agent.OnEpisodeBegin()` — Resets the agent cube and ball to their starting positions. The function randomizes the reset values so that the training generalizes to more than a specific starting position and agent cube orientation.
- `Agent.CollectObservations(VectorSensor sensor)` — Adds information about the orientation of the agent cube, the ball velocity, and the relative position between the ball and the cube. Since the  `CollectObservations()` method calls `VectorSensor.AddObservation()` such that vector size adds up to 8, the Behavior Parameters of the Agent are set with vector observation space with a state size of 8.
- `Agent.OnActionReceived()` — The action results in a small change in the agent cube's rotation at each step. In this example, an Agent receives a small positive reward for each step it keeps the ball on the agent cube's head and a larger, negative reward for dropping the ball. An Agent's episode is also ended when it drops the ball so that it will reset with a new ball for the next simulation step.
- `Agent.Heuristic()` - Converts the keyboard inputs into actions.

## Decisions

The observation-decision-action-reward cycle repeats each time the Agent request a decision. Agents will request a decision when `Agent.RequestDecision()` is called. If you need the Agent to request decisions on its own at regular intervals, add a `Decision Requester` component to the Agent's GameObject. Making decisions at regular step intervals is generally most appropriate for physics-based simulations. For example, an agent in a robotic simulator that must provide fine-control of joint torques should make its decisions every step of the simulation. In games such as real-time strategy, where many agents make their decisions at regular intervals, the decision timing for each agent can be staggered by setting the `DecisionStep` parameter in the `Decision Requester` component for each agent. On the other hand, an agent that only needs to make decisions when certain game or simulation events occur, such as in a turn-based game, should call `Agent.RequestDecision()` manually.

## Observations and Sensors
In order for an agent to learn, the observations should include all the information an agent needs to accomplish its task. Without sufficient and relevant information, an agent may learn poorly or may not learn at all. A reasonable approach for determining what information should be included is to consider what you would need to calculate an analytical solution to the problem, or what you would expect a human to be able to use to solve the problem.

### Generating Observations
ML-Agents provides multiple ways for an Agent to make observations:
  1. Overriding the `Agent.CollectObservations()` method and passing the observations to the provided `VectorSensor`.
  2. Adding the `[Observable]` attribute to fields and properties on the Agent.
  3. Implementing the `ISensor` interface, using a `SensorComponent` attached to the Agent to create the `ISensor`.

#### Agent.CollectObservations()
Agent.CollectObservations() is best used for aspects of the environment which are numerical and non-visual. The Policy class calls the `CollectObservations(VectorSensor sensor)` method of each Agent. Your implementation of this function must call `VectorSensor.AddObservation` to add vector observations.

The `VectorSensor.AddObservation` method provides a number of overloads for adding common types of data to your observation vector. You can add Integers and booleans directly to the observation vector, as well as some common Unity data types such as `Vector2`, `Vector3`, and `Quaternion`.

For examples of various state observation functions, you can look at the [example environments](Learning-Environment-Examples.md) included in the ML-Agents SDK. For instance, the 3DBall example uses the rotation of the platform, the relative position of the ball, and the velocity of the ball as its state observation.

```csharp
public GameObject ball;

public override void CollectObservations(VectorSensor sensor)
{
    // Orientation of the cube (2 floats)
    sensor.AddObservation(gameObject.transform.rotation.z);
    sensor.AddObservation(gameObject.transform.rotation.x);
    // Relative position of the ball to the cube (3 floats)
    sensor.AddObservation(ball.transform.position - gameObject.transform.position);
    // Velocity of the ball (3 floats)
    sensor.AddObservation(m_BallRb.velocity);
    // 8 floats total
}
```

As an experiment, you can remove the velocity components from the observation and retrain the 3DBall agent. While it will learn to balance the ball reasonably well, the performance of the agent without using velocity is noticeably worse.

The observations passed to `VectorSensor.AddObservation()` must always contain the same number of elements must always be in the same order. If the number of observed entities in an environment can vary, you can pad the calls with zeros for any missing entities in a specific observation, or you can limit an agent's observations to a fixed subset. For example, instead of observing every enemy in an environment, you could only observe the closest five.

Additionally, when you set up an Agent's `Behavior Parameters` in the Unity Editor, you must set the **Vector Observations > Space Size** to equal the number of floats that are written by `CollectObservations()`.

#### Observable Fields and Properties
Another approach is to define the relevant observations as fields or properties on your Agent class, and annotate them with an `ObservableAttribute`. For example, in the Ball3DHardAgent, the difference between positions could be observed by adding a property to the Agent:
```csharp
using Unity.MLAgents.Sensors.Reflection;

public class Ball3DHardAgent : Agent {

    [Observable(numStackedObservations: 9)]
    Vector3 PositionDelta
    {
        get
        {
            return ball.transform.position - gameObject.transform.position;
        }
    }
}
```
`ObservableAttribute` currently supports most basic types (e.g. floats, ints, bools), as well as `Vector2`, `Vector3`, `Vector4`, `Quaternion`, and enums.

The behavior of `ObservableAttribute`s are controlled by the "Observable Attribute Handling" in the Agent's `Behavior Parameters`. The possible values for this are:
 * **Ignore** (default) - All ObservableAttributes on the Agent will be ignored. If there are no ObservableAttributes on the Agent, this will result in the fastest initialization time.
 * **Exclude Inherited** - Only members on the declared class will be examined; members that are inherited are ignored. This is a reasonable tradeoff between performance and flexibility.
 * **Examine All** All members on the class will be examined. This can lead to slower startup times.

"Exclude Inherited" is generally sufficient, but if your Agent inherits from another Agent implementation that has Observable members, you will need to use "Examine All".

Internally, ObservableAttribute uses reflection to determine which members of the Agent have ObservableAttributes, and also uses reflection to access the fields or invoke the properties at runtime. This may be slower than using CollectObservations or an ISensor, although this might not be enough to noticeably affect performance.

**NOTE**: you do not need to adjust the Space Size in the Agent's `Behavior Parameters` when you add `[Observable]` fields or properties to an Agent, since their size can be computed before they are used.

#### ISensor interface and SensorComponents
The `ISensor` interface is generally intended for advanced users. The `Write()` method is used to actually generate the observation, but some other methods such as returning the shape of the observations must also be implemented.

The `SensorComponent` abstract class is used to create the actual `ISensor` at runtime. It must be attached to the same `GameObject` as the `Agent`, or to a child `GameObject`.

There are several SensorComponents provided in the API, including:
- `CameraSensorComponent` - Uses images from a `Camera` as observations.
- `RenderTextureSensorComponent` - Uses the content of a `RenderTexture` as observations.
- `RayPerceptionSensorComponent` - Uses the information from set of ray casts as observations.
- `Match3SensorComponent` - Uses the board of a [Match-3 game](Integrations-Match3.md) as observations.
- `GridSensorComponent` - Uses a set of box queries in a grid shape as observations.

**NOTE**: you do not need to adjust the Space Size in the Agent's `Behavior Parameters` when using `SensorComponents`s.

Internally, both `Agent.CollectObservations` and `[Observable]` attribute use an ISensors to write observations, although this is mostly abstracted from the user.

### Vector Observations
Both `Agent.CollectObservations()` and `ObservableAttribute`s produce vector observations, which are represented at lists of `float`s. `ISensor`s can produce both vector observations and visual observations, which are multi-dimensional arrays of floats.

Below are some additional considerations when dealing with vector observations:

#### One-hot encoding categorical information

Type enumerations should be encoded in the _one-hot_ style. That is, add an element to the feature vector for each element of enumeration, setting the element representing the observed member to one and set the rest to zero. For example, if your enumeration contains \[Sword, Shield, Bow\] and the agent observes that the current item is a Bow, you would add the elements: 0, 0, 1 to the feature vector. The following code example illustrates how to add.

```csharp
enum ItemType { Sword, Shield, Bow, LastItem }
public override void CollectObservations(VectorSensor sensor)
{
    for (int ci = 0; ci < (int)ItemType.LastItem; ci++)
    {
        sensor.AddObservation((int)currentItem == ci ? 1.0f : 0.0f);
    }
}
```

`VectorSensor` also provides a two-argument function `AddOneHotObservation()` as a shortcut for _one-hot_ style observations. The following example is identical to the previous one.

```csharp
enum ItemType { Sword, Shield, Bow, LastItem }
const int NUM_ITEM_TYPES = (int)ItemType.LastItem + 1;

public override void CollectObservations(VectorSensor sensor)
{
    // The first argument is the selection index; the second is the
    // number of possibilities
    sensor.AddOneHotObservation((int)currentItem, NUM_ITEM_TYPES);
}
```

`ObservableAttribute` has built-in support for enums. Note that you don't need the `LastItem` placeholder in this case:
```csharp
enum ItemType { Sword, Shield, Bow }

public class HeroAgent : Agent
{
    [Observable]
    ItemType m_CurrentItem;
}
```

#### Normalization

For the best results when training, you should normalize the components of your feature vector to the range [-1, +1] or [0, 1]. When you normalize the values, the PPO neural network can often converge to a solution faster. Note that it isn't always necessary to normalize to these recommended ranges, but it is considered a best practice when using neural networks. The greater the variation in ranges between the components of your observation, the more likely that training will be affected.

To normalize a value to [0, 1], you can use the following formula:

```csharp
normalizedValue = (currentValue - minValue)/(maxValue - minValue)
```

:warning: For vectors, you should apply the above formula to each component (x, y, and z). Note that this is _not_ the same as using the `Vector3.normalized` property or `Vector3.Normalize()` method in Unity (and similar for `Vector2`).

Rotations and angles should also be normalized. For angles between 0 and 360 degrees, you can use the following formulas:

```csharp
Quaternion rotation = transform.rotation;
Vector3 normalized = rotation.eulerAngles / 180.0f - Vector3.one;  // [-1,1]
Vector3 normalized = rotation.eulerAngles / 360.0f;  // [0,1]
```

For angles that can be outside the range [0,360], you can either reduce the angle, or, if the number of turns is significant, increase the maximum value used in your normalization formula.

#### Stacking
Stacking refers to repeating observations from previous steps as part of a larger observation. For example, consider an Agent that generates these observations in four steps
```
step 1: [0.1]
step 2: [0.2]
step 3: [0.3]
step 4: [0.4]
```

If we use a stack size of 3, the observations would instead be:
```csharp
step 1: [0.1, 0.0, 0.0]
step 2: [0.2, 0.1, 0.0]
step 3: [0.3, 0.2, 0.1]
step 4: [0.4, 0.3, 0.2]
```
(The observations are padded with zeroes for the first `stackSize-1` steps). This is a simple way to give an Agent limited "memory" without the complexity of adding a recurrent neural network (RNN).

The steps for enabling stacking depends on how you generate observations:
* For Agent.CollectObservations(), set "Stacked Vectors" on the Agent's `Behavior Parameters` to a value greater than 1.
* For ObservableAttribute, set the `numStackedObservations` parameter in the constructor, e.g. `[Observable(numStackedObservations: 2)]`.
* For `ISensor`s, wrap them in a `StackingSensor` (which is also an `ISensor`). Generally, this should happen in the `CreateSensor()` method of your `SensorComponent`.

#### Vector Observation Summary & Best Practices

- Vector Observations should include all variables relevant for allowing the agent to take the optimally informed decision, and ideally no extraneous information.
- In cases where Vector Observations need to be remembered or compared over time, either an RNN should be used in the model, or the `Stacked Vectors` value in the agent GameObject's `Behavior Parameters` should be changed.
- Categorical variables such as type of object (Sword, Shield, Bow) should be encoded in one-hot fashion (i.e. `3` -> `0, 0, 1`). This can be done automatically using the `AddOneHotObservation()` method of the `VectorSensor`, or using `[Observable]` on an enum field or property of the Agent.
- In general, all inputs should be normalized to be in the range 0 to +1 (or -1 to 1). For example, the `x` position information of an agent where the maximum possible value is `maxValue` should be recorded as `VectorSensor.AddObservation(transform.position.x / maxValue);` rather than `VectorSensor.AddObservation(transform.position.x);`.
- Positional information of relevant GameObjects should be encoded in relative coordinates wherever possible. This is often relative to the agent position.

### Visual Observations

Visual observations are generally provided to agent via either a `CameraSensor` or `RenderTextureSensor`. These collect image information and transforms it into a 3D Tensor which can be fed into the convolutional neural network (CNN) of the agent policy. For more information on CNNs, see [this guide](http://cs231n.github.io/convolutional-networks/). This allows agents to learn from spatial regularities in the observation images. It is possible to use visual and vector observations with the same agent.

Agents using visual observations can capture state of arbitrary complexity and are useful when the state is difficult to describe numerically. However, they are also typically less efficient and slower to train, and sometimes don't succeed at all as compared to vector observations. As such, they should only be used when it is not possible to properly define the problem using vector or ray-cast observations.

Visual observations can be derived from Cameras or RenderTextures within your scene. To add a visual observation to an Agent, add either a Camera Sensor Component or RenderTextures Sensor Component to the Agent. Then drag the camera or render texture you want to add to the `Camera` or `RenderTexture` field. You can have more than one camera or render texture and even use a combination of both attached to an Agent. For each visual observation, set the width and height of the image (in pixels) and whether or not the observation is color or grayscale.

![Agent Camera](images/visual-observation.png)

or

![Agent RenderTexture](images/visual-observation-rendertexture.png)

Each Agent that uses the same Policy must have the same number of visual observations, and they must all have the same resolutions (including whether or not they are grayscale). Additionally, each Sensor Component on an Agent must have a unique name so that they can be sorted deterministically (the name must be unique for that Agent, but multiple Agents can have a Sensor Component with the same name).

Visual observations also support stacking, by specifying `Observation Stacks` to a value greater than 1. The visual observations from the last `stackSize` steps will be stacked on the last dimension (channel dimension).

When using `RenderTexture` visual observations, a handy feature for debugging is adding a `Canvas`, then adding a `Raw Image` with it's texture set to the Agent's `RenderTexture`. This will render the agent observation on the game screen.

![RenderTexture with Raw Image](images/visual-observation-rawimage.png)

The [GridWorld environment](Learning-Environment-Examples.md#gridworld) is an example on how to use a RenderTexture for both debugging and observation. Note that in this example, a Camera is rendered to a RenderTexture, which is then used for observations and debugging. To update the RenderTexture, the Camera must be asked to render every time a decision is requested within the game code. When using Cameras as observations directly, this is done automatically by the Agent.

![Agent RenderTexture Debug](images/gridworld.png)

#### Visual Observation Summary & Best Practices

- To collect visual observations, attach `CameraSensor` or `RenderTextureSensor` components to the agent GameObject.
- Visual observations should generally only be used when vector observations are not sufficient.
- Image size should be kept as small as possible, without the loss of needed details for decision making.
- Images should be made grayscale in situations where color information is not needed for making informed decisions.

### Raycast Observations

Raycasts are another possible method for providing observations to an agent. This can be easily implemented by adding a `RayPerceptionSensorComponent3D` (or `RayPerceptionSensorComponent2D`) to the Agent GameObject.

During observations, several rays (or spheres, depending on settings) are cast into the physics world, and the objects that are hit determine the observation vector that is produced.

![Agent with two RayPerceptionSensorComponent3Ds](images/ray_perception.png)

Both sensor components have several settings:

- _Detectable Tags_ A list of strings corresponding to the types of objects that the Agent should be able to distinguish between. For example, in the WallJump example, we use "wall", "goal", and "block" as the list of objects to detect.
- _Rays Per Direction_ Determines the number of rays that are cast. One ray is always cast forward, and this many rays are cast to the left and right.
- _Max Ray Degrees_ The angle (in degrees) for the outermost rays. 90 degrees corresponds to the left and right of the agent.
- _Sphere Cast Radius_ The size of the sphere used for sphere casting. If set to 0, rays will be used instead of spheres. Rays may be more efficient, especially in complex scenes.
- _Ray Length_ The length of the casts
- _Ray Layer Mask_ The [LayerMask](https://docs.unity3d.com/ScriptReference/LayerMask.html) passed to the raycast or spherecast. This can be used to ignore certain types of objects when casting.
- _Observation Stacks_ The number of previous results to "stack" with the cast results. Note that this can be independent of the "Stacked Vectors" setting in `Behavior Parameters`.
- _Start Vertical Offset_ (3D only) The vertical offset of the ray start point.
- _End Vertical Offset_ (3D only) The vertical offset of the ray end point.
- _Alternating Ray Order_ Alternating is the default, it gives an order of (0, -delta, delta, -2*delta, 2*delta, ..., -n*delta, n*delta). If alternating is disabled the order is left to right (-n*delta, -(n-1)*delta, ..., -delta, 0, delta, ..., (n-1)*delta, n*delta). For general usage there is no difference but if using custom models the left-to-right layout that matches the spatial structuring can be preferred (e.g. for processing with conv nets).
- _Use Batched Raycasts_ (3D only) Whether to use batched raycasts. Enable to use batched raycasts and the jobs system.

In the example image above, the Agent has two `RayPerceptionSensorComponent3D`s. Both use 3 Rays Per Direction and 90 Max Ray Degrees. One of the components had a vertical offset, so the Agent can tell whether it's clear to jump over the wall.

The total size of the created observations is

```
(Observation Stacks) * (1 + 2 * Rays Per Direction) * (Num Detectable Tags + 2)
```

so the number of rays and tags should be kept as small as possible to reduce the amount of data used. Note that this is separate from the State Size defined in `Behavior Parameters`, so you don't need to worry about the formula above when setting the State Size.

#### RayCast Observation Summary & Best Practices

- Attach `RayPerceptionSensorComponent3D` or `RayPerceptionSensorComponent2D` to use.
- This observation type is best used when there is relevant spatial information for the agent that doesn't require a fully rendered image to convey.
- Use as few rays and tags as necessary to solve the problem in order to improve learning stability and agent performance.
- If you run into performance issues, try using batched raycasts by enabling the _Use Batched Raycast_ setting. (Only available for 3D ray perception sensors.)

### Grid Observations
Grid-base observations combine the advantages of 2D spatial representation in visual observations, and the flexibility of defining detectable objects in RayCast observations. The sensor uses a set of box queries in a grid shape and gives a top-down 2D view around the agent. This can be implemented by adding a `GridSensorComponent` to the Agent GameObject.

During observations, the sensor detects the presence of detectable objects in each cell and encode that into one-hot representation. The collected information from each cell forms a 3D tensor observation and will be fed into the convolutional neural network (CNN) of the agent policy just like visual observations.

![Agent with GridSensorComponent](images/grid_sensor.png)

The sensor component has the following settings:
- _Cell Scale_ The scale of each cell in the grid.
- _Grid Size_ Number of cells on each side of the grid.
- _Agent Game Object_ The Agent that holds the grid sensor. This is used to disambiguate objects with the same tag as the agent so that the agent doesn't detect itself.
- _Rotate With Agent_ Whether the grid rotates with the Agent.
- _Detectable Tags_ A list of strings corresponding to the types of objects that the Agent should be able to distinguish between.
- _Collider Mask_ The [LayerMask](https://docs.unity3d.com/ScriptReference/LayerMask.html) passed to the collider detection. This can be used to ignore certain types of objects.
- _Initial Collider Buffer Size_ The initial size of the Collider buffer used in the non-allocating Physics calls for each cell.
- _Max Collider Buffer Size_ The max size of the Collider buffer used in the non-allocating Physics calls for each cell.

The observation for each grid cell is a one-hot encoding of the detected object. The total size of the created observations is

```
GridSize.x * GridSize.z * Num Detectable Tags
```

so the number of detectable tags and size of the grid should be kept as small as possible to reduce the amount of data used. This makes a trade-off between the granularity of the observation and training speed.

To allow more variety of observations that grid sensor can capture, the `GridSensorComponent` and the underlying `GridSensorBase` also provides interfaces that can be overridden to collect customized observation from detected objects. See the Unity package documentation for more details on custom grid sensors.

__Note__: The `GridSensor` only works in 3D environments and will not behave properly in 2D environments.

#### Grid Observation Summary & Best Practices

- Attach `GridSensorComponent` to use.
- This observation type is best used when there is relevant non-visual spatial information that can be best captured in 2D representations.
- Use as small grid size and as few tags as necessary to solve the problem in order to improve learning stability and agent performance.
- Do not use `GridSensor` in a 2D game.

### Variable Length Observations

It is possible for agents to collect observations from a varying number of GameObjects by using a `BufferSensor`. You can add a `BufferSensor` to your Agent by adding a `BufferSensorComponent` to its GameObject. The `BufferSensor` can be useful in situations in which the Agent must pay attention to a varying number of entities (for example, a varying number of enemies or projectiles). On the trainer side, the `BufferSensor` is processed using an attention module. More information about attention mechanisms can be found [here](https://arxiv.org/abs/1706.03762). Training or doing inference with variable length observations can be slower than using a flat vector observation. However, attention mechanisms enable solving problems that require comparative reasoning between entities in a scene such as our [Sorter environment](Learning-Environment-Examples.md#sorter). Note that even though the `BufferSensor` can process a variable number of entities, you still need to define a maximum number of entities. This is because our network architecture requires to know what the shape of the observations will be. If fewer entities are observed than the maximum, the observation will be padded with zeros and the trainer will ignore the padded observations. Note that attention layers are invariant to the order of the entities, so there is no need to properly "order" the entities before feeding them into the `BufferSensor`.

The `BufferSensorComponent` Editor inspector has two arguments:

 - `Observation Size` : This is how many floats each entities will be represented with. This number is fixed and all entities must have the same representation. For example, if the entities you want to put into the `BufferSensor` have for relevant information position and speed, then the `Observation Size` should be 6 floats.
 - `Maximum Number of Entities` : This is the maximum number of entities the `BufferSensor` will be able to collect.

To add an entity's observations to a `BufferSensorComponent`, you need to call `BufferSensorComponent.AppendObservation()` in the Agent.CollectObservations() method with a float array of size `Observation Size` as argument.

__Note__: Currently, the observations put into the `BufferSensor` are not normalized, you will need to normalize your observations manually between -1 and 1.

#### Variable Length Observation Summary & Best Practices
 - Attach `BufferSensorComponent` to use.
 - Call `BufferSensorComponent.AppendObservation()` in the Agent.CollectObservations() methodto add the observations of an entity to the `BufferSensor`.
 - Normalize the entities observations before feeding them into the `BufferSensor`.

### Goal Signal

It is possible for agents to collect observations that will be treated as "goal signal". A goal signal is used to condition the policy of the agent, meaning that if the goal changes, the policy (i.e. the mapping from observations to actions) will change as well. Note that this is true for any observation since all observations influence the policy of the Agent to some degree. But by specifying a goal signal explicitly, we can make this conditioning more important to the agent. This feature can be used in settings where an agent must learn to solve different tasks that are similar by some aspects because the agent will learn to reuse learnings from different tasks to generalize better. In Unity, you can specify that a `VectorSensor` or a `CameraSensor` is a goal by attaching a `VectorSensorComponent` or a `CameraSensorComponent` to the Agent and selecting `Goal Signal` as `Observation Type`. On the trainer side, there are two different ways to condition the policy. This setting is determined by the [goal_conditioning_type parameter](Training-Configuration-File.md#common-trainer-configurations). If set to `hyper` (default) a [HyperNetwork](https://arxiv.org/pdf/1609.09106.pdf) will be used to generate some of the weights of the policy using the goal observations as input. Note that using a HyperNetwork requires a lot of computations, it is recommended to use a smaller number of hidden units in the policy to alleviate this. If set to `none` the goal signal will be considered as regular observations. For an example on how to use a goal signal, see the [GridWorld example](Learning-Environment-Examples.md#gridworld).

#### Goal Signal Summary & Best Practices
 - Attach a `VectorSensorComponent` or `CameraSensorComponent` to an agent and set the observation type to goal to use the feature.
 - Set the goal_conditioning_type parameter in the training configuration.
 - Reduce the number of hidden units in the network when using the HyperNetwork conditioning type.

## Actions and Actuators

An action is an instruction from the Policy that the agent carries out. The action is passed to the an `IActionReceiver` (either an `Agent` or an `IActuator`) as the `ActionBuffers` parameter when the Academy invokes the `IActionReciever.OnActionReceived()` function. There are two types of actions supported: **Continuous** and **Discrete**.

Neither the Policy nor the training algorithm know anything about what the action values themselves mean. The training algorithm simply tries different values for the action list and observes the affect on the accumulated rewards over time and many training episodes. Thus, the only place actions are defined for an Agent is in the `OnActionReceived()` function.

For example, if you designed an agent to move in two dimensions, you could use either continuous or the discrete actions. In the continuous case, you would set the action size to two (one for each dimension), and the agent's Policy would output an action with two floating point values. In the discrete case, you would use one Branch with a size of four (one for each direction), and the Policy would create an action array containing a single element with a value ranging from zero to three. Alternatively, you could create two branches of size two (one for horizontal movement and one for vertical movement), and the Policy would output an action array containing two elements with values ranging from zero to one. You could alternatively use a combination of continuous and discrete actions e.g., using one continuous action for horizontal movement and a discrete branch of size two for the vertical movement.

Note that when you are programming actions for an agent, it is often helpful to test your action logic using the `Heuristic()` method of the Agent, which lets you map keyboard commands to actions.

### Continuous Actions

When an Agent's Policy has **Continuous** actions, the `ActionBuffers.ContinuousActions` passed to the Agent's `OnActionReceived()` function is an array with length equal to the `Continuous Action Size` property value. The individual values in the array have whatever meanings that you ascribe to them. If you assign an element in the array as the speed of an Agent, for example, the training process learns to control the speed of the Agent through this parameter.

The [3DBall example](Learning-Environment-Examples.md#3dball-3d-balance-ball) uses continuous actions with two control values.

![3DBall](images/balance.png)

These control values are applied as rotation to the cube:

```csharp
    public override void OnActionReceived(ActionBuffers actionBuffers)
    {
        var actionZ = 2f * Mathf.Clamp(actionBuffers.ContinuousActions[0], -1f, 1f);
        var actionX = 2f * Mathf.Clamp(actionBuffers.ContinuousActions[1], -1f, 1f);

        gameObject.transform.Rotate(new Vector3(0, 0, 1), actionZ);
        gameObject.transform.Rotate(new Vector3(1, 0, 0), actionX);
    }
```

By default, the output from our provided PPO algorithm pre-clamps the values of `ActionBuffers.ContinuousActions` into the [-1, 1] range. It is a best practice to manually clip these as well, if you plan to use a 3rd party algorithm with your environment. As shown above, you can scale the control values as needed after clamping them.

### Discrete Actions

When an Agent's Policy uses **discrete** actions, the `ActionBuffers.DiscreteActions` passed to the Agent's `OnActionReceived()` function is an array of integers with length equal to `Discrete Branch Size`. When defining the discrete actions, `Branches` is an array of integers, each value corresponds to the number of possibilities for each branch.

For example, if we wanted an Agent that can move in a plane and jump, we could define two branches (one for motion and one for jumping) because we want our agent be able to move **and** jump concurrently. We define the first branch to have 5 possible actions (don't move, go left, go right, go backward, go forward) and the second one to have 2 possible actions (don't jump, jump). The `OnActionReceived()` method would look something like:

```csharp
// Get the action index for movement
int movement = actionBuffers.DiscreteActions[0];
// Get the action index for jumping
int jump = actionBuffers.DiscreteActions[1];

// Look up the index in the movement action list:
if (movement == 1) { directionX = -1; }
if (movement == 2) { directionX = 1; }
if (movement == 3) { directionZ = -1; }
if (movement == 4) { directionZ = 1; }
// Look up the index in the jump action list:
if (jump == 1 && IsGrounded()) { directionY = 1; }

// Apply the action results to move the Agent
gameObject.GetComponent<Rigidbody>().AddForce(
    new Vector3(
        directionX * 40f, directionY * 300f, directionZ * 40f));
```

#### Masking Discrete Actions

When using Discrete Actions, it is possible to specify that some actions are impossible for the next decision. When the Agent is controlled by a neural network, the Agent will be unable to perform the specified action. Note that when the Agent is controlled by its Heuristic, the Agent will still be able to decide to perform the masked action. In order to disallow an action, override the `Agent.WriteDiscreteActionMask()` virtual method, and call `SetActionEnabled()` on the provided `IDiscreteActionMask`:

```csharp
public override void WriteDiscreteActionMask(IDiscreteActionMask actionMask)
{
    actionMask.SetActionEnabled(branch, actionIndex, isEnabled);
}
```

Where:

- `branch` is the index (starting at 0) of the branch on which you want to allow or disallow the action
- `actionIndex` is the index of the action that you want to allow or disallow.
- `isEnabled` is a bool indicating whether the action should be allowed or now.

For example, if you have an Agent with 2 branches and on the first branch (branch 0) there are 4 possible actions : _"do nothing"_, _"jump"_, _"shoot"_ and _"change weapon"_. Then with the code bellow, the Agent will either _"do nothing"_ or _"change weapon"_ for their next decision (since action index 1 and 2 are masked)

```csharp
actionMask.SetActionEnabled(0, 1, false);
actionMask.SetActionEnabled(0, 2, false);
```

Notes:

- You can call `SetActionEnabled` multiple times if you want to put masks on multiple branches.
- At each step, the state of an action is reset and enabled by default.
- You cannot mask all the actions of a branch.
- You cannot mask actions in continuous control.


### IActuator interface and ActuatorComponents
The Actuator API allows users to abstract behavior out of Agents and in to components (similar to the ISensor API).  The `IActuator` interface and `Agent` class both implement the `IActionReceiver` interface to allow for backward compatibility with the current `Agent.OnActionReceived`. This means you will not have to change your code until you decide to use the `IActuator` API.

Like the `ISensor` interface, the `IActuator` interface is intended for advanced users.

The `ActuatorComponent` abstract class is used to create the actual `IActuator` at runtime. It must be attached to the same `GameObject` as the `Agent`, or to a child `GameObject`.  Actuators and all of their data structures are initialized during `Agent.Initialize`.  This was done to prevent an unexpected allocations at runtime.

You can find an example of an `IActuator` implementation in the `Basic` example scene. **NOTE**: you do not need to adjust the Actions in the Agent's `Behavior Parameters` when using an `IActuator` and `ActuatorComponents`.

Internally, `Agent.OnActionReceived` uses an `IActuator` to send actions to the Agent, although this is mostly abstracted from the user.


### Actions Summary & Best Practices

- Agents can use `Discrete` and/or `Continuous` actions.
- Discrete actions can have multiple action branches, and it's possible to mask certain actions so that they won't be taken.
- In general, fewer actions will make for easier learning.
- Be sure to set the Continuous Action Size and Discrete Branch Size to the desired number for each type of action, and not greater, as doing the latter can interfere with the efficiency of the training process.
- Continuous action values should be clipped to an appropriate range. The provided PPO model automatically clips these values between -1 and 1, but third party training systems may not do so.

## Rewards

In reinforcement learning, the reward is a signal that the agent has done something right. The PPO reinforcement learning algorithm works by optimizing the choices an agent makes such that the agent earns the highest cumulative reward over time. The better your reward mechanism, the better your agent will learn.

**Note:** Rewards are not used during inference by an Agent using a trained model and is also not used during imitation learning.

Perhaps the best advice is to start simple and only add complexity as needed. In general, you should reward results rather than actions you think will lead to the desired results. You can even use the Agent's Heuristic to control the Agent while watching how it accumulates rewards.

Allocate rewards to an Agent by calling the `AddReward()` or `SetReward()` methods on the agent. The reward assigned between each decision should be in the range [-1,1]. Values outside this range can lead to unstable training. The `reward` value is reset to zero when the agent receives a new decision. If there are multiple calls to `AddReward()` for a single agent decision, the rewards will be summed together to evaluate how good the previous decision was. The `SetReward()` will override all previous rewards given to an agent since the previous decision.

### Examples

You can examine the `OnActionReceived()` functions defined in the [example environments](Learning-Environment-Examples.md) to see how those projects allocate rewards.

The `GridAgent` class in the [GridWorld example](Learning-Environment-Examples.md#gridworld) uses a very simple reward system:

```csharp
Collider[] hitObjects = Physics.OverlapBox(trueAgent.transform.position,
                                           new Vector3(0.3f, 0.3f, 0.3f));
if (hitObjects.Where(col => col.gameObject.tag == "goal").ToArray().Length == 1)
{
    AddReward(1.0f);
    EndEpisode();
}
else if (hitObjects.Where(col => col.gameObject.tag == "pit").ToArray().Length == 1)
{
    AddReward(-1f);
    EndEpisode();
}
```

The agent receives a positive reward when it reaches the goal and a negative reward when it falls into the pit. Otherwise, it gets no rewards. This is an example of a _sparse_ reward system. The agent must explore a lot to find the infrequent reward.

In contrast, the `AreaAgent` in the [Area example](Learning-Environment-Examples.md#push-block) gets a small negative reward every step. In order to get the maximum reward, the agent must finish its task of reaching the goal square as quickly as possible:

```csharp
AddReward( -0.005f);
MoveAgent(act);

if (gameObject.transform.position.y < 0.0f ||
    Mathf.Abs(gameObject.transform.position.x - area.transform.position.x) > 8f ||
    Mathf.Abs(gameObject.transform.position.z + 5 - area.transform.position.z) > 8)
{
    AddReward(-1f);
    EndEpisode();
}
```

The agent also gets a larger negative penalty if it falls off the playing surface.

The `Ball3DAgent` in the [3DBall](Learning-Environment-Examples.md#3dball-3d-balance-ball) takes a similar approach, but allocates a small positive reward as long as the agent balances the ball. The agent can maximize its rewards by keeping the ball on the platform:

```csharp

SetReward(0.1f);

// When ball falls mark Agent as finished and give a negative penalty
if ((ball.transform.position.y - gameObject.transform.position.y) < -2f ||
    Mathf.Abs(ball.transform.position.x - gameObject.transform.position.x) > 3f ||
    Mathf.Abs(ball.transform.position.z - gameObject.transform.position.z) > 3f)
{
    SetReward(-1f);
    EndEpisode();

}
```

The `Ball3DAgent` also assigns a negative penalty when the ball falls off the platform.

Note that all of these environments make use of the `EndEpisode()` method, which manually terminates an episode when a termination condition is reached. This can be called independently of the `Max Step` property.

### Rewards Summary & Best Practices

- Use `AddReward()` to accumulate rewards between decisions. Use `SetReward()` to overwrite any previous rewards accumulate between decisions.
- The magnitude of any given reward should typically not be greater than 1.0 in order to ensure a more stable learning process.
- Positive rewards are often more helpful to shaping the desired behavior of an agent than negative rewards. Excessive negative rewards can result in the agent failing to learn any meaningful behavior.
- For locomotion tasks, a small positive reward (+0.1) for forward velocity is typically used.
- If you want the agent to finish a task quickly, it is often helpful to provide a small penalty every step (-0.05) that the agent does not complete the task. In this case completion of the task should also coincide with the end of the episode by calling `EndEpisode()` on the agent when it has accomplished its goal.

## Agent Properties

![Agent Inspector](images/3dball_learning_brain.png)

- `Behavior Parameters` - The parameters dictating what Policy the Agent will receive.
  - `Behavior Name` - The identifier for the behavior. Agents with the same behavior name will learn the same policy.
  - `Vector Observation`
    - `Space Size` - Length of vector observation for the Agent.
    - `Stacked Vectors` - The number of previous vector observations that will be stacked and used collectively for decision making. This results in the effective size of the vector observation being passed to the Policy being: _Space Size_ x _Stacked Vectors_.
  - `Actions`
    - `Continuous Actions` - The number of concurrent continuous actions that the Agent can take.
    - `Discrete Branches` - An array of integers, defines multiple concurrent discrete actions. The values in the `Discrete Branches` array correspond to the number of possible discrete values for each action branch.
  - `Model` - The neural network model used for inference (obtained after training)
  - `Inference Device` - Whether to use CPU or GPU to run the model during inference
  - `Behavior Type` - Determines whether the Agent will do training, inference, or use its Heuristic() method:
    - `Default` - the Agent will train if they connect to a python trainer, otherwise they will perform inference.
    - `Heuristic Only` - the Agent will always use the `Heuristic()` method.
    - `Inference Only` - the Agent will always perform inference.
  - `Team ID` - Used to define the team for self-play
  - `Use Child Sensors` - Whether to use all Sensor components attached to child GameObjects of this Agent.
- `Max Step` - The per-agent maximum number of steps. Once this number is reached, the Agent will be reset.

## Destroying an Agent

You can destroy an Agent GameObject during the simulation. Make sure that there is always at least one Agent training at all times by either spawning a new Agent every time one is destroyed or by re-spawning new Agents when the whole environment resets.

## Defining Multi-agent Scenarios

### Teams for Adversarial Scenarios

Self-play is triggered by including the self-play hyperparameter hierarchy in the [trainer configuration](Training-ML-Agents.md#training-configurations). To distinguish opposing agents, set the team ID to different integer values in the behavior parameters script on the agent prefab.

<p align="center"> <img src="images/team_id.png" alt="Team ID" width="375" border="10" /> </p>

**_Team ID must be 0 or an integer greater than 0._**

In symmetric games, since all agents (even on opposing teams) will share the same policy, they should have the same 'Behavior Name' in their Behavior Parameters Script. In asymmetric games, they should have a different Behavior Name in their Behavior Parameters script. Note, in asymmetric games, the agents must have both different Behavior Names _and_ different team IDs!

For examples of how to use this feature, you can see the trainer configurations and agent prefabs for our Tennis and Soccer environments. Tennis and Soccer provide examples of symmetric games. To train an asymmetric game, specify trainer configurations for each of your behavior names and include the self-play hyperparameter hierarchy in both.

### Groups for Cooperative Scenarios

Cooperative behavior in ML-Agents can be enabled by instantiating a `SimpleMultiAgentGroup`, typically in an environment controller or similar script, and adding agents to it using the `RegisterAgent(Agent agent)` method. Note that all agents added to the same `SimpleMultiAgentGroup` must have the same behavior name and Behavior Parameters. Using `SimpleMultiAgentGroup` enables the agents within a group to learn how to work together to achieve a common goal (i.e., maximize a group-given reward), even if one or more of the group members are removed before the episode ends. You can then use this group to add/set rewards, end or interrupt episodes at a group level using the `AddGroupReward()`, `SetGroupReward()`, `EndGroupEpisode()`, and
`GroupEpisodeInterrupted()` methods. For example:

```csharp
// Create a Multi Agent Group in Start() or Initialize()
m_AgentGroup = new SimpleMultiAgentGroup();

// Register agents in group at the beginning of an episode
for (var agent in AgentList)
{
  m_AgentGroup.RegisterAgent(agent);
}

// if the team scores a goal
m_AgentGroup.AddGroupReward(rewardForGoal);

// If the goal is reached and the episode is over
m_AgentGroup.EndGroupEpisode();
ResetScene();

// If time ran out and we need to interrupt the episode
m_AgentGroup.GroupEpisodeInterrupted();
ResetScene();
```

Multi Agent Groups should be used with the MA-POCA trainer, which is explicitly designed to train cooperative environments. This can be enabled by using the `poca` trainer - see the [training configurations](Training-Configuration-File.md) doc for more information on configuring MA-POCA. When using MA-POCA, agents which are deactivated or removed from the Scene during the episode will still learn to contribute to the group's long term rewards, even if they are not active in the scene to experience them.

See the [Cooperative Push Block](Learning-Environment-Examples.md#cooperative-push-block) environment for an example of how to use Multi Agent Groups, and the [Dungeon Escape](Learning-Environment-Examples.md#dungeon-escape) environment for an example of how the Multi Agent Group can be used with agents that are removed from the scene mid-episode.

**NOTE**: Groups differ from Teams (for competitive settings) in the following way - Agents working together should be added to the same Group, while agents playing against each other should be given different Team Ids. If in the Scene there is one playing field and two teams, there should be two Groups, one for each team, and each team should be assigned a different Team Id. If this playing field is duplicated many times in the Scene (e.g. for training speedup), there should be two Groups _per playing field_, and two unique Team Ids _for the entire Scene_. In environments with both Groups and Team Ids configured, MA-POCA and self-play can be used together for training. In the diagram below, there are two agents on each team, and two playing fields where teams are pitted against each other. All the blue agents should share a Team Id (and the orange ones a different ID), and there should be four group managers, one per pair of agents.

<p align="center"> <img src="images/groupmanager_teamid.png" alt="Group Manager vs Team Id" width="650" border="10" /> </p>

Please see the [SoccerTwos](Learning-Environment-Examples.md#soccer-twos) environment for an example.

#### Cooperative Behaviors Notes and Best Practices
* An agent can only be registered to one MultiAgentGroup at a time. If you want to re-assign an agent from one group to another, you have to unregister it from the current group first.

* Agents with different behavior names in the same group are not supported.

* Agents within groups should always set the `Max Steps` parameter in the Agent script to 0. Instead, handle Max Steps using the MultiAgentGroup by ending the episode for the entire Group using `GroupEpisodeInterrupted()`.

* `EndGroupEpisode` and `GroupEpisodeInterrupted` do the same job in the game, but has slightly different effect on the training. If the episode is completed, you would want to call `EndGroupEpisode`. But if the episode is not over but it has been running for enough steps, i.e. reaching max step, you would call `GroupEpisodeInterrupted`.

* If an agent finished earlier, e.g. completed tasks/be removed/be killed in the game, do not call `EndEpisode()` on the Agent. Instead, disable the agent and re-enable it when the next episode starts, or destroy the agent entirely. This is because calling `EndEpisode()` will call `OnEpisodeBegin()`, which will reset the agent immediately. While it is possible to call `EndEpisode()` in this way, it is usually not the desired behavior when training groups of agents.

* If an agent that was disabled in a scene needs to be re-enabled, it must be re-registered to the MultiAgentGroup.

* Group rewards are meant to reinforce agents to act in the group's best interest instead of individual ones, and are treated differently than individual agent rewards during training. So calling `AddGroupReward()` is not equivalent to calling agent.AddReward() on each agent in the group.

* You can still add incremental rewards to agents using `Agent.AddReward()` if they are in a Group. These rewards will only be given to those agents and are received when the Agent is active.

* Environments which use Multi Agent Groups can be trained using PPO or SAC, but agents will not be able to learn from group rewards after deactivation/removal, nor will they behave as cooperatively.

## Recording Demonstrations

In order to record demonstrations from an agent, add the `Demonstration Recorder` component to a GameObject in the scene which contains an `Agent` component. Once added, it is possible to name the demonstration that will be recorded from the agent.

<p align="center"> <img src="images/demo_component.png" alt="Demonstration Recorder" width="650" border="10" /> </p>

When `Record` is checked, a demonstration will be created whenever the scene is played from the Editor. Depending on the complexity of the task, anywhere from a few minutes or a few hours of demonstration data may be necessary to be useful for imitation learning. To specify an exact number of steps you want to record use the `Num Steps To Record` field and the editor will end your play session automatically once that many steps are recorded. If you set `Num Steps To Record` to `0` then recording will continue until you manually end the play session. Once the play session ends a `.demo` file will be created in the `Assets/Demonstrations` folder (by default). This file contains the demonstrations. Clicking on the file will provide metadata about the demonstration in the inspector.

<p align="center"> <img src="images/demo_inspector.png" alt="Demonstration Inspector" width="375" border="10" /> </p>

You can then specify the path to this file in your [training configurations](Training-Configuration-File.md#behavioral-cloning).


## Links discovered
- [example environments](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Examples.md)
- [Match-3 game](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Integrations-Match3.md)
- [this guide](http://cs231n.github.io/convolutional-networks/)
- [Agent Camera](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/visual-observation.png)
- [Agent RenderTexture](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/visual-observation-rendertexture.png)
- [RenderTexture with Raw Image](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/visual-observation-rawimage.png)
- [GridWorld environment](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Examples.md#gridworld)
- [Agent RenderTexture Debug](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/gridworld.png)
- [Agent with two RayPerceptionSensorComponent3Ds](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/ray_perception.png)
- [LayerMask](https://docs.unity3d.com/ScriptReference/LayerMask.html)
- [Agent with GridSensorComponent](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/grid_sensor.png)
- [here](https://arxiv.org/abs/1706.03762)
- [Sorter environment](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Examples.md#sorter)
- [goal_conditioning_type parameter](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Training-Configuration-File.md#common-trainer-configurations)
- [HyperNetwork](https://arxiv.org/pdf/1609.09106.pdf)
- [GridWorld example](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Examples.md#gridworld)
- [3DBall example](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Examples.md#3dball-3d-balance-ball)
- [3DBall](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/balance.png)
- [Area example](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Examples.md#push-block)
- [3DBall](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Examples.md#3dball-3d-balance-ball)
- [Agent Inspector](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/images/3dball_learning_brain.png)
- [trainer configuration](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Training-ML-Agents.md#training-configurations)
- [training configurations](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Training-Configuration-File.md)
- [Cooperative Push Block](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Examples.md#cooperative-push-block)
- [Dungeon Escape](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Examples.md#dungeon-escape)
- [SoccerTwos](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environment-Examples.md#soccer-twos)
- [training configurations](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Training-Configuration-File.md#behavioral-cloning)

--- LICENSE.md ---
ML Agents copyright © 2017 Unity Technologies

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


--- com.unity.ml-agents/CHANGELOG.md ---
# Changelog

All notable changes to this package will be documented in this file.

The format is based on [Keep a Changelog](http://keepachangelog.com/en/1.0.0/)
and this project adheres to
[Semantic Versioning](http://semver.org/spec/v2.0.0.html).

## [4.0.1] - 2025-12-04
### Minor Changes
#### com.unity.ml-agents (C#)
- Upgraded to Inference Engine 2.4.1 (#6269)
- Fixed tensor indexing to use correct CHW layout (#6239)
- Updated the installation doc (#6242)
- Fixed Unity Editor crashing when quitting in play mode (#6274)

#### ml-agents / ml-agents-envs
- Set the Torch version constraint to 2.8 (#6251)
- Fixed CUDA/CPU mismatch in threaded training (#6245)

## [4.0.0] - 2025-08-28
### Major Changes
#### com.unity.ml-agents (C#)
- Upgraded to Inference Engine 2.2.1 (#6212)
- The minimum supported Unity version was updated to 6000.0. (#6207)
- Merged the extension package com.unity.ml-agents.extensions to the main package com.unity.ml-agents. (#6227)

### Minor Changes
#### com.unity.ml-agents (C#)
- Removed broken sample from the package (#6230)
- Moved to Unity Package documentation as the primary developer documentation. (#6232)

#### ml-agents / ml-agents-envs
- Bumped grpcio version to >=1.11.0,<=1.53.2 (#6208)

## [3.0.0] - 2024-09-02
### Major Changes
#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
- Upgraded to Sentis 2.1.0 (#6153)
- Upgraded to Sentis 2.0.0 (#6137)
- Upgraded to Sentis 1.3.0-pre.3 (#6070)
- Upgraded to Sentis 1.3.0-exp.2 (#6013)
- The minimum supported Unity version was updated to 2023.2. (#6071)

#### ml-agents / ml-agents-envs
- Upgraded to PyTorch 2.1.1. (#6013)

### Minor Changes
#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
- Added no-graphics-monitor. (#6014)

#### ml-agents / ml-agents-envs
- Update Installation.md (#6004)
- Updated Using-Virtual-Environment.md (#6033)

### Bug Fixes
#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
- Fix failing ci post upgrade (#6141)
- Fixed missing assembly reference for google protobuf. (#6099)
- Fixed missing tensor Dispose in ModelRunner. (#6028)
- Fixed 3DBall sample package to remove Barracuda dependency. (#6030)

#### ml-agents / ml-agents-envs
- Fix sample code indentation in migrating.md (#5840)
- Fixed continuous integration tests (#6079)
- Fixed bad like format (#6078)
- Bumped numpy version to >=1.23.5,<1.24.0 (#6082)
- Bumped onnx version to 1.15.0 (#6062)
- Bumped protobuf version to >=3.6,<21 (#6062)

## [3.0.0-exp.1] - 2023-10-09
### Major Changes
#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
- Upgraded ML-Agents to Sentis 1.2.0-exp.2 and deprecated Barracuda. (#5979)
- The minimum supported Unity version was updated to 2022.3. (#5950)
- Added batched raycast sensor option. (#5950)

#### ml-agents / ml-agents-envs
- Updated to PyTorch 1.13.1 (#5982)
- Deprecated support for Python 3.8.x and 3.9.x (#5981)

### Minor Changes
#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
- Added DecisionStep parameter to DecisionRequester (#5940)
  - This will allow the staggering of execution timing when using multi-agents, leading to more stable performance.

#### ml-agents / ml-agents-envs
- Added timeout cli and yaml config file support for specifying environment timeout. (#5991)
- Added training config feature to evenly distribute checkpoints throughout training. (#5842)
- Updated training area replicator to add a condition to only replicate training areas when running a build. (#5842)

### Bug Fixes
#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
- Compiler errors when using IAsyncEnumerable<T> with .NET Standard 2.1 enabled (#5951)
#### ml-agents / ml-agents-envs


## [2.3.0-exp.3] - 2022-11-21
### Major Changes
#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
- The minimum supported Unity version was updated to 2021.3. (#)

#### ml-agents / ml-agents-envs
- Add your trainers to the package using Ml-Agents Custom Trainers plugin. (#)
  - ML-Agents Custom Trainers plugin is an extensible plugin system to define new trainers based on the
  High level trainer API, read more [here](../docs/Python-Custom-Trainer-Plugin.md).
- Refactored core modules to make ML-Agents internal classes more generalizable to various RL algorithms. (#)
- The minimum supported Python version for ML-agents has changed to 3.8.13. (#)
- The minimum supported version of PyTorch was changed to 1.8.0. (#)
- Add shared critic configurability for PPO. (#)
- We moved `UnityToGymWrapper` and `PettingZoo` API to `ml-agents-envs` package. All these environments will be
versioned under `ml-agents-envs` package in the future (#)

### Minor Changes
#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
- Added switch to RayPerceptionSensor to allow rays to be ordered left to right. (#26)
    - Current alternating order is still the default but will be deprecated.
- Added support for enabling/disabling camera object attached to camera sensor in order to improve performance. (#31)

#### ml-agents / ml-agents-envs
- Renaming the path that shadows torch with "mlagents/trainers/torch_entities" and update respective imports (#)


### Bug Fixes
#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
#### ml-agents / ml-agents-envs


## [2.3.0-exp.2] - 2022-03-28
### Major Changes
#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
#### ml-agents / ml-agents-envs
- Refactored to support the new ML-Agents Pro package.
- The minimum supported Python version for ML-Agents-envs is changed to 3.7.2 (#)
- Added support for the PettingZoo multi-agent API (#)
- Refactored `gym-unity` into the `ml-agents-envs` package (#)

### Minor Changes
#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
- Upgrade barracuda dependency to 3.0.0 (#)
#### ml-agents / ml-agents-envs
- Added the new unity_vec_env file to the ml-agents-envs module
- Extended support to python 3.9.10

### Bug Fixes
#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
#### ml-agents / ml-agents-envs

## [2.2.1-exp.1] - 2022-01-14
### Major Changes

#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
- The minimum supported Unity version was updated to 2020.3. (#5673)
- Added a new feature to replicate training areas dynamically during runtime. (#5568)
- Update Barracuda to 2.3.1-preview (#5591)
- Update Input System to 1.3.0 (#5661)

#### ml-agents / ml-agents-envs / gym-unity (Python)

### Minor Changes

#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
- Added the capacity to initialize behaviors from any checkpoint and not just the latest one (#5525)
- Added the ability to get a read-only view of the stacked observations (#5523)

#### ml-agents / ml-agents-envs / gym-unity (Python)
- Set gym version in gym-unity to gym release 0.20.0 (#5540)
- Added support for having `beta`, `epsilon`, and `learning rate` on separate schedules (affects only PPO and POCA). (#5538)
- Changed default behavior to restart crashed Unity environments rather than exiting. (#5553)
  - Rate & lifetime limits on this are configurable via 3 new yaml options
    1. env_params.max_lifetime_restarts (--max-lifetime-restarts) [default=10]
    2. env_params.restarts_rate_limit_n (--restarts-rate-limit-n) [default=1]
    3. env_params.restarts_rate_limit_period_s (--restarts-rate-limit-period-s) [default=60]
- Deterministic action selection is now supported during training and inference(#5619)
    - Added a new `--deterministic` cli flag to deterministically select the most probable actions in policy. The same thing can
      be achieved by adding `deterministic: true` under `network_settings` of the run options configuration.(#5597)
    - Extra tensors are now serialized to support deterministic action selection in onnx. (#5593)
    - Support inference with deterministic action selection in editor (#5599)
- Added minimal analytics collection to LL-API (#5511)
- Update Colab notebooks for GridWorld example with DQN illustrating the use of the Python API and how to export to ONNX (#5643)

### Bug Fixes
#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
- Update gRPC native lib to universal for arm64 and x86_64. This change should enable ml-agents usage on mac M1 (#5283, #5519)
- Fixed a bug where ml-agents code wouldn't compile on platforms that didn't support analytics (PS4/5, XBoxOne) (#5628)

#### ml-agents / ml-agents-envs / gym-unity (Python)
- Fixed a bug where the critics were not being normalized during training. (#5595)
- Fixed the bug where curriculum learning would crash because of the incorrect run_options parsing. (#5586)
- Fixed a bug in multi-agent cooperative training where agents might not receive all of the states of
terminated teammates. (#5441)
- Fixed wrong attribute name in argparser for torch device option (#5433)(#5467)
- Fixed conflicting CLI and yaml options regarding resume & initialize_from (#5495)
- Fixed failing tests for gym-unity due to gym 0.20.0 release (#5540)
- Fixed a bug in VAIL where the variational bottleneck was not properly passing gradients (#5546)
- Harden user PII protection logic and extend TrainingAnalytics to expose detailed configuration parameters. (#5512)

## [2.1.0-exp.1] - 2021-06-09
### Minor Changes
#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
- update Barracuda to 2.0.0-pre.3. (#5385)
- Fixed NullReferenceException when adding Behavior Parameters with no Agent. (#5382)
- Add stacking option in Editor for `VectorSensorComponent`. (#5376)

#### ml-agents / ml-agents-envs / gym-unity (Python)
- Lock cattrs dependency version to 1.6. (#5397)
- Added a fully connected visual encoder for environments with very small image inputs. (#5351)
- Colab notebooks illustrating the use of the Python API are now part of the repository. (#5399)

### Bug Fixes
#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
- RigidBodySensorComponent now displays a warning if it's used in a way that won't generate useful observations. (#5387)
- Update the documentation with a note saying that `GridSensor` does not work in 2D environments. (#5396)
- Fixed an error where sensors would not reset properly before collecting the last observation at the end of an
episode. (#5375)

#### ml-agents / ml-agents-envs / gym-unity (Python)
- The calculation of the target entropy of SAC with continuous actions was incorrect and has been fixed. (#5372)
- Fixed an issue where the histogram stats would not be reported correctly in TensorBoard. (#5410)
- Fixed error when importing models which use the ResNet encoder. (#5358)


## [2.0.0-exp.1] - 2021-04-22
### Major Changes
#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
- The minimum supported Unity version was updated to 2019.4. (#5166)
- Several breaking interface changes were made. See the
[Migration Guide](https://github.com/Unity-Technologies/ml-agents/blob/release_17_docs/docs/Migrating.md) for more
details.
- Some methods previously marked as `Obsolete` have been removed. If you were using these methods, you need to replace them with their supported counterpart.
- The interface for disabling discrete actions in `IDiscreteActionMask` has changed.
`WriteMask(int branch, IEnumerable<int> actionIndices)` was replaced with
`SetActionEnabled(int branch, int actionIndex, bool isEnabled)`. (#5060)
- IActuator now implements IHeuristicProvider. (#5110)
- `ISensor.GetObservationShape()` was removed, and `GetObservationSpec()` was added. The `ITypedSensor`
and `IDimensionPropertiesSensor` interfaces were removed. (#5127)
- `ISensor.GetCompressionType()` was removed, and `GetCompressionSpec()` was added. The `ISparseChannelSensor`
interface was removed. (#5164)
- The abstract method `SensorComponent.GetObservationShape()` was no longer being called, so it has been removed. (#5172)
- `SensorComponent.CreateSensor()` was replaced with `SensorComponent.CreateSensors()`, which returns an `ISensor[]`. (#5181)
- `Match3Sensor` was refactored to produce cell and special type observations separately, and `Match3SensorComponent` now
produces two `Match3Sensor`s (unless there are no special types). Previously trained models will have different observation
sizes and will need to be retrained. (#5181)
- The `AbstractBoard` class for integration with Match-3 games was changed to make it easier to support boards with
different sizes using the same model. For a summary of the interface changes, please see the Migration Guide. (##5189)
- Updated the Barracuda package to version `1.4.0-preview`(#5236)
- `GridSensor` has been refactored and moved to main package, with changes to both sensor interfaces and behaviors.
Exsisting GridSensor created by extension package will not work in newer version. Previously trained models will
need to be retrained. Please see the Migration Guide for more details. (#5256)
- Models trained with 1.x versions of ML-Agents will no longer work at inference if they were trained using recurrent neural networks (#5254)

### Minor Changes
#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
- The `.onnx` models input names have changed. All input placeholders will now use the prefix `obs_` removing the distinction between visual and vector observations. In addition, the inputs and outputs of LSTM changed. Models created with this version will not be usable with previous versions of the package (#5080, #5236)
- The `.onnx` models discrete action output now contains the discrete actions values and not the logits. Models created with this version will not be usable with previous versions of the package (#5080)
- Added ML-Agents package settings. (#5027)
- Make com.unity.modules.unityanalytics an optional dependency. (#5109)
- Make com.unity.modules.physics and com.unity.modules.physics2d optional dependencies. (#5112)
- The default `InferenceDevice` is now `InferenceDevice.Default`, which is equivalent to `InferenceDevice.Burst`. If you
depend on the previous behavior, you can explicitly set the Agent's `InferenceDevice` to `InferenceDevice.CPU`. (#5175)
- Added support for `Goal Signal` as a type of observation. Trainers can now use HyperNetworks to process `Goal Signal`. Trainers with HyperNetworks are more effective at solving multiple tasks. (#5142, #5159, #5149)
- Modified the [GridWorld environment](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#gridworld) to use the new `Goal Signal` feature. (#5193)
- `DecisionRequester.ShouldRequestDecision()` and `ShouldRequestAction()`methods were added. These are used to
determine whether `Agent.RequestDecision()` and `Agent.RequestAction()` are called (respectively). (#5223)
- `RaycastPerceptionSensor` now caches its raycast results; they can be accessed via `RayPerceptionSensor.RayPerceptionOutput`. (#5222)
- `ActionBuffers` are now reset to zero before being passed to `Agent.Heuristic()` and
`IHeuristicProvider.Heuristic()`. (#5227)
- `Agent` will now call `IDisposable.Dispose()` on all `ISensor`s that implement the `IDisposable` interface. (#5233)
- `CameraSensor`, `RenderTextureSensor`, and `Match3Sensor` will now reuse their `Texture2D`s, reducing the
amount of memory that needs to be allocated during runtime. (#5233)
- Optimzed `ObservationWriter.WriteTexture()` so that it doesn't call `Texture2D.GetPixels32()` for `RGB24` textures.
This results in much less memory being allocated during inference with `CameraSensor` and `RenderTextureSensor`. (#5233)
- The Match-3 integration utilities were moved from `com.unity.ml-agents.extensions` to `com.unity.ml-agents`. (#5259)

#### ml-agents / ml-agents-envs / gym-unity (Python)
- Some console output have been moved from `info` to `debug` and will not be printed by default. If you want all messages to be printed, you can run `mlagents-learn` with the `--debug` option or add the line `debug: true` at the top of the yaml config file. (#5211)
- When using a configuration YAML, it is required to define all behaviors found in a Unity
executable in the trainer configuration YAML, or specify `default_settings`. (#5210)
- The embedding size of attention layers used when a BufferSensor is in the scene has been changed. It is now fixed to 128 units. It might be impossible to resume training from a checkpoint of a previous version. (#5272)

### Bug Fixes
#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
- Fixed a bug where sensors and actuators could get sorted inconsistently on different systems to different Culture
settings. Unfortunately, this may require retraining models if it changes the resulting order of the sensors
or actuators on your system. (#5194)
- Removed additional memory allocations that were occurring due to assert messages and iterating of DemonstrationRecorders. (#5246)
- Fixed a bug where agent trying to access unintialized fields when creating a new RayPerceptionSensorComponent on an agent. (#5261)
- Fixed a bug where the DemonstrationRecorder would throw a null reference exception if Num Steps To Record was > 0 and Record was turned off. (#5274)

#### ml-agents / ml-agents-envs / gym-unity (Python)
- Fixed a bug where --results-dir has no effect. (#5269)
- Fixed a bug where old `.pt` checkpoints were not deleted during training. (#5271)
- The `UnityToGymWrapper` initializer now accepts an optional `action_space_seed` seed. If this is specified, it will
be used to set the random seed on the resulting action space. (#5303)


## [1.9.1-preview] - 2021-04-13
### Major Changes
#### ml-agents / ml-agents-envs / gym-unity (Python)
- The `--resume` flag now supports resuming experiments with additional reward providers or
 loading partial models if the network architecture has changed. See
 [here](https://github.com/Unity-Technologies/ml-agents/blob/release_16_docs/docs/Training-ML-Agents.md#loading-an-existing-model)
 for more details. (#5213)

### Bug Fixes
#### com.unity.ml-agents (C#)
- Fixed erroneous warnings when using the Demonstration Recorder. (#5216)

#### ml-agents / ml-agents-envs / gym-unity (Python)
- Fixed an issue which was causing increased variance when using LSTMs. Also fixed an issue with LSTM when used with POCA and `sequence_length` < `time_horizon`. (#5206)
- Fixed a bug where the SAC replay buffer would not be saved out at the end of a run, even if `save_replay_buffer` was enabled. (#5205)
- ELO now correctly resumes when loading from a checkpoint. (#5202)
- In the Python API, fixed `validate_action` to expect the right dimensions when `set_action_single_agent` is called. (#5208)
- In the `GymToUnityWrapper`, raise an appropriate warning if `step()` is called after an environment is done. (#5204)
- Fixed an issue where using one of the `gym` wrappers would override user-set log levels. (#5201)
## [1.9.0-preview] - 2021-03-17
### Major Changes
#### com.unity.ml-agents (C#)
- The `BufferSensor` and `BufferSensorComponent` have been added. They allow the Agent to observe variable number of entities. For an example, see the [Sorter environment](https://github.com/Unity-Technologies/ml-agents/blob/release_15_docs/docs/Learning-Environment-Examples.md#sorter). (#4909)
- The `SimpleMultiAgentGroup` class and `IMultiAgentGroup` interface have been added. These allow Agents to be given rewards and
  end episodes in groups. For examples, see the [Cooperative Push Block](https://github.com/Unity-Technologies/ml-agents/blob/release_15_docs/docs/Learning-Environment-Examples.md#cooperative-push-block), [Dungeon Escape](https://github.com/Unity-Technologies/ml-agents/blob/release_15_docs/docs/Learning-Environment-Examples.md#dungeon-escape) and [Soccer](https://github.com/Unity-Technologies/ml-agents/blob/release_15_docs/docs/Learning-Environment-Examples.md#soccer-twos) environments. (#4923)
#### ml-agents / ml-agents-envs / gym-unity (Python)
- The MA-POCA trainer has been added. This is a new trainer that enables Agents to learn how to work together in groups. Configure
  `poca` as the trainer in the configuration YAML after instantiating a `SimpleMultiAgentGroup` to use this feature. (#5005)

### Minor Changes
#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
- Updated com.unity.barracuda to 1.3.2-preview. (#5084)
- Added 3D Ball to the `com.unity.ml-agents` samples. (#5077)
- Make com.unity.modules.unityanalytics an optional dependency. (#5109)

#### ml-agents / ml-agents-envs / gym-unity (Python)
- The `encoding_size` setting for RewardSignals has been deprecated. Please use `network_settings` instead. (#4982)
- Sensor names are now passed through to `ObservationSpec.name`. (#5036)

### Bug Fixes
#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
#### ml-agents / ml-agents-envs / gym-unity (Python)
- An issue that caused `GAIL` to fail for environments where agents can terminate episodes by self-sacrifice has been fixed. (#4971)
- Made the error message when observations of different shapes are sent to the trainer clearer. (#5030)
- An issue that prevented curriculums from incrementing with self-play has been fixed. (#5098)

## [1.8.1-preview] - 2021-03-08
### Minor Changes
#### ml-agents / ml-agents-envs / gym-unity (Python)
- The `cattrs` version dependency was updated to allow `>=1.1.0` on Python 3.8 or higher. (#4821)

### Bug Fixes
#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
- Fix an issue where queuing InputEvents overwrote data from previous events in the same frame. (#5034)

## [1.8.0-preview] - 2021-02-17
### Major Changes
#### com.unity.ml-agents (C#)
#### ml-agents / ml-agents-envs / gym-unity (Python)
- TensorFlow trainers have been removed, please use the Torch trainers instead. (#4707)
- A plugin system for `mlagents-learn` has been added. You can now define custom
  `StatsWriter` implementations and register them to be called during training.
  More types of plugins will be added in the future. (#4788)

### Minor Changes
#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
- The `ActionSpec` constructor is now public. Previously, it was not possible to create an
  ActionSpec with both continuous and discrete actions from code. (#4896)
- `StatAggregationMethod.Sum` can now be passed to `StatsRecorder.Add()`. This
  will result in the values being summed (instead of averaged) when written to
  TensorBoard. Thanks to @brccabral for the contribution! (#4816)
- The upper limit for the time scale (by setting the `--time-scale` paramater in mlagents-learn) was
  removed when training with a player. The Editor still requires it to be clamped to 100. (#4867)
- Added the IHeuristicProvider interface to allow IActuators as well as Agent implement the Heuristic function to generate actions.
  Updated the Basic example and the Match3 Example to use Actuators.
  Changed the namespace and file names of classes in com.unity.ml-agents.extensions. (#4849)
- Added `VectorSensor.AddObservation(IList<float>)`. `VectorSensor.AddObservation(IEnumerable<float>)`
  is deprecated. The `IList` version is recommended, as it does not generate any
  additional memory allocations. (#4887)
- Added `ObservationWriter.AddList()` and deprecated `ObservationWriter.AddRange()`.
  `AddList()` is recommended, as it does not generate any additional memory allocations. (#4887)
- The Barracuda dependency was upgraded to 1.3.0. (#4898)
- Added `ActuatorComponent.CreateActuators`, and deprecate `ActuatorComponent.CreateActuator`.  The
  default implementation will wrap `ActuatorComponent.CreateActuator` in an array and return that. (#4899)
- `InferenceDevice.Burst` was added, indicating that Agent's model will be run using Barracuda's Burst backend.
  This is the default for new Agents, but existing ones that use `InferenceDevice.CPU` should update to
  `InferenceDevice.Burst`. (#4925)
- Add an InputActuatorComponent to allow the generation of Agent action spaces from an InputActionAsset.
  Projects wanting to use this feature will need to add the
  [Input System Package](https://docs.unity3d.com/Packages/com.unity.inputsystem@1.1/manual/index.html)
  at version 1.1.0-preview.3 or later. (#4881)

#### ml-agents / ml-agents-envs / gym-unity (Python)
- Tensorboard now logs the Environment Reward as both a scalar and a histogram. (#4878)
- Added a `--torch-device` commandline option to `mlagents-learn`, which sets the default
  [`torch.device`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.device) used for training. (#4888)
- The `--cpu` commandline option had no effect and was removed. Use `--torch-device=cpu` to force CPU training. (#4888)
- The `mlagents_env` API has changed, `BehaviorSpec` now has a `observation_specs` property containing a list of `ObservationSpec`. For more information on `ObservationSpec` see [here](https://github.com/Unity-Technologies/ml-agents/blob/release_13_docs/docs/Python-API.md#behaviorspec). (#4763, #4825)

### Bug Fixes
#### com.unity.ml-agents (C#)
- Fix a compile warning about using an obsolete enum in `GrpcExtensions.cs`. (#4812)
- CameraSensor now logs an error if the GraphicsDevice is null. (#4880)
- Removed unnecessary memory allocations in `ActuatorManager.UpdateActionArray()` (#4877)
- Removed unnecessary memory allocations in `SensorShapeValidator.ValidateSensors()` (#4879)
- Removed unnecessary memory allocations in `SideChannelManager.GetSideChannelMessage()` (#4886)
- Removed several memory allocations that happened during inference. On a test scene, this
  reduced the amount of memory allocated by approximately 25%. (#4887)
- Removed several memory allocations that happened during inference with discrete actions. (#4922)
- Properly catch permission errors when writing timer files. (#4921)
- Unexpected exceptions during training initialization and shutdown are now logged. If you see
  "noisy" logs, please let us know! (#4930, #4935)

#### ml-agents / ml-agents-envs / gym-unity (Python)
- Fixed a bug that would cause an exception when `RunOptions` was deserialized via `pickle`. (#4842)
- Fixed a bug that can cause a crash if a behavior can appear during training in multi-environment training. (#4872)
- Fixed the computation of entropy for continuous actions. (#4869)
- Fixed a bug that would cause `UnityEnvironment` to wait the full timeout
  period and report a misleading error message if the executable crashed
  without closing the connection. It now periodically checks the process status
  while waiting for a connection, and raises a better error message if it crashes. (#4880)
- Passing a `-logfile` option in the `--env-args` option to `mlagents-learn` is
  no longer overwritten. (#4880)
- The `load_weights` function was being called unnecessarily often in the Ghost Trainer leading to training slowdowns. (#4934)


## [1.7.2-preview] - 2020-12-22
### Bug Fixes
#### com.unity.ml-agents (C#)
- Add analytics package dependency to the package manifest. (#4794)
#### ml-agents / ml-agents-envs / gym-unity (Python)
- Fixed the docker build process. (#4791)


## [1.7.0-preview] - 2020-12-21
### Major Changes
#### com.unity.ml-agents (C#)
#### ml-agents / ml-agents-envs / gym-unity (Python)
- PyTorch trainers now support training agents with both continuous and discrete action spaces. (#4702)
The `.onnx` models generated by the trainers of this release are incompatible with versions of Barracuda before `1.2.1-preview`. If you upgrade the trainers, you must upgrade the version of the Barracuda package as well (which can be done by upgrading the `com.unity.ml-agents` package).
### Minor Changes
#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
- Agents with both continuous and discrete actions are now supported. You can specify
both continuous and discrete action sizes in Behavior Parameters. (#4702, #4718)
- In order to improve the developer experience for Unity ML-Agents Toolkit, we have added in-editor analytics.
Please refer to "Information that is passively collected by Unity" in the
[Unity Privacy Policy](https://unity3d.com/legal/privacy-policy). (#4677)
- The FoodCollector example environment now uses continuous actions for moving and
discrete actions for shooting. (#4746)
#### ml-agents / ml-agents-envs / gym-unity (Python)
- `ActionSpec.validate_action()` now enforces that `UnityEnvironment.set_action_for_agent()` receives a 1D `np.array`. (#4691)

### Bug Fixes
#### com.unity.ml-agents (C#)
- Removed noisy warnings about API minor version mismatches in both the C# and python code. (#4688)
#### ml-agents / ml-agents-envs / gym-unity (Python)


## [1.6.0-preview] - 2020-11-18
### Major Changes
#### com.unity.ml-agents (C#)
#### ml-agents / ml-agents-envs / gym-unity (Python)
 - PyTorch trainers are now the default. See the
 [installation docs](https://github.com/Unity-Technologies/ml-agents/blob/release_10_docs/docs/Installation.md) for
 more information on installing PyTorch. For the time being, TensorFlow is still available;
 you can use the TensorFlow backend by adding `--tensorflow` to the CLI, or
 adding `framework: tensorflow` in the configuration YAML. (#4517)

### Minor Changes
#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
- The Barracuda dependency was upgraded to 1.1.2 (#4571)
- Utilities were added to `com.unity.ml-agents.extensions` to make it easier to
integrate with match-3 games. See the [readme](https://github.com/Unity-Technologies/ml-agents/blob/release_10_docs/com.unity.ml-agents.extensions/Documentation~/Match3.md)
for more details. (#4515)
#### ml-agents / ml-agents-envs / gym-unity (Python)
- The `action_probs` node is no longer listed as an output in TensorFlow models (#4613).

### Bug Fixes
#### com.unity.ml-agents (C#)
- `Agent.CollectObservations()` and `Agent.EndEpisode()` will now throw an exception
if they are called recursively (for example, if they call `Agent.EndEpisode()`).
Previously, this would result in an infinite loop and cause the editor to hang. (#4573)
#### ml-agents / ml-agents-envs / gym-unity (Python)
- Fixed an issue where runs could not be resumed when using TensorFlow and Ghost Training. (#4593)
- Change the tensor type of step count from int32 to int64 to address the overflow issue when step
goes larger than 2^31. Previous Tensorflow checkpoints will become incompatible and cannot be loaded. (#4607)
- Remove extra period after "Training" in console log. (#4674)


## [1.5.0-preview] - 2020-10-14
### Major Changes
#### com.unity.ml-agents (C#)
#### ml-agents / ml-agents-envs / gym-unity (Python)
 - Added the Random Network Distillation (RND) intrinsic reward signal to the Pytorch
 trainers. To use RND, add a `rnd` section to the `reward_signals` section of your
 yaml configuration file. [More information here](https://github.com/Unity-Technologies/ml-agents/blob/release_9_docs/docs/Training-Configuration-File.md#rnd-intrinsic-reward) (#4473)
### Minor Changes
#### com.unity.ml-agents (C#)
 - Stacking for compressed observations is now supported. An additional setting
 option `Observation Stacks` is added in editor to sensor components that support
 compressed observations. A new class `ISparseChannelSensor` with an
 additional method `GetCompressedChannelMapping()`is added to generate a mapping
 of the channels in compressed data to the actual channel after decompression,
 for the python side to decompress correctly. (#4476)
 - Added a new visual 3DBall environment. (#4513)
#### ml-agents / ml-agents-envs / gym-unity (Python)
 - The Communication API was changed to 1.2.0 to indicate support for stacked
 compressed observation. A new entry `compressed_channel_mapping` is added to the
 proto to handle decompression correctly. Newer versions of the package that wish to
 make use of this will also need a compatible version of the Python trainers. (#4476)
 - In the `VisualFoodCollector` scene, a vector flag representing the frozen state of
 the agent is added to the input observations in addition to the original first-person
 camera frame. The scene is able to train with the provided default config file. (#4511)
 - Added conversion to string for sampler classes to increase the verbosity of
 the curriculum lesson changes. The lesson updates would now output the sampler
 stats in addition to the lesson and parameter name to the console.  (#4484)
 - Localized documentation in Russian is added. Thanks to @SergeyMatrosov for
 the contribution. (#4529)
### Bug Fixes
#### com.unity.ml-agents (C#)
 - Fixed a bug where accessing the Academy outside of play mode would cause the
 Academy to get stepped multiple times when in play mode. (#4532)
#### ml-agents / ml-agents-envs / gym-unity (Python)


## [1.4.0-preview] - 2020-09-16
### Major Changes
#### com.unity.ml-agents (C#)
#### ml-agents / ml-agents-envs / gym-unity (Python)

### Minor Changes
#### com.unity.ml-agents (C#)
- The `IActuator` interface and `ActuatorComponent` abstract class were added.
These are analogous to `ISensor` and `SensorComponent`, but for applying actions
for an Agent. They allow you to control the action space more programmatically
than defining the actions in the Agent's Behavior Parameters. See
[BasicActuatorComponent.cs](https://github.com/Unity-Technologies/ml-agents/blob/release_7_docs/Project/Assets/ML-Agents/Examples/Basic/Scripts/BasicActuatorComponent.cs)
 for an example of how to use them. (#4297, #4315)
- Update Barracuda to 1.1.1-preview (#4482)
- Enabled C# formatting using `dotnet-format`. (#4362)
- GridSensor was added to the `com.unity.ml-agents.extensions` package. Thank you
to Jaden Travnik from Eidos Montreal for the contribution! (#4399)
- Added `Agent.EpisodeInterrupted()`, which can be used to reset the agent when
it has reached a user-determined maximum number of steps. This behaves similarly
to `Agent.EndEpsiode()` but has a slightly different effect on training (#4453).
#### ml-agents / ml-agents-envs / gym-unity (Python)
- Experimental PyTorch support has been added. Use `--torch` when running `mlagents-learn`, or add
`framework: pytorch` to your trainer configuration (under the behavior name) to enable it.
Note that PyTorch 1.6.0 or greater should be installed to use this feature; see
[the PyTorch website](https://pytorch.org/) for installation instructions and
[the relevant ML-Agents docs](https://github.com/Unity-Technologies/ml-agents/blob/release_7_docs/docs/Training-ML-Agents.md#using-pytorch-experimental) for usage. (#4335)
- The minimum supported version of TensorFlow was increased to 1.14.0. (#4411)
- Compressed visual observations with >3 channels are now supported. In
`ISensor.GetCompressedObservation()`, this can be done by writing 3 channels at a
time to a PNG and concatenating the resulting bytes. (#4399)
- The Communication API was changed to 1.1.0 to indicate support for concatenated PNGs
(see above). Newer versions of the package that wish to make use of this will also need
a compatible version of the trainer. (#4462)
- A CNN (`vis_encode_type: match3`) for smaller grids, e.g. board games, has been added.
(#4434)
- You can now again specify a default configuration for your behaviors. Specify `default_settings` in
your trainer configuration to do so. (#4448)
- Improved the executable detection logic for environments on Windows. (#4485)

### Bug Fixes
#### com.unity.ml-agents (C#)
- Previously, `com.unity.ml-agents` was not declaring built-in packages as
dependencies in its package.json. The relevant dependencies are now listed. (#4384)
- Agents no longer try to send observations when they become disabled if the
Academy has been shut down. (#4489)
#### ml-agents / ml-agents-envs / gym-unity (Python)
- Fixed the sample code in the custom SideChannel example. (#4466)
- A bug in the observation normalizer that would cause rewards to decrease
when using `--resume` was fixed. (#4463)
- Fixed a bug in exporting Pytorch models when using multiple discrete actions. (#4491)

## [1.3.0-preview] - 2020-08-12

### Major Changes
#### com.unity.ml-agents (C#)
#### ml-agents / ml-agents-envs / gym-unity (Python)
- The minimum supported Python version for ml-agents-envs was changed to 3.6.1. (#4244)
- The interaction between EnvManager and TrainerController was changed; EnvManager.advance() was split into to stages,
and TrainerController now uses the results from the first stage to handle new behavior names. This change speeds up
Python training by approximately 5-10%. (#4259)

### Minor Changes
#### com.unity.ml-agents (C#)
- StatsSideChannel now stores multiple values per key. This means that multiple
calls to `StatsRecorder.Add()` with the same key in the same step will no
longer overwrite each other. (#4236)
#### ml-agents / ml-agents-envs / gym-unity (Python)
- The versions of `numpy` supported by ml-agents-envs were changed to disallow 1.19.0 or later. This was done to reflect
a similar change in TensorFlow's requirements. (#4274)
- Model checkpoints are now also saved as .nn files during training. (#4127)
- Model checkpoint info is saved in TrainingStatus.json after training is concluded (#4127)
- CSV statistics writer was removed (#4300).

### Bug Fixes
#### com.unity.ml-agents (C#)
- Academy.EnvironmentStep() will now throw an exception if it is called
recursively (for example, by an Agent's CollectObservations method).
Previously, this would result in an infinite loop and cause the editor to hang.
(#4226)
#### ml-agents / ml-agents-envs / gym-unity (Python)
- The algorithm used to normalize observations was introducing NaNs if the initial observations were too large
due to incorrect initialization. The initialization was fixed and is now the observation means from the
first trajectory processed. (#4299)

## [1.2.0-preview] - 2020-07-15

### Major Changes
#### ml-agents / ml-agents-envs / gym-unity (Python)
- The Parameter Randomization feature has been refactored to enable sampling of new parameters per episode to improve robustness. The
  `resampling-interval` parameter has been removed and the config structure updated. More information [here](https://github.com/Unity-Technologies/ml-agents/blob/release_5_docs/docs/Training-ML-Agents.md). (#4065)
- The Parameter Randomization feature has been merged with the Curriculum feature. It is now possible to specify a sampler
in the lesson of a Curriculum. Curriculum has been refactored and is now specified at the level of the parameter, not the
behavior. More information
[here](https://github.com/Unity-Technologies/ml-agents/blob/release_5_docs/docs/Training-ML-Agents.md).(#4160)

### Minor Changes
#### com.unity.ml-agents (C#)
- `SideChannelsManager` was renamed to `SideChannelManager`. The old name is still supported, but deprecated. (#4137)
- `RayPerceptionSensor.Perceive()` now additionally store the GameObject that was hit by the ray. (#4111)
- The Barracuda dependency was upgraded to 1.0.1 (#4188)
#### ml-agents / ml-agents-envs / gym-unity (Python)
- Added new Google Colab notebooks to show how to use `UnityEnvironment'. (#4117)

### Bug Fixes
#### com.unity.ml-agents (C#)
- Fixed an issue where RayPerceptionSensor would raise an exception when the
list of tags was empty, or a tag in the list was invalid (unknown, null, or
empty string). (#4155)

#### ml-agents / ml-agents-envs / gym-unity (Python)
- Fixed an error when setting `initialize_from` in the trainer confiiguration YAML to
`null`. (#4175)
- Fixed issue with FoodCollector, Soccer, and WallJump when playing with keyboard. (#4147, #4174)
- Fixed a crash in StatsReporter when using threaded trainers with very frequent summary writes
(#4201)
- `mlagents-learn` will now raise an error immediately if `--num-envs` is greater than 1 without setting the `--env`
argument. (#4203)

## [1.1.0-preview] - 2020-06-10
### Major Changes
#### com.unity.ml-agents (C#)
#### ml-agents / ml-agents-envs / gym-unity (Python)
- Added new Walker environments. Improved ragdoll stability/performance. (#4037)
- `max_step` in the `TerminalStep` and `TerminalSteps` objects was renamed `interrupted`.
- `beta` and `epsilon` in `PPO` are no longer decayed by default but follow the same schedule as learning rate. (#3940)
- `get_behavior_names()` and `get_behavior_spec()` on UnityEnvironment were replaced by the `behavior_specs` property. (#3946)
- The first version of the Unity Environment Registry (Experimental) has been released. More information [here](https://github.com/Unity-Technologies/ml-agents/blob/release_5_docs/docs/Unity-Environment-Registry.md)(#3967)
- `use_visual` and `allow_multiple_visual_obs` in the `UnityToGymWrapper` constructor
were replaced by `allow_multiple_obs` which allows one or more visual observations and
vector observations to be used simultaneously. (#3981) Thank you @shakenes !
- Curriculum and Parameter Randomization configurations have been merged
  into the main training configuration file. Note that this means training
  configuration files are now environment-specific. (#3791)
- The format for trainer configuration has changed, and the "default" behavior has been deprecated.
  See the [Migration Guide](https://github.com/Unity-Technologies/ml-agents/blob/release_5_docs/docs/Migrating.md) for more details. (#3936)
- Training artifacts (trained models, summaries) are now found in the `results/`
  directory. (#3829)
- When using Curriculum, the current lesson will resume if training is quit and resumed. As such,
  the `--lesson` CLI option has been removed. (#4025)
### Minor Changes
#### com.unity.ml-agents (C#)
- `ObservableAttribute` was added. Adding the attribute to fields or properties on an Agent will allow it to generate
  observations via reflection. (#3925, #4006)
#### ml-agents / ml-agents-envs / gym-unity (Python)
- Unity Player logs are now written out to the results directory. (#3877)
- Run configuration YAML files are written out to the results directory at the end of the run. (#3815)
- The `--save-freq` CLI option has been removed, and replaced by a `checkpoint_interval` option in the trainer configuration YAML. (#4034)
- When trying to load/resume from a checkpoint created with an earlier verison of ML-Agents,
  a warning will be thrown. (#4035)
### Bug Fixes
- Fixed an issue where SAC would perform too many model updates when resuming from a
  checkpoint, and too few when using `buffer_init_steps`. (#4038)
- Fixed a bug in the onnx export that would cause constants needed for inference to not be visible to some versions of
  the Barracuda importer. (#4073)
#### com.unity.ml-agents (C#)
#### ml-agents / ml-agents-envs / gym-unity (Python)


## [1.0.2-preview] - 2020-05-20
### Bug Fixes
#### com.unity.ml-agents (C#)
- Fix missing .meta file


## [1.0.1-preview] - 2020-05-19
### Bug Fixes
#### com.unity.ml-agents (C#)
- A bug that would cause the editor to go into a loop when a prefab was selected was fixed. (#3949)
- BrainParameters.ToProto() no longer throws an exception if none of the fields have been set. (#3930)
- The Barracuda dependency was upgraded to 0.7.1-preview. (#3977)
#### ml-agents / ml-agents-envs / gym-unity (Python)
- An issue was fixed where using `--initialize-from` would resume from the past step count. (#3962)
- The gym wrapper error for the wrong number of agents now fires more consistently, and more details
  were added to the error message when the input dimension is wrong. (#3963)


## [1.0.0-preview] - 2020-04-30
### Major Changes
#### com.unity.ml-agents (C#)

- The `MLAgents` C# namespace was renamed to `Unity.MLAgents`, and other nested
  namespaces were similarly renamed. (#3843)
- The offset logic was removed from DecisionRequester. (#3716)
- The signature of `Agent.Heuristic()` was changed to take a float array as a
  parameter, instead of returning the array. This was done to prevent a common
  source of error where users would return arrays of the wrong size. (#3765)
- The communication API version has been bumped up to 1.0.0 and will use
  [Semantic Versioning](https://semver.org/) to do compatibility checks for
  communication between Unity and the Python process. (#3760)
- The obsolete `Agent` methods `GiveModel`, `Done`, `InitializeAgent`,
  `AgentAction` and `AgentReset` have been removed. (#3770)
- The SideChannel API has changed:
  - Introduced the `SideChannelManager` to register, unregister and access side
    channels. (#3807)
  - `Academy.FloatProperties` was replaced by `Academy.EnvironmentParameters`.
    See the [Migration Guide](https://github.com/Unity-Technologies/ml-agents/blob/release_1_docs/docs/Migrating.md)
    for more details on upgrading. (#3807)
  - `SideChannel.OnMessageReceived` is now a protected method (was public)
  - SideChannel IncomingMessages methods now take an optional default argument,
    which is used when trying to read more data than the message contains. (#3751)
  - Added a feature to allow sending stats from C# environments to TensorBoard
    (and other python StatsWriters). To do this from your code, use
    `Academy.Instance.StatsRecorder.Add(key, value)`. (#3660)
- `CameraSensorComponent.m_Grayscale` and
  `RenderTextureSensorComponent.m_Grayscale` were changed from `public` to
  `private`. These can still be accessed via their corresponding properties.
  (#3808)
- Public fields and properties on several classes were renamed to follow Unity's
  C# style conventions. All public fields and properties now use "PascalCase"
  instead of "camelCase"; for example, `Agent.maxStep` was renamed to
  `Agent.MaxStep`. For a full list of changes, see the pull request. (#3828)
- `WriteAdapter` was renamed to `ObservationWriter`. If you have a custom
  `ISensor` implementation, you will need to change the signature of its
  `Write()` method. (#3834)
- The Barracuda dependency was upgraded to 0.7.0-preview (which has breaking
  namespace and assembly name changes). (#3875)

#### ml-agents / ml-agents-envs / gym-unity (Python)

- The `--load` and `--train` command-line flags have been deprecated. Training
  now happens by default, and use `--resume` to resume training instead of
  `--load`. (#3705)
- The Jupyter notebooks have been removed from the repository. (#3704)
- The multi-agent gym option was removed from the gym wrapper. For multi-agent
  scenarios, use the [Low Level Python API](https://github.com/Unity-Technologies/ml-agents/blob/release_1_docs/docs/Python-API.md). (#3681)
- The low level Python API has changed. You can look at the document
  [Low Level Python API](https://github.com/Unity-Technologies/ml-agents/blob/release_1_docs/docs/Python-API.md)
  documentation for more information. If you use `mlagents-learn` for training, this should be a
  transparent change. (#3681)
- Added ability to start training (initialize model weights) from a previous run
  ID. (#3710)
- The GhostTrainer has been extended to support asymmetric games and the
  asymmetric example environment Strikers Vs. Goalie has been added. (#3653)
- The `UnityEnv` class from the `gym-unity` package was renamed
  `UnityToGymWrapper` and no longer creates the `UnityEnvironment`. Instead, the
  `UnityEnvironment` must be passed as input to the constructor of
  `UnityToGymWrapper` (#3812)

### Minor Changes

#### com.unity.ml-agents (C#)

- Added new 3-joint Worm ragdoll environment. (#3798)
- `StackingSensor` was changed from `internal` visibility to `public`. (#3701)
- The internal event `Academy.AgentSetStatus` was renamed to
  `Academy.AgentPreStep` and made public. (#3716)
- Academy.InferenceSeed property was added. This is used to initialize the
  random number generator in ModelRunner, and is incremented for each
  ModelRunner. (#3823)
- `Agent.GetObservations()` was added, which returns a read-only view of the
  observations added in `CollectObservations()`. (#3825)
- `UnityRLCapabilities` was added to help inform users when RL features are
  mismatched between C# and Python packages. (#3831)

#### ml-agents / ml-agents-envs / gym-unity (Python)

- Format of console output has changed slightly and now matches the name of the
  model/summary directory. (#3630, #3616)
- Renamed 'Generalization' feature to 'Environment Parameter Randomization'.
  (#3646)
- Timer files now contain a dictionary of metadata, including things like the
  package version numbers. (#3758)
- The way that UnityEnvironment decides the port was changed. If no port is
  specified, the behavior will depend on the `file_name` parameter. If it is
  `None`, 5004 (the editor port) will be used; otherwise 5005 (the base
  environment port) will be used. (#3673)
- Running `mlagents-learn` with the same `--run-id` twice will no longer
  overwrite the existing files. (#3705)
- Model updates can now happen asynchronously with environment steps for better
  performance. (#3690)
- `num_updates` and `train_interval` for SAC were replaced with
  `steps_per_update`. (#3690)
- The maximum compatible version of tensorflow was changed to allow tensorflow
  2.1 and 2.2. This will allow use with python 3.8 using tensorflow 2.2.0rc3.
  (#3830)
- `mlagents-learn` will no longer set the width and height of the executable
  window to 84x84 when no width nor height arguments are given. (#3867)

### Bug Fixes

#### com.unity.ml-agents (C#)

- Fixed a display bug when viewing Demonstration files in the inspector. The
  shapes of the observations in the file now display correctly. (#3771)

#### ml-agents / ml-agents-envs / gym-unity (Python)

- Fixed an issue where exceptions from environments provided a return code of 0.
  (#3680)
- Self-Play team changes will now trigger a full environment reset. This
  prevents trajectories in progress during a team change from getting into the
  buffer. (#3870)

## [0.15.1-preview] - 2020-03-30

### Bug Fixes

- Raise the wall in CrawlerStatic scene to prevent Agent from falling off.
  (#3650)
- Fixed an issue where specifying `vis_encode_type` was required only for SAC.
  (#3677)
- Fixed the reported entropy values for continuous actions (#3684)
- Fixed an issue where switching models using `SetModel()` during training would
  use an excessive amount of memory. (#3664)
- Environment subprocesses now close immediately on timeout or wrong API
  version. (#3679)
- Fixed an issue in the gym wrapper that would raise an exception if an Agent
  called EndEpisode multiple times in the same step. (#3700)
- Fixed an issue where logging output was not visible; logging levels are now
  set consistently. (#3703)

## [0.15.0-preview] - 2020-03-18

### Major Changes

- `Agent.CollectObservations` now takes a VectorSensor argument. (#3352, #3389)
- Added `Agent.CollectDiscreteActionMasks` virtual method with a
  `DiscreteActionMasker` argument to specify which discrete actions are
  unavailable to the Agent. (#3525)
- Beta support for ONNX export was added. If the `tf2onnx` python package is
  installed, models will be saved to `.onnx` as well as `.nn` format. Note that
  Barracuda 0.6.0 or later is required to import the `.onnx` files properly
- Multi-GPU training and the `--multi-gpu` option has been removed temporarily.
  (#3345)
- All Sensor related code has been moved to the namespace `MLAgents.Sensors`.
- All SideChannel related code has been moved to the namespace
  `MLAgents.SideChannels`.
- `BrainParameters` and `SpaceType` have been removed from the public API
- `BehaviorParameters` have been removed from the public API.
- The following methods in the `Agent` class have been deprecated and will be
  removed in a later release:
  - `InitializeAgent()` was renamed to `Initialize()`
  - `AgentAction()` was renamed to `OnActionReceived()`
  - `AgentReset()` was renamed to `OnEpisodeBegin()`
  - `Done()` was renamed to `EndEpisode()`
  - `GiveModel()` was renamed to `SetModel()`

### Minor Changes

- Monitor.cs was moved to Examples. (#3372)
- Automatic stepping for Academy is now controlled from the
  AutomaticSteppingEnabled property. (#3376)
- The GetEpisodeCount, GetStepCount, GetTotalStepCount and methods of Academy
  were changed to EpisodeCount, StepCount, TotalStepCount properties
  respectively. (#3376)
- Several classes were changed from public to internal visibility. (#3390)
- Academy.RegisterSideChannel and UnregisterSideChannel methods were added.
  (#3391)
- A tutorial on adding custom SideChannels was added (#3391)
- The stepping logic for the Agent and the Academy has been simplified (#3448)
- Update Barracuda to 0.6.1-preview

* The interface for `RayPerceptionSensor.PerceiveStatic()` was changed to take
  an input class and write to an output class, and the method was renamed to
  `Perceive()`.

- The checkpoint file suffix was changed from `.cptk` to `.ckpt` (#3470)
- The command-line argument used to determine the port that an environment will
  listen on was changed from `--port` to `--mlagents-port`.
- `DemonstrationRecorder` can now record observations outside of the editor.
- `DemonstrationRecorder` now has an optional path for the demonstrations. This
  will default to `Application.dataPath` if not set.
- `DemonstrationStore` was changed to accept a `Stream` for its constructor, and
  was renamed to `DemonstrationWriter`
- The method `GetStepCount()` on the Agent class has been replaced with the
  property getter `StepCount`
- `RayPerceptionSensorComponent` and related classes now display the debug
  gizmos whenever the Agent is selected (not just Play mode).
- Most fields on `RayPerceptionSensorComponent` can now be changed while the
  editor is in Play mode. The exceptions to this are fields that affect the
  number of observations.
- Most fields on `CameraSensorComponent` and `RenderTextureSensorComponent` were
  changed to private and replaced by properties with the same name.
- Unused static methods from the `Utilities` class (ShiftLeft, ReplaceRange,
  AddRangeNoAlloc, and GetSensorFloatObservationSize) were removed.
- The `Agent` class is no longer abstract.
- SensorBase was moved out of the package and into the Examples directory.
- `AgentInfo.actionMasks` has been renamed to `AgentInfo.discreteActionMasks`.
- `DecisionRequester` has been made internal (you can still use the
  DecisionRequesterComponent from the inspector). `RepeatAction` was renamed
  `TakeActionsBetweenDecisions` for clarity. (#3555)
- The `IFloatProperties` interface has been removed.
- Fix #3579.
- Improved inference performance for models with multiple action branches.
  (#3598)
- Fixed an issue when using GAIL with less than `batch_size` number of
  demonstrations. (#3591)
- The interfaces to the `SideChannel` classes (on C# and python) have changed to
  use new `IncomingMessage` and `OutgoingMessage` classes. These should make
  reading and writing data to the channel easier. (#3596)
- Updated the ExpertPyramid.demo example demonstration file (#3613)
- Updated project version for example environments to 2018.4.18f1. (#3618)
- Changed the Product Name in the example environments to remove spaces, so that
  the default build executable file doesn't contain spaces. (#3612)

## [0.14.1-preview] - 2020-02-25

### Bug Fixes

- Fixed an issue which caused self-play training sessions to consume a lot of
  memory. (#3451)
- Fixed an IndexError when using GAIL or behavioral cloning with demonstrations
  recorded with 0.14.0 or later (#3464)
- Updated the `gail_config.yaml` to work with per-Agent steps (#3475)
- Fixed demonstration recording of experiences when the Agent is done. (#3463)
- Fixed a bug with the rewards of multiple Agents in the gym interface (#3471,
  #3496)

## [0.14.0-preview] - 2020-02-13

### Major Changes

- A new self-play mechanism for training agents in adversarial scenarios was
  added (#3194)
- Tennis and Soccer environments were refactored to enable training with
  self-play (#3194, #3331)
- UnitySDK folder was split into a Unity Package (com.unity.ml-agents) and our
  examples were moved to the Project folder (#3267)
- Academy is now a singleton and is no longer abstract (#3210, #3184)
- In order to reduce the size of the API, several classes and methods were
  marked as internal or private. Some public fields on the Agent were trimmed
  (#3342, #3353, #3269)
- Decision Period and on-demand decision checkboxes were removed from the Agent.
  on-demand decision is now the default (#3243)
- Calling Done() on the Agent will reset it immediately and call the AgentReset
  virtual method (#3291, #3242)
- The "Reset on Done" setting in AgentParameters was removed; this is now always
  true. AgentOnDone virtual method on the Agent was removed (#3311, #3222)
- Trainer steps are now counted per-Agent, not per-environment as in previous
  versions. For instance, if you have 10 Agents in the scene, 20 environment
  steps now correspond to 200 steps as printed in the terminal and in
  Tensorboard (#3113)

### Minor Changes

- Barracuda was updated to 0.5.0-preview (#3329)
- --num-runs option was removed from mlagents-learn (#3155)
- Curriculum config files are now YAML formatted and all curricula for a
  training run are combined into a single file (#3186)
- ML-Agents components, such as BehaviorParameters and various Sensor
  implementations, now appear in the Components menu (#3231)
- Exceptions are now raised in Unity (in debug mode only) if NaN observations or
  rewards are passed (#3221)
- RayPerception MonoBehavior, which was previously deprecated, was removed
  (#3304)
- Uncompressed visual (i.e. 3d float arrays) observations are now supported.
  CameraSensorComponent and RenderTextureSensor now have an option to write
  uncompressed observations (#3148)
- Agent’s handling of observations during training was improved so that an extra
  copy of the observations is no longer maintained (#3229)
- Error message for missing trainer config files was improved to include the
  absolute path (#3230)
- Support for 2017.4 LTS was dropped (#3121, #3168)
- Some documentation improvements were made (#3296, #3292, #3295, #3281)

### Bug Fixes

- Numpy warning when stats don’t exist (#3251)
- A bug that caused RayPerceptionSensor to behave inconsistently with transforms
  that have non-1 scale was fixed (#3321)
- Some small bugfixes to tensorflow_to_barracuda.py were backported from the
  barracuda release (#3341)
- Base port in the jupyter notebook example was updated to use the same port
  that the editor uses (#3283)

## [0.13.0-preview] - 2020-01-24

### This is the first release of _Unity Package ML-Agents_.

 - Initial release.


## Links discovered
- [Keep a Changelog](http://keepachangelog.com/en/1.0.0/)
- [Semantic Versioning](http://semver.org/spec/v2.0.0.html)
- [here](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Python-Custom-Trainer-Plugin.md)
- [Migration Guide](https://github.com/Unity-Technologies/ml-agents/blob/release_17_docs/docs/Migrating.md)
- [GridWorld environment](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#gridworld)
- [here](https://github.com/Unity-Technologies/ml-agents/blob/release_16_docs/docs/Training-ML-Agents.md#loading-an-existing-model)
- [Sorter environment](https://github.com/Unity-Technologies/ml-agents/blob/release_15_docs/docs/Learning-Environment-Examples.md#sorter)
- [Cooperative Push Block](https://github.com/Unity-Technologies/ml-agents/blob/release_15_docs/docs/Learning-Environment-Examples.md#cooperative-push-block)
- [Dungeon Escape](https://github.com/Unity-Technologies/ml-agents/blob/release_15_docs/docs/Learning-Environment-Examples.md#dungeon-escape)
- [Soccer](https://github.com/Unity-Technologies/ml-agents/blob/release_15_docs/docs/Learning-Environment-Examples.md#soccer-twos)
- [Input System Package](https://docs.unity3d.com/Packages/com.unity.inputsystem@1.1/manual/index.html)
- [`torch.device`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.device)
- [here](https://github.com/Unity-Technologies/ml-agents/blob/release_13_docs/docs/Python-API.md#behaviorspec)
- [Unity Privacy Policy](https://unity3d.com/legal/privacy-policy)
- [installation docs](https://github.com/Unity-Technologies/ml-agents/blob/release_10_docs/docs/Installation.md)
- [readme](https://github.com/Unity-Technologies/ml-agents/blob/release_10_docs/com.unity.ml-agents.extensions/Documentation~/Match3.md)
- [More information here](https://github.com/Unity-Technologies/ml-agents/blob/release_9_docs/docs/Training-Configuration-File.md#rnd-intrinsic-reward)
- [BasicActuatorComponent.cs](https://github.com/Unity-Technologies/ml-agents/blob/release_7_docs/Project/Assets/ML-Agents/Examples/Basic/Scripts/BasicActuatorComponent.cs)
- [the PyTorch website](https://pytorch.org/)
- [the relevant ML-Agents docs](https://github.com/Unity-Technologies/ml-agents/blob/release_7_docs/docs/Training-ML-Agents.md#using-pytorch-experimental)
- [here](https://github.com/Unity-Technologies/ml-agents/blob/release_5_docs/docs/Training-ML-Agents.md)
- [here](https://github.com/Unity-Technologies/ml-agents/blob/release_5_docs/docs/Unity-Environment-Registry.md)
- [Migration Guide](https://github.com/Unity-Technologies/ml-agents/blob/release_5_docs/docs/Migrating.md)
- [Semantic Versioning](https://semver.org/)
- [Migration Guide](https://github.com/Unity-Technologies/ml-agents/blob/release_1_docs/docs/Migrating.md)
- [Low Level Python API](https://github.com/Unity-Technologies/ml-agents/blob/release_1_docs/docs/Python-API.md)

--- com.unity.ml-agents/CONTRIBUTING.md ---
# Contribution Guidelines

Thank you for your interest in contributing to the ML-Agents Toolkit! We are
incredibly excited to see how members of our community will use and extend the
ML-Agents Toolkit. To facilitate your contributions, we've outlined a brief set
of guidelines to ensure that your extensions can be easily integrated.

## Communication

First, please read through our
[code of conduct](https://github.com/Unity-Technologies/ml-agents/blob/main/CODE_OF_CONDUCT.md),
as we expect all our contributors to follow it.

Second, before starting on a project that you intend to contribute to the
ML-Agents Toolkit (whether environments or modifications to the codebase), we
**strongly** recommend posting on our
[Issues page](https://github.com/Unity-Technologies/ml-agents/issues) and
briefly outlining the changes you plan to make. This will enable us to provide
some context that may be helpful for you. This could range from advice and
feedback on how to optimally perform your changes or reasons for not doing it.

Lastly, if you're looking for input on what to contribute, feel free to reach
out to us directly at ml-agents@unity3d.com and/or browse the GitHub issues with
the `Requests` or `Bug` label.

## Git Branches

The main branch corresponds to the most recent version of the project. Note
that this may be newer that the
[latest release](https://github.com/Unity-Technologies/ml-agents/releases/tag/latest_release).

When contributing to the project, please make sure that your Pull Request (PR)
contains the following:

- Detailed description of the changes performed
- Corresponding changes to documentation, unit tests and sample environments (if
  applicable)
- Summary of the tests performed to validate your changes
- Issue numbers that the PR resolves (if any)

## Environments

We are currently not accepting environment contributions directly into ML-Agents.
However, we believe community created enviornments have a lot of value to the
community. If you have an interesting enviornment and are willing to share,
feel free to showcase it and share any relevant files in the
[ML-Agents forum](https://forum.unity.com/forums/ml-agents.453/).

## Continuous Integration (CI)

We run continuous integration on all PRs; all tests must be passing before the PR is merged.

Several static checks are run on the codebase using the
[pre-commit framework](https://pre-commit.com/) during CI. To execute the same
checks locally, run:
```bash
pip install pre-commit>=2.8.0
pip install identify>==2.1.3
pre-commit run --all-files
```

Some hooks (for example, `black`) will output the corrected version of the code;
others (like `mypy`) may require more effort to fix. You can optionally run
`pre-commit install` to install it as a git hook; after this it will run on all
commits that you make.

### Code style

All python code should be formatted with
[`black`](https://github.com/psf/black).

C# code is formatted using [`dotnet-format`](https://github.com/dotnet/format).
You must have [dotnet](https://dotnet.microsoft.com/download) installed first
(but don't need to install `dotnet-format` - `pre-commit` will do that for you).

### Python type annotations

We use [`mypy`](http://mypy-lang.org/) to perform static type checking on python
code. Currently not all code is annotated but we will increase coverage over
time. If you are adding or refactoring code, please

1. Add type annotations to the new or refactored code.
2. Make sure that code calling or called by the modified code also has type
   annotations.

The
[type hint cheat sheet](https://mypy.readthedocs.io/en/stable/cheat_sheet_py3.html)
provides a good introduction to adding type hints.

## Contributor License Agreements

When you open a pull request, you will be asked to acknolwedge our Contributor
License Agreement. We allow both individual contributions and contributions made
on behalf of companies. We use an open source tool called CLA assistant. If you
have any questions on our CLA, please
[submit an issue](https://github.com/Unity-Technologies/ml-agents/issues) or
email us at ml-agents@unity3d.com.


## Links discovered
- [code of conduct](https://github.com/Unity-Technologies/ml-agents/blob/main/CODE_OF_CONDUCT.md)
- [Issues page](https://github.com/Unity-Technologies/ml-agents/issues)
- [latest release](https://github.com/Unity-Technologies/ml-agents/releases/tag/latest_release)
- [ML-Agents forum](https://forum.unity.com/forums/ml-agents.453/)
- [pre-commit framework](https://pre-commit.com/)
- [`black`](https://github.com/psf/black)
- [`dotnet-format`](https://github.com/dotnet/format)
- [dotnet](https://dotnet.microsoft.com/download)
- [`mypy`](http://mypy-lang.org/)
- [type hint cheat sheet](https://mypy.readthedocs.io/en/stable/cheat_sheet_py3.html)
- [submit an issue](https://github.com/Unity-Technologies/ml-agents/issues)

--- com.unity.ml-agents/LICENSE.md ---
ML Agents copyright © 2017 Unity Technologies

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


--- utils/validate_release_links.py ---
#!/usr/bin/env python3

import ast
import sys
import os
import re
import subprocess
import tempfile
from typing import List, Optional, Pattern

RELEASE_PATTERN = re.compile(r"release_[0-9]+(_docs)*")
# This matches the various ways to invoke pip: "pip", "pip3", "python -m pip"
# It matches "mlagents" and "mlagents_envs", accessible as group "package"
# and optionally matches the version, e.g. "==1.2.3"
PIP_INSTALL_PATTERN = re.compile(
    r"(python -m )?pip3* install (?P<quiet>-q )?(?P<package>mlagents(_envs)?)(==[0-9]+\.[0-9]+\.[0-9]+(\.dev[0-9]+)?)?"
)
TRAINER_INIT_FILE = "ml-agents/mlagents/trainers/__init__.py"

MATCH_ANY = re.compile(r"(?s).*")
# Filename -> regex list to allow specific lines.
# To allow everything in the file (effectively skipping it), use MATCH_ANY for the value
ALLOW_LIST = {
    # Previous release table
    "docs/Python-PettingZoo-API.md": re.compile(
        r"\*\*(Verified Package ([0-9]\.?)*|Release [0-9]+)\*\*"
    ),
    "docs/Versioning.md": MATCH_ANY,
    "com.unity.ml-agents/CHANGELOG.md": MATCH_ANY,
    "utils/make_readme_table.py": MATCH_ANY,
    "utils/validate_release_links.py": MATCH_ANY,
}


def test_release_pattern():
    # Just some sanity check that the regex works as expected.
    for s, expected in [
        (
            "https://github.com/Unity-Technologies/ml-agents/blob/release_4_docs/Food.md",
            True,
        ),
        ("https://github.com/Unity-Technologies/ml-agents/blob/release_4/Foo.md", True),
        (
            "git clone --branch release_4 https://github.com/Unity-Technologies/ml-agents.git",
            True,
        ),
        (
            "https://github.com/Unity-Technologies/ml-agents/blob/release_123_docs/Foo.md",
            True,
        ),
        (
            "https://github.com/Unity-Technologies/ml-agents/blob/release_123/Foo.md",
            True,
        ),
        (
            "https://github.com/Unity-Technologies/ml-agents/blob/latest_release/docs/Foo.md",
            False,
        ),
    ]:
        assert bool(RELEASE_PATTERN.search(s)) is expected

    print("release tests OK!")


def test_pip_pattern():
    # Just some sanity check that the regex works as expected.
    for s, expected in [
        ("pip install mlagents", True),
        ("pip3 install -q mlagents", True),
        ("python -m pip install mlagents", True),
        ("python -m pip install mlagents==1.2.3", True),
        ("python -m pip install mlagents_envs==1.2.3", True),
        ("python -m pip install mlagents==11.222.3333", True),
        ("python -m pip install mlagents_envs==11.222.3333", True),
    ]:
        assert bool(PIP_INSTALL_PATTERN.search(s)) is expected

    sub_expected = "Try running rm -rf / to install"
    assert sub_expected == PIP_INSTALL_PATTERN.sub(
        "rm -rf /", "Try running python -m pip install mlagents==1.2.3 to install"
    )

    print("pip tests OK!")


def update_pip_install_line(line, package_verion):
    match = PIP_INSTALL_PATTERN.search(line)
    if match is not None:  # if there is a pip install line
        package_name = match.group("package")
        quiet_option = match.group("quiet") or ""
        replacement_version = (
            f"python -m pip install {quiet_option}{package_name}=={package_verion}"
        )
        updated = PIP_INSTALL_PATTERN.sub(replacement_version, line)
        return updated
    else:  # Don't do anything
        return line


def git_ls_files() -> List[str]:
    """
    Run "git ls-files" and return a list with one entry per line.
    This returns the list of all files tracked by git.
    """
    return subprocess.check_output(["git", "ls-files"], universal_newlines=True).split(
        "\n"
    )


def get_release_tag() -> Optional[str]:
    """
    Returns the release tag for the mlagents python package.
    This will be None on the main branch.
    :return:
    """
    with open(TRAINER_INIT_FILE) as f:
        for line in f:
            if "__release_tag__" in line:
                lhs, equals_string, rhs = line.strip().partition(" = ")
                # Evaluate the right hand side of the expression
                return ast.literal_eval(rhs)
    # If we couldn't find the release tag, raise an exception
    # (since we can't return None here)
    raise RuntimeError("Can't determine release tag")


def get_python_package_version() -> str:
    """
    Returns the mlagents python package.
    :return:
    """
    with open(TRAINER_INIT_FILE) as f:
        for line in f:
            if "__version__" in line:
                lhs, equals_string, rhs = line.strip().partition(" = ")
                # Evaluate the right hand side of the expression
                return ast.literal_eval(rhs)
    # If we couldn't find the release tag, raise an exception
    # (since we can't return None here)
    raise RuntimeError("Can't determine python package version")


def check_file(
    filename: str,
    release_tag_pattern: Pattern,
    release_tag: str,
    pip_allow_pattern: Pattern,
    package_version: str,
) -> List[str]:
    """
    Validate a single file and return any offending lines.
    """
    bad_lines = []
    with tempfile.TemporaryDirectory() as tempdir:
        if not os.path.exists(tempdir):
            os.makedirs(tempdir)
        new_file_name = os.path.join(tempdir, os.path.basename(filename))
        with open(new_file_name, "w+") as new_file:
            # default to match everything if there is nothing in the ALLOW_LIST
            allow_list_pattern = ALLOW_LIST.get(filename, None)
            with open(filename) as f:
                for line in f:
                    # Does it contain anything of the form release_123
                    has_release_pattern = RELEASE_PATTERN.search(line) is not None
                    # Does it contain this particular release, e.g. release_42 or release_42_docs
                    has_release_tag_pattern = (
                        release_tag_pattern.search(line) is not None
                    )
                    # Does it contain the allow list pattern for the file (if there is one)
                    has_allow_list_pattern = (
                        allow_list_pattern
                        and allow_list_pattern.search(line) is not None
                    )

                    pip_install_ok = (
                        has_allow_list_pattern
                        or PIP_INSTALL_PATTERN.search(line) is None
                        or pip_allow_pattern.search(line) is not None
                    )

                    release_tag_ok = (
                        not has_release_pattern
                        or has_release_tag_pattern
                        or has_allow_list_pattern
                    )

                    if release_tag_ok and pip_install_ok:
                        new_file.write(line)
                    else:
                        bad_lines.append(f"{filename}: {line}")
                        new_line = re.sub(r"release_[0-9]+", rf"{release_tag}", line)
                        new_line = update_pip_install_line(new_line, package_version)
                        new_file.write(new_line)
        if bad_lines:
            if os.path.exists(filename):
                os.remove(filename)
                os.rename(new_file_name, filename)

    return bad_lines


def check_all_files(
    release_allow_pattern: Pattern,
    release_tag: str,
    pip_allow_pattern: Pattern,
    package_version: str,
) -> List[str]:
    """
    Validate all files tracked by git.
    :param release_allow_pattern:
    """
    bad_lines = []
    file_types = {".py", ".md", ".cs", ".ipynb"}
    for file_name in git_ls_files():
        if "localized" in file_name or os.path.splitext(file_name)[1] not in file_types:
            continue
        bad_lines += check_file(
            file_name,
            release_allow_pattern,
            release_tag,
            pip_allow_pattern,
            package_version,
        )
    return bad_lines


def main():
    release_tag = get_release_tag()
    if not release_tag:
        print("Release tag is None, exiting")
        sys.exit(0)

    package_version = get_python_package_version()
    print(f"Release tag: {release_tag}")
    print(f"Python package version: {package_version}")
    release_allow_pattern = re.compile(f"{release_tag}(_docs)?")
    pip_allow_pattern = re.compile(
        rf"python -m pip install (-q )?mlagents(_envs)?=={package_version}"
    )
    bad_lines = check_all_files(
        release_allow_pattern, release_tag, pip_allow_pattern, package_version
    )
    if bad_lines:
        for line in bad_lines:
            print(line)

        print("*************************************************************")
        print(
            "This script attempted to fix the above errors. Please double "
            + "check them to make sure the replacements were done correctly"
        )

    sys.exit(1 if bad_lines else 0)


if __name__ == "__main__":
    if "--test" in sys.argv:
        test_release_pattern()
        test_pip_pattern()
    main()


--- com.unity.ml-agents/Documentation~/CONTRIBUTING.md ---
# How to Contribute to ML-Agents

## 1.Fork the repository
Fork the ML-Agents repository by clicking on the "Fork" button in the top right corner of the GitHub page. This creates a copy of the repository under your GitHub account.

## 2. Set up your development environment
Clone the forked repository to your local machine using Git. Install the necessary dependencies and follow the instructions provided in the project's documentation to set up your development environment properly.

## 3. Choose an issue or feature
Browse the project's issue tracker or discussions to find an open issue or feature that you would like to contribute to. Read the guidelines and comments associated with the issue to understand the requirements and constraints.

## 4. Make your changes
Create a new branch for your changes based on the main branch of the ML-Agents repository. Implement your code changes or add new features as necessary. Ensure that your code follows the project's coding style and conventions.

* Example: Let's say you want to add support for a new type of reward function in the ML-Agents framework. You can create a new branch named feature/reward-function to implement this feature.

## 5. Test your changes
Run the appropriate tests to ensure your changes work as intended. If necessary, add new tests to cover your code and verify that it doesn't introduce regressions.

* Example: For the reward function feature, you would write tests to check different scenarios and expected outcomes of the new reward function.

## 6. Submit a pull request
Push your branch to your forked repository and submit a pull request (PR) to the ML-Agents main repository. Provide a clear and concise description of your changes, explaining the problem you solved or the feature you added.

* Example: In the pull request description, you would explain how the new reward function works, its benefits, and any relevant implementation details.

## 7. Respond to feedback
Be responsive to any feedback or comments provided by the project maintainers. Address the feedback by making necessary revisions to your code and continue the discussion if required.

## 8. Continuous integration and code review
The ML-Agents project utilizes automated continuous integration (CI) systems to run tests on pull requests. Address any issues flagged by the CI system and actively participate in the code review process by addressing comments from reviewers.

## 9. Merge your changes
Once your pull request has been approved and meets all the project's requirements, a project maintainer will merge your changes into the main repository. Congratulations, your contribution has been successfully integrated!

**Remember to always adhere to the project's code of conduct, be respectful, and follow any specific contribution guidelines provided by the ML-Agents project. Happy contributing!**


--- .yamato/sparse-checkouts/upm-packages.txt ---
.github
.yamato
com.unity.ml-agents
Tools

--- CODE_OF_CONDUCT.md ---
# Contributor Covenant Code of Conduct

## Our Pledge

In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to making participation in our project and
our community a harassment-free experience for everyone, regardless of age, body
size, disability, ethnicity, gender identity and expression, level of experience,
nationality, personal appearance, race, religion, or sexual identity and
orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment
include:

* Using welcoming and inclusive language
* Being respectful of differing viewpoints and experiences
* Gracefully accepting constructive criticism
* Focusing on what is best for the community
* Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery and unwelcome sexual attention or
  advances
* Trolling, insulting/derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or electronic
  address, without explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable
behavior and are expected to take appropriate and fair corrective action in
response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or
reject comments, commits, code, wiki edits, issues, and other contributions
that are not aligned to this Code of Conduct, or to ban temporarily or
permanently any contributor for other behaviors that they deem inappropriate,
threatening, offensive, or harmful.

## Scope

This Code of Conduct applies both within project spaces and in public spaces
when an individual is representing the project or its community. Examples of
representing a project or community include using an official project e-mail
address, posting via an official social media account, or acting as an appointed
representative at an online or offline event. Representation of a project may be
further defined and clarified by project maintainers.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by contacting the project team at ml-agents@unity3d.com. All
complaints will be reviewed and investigated and will result in a response that
is deemed necessary and appropriate to the circumstances. The project team is
obligated to maintain confidentiality with regard to the reporter of an incident.
Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good
faith may face temporary or permanent repercussions as determined by other
members of the project's leadership.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 1.4, available at
https://www.contributor-covenant.org/version/1/4/code-of-conduct/

[homepage]: https://www.contributor-covenant.org


--- PerformanceProject/ProjectSettings/ProjectVersion.txt ---
m_EditorVersion: 6000.0.40f1
m_EditorVersionWithRevision: 6000.0.40f1 (157d81624ddf)


--- Project/ProjectSettings/ProjectVersion.txt ---
m_EditorVersion: 6000.0.40f1
m_EditorVersionWithRevision: 6000.0.40f1 (157d81624ddf)


--- Readme.md ---
# Unity ML-Agents Toolkit

[![docs badge](https://img.shields.io/badge/docs-reference-blue.svg)](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest)

[![license badge](https://img.shields.io/badge/license-Apache--2.0-green.svg)](https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/LICENSE.md)

([latest release](https://github.com/Unity-Technologies/ml-agents/releases/tag/latest_release)) ([all releases](https://github.com/Unity-Technologies/ml-agents/releases))

**The Unity Machine Learning Agents Toolkit** (ML-Agents) is an open-source project that enables games and simulations to serve as environments for training intelligent agents. We provide implementations (based on PyTorch) of state-of-the-art algorithms to enable game developers and hobbyists to easily train intelligent agents for 2D, 3D and VR/AR games. Researchers can also use the provided simple-to-use Python API to train Agents using reinforcement learning, imitation learning, neuroevolution, or any other methods. These trained agents can be used for multiple purposes, including controlling NPC behavior (in a variety of settings such as multi-agent and adversarial), automated testing of game builds and evaluating different game design decisions pre-release. The ML-Agents Toolkit is mutually beneficial for both game developers and AI researchers as it provides a central platform where advances in AI can be evaluated on Unity’s rich environments and then made accessible to the wider research and game developer communities.

## Features
- 17+ [example Unity environments](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Learning-Environment-Examples.html)
- Support for multiple environment configurations and training scenarios
- Flexible Unity SDK that can be integrated into your game or custom Unity scene
- Support for training single-agent, multi-agent cooperative, and multi-agent competitive scenarios via several Deep Reinforcement Learning algorithms (PPO, SAC, MA-POCA, self-play).
- Support for learning from demonstrations through two Imitation Learning algorithms (BC and GAIL).
- Quickly and easily add your own [custom training algorithm](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Python-Custom-Trainer-Plugin.html) and/or components.
- Easily definable Curriculum Learning scenarios for complex tasks
- Train robust agents using environment randomization
- Flexible agent control with On Demand Decision Making
- Train using multiple concurrent Unity environment instances
- Utilizes the [Inference Engine](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Inference-Engine.html) to provide native cross-platform support
- Unity environment [control from Python](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Python-LLAPI.html)
- Wrap Unity learning environments as a [gym](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Python-Gym-API.html) environment
- Wrap Unity learning environments as a [PettingZoo](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Python-PettingZoo-API.html) environment

## Releases & Documentation

> **⚠️ Documentation Migration Notice**
> We have moved to [Unity Package documentation](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest) as the **primary developer documentation** and have **deprecated** the maintenance of [web docs](https://unity-technologies.github.io/ml-agents/). Please use the Unity Package documentation for the most up-to-date information.

The table below shows our latest release, including our `develop` branch which is under active development and may be unstable. A few helpful guidelines:

- The [Versioning page](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Versioning.html) overviews how we manage our GitHub releases and the versioning process for each of the ML-Agents components.
- The [Releases page](https://github.com/Unity-Technologies/ml-agents/releases) contains details of the changes between releases.
- The [Migration page](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Migrating.html) contains details on how to upgrade from earlier releases of the ML-Agents Toolkit.
- The `com.unity.ml-agents` package is [verified](https://docs.unity3d.com/2020.1/Documentation/Manual/pack-safe.html) for Unity 2020.1 and later. Verified packages releases are numbered 1.0.x.

|      **Version**       |  **Release Date**   |                                  **Source**                                   |                                                 **Documentation**                                                  |                                      **Download**                                      |                  **Python Package**                   |                                   **Unity Package**                                   |
|:----------------------:|:-------------------:|:-----------------------------------------------------------------------------:|:------------------------------------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------------------:|:-----------------------------------------------------:|:-------------------------------------------------------------------------------------:|
|     **Release 23**     | **August 28, 2025** | **[source](https://github.com/Unity-Technologies/ml-agents/tree/release_23)** |              **[docs](https://docs.unity3d.com/Packages/com.unity.ml-agents@4.0/manual/index.html)**               | **[download](https://github.com/Unity-Technologies/ml-agents/archive/release_23.zip)** | **[1.1.0](https://pypi.org/project/mlagents/1.1.0/)** |                                       **4.0.0**                                       |
| **develop (unstable)** |         --          |    [source](https://github.com/Unity-Technologies/ml-agents/tree/develop)     | [docs](https://github.com/Unity-Technologies/ml-agents/tree/develop/com.unity.ml-agents/Documentation~/index.md)   |    [download](https://github.com/Unity-Technologies/ml-agents/archive/develop.zip)     |                         --                            |                                          --                                           |



If you are a researcher interested in a discussion of Unity as an AI platform, see a pre-print of our [reference paper on Unity and the ML-Agents Toolkit](https://arxiv.org/abs/1809.02627).

If you use Unity or the ML-Agents Toolkit to conduct research, we ask that you cite the following paper as a reference:

```
@article{juliani2020,
  title={Unity: A general platform for intelligent agents},
  author={Juliani, Arthur and Berges, Vincent-Pierre and Teng, Ervin and Cohen, Andrew and Harper, Jonathan and Elion, Chris and Goy, Chris and Gao, Yuan and Henry, Hunter and Mattar, Marwan and Lange, Danny},
  journal={arXiv preprint arXiv:1809.02627},
  url={https://arxiv.org/pdf/1809.02627.pdf},
  year={2020}
}
```

Additionally, if you use the MA-POCA trainer in your research, we ask that you cite the following paper as a reference:

```
@article{cohen2022,
  title={On the Use and Misuse of Absorbing States in Multi-agent Reinforcement Learning},
  author={Cohen, Andrew and Teng, Ervin and Berges, Vincent-Pierre and Dong, Ruo-Ping and Henry, Hunter and Mattar, Marwan and Zook, Alexander and Ganguly, Sujoy},
  journal={RL in Games Workshop AAAI 2022},
  url={http://aaai-rlg.mlanctot.info/papers/AAAI22-RLG_paper_32.pdf},
  year={2022}
}
```


## Additional Resources

* [Unity Discussions](https://discussions.unity.com/tag/ml-agents)
* [ML-Agents tutorials by CodeMonkeyUnity](https://www.youtube.com/playlist?list=PLzDRvYVwl53vehwiN_odYJkPBzcqFw110)
* [Introduction to ML-Agents by Huggingface](https://huggingface.co/learn/deep-rl-course/en/unit5/introduction)
* [Community created ML-Agents projects](https://discussions.unity.com/t/post-your-ml-agents-project/816756)
* [ML-Agents models on Huggingface](https://huggingface.co/models?library=ml-agents)
* [Blog posts](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Blog-posts.html)
* [Discord](https://discord.com/channels/489222168727519232/1202574086115557446)

## Community and Feedback

The ML-Agents Toolkit is an open-source project and we encourage and welcome contributions. If you wish to contribute, be sure to review our [contribution guidelines](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/CONTRIBUTING.html) and [code of conduct](https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/CODE_OF_CONDUCT.md).

For problems with the installation and setup of the ML-Agents Toolkit, or discussions about how to best setup or train your agents, please create a new thread on the [Unity ML-Agents discussion forum](https://discussions.unity.com/tag/ml-agents). Be sure to include as many details as possible to help others assist you effectively. If you run into any other problems using the ML-Agents Toolkit or have a specific feature request, please [submit a GitHub issue](https://github.com/Unity-Technologies/ml-agents/issues).

Please tell us which samples you would like to see shipped with the ML-Agents Unity package by replying to [this discussion thread](https://discussions.unity.com/t/help-shape-the-future-of-ml-agents/1661019).

## Privacy

In order to improve the developer experience for Unity ML-Agents Toolkit, we have added in-editor analytics. Please refer to "Information that is passively collected by Unity" in the [Unity Privacy Policy](https://unity3d.com/legal/privacy-policy).


## Links discovered
- [![docs badge](https://img.shields.io/badge/docs-reference-blue.svg)
- [![license badge](https://img.shields.io/badge/license-Apache--2.0-green.svg)
- [latest release](https://github.com/Unity-Technologies/ml-agents/releases/tag/latest_release)
- [all releases](https://github.com/Unity-Technologies/ml-agents/releases)
- [example Unity environments](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Learning-Environment-Examples.html)
- [custom training algorithm](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Python-Custom-Trainer-Plugin.html)
- [Inference Engine](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Inference-Engine.html)
- [control from Python](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Python-LLAPI.html)
- [gym](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Python-Gym-API.html)
- [PettingZoo](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Python-PettingZoo-API.html)
- [Unity Package documentation](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest)
- [web docs](https://unity-technologies.github.io/ml-agents/)
- [Versioning page](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Versioning.html)
- [Releases page](https://github.com/Unity-Technologies/ml-agents/releases)
- [Migration page](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Migrating.html)
- [verified](https://docs.unity3d.com/2020.1/Documentation/Manual/pack-safe.html)
- [source](https://github.com/Unity-Technologies/ml-agents/tree/release_23)
- [docs](https://docs.unity3d.com/Packages/com.unity.ml-agents@4.0/manual/index.html)
- [download](https://github.com/Unity-Technologies/ml-agents/archive/release_23.zip)
- [1.1.0](https://pypi.org/project/mlagents/1.1.0/)
- [source](https://github.com/Unity-Technologies/ml-agents/tree/develop)
- [docs](https://github.com/Unity-Technologies/ml-agents/tree/develop/com.unity.ml-agents/Documentation~/index.md)
- [download](https://github.com/Unity-Technologies/ml-agents/archive/develop.zip)
- [reference paper on Unity and the ML-Agents Toolkit](https://arxiv.org/abs/1809.02627)
- [Unity Discussions](https://discussions.unity.com/tag/ml-agents)
- [ML-Agents tutorials by CodeMonkeyUnity](https://www.youtube.com/playlist?list=PLzDRvYVwl53vehwiN_odYJkPBzcqFw110)
- [Introduction to ML-Agents by Huggingface](https://huggingface.co/learn/deep-rl-course/en/unit5/introduction)
- [Community created ML-Agents projects](https://discussions.unity.com/t/post-your-ml-agents-project/816756)
- [ML-Agents models on Huggingface](https://huggingface.co/models?library=ml-agents)
- [Blog posts](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Blog-posts.html)
- [Discord](https://discord.com/channels/489222168727519232/1202574086115557446)
- [contribution guidelines](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/CONTRIBUTING.html)
- [code of conduct](https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/CODE_OF_CONDUCT.md)
- [Unity ML-Agents discussion forum](https://discussions.unity.com/tag/ml-agents)
- [submit a GitHub issue](https://github.com/Unity-Technologies/ml-agents/issues)
- [this discussion thread](https://discussions.unity.com/t/help-shape-the-future-of-ml-agents/1661019)
- [Unity Privacy Policy](https://unity3d.com/legal/privacy-policy)

--- SURVEY.md ---
# Unity ML-Agents Toolkit Survey

Your opinion matters a great deal to us. Only by hearing your thoughts on the
Unity ML-Agents Toolkit can we continue to improve and grow. Please take a few
minutes to let us know about it. Please email us at [ml-agents@unity3d.com](mailto:ml-agents@unity3d.com).

<!-- [Fill out the survey](https://goo.gl/forms/qFMYSYr5TlINvG6f1) -->


## Links discovered
- [Fill out the survey](https://goo.gl/forms/qFMYSYr5TlINvG6f1)

--- Tools/ci/onboard.py ---
# Description: This script is used to onboard a project onto Wrench
import argparse
import json
import os
import shutil
import subprocess

TEMPLATE_CSPROJ = "TEMPLATE.Cookbook.csproj"
TEMPLATE_SLN = "TEMPLATE-recipes.sln"


def get_args():
    parser = argparse.ArgumentParser(description="Onboard a project onto Wrench")
    parser.add_argument(
        "--settings-name",
        required=True,
        help="The name of the settings file",
        dest="settings_name",
    )
    return parser.parse_args()


def delete_git_folder():
    print("Deleting .git folder")
    if os.path.isfile(".gitignore"):
        os.remove(".gitignore")
    if os.path.exists(".git"):
        shutil.rmtree(".git", ignore_errors=True)


def get_git_root_dir():
    git_dir = subprocess.run(
        ["git", "rev-parse", "--show-toplevel"], stdout=subprocess.PIPE
    ).stdout
    return git_dir.decode("utf-8").strip()


def find_package_json_files(root_dir, initial=True):
    # walk the directory recursively and find all package.json files
    rtn_list = set()
    for root, dirs, files in os.walk(root_dir):
        for file in files:
            if file == "package.json":
                if initial:
                    file_location = file
                else:
                    file_location = os.path.join(root, file)
                # validate the package.json file
                with open(os.path.join(root, file_location)) as f:
                    data = json.loads(f.read())
                    if "unity" in data and "name" in data:
                        rtn_list.add(data["name"])
        for directory in dirs:
            rtn_list.update(
                find_package_json_files(os.path.join(root, directory), False)
            )

    return rtn_list


def create_package_option(name, first):
    initial_char = ""
    initial_tab = ""
    if not first:
        initial_char = ",\n"
        initial_tab = "        "
    return (
        f'{initial_char}{initial_tab}{{\n            "{name}",\n'
        f"            new PackageOptions() {{ ReleaseOptions = "
        f"new ReleaseOptions() {{ IsReleasing = true }} }}\n        }}"
    )


def update_template_variables(packages):
    template_var = "TEMPLATESettings"
    settings = get_args()
    settings_content = open(os.path.join("Settings", template_var + ".cs")).read()
    # replace the template settings with the actual settings
    settings_content = settings_content.replace(
        "TEMPLATESettings", f"{settings.settings_name}Settings"
    )
    settings_content = settings_content.replace("PACKAGES_ROOTS", ".")

    package_replace_string = ""
    first = True
    for package in packages:
        package_replace_string += create_package_option(package, first)
        first = False
    settings_content = settings_content.replace(
        '//"PACKAGES_TO_RELEASE"', package_replace_string
    )
    settings_content = settings_content.replace(
        "TEMPLATE.Cookbook.Settings", settings.settings_name + ".Cookbook.Settings"
    )

    # Write the updated settings file
    with open(
        os.path.join("Settings", settings.settings_name + "Settings.cs"), "w"
    ) as f:
        f.write(settings_content)
    # Update the Program.cs file
    program_content = open("Program.cs").read()
    with open("Program.cs", "w") as f:
        program_content = program_content.replace(
            "using TEMPLATE.Cookbook.Settings;",
            f"using {settings.settings_name}.Cookbook.Settings;",
        )
        program_content = program_content.replace(
            "TEMPLATE.Cookbook", settings.settings_name
        )
        program_content = program_content.replace("PACKAGES_ROOT", ".")
        program_content = program_content.replace(
            "TEMPLATESettings", settings.settings_name + "Settings"
        )
        f.write(program_content)
    # delete the template file
    os.remove(os.path.join("Settings", template_var + ".cs"))
    # update csproj file
    csproj_content = open(TEMPLATE_CSPROJ).read()
    with open(TEMPLATE_CSPROJ.replace("TEMPLATE", settings.settings_name), "w") as f:
        f.write(csproj_content)
    os.remove(TEMPLATE_CSPROJ)
    # update sln file
    sln_content = open(TEMPLATE_SLN).read()
    with open(TEMPLATE_SLN.replace("TEMPLATE", settings.settings_name), "w") as f:
        sln_content = sln_content.replace("TEMPLATE", settings.settings_name)
        f.write(sln_content)
    os.remove(TEMPLATE_SLN)


def update_shell_scripts(root_dir):
    split_root = os.path.normpath(root_dir).split(os.sep)
    split_cwd = os.getcwd().split(os.sep)

    relative_path = os.path.relpath(os.getcwd(), root_dir)
    csproj_path = os.path.join(
        relative_path, f"{get_args().settings_name}.Cookbook.csproj"
    )

    path_diff = len(split_cwd) - len(split_root)
    cd_string = ""
    for _ in range(path_diff):
        cd_string += "../"

    bash_content = open("regenerate.bat").read()
    bash_content = bash_content.replace("STEPS_TO_ROOT", cd_string)
    bash_content = bash_content.replace("PATH_TO_CSPROJ", csproj_path)
    with open("regenerate.bat", "w") as f:
        f.write(bash_content)

    shell_content = open("regenerate.sh").read()
    shell_content = shell_content.replace("STEPS_TO_ROOT", cd_string)
    shell_content = shell_content.replace("PATH_TO_CSPROJ", csproj_path)
    with open("regenerate.sh", "w") as f:
        f.write(shell_content)


def main():
    print("Starting process of onboarding")
    # delete git folder
    delete_git_folder()
    # get root folder
    root_dir = get_git_root_dir()
    # find all appropriate package.json files
    package_files = find_package_json_files(root_dir)
    packages = set()
    # process out any testing packages
    for package in package_files:
        if ".tests" in package:
            continue
        packages.add(package)
    # Replace Template naming with actual naming
    update_template_variables(packages)
    update_shell_scripts(root_dir)


if "__main__" in __name__:
    main()


--- colab_requirements.txt ---
matplotlib
jupyter


--- conftest.py ---
"""
This module provides the 'base_port' pytest fixture for mlagents tests.

This is useful because each mlagents environment requires a unique port to communicate over and will fail on collisions.
Normally this would prevent tests from being run in parallel but with the help of this fixture we can guarantee every
test gets the ports it needs.

See the base_port function for usage details.
"""
import tempfile
from pathlib import Path

import pytest
from filelock import FileLock

# TODO: Use this in all ml-agents tests so they can all run in parallel.
import mlagents.plugins.trainer_type

_BASE_PORT = 6005


# Hook for xdist
# https://github.com/ohmu/pytest-xdist/blob/master/xdist/newhooks.py
def pytest_testnodeready():
    PortAllocator().setup_once_per_node()


class PortAllocator:
    """
    WARNING: Should only be used within this file.
    Handles handing out unique ports to tests that need ports to test.
    Shares state between parallel tests on the same node via a text file and lockfile.
    Should only be used through the base_port test fixture.
    """

    def __init__(self):
        self._port_alloc_file_path: Path = (
            Path(tempfile.gettempdir()) / "next_mla_test_port.txt"
        )
        self._port_alloc_lock_path: Path = self._port_alloc_file_path.with_suffix(
            ".lock"
        )
        self.lock = FileLock(str(self._port_alloc_lock_path))

    def reserve_n_ports(self, n: int) -> int:
        with self.lock:
            if self._port_alloc_file_path.is_file():
                base_port = int(self._port_alloc_file_path.read_text())
            else:
                base_port = 6005
            self._port_alloc_file_path.write_text(str(base_port + n))
        return base_port

    def setup_once_per_node(self) -> None:
        """
        Clean up state files from previous runs, should only be called once per node.
        Intended to only be called via xdist hooks.
        """
        if self._port_alloc_lock_path.exists():
            self._port_alloc_lock_path.unlink(missing_ok=True)
        if self._port_alloc_file_path.exists():
            self._port_alloc_file_path.unlink(missing_ok=True)


@pytest.fixture
def base_port(n_ports: int) -> int:
    """
    Reserve a range of ports for testing (allows parallel testing even with envs).
    Usage:
        @pytest.mark.parametrize("n_ports", [2])
        def test_something(base_port: int) -> None:
            do_something(base_port)
            do_something(base_port + 1)
    :param _port_allocator: The global port allocator (custom pytest fixture).
    :param n_ports: The number of ports needed.
    :return: The base port number.
    """
    return PortAllocator().reserve_n_ports(n_ports)


@pytest.fixture(scope="session", autouse=True)
def setup_plugin_trainers():
    _, _ = mlagents.plugins.trainer_type.register_trainer_plugins()


--- protobuf-definitions/README.md ---
# Unity ML-Agents Protobuf Definitions

Contains relevant definitions needed to generate probobuf files used in [ML-Agents Toolkit](https://github.com/Unity-Technologies/ml-agents).

## Requirements

* protobuf 3.19.6
* grpcio-tools 1.48.2
* Grpc.Tools 1.14.1

## Set-up & Installation

First we will follow these steps once install protobuf and grpcio-tools via your terminal.
Assume the ml-agents repository is checked out to a folder named $MLAGENTS_ROOT.
**Note:** If you're using Anaconda, don't forget to activate the ml-agents environment first.

`pip install protobuf==3.19.6 --force`

`pip install grpcio-tools==1.28.1`

`pip install mypy-protobuf==1.16.0`


#### On Windows

Download and install the latest version of [nuget](https://www.nuget.org/downloads).

#### On Mac

`brew install nuget`

#### On Linux

`sudo apt-get install nuget`


Navigate to your installation of nuget and run the following:

`nuget install Grpc.Tools -Version 1.14.1 -OutputDirectory $MLAGENTS_ROOT\protobuf-definitions`

## Running

Whenever you change the fields of a message, you must follow the steps below to create C# and Python files corresponding to the new message.

1. Open a terminal. **Note:** If you're using Anaconda, don't forget to activate the ml-agents environment first.
2. Un-comment line 7 in `make.sh` (for Windows, use `make_for_win.bat`), and set to correct Grpc.Tools sub-directory.
3. Run the protobuf generation script from the terminal by navigating to `$MLAGENTS_ROOT\protobuf-definitions` and entering `make.sh` (for Windows, use `make_for_win.bat`)
4. Note any errors generated that may result from setting the wrong directory in step 2.
5. In the generated `UnityToExternalGrpc.cs` file in the `$MLAGENTS_ROOT/com.unity.ml-agents/Runtime/Grpc/CommunicatorObjects` folder, check to see if you need to add the following to the beginning of the file:

```csharp
# if UNITY_EDITOR || UNITY_STANDALONE
```
 and the following line to the end

 ```csharp
 #endif
 ```
This is to make sure the generated code does not try to access the Grpc library
on platforms that are not supported by Grpc.

Finally, re-install the mlagents packages by running the following commands from the same `$MLAGENTS_ROOT\protobuf-definitions` directory.

```
cd ..
cd ml-agents-envs
pip install -e .
cd ..
cd ml-agents
pip install -e .
mlagents-learn
```

The final line will test if everything was generated and installed correctly. If it worked, you should see the Unity logo.


## Links discovered
- [ML-Agents Toolkit](https://github.com/Unity-Technologies/ml-agents)
- [nuget](https://www.nuget.org/downloads)

--- test_constraints_version.txt ---
# pip constraints to use the *highest* versions allowed in ml-agents/setup.py
# For projects with upper bounds, we should periodically update this list to the latest
torch==2.1.1


--- test_requirements.txt ---
# Test-only dependencies should go here, not in setup.py
pytest>=6.2.5
pytest-cov>=2.12.1
pytest-xdist>=2.5.0
filelock>=3.4.2


--- .github/PULL_REQUEST_TEMPLATE.md ---
### Proposed change(s)

Describe the changes made in this PR.

### Useful links (Github issues, JIRA tickets, ML-Agents forum threads etc.)



### Types of change(s)

- [ ] Bug fix
- [ ] New feature
- [ ] Code refactor
- [ ] Breaking change
- [ ] Documentation update
- [ ] Other (please describe)

### Checklist
- [ ] Added tests that prove my fix is effective or that my feature works
- [ ] Updated the changelog (if applicable)
- [ ] Updated the documentation (if applicable)
- [ ] Updated the migration guide (if applicable)

### Other comments


--- .github/ISSUE_TEMPLATE/bug_report.md ---
---
name: Bug report
about: Report a bug with ML-Agents
title: ''
labels: bug
assignees: ''

---

**Describe the bug**
A clear and concise description of what the bug is.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Console logs / stack traces**
Please wrap in triple backticks (```) to make it easier to read.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Environment (please complete the following information):**
- Unity Version: [e.g. Unity 6000.0.40f1]
- OS + version: [e.g. Windows 10]
- _ML-Agents version_: (e.g. ML-Agents v0.8, or latest `develop` branch from source)
- _Torch version_: (you can run `pip3 show torch` to get this)
- _Environment_: (which example environment you used to reproduce the error)

**NOTE:** We are unable to help reproduce bugs with custom environments.  Please attempt to reproduce your issue with one of the example environments, or provide a minimal patch to one of the environments needed to reproduce the issue.


--- .github/ISSUE_TEMPLATE/feature_request.md ---
---
name: Feature request
about: Suggest an idea for this project
title: ''
labels: request
assignees: ''

---

**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.


--- colab/Colab_UnityEnvironment_1_Run.ipynb ---
{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Colab-UnityEnvironment-1-Run.ipynb",
   "private_outputs": true,
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbVXrmEsLXDt"
   },
   "source": [
    "# ML-Agents Open a UnityEnvironment\n",
    "<img src=\"https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/com.unity.ml-agents/Documentation~/images/image-banner.png?raw=true\" align=\"middle\" width=\"435\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNKTwHU3d2-l"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "htb-p1hSNX7D"
   },
   "source": [
    "#@title Install Rendering Dependencies { display-mode: \"form\" }\n",
    "#@markdown (You only need to run this code when using Colab's hosted runtime)\n",
    "\n",
    "import os\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def progress(value, max=100):\n",
    "    return HTML(\"\"\"\n",
    "        <progress\n",
    "            value='{value}'\n",
    "            max='{max}',\n",
    "            style='width: 100%'\n",
    "        >\n",
    "            {value}\n",
    "        </progress>\n",
    "    \"\"\".format(value=value, max=max))\n",
    "\n",
    "pro_bar = display(progress(0, 100), display_id=True)\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  INSTALL_XVFB = True\n",
    "except ImportError:\n",
    "  INSTALL_XVFB = 'COLAB_ALWAYS_INSTALL_XVFB' in os.environ\n",
    "\n",
    "if INSTALL_XVFB:\n",
    "  !sudo apt-get update -qq\n",
    "  pro_bar.update(progress(50, 100))\n",
    "  !sudo DEBIAN_FRONTEND=noninteractive apt-get install -y -qq xvfb\n",
    "  pro_bar.update(progress(90, 100))\n",
    "  import subprocess\n",
    "  subprocess.Popen(['Xvfb', ':1', '-screen', '0', '1024x768x24', '-ac', '+extension', 'GLX', '+render', '-noreset'], \n",
    "                   stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "  os.environ[\"DISPLAY\"] = \":1\"\n",
    "pro_bar.update(progress(100, 100))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pzj7wgapAcDs"
   },
   "source": [
    "### Installing ml-agents"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N8yfQqkbebQ5",
    "ExecuteTime": {
     "start_time": "2023-10-04T12:52:21.641839Z",
     "end_time": "2023-10-04T12:52:21.642251Z"
    }
   },
   "source": [
    "try:\n",
    "  import mlagents\n",
    "  print(\"ml-agents already installed\")\n",
    "except ImportError:\n",
    "  !python -m pip install -q mlagents==1.1.0\n",
    "  print(\"Installed ml-agents\")"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml-agents already installed\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_u74YhSmW6gD"
   },
   "source": [
    "## Run the Environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DpZPbRvRuLZv",
    "ExecuteTime": {
     "start_time": "2023-10-04T12:52:23.330185Z",
     "end_time": "2023-10-04T12:52:23.339236Z"
    }
   },
   "source": [
    "#@title Select Environment { display-mode: \"form\" }\n",
    "env_id = \"GridWorld\" #@param ['Basic', '3DBall', '3DBallHard', 'GridWorld', 'Hallway', 'VisualHallway', 'CrawlerDynamicTarget', 'CrawlerStaticTarget', 'Bouncer', 'SoccerTwos', 'PushBlock', 'VisualPushBlock', 'WallJump', 'Tennis', 'Reacher', 'Pyramids', 'VisualPyramids', 'Walker', 'FoodCollector', 'VisualFoodCollector', 'StrikersVsGoalie', 'WormStaticTarget', 'WormDynamicTarget']\n"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-r_cB2rqp5x"
   },
   "source": [
    "### Start Environment from the registry"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YSf-WhxbqtLw",
    "ExecuteTime": {
     "start_time": "2023-10-04T12:52:25.056933Z",
     "end_time": "2023-10-04T12:52:26.115543Z"
    }
   },
   "source": [
    "# -----------------\n",
    "# This code is used to close an env that might not have been closed before\n",
    "try:\n",
    "  env.close()\n",
    "except:\n",
    "  pass\n",
    "# -----------------\n",
    "\n",
    "from mlagents_envs.registry import default_registry\n",
    "\n",
    "env = default_registry[env_id].make()"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1lIx3_l24OP"
   },
   "source": [
    "### Reset the environment\n",
    "To reset the environment, simply call `env.reset()`. This method takes no argument and returns nothing but will send a signal to the simulation to reset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dhtl0mpeqxYi",
    "ExecuteTime": {
     "start_time": "2023-10-04T12:52:40.819560Z",
     "end_time": "2023-10-04T12:52:41.038983Z"
    }
   },
   "source": [
    "env.reset()"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1rwnVq2qyoO"
   },
   "source": [
    "### Behavior Specs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrD0rSv92T8A"
   },
   "source": [
    "#### Get the Behavior Specs from the Environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "a7KatdThq7OV",
    "ExecuteTime": {
     "start_time": "2023-10-04T12:52:47.812858Z",
     "end_time": "2023-10-04T12:52:47.820527Z"
    }
   },
   "source": [
    "# We will only consider the first Behavior\n",
    "behavior_name = list(env.behavior_specs)[0]\n",
    "print(f\"Name of the behavior : {behavior_name}\")\n",
    "spec = env.behavior_specs[behavior_name]"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of the behavior : GridWorld?team=0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1L8DHADrAbe"
   },
   "source": [
    "#### Get the Observation Space from the Behavior Specs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PqDTV5mSrJF5",
    "ExecuteTime": {
     "start_time": "2023-10-04T12:52:50.586284Z",
     "end_time": "2023-10-04T12:52:50.596936Z"
    }
   },
   "source": [
    "# Examine the number of observations per Agent\n",
    "print(\"Number of observations : \", len(spec.observation_specs))\n",
    "\n",
    "# Is there a visual observation ?\n",
    "# Visual observation have 3 dimensions: Height, Width and number of channels\n",
    "vis_obs = any(len(spec.shape) == 3 for spec in spec.observation_specs)\n",
    "print(\"Is there a visual observation ?\", vis_obs)"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations :  2\n",
      "Is there a visual observation ? True\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVLN_wbG1G5-"
   },
   "source": [
    "#### Get the Action Space from the Behavior Specs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "M9zk1-az1L-G",
    "ExecuteTime": {
     "start_time": "2023-10-04T12:52:52.411887Z",
     "end_time": "2023-10-04T12:52:52.456259Z"
    }
   },
   "source": [
    "# Is the Action continuous or multi-discrete ?\n",
    "if spec.action_spec.continuous_size > 0:\n",
    "  print(f\"There are {spec.action_spec.continuous_size} continuous actions\")\n",
    "if spec.action_spec.is_discrete():\n",
    "  print(f\"There are {spec.action_spec.discrete_size} discrete actions\")\n",
    "\n",
    "\n",
    "# How many actions are possible ?\n",
    "#print(f\"There are {spec.action_size} action(s)\")\n",
    "\n",
    "# For discrete actions only : How many different options does each action has ?\n",
    "if spec.action_spec.discrete_size > 0:\n",
    "  for action, branch_size in enumerate(spec.action_spec.discrete_branches):\n",
    "    print(f\"Action number {action} has {branch_size} different options\")\n",
    "\n"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 discrete actions\n",
      "Action number 0 has 5 different options\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cX07SGw22Lm"
   },
   "source": [
    "### Stepping the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xO5p0s0prfsQ"
   },
   "source": [
    "#### Get the steps from the Environment\n",
    "You can do this with the `env.get_steps(behavior_name)` method. If there are multiple behaviors in the Environment, you can call this method with each of the behavior's names.\n",
    "_Note_ This will not move the simulation forward."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ePZtcHXUrjyf",
    "ExecuteTime": {
     "start_time": "2023-10-04T12:52:55.105403Z",
     "end_time": "2023-10-04T12:52:55.111994Z"
    }
   },
   "source": [
    "decision_steps, terminal_steps = env.get_steps(behavior_name)"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-Oj3ix530mx"
   },
   "source": [
    "#### Set actions for each behavior\n",
    "You can set the actions for the Agents of a Behavior by calling `env.set_actions()` you will need to specify the behavior name and pass a tensor of dimension 2. The first dimension of the action must be equal to the number of Agents that requested a decision during the step."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KB-nxfbw337g",
    "ExecuteTime": {
     "start_time": "2023-10-04T12:52:56.360968Z",
     "end_time": "2023-10-04T12:52:56.368561Z"
    }
   },
   "source": [
    "env.set_actions(behavior_name, spec.action_spec.empty_action(len(decision_steps)))"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQCybRs84cmq"
   },
   "source": [
    "#### Move the simulation forward\n",
    "Call `env.step()` to move the simulation forward. The simulation will progress until an Agent requestes a decision or terminates."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nl3K40ZR4bh2",
    "ExecuteTime": {
     "start_time": "2023-10-04T12:52:57.609971Z",
     "end_time": "2023-10-04T12:52:57.664885Z"
    }
   },
   "source": [
    "env.step()"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9gdextn2vJy"
   },
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAMqnnddr8Xo"
   },
   "source": [
    "#### Show the observations for one of the Agents\n",
    "`DecisionSteps.obs` is a tuple containing all of the observations for all of the Agents with the provided Behavior name.\n",
    "Each value in the tuple is an observation tensor containing the observation data for all of the agents."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OJpta61TsBiO",
    "ExecuteTime": {
     "start_time": "2023-10-04T12:53:00.550680Z",
     "end_time": "2023-10-04T12:53:00.862654Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "for index, obs_spec in enumerate(spec.observation_specs):\n",
    "  if len(obs_spec.shape) == 3:\n",
    "    print(\"Here is the first visual observation\")\n",
    "    plt.imshow(np.moveaxis(decision_steps.obs[index][0, :, :, :], 0, -1))\n",
    "    plt.show()\n",
    "\n",
    "for index, obs_spec in enumerate(spec.observation_specs):\n",
    "  if len(obs_spec.shape) == 1:\n",
    "    print(\"First vector observations : \", decision_steps.obs[index][0,:])"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the first visual observation\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhQAAAGfCAYAAAAH5UtjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAk3ElEQVR4nO3df3BU1f3/8dduyG6iwEZQNkQSjD+DP6AYNKxgWzEtQx0rhVq12OKP6mgD8sNWiQpoq8bqVPFHxGox1FGaEUdQbIXaKOFrGxAiqGgbUalEYYPWZgMomww53z/8uOPCXuXmbNzN5vmYuTPm3LMn75OLm1fu3nOvxxhjBAAAYMGb6gIAAEDPR6AAAADWCBQAAMAagQIAAFgjUAAAAGsECgAAYI1AAQAArBEoAACANQIFAACwRqAAAADW+nTXwNXV1brrrrsUDoc1YsQI3X///Tr99NO/9nWdnZ3avn27+vXrJ4/H013lAQCAr2GM0a5du1RQUCCv92vOQZhuUFtba3w+n3n00UfNm2++aa644gqTl5dnWlpavva1zc3NRhIbGxsbGxtbmmzNzc1f+/vbY0zyHw5WVlam0047TQ888ICkz886FBYWavr06ZozZ85XvjYSiSgvL08Xnz9Rvuzs/fYmvVQAABAT/8lAe0eHHl+6XK2trQoEAl/5yqR/5NHe3q7GxkZVVlbG2rxer8rLy9XQ0HBA/2g0qmg0Gvt6165dkiRfdrZ8PgIFAADfnMSXGhzMJQhJvyjz448/1r59+xQMBuPag8GgwuHwAf2rqqoUCARiW2FhYbJLAgAA3SzlqzwqKysViURiW3Nzc6pLAgAALiX9I4/DDz9cWVlZamlpiWtvaWlRfn7+Af39fr/8fn+Ckb64FgQAAHwz9v+9e/C/h5N+hsLn86m0tFR1dXWxts7OTtXV1SkUCiX72wEAgDTQLfehmD17tqZOnapRo0bp9NNP14IFC7Rnzx5deuml3fHtAABAinVLoLjgggv00Ucfad68eQqHw/rWt76llStXHnChJgAAyAzddqfMadOmadq0ad01PAAASCMpX+UBAAB6PgIFAACwRqAAAADWCBQAAMAagQIAAFgjUAAAAGsECgAAYI1AAQAArBEoAACANQIFAACwRqAAAADWCBQAAMAagQIAAFgjUAAAAGsECgAAYI1AAQAArBEoAACANQIFAACwRqAAAADWCBQAAMAagQIAAFgjUAAAAGsECgAAYI1AAQAArBEoAACANQIFAACwRqAAAADWCBQAAMAagQIAAFgjUAAAAGsECgAAYI1AAQAArBEoAACANQIFAACwRqAAAADWCBQAAMAagQIAAFgjUAAAAGsECgAAYI1AAQAArBEoAACANQIFAACwRqAAAADWCBQAAMAagQIAAFgjUAAAAGsECgAAYM11oFizZo3OPfdcFRQUyOPxaPny5XH7jTGaN2+eBg8erNzcXJWXl2vLli3JqhcAAKQh14Fiz549GjFihKqrqxPuv/POO3XffffpoYce0rp163TooYdq/Pjx2rt3r3WxAAAgPfVx+4IJEyZowoQJCfcZY7RgwQLddNNNOu+88yRJjz32mILBoJYvX64LL7zwgNdEo1FFo9HY121tbW5LAgAAKZbUayi2bt2qcDis8vLyWFsgEFBZWZkaGhoSvqaqqkqBQCC2FRYWJrMkAADwDUhqoAiHw5KkYDAY1x4MBmP79ldZWalIJBLbmpubk1kSAAD4Brj+yCPZ/H6//H5/qssAAAAWknqGIj8/X5LU0tIS197S0hLbBwAAMk9SA0VxcbHy8/NVV1cXa2tra9O6desUCoWS+a0AAEAacf2Rx+7du/XOO+/Evt66das2bdqkAQMGqKioSDNnztStt96q4447TsXFxZo7d64KCgo0ceLEZNYNAADSiOtAsWHDBp111lmxr2fPni1Jmjp1qhYvXqzrrrtOe/bs0ZVXXqnW1laNHTtWK1euVE5OTvKqBgAAacVjjDGpLuLL2traFAgEdNlPfyyfLzvV5QAA0Gu1t3fo0SVPKRKJqH///l/Zl2d5AAAAawQKAABgjUABAACsESgAAIA1AgUAALBGoAAAANYIFAAAwBqBAgAAWCNQAAAAawQKAABgjUABAACsESgAAIA1AgUAALBGoAAAANYIFAAAwBqBAgAAWCNQAAAAawQKAABgjUABAACsESgAAIA1AgUAALBGoAAAANYIFAAAwFqfVBcA9DR9g0cnbPckaDMux040RrcP7jBOombH+hx4sxL/zeLxuB0p4SjWvZNTR3J88kFTqksArHCGAgAAWCNQAAAAawQKAABgjUABAACsESgAAIA1VnkALrlcLJEUxsXgjvU5juG048CRnHp6vU5/mzhUk5QflsMgTj8Az4E1Jm0VDgDOUAAAAHsECgAAYI1AAQAArBEoAACANQIFAACwxioPwKVkLFBIxkoR12M4LfNwep5Fgv7erCxXQzh9T+NquYS7tRVe52Ksxz749TBA78MZCgAAYI1AAQAArBEoAACANQIFAACwRqAAAADWWOWBXqNf8GhX/ZOymuPgF1B89TiJxnDq7Liaw11/b9bB/73hdj5ufrgeh86Ozw9xtRQjOU9gMQ4/XDerPwYMOSEptXzyQVNSxgHc4gwFAACwRqAAAADWCBQAAMAagQIAAFhzFSiqqqp02mmnqV+/fho0aJAmTpyopqb4C4D27t2riooKDRw4UH379tXkyZPV0tKS1KIBAEB6cRUo6uvrVVFRobVr1+qFF15QR0eHvv/972vPnj2xPrNmzdKKFSu0dOlS1dfXa/v27Zo0aVLSCwdc8yTejBJvDt2dh/d4DtgcB3ccI/GWcAhjEm8OnLp7vd6Em8MoiTenwZOwebzehJvTj9bxR+72BW6Om8MG9Caulo2uXLky7uvFixdr0KBBamxs1Le//W1FIhEtWrRIS5Ys0bhx4yRJNTU1GjZsmNauXavRo0cnr3IAAJA2rK6hiEQikqQBAwZIkhobG9XR0aHy8vJYn5KSEhUVFamhoSHhGNFoVG1tbXEbAADoWbocKDo7OzVz5kyNGTNGJ598siQpHA7L5/MpLy8vrm8wGFQ4HE44TlVVlQKBQGwrLCzsakkAACBFuhwoKioqtHnzZtXW1loVUFlZqUgkEtuam5utxgMAAN+8Lt16e9q0aXruuee0Zs0aDRkyJNaen5+v9vZ2tba2xp2laGlpUX5+fsKx/H6//H5/V8oA3HF7R2qHdk8S7qfteggXYzv1zHK80PLgB/qKSz7dje0gy5uVYGiXB85B4lHc3TLbw6WWgCNX7zDGGE2bNk3Lli3Tiy++qOLi4rj9paWlys7OVl1dXaytqalJ27ZtUygUSk7FAAAg7bg6Q1FRUaElS5bomWeeUb9+/WLXRQQCAeXm5ioQCOjyyy/X7NmzNWDAAPXv31/Tp09XKBRihQcAABnMVaBYuHChJOm73/1uXHtNTY0uueQSSdI999wjr9eryZMnKxqNavz48XrwwQeTUiwAAEhPrgKFOYjPcXNyclRdXa3q6uouFwUAAHoWnuUBAACsdWmVB9AjuVxa4XXob9ysaHAYY823futQi8MwbqK/ywUXjt3dTDPB4oyUcTjMrn6GLp218ZbuGxzoIThDAQAArBEoAACANQIFAACwRqAAAADWCBQAAMAaqzzQizg8zcHrsJrD5WqJRM/4cLx3Szeu5kjGqg2n/p7ufsdwU6PTz4rHbQApwRkKAABgjUABAACsESgAAIA1AgUAALBGoAAAANZY5YFew+O0hMJhJYbzYoGDf8aH06IFt8+VOHPj3ITfMWEdLldz/L8RtyZsT7SiY8yGGxL2zcpyeitxtxRlzcgEzzhxOBDffnVewvZEq20kyeuwmieR1SNvPui+AD7HGQoAAGCNQAEAAKwRKAAAgDUCBQAAsEagAAAA1ljlgV7E6QEaLkdxWkXh9lkZrsZIsILE9bM5HFazZB18LVnexJ0dn1niVqJj4fBnj9Nhc7OaA0DycIYCAABYI1AAAABrBAoAAGCNQAEAAKxxUSZ6kVRcfenu1tOuS3HROdGtwb96HBdjON5j/ODHluTqTxyv1+lW6gc/hhxu0w3APc5QAAAAawQKAABgjUABAACsESgAAIA1AgUAALDGKg/0Gm4XHLiVaByvx+V9o13cHdzpdteOa1O6cdGKY3cXtxJ3O77b4+ZJsKKDNR5A8nCGAgAAWCNQAAAAawQKAABgjUABAACsESgAAIA1VnkASXjGhSR5vQnWDCRnaK351q0HtDktIHE9uIvuier4ylqS8CeL09j1p95sPziApOEMBQAAsEagAAAA1ggUAADAGoECAABYI1AAAABrrPJAr+H22Q9O/T1epxyerKeCJPieWS6+nWPhDt1djNOdqzk+/wZJGgfAN44zFAAAwBqBAgAAWCNQAAAAawQKAABgzdVFmQsXLtTChQv1n//8R5J00kknad68eZowYYIkae/evbr22mtVW1uraDSq8ePH68EHH1QwGEx64UDSON1K25P4CkFXl146jOE0iNNFj2MbbzygzevQ2TgN7tDseDvtBBeCjt1wYB2S5HW4UNWp3fEW46W3OOw50FkbD76vWy+NnN9tYwOZytUZiiFDhuiOO+5QY2OjNmzYoHHjxum8887Tm2++KUmaNWuWVqxYoaVLl6q+vl7bt2/XpEmTuqVwAACQPlydoTj33HPjvr7tttu0cOFCrV27VkOGDNGiRYu0ZMkSjRs3TpJUU1OjYcOGae3atRo9enTyqgYAAGmly9dQ7Nu3T7W1tdqzZ49CoZAaGxvV0dGh8vLyWJ+SkhIVFRWpoaHBcZxoNKq2tra4DQAA9CyuA8Ubb7yhvn37yu/366qrrtKyZct04oknKhwOy+fzKS8vL65/MBhUOBx2HK+qqkqBQCC2FRYWup4EAABILdeB4oQTTtCmTZu0bt06XX311Zo6dareeuutLhdQWVmpSCQS25qbm7s8FgAASA3Xt972+Xw69thjJUmlpaVav3697r33Xl1wwQVqb29Xa2tr3FmKlpYW5efnO47n9/vl9/vdVw4kicfr8n7PTis0EqzocLyttcso7/UeuOTCOA3ueOvtxPNMeFtvKeFtsJO1mqM7b1MOIDWs70PR2dmpaDSq0tJSZWdnq66uLravqalJ27ZtUygUsv02AAAgjbk6Q1FZWakJEyaoqKhIu3bt0pIlS7R69WqtWrVKgUBAl19+uWbPnq0BAwaof//+mj59ukKhECs8AADIcK4Cxc6dO/Xzn/9cO3bsUCAQ0PDhw7Vq1Sp973vfkyTdc8898nq9mjx5ctyNrQAAQGZzFSgWLVr0lftzcnJUXV2t6upqq6IAAEDPwrM8AACANderPICeKtEqjK/ksIrC+Rkf3bly4eDHdpqnY30ufizJWs3hNA6Anov/qwEAgDUCBQAAsEagAAAA1ggUAADAGoECAABYY5UHeg3HRR6Oz9twyNuOD+g48Bt43K78cFGj46oVx/rsJWs1h8fN0hIAPQJnKAAAgDUCBQAAsEagAAAA1ggUAADAGoECAABYY5UHej3nZ184vcCh3c1KDJeLHNac+tuDH8PtahYnCcZZU3qLy0G6z0sj56e6BABfwhkKAABgjUABAACsESgAAIA1AgUAALDGRZnoNTwep/yc+GpFj8NFjI433k50AWayboPdjdHf8ccCAC7wVgIAAKwRKAAAgDUCBQAAsEagAAAA1ggUAADAGqs8AJc8Dve2Triew+Uttp36f+fVA28z7XxXb3e3Eq8/9eavLesLZ23s3ltvu7mddnfWwm29Afc4QwEAAKwRKAAAgDUCBQAAsEagAAAA1ggUAADAGqs8AKelFW4f5pFwZMelGK54Ey3pcBojSY8PAQA3OEMBAACsESgAAIA1AgUAALBGoAAAANYIFAAAwBqrPAC3XD1DI/GSC6fncLj9nq6GcP1NAeDgcYYCAABYI1AAAABrBAoAAGCNQAEAAKxxUSZ6Dfd3qu6hFzE6XHzZQ2cDoIfgDAUAALBGoAAAANYIFAAAwBqBAgAAWCNQAAAAa1arPO644w5VVlZqxowZWrBggSRp7969uvbaa1VbW6toNKrx48frwQcfVDAYTEa9wDfG7aoIh5ts2xciafXIm5Myjq2XRs5PdQkx6VQLAIszFOvXr9cf/vAHDR8+PK591qxZWrFihZYuXar6+npt375dkyZNsi4UAACkry4Fit27d2vKlCl65JFHdNhhh8XaI5GIFi1apLvvvlvjxo1TaWmpampq9M9//lNr165NWtEAACC9dClQVFRU6JxzzlF5eXlce2Njozo6OuLaS0pKVFRUpIaGhoRjRaNRtbW1xW0AAKBncX0NRW1trV599VWtX7/+gH3hcFg+n095eXlx7cFgUOFwOOF4VVVVuuWWW9yWAQAA0oirMxTNzc2aMWOGnnjiCeXk5CSlgMrKSkUikdjW3NyclHEBAMA3x9UZisbGRu3cuVOnnnpqrG3fvn1as2aNHnjgAa1atUrt7e1qbW2NO0vR0tKi/Pz8hGP6/X75/f6uVQ8kQbKeccGzMgD0Zq4Cxdlnn6033ngjru3SSy9VSUmJrr/+ehUWFio7O1t1dXWaPHmyJKmpqUnbtm1TKBRKXtUAACCtuAoU/fr108knnxzXduihh2rgwIGx9ssvv1yzZ8/WgAED1L9/f02fPl2hUEijR49OXtUAACCtJP3x5ffcc4+8Xq8mT54cd2MrAACQuawDxerVq+O+zsnJUXV1taqrq22HBgAAPQTP8gAAANaS/pEHgK931kbuvQIgs3CGAgAAWCNQAAAAawQKAABgjUABAACsESgAAIA1AgUAALBGoAAAANYIFAAAwBqBAgAAWCNQAAAAawQKAABgjUABAACsESgAAIA1AgUAALBGoAAAANYIFAAAwFqfVBcAfFM++aApKeMMGHJCUsYBvixZ/z6BVOEMBQAAsEagAAAA1ggUAADAGoECAABYI1AAAABrBAoAAGCNQAEAAKwRKAAAgDUCBQAAsEagAAAA1ggUAADAGoECAABYI1AAAABrBAoAAGCNQAEAAKwRKAAAgLU+qS4A6Gk++aAp1SUAQNrhDAUAALBGoAAAANYIFAAAwBqBAgAAWCNQAAAAawQKAABgjUABAACsESgAAIA1AgUAALBGoAAAANYIFAAAwJqrQHHzzTfL4/HEbSUlJbH9e/fuVUVFhQYOHKi+fftq8uTJamlpSXrRAAAgvbg+Q3HSSSdpx44dse3ll1+O7Zs1a5ZWrFihpUuXqr6+Xtu3b9ekSZOSWjAAAEg/rp822qdPH+Xn5x/QHolEtGjRIi1ZskTjxo2TJNXU1GjYsGFau3atRo8enXC8aDSqaDQa+7qtrc1tSQAAIMVcn6HYsmWLCgoKdPTRR2vKlCnatm2bJKmxsVEdHR0qLy+P9S0pKVFRUZEaGhocx6uqqlIgEIhthYWFXZgGAABIJVeBoqysTIsXL9bKlSu1cOFCbd26VWeeeaZ27dqlcDgsn8+nvLy8uNcEg0GFw2HHMSsrKxWJRGJbc3NzlyYCAABSx9VHHhMmTIj99/Dhw1VWVqahQ4fqySefVG5ubpcK8Pv98vv9XXotAABID1bLRvPy8nT88cfrnXfeUX5+vtrb29Xa2hrXp6WlJeE1FwAAIHNYBYrdu3fr3Xff1eDBg1VaWqrs7GzV1dXF9jc1NWnbtm0KhULWhQIAgPTl6iOPX/3qVzr33HM1dOhQbd++XfPnz1dWVpYuuugiBQIBXX755Zo9e7YGDBig/v37a/r06QqFQo4rPAAAQGZwFSg++OADXXTRRfrvf/+rI444QmPHjtXatWt1xBFHSJLuueceeb1eTZ48WdFoVOPHj9eDDz7YLYUDAID04THGmFQX8WVtbW0KBAK67Kc/ls+XnepyAADotdrbO/TokqcUiUTUv3//r+zLszwAAIA1AgUAALBGoAAAANYIFAAAwBqBAgAAWCNQAAAAawQKAABgjUABAACsESgAAIA1AgUAALBGoAAAANYIFAAAwBqBAgAAWCNQAAAAawQKAABgjUABAACsESgAAIA1AgUAALBGoAAAANYIFAAAwBqBAgAAWCNQAAAAawQKAABgjUABAACsESgAAIA1AgUAALBGoAAAANYIFAAAwBqBAgAAWCNQAAAAawQKAABgjUABAACsESgAAIA1AgUAALBGoAAAANYIFAAAwBqBAgAAWCNQAAAAawQKAABgjUABAACsESgAAIA1AgUAALBGoAAAANYIFAAAwBqBAgAAWCNQAAAAa64DxYcffqiLL75YAwcOVG5urk455RRt2LAhtt8Yo3nz5mnw4MHKzc1VeXm5tmzZktSiAQBAenEVKP73v/9pzJgxys7O1vPPP6+33npLv//973XYYYfF+tx5552677779NBDD2ndunU69NBDNX78eO3duzfpxQMAgPTQx03n3/3udyosLFRNTU2srbi4OPbfxhgtWLBAN910k8477zxJ0mOPPaZgMKjly5frwgsvTFLZAAAgnbg6Q/Hss89q1KhROv/88zVo0CCNHDlSjzzySGz/1q1bFQ6HVV5eHmsLBAIqKytTQ0NDwjGj0aja2triNgAA0LO4ChTvvfeeFi5cqOOOO06rVq3S1VdfrWuuuUZ/+tOfJEnhcFiSFAwG414XDAZj+/ZXVVWlQCAQ2woLC7syDwAAkEKuAkVnZ6dOPfVU3X777Ro5cqSuvPJKXXHFFXrooYe6XEBlZaUikUhsa25u7vJYAAAgNVwFisGDB+vEE0+Maxs2bJi2bdsmScrPz5cktbS0xPVpaWmJ7duf3+9X//794zYAANCzuAoUY8aMUVNTU1zb22+/raFDh0r6/ALN/Px81dXVxfa3tbVp3bp1CoVCSSgXAACkI1erPGbNmqUzzjhDt99+u37yk5/olVde0cMPP6yHH35YkuTxeDRz5kzdeuutOu6441RcXKy5c+eqoKBAEydO7I76AQBAGnAVKE477TQtW7ZMlZWV+s1vfqPi4mItWLBAU6ZMifW57rrrtGfPHl155ZVqbW3V2LFjtXLlSuXk5CS9eAAAkB48xhiT6iK+rK2tTYFAQJf99Mfy+bJTXQ4AAL1We3uHHl3ylCKRyNde48izPAAAgDUCBQAAsEagAAAA1ggUAADAGoECAABYI1AAAABrBAoAAGDN1Y2tvlme/9u+LK1umQEAQIbZ//fu/l874wwFAACwRqAAAADWCBQAAMAagQIAAFhLu4syv3hWWXtHR6K932wxAAD0KvEXYX7xu/hgniOadk8b/eCDD1RYWJjqMgAAwP9pbm7WkCFDvrJP2gWKzs5Obd++Xf369dOuXbtUWFio5ubmr31sak/W1tbGPDNEb5ijxDwzTW+YZ2+Yo5T8eRpjtGvXLhUUFMjr/eqrJNLuIw+v1xtLQR7P56de+vfvn9H/AL7APDNHb5ijxDwzTW+YZ2+Yo5TceQYCgYPqx0WZAADAGoECAABYS+tA4ff7NX/+fPn9/lSX0q2YZ+boDXOUmGem6Q3z7A1zlFI7z7S7KBMAAPQ8aX2GAgAA9AwECgAAYI1AAQAArBEoAACANQIFAACwltaBorq6WkcddZRycnJUVlamV155JdUlWVmzZo3OPfdcFRQUyOPxaPny5XH7jTGaN2+eBg8erNzcXJWXl2vLli2pKbaLqqqqdNppp6lfv34aNGiQJk6cqKamprg+e/fuVUVFhQYOHKi+fftq8uTJamlpSVHFXbNw4UINHz48dje6UCik559/PrY/E+a4vzvuuEMej0czZ86MtWXCPG+++WZ5PJ64raSkJLY/E+b4hQ8//FAXX3yxBg4cqNzcXJ1yyinasGFDbH8mvAcdddRRBxxPj8ejiooKSZlxPPft26e5c+equLhYubm5OuaYY/Tb3/427gFeKTmWJk3V1tYan89nHn30UfPmm2+aK664wuTl5ZmWlpZUl9Zlf/3rX82NN95onn76aSPJLFu2LG7/HXfcYQKBgFm+fLl57bXXzA9/+ENTXFxsPvvss9QU3AXjx483NTU1ZvPmzWbTpk3mBz/4gSkqKjK7d++O9bnqqqtMYWGhqaurMxs2bDCjR482Z5xxRgqrdu/ZZ581f/nLX8zbb79tmpqazA033GCys7PN5s2bjTGZMccve+WVV8xRRx1lhg8fbmbMmBFrz4R5zp8/35x00klmx44dse2jjz6K7c+EORpjzCeffGKGDh1qLrnkErNu3Trz3nvvmVWrVpl33nkn1icT3oN27twZdyxfeOEFI8m89NJLxpjMOJ633XabGThwoHnuuefM1q1bzdKlS03fvn3NvffeG+uTimOZtoHi9NNPNxUVFbGv9+3bZwoKCkxVVVUKq0qe/QNFZ2enyc/PN3fddVesrbW11fj9fvPnP/85BRUmx86dO40kU19fb4z5fE7Z2dlm6dKlsT7/+te/jCTT0NCQqjKT4rDDDjN//OMfM26Ou3btMscdd5x54YUXzHe+851YoMiUec6fP9+MGDEi4b5MmaMxxlx//fVm7Nixjvsz9T1oxowZ5phjjjGdnZ0ZczzPOeccc9lll8W1TZo0yUyZMsUYk7pjmZYfebS3t6uxsVHl5eWxNq/Xq/LycjU0NKSwsu6zdetWhcPhuDkHAgGVlZX16DlHIhFJ0oABAyRJjY2N6ujoiJtnSUmJioqKeuw89+3bp9raWu3Zs0ehUCjj5lhRUaFzzjknbj5SZh3LLVu2qKCgQEcffbSmTJmibdu2ScqsOT777LMaNWqUzj//fA0aNEgjR47UI488Etufie9B7e3tevzxx3XZZZfJ4/FkzPE844wzVFdXp7fffluS9Nprr+nll1/WhAkTJKXuWKbd00Yl6eOPP9a+ffsUDAbj2oPBoP7973+nqKruFQ6HJSnhnL/Y19N0dnZq5syZGjNmjE4++WRJn8/T5/MpLy8vrm9PnOcbb7yhUCikvXv3qm/fvlq2bJlOPPFEbdq0KWPmWFtbq1dffVXr168/YF+mHMuysjItXrxYJ5xwgnbs2KFbbrlFZ555pjZv3pwxc5Sk9957TwsXLtTs2bN1ww03aP369brmmmvk8/k0derUjHwPWr58uVpbW3XJJZdIypx/s3PmzFFbW5tKSkqUlZWlffv26bbbbtOUKVMkpe73SVoGCmSGiooKbd68WS+//HKqS+kWJ5xwgjZt2qRIJKKnnnpKU6dOVX19farLSprm5mbNmDFDL7zwgnJyclJdTrf54q86SRo+fLjKyso0dOhQPfnkk8rNzU1hZcnV2dmpUaNG6fbbb5ckjRw5Ups3b9ZDDz2kqVOnpri67rFo0SJNmDBBBQUFqS4lqZ588kk98cQTWrJkiU466SRt2rRJM2fOVEFBQUqPZVp+5HH44YcrKyvrgCtvW1palJ+fn6KqutcX88qUOU+bNk3PPfecXnrpJQ0ZMiTWnp+fr/b2drW2tsb174nz9Pl8OvbYY1VaWqqqqiqNGDFC9957b8bMsbGxUTt37tSpp56qPn36qE+fPqqvr9d9992nPn36KBgMZsQ895eXl6fjjz9e77zzTsYcS0kaPHiwTjzxxLi2YcOGxT7eybT3oPfff19///vf9Ytf/CLWlinH89e//rXmzJmjCy+8UKeccop+9rOfadasWaqqqpKUumOZloHC5/OptLRUdXV1sbbOzk7V1dUpFAqlsLLuU1xcrPz8/Lg5t7W1ad26dT1qzsYYTZs2TcuWLdOLL76o4uLiuP2lpaXKzs6Om2dTU5O2bdvWo+aZSGdnp6LRaMbM8eyzz9Ybb7yhTZs2xbZRo0ZpypQpsf/OhHnub/fu3Xr33Xc1ePDgjDmWkjRmzJgDlnC//fbbGjp0qKTMeQ/6Qk1NjQYNGqRzzjkn1pYpx/PTTz+V1xv/6zsrK0udnZ2SUngsu+1yT0u1tbXG7/ebxYsXm7feestceeWVJi8vz4TD4VSX1mW7du0yGzduNBs3bjSSzN133202btxo3n//fWPM58t88vLyzDPPPGNef/11c9555/W4JVtXX321CQQCZvXq1XFLtz799NNYn6uuusoUFRWZF1980WzYsMGEQiETCoVSWLV7c+bMMfX19Wbr1q3m9ddfN3PmzDEej8f87W9/M8ZkxhwT+fIqD2MyY57XXnutWb16tdm6dav5xz/+YcrLy83hhx9udu7caYzJjDka8/nS3z59+pjbbrvNbNmyxTzxxBPmkEMOMY8//nisTya8Bxnz+arAoqIic/311x+wLxOO59SpU82RRx4ZWzb69NNPm8MPP9xcd911sT6pOJZpGyiMMeb+++83RUVFxufzmdNPP92sXbs21SVZeemll4ykA7apU6caYz5f6jN37lwTDAaN3+83Z599tmlqakpt0S4lmp8kU1NTE+vz2WefmV/+8pfmsMMOM4cccoj50Y9+ZHbs2JG6orvgsssuM0OHDjU+n88cccQR5uyzz46FCWMyY46J7B8oMmGeF1xwgRk8eLDx+XzmyCOPNBdccEHcvRkyYY5fWLFihTn55JON3+83JSUl5uGHH47bnwnvQcYYs2rVKiMpYe2ZcDzb2trMjBkzTFFRkcnJyTFHH320ufHGG000Go31ScWx9BjzpVtrAQAAdEFaXkMBAAB6FgIFAACwRqAAAADWCBQAAMAagQIAAFgjUAAAAGsECgAAYI1AAQAArBEoAACANQIFAACwRqAAAADW/j+eh66m08pdXQAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First vector observations :  [1. 0.]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y60u21sys8kA"
   },
   "source": [
    "### Run the Environment for a few episodes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "a2uQUsoMtIUK",
    "ExecuteTime": {
     "start_time": "2023-10-04T12:53:02.785602Z",
     "end_time": "2023-10-04T12:53:04.145406Z"
    }
   },
   "source": [
    "for episode in range(3):\n",
    "  env.reset()\n",
    "  decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "  tracked_agent = -1 # -1 indicates not yet tracking\n",
    "  done = False # For the tracked_agent\n",
    "  episode_rewards = 0 # For the tracked_agent\n",
    "  while not done:\n",
    "    # Track the first agent we see if not tracking\n",
    "    # Note : len(decision_steps) = [number of agents that requested a decision]\n",
    "    if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "      tracked_agent = decision_steps.agent_id[0]\n",
    "\n",
    "    # Generate an action for all agents\n",
    "    action = spec.action_spec.random_action(len(decision_steps))\n",
    "\n",
    "    # Set the actions\n",
    "    env.set_actions(behavior_name, action)\n",
    "\n",
    "    # Move the simulation forward\n",
    "    env.step()\n",
    "\n",
    "    # Get the new simulation results\n",
    "    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    if tracked_agent in decision_steps: # The agent requested a decision\n",
    "      episode_rewards += decision_steps[tracked_agent].reward\n",
    "    if tracked_agent in terminal_steps: # The agent terminated its episode\n",
    "      episode_rewards += terminal_steps[tracked_agent].reward\n",
    "      done = True\n",
    "  print(f\"Total rewards for episode {episode} is {episode_rewards}\")\n"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rewards for episode 0 is -1.1499999966472387\n",
      "Total rewards for episode 1 is -1.5599999874830246\n",
      "Total rewards for episode 2 is -1.049999998882413\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-3grXNEtJPa"
   },
   "source": [
    "### Close the Environment to free the port it is using"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vdWG6_SqtNtv",
    "ExecuteTime": {
     "start_time": "2023-10-04T12:53:06.093669Z",
     "end_time": "2023-10-04T12:53:06.416573Z"
    }
   },
   "source": [
    "env.close()\n",
    "print(\"Closed environment\")\n"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed environment\n"
     ]
    }
   ]
  }
 ]
}


--- colab/Colab_UnityEnvironment_2_Train.ipynb ---
{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Colab-UnityEnvironment-2-Train.ipynb",
   "private_outputs": true,
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbVXrmEsLXDt"
   },
   "source": [
    "# ML-Agents Q-Learning with GridWorld\n",
    "<img src=\"https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/com.unity.ml-agents/Documentation~/images/gridworld.png?raw=true\" align=\"middle\" width=\"435\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNKTwHU3d2-l"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "htb-p1hSNX7D",
    "ExecuteTime": {
     "start_time": "2023-10-04T14:05:33.108073Z",
     "end_time": "2023-10-04T14:05:33.162099Z"
    }
   },
   "source": [
    "#@title Install Rendering Dependencies { display-mode: \"form\" }\n",
    "#@markdown (You only need to run this code when using Colab's hosted runtime)\n",
    "\n",
    "import os\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def progress(value, max=100):\n",
    "    return HTML(\"\"\"\n",
    "        <progress\n",
    "            value='{value}'\n",
    "            max='{max}',\n",
    "            style='width: 100%'\n",
    "        >\n",
    "            {value}\n",
    "        </progress>\n",
    "    \"\"\".format(value=value, max=max))\n",
    "\n",
    "pro_bar = display(progress(0, 100), display_id=True)\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  INSTALL_XVFB = True\n",
    "except ImportError:\n",
    "  INSTALL_XVFB = 'COLAB_ALWAYS_INSTALL_XVFB' in os.environ\n",
    "\n",
    "if INSTALL_XVFB:\n",
    "  !sudo apt-get update -qq\n",
    "  pro_bar.update(progress(50, 100))\n",
    "  !sudo DEBIAN_FRONTEND=noninteractive apt-get install -y -qq xvfb\n",
    "  pro_bar.update(progress(90, 100))\n",
    "  import subprocess\n",
    "  subprocess.Popen(['Xvfb', ':1', '-screen', '0', '1024x768x24', '-ac', '+extension', 'GLX', '+render', '-noreset'], \n",
    "                   stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "  os.environ[\"DISPLAY\"] = \":1\"\n",
    "pro_bar.update(progress(100, 100))"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n        <progress\n            value='0'\n            max='100',\n            style='width: 100%'\n        >\n            0\n        </progress>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pzj7wgapAcDs"
   },
   "source": [
    "### Installing ml-agents"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N8yfQqkbebQ5",
    "ExecuteTime": {
     "start_time": "2023-10-04T14:05:33.154160Z",
     "end_time": "2023-10-04T14:05:33.162429Z"
    }
   },
   "source": [
    "try:\n",
    "  import mlagents\n",
    "  print(\"ml-agents already installed\")\n",
    "except ImportError:\n",
    "  !python -m pip install -q mlagents==1.1.0\n",
    "  print(\"Installed ml-agents\")"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml-agents already installed\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jz81TWAkbuFY"
   },
   "source": [
    "## Train the GridWorld Environment with Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29n3dt1Zx5ty"
   },
   "source": [
    "### What is the GridWorld Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZhVRfdoyPmv"
   },
   "source": [
    "The [GridWorld](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Learning-Environment-Examples.html#gridworld) Environment is a simple Unity visual environment. The Agent is a blue square in a 3x3 grid that is trying to reach a green __`+`__ while avoiding a red __`x`__.\n",
    "\n",
    "The observation is an image obtained by a camera on top of the grid.\n",
    "\n",
    "The Action can be one of 5 :\n",
    " - Do not move\n",
    " - Move up\n",
    " - Move down\n",
    " - Move right\n",
    " - Move left\n",
    "\n",
    "The Agent receives a reward of _1.0_ if it reaches the green __`+`__, a penalty of _-1.0_ if it touches the red __`x`__ and a penalty of `-0.01` at every step (to force the Agent to solve the task as fast as possible)\n",
    "\n",
    "__Note__ There are 9 Agents, each in their own grid, at once in the environment. This alows for faster data collection.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Q-Learning Algorithm\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "q79rUp_Sx6A_",
    "ExecuteTime": {
     "start_time": "2023-10-04T14:05:33.154312Z",
     "end_time": "2023-10-04T14:05:33.643998Z"
    }
   },
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "from math import floor\n",
    "from torch.nn import Parameter\n",
    "\n",
    "\n",
    "class VisualQNetwork(torch.nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    input_shape: Tuple[int, int, int],\n",
    "    encoding_size: int,\n",
    "    output_size: int\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Creates a neural network that takes as input a batch of images (3\n",
    "    dimensional tensors) and outputs a batch of outputs (1 dimensional\n",
    "    tensors)\n",
    "    \"\"\"\n",
    "    super(VisualQNetwork, self).__init__()\n",
    "    height = input_shape[1]\n",
    "    width = input_shape[2]\n",
    "    initial_channels = input_shape[0]\n",
    "    conv_1_hw = self.conv_output_shape((height, width), 8, 4)\n",
    "    conv_2_hw = self.conv_output_shape(conv_1_hw, 4, 2)\n",
    "    self.final_flat = conv_2_hw[0] * conv_2_hw[1] * 32\n",
    "    self.conv1 = torch.nn.Conv2d(initial_channels, 16, [8, 8], [4, 4])\n",
    "    self.conv2 = torch.nn.Conv2d(16, 32, [4, 4], [2, 2])\n",
    "    self.dense1 = torch.nn.Linear(self.final_flat, encoding_size)\n",
    "    self.dense2 = torch.nn.Linear(encoding_size, output_size)\n",
    "\n",
    "  def forward(self, visual_obs: torch.tensor):\n",
    "    conv_1 = torch.relu(self.conv1(visual_obs))\n",
    "    conv_2 = torch.relu(self.conv2(conv_1))\n",
    "    hidden = self.dense1(conv_2.reshape([-1, self.final_flat]))\n",
    "    hidden = torch.relu(hidden)\n",
    "    hidden = self.dense2(hidden)\n",
    "    return hidden\n",
    "\n",
    "  @staticmethod\n",
    "  def conv_output_shape(\n",
    "    h_w: Tuple[int, int],\n",
    "    kernel_size: int = 1,\n",
    "    stride: int = 1,\n",
    "    pad: int = 0,\n",
    "    dilation: int = 1,\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Computes the height and width of the output of a convolution layer.\n",
    "    \"\"\"\n",
    "    h = floor(\n",
    "      ((h_w[0] + (2 * pad) - (dilation * (kernel_size - 1)) - 1) / stride) + 1\n",
    "    )\n",
    "    w = floor(\n",
    "      ((h_w[1] + (2 * pad) - (dilation * (kernel_size - 1)) - 1) / stride) + 1\n",
    "    )\n",
    "    return h, w\n"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this Notebook, we will implement a very simple Q-Learning algorithm. We will use [pytorch](https://pytorch.org/) to do so.\n",
    "\n",
    "Below is the code to create the neural network we will use in the Notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZoaEBAo2L0F"
   },
   "source": [
    "We will now create a few classes to help us store the data we will use to train the Q-Learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "L772fe2q39DO",
    "ExecuteTime": {
     "start_time": "2023-10-04T14:05:33.647166Z",
     "end_time": "2023-10-04T14:05:33.648666Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from typing import NamedTuple, List\n",
    "\n",
    "\n",
    "class Experience(NamedTuple):\n",
    "  \"\"\"\n",
    "  An experience contains the data of one Agent transition.\n",
    "  - Observation\n",
    "  - Action\n",
    "  - Reward\n",
    "  - Done flag\n",
    "  - Next Observation\n",
    "  \"\"\"\n",
    "\n",
    "  obs: np.ndarray\n",
    "  action: np.ndarray\n",
    "  reward: float\n",
    "  done: bool\n",
    "  next_obs: np.ndarray\n",
    "\n",
    "# A Trajectory is an ordered sequence of Experiences\n",
    "Trajectory = List[Experience]\n",
    "\n",
    "# A Buffer is an unordered list of Experiences from multiple Trajectories\n",
    "Buffer = List[Experience]"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HsM1d5I3_Tj"
   },
   "source": [
    "Now, we can create our trainer class. The role of this trainer is to collect data from the Environment according to a Policy, and then train the Q-Network with that data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KkzBoRJCb18t",
    "ExecuteTime": {
     "start_time": "2023-10-04T14:05:33.657806Z",
     "end_time": "2023-10-04T14:05:33.736196Z"
    }
   },
   "source": [
    "from mlagents_envs.environment import ActionTuple, BaseEnv\n",
    "from typing import Dict\n",
    "import random\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "  @staticmethod\n",
    "  def generate_trajectories(\n",
    "    env: BaseEnv, q_net: VisualQNetwork, buffer_size: int, epsilon: float\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Given a Unity Environment and a Q-Network, this method will generate a\n",
    "    buffer of Experiences obtained by running the Environment with the Policy\n",
    "    derived from the Q-Network.\n",
    "    :param BaseEnv: The UnityEnvironment used.\n",
    "    :param q_net: The Q-Network used to collect the data.\n",
    "    :param buffer_size: The minimum size of the buffer this method will return.\n",
    "    :param epsilon: Will add a random normal variable with standard deviation.\n",
    "    epsilon to the value heads of the Q-Network to encourage exploration.\n",
    "    :returns: a Tuple containing the created buffer and the average cumulative\n",
    "    the Agents obtained.\n",
    "    \"\"\"\n",
    "    # Create an empty Buffer\n",
    "    buffer: Buffer = []\n",
    "\n",
    "    # Reset the environment\n",
    "    env.reset()\n",
    "    # Read and store the Behavior Name of the Environment\n",
    "    behavior_name = list(env.behavior_specs)[0]\n",
    "    # Read and store the Behavior Specs of the Environment\n",
    "    spec = env.behavior_specs[behavior_name]\n",
    "\n",
    "    # Create a Mapping from AgentId to Trajectories. This will help us create\n",
    "    # trajectories for each Agents\n",
    "    dict_trajectories_from_agent: Dict[int, Trajectory] = {}\n",
    "    # Create a Mapping from AgentId to the last observation of the Agent\n",
    "    dict_last_obs_from_agent: Dict[int, np.ndarray] = {}\n",
    "    # Create a Mapping from AgentId to the last observation of the Agent\n",
    "    dict_last_action_from_agent: Dict[int, np.ndarray] = {}\n",
    "    # Create a Mapping from AgentId to cumulative reward (Only for reporting)\n",
    "    dict_cumulative_reward_from_agent: Dict[int, float] = {}\n",
    "    # Create a list to store the cumulative rewards obtained so far\n",
    "    cumulative_rewards: List[float] = []\n",
    "\n",
    "    while len(buffer) < buffer_size:  # While not enough data in the buffer\n",
    "      # Get the Decision Steps and Terminal Steps of the Agents\n",
    "      decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "      # permute the tensor to go from NHWC to NCHW\n",
    "      # order = (0, 3, 1, 2)\n",
    "      # decision_steps.obs = [np.transpose(obs, order) for obs in decision_steps.obs]\n",
    "      # terminal_steps.obs = [np.transpose(obs, order) for obs in terminal_steps.obs]\n",
    "\n",
    "      # For all Agents with a Terminal Step:\n",
    "      for agent_id_terminated in terminal_steps:\n",
    "        # Create its last experience (is last because the Agent terminated)\n",
    "        last_experience = Experience(\n",
    "          obs=dict_last_obs_from_agent[agent_id_terminated].copy(),\n",
    "          reward=terminal_steps[agent_id_terminated].reward,\n",
    "          done=not terminal_steps[agent_id_terminated].interrupted,\n",
    "          action=dict_last_action_from_agent[agent_id_terminated].copy(),\n",
    "          next_obs=terminal_steps[agent_id_terminated].obs[0],\n",
    "        )\n",
    "        # Clear its last observation and action (Since the trajectory is over)\n",
    "        dict_last_obs_from_agent.pop(agent_id_terminated)\n",
    "        dict_last_action_from_agent.pop(agent_id_terminated)\n",
    "        # Report the cumulative reward\n",
    "        cumulative_reward = (\n",
    "          dict_cumulative_reward_from_agent.pop(agent_id_terminated)\n",
    "          + terminal_steps[agent_id_terminated].reward\n",
    "        )\n",
    "        cumulative_rewards.append(cumulative_reward)\n",
    "        # Add the Trajectory and the last experience to the buffer\n",
    "        buffer.extend(dict_trajectories_from_agent.pop(agent_id_terminated))\n",
    "        buffer.append(last_experience)\n",
    "\n",
    "      # For all Agents with a Decision Step:\n",
    "      for agent_id_decisions in decision_steps:\n",
    "        # If the Agent does not have a Trajectory, create an empty one\n",
    "        if agent_id_decisions not in dict_trajectories_from_agent:\n",
    "          dict_trajectories_from_agent[agent_id_decisions] = []\n",
    "          dict_cumulative_reward_from_agent[agent_id_decisions] = 0\n",
    "\n",
    "        # If the Agent requesting a decision has a \"last observation\"\n",
    "        if agent_id_decisions in dict_last_obs_from_agent:\n",
    "          # Create an Experience from the last observation and the Decision Step\n",
    "          exp = Experience(\n",
    "            obs=dict_last_obs_from_agent[agent_id_decisions].copy(),\n",
    "            reward=decision_steps[agent_id_decisions].reward,\n",
    "            done=False,\n",
    "            action=dict_last_action_from_agent[agent_id_decisions].copy(),\n",
    "            next_obs=decision_steps[agent_id_decisions].obs[0],\n",
    "          )\n",
    "          # Update the Trajectory of the Agent and its cumulative reward\n",
    "          dict_trajectories_from_agent[agent_id_decisions].append(exp)\n",
    "          dict_cumulative_reward_from_agent[agent_id_decisions] += (\n",
    "            decision_steps[agent_id_decisions].reward\n",
    "          )\n",
    "        # Store the observation as the new \"last observation\"\n",
    "        dict_last_obs_from_agent[agent_id_decisions] = (\n",
    "          decision_steps[agent_id_decisions].obs[0]\n",
    "        )\n",
    "\n",
    "      # Generate an action for all the Agents that requested a decision\n",
    "      # Compute the values for each action given the observation\n",
    "      actions_values = (\n",
    "        q_net(torch.from_numpy(decision_steps.obs[0])).detach().numpy()\n",
    "      )\n",
    "      # Add some noise with epsilon to the values\n",
    "      actions_values += epsilon * (\n",
    "        np.random.randn(actions_values.shape[0], actions_values.shape[1])\n",
    "      ).astype(np.float32)\n",
    "      # Pick the best action using argmax\n",
    "      actions = np.argmax(actions_values, axis=1)\n",
    "      actions.resize((len(decision_steps), 1))\n",
    "      # Store the action that was picked, it will be put in the trajectory later\n",
    "      for agent_index, agent_id in enumerate(decision_steps.agent_id):\n",
    "        dict_last_action_from_agent[agent_id] = actions[agent_index]\n",
    "\n",
    "      # Set the actions in the environment\n",
    "      # Unity Environments expect ActionTuple instances.\n",
    "      action_tuple = ActionTuple()\n",
    "      action_tuple.add_discrete(actions)\n",
    "      env.set_actions(behavior_name, action_tuple)\n",
    "      # Perform a step in the simulation\n",
    "      env.step()\n",
    "    return buffer, np.mean(cumulative_rewards)\n",
    "\n",
    "  @staticmethod\n",
    "  def update_q_net(\n",
    "    q_net: VisualQNetwork,\n",
    "    optimizer: torch.optim,\n",
    "    buffer: Buffer,\n",
    "    action_size: int\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Performs an update of the Q-Network using the provided optimizer and buffer\n",
    "    \"\"\"\n",
    "    BATCH_SIZE = 1000\n",
    "    NUM_EPOCH = 3\n",
    "    GAMMA = 0.9\n",
    "    batch_size = min(len(buffer), BATCH_SIZE)\n",
    "    random.shuffle(buffer)\n",
    "    # Split the buffer into batches\n",
    "    batches = [\n",
    "      buffer[batch_size * start : batch_size * (start + 1)]\n",
    "      for start in range(int(len(buffer) / batch_size))\n",
    "    ]\n",
    "    for _ in range(NUM_EPOCH):\n",
    "      for batch in batches:\n",
    "        # Create the Tensors that will be fed in the network\n",
    "        obs = torch.from_numpy(np.stack([ex.obs for ex in batch]))\n",
    "        reward = torch.from_numpy(\n",
    "          np.array([ex.reward for ex in batch], dtype=np.float32).reshape(-1, 1)\n",
    "        )\n",
    "        done = torch.from_numpy(\n",
    "          np.array([ex.done for ex in batch], dtype=np.float32).reshape(-1, 1)\n",
    "        )\n",
    "        action = torch.from_numpy(np.stack([ex.action for ex in batch]))\n",
    "        next_obs = torch.from_numpy(np.stack([ex.next_obs for ex in batch]))\n",
    "\n",
    "        # Use the Bellman equation to update the Q-Network\n",
    "        target = (\n",
    "          reward\n",
    "          + (1.0 - done)\n",
    "          * GAMMA\n",
    "          * torch.max(q_net(next_obs).detach(), dim=1, keepdim=True).values\n",
    "        )\n",
    "        mask = torch.zeros((len(batch), action_size))\n",
    "        mask.scatter_(1, action, 1)\n",
    "        prediction = torch.sum(q_net(obs) * mask, dim=1, keepdim=True)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        loss = criterion(prediction, target)\n",
    "\n",
    "        # Perform the backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcU4ZMAEWCvX"
   },
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_lIHijQfbYjh",
    "ExecuteTime": {
     "start_time": "2023-10-04T14:05:33.743251Z",
     "end_time": "2023-10-04T14:06:37.703976Z"
    }
   },
   "source": [
    "# -----------------\n",
    "# This code is used to close an env that might not have been closed before\n",
    "try:\n",
    "  env.close()\n",
    "except:\n",
    "  pass\n",
    "# -----------------\n",
    "\n",
    "from mlagents_envs.registry import default_registry\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# Create the GridWorld Environment from the registry\n",
    "env = default_registry[\"GridWorld\"].make()\n",
    "print(\"GridWorld environment created.\")\n",
    "\n",
    "num_actions = 5\n",
    "\n",
    "try:\n",
    "  # Create a new Q-Network.\n",
    "  qnet = VisualQNetwork((3, 64, 84), 126, num_actions)\n",
    "\n",
    "  experiences: Buffer = []\n",
    "  optim = torch.optim.Adam(qnet.parameters(), lr= 0.001)\n",
    "\n",
    "  cumulative_rewards: List[float] = []\n",
    "\n",
    "  # The number of training steps that will be performed\n",
    "  NUM_TRAINING_STEPS = int(os.getenv('QLEARNING_NUM_TRAINING_STEPS', 10))\n",
    "  # The number of experiences to collect per training step\n",
    "  NUM_NEW_EXP = int(os.getenv('QLEARNING_NUM_NEW_EXP', 1000))\n",
    "  # The maximum size of the Buffer\n",
    "  BUFFER_SIZE = int(os.getenv('QLEARNING_BUFFER_SIZE', 10000))\n",
    "\n",
    "  for n in range(NUM_TRAINING_STEPS):\n",
    "    new_exp,_ = Trainer.generate_trajectories(env, qnet, NUM_NEW_EXP, epsilon=0.1)\n",
    "    random.shuffle(experiences)\n",
    "    if len(experiences) > BUFFER_SIZE:\n",
    "      experiences = experiences[:BUFFER_SIZE]\n",
    "    experiences.extend(new_exp)\n",
    "    Trainer.update_q_net(qnet, optim, experiences, num_actions)\n",
    "    _, rewards = Trainer.generate_trajectories(env, qnet, 100, epsilon=0)\n",
    "    cumulative_rewards.append(rewards)\n",
    "    print(\"Training step \", n+1, \"\\treward \", rewards)\n",
    "except KeyboardInterrupt:\n",
    "  print(\"\\nTraining interrupted, continue to next cell to save to save the model.\")\n",
    "finally:\n",
    "  env.close()\n",
    "\n",
    "# Show the training graph\n",
    "try:\n",
    "  plt.plot(range(NUM_TRAINING_STEPS), cumulative_rewards)\n",
    "except ValueError:\n",
    "  print(\"\\nPlot failed on interrupted training.\")\n"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "GridWorld environment created.\n",
      "Training step  1 \treward  -1.001111091218061\n",
      "Training step  2 \treward  -0.9999999776482582\n",
      "Training step  3 \treward  -0.3366666516909997\n",
      "Training step  4 \treward  -0.8029999820515513\n",
      "Training step  5 \treward  -0.9999999776482582\n",
      "Training step  6 \treward  -0.7799999800821146\n",
      "Training step  7 \treward  -0.5644444293446012\n",
      "Training step  8 \treward  -0.7899999848256508\n",
      "Training step  9 \treward  -0.7799999800821146\n",
      "Training step  10 \treward  -0.7777777603930898\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Export PyTorch Model to ONNX"
   ],
   "metadata": {
    "id": "pFEP9JT5KuO-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell provides an example of some of the extra tensors a model needs to work for ML-Agents inference with Inference Engine. The GridWorldColab scene is configured to work with this ONNX file.\n",
    "Only policy models need to be exported for inference and they need the following additional tensors:\n",
    "\n",
    "*   All models need version_number\n",
    "*   All models need memory_size\n",
    "*   Models with continuous outputs need continuous_action_output_shape\n",
    "*   Models with discrete outputs need discrete_action_output_shape and an additional mask input that matches the shape of the discrete outputs\n",
    "*   The mask input must be connected to the outputs or it will be pruned on export, if mask values aren't being set they will be 1, so multiplying the discrete outputs by the mask will have no effect"
   ],
   "metadata": {
    "id": "V0V7bH4mOnUF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class WrapperNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            qnet: VisualQNetwork,\n",
    "            discrete_output_sizes: List[int],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Wraps the VisualQNetwork adding extra constants and dummy mask inputs\n",
    "        required by runtime inference with Inference Engine.\n",
    "\n",
    "        For environment continuous actions outputs would need to add them\n",
    "        similarly to how discrete action outputs work, both in the wrapper\n",
    "        and in the ONNX output_names / dynamic_axes.\n",
    "        \"\"\"\n",
    "        super(WrapperNet, self).__init__()\n",
    "        self.qnet = qnet\n",
    "\n",
    "        # version_number\n",
    "        #   MLAgents1_0 = 2   (not covered by this example)\n",
    "        #   MLAgents2_0 = 3\n",
    "        version_number = torch.Tensor([3])\n",
    "        self.version_number = Parameter(version_number, requires_grad=False)\n",
    "\n",
    "        # memory_size\n",
    "        # TODO: document case where memory is not zero.\n",
    "        memory_size = torch.Tensor([0])\n",
    "        self.memory_size = Parameter(memory_size, requires_grad=False)\n",
    "\n",
    "        # discrete_action_output_shape\n",
    "        output_shape=torch.Tensor([discrete_output_sizes])\n",
    "        self.discrete_shape = Parameter(output_shape, requires_grad=False)\n",
    "\n",
    "\n",
    "    # if you have discrete actions ML-agents expects corresponding a mask\n",
    "    # tensor with the same shape to exist as input\n",
    "    def forward(self, visual_obs: torch.tensor, mask: torch.tensor):\n",
    "        qnet_result = self.qnet(visual_obs)\n",
    "        # Connect mask to keep it from getting pruned\n",
    "        # Mask values will be 1 if you never call SetActionMask() in\n",
    "        # WriteDiscreteActionMask()\n",
    "        qnet_result = torch.mul(qnet_result, mask)\n",
    "        action = torch.argmax(qnet_result, dim=1, keepdim=True)\n",
    "        return [action], self.discrete_shape, self.version_number, self.memory_size\n",
    "\n",
    "\n",
    "torch.onnx.export(\n",
    "    WrapperNet(qnet, [num_actions]),\n",
    "    # A tuple with an example of the input tensors\n",
    "    (torch.tensor([experiences[0].obs]), torch.ones(1, num_actions)),\n",
    "    'GridWorldColab.onnx',\n",
    "    opset_version=9,\n",
    "    # input_names must correspond to the WrapperNet forward parameters\n",
    "    # obs will be obs_0, obs_1, etc.\n",
    "    input_names=[\"obs_0\", \"action_masks\"],\n",
    "    # output_names must correspond to the return tuple of the WrapperNet\n",
    "    # forward function.\n",
    "    output_names=[\"discrete_actions\", \"discrete_action_output_shape\",\n",
    "                  \"version_number\", \"memory_size\"],\n",
    "    # All inputs and outputs should have their 0th dimension be designated\n",
    "    # as 'batch'\n",
    "    dynamic_axes={'obs_0': {0: 'batch'},\n",
    "                  'action_masks': {0: 'batch'},\n",
    "                  'discrete_actions': {0: 'batch'},\n",
    "                  'discrete_action_output_shape': {0: 'batch'}\n",
    "                 }\n",
    "    )\n"
   ],
   "metadata": {
    "id": "T0zzg8fWpTYO",
    "ExecuteTime": {
     "start_time": "2023-10-04T14:06:37.715502Z",
     "end_time": "2023-10-04T14:06:37.802791Z"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_174482/2956909052.py:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  (torch.tensor([experiences[0].obs]), torch.ones(1, num_actions)),\n"
     ]
    }
   ]
  }
 ]
}


## Links discovered
- ["The [GridWorld](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Learning-Environment-Examples.html#gridworld)
- ["In this Notebook, we will implement a very simple Q-Learning algorithm. We will use [pytorch](https://pytorch.org/)

--- colab/Colab_UnityEnvironment_3_SideChannel.ipynb ---
{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Colab-UnityEnvironment-3-SideChannel.ipynb",
   "private_outputs": true,
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbVXrmEsLXDt"
   },
   "source": [
    "# ML-Agents Use SideChannels\n",
    "<img src=\"https://raw.githubusercontent.com/Unity-Technologies/ml-agents/release/4.0.0/docs/images/3dball_big.png\" align=\"middle\" width=\"435\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNKTwHU3d2-l"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "htb-p1hSNX7D",
    "ExecuteTime": {
     "start_time": "2023-10-04T14:07:19.585847Z",
     "end_time": "2023-10-04T14:07:19.586641Z"
    }
   },
   "source": [
    "#@title Install Rendering Dependencies { display-mode: \"form\" }\n",
    "#@markdown (You only need to run this code when using Colab's hosted runtime)\n",
    "\n",
    "import os\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def progress(value, max=100):\n",
    "    return HTML(\"\"\"\n",
    "        <progress\n",
    "            value='{value}'\n",
    "            max='{max}',\n",
    "            style='width: 100%'\n",
    "        >\n",
    "            {value}\n",
    "        </progress>\n",
    "    \"\"\".format(value=value, max=max))\n",
    "\n",
    "pro_bar = display(progress(0, 100), display_id=True)\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  INSTALL_XVFB = True\n",
    "except ImportError:\n",
    "  INSTALL_XVFB = 'COLAB_ALWAYS_INSTALL_XVFB' in os.environ\n",
    "\n",
    "if INSTALL_XVFB:\n",
    "  !sudo apt-get update -qq\n",
    "  pro_bar.update(progress(50, 100))\n",
    "  !sudo DEBIAN_FRONTEND=noninteractive apt-get install -y -qq xvfb\n",
    "  pro_bar.update(progress(90, 100))\n",
    "  import subprocess\n",
    "  subprocess.Popen(['Xvfb', ':1', '-screen', '0', '1024x768x24', '-ac', '+extension', 'GLX', '+render', '-noreset'], \n",
    "                   stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "  os.environ[\"DISPLAY\"] = \":1\"\n",
    "pro_bar.update(progress(100, 100))"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n        <progress\n            value='0'\n            max='100',\n            style='width: 100%'\n        >\n            0\n        </progress>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pzj7wgapAcDs"
   },
   "source": [
    "### Installing ml-agents"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N8yfQqkbebQ5",
    "ExecuteTime": {
     "start_time": "2023-10-04T14:07:19.586390Z",
     "end_time": "2023-10-04T14:07:19.587185Z"
    }
   },
   "source": [
    "try:\n",
    "  import mlagents\n",
    "  print(\"ml-agents already installed\")\n",
    "except ImportError:\n",
    "  !python -m pip install -q mlagents==1.1.0\n",
    "  print(\"Installed ml-agents\")"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml-agents already installed\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_u74YhSmW6gD"
   },
   "source": [
    "## Side Channel\n",
    "\n",
    "SideChannels are objects that can be passed to the constructor of a UnityEnvironment or the `make()` method of a registry entry to send non Reinforcement Learning related data.\n",
    "More information available [here](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Python-LLAPI.html#communicating-additional-information-with-the-environment)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4RXnhLRk7Uc"
   },
   "source": [
    "### Engine Configuration SideChannel\n",
    "The [Engine Configuration Side Channel](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Python-LLAPI.html#engineconfigurationchannel) is used to configure how the Unity Engine should run.\n",
    "We will use the GridWorld environment to demonstrate how to use the EngineConfigurationChannel."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YSf-WhxbqtLw",
    "ExecuteTime": {
     "start_time": "2023-10-04T14:07:19.586596Z",
     "end_time": "2023-10-04T14:07:21.264486Z"
    }
   },
   "source": [
    "# -----------------\n",
    "# This code is used to close an env that might not have been closed before\n",
    "try:\n",
    "  env.close()\n",
    "except:\n",
    "  pass\n",
    "# -----------------\n",
    "\n",
    "from mlagents_envs.registry import default_registry\n",
    "env_id = \"GridWorld\"\n",
    "\n",
    "# Import the EngineConfigurationChannel class\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "\n",
    "# Create the side channel\n",
    "engine_config_channel = EngineConfigurationChannel()\n",
    "\n",
    "# Pass the side channel to the make method\n",
    "# Note, the make method takes a LIST of SideChannel as input\n",
    "env = default_registry[env_id].make(side_channels = [engine_config_channel])\n",
    "\n",
    "# Configure the Unity Engine\n",
    "engine_config_channel.set_configuration_parameters(target_frame_rate = 30)\n",
    "\n",
    "env.reset()\n",
    "\n",
    "# ...\n",
    "# Perform experiment on environment\n",
    "# ...\n",
    "\n",
    "env.close()"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1lIx3_l24OP"
   },
   "source": [
    "### Environment Parameters Channel\n",
    "The [Environment Parameters Side Channel](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Python-LLAPI.html#environmentparameters) is used to modify environment parameters during the simulation.\n",
    "We will use the GridWorld environment to demonstrate how to use the EngineConfigurationChannel."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dhtl0mpeqxYi",
    "ExecuteTime": {
     "start_time": "2023-10-04T14:09:13.046780Z",
     "end_time": "2023-10-04T14:09:14.768804Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# -----------------\n",
    "# This code is used to close an env that might not have been closed before\n",
    "try:\n",
    "  env.close()\n",
    "except:\n",
    "  pass\n",
    "# -----------------\n",
    "\n",
    "from mlagents_envs.registry import default_registry\n",
    "env_id = \"GridWorld\"\n",
    "\n",
    "# Import the EngineConfigurationChannel class\n",
    "from mlagents_envs.side_channel.environment_parameters_channel import EnvironmentParametersChannel\n",
    "\n",
    "# Create the side channel\n",
    "env_parameters = EnvironmentParametersChannel()\n",
    "\n",
    "# Pass the side channel to the make method\n",
    "# Note, the make method takes a LIST of SideChannel as input\n",
    "env = default_registry[env_id].make(side_channels = [env_parameters])\n",
    "\n",
    "env.reset()\n",
    "behavior_name = list(env.behavior_specs)[0]\n",
    "\n",
    "print(\"Observation without changing the environment parameters\")\n",
    "decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "plt.imshow(np.moveaxis(decision_steps.obs[0][0,:,:,:], 0, -1))\n",
    "plt.show()\n",
    "\n",
    "print(\"Increasing the dimensions of the grid from 5 to 7\")\n",
    "env_parameters.set_float_parameter(\"gridSize\", 7)\n",
    "print(\"Increasing the number of X from 1 to 5\")\n",
    "env_parameters.set_float_parameter(\"numObstacles\", 5)\n",
    "\n",
    "# Any change to a SideChannel will only be effective after a step or reset\n",
    "# In the GridWorld Environment, the grid's dimensions can only change at reset\n",
    "env.reset()\n",
    "\n",
    "\n",
    "decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "plt.imshow(np.moveaxis(decision_steps.obs[0][0,:,:,:], 0, -1))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "env.close()"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "Observation without changing the environment parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increasing the dimensions of the grid from 5 to 7\n",
      "Increasing the number of X from 1 to 5\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1rwnVq2qyoO"
   },
   "source": [
    "### Creating your own Side Channels\n",
    "You can send various kinds of data between a Unity Environment and Python but you will need to [create your own implementation of a Side Channel](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Custom-SideChannels.html#custom-side-channels) for advanced use cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}


## Links discovered
- ["## Side Channel\n",
    "\n",
    "SideChannels are objects that can be passed to the constructor of a UnityEnvironment or the `make()` method of a registry entry to send non Reinforcement Learning related data.\n",
    "More information available [here](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Python-LLAPI.html#communicating-additional-information-with-the-environment)
- ["### Engine Configuration SideChannel\n",
    "The [Engine Configuration Side Channel](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Python-LLAPI.html#engineconfigurationchannel)
- ["### Environment Parameters Channel\n",
    "The [Environment Parameters Side Channel](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Python-LLAPI.html#environmentparameters)
- ["### Creating your own Side Channels\n",
    "You can send various kinds of data between a Unity Environment and Python but you will need to [create your own implementation of a Side Channel](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Custom-SideChannels.html#custom-side-channels)

--- colab/Colab_UnityEnvironment_4_SB3VectorEnv.ipynb ---
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbVXrmEsLXDt"
   },
   "source": [
    "# ML-Agents run with Stable Baselines 3\n",
    "<img src=\"https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/com.unity.ml-agents/Documentation~/images/image-banner.png?raw=true\" align=\"middle\" width=\"435\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNKTwHU3d2-l"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Install Rendering Dependencies { display-mode: \"form\" }\n",
    "#@markdown (You only need to run this code when using Colab's hosted runtime)\n",
    "\n",
    "import os\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def progress(value, max=100):\n",
    "    return HTML(\"\"\"\n",
    "        <progress\n",
    "            value='{value}'\n",
    "            max='{max}',\n",
    "            style='width: 100%'\n",
    "        >\n",
    "            {value}\n",
    "        </progress>\n",
    "    \"\"\".format(value=value, max=max))\n",
    "\n",
    "pro_bar = display(progress(0, 100), display_id=True)\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  INSTALL_XVFB = True\n",
    "except ImportError:\n",
    "  INSTALL_XVFB = 'COLAB_ALWAYS_INSTALL_XVFB' in os.environ\n",
    "\n",
    "if INSTALL_XVFB:\n",
    "  with open('frame-buffer', 'w') as writefile:\n",
    "    writefile.write(\"\"\"#taken from https://gist.github.com/jterrace/2911875\n",
    "XVFB=/usr/bin/Xvfb\n",
    "XVFBARGS=\":1 -screen 0 1024x768x24 -ac +extension GLX +render -noreset\"\n",
    "PIDFILE=./frame-buffer.pid\n",
    "case \"$1\" in\n",
    "  start)\n",
    "    echo -n \"Starting virtual X frame buffer: Xvfb\"\n",
    "    /sbin/start-stop-daemon --start --quiet --pidfile $PIDFILE --make-pidfile --background --exec $XVFB -- $XVFBARGS\n",
    "    echo \".\"\n",
    "    ;;\n",
    "  stop)\n",
    "    echo -n \"Stopping virtual X frame buffer: Xvfb\"\n",
    "    /sbin/start-stop-daemon --stop --quiet --pidfile $PIDFILE\n",
    "    rm $PIDFILE\n",
    "    echo \".\"\n",
    "    ;;\n",
    "  restart)\n",
    "    $0 stop\n",
    "    $0 start\n",
    "    ;;\n",
    "  *)\n",
    "        echo \"Usage: /etc/init.d/xvfb {start|stop|restart}\"\n",
    "        exit 1\n",
    "esac\n",
    "exit 0\n",
    "    \"\"\")\n",
    "  !sudo apt-get update\n",
    "  pro_bar.update(progress(10, 100))\n",
    "  !sudo DEBIAN_FRONTEND=noninteractive apt install -y daemon wget gdebi-core build-essential libfontenc1 libfreetype6 xorg-dev xorg\n",
    "  pro_bar.update(progress(20, 100))\n",
    "  !wget http://security.ubuntu.com/ubuntu/pool/main/libx/libxfont/libxfont1_1.5.1-1ubuntu0.16.04.4_amd64.deb 2>&1\n",
    "  pro_bar.update(progress(30, 100))\n",
    "  !wget --output-document xvfb.deb http://security.ubuntu.com/ubuntu/pool/universe/x/xorg-server/xvfb_1.18.4-0ubuntu0.12_amd64.deb 2>&1\n",
    "  pro_bar.update(progress(40, 100))\n",
    "  !sudo dpkg -i libxfont1_1.5.1-1ubuntu0.16.04.4_amd64.deb 2>&1\n",
    "  pro_bar.update(progress(50, 100))\n",
    "  !sudo dpkg -i xvfb.deb 2>&1\n",
    "  pro_bar.update(progress(70, 100))\n",
    "  !rm libxfont1_1.5.1-1ubuntu0.16.04.4_amd64.deb\n",
    "  pro_bar.update(progress(80, 100))\n",
    "  !rm xvfb.deb\n",
    "  pro_bar.update(progress(90, 100))\n",
    "  !bash frame-buffer start\n",
    "  os.environ[\"DISPLAY\"] = \":1\"\n",
    "pro_bar.update(progress(100, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pzj7wgapAcDs"
   },
   "source": [
    "### Installing ml-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8yfQqkbebQ5",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import mlagents\n",
    "  print(\"ml-agents already installed\")\n",
    "except ImportError:\n",
    "  !python -m pip install -q mlagents==1.1.0\n",
    "  print(\"Installed ml-agents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_u74YhSmW6gD"
   },
   "source": [
    "## Run the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-r_cB2rqp5x"
   },
   "source": [
    "### Import dependencies and set some high level parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSf-WhxbqtLw"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Callable, Any\n",
    "\n",
    "import gym\n",
    "from gym import Env\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecMonitor, VecEnv, SubprocVecEnv\n",
    "from supersuit import observation_lambda_v0\n",
    "\n",
    "\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.envs.unity_gym_env import UnityToGymWrapper\n",
    "from mlagents_envs.registry import UnityEnvRegistry, default_registry\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import (\n",
    "    EngineConfig,\n",
    "    EngineConfigurationChannel,\n",
    ")\n",
    "\n",
    "NUM_ENVS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment  and Engine Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default values from CLI (See cli_utils.py)\n",
    "DEFAULT_ENGINE_CONFIG = EngineConfig(\n",
    "    width=84,\n",
    "    height=84,\n",
    "    quality_level=4,\n",
    "    time_scale=20,\n",
    "    target_frame_rate=-1,\n",
    "    capture_frame_rate=60,\n",
    ")\n",
    "\n",
    "# Some config subset of an actual config.yaml file for MLA.\n",
    "@dataclass\n",
    "class LimitedConfig:\n",
    "    # The local path to a Unity executable or the name of an entry in the registry.\n",
    "    env_path_or_name: str\n",
    "    base_port: int\n",
    "    base_seed: int = 0\n",
    "    num_env: int = 1\n",
    "    engine_config: EngineConfig = DEFAULT_ENGINE_CONFIG\n",
    "    visual_obs: bool = False\n",
    "    # TODO: Decide if we should just tell users to always use MultiInputPolicy so we can simplify the user workflow.\n",
    "    # WARNING: Make sure to use MultiInputPolicy if you turn this on.\n",
    "    allow_multiple_obs: bool = False\n",
    "    env_registry: UnityEnvRegistry = default_registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unity Environment SB3 Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _unity_env_from_path_or_registry(\n",
    "    env: str, registry: UnityEnvRegistry, **kwargs: Any\n",
    ") -> UnityEnvironment:\n",
    "    if Path(env).expanduser().absolute().exists():\n",
    "        return UnityEnvironment(file_name=env, **kwargs)\n",
    "    elif env in registry:\n",
    "        return registry.get(env).make(**kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Environment '{env}' wasn't a local path or registry entry\")\n",
    "        \n",
    "def make_mla_sb3_env(config: LimitedConfig, **kwargs: Any) -> VecEnv:\n",
    "    def handle_obs(obs, space):\n",
    "        if isinstance(space, gym.spaces.Tuple):\n",
    "            if len(space) == 1:\n",
    "                return obs[0]\n",
    "            # Turn the tuple into a dict (stable baselines can handle spaces.Dict but not spaces.Tuple).\n",
    "            return {str(i): v for i, v in enumerate(obs)}\n",
    "        return obs\n",
    "\n",
    "    def handle_obs_space(space):\n",
    "        if isinstance(space, gym.spaces.Tuple):\n",
    "            if len(space) == 1:\n",
    "                return space[0]\n",
    "            # Turn the tuple into a dict (stable baselines can handle spaces.Dict but not spaces.Tuple).\n",
    "            return gym.spaces.Dict({str(i): v for i, v in enumerate(space)})\n",
    "        return space\n",
    "\n",
    "    def create_env(env: str, worker_id: int) -> Callable[[], Env]:\n",
    "        def _f() -> Env:\n",
    "            engine_configuration_channel = EngineConfigurationChannel()\n",
    "            engine_configuration_channel.set_configuration(config.engine_config)\n",
    "            kwargs[\"side_channels\"] = kwargs.get(\"side_channels\", []) + [\n",
    "                engine_configuration_channel\n",
    "            ]\n",
    "            unity_env = _unity_env_from_path_or_registry(\n",
    "                env=env,\n",
    "                registry=config.env_registry,\n",
    "                worker_id=worker_id,\n",
    "                base_port=config.base_port,\n",
    "                seed=config.base_seed + worker_id,\n",
    "                **kwargs,\n",
    "            )\n",
    "            new_env = UnityToGymWrapper(\n",
    "                unity_env=unity_env,\n",
    "                uint8_visual=config.visual_obs,\n",
    "                allow_multiple_obs=config.allow_multiple_obs,\n",
    "            )\n",
    "            new_env = observation_lambda_v0(new_env, handle_obs, handle_obs_space)\n",
    "            return new_env\n",
    "\n",
    "        return _f\n",
    "\n",
    "    env_facts = [\n",
    "        create_env(config.env_path_or_name, worker_id=x) for x in range(config.num_env)\n",
    "    ]\n",
    "    return SubprocVecEnv(env_facts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Environment from the registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# This code is used to close an env that might not have been closed before\n",
    "try:\n",
    "  env.close()\n",
    "except:\n",
    "  pass\n",
    "# -----------------\n",
    "\n",
    "env = make_mla_sb3_env(\n",
    "    config=LimitedConfig(\n",
    "        env_path_or_name='Basic',  # Can use any name from a registry or a path to your own unity build.\n",
    "        base_port=6006,\n",
    "        base_seed=42,\n",
    "        num_env=NUM_ENVS,\n",
    "        allow_multiple_obs=True,\n",
    "    ),\n",
    "    no_graphics=True,  # Set to false if you are running locally and want to watch the environments move around as they train.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 250K should train to a reward ~= 0.90 for the \"Basic\" environment.\n",
    "# We set the value lower here to demonstrate just a small amount of trianing.\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 256\n",
    "UPDATES = 50\n",
    "TOTAL_TAINING_STEPS_GOAL = BUFFER_SIZE * UPDATES\n",
    "BETA = 0.0005\n",
    "N_EPOCHS = 3 \n",
    "STEPS_PER_UPDATE = BUFFER_SIZE / NUM_ENVS\n",
    "\n",
    "# Helps gather stats for our eval() calls later so we can see reward stats.\n",
    "env = VecMonitor(env)\n",
    "\n",
    "#Policy and Value function with 2 layers of 128 units each and no shared layers.\n",
    "policy_kwargs = {\"net_arch\" : [{\"pi\": [32,32], \"vf\": [32,32]}]}\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=lambda progress: 0.0003 * (1.0 - progress),\n",
    "    clip_range=lambda progress: 0.2 * (1.0 - progress),\n",
    "    clip_range_vf=lambda progress: 0.2 * (1.0 - progress),\n",
    "    # Uncomment this if you want to log tensorboard results when running this notebook locally.\n",
    "    # tensorboard_log=\"results\",\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    n_steps=int(STEPS_PER_UPDATE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    ent_coef=BETA,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 0.93 is considered solved for the Basic environment\n",
    "for i in range(UPDATES):\n",
    "    print(f\"Training round {i + 1}/{UPDATES}\")\n",
    "    # NOTE: rest_num_timesteps should only happen the first time so that tensorboard logs are consistent.\n",
    "    model.learn(total_timesteps=BUFFER_SIZE, reset_num_timesteps=(i == 0))\n",
    "    model.policy.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1lIx3_l24OP"
   },
   "source": [
    "### Close the environment\n",
    "Frees up the ports being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vdWG6_SqtNtv",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env.close()\n",
    "print(\"Closed environment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Colab-UnityEnvironment-1-Run.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}


--- com.unity.ml-agents/README.md ---
# com.unity.ml-agents

ML-Agents is a Unity package that allows users to use state-of-the-art machine learning to create intelligent character behaviors in any Unity environment (games, robotics, film, etc.).

Please refer to the [Unity Package Documentation](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest) for comprehensive installation, usage guides, tutorials, and API references.



## Links discovered
- [Unity Package Documentation](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest)

--- com.unity.ml-agents/Documentation~/index.md ---
# ML-Agents Overview

<img src="images/image-banner.png" align="middle" width="3000"/>

ML-Agents enable games and simulations to serve as environments for training intelligent agents in Unity. Training can be done with reinforcement learning, imitation learning, neuroevolution, or any other methods. Trained agents can be used for many use cases, including controlling NPC behavior (in a variety of settings such as multi-agent and adversarial), automated testing of game builds and evaluating different game design decisions pre-release.

This documentation contains comprehensive instructions about [Unity ML-Agents Toolkit](https://github.com/Unity-Technologies/ml-agents) including the C# package, along with detailed training guides and Python API references. Note that the C# package does not contain the machine learning algorithms for training behaviors. The C# package only supports instrumenting a Unity scene, setting it up for training, and then embedding the trained model back into your Unity scene. The machine learning algorithms that orchestrate training are part of the companion Python package.

## Documentation structure

| **Section**                                                         | **Description**                                                                                                                                                      |
|---------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [ML-Agents Theory](ML-Agents-Overview.md)                           | Learn about core concepts of ML-Agents.                                                                                                                              |
| [Get started](Get-Started.md)                                       | Learn how to install ML-Agents and explore samples.                                                                                                                  |
| [Learning Environments and Agents](Learning-Environments-Agents.md) | Learn about Environments, Agents, creating environments, and using executable builds.                                                                                |
| [Training](Training.md)                                             | Training workflow, config file, monitoring tools, custom plugins, and profiling.                                                                                     |
| [Python APIs](Python-APIs.md)                                       | Gym, PettingZoo, low-level interfaces, and trainer documentation.                                                                                                    |
| [Python Tutorial with Google Colab](Tutorial-Colab.md)              | Interactive tutorials for using ML-Agents with Google Colab.                                                                                                         |
| [Advanced Features](Advanced-Features.md)                           | Custom sensors, side channels, package settings, environment registry, input system integrations, and game integrations (e.g., [Match-3](Integrations-Match3.md)).   |
| [Cloud & Deployment](Cloud-Deployment.md)                           | Legacy cloud deployment guides (deprecated).                                                                                                                         |
| [Reference & Support](Reference-Support.md)                         | FAQ, troubleshooting, and migration guides.                                                                                                                          |
| [Background](Background.md)                                         | Machine Learning, Unity, PyTorch fundamentals, virtual environments, and ELO rating systems.                                                                         |

## Capabilities
The package allows you to convert any Unity scene into a learning environment and train character behaviors using a variety of machine-learning algorithms. Additionally, it allows you to embed these trained behaviors back into Unity scenes to control your characters. More specifically, the package provides the following core functionalities:

* Define Agents: entities, or characters, whose behavior will be learned. Agents are entities that generate observations (through sensors), take actions, and receive rewards from the environment.
* Define Behaviors: entities that specify how an agent should act. Multiple agents can share the same Behavior and a scene may have multiple Behaviors.
* Record demonstrations: To show the behaviors of an agent within the Editor. You can use demonstrations to help train a behavior for that agent.
* Embed a trained behavior (aka: run your ML model) in the scene via the [Inference Engine](https://docs.unity3d.com/Packages/com.unity.ai.inference@latest). Embedded behaviors allow you to switch an Agent between learning and inference.

## Community and Feedback

The ML-Agents Toolkit is an open-source project, and we encourage and welcome contributions. If you wish to contribute, be sure to review our [contribution guidelines](CONTRIBUTING.md) and [code of conduct](https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/CODE_OF_CONDUCT.md).

For problems with the installation and setup of the ML-Agents Toolkit, or discussions about how to best setup or train your agents, please create a new thread on the [Unity ML-Agents discussion forum](https://discussions.unity.com/tag/ml-agents). Be sure to include as many details as possible to help others assist you effectively. If you run into any other problems using the ML-Agents Toolkit or have a specific feature request, please [submit a GitHub issue](https://github.com/Unity-Technologies/ml-agents/issues).

Please tell us which samples you would like to see shipped with the ML-Agents Unity package by replying to [this discussion thread](https://discussions.unity.com/t/help-shape-the-future-of-ml-agents/1661019).

## Privacy
In order to improve the developer experience for Unity ML-Agents Toolkit, we have added in-editor analytics. Please refer to "Information that is passively collected by Unity" in the [Unity Privacy Policy](https://unity3d.com/legal/privacy-policy).

## Additional Resources

* [GitHub repository](https://github.com/Unity-Technologies/ml-agents)
* [Unity Discussions](https://discussions.unity.com/tag/ml-agents)
* [ML-Agents tutorials by CodeMonkeyUnity](https://www.youtube.com/playlist?list=PLzDRvYVwl53vehwiN_odYJkPBzcqFw110)
* [Introduction to ML-Agents by Huggingface](https://huggingface.co/learn/deep-rl-course/en/unit5/introduction)
* [Community created ML-Agents projects](https://discussions.unity.com/t/post-your-ml-agents-project/816756)
* [ML-Agents models on Huggingface](https://huggingface.co/models?library=ml-agents)
* [Blog posts](Blog-posts.md)
* [Discord](https://discord.com/channels/489222168727519232/1202574086115557446)


## Links discovered
- [Unity ML-Agents Toolkit](https://github.com/Unity-Technologies/ml-agents)
- [ML-Agents Theory](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/ML-Agents-Overview.md)
- [Get started](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Get-Started.md)
- [Learning Environments and Agents](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Learning-Environments-Agents.md)
- [Training](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Training.md)
- [Python APIs](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Python-APIs.md)
- [Python Tutorial with Google Colab](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Tutorial-Colab.md)
- [Advanced Features](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Advanced-Features.md)
- [Match-3](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Integrations-Match3.md)
- [Cloud & Deployment](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Cloud-Deployment.md)
- [Reference & Support](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Reference-Support.md)
- [Background](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Background.md)
- [Inference Engine](https://docs.unity3d.com/Packages/com.unity.ai.inference@latest)
- [contribution guidelines](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/CONTRIBUTING.md)
- [code of conduct](https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/CODE_OF_CONDUCT.md)
- [Unity ML-Agents discussion forum](https://discussions.unity.com/tag/ml-agents)
- [submit a GitHub issue](https://github.com/Unity-Technologies/ml-agents/issues)
- [this discussion thread](https://discussions.unity.com/t/help-shape-the-future-of-ml-agents/1661019)
- [Unity Privacy Policy](https://unity3d.com/legal/privacy-policy)
- [GitHub repository](https://github.com/Unity-Technologies/ml-agents)
- [Unity Discussions](https://discussions.unity.com/tag/ml-agents)
- [ML-Agents tutorials by CodeMonkeyUnity](https://www.youtube.com/playlist?list=PLzDRvYVwl53vehwiN_odYJkPBzcqFw110)
- [Introduction to ML-Agents by Huggingface](https://huggingface.co/learn/deep-rl-course/en/unit5/introduction)
- [Community created ML-Agents projects](https://discussions.unity.com/t/post-your-ml-agents-project/816756)
- [ML-Agents models on Huggingface](https://huggingface.co/models?library=ml-agents)
- [Blog posts](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Blog-posts.md)
- [Discord](https://discord.com/channels/489222168727519232/1202574086115557446)

--- com.unity.ml-agents/Documentation~/Advanced-Features.md ---
# Advanced Features

The ML-Agents Toolkit provides several advanced features that extend the core functionality and enable sophisticated use cases.


| **Feature**                                                 | **Description**                                                              |
|-------------------------------------------------------------|------------------------------------------------------------------------------|
| [Custom Side Channels](Custom-SideChannels.md)              | Create custom communication channels between Unity and Python.               |
| [Custom Grid Sensors](Custom-GridSensors.md)                | Build specialized grid-based sensors for spatial data.                       |
| [Input System Integration](InputSystem-Integration.md)      | Integrate ML-Agents with Unity's Input System.                               |
| [Inference Engine](Inference-Engine.md)                     | Deploy trained models for real-time inference.                               |
| [Hugging Face Integration](Hugging-Face-Integration.md)     | Connect with Hugging Face models and ecosystem.                              |
| [Game Integrations](Integrations.md)                        | Integrate ML-Agents with specific game genres and mechanics (e.g., Match-3). |
| [Match-3 Integration](Integrations-Match3.md)               | Abstraction and tools for Match-3 board games (board, sensors, actuators).   |
| [ML-Agents Package Settings](Package-Settings.md)           | Configure advanced package settings and preferences.                         |
| [Unity Environment Registry](Unity-Environment-Registry.md) | Manage and register Unity environments programmatically.                     |


## Links discovered
- [Custom Side Channels](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Custom-SideChannels.md)
- [Custom Grid Sensors](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Custom-GridSensors.md)
- [Input System Integration](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/InputSystem-Integration.md)
- [Inference Engine](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Inference-Engine.md)
- [Hugging Face Integration](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Hugging-Face-Integration.md)
- [Game Integrations](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Integrations.md)
- [Match-3 Integration](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Integrations-Match3.md)
- [ML-Agents Package Settings](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Package-Settings.md)
- [Unity Environment Registry](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Unity-Environment-Registry.md)

--- com.unity.ml-agents/Documentation~/Background.md ---
# Background

This section provides foundational knowledge to help you understand the technologies and concepts that power the ML-Agents Toolkit.

| **Topic**                                                 | **Description**                                                               |
|-----------------------------------------------------------|-------------------------------------------------------------------------------|
| [Machine Learning](Background-Machine-Learning.md)        | Introduction to ML concepts, reinforcement learning, and training principles. |
| [Unity](Background-Unity.md)                              | Unity fundamentals for ML-Agents development and environment creation.        |
| [PyTorch](Background-PyTorch.md)                          | PyTorch basics for understanding the training pipeline and neural networks.   |
| [Using Virtual Environment](Using-Virtual-Environment.md) | Setting up and managing Python virtual environments for ML-Agents.            |
| [ELO Rating System](ELO-Rating-System.md)                 | Understanding ELO rating system for multi-agent training evaluation.          |


## Links discovered
- [Machine Learning](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Background-Machine-Learning.md)
- [Unity](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Background-Unity.md)
- [PyTorch](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Background-PyTorch.md)
- [Using Virtual Environment](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Using-Virtual-Environment.md)
- [ELO Rating System](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/ELO-Rating-System.md)

--- com.unity.ml-agents/Documentation~/Background-Machine-Learning.md ---
# Background: Machine Learning

Given that a number of users of the ML-Agents Toolkit might not have a formal machine learning background, this page provides an overview to facilitate the understanding of the ML-Agents Toolkit. However, we will not attempt to provide a thorough treatment of machine learning as there are fantastic resources online.

Machine learning, a branch of artificial intelligence, focuses on learning patterns from data. The three main classes of machine learning algorithms include: unsupervised learning, supervised learning and reinforcement learning. Each class of algorithm learns from a different type of data. The following paragraphs provide an overview for each of these classes of machine learning, as well as introductory examples.

## Unsupervised Learning

The goal of [unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning) is to group or cluster similar items in a data set. For example, consider the players of a game. We may want to group the players depending on how engaged they are with the game. This would enable us to target different groups (e.g. for highly-engaged players we might invite them to be beta testers for new features, while for unengaged players we might email them helpful tutorials). Say that we wish to split our players into two groups. We would first define basic attributes of the players, such as the number of hours played, total money spent on in-app purchases and number of levels completed. We can then feed this data set (three attributes for every player) to an unsupervised learning algorithm where we specify the number of groups to be two. The algorithm would then split the data set of players into two groups where the players within each group would be similar to each other. Given the attributes we used to describe each player, in this case, the output would be a split of all the players into two groups, where one group would semantically represent the engaged players and the second group would semantically represent the unengaged players.

With unsupervised learning, we did not provide specific examples of which players are considered engaged and which are considered unengaged. We just defined the appropriate attributes and relied on the algorithm to uncover the two groups on its own. This type of data set is typically called an unlabeled data set as it is lacking these direct labels. Consequently, unsupervised learning can be helpful in situations where these labels can be expensive or hard to produce. In the next paragraph, we overview supervised learning algorithms which accept input labels in addition to attributes.

## Supervised Learning

In [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning), we do not want to just group similar items but directly learn a mapping from each item to the group (or class) that it belongs to. Returning to our earlier example of clustering players, let's say we now wish to predict which of our players are about to churn (that is stop playing the game for the next 30 days). We can look into our historical records and create a data set that contains attributes of our players in addition to a label indicating whether they have churned or not. Note that the player attributes we use for this churn prediction task may be different from the ones we used for our earlier clustering task. We can then feed this data set (attributes **and** label for each player) into a supervised learning algorithm which would learn a mapping from the player attributes to a label indicating whether that player will churn or not. The intuition is that the supervised learning algorithm will learn which values of these attributes typically correspond to players who have churned and not churned (for example, it may learn that players who spend very little and play for very short periods will most likely churn). Now given this learned model, we can provide it the attributes of a new player (one that recently started playing the game) and it would output a _predicted_ label for that player. This prediction is the algorithms expectation of whether the player will churn or not. We can now use these predictions to target the players who are expected to churn and entice them to continue playing the game.

As you may have noticed, for both supervised and unsupervised learning, there are two tasks that need to be performed: attribute selection and model selection. Attribute selection (also called feature selection) pertains to selecting how we wish to represent the entity of interest, in this case, the player. Model selection, on the other hand, pertains to selecting the algorithm (and its parameters) that perform the task well. Both of these tasks are active areas of machine learning research and, in practice, require several iterations to achieve good performance.

We now switch to reinforcement learning, the third class of machine learning algorithms, and arguably the one most relevant for the ML-Agents Toolkit.

## Reinforcement Learning

[Reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning) can be viewed as a form of learning for sequential decision making that is commonly associated with controlling robots (but is, in fact, much more general). Consider an autonomous firefighting robot that is tasked with navigating into an area, finding the fire and neutralizing it. At any given moment, the robot perceives the environment through its sensors (e.g. camera, heat, touch), processes this information and produces an action (e.g. move to the left, rotate the water hose, turn on the water). In other words, it is continuously making decisions about how to interact in this environment given its view of the world (i.e. sensors input) and objective (i.e. neutralizing the fire). Teaching a robot to be a successful firefighting machine is precisely what reinforcement learning is designed to do.

More specifically, the goal of reinforcement learning is to learn a **policy**, which is essentially a mapping from **observations** to **actions**. An observation is what the robot can measure from its **environment** (in this case, all its sensory inputs) and an action, in its most raw form, is a change to the configuration of the robot (e.g. position of its base, position of its water hose and whether the hose is on or off).

The last remaining piece of the reinforcement learning task is the **reward signal**. The robot is trained to learn a policy that maximizes its overall rewards. When training a robot to be a mean firefighting machine, we provide it with rewards (positive and negative) indicating how well it is doing on completing the task. Note that the robot does not _know_ how to put out fires before it is trained. It learns the objective because it receives a large positive reward when it puts out the fire and a small negative reward for every passing second. The fact that rewards are sparse (i.e. may not be provided at every step, but only when a robot arrives at a success or failure situation), is a defining characteristic of reinforcement learning and precisely why learning good policies can be difficult (and/or time-consuming) for complex environments.

<div style="text-align: center"><img src="images/rl_cycle.png" alt="The reinforcement learning lifecycle."></div>

Learning a policy usually requires many trials and iterative policy updates. More specifically, the robot is placed in several fire situations and over time learns an optimal policy which allows it to put out fires more effectively. Obviously, we cannot expect to train a robot repeatedly in the real world, particularly when fires are involved. This is precisely why the use of Unity as a simulator serves as the perfect training grounds for learning such behaviors. While our discussion of reinforcement learning has centered around robots, there are strong parallels between robots and characters in a game. In fact, in many ways, one can view a non-playable character (NPC) as a virtual robot, with its own observations about the environment, its own set of actions and a specific objective. Thus it is natural to explore how we can train behaviors within Unity using reinforcement learning. This is precisely what the ML-Agents Toolkit offers. The video linked below includes a reinforcement learning demo showcasing training character behaviors using the ML-Agents Toolkit.

<p align="center"> <a href="http://www.youtube.com/watch?feature=player_embedded&v=fiQsmdwEGT8" target="_blank"> <img src="http://img.youtube.com/vi/fiQsmdwEGT8/0.jpg" alt="RL Demo" width="400" border="10" /> </a> </p>

Similar to both unsupervised and supervised learning, reinforcement learning also involves two tasks: attribute selection and model selection. Attribute selection is defining the set of observations for the robot that best help it complete its objective, while model selection is defining the form of the policy (mapping from observations to actions) and its parameters. In practice, training behaviors is an iterative process that may require changing the attribute and model choices.

## Training and Inference

One common aspect of all three branches of machine learning is that they all involve a **training phase** and an **inference phase**. While the details of the training and inference phases are different for each of the three, at a high-level, the training phase involves building a model using the provided data, while the inference phase involves applying this model to new, previously unseen, data. More specifically:

- For our unsupervised learning example, the training phase learns the optimal two clusters based on the data describing existing players, while the inference phase assigns a new player to one of these two clusters.
- For our supervised learning example, the training phase learns the mapping from player attributes to player label (whether they churned or not), and the inference phase predicts whether a new player will churn or not based on that learned mapping.
- For our reinforcement learning example, the training phase learns the optimal policy through guided trials, and in the inference phase, the agent observes and takes actions in the wild using its learned policy.

To briefly summarize: all three classes of algorithms involve training and inference phases in addition to attribute and model selections. What ultimately separates them is the type of data available to learn from. In unsupervised learning our data set was a collection of attributes, in supervised learning our data set was a collection of attribute-label pairs, and, lastly, in reinforcement learning our data set was a collection of observation-action-reward tuples.

## Deep Learning

[Deep learning](https://en.wikipedia.org/wiki/Deep_learning) is a family of algorithms that can be used to address any of the problems introduced above. More specifically, they can be used to solve both attribute and model selection tasks. Deep learning has gained popularity in recent years due to its outstanding performance on several challenging machine learning tasks. One example is [AlphaGo](https://en.wikipedia.org/wiki/AlphaGo), a [computer Go](https://en.wikipedia.org/wiki/Computer_Go) program, that leverages deep learning, that was able to beat Lee Sedol (a Go world champion).

A key characteristic of deep learning algorithms is their ability to learn very complex functions from large amounts of training data. This makes them a natural choice for reinforcement learning tasks when a large amount of data can be generated, say through the use of a simulator or engine such as Unity. By generating hundreds of thousands of simulations of the environment within Unity, we can learn policies for very complex environments (a complex environment is one where the number of observations an agent perceives and the number of actions they can take are large). Many of the algorithms we provide in ML-Agents use some form of deep learning, built on top of the open-source library, [PyTorch](Background-PyTorch.md).


## Links discovered
- [unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning)
- [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning)
- [Reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning)
- [Deep learning](https://en.wikipedia.org/wiki/Deep_learning)
- [AlphaGo](https://en.wikipedia.org/wiki/AlphaGo)
- [computer Go](https://en.wikipedia.org/wiki/Computer_Go)
- [PyTorch](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Background-PyTorch.md)
- [<img src="http://img.youtube.com/vi/fiQsmdwEGT8/0.jpg" alt="RL Demo" width="400" border="10" />](http://www.youtube.com/watch?feature=player_embedded&v=fiQsmdwEGT8)

--- com.unity.ml-agents/Documentation~/Background-PyTorch.md ---
# Background: PyTorch

As discussed in our [machine learning background page](Background-Machine-Learning.md), many of the algorithms we provide in the ML-Agents Toolkit leverage some form of deep learning. More specifically, our implementations are built on top of the open-source library [PyTorch](https://pytorch.org/). In this page we provide a brief overview of PyTorch and TensorBoard that we leverage within the ML-Agents Toolkit.

## PyTorch

[PyTorch](https://pytorch.org/) is an open source library for performing computations using data flow graphs, the underlying representation of deep learning models. It facilitates training and inference on CPUs and GPUs in a desktop, server, or mobile device. Within the ML-Agents Toolkit, when you train the behavior of an agent, the output is a model (.onnx) file that you can then associate with an Agent. Unless you implement a new algorithm, the use of PyTorch is mostly abstracted away and behind the scenes.

## TensorBoard

One component of training models with PyTorch is setting the values of certain model attributes (called _hyperparameters_). Finding the right values of these hyperparameters can require a few iterations. Consequently, we leverage a visualization tool called [TensorBoard](https://www.tensorflow.org/tensorboard). It allows the visualization of certain agent attributes (e.g. reward) throughout training which can be helpful in both building intuitions for the different hyperparameters and setting the optimal values for your Unity environment. We provide more details on setting the hyperparameters in the [Training ML-Agents](Training-ML-Agents.md) page. If you are unfamiliar with TensorBoard we recommend our guide on [using TensorBoard with ML-Agents](Using-Tensorboard.md) or this [tutorial](https://github.com/dandelionmane/tf-dev-summit-tensorboard-tutorial).

## Links discovered
- [machine learning background page](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Background-Machine-Learning.md)
- [PyTorch](https://pytorch.org/)
- [TensorBoard](https://www.tensorflow.org/tensorboard)
- [Training ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Training-ML-Agents.md)
- [using TensorBoard with ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Using-Tensorboard.md)
- [tutorial](https://github.com/dandelionmane/tf-dev-summit-tensorboard-tutorial)

--- com.unity.ml-agents/Documentation~/Background-Unity.md ---
# Background: Unity

If you are not familiar with the [Unity Engine](https://unity3d.com/unity), we highly recommend the [Unity Manual](https://docs.unity3d.com/Manual/index.html) and [Tutorials page](https://unity3d.com/learn/tutorials). The [Roll-a-ball tutorial](https://learn.unity.com/project/roll-a-ball) is a fantastic resource to learn all the basic concepts of Unity to get started with the ML-Agents Toolkit:

- [Editor](https://docs.unity3d.com/Manual/sprite/sprite-editor/use-editor.html)
- [Scene](https://docs.unity3d.com/Manual/CreatingScenes.html)
- [GameObject](https://docs.unity3d.com/Manual/GameObjects.html)
- [Rigidbody](https://docs.unity3d.com/ScriptReference/Rigidbody.html)
- [Camera](https://docs.unity3d.com/Manual/Cameras.html)
- [Scripting](https://docs.unity3d.com/Manual/ScriptingSection.html)
- [Physics](https://docs.unity3d.com/Manual/PhysicsSection.html)
- [Ordering of event functions](https://docs.unity3d.com/Manual/ExecutionOrder.html)
(e.g. FixedUpdate, Update)
- [Prefabs](https://docs.unity3d.com/Manual/Prefabs.html)


## Links discovered
- [Unity Engine](https://unity3d.com/unity)
- [Unity Manual](https://docs.unity3d.com/Manual/index.html)
- [Tutorials page](https://unity3d.com/learn/tutorials)
- [Roll-a-ball tutorial](https://learn.unity.com/project/roll-a-ball)
- [Editor](https://docs.unity3d.com/Manual/sprite/sprite-editor/use-editor.html)
- [Scene](https://docs.unity3d.com/Manual/CreatingScenes.html)
- [GameObject](https://docs.unity3d.com/Manual/GameObjects.html)
- [Rigidbody](https://docs.unity3d.com/ScriptReference/Rigidbody.html)
- [Camera](https://docs.unity3d.com/Manual/Cameras.html)
- [Scripting](https://docs.unity3d.com/Manual/ScriptingSection.html)
- [Physics](https://docs.unity3d.com/Manual/PhysicsSection.html)
- [Ordering of event functions](https://docs.unity3d.com/Manual/ExecutionOrder.html)
- [Prefabs](https://docs.unity3d.com/Manual/Prefabs.html)

--- com.unity.ml-agents/Documentation~/Blog-posts.md ---
We have published a series of blog posts that are relevant for ML-Agents:

- (July 12, 2021)
  [ML-Agents plays Dodgeball](https://blog.unity.com/technology/ml-agents-plays-dodgeball)
- (May 5, 2021)
  [ML-Agents v2.0 release: Now supports training complex cooperative behaviors](https://blogs.unity3d.com/2021/05/05/ml-agents-v2-0-release-now-supports-training-complex-cooperative-behaviors/)
- (November 20, 2020)
  [How Eidos-Montréal created Grid Sensors to improve observations for training agents](https://www.eidosmontreal.com/news/the-grid-sensor-for-automated-game-testing/)
- (February 28, 2020)
  [Training intelligent adversaries using self-play with ML-Agents](https://blogs.unity3d.com/2020/02/28/training-intelligent-adversaries-using-self-play-with-ml-agents/)
- (November 11, 2019)
  [Training your agents 7 times faster with ML-Agents](https://blogs.unity3d.com/2019/11/11/training-your-agents-7-times-faster-with-ml-agents/)
- (June 26, 2018)
  [Solving sparse-reward tasks with Curiosity](https://blogs.unity3d.com/2018/06/26/solving-sparse-reward-tasks-with-curiosity/)
- (June 19, 2018)
  [Unity ML-Agents Toolkit v0.4 and Udacity Deep Reinforcement Learning Nanodegree](https://github.com/udacity/deep-reinforcement-learning)
- (September 19, 2017)
  [Introducing: Unity Machine Learning Agents Toolkit](https://blogs.unity3d.com/2017/09/19/introducing-unity-machine-learning-agents/)


## Links discovered
- [ML-Agents plays Dodgeball](https://blog.unity.com/technology/ml-agents-plays-dodgeball)
- [ML-Agents v2.0 release: Now supports training complex cooperative behaviors](https://blogs.unity3d.com/2021/05/05/ml-agents-v2-0-release-now-supports-training-complex-cooperative-behaviors/)
- [How Eidos-Montréal created Grid Sensors to improve observations for training agents](https://www.eidosmontreal.com/news/the-grid-sensor-for-automated-game-testing/)
- [Training intelligent adversaries using self-play with ML-Agents](https://blogs.unity3d.com/2020/02/28/training-intelligent-adversaries-using-self-play-with-ml-agents/)
- [Training your agents 7 times faster with ML-Agents](https://blogs.unity3d.com/2019/11/11/training-your-agents-7-times-faster-with-ml-agents/)
- [Solving sparse-reward tasks with Curiosity](https://blogs.unity3d.com/2018/06/26/solving-sparse-reward-tasks-with-curiosity/)
- [Unity ML-Agents Toolkit v0.4 and Udacity Deep Reinforcement Learning Nanodegree](https://github.com/udacity/deep-reinforcement-learning)
- [Introducing: Unity Machine Learning Agents Toolkit](https://blogs.unity3d.com/2017/09/19/introducing-unity-machine-learning-agents/)

--- com.unity.ml-agents/Documentation~/Cloud-Deployment.md ---
# Cloud & Deployment (deprecated)


This section contains legacy documentation for deploying ML-Agents training in cloud environments. While these approaches may still work, they are no longer actively maintained or recommended.


| **Platform**                                             | **Description**                                      |
|----------------------------------------------------------|------------------------------------------------------|
| [Using Docker](Using-Docker.md)                          | Containerized deployment with Docker (deprecated).   |
| [Amazon Web Services](Training-on-Amazon-Web-Service.md) | Training on AWS cloud infrastructure (deprecated).   |
| [Microsoft Azure](Training-on-Microsoft-Azure.md)        | Training on Azure cloud services (deprecated).       |



## Links discovered
- [Using Docker](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Using-Docker.md)
- [Amazon Web Services](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Training-on-Amazon-Web-Service.md)
- [Microsoft Azure](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Training-on-Microsoft-Azure.md)

--- com.unity.ml-agents/Documentation~/Custom-GridSensors.md ---
# Custom Grid Sensors

Grid Sensor provides a 2D observation that detects objects around an agent from a top-down view. Compared to RayCasts, it receives a full observation in a grid area without gaps, and the detection is not blocked by objects around the agents. This gives a more granular view while requiring a higher usage of compute resources.

One extra feature with Grid Sensors is that you can derive from the Grid Sensor base class to collect custom data besides the object tags, to include custom attributes as observations. This allows more flexibility for the use of GridSensor.

## Creating Custom Grid Sensors
To create a custom grid sensor, you'll need to derive from two classes: `GridSensorBase` and `GridSensorComponent`.

## Deriving from `GridSensorBase`
This is the implementation of your sensor. This defines how your sensor process detected colliders, what the data looks like, and how the observations are constructed from the detected objects. Consider overriding the following methods depending on your use case:
* `protected virtual int GetCellObservationSize()`: Return the observation size per cell. Default to `1`.
* `protected virtual void GetObjectData(GameObject detectedObject, int tagIndex, float[] dataBuffer)`: Constructs observations from the detected object. The input provides the detected GameObject and the index of its tag (0-indexed). The observations should be written to the given `dataBuffer` and the buffer size is defined in `GetCellObservationSize()`. This data will be gathered from each cell and sent to the trainer as observation.
* `protected virtual bool IsDataNormalized()`: Return whether the observation is normalized to 0~1. This affects whether you're able to use compressed observations as compressed data only supports normalized data. Return `true` if all the values written in `GetObjectData` are within the range of (0, 1), otherwise return `false`. Default to `false`.

There might be cases when your data is not in the range of (0, 1) but you still wish to use compressed data to speed up training. If your data is naturally bounded within a range, normalize your data first to the possible range and fill the buffer with normalized data. For example, since the angle of rotation is bounded within `0 ~ 360`, record an angle `x` as `x/360` instead of `x`. If your data value is not bounded (position, velocity, etc.), consider setting a reasonable min/max value and use that to normalize your data.
* `protected internal virtual ProcessCollidersMethod GetProcessCollidersMethod()`: Return the method to process colliders detected in a cell. This defines the sensor behavior when multiple objects with detectable tags are detected within a cell.
Currently, two methods are provided:
  * `ProcessCollidersMethod.ProcessClosestColliders` (Default): Process the closest collider to the agent. In this case each cell's data is represented by one object.
  * `ProcessCollidersMethod.ProcessAllColliders`: Process all detected colliders. This is useful when the data from each cell is additive, for instance, the count of detected objects in a cell. When using this option, the input `dataBuffer` in `GetObjectData()` will contain processed data from other colliders detected in the cell. You'll more likely want to add/subtract values from the buffer instead of overwrite it completely.

## Deriving from `GridSensorComponent`
To create your sensor, you need to override the sensor component and add your sensor to the creation. Specifically, you need to override `GetGridSensors()` and return an array of grid sensors you want to use in the component. It can be used to create multiple different customized grid sensors, or you can also include the ones provided in our package (listed in the next section).

Example:
```csharp
public class CustomGridSensorComponent : GridSensorComponent
{
    protected override GridSensorBase[] GetGridSensors()
    {
        return new GridSensorBase[] { new CustomGridSensor(...)};
    }
}
```

## Grid Sensor Types
Here we list two types of grid sensor provided in the package: `OneHotGridSensor` and `CountingGridSensor`. Their implementations are also a good reference for making you own ones.

### OneHotGridSensor
This is the default sensor used by `GridSensorComponent`. It detects objects with detectable tags and the observation is the one-hot representation of the detected tag index.

The implementation of the sensor is defined as following:
* `GetCellObservationSize()`: `detectableTags.Length`
* `IsDataNormalized()`: `true`
* `ProcessCollidersMethod()`: `ProcessCollidersMethod.ProcessClosestColliders`
* `GetObjectData()`:

```csharp
protected override void GetObjectData(GameObject detectedObject, int tagIndex, float[] dataBuffer)
{
    dataBuffer[tagIndex] = 1;
}
```

### CountingGridSensor
This is an example of using all colliders detected in a cell. It counts the number of objects detected for each detectable tag. The sensor cannot be used with data compression.

The implementation of the sensor is defined as following:
* `GetCellObservationSize()`: `detectableTags.Length`
* `IsDataNormalized()`: `false`
* `ProcessCollidersMethod()`: `ProcessCollidersMethod.ProcessAllColliders`
* `GetObjectData()`:

```csharp
protected override void GetObjectData(GameObject detectedObject, int tagIndex, float[] dataBuffer)
{
    dataBuffer[tagIndex] += 1;
}
```


--- DevProject/Assets/placeholder.txt ---
lorem ipsum

--- DevProject/ProjectSettings/ProjectVersion.txt ---
m_EditorVersion: 6000.0.40f1
m_EditorVersionWithRevision: 6000.0.40f1 (157d81624ddf)


--- ml-agents/README.md ---
# Unity ML-Agents Trainers

The `mlagents` Python package is part of the
[ML-Agents Toolkit](https://github.com/Unity-Technologies/ml-agents). `mlagents`
provides a set of reinforcement and imitation learning algorithms designed to be
used with Unity environments. The algorithms interface with the Python API
provided by the `mlagents_envs` package. See [here](../com.unity.ml-agents/Documentation~/Python-LLAPI.md) for
more information on `mlagents_envs`.

The algorithms can be accessed using the: `mlagents-learn` access point. See
[here](../com.unity.ml-agents/Documentation~/Training-ML-Agents.md) for more information on using this
package.

## Installation

Install the `mlagents` package with:

```sh
python -m pip install mlagents==1.1.0
```

## Usage & More Information

For more information on the ML-Agents Toolkit and how to instrument a Unity
scene with the ML-Agents SDK, check out the main
[ML-Agents Toolkit documentation](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest).

## Limitations

- Resuming self-play from a checkpoint resets the reported ELO to the default
  value.


## Links discovered
- [ML-Agents Toolkit](https://github.com/Unity-Technologies/ml-agents)
- [here](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Python-LLAPI.md)
- [here](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Training-ML-Agents.md)
- [ML-Agents Toolkit documentation](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest)

--- ml-agents/setup.py ---
import os
import sys

from setuptools import setup, find_packages
from setuptools.command.install import install
from mlagents.plugins import ML_AGENTS_STATS_WRITER, ML_AGENTS_TRAINER_TYPE
import mlagents.trainers

VERSION = mlagents.trainers.__version__
EXPECTED_TAG = mlagents.trainers.__release_tag__

here = os.path.abspath(os.path.dirname(__file__))


class VerifyVersionCommand(install):
    """
    Custom command to verify that the git tag is the expected one for the release.
    Originally based on https://circleci.com/blog/continuously-deploying-python-packages-to-pypi-with-circleci/
    This differs slightly because our tags and versions are different.
    """

    description = "verify that the git tag matches our version"

    def run(self):
        tag = os.getenv("GITHUB_REF", "NO GITHUB TAG!").replace("refs/tags/", "")

        if tag != EXPECTED_TAG:
            info = "Git tag: {} does not match the expected tag of this app: {}".format(
                tag, EXPECTED_TAG
            )
            sys.exit(info)


# Get the long description from the README file
with open(os.path.join(here, "README.md"), encoding="utf-8") as f:
    long_description = f.read()

setup(
    name="mlagents",
    version=VERSION,
    description="Unity Machine Learning Agents",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/Unity-Technologies/ml-agents",
    author="Unity Technologies",
    author_email="ML-Agents@unity3d.com",
    classifiers=[
        "Intended Audience :: Developers",
        "Topic :: Scientific/Engineering :: Artificial Intelligence",
        "License :: OSI Approved :: Apache Software License",
        "Programming Language :: Python :: 3.10",
    ],
    # find_namespace_packages will recurse through the directories and find all the packages
    packages=find_packages(exclude=["*.tests", "*.tests.*", "tests.*", "tests"]),
    zip_safe=False,
    install_requires=[
        # Test-only dependencies should go in test_requirements.txt, not here.
        "grpcio>=1.11.0,<=1.53.2",
        "h5py>=2.9.0",
        f"mlagents_envs=={VERSION}",
        "numpy>=1.23.5,<1.24.0",
        "Pillow>=4.2.1",
        "protobuf>=3.6,<3.21",
        "pyyaml>=3.1.0",
        "torch>=2.1.1,<=2.8.0",
        "tensorboard>=2.14",
        # adding six explicit dependency since tensorboard needs it but doesn't declare it as a dep
        "six>=1.16",
        # cattrs 1.1.0 dropped support for python 3.6, but 1.0.0 doesn't work for python 3.9
        # Since there's no version that supports both, we have to draw the line somewhere.
        "cattrs>=1.1.0,<1.7; python_version>='3.8'",
        "attrs>=19.3.0",
        "huggingface_hub>=0.14",
        'pypiwin32==223;platform_system=="Windows"',
        "onnx==1.15.0",
    ],
    python_requires=">=3.10.1,<=3.10.12",
    entry_points={
        "console_scripts": [
            "mlagents-learn=mlagents.trainers.learn:main",
            "mlagents-run-experiment=mlagents.trainers.run_experiment:main",
            "mlagents-push-to-hf=mlagents.utils.push_to_hf:main",
            "mlagents-load-from-hf=mlagents.utils.load_from_hf:main",
        ],
        # Plugins - each plugin type should have an entry here for the default behavior
        ML_AGENTS_STATS_WRITER: [
            "default=mlagents.plugins.stats_writer:get_default_stats_writers"
        ],
        ML_AGENTS_TRAINER_TYPE: [
            "default=mlagents.plugins.trainer_type:get_default_trainer_types"
        ],
    },
    # TODO: Remove this once mypy stops having spurious setuptools issues.
    cmdclass={"verify": VerifyVersionCommand},  # type: ignore
)


--- ml-agents/mlagents/__init__.py ---


--- ml-agents/tests/__init__.py ---


--- ml-agents/mlagents/trainers/action_info.py ---
from typing import NamedTuple, Any, Dict, List
import numpy as np
from mlagents_envs.base_env import AgentId

ActionInfoOutputs = Dict[str, np.ndarray]


class ActionInfo(NamedTuple):
    """
    A NamedTuple containing actions and related quantities to the policy forward
    pass. Additionally contains the agent ids in the corresponding DecisionStep
    :param action: The action output of the policy
    :param env_action: The possibly clipped action to be executed in the environment
    :param outputs: Dict of all quantities associated with the policy forward pass
    :param agent_ids: List of int agent ids in DecisionStep
    """

    action: Any
    env_action: Any
    outputs: ActionInfoOutputs
    agent_ids: List[AgentId]

    @staticmethod
    def empty() -> "ActionInfo":
        return ActionInfo([], [], {}, [])


--- ml-agents/mlagents/trainers/agent_processor.py ---
import sys
import numpy as np
from typing import List, Dict, TypeVar, Generic, Tuple, Any, Union
from collections import defaultdict, Counter
import queue
from mlagents.torch_utils import torch

from mlagents_envs.base_env import (
    ActionTuple,
    DecisionSteps,
    DecisionStep,
    TerminalSteps,
    TerminalStep,
)
from mlagents_envs.side_channel.stats_side_channel import (
    StatsAggregationMethod,
    EnvironmentStats,
)
from mlagents.trainers.exception import UnityTrainerException
from mlagents.trainers.trajectory import AgentStatus, Trajectory, AgentExperience
from mlagents.trainers.policy import Policy
from mlagents.trainers.action_info import ActionInfo, ActionInfoOutputs
from mlagents.trainers.stats import StatsReporter
from mlagents.trainers.behavior_id_utils import (
    get_global_agent_id,
    get_global_group_id,
    GlobalAgentId,
    GlobalGroupId,
)
from mlagents.trainers.torch_entities.action_log_probs import LogProbsTuple
from mlagents.trainers.torch_entities.utils import ModelUtils

T = TypeVar("T")


class AgentProcessor:
    """
    AgentProcessor contains a dictionary per-agent trajectory buffers. The buffers are indexed by agent_id.
    Buffer also contains an update_buffer that corresponds to the buffer used when updating the model.
    One AgentProcessor should be created per agent group.
    """

    def __init__(
        self,
        policy: Policy,
        behavior_id: str,
        stats_reporter: StatsReporter,
        max_trajectory_length: int = sys.maxsize,
    ):
        """
        Create an AgentProcessor.

        :param trainer: Trainer instance connected to this AgentProcessor. Trainer is given trajectory
        when it is finished.
        :param policy: Policy instance associated with this AgentProcessor.
        :param max_trajectory_length: Maximum length of a trajectory before it is added to the trainer.
        :param stats_category: The category under which to write the stats. Usually, this comes from the Trainer.
        """
        self._experience_buffers: Dict[
            GlobalAgentId, List[AgentExperience]
        ] = defaultdict(list)
        self._last_step_result: Dict[GlobalAgentId, Tuple[DecisionStep, int]] = {}
        # current_group_obs is used to collect the current (i.e. the most recently seen)
        # obs of all the agents in the same group, and assemble the group obs.
        # It is a dictionary of GlobalGroupId to dictionaries of GlobalAgentId to observation.
        self._current_group_obs: Dict[
            GlobalGroupId, Dict[GlobalAgentId, List[np.ndarray]]
        ] = defaultdict(lambda: defaultdict(list))
        # group_status is used to collect the current, most recently seen
        # group status of all the agents in the same group, and assemble the group's status.
        # It is a dictionary of GlobalGroupId to dictionaries of GlobalAgentId to AgentStatus.
        self._group_status: Dict[
            GlobalGroupId, Dict[GlobalAgentId, AgentStatus]
        ] = defaultdict(lambda: defaultdict(None))
        # last_take_action_outputs stores the action a_t taken before the current observation s_(t+1), while
        # grabbing previous_action from the policy grabs the action PRIOR to that, a_(t-1).
        self._last_take_action_outputs: Dict[GlobalAgentId, ActionInfoOutputs] = {}

        self._episode_steps: Counter = Counter()
        self._episode_rewards: Dict[GlobalAgentId, float] = defaultdict(float)
        self._stats_reporter = stats_reporter
        self._max_trajectory_length = max_trajectory_length
        self._trajectory_queues: List[AgentManagerQueue[Trajectory]] = []
        self._behavior_id = behavior_id

        # Note: In the future this policy reference will be the policy of the env_manager and not the trainer.
        # We can in that case just grab the action from the policy rather than having it passed in.
        self.policy = policy

    def add_experiences(
        self,
        decision_steps: DecisionSteps,
        terminal_steps: TerminalSteps,
        worker_id: int,
        previous_action: ActionInfo,
    ) -> None:
        """
        Adds experiences to each agent's experience history.
        :param decision_steps: current DecisionSteps.
        :param terminal_steps: current TerminalSteps.
        :param previous_action: The outputs of the Policy's get_action method.
        """
        take_action_outputs = previous_action.outputs
        if take_action_outputs:
            try:
                for _entropy in take_action_outputs["entropy"]:
                    if isinstance(_entropy, torch.Tensor):
                        _entropy = ModelUtils.to_numpy(_entropy)
                    self._stats_reporter.add_stat("Policy/Entropy", _entropy)
            except KeyError:
                pass

        # Make unique agent_ids that are global across workers
        action_global_agent_ids = [
            get_global_agent_id(worker_id, ag_id) for ag_id in previous_action.agent_ids
        ]
        for global_id in action_global_agent_ids:
            if global_id in self._last_step_result:  # Don't store if agent just reset
                self._last_take_action_outputs[global_id] = take_action_outputs

        # Iterate over all the terminal steps, first gather all the group obs
        # and then create the AgentExperiences/Trajectories. _add_to_group_status
        # stores Group statuses in a common data structure self.group_status
        for terminal_step in terminal_steps.values():
            self._add_group_status_and_obs(terminal_step, worker_id)
        for terminal_step in terminal_steps.values():
            local_id = terminal_step.agent_id
            global_id = get_global_agent_id(worker_id, local_id)
            self._process_step(
                terminal_step, worker_id, terminal_steps.agent_id_to_index[local_id]
            )

        # Iterate over all the decision steps, first gather all the group obs
        # and then create the trajectories. _add_to_group_status
        # stores Group statuses in a common data structure self.group_status
        for ongoing_step in decision_steps.values():
            self._add_group_status_and_obs(ongoing_step, worker_id)
        for ongoing_step in decision_steps.values():
            local_id = ongoing_step.agent_id
            self._process_step(
                ongoing_step, worker_id, decision_steps.agent_id_to_index[local_id]
            )
        # Clear the last seen group obs when agents die, but only after all of the group
        # statuses were added to the trajectory.
        for terminal_step in terminal_steps.values():
            local_id = terminal_step.agent_id
            global_id = get_global_agent_id(worker_id, local_id)
            self._clear_group_status_and_obs(global_id)

        for _gid in action_global_agent_ids:
            # If the ID doesn't have a last step result, the agent just reset,
            # don't store the action.
            if _gid in self._last_step_result:
                if "action" in take_action_outputs:
                    self.policy.save_previous_action(
                        [_gid], take_action_outputs["action"]
                    )

    def _add_group_status_and_obs(
        self, step: Union[TerminalStep, DecisionStep], worker_id: int
    ) -> None:
        """
        Takes a TerminalStep or DecisionStep and adds the information in it
        to self.group_status. This information can then be retrieved
        when constructing trajectories to get the status of group mates. Also stores the current
        observation into current_group_obs, to be used to get the next group observations
        for bootstrapping.
        :param step: TerminalStep or DecisionStep
        :param worker_id: Worker ID of this particular environment. Used to generate a
            global group id.
        """
        global_agent_id = get_global_agent_id(worker_id, step.agent_id)
        stored_decision_step, idx = self._last_step_result.get(
            global_agent_id, (None, None)
        )
        stored_take_action_outputs = self._last_take_action_outputs.get(
            global_agent_id, None
        )
        if stored_decision_step is not None and stored_take_action_outputs is not None:
            # 0, the default group_id, means that the agent doesn't belong to an agent group.
            # If 0, don't add any groupmate information.
            if step.group_id > 0:
                global_group_id = get_global_group_id(worker_id, step.group_id)
                stored_actions = stored_take_action_outputs["action"]
                action_tuple = ActionTuple(
                    continuous=stored_actions.continuous[idx],
                    discrete=stored_actions.discrete[idx],
                )
                group_status = AgentStatus(
                    obs=stored_decision_step.obs,
                    reward=step.reward,
                    action=action_tuple,
                    done=isinstance(step, TerminalStep),
                )
                self._group_status[global_group_id][global_agent_id] = group_status
                self._current_group_obs[global_group_id][global_agent_id] = step.obs

    def _clear_group_status_and_obs(self, global_id: GlobalAgentId) -> None:
        """
        Clears an agent from self._group_status and self._current_group_obs.
        """
        self._delete_in_nested_dict(self._current_group_obs, global_id)
        self._delete_in_nested_dict(self._group_status, global_id)

    def _delete_in_nested_dict(self, nested_dict: Dict[str, Any], key: str) -> None:
        for _manager_id in list(nested_dict.keys()):
            _team_group = nested_dict[_manager_id]
            self._safe_delete(_team_group, key)
            if not _team_group:  # if dict is empty
                self._safe_delete(nested_dict, _manager_id)

    def _process_step(
        self, step: Union[TerminalStep, DecisionStep], worker_id: int, index: int
    ) -> None:
        terminated = isinstance(step, TerminalStep)
        global_agent_id = get_global_agent_id(worker_id, step.agent_id)
        global_group_id = get_global_group_id(worker_id, step.group_id)
        stored_decision_step, idx = self._last_step_result.get(
            global_agent_id, (None, None)
        )
        stored_take_action_outputs = self._last_take_action_outputs.get(
            global_agent_id, None
        )
        if not terminated:
            # Index is needed to grab from last_take_action_outputs
            self._last_step_result[global_agent_id] = (step, index)

        # This state is the consequence of a past action
        if stored_decision_step is not None and stored_take_action_outputs is not None:
            obs = stored_decision_step.obs
            if self.policy.use_recurrent:
                memory = self.policy.retrieve_previous_memories([global_agent_id])[0, :]
            else:
                memory = None
            done = terminated  # Since this is an ongoing step
            interrupted = step.interrupted if terminated else False
            # Add the outputs of the last eval
            stored_actions = stored_take_action_outputs["action"]
            action_tuple = ActionTuple(
                continuous=stored_actions.continuous[idx],
                discrete=stored_actions.discrete[idx],
            )
            try:
                stored_action_probs = stored_take_action_outputs["log_probs"]
                if not isinstance(stored_action_probs, LogProbsTuple):
                    stored_action_probs = stored_action_probs.to_log_probs_tuple()
                log_probs_tuple = LogProbsTuple(
                    continuous=stored_action_probs.continuous[idx],
                    discrete=stored_action_probs.discrete[idx],
                )
            except KeyError:
                log_probs_tuple = LogProbsTuple.empty_log_probs()

            action_mask = stored_decision_step.action_mask
            prev_action = self.policy.retrieve_previous_action([global_agent_id])[0, :]

            # Assemble teammate_obs. If none saved, then it will be an empty list.
            group_statuses = []
            for _id, _mate_status in self._group_status[global_group_id].items():
                if _id != global_agent_id:
                    group_statuses.append(_mate_status)

            experience = AgentExperience(
                obs=obs,
                reward=step.reward,
                done=done,
                action=action_tuple,
                action_probs=log_probs_tuple,
                action_mask=action_mask,
                prev_action=prev_action,
                interrupted=interrupted,
                memory=memory,
                group_status=group_statuses,
                group_reward=step.group_reward,
            )
            # Add the value outputs if needed
            self._experience_buffers[global_agent_id].append(experience)
            self._episode_rewards[global_agent_id] += step.reward
            if not terminated:
                self._episode_steps[global_agent_id] += 1

            # Add a trajectory segment to the buffer if terminal or the length has reached the time horizon
            if (
                len(self._experience_buffers[global_agent_id])
                >= self._max_trajectory_length
                or terminated
            ):
                next_obs = step.obs
                next_group_obs = []
                for _id, _obs in self._current_group_obs[global_group_id].items():
                    if _id != global_agent_id:
                        next_group_obs.append(_obs)

                trajectory = Trajectory(
                    steps=self._experience_buffers[global_agent_id],
                    agent_id=global_agent_id,
                    next_obs=next_obs,
                    next_group_obs=next_group_obs,
                    behavior_id=self._behavior_id,
                )
                for traj_queue in self._trajectory_queues:
                    traj_queue.put(trajectory)
                self._experience_buffers[global_agent_id] = []
            if terminated:
                # Record episode length.
                self._stats_reporter.add_stat(
                    "Environment/Episode Length",
                    self._episode_steps.get(global_agent_id, 0),
                )
                self._clean_agent_data(global_agent_id)

    def _clean_agent_data(self, global_id: GlobalAgentId) -> None:
        """
        Removes the data for an Agent.
        """
        self._safe_delete(self._experience_buffers, global_id)
        self._safe_delete(self._last_take_action_outputs, global_id)
        self._safe_delete(self._last_step_result, global_id)
        self._safe_delete(self._episode_steps, global_id)
        self._safe_delete(self._episode_rewards, global_id)
        self.policy.remove_previous_action([global_id])
        self.policy.remove_memories([global_id])

    def _safe_delete(self, my_dictionary: Dict[Any, Any], key: Any) -> None:
        """
        Safe removes data from a dictionary. If not found,
        don't delete.
        """
        if key in my_dictionary:
            del my_dictionary[key]

    def publish_trajectory_queue(
        self, trajectory_queue: "AgentManagerQueue[Trajectory]"
    ) -> None:
        """
        Adds a trajectory queue to the list of queues to publish to when this AgentProcessor
        assembles a Trajectory
        :param trajectory_queue: Trajectory queue to publish to.
        """
        self._trajectory_queues.append(trajectory_queue)

    def end_episode(self) -> None:
        """
        Ends the episode, terminating the current trajectory and stopping stats collection for that
        episode. Used for forceful reset (e.g. in curriculum or generalization training.)
        """
        all_gids = list(self._experience_buffers.keys())  # Need to make copy
        for _gid in all_gids:
            self._clean_agent_data(_gid)


class AgentManagerQueue(Generic[T]):
    """
    Queue used by the AgentManager. Note that we make our own class here because in most implementations
    deque is sufficient and faster. However, if we want to switch to multiprocessing, we'll need to change
    out this implementation.
    """

    class Empty(Exception):
        """
        Exception for when the queue is empty.
        """

        pass

    def __init__(self, behavior_id: str, maxlen: int = 0):
        """
        Initializes an AgentManagerQueue. Note that we can give it a behavior_id so that it can be identified
        separately from an AgentManager.
        """
        self._maxlen: int = maxlen
        self._queue: queue.Queue = queue.Queue(maxsize=maxlen)
        self._behavior_id = behavior_id

    @property
    def maxlen(self):
        """
        The maximum length of the queue.
        :return: Maximum length of the queue.
        """
        return self._maxlen

    @property
    def behavior_id(self):
        """
        The Behavior ID of this queue.
        :return: Behavior ID associated with the queue.
        """
        return self._behavior_id

    def qsize(self) -> int:
        """
        Returns the approximate size of the queue. Note that values may differ
        depending on the underlying queue implementation.
        """
        return self._queue.qsize()

    def empty(self) -> bool:
        return self._queue.empty()

    def get_nowait(self) -> T:
        """
        Gets the next item from the queue, throwing an AgentManagerQueue.Empty exception
        if the queue is empty.
        """
        try:
            return self._queue.get_nowait()
        except queue.Empty:
            raise self.Empty("The AgentManagerQueue is empty.")

    def put(self, item: T) -> None:
        self._queue.put(item)


class AgentManager(AgentProcessor):
    """
    An AgentManager is an AgentProcessor that also holds a single trajectory and policy queue.
    Note: this leaves room for adding AgentProcessors that publish multiple trajectory queues.
    """

    def __init__(
        self,
        policy: Policy,
        behavior_id: str,
        stats_reporter: StatsReporter,
        max_trajectory_length: int = sys.maxsize,
        threaded: bool = True,
    ):
        super().__init__(policy, behavior_id, stats_reporter, max_trajectory_length)
        trajectory_queue_len = 20 if threaded else 0
        self.trajectory_queue: AgentManagerQueue[Trajectory] = AgentManagerQueue(
            self._behavior_id, maxlen=trajectory_queue_len
        )
        # NOTE: we make policy queues of infinite length to avoid lockups of the trainers.
        # In the environment manager, we make sure to empty the policy queue before continuing to produce steps.
        self.policy_queue: AgentManagerQueue[Policy] = AgentManagerQueue(
            self._behavior_id, maxlen=0
        )
        self.publish_trajectory_queue(self.trajectory_queue)

    def record_environment_stats(
        self, env_stats: EnvironmentStats, worker_id: int
    ) -> None:
        """
        Pass stats from the environment to the StatsReporter.
        Depending on the StatsAggregationMethod, either StatsReporter.add_stat or StatsReporter.set_stat is used.
        The worker_id is used to determine whether StatsReporter.set_stat should be used.

        :param env_stats:
        :param worker_id:
        :return:
        """
        for stat_name, value_list in env_stats.items():
            for val, agg_type in value_list:
                if agg_type == StatsAggregationMethod.AVERAGE:
                    self._stats_reporter.add_stat(stat_name, val, agg_type)
                elif agg_type == StatsAggregationMethod.SUM:
                    self._stats_reporter.add_stat(stat_name, val, agg_type)
                elif agg_type == StatsAggregationMethod.HISTOGRAM:
                    self._stats_reporter.add_stat(stat_name, val, agg_type)
                elif agg_type == StatsAggregationMethod.MOST_RECENT:
                    # In order to prevent conflicts between multiple environments,
                    # only stats from the first environment are recorded.
                    if worker_id == 0:
                        self._stats_reporter.set_stat(stat_name, val)
                else:
                    raise UnityTrainerException(
                        f"Unknown StatsAggregationMethod encountered. {agg_type}"
                    )


--- ml-agents/mlagents/trainers/behavior_id_utils.py ---
from typing import NamedTuple
from urllib.parse import urlparse, parse_qs
from mlagents_envs.base_env import AgentId, GroupId

GlobalGroupId = str
GlobalAgentId = str


class BehaviorIdentifiers(NamedTuple):
    """
    BehaviorIdentifiers is a named tuple of the identifiers that uniquely distinguish
    an agent encountered in the trainer_controller. The named tuple consists of the
    fully qualified behavior name, the name of the brain name (corresponds to trainer
    in the trainer controller) and the team id.  In the future, this can be extended
    to support further identifiers.
    """

    behavior_id: str
    brain_name: str
    team_id: int

    @staticmethod
    def from_name_behavior_id(name_behavior_id: str) -> "BehaviorIdentifiers":
        """
        Parses a name_behavior_id of the form name?team=0
        into a BehaviorIdentifiers NamedTuple.
        This allows you to access the brain name and team id of an agent
        :param name_behavior_id: String of behavior params in HTTP format.
        :returns: A BehaviorIdentifiers object.
        """

        parsed = urlparse(name_behavior_id)
        name = parsed.path
        ids = parse_qs(parsed.query)
        team_id: int = 0
        if "team" in ids:
            team_id = int(ids["team"][0])
        return BehaviorIdentifiers(
            behavior_id=name_behavior_id, brain_name=name, team_id=team_id
        )


def create_name_behavior_id(name: str, team_id: int) -> str:
    """
    Reconstructs fully qualified behavior name from name and team_id
    :param name: brain name
    :param team_id: team ID
    :return: name_behavior_id
    """
    return name + "?team=" + str(team_id)


def get_global_agent_id(worker_id: int, agent_id: AgentId) -> GlobalAgentId:
    """
    Create an agent id that is unique across environment workers using the worker_id.
    """
    return f"agent_{worker_id}-{agent_id}"


def get_global_group_id(worker_id: int, group_id: GroupId) -> GlobalGroupId:
    """
    Create a group id that is unique across environment workers when using the worker_id.
    """
    return f"group_{worker_id}-{group_id}"


--- ml-agents/mlagents/trainers/buffer.py ---
from collections import defaultdict
from collections.abc import MutableMapping
import enum
import itertools
from typing import BinaryIO, DefaultDict, List, Tuple, Union, Optional

import numpy as np
import h5py

from mlagents_envs.exception import UnityException

# Elements in the buffer can be np.ndarray, or in the case of teammate obs, actions, rewards,
# a List of np.ndarray. This is done so that we don't have duplicated np.ndarrays, only references.
BufferEntry = Union[np.ndarray, List[np.ndarray]]


class BufferException(UnityException):
    """
    Related to errors with the Buffer.
    """

    pass


class BufferKey(enum.Enum):
    ACTION_MASK = "action_mask"
    CONTINUOUS_ACTION = "continuous_action"
    NEXT_CONT_ACTION = "next_continuous_action"
    CONTINUOUS_LOG_PROBS = "continuous_log_probs"
    DISCRETE_ACTION = "discrete_action"
    NEXT_DISC_ACTION = "next_discrete_action"
    DISCRETE_LOG_PROBS = "discrete_log_probs"
    DONE = "done"
    ENVIRONMENT_REWARDS = "environment_rewards"
    MASKS = "masks"
    MEMORY = "memory"
    CRITIC_MEMORY = "critic_memory"
    BASELINE_MEMORY = "poca_baseline_memory"
    PREV_ACTION = "prev_action"

    ADVANTAGES = "advantages"
    DISCOUNTED_RETURNS = "discounted_returns"

    GROUP_DONES = "group_dones"
    GROUPMATE_REWARDS = "groupmate_reward"
    GROUP_REWARD = "group_reward"
    GROUP_CONTINUOUS_ACTION = "group_continuous_action"
    GROUP_DISCRETE_ACTION = "group_discrete_aaction"
    GROUP_NEXT_CONT_ACTION = "group_next_cont_action"
    GROUP_NEXT_DISC_ACTION = "group_next_disc_action"


class ObservationKeyPrefix(enum.Enum):
    OBSERVATION = "obs"
    NEXT_OBSERVATION = "next_obs"

    GROUP_OBSERVATION = "group_obs"
    NEXT_GROUP_OBSERVATION = "next_group_obs"


class RewardSignalKeyPrefix(enum.Enum):
    # Reward signals
    REWARDS = "rewards"
    VALUE_ESTIMATES = "value_estimates"
    RETURNS = "returns"
    ADVANTAGE = "advantage"
    BASELINES = "baselines"


AgentBufferKey = Union[
    BufferKey, Tuple[ObservationKeyPrefix, int], Tuple[RewardSignalKeyPrefix, str]
]


class RewardSignalUtil:
    @staticmethod
    def rewards_key(name: str) -> AgentBufferKey:
        return RewardSignalKeyPrefix.REWARDS, name

    @staticmethod
    def value_estimates_key(name: str) -> AgentBufferKey:
        return RewardSignalKeyPrefix.RETURNS, name

    @staticmethod
    def returns_key(name: str) -> AgentBufferKey:
        return RewardSignalKeyPrefix.RETURNS, name

    @staticmethod
    def advantage_key(name: str) -> AgentBufferKey:
        return RewardSignalKeyPrefix.ADVANTAGE, name

    @staticmethod
    def baseline_estimates_key(name: str) -> AgentBufferKey:
        return RewardSignalKeyPrefix.BASELINES, name


class AgentBufferField(list):
    """
    AgentBufferField is a list of numpy arrays, or List[np.ndarray] for group entries.
    When an agent collects a field, you can add it to its AgentBufferField with the append method.
    """

    def __init__(self, *args, **kwargs):
        self.padding_value = 0
        super().__init__(*args, **kwargs)

    def __str__(self) -> str:
        return f"AgentBufferField: {super().__str__()}"

    def __getitem__(self, index):
        return_data = super().__getitem__(index)
        if isinstance(return_data, list):
            return AgentBufferField(return_data)
        else:
            return return_data

    @property
    def contains_lists(self) -> bool:
        """
        Checks whether this AgentBufferField contains List[np.ndarray].
        """
        return len(self) > 0 and isinstance(self[0], list)

    def append(self, element: BufferEntry, padding_value: float = 0.0) -> None:
        """
        Adds an element to this list. Also lets you change the padding
        type, so that it can be set on append (e.g. action_masks should
        be padded with 1.)
        :param element: The element to append to the list.
        :param padding_value: The value used to pad when get_batch is called.
        """
        super().append(element)
        self.padding_value = padding_value

    def set(self, data: List[BufferEntry]) -> None:
        """
        Sets the list of BufferEntry to the input data
        :param data: The BufferEntry list to be set.
        """
        self[:] = data

    def get_batch(
        self,
        batch_size: int = None,
        training_length: Optional[int] = 1,
        sequential: bool = True,
    ) -> List[BufferEntry]:
        """
        Retrieve the last batch_size elements of length training_length
        from the list of np.array
        :param batch_size: The number of elements to retrieve. If None:
        All elements will be retrieved.
        :param training_length: The length of the sequence to be retrieved. If
        None: only takes one element.
        :param sequential: If true and training_length is not None: the elements
        will not repeat in the sequence. [a,b,c,d,e] with training_length = 2 and
        sequential=True gives [[0,a],[b,c],[d,e]]. If sequential=False gives
        [[a,b],[b,c],[c,d],[d,e]]
        """
        if training_length is None:
            training_length = 1
        if sequential:
            # The sequences will not have overlapping elements (this involves padding)
            leftover = len(self) % training_length
            # leftover is the number of elements in the first sequence (this sequence might need 0 padding)
            if batch_size is None:
                # retrieve the maximum number of elements
                batch_size = len(self) // training_length + 1 * (leftover != 0)
            # The maximum number of sequences taken from a list of length len(self) without overlapping
            # with padding is equal to batch_size
            if batch_size > (len(self) // training_length + 1 * (leftover != 0)):
                raise BufferException(
                    "The batch size and training length requested for get_batch where"
                    " too large given the current number of data points."
                )
            if batch_size * training_length > len(self):
                if self.contains_lists:
                    padding = []
                else:
                    # We want to duplicate the last value in the array, multiplied by the padding_value.
                    padding = np.array(self[-1], dtype=np.float32) * self.padding_value
                return self[:] + [padding] * (training_length - leftover)

            else:
                return self[len(self) - batch_size * training_length :]
        else:
            # The sequences will have overlapping elements
            if batch_size is None:
                # retrieve the maximum number of elements
                batch_size = len(self) - training_length + 1
            # The number of sequences of length training_length taken from a list of len(self) elements
            # with overlapping is equal to batch_size
            if (len(self) - training_length + 1) < batch_size:
                raise BufferException(
                    "The batch size and training length requested for get_batch where"
                    " too large given the current number of data points."
                )
            tmp_list: List[np.ndarray] = []
            for end in range(len(self) - batch_size + 1, len(self) + 1):
                tmp_list += self[end - training_length : end]
            return tmp_list

    def reset_field(self) -> None:
        """
        Resets the AgentBufferField
        """
        self[:] = []

    def padded_to_batch(
        self, pad_value: np.float = 0, dtype: np.dtype = np.float32
    ) -> Union[np.ndarray, List[np.ndarray]]:
        """
        Converts this AgentBufferField (which is a List[BufferEntry]) into a numpy array
        with first dimension equal to the length of this AgentBufferField. If this AgentBufferField
        contains a List[List[BufferEntry]] (i.e., in the case of group observations), return a List
        containing numpy arrays or tensors, of length equal to the maximum length of an entry. Missing
        For entries with less than that length, the array will be padded with pad_value.
        :param pad_value: Value to pad List AgentBufferFields, when there are less than the maximum
            number of agents present.
        :param dtype: Dtype of output numpy array.
        :return: Numpy array or List of numpy arrays representing this AgentBufferField, where the first
            dimension is equal to the length of the AgentBufferField.
        """
        if len(self) > 0 and not isinstance(self[0], list):
            return np.asanyarray(self, dtype=dtype)

        shape = None
        for _entry in self:
            # _entry could be an empty list if there are no group agents in this
            # step. Find the first non-empty list and use that shape.
            if _entry:
                shape = _entry[0].shape
                break
        # If there were no groupmate agents in the entire batch, return an empty List.
        if shape is None:
            return []

        # Convert to numpy array while padding with 0's
        new_list = list(
            map(
                lambda x: np.asanyarray(x, dtype=dtype),
                itertools.zip_longest(*self, fillvalue=np.full(shape, pad_value)),
            )
        )
        return new_list

    def to_ndarray(self):
        """
        Returns the AgentBufferField which is a list of numpy ndarrays (or List[np.ndarray]) as an ndarray.
        """
        return np.array(self)


class AgentBuffer(MutableMapping):
    """
    AgentBuffer contains a dictionary of AgentBufferFields. Each agent has his own AgentBuffer.
    The keys correspond to the name of the field. Example: state, action
    """

    # Whether or not to validate the types of keys at runtime
    # This should be off for training, but enabled for testing
    CHECK_KEY_TYPES_AT_RUNTIME = False

    def __init__(self):
        self.last_brain_info = None
        self.last_take_action_outputs = None
        self._fields: DefaultDict[AgentBufferKey, AgentBufferField] = defaultdict(
            AgentBufferField
        )

    def __str__(self):
        return ", ".join([f"'{k}' : {str(self[k])}" for k in self._fields.keys()])

    def reset_agent(self) -> None:
        """
        Resets the AgentBuffer
        """
        for f in self._fields.values():
            f.reset_field()
        self.last_brain_info = None
        self.last_take_action_outputs = None

    @staticmethod
    def _check_key(key):
        if isinstance(key, BufferKey):
            return
        if isinstance(key, tuple):
            key0, key1 = key
            if isinstance(key0, ObservationKeyPrefix):
                if isinstance(key1, int):
                    return
                raise KeyError(f"{key} has type ({type(key0)}, {type(key1)})")
            if isinstance(key0, RewardSignalKeyPrefix):
                if isinstance(key1, str):
                    return
                raise KeyError(f"{key} has type ({type(key0)}, {type(key1)})")
        raise KeyError(f"{key} is a {type(key)}")

    @staticmethod
    def _encode_key(key: AgentBufferKey) -> str:
        """
        Convert the key to a string representation so that it can be used for serialization.
        """
        if isinstance(key, BufferKey):
            return key.value
        prefix, suffix = key
        return f"{prefix.value}:{suffix}"

    @staticmethod
    def _decode_key(encoded_key: str) -> AgentBufferKey:
        """
        Convert the string representation back to a key after serialization.
        """
        # Simple case: convert the string directly to a BufferKey
        try:
            return BufferKey(encoded_key)
        except ValueError:
            pass

        # Not a simple key, so split into two parts
        prefix_str, _, suffix_str = encoded_key.partition(":")

        # See if it's an ObservationKeyPrefix first
        try:
            return ObservationKeyPrefix(prefix_str), int(suffix_str)
        except ValueError:
            pass

        # If not, it had better be a RewardSignalKeyPrefix
        try:
            return RewardSignalKeyPrefix(prefix_str), suffix_str
        except ValueError:
            raise ValueError(f"Unable to convert {encoded_key} to an AgentBufferKey")

    def __getitem__(self, key: AgentBufferKey) -> AgentBufferField:
        if self.CHECK_KEY_TYPES_AT_RUNTIME:
            self._check_key(key)
        return self._fields[key]

    def __setitem__(self, key: AgentBufferKey, value: AgentBufferField) -> None:
        if self.CHECK_KEY_TYPES_AT_RUNTIME:
            self._check_key(key)
        self._fields[key] = value

    def __delitem__(self, key: AgentBufferKey) -> None:
        if self.CHECK_KEY_TYPES_AT_RUNTIME:
            self._check_key(key)
        self._fields.__delitem__(key)

    def __iter__(self):
        return self._fields.__iter__()

    def __len__(self) -> int:
        return self._fields.__len__()

    def __contains__(self, key):
        if self.CHECK_KEY_TYPES_AT_RUNTIME:
            self._check_key(key)
        return self._fields.__contains__(key)

    def check_length(self, key_list: List[AgentBufferKey]) -> bool:
        """
        Some methods will require that some fields have the same length.
        check_length will return true if the fields in key_list
        have the same length.
        :param key_list: The fields which length will be compared
        """
        if self.CHECK_KEY_TYPES_AT_RUNTIME:
            for k in key_list:
                self._check_key(k)

        if len(key_list) < 2:
            return True
        length = None
        for key in key_list:
            if key not in self._fields:
                return False
            if (length is not None) and (length != len(self[key])):
                return False
            length = len(self[key])
        return True

    def shuffle(
        self, sequence_length: int, key_list: List[AgentBufferKey] = None
    ) -> None:
        """
        Shuffles the fields in key_list in a consistent way: The reordering will
        be the same across fields.
        :param key_list: The fields that must be shuffled.
        """
        if key_list is None:
            key_list = list(self._fields.keys())
        if not self.check_length(key_list):
            raise BufferException(
                "Unable to shuffle if the fields are not of same length"
            )
        s = np.arange(len(self[key_list[0]]) // sequence_length)
        np.random.shuffle(s)
        for key in key_list:
            buffer_field = self[key]
            tmp: List[np.ndarray] = []
            for i in s:
                tmp += buffer_field[i * sequence_length : (i + 1) * sequence_length]
            buffer_field.set(tmp)

    def make_mini_batch(self, start: int, end: int) -> "AgentBuffer":
        """
        Creates a mini-batch from buffer.
        :param start: Starting index of buffer.
        :param end: Ending index of buffer.
        :return: Dict of mini batch.
        """
        mini_batch = AgentBuffer()
        for key, field in self._fields.items():
            # slicing AgentBufferField returns a List[Any}
            mini_batch[key] = field[start:end]  # type: ignore
        return mini_batch

    def sample_mini_batch(
        self, batch_size: int, sequence_length: int = 1
    ) -> "AgentBuffer":
        """
        Creates a mini-batch from a random start and end.
        :param batch_size: number of elements to withdraw.
        :param sequence_length: Length of sequences to sample.
            Number of sequences to sample will be batch_size/sequence_length.
        """
        num_seq_to_sample = batch_size // sequence_length
        mini_batch = AgentBuffer()
        buff_len = self.num_experiences
        num_sequences_in_buffer = buff_len // sequence_length
        start_idxes = (
            np.random.randint(num_sequences_in_buffer, size=num_seq_to_sample)
            * sequence_length
        )  # Sample random sequence starts
        for key in self:
            buffer_field = self[key]
            mb_list = (buffer_field[i : i + sequence_length] for i in start_idxes)
            # See comparison of ways to make a list from a list of lists here:
            # https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists
            mini_batch[key].set(list(itertools.chain.from_iterable(mb_list)))
        return mini_batch

    def save_to_file(self, file_object: BinaryIO) -> None:
        """
        Saves the AgentBuffer to a file-like object.
        """
        with h5py.File(file_object, "w") as write_file:
            for key, data in self.items():
                write_file.create_dataset(
                    self._encode_key(key), data=data, dtype="f", compression="gzip"
                )

    def load_from_file(self, file_object: BinaryIO) -> None:
        """
        Loads the AgentBuffer from a file-like object.
        """
        with h5py.File(file_object, "r") as read_file:
            for key in list(read_file.keys()):
                decoded_key = self._decode_key(key)
                self[decoded_key] = AgentBufferField()
                # extend() will convert the numpy array's first dimension into list
                self[decoded_key].extend(read_file[key][()])

    def truncate(self, max_length: int, sequence_length: int = 1) -> None:
        """
        Truncates the buffer to a certain length.

        This can be slow for large buffers. We compensate by cutting further than we need to, so that
        we're not truncating at each update. Note that we must truncate an integer number of sequence_lengths
        param: max_length: The length at which to truncate the buffer.
        """
        current_length = self.num_experiences
        # make max_length an integer number of sequence_lengths
        max_length -= max_length % sequence_length
        if current_length > max_length:
            for _key in self.keys():
                self[_key][:] = self[_key][current_length - max_length :]

    def resequence_and_append(
        self,
        target_buffer: "AgentBuffer",
        key_list: List[AgentBufferKey] = None,
        batch_size: int = None,
        training_length: int = None,
    ) -> None:
        """
        Takes in a batch size and training length (sequence length), and appends this AgentBuffer to target_buffer
        properly padded for LSTM use. Optionally, use key_list to restrict which fields are inserted into the new
        buffer.
        :param target_buffer: The buffer which to append the samples to.
        :param key_list: The fields that must be added. If None: all fields will be appended.
        :param batch_size: The number of elements that must be appended. If None: All of them will be.
        :param training_length: The length of the samples that must be appended. If None: only takes one element.
        """
        if key_list is None:
            key_list = list(self.keys())
        if not self.check_length(key_list):
            raise BufferException(
                f"The length of the fields {key_list} were not of same length"
            )
        for field_key in key_list:
            target_buffer[field_key].extend(
                self[field_key].get_batch(
                    batch_size=batch_size, training_length=training_length
                )
            )

    @property
    def num_experiences(self) -> int:
        """
        The number of agent experiences in the AgentBuffer, i.e. the length of the buffer.

        An experience consists of one element across all of the fields of this AgentBuffer.
        Note that these all have to be the same length, otherwise shuffle and append_to_update_buffer
        will fail.
        """
        if self.values():
            return len(next(iter(self.values())))
        else:
            return 0


--- ml-agents/tests/yamato/check_coverage_percent.py ---
import sys
import os

SUMMARY_XML_FILENAME = "Summary.xml"


def check_coverage(root_dir, min_percentage):
    # Walk the root directory looking for the summary file that
    # is output by ther code coverage checks. It's possible that
    # we'll need to refine this later in case there are multiple
    # such files.
    summary_xml = None
    for dirpath, _, filenames in os.walk(root_dir):
        if SUMMARY_XML_FILENAME in filenames:
            summary_xml = os.path.join(dirpath, SUMMARY_XML_FILENAME)
            break
    if not summary_xml:
        print(f"Couldn't find {SUMMARY_XML_FILENAME} in root directory")
        sys.exit(1)

    with open(summary_xml) as f:
        # Rather than try to parse the XML, just look for a line of the form
        # <Linecoverage>73.9</Linecoverage>
        lines = f.readlines()
        for line in lines:
            if "Linecoverage" in line:
                pct = line.replace("<Linecoverage>", "").replace("</Linecoverage>", "")
                pct = float(pct)
                if pct < min_percentage:
                    print(
                        f"Coverage {pct} is below the min percentage of {min_percentage}."
                    )
                    sys.exit(1)
                else:
                    print(
                        f"Coverage {pct} is above the min percentage of {min_percentage}."
                    )
                    sys.exit(0)

    # Couldn't find the results in the file.
    print("Couldn't find Linecoverage in summary file")
    sys.exit(1)


def main():
    root_dir = sys.argv[1]
    min_percent = float(sys.argv[2])
    if min_percent > 0:
        # This allows us to set 0% coverage on 2018.4
        check_coverage(root_dir, min_percent)


if __name__ == "__main__":
    main()


--- ml-agents/mlagents/trainers/cli_utils.py ---
from typing import Set, Dict, Any, TextIO
import os
import yaml
from mlagents.trainers.exception import TrainerConfigError
from mlagents_envs.environment import UnityEnvironment
import argparse
from mlagents_envs import logging_util

logger = logging_util.get_logger(__name__)


class RaiseRemovedWarning(argparse.Action):
    """
    Internal custom Action to raise warning when argument is called.
    """

    def __init__(self, nargs=0, **kwargs):
        super().__init__(nargs=nargs, **kwargs)

    def __call__(self, arg_parser, namespace, values, option_string=None):
        logger.warning(f"The command line argument {option_string} was removed.")


class DetectDefault(argparse.Action):
    """
    Internal custom Action to help detect arguments that aren't default.
    """

    non_default_args: Set[str] = set()

    def __call__(self, arg_parser, namespace, values, option_string=None):
        setattr(namespace, self.dest, values)
        DetectDefault.non_default_args.add(self.dest)


class DetectDefaultStoreTrue(DetectDefault):
    """
    Internal class to help detect arguments that aren't default.
    Used for store_true arguments.
    """

    def __init__(self, nargs=0, **kwargs):
        super().__init__(nargs=nargs, **kwargs)

    def __call__(self, arg_parser, namespace, values, option_string=None):
        super().__call__(arg_parser, namespace, True, option_string)


class StoreConfigFile(argparse.Action):
    """
    Custom Action to store the config file location not as part of the CLI args.
    This is because we want to maintain an equivalence between the config file's
    contents and the args themselves.
    """

    trainer_config_path: str

    def __call__(self, arg_parser, namespace, values, option_string=None):
        delattr(namespace, self.dest)
        StoreConfigFile.trainer_config_path = values


def _create_parser() -> argparse.ArgumentParser:
    argparser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    argparser.add_argument(
        "trainer_config_path", action=StoreConfigFile, nargs="?", default=None
    )
    argparser.add_argument(
        "--env",
        default=None,
        dest="env_path",
        help="Path to the Unity executable to train",
        action=DetectDefault,
    )
    argparser.add_argument(
        "--load",
        default=False,
        dest="load_model",
        action=DetectDefaultStoreTrue,
        help=argparse.SUPPRESS,  # Deprecated but still usable for now.
    )
    argparser.add_argument(
        "--resume",
        default=False,
        dest="resume",
        action=DetectDefaultStoreTrue,
        help="Whether to resume training from a checkpoint. Specify a --run-id to use this option. "
        "If set, the training code loads an already trained model to initialize the neural network "
        "before resuming training. This option is only valid when the models exist, and have the same "
        "behavior names as the current agents in your scene.",
    )
    argparser.add_argument(
        "--deterministic",
        default=False,
        dest="deterministic",
        action=DetectDefaultStoreTrue,
        help="Whether to select actions deterministically in policy. `dist.mean` for continuous action "
        "space, and `dist.argmax` for deterministic action space ",
    )
    argparser.add_argument(
        "--force",
        default=False,
        dest="force",
        action=DetectDefaultStoreTrue,
        help="Whether to force-overwrite this run-id's existing summary and model data. (Without "
        "this flag, attempting to train a model with a run-id that has been used before will throw "
        "an error.",
    )
    argparser.add_argument(
        "--run-id",
        default="ppo",
        help="The identifier for the training run. This identifier is used to name the "
        "subdirectories in which the trained model and summary statistics are saved as well "
        "as the saved model itself. If you use TensorBoard to view the training statistics, "
        "always set a unique run-id for each training run. (The statistics for all runs with the "
        "same id are combined as if they were produced by a the same session.)",
        action=DetectDefault,
    )
    argparser.add_argument(
        "--initialize-from",
        metavar="RUN_ID",
        default=None,
        help="Specify a previously saved run ID from which to initialize the model from. "
        "This can be used, for instance, to fine-tune an existing model on a new environment. "
        "Note that the previously saved models must have the same behavior parameters as your "
        "current environment.",
        action=DetectDefault,
    )
    argparser.add_argument(
        "--seed",
        default=-1,
        type=int,
        help="A number to use as a seed for the random number generator used by the training code",
        action=DetectDefault,
    )
    argparser.add_argument(
        "--train",
        default=False,
        dest="train_model",
        action=DetectDefaultStoreTrue,
        help=argparse.SUPPRESS,
    )
    argparser.add_argument(
        "--inference",
        default=False,
        dest="inference",
        action=DetectDefaultStoreTrue,
        help="Whether to run in Python inference mode (i.e. no training). Use with --resume to load "
        "a model trained with an existing run ID.",
    )
    argparser.add_argument(
        "--base-port",
        default=UnityEnvironment.BASE_ENVIRONMENT_PORT,
        type=int,
        help="The starting port for environment communication. Each concurrent Unity environment "
        "instance will get assigned a port sequentially, starting from the base-port. Each instance "
        "will use the port (base_port + worker_id), where the worker_id is sequential IDs given to "
        "each instance from 0 to (num_envs - 1). Note that when training using the Editor rather "
        "than an executable, the base port will be ignored.",
        action=DetectDefault,
    )
    argparser.add_argument(
        "--num-envs",
        default=1,
        type=int,
        help="The number of concurrent Unity environment instances to collect experiences "
        "from when training",
        action=DetectDefault,
    )

    argparser.add_argument(
        "--num-areas",
        default=1,
        type=int,
        help="The number of parallel training areas in each Unity environment instance.",
        action=DetectDefault,
    )

    argparser.add_argument(
        "--debug",
        default=False,
        action=DetectDefaultStoreTrue,
        help="Whether to enable debug-level logging for some parts of the code",
    )
    argparser.add_argument(
        "--env-args",
        default=None,
        nargs=argparse.REMAINDER,
        help="Arguments passed to the Unity executable. Be aware that the standalone build will also "
        "process these as Unity Command Line Arguments. You should choose different argument names if "
        "you want to create environment-specific arguments. All arguments after this flag will be "
        "passed to the executable.",
        action=DetectDefault,
    )
    argparser.add_argument(
        "--max-lifetime-restarts",
        default=10,
        help="The max number of times a single Unity executable can crash over its lifetime before ml-agents exits. "
        "Can be set to -1 if no limit is desired.",
        action=DetectDefault,
    )
    argparser.add_argument(
        "--restarts-rate-limit-n",
        default=1,
        help="The maximum number of times a single Unity executable can crash over a period of time (period set in "
        "restarts-rate-limit-period-s). Can be set to -1 to not use rate limiting with restarts.",
        action=DetectDefault,
    )
    argparser.add_argument(
        "--restarts-rate-limit-period-s",
        default=60,
        help="The period of time --restarts-rate-limit-n applies to.",
        action=DetectDefault,
    )
    argparser.add_argument(
        "--torch",
        default=False,
        action=RaiseRemovedWarning,
        help="(Removed) Use the PyTorch framework.",
    )
    argparser.add_argument(
        "--tensorflow",
        default=False,
        action=RaiseRemovedWarning,
        help="(Removed) Use the TensorFlow framework.",
    )
    argparser.add_argument(
        "--results-dir",
        default="results",
        action=DetectDefault,
        help="Results base directory",
    )

    argparser.add_argument(
        "--timeout-wait",
        default=60,
        help="The period of time to wait on a Unity environment to startup for training.",
        action=DetectDefault,
    )

    eng_conf = argparser.add_argument_group(title="Engine Configuration")
    eng_conf.add_argument(
        "--width",
        default=84,
        type=int,
        help="The width of the executable window of the environment(s) in pixels "
        "(ignored for editor training).",
        action=DetectDefault,
    )
    eng_conf.add_argument(
        "--height",
        default=84,
        type=int,
        help="The height of the executable window of the environment(s) in pixels "
        "(ignored for editor training)",
        action=DetectDefault,
    )
    eng_conf.add_argument(
        "--quality-level",
        default=5,
        type=int,
        help="The quality level of the environment(s). Equivalent to calling "
        "QualitySettings.SetQualityLevel in Unity.",
        action=DetectDefault,
    )
    eng_conf.add_argument(
        "--time-scale",
        default=20,
        type=float,
        help="The time scale of the Unity environment(s). Equivalent to setting "
        "Time.timeScale in Unity.",
        action=DetectDefault,
    )
    eng_conf.add_argument(
        "--target-frame-rate",
        default=-1,
        type=int,
        help="The target frame rate of the Unity environment(s). Equivalent to setting "
        "Application.targetFrameRate in Unity.",
        action=DetectDefault,
    )
    eng_conf.add_argument(
        "--capture-frame-rate",
        default=60,
        type=int,
        help="The capture frame rate of the Unity environment(s). Equivalent to setting "
        "Time.captureFramerate in Unity.",
        action=DetectDefault,
    )
    eng_conf.add_argument(
        "--no-graphics",
        default=False,
        action=DetectDefaultStoreTrue,
        help="Whether to run the Unity executable in no-graphics mode (i.e. without initializing "
        "the graphics driver. Use this only if your agents don't use visual observations.",
    )

    eng_conf.add_argument(
        "--no-graphics-monitor",
        default=False,
        action=DetectDefaultStoreTrue,
        help="Whether to run the main Unity worker in graphics mode with the remaining workers in no graphics mode"
        "(i.e. without initializing the graphics driver. Use this only if your agents don't use visual "
        "observations.",
    )

    torch_conf = argparser.add_argument_group(title="Torch Configuration")
    torch_conf.add_argument(
        "--torch-device",
        default=None,
        dest="device",
        action=DetectDefault,
        help='Settings for the default torch.device used in training, for example, "cpu", "cuda", or "cuda:0"',
    )
    return argparser


def load_config(config_path: str) -> Dict[str, Any]:
    try:
        with open(config_path) as data_file:
            return _load_config(data_file)
    except OSError:
        abs_path = os.path.abspath(config_path)
        raise TrainerConfigError(f"Config file could not be found at {abs_path}.")
    except UnicodeDecodeError:
        raise TrainerConfigError(
            f"There was an error decoding Config file from {config_path}. "
            f"Make sure your file is save using UTF-8"
        )


def _load_config(fp: TextIO) -> Dict[str, Any]:
    """
    Load the yaml config from the file-like object.
    """
    try:
        return yaml.safe_load(fp)
    except yaml.parser.ParserError as e:
        raise TrainerConfigError(
            "Error parsing yaml file. Please check for formatting errors. "
            "A tool such as http://www.yamllint.com/ can be helpful with this."
        ) from e


parser = _create_parser()


--- ml-agents-envs/README.md ---
# Unity ML-Agents Python Interface

The `mlagents_envs` Python package is part of the
[ML-Agents Toolkit](https://github.com/Unity-Technologies/ml-agents).
`mlagents_envs` provides three Python APIs that allows direct interaction with the
Unity game engine:
- A single agent API (Gym API)
- A gym-like multi-agent API (PettingZoo API)
- A low-level API (LLAPI)

The LLAPI is used by the trainer implementation in `mlagents`.
`mlagents_envs` can be used independently of `mlagents` for Python
communication.

## Installation

Install the `mlagents_envs` package with:

```sh
python -m pip install mlagents_envs==1.1.0
```

## Usage & More Information

See
- [Gym API Guide](../com.unity.ml-agents/Documentation~/Python-Gym-API.md)
- [PettingZoo API Guide](../com.unity.ml-agents/Documentation~/Python-PettingZoo-API.md)
- [Python API Guide](../com.unity.ml-agents/Documentation~/Python-LLAPI.md)

for more information on how to use the API to interact with a Unity environment.

For more information on the ML-Agents Toolkit and how to instrument a Unity
scene with the ML-Agents SDK, check out the main
[ML-Agents Toolkit documentation](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest).

## Limitations

- `mlagents_envs` uses localhost ports to exchange data between Unity and
  Python. As such, multiple instances can have their ports collide, leading to
  errors. Make sure to use a different port if you are using multiple instances
  of `UnityEnvironment`.
- Communication between Unity and the Python `UnityEnvironment` is not secure.
- On Linux, ports are not released immediately after the communication closes.
  As such, you cannot reuse ports right after closing a `UnityEnvironment`.


## Links discovered
- [ML-Agents Toolkit](https://github.com/Unity-Technologies/ml-agents)
- [Gym API Guide](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Python-Gym-API.md)
- [PettingZoo API Guide](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Python-PettingZoo-API.md)
- [Python API Guide](https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Documentation~/Python-LLAPI.md)
- [ML-Agents Toolkit documentation](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest)

--- ml-agents-envs/setup.py ---
import os
import sys
from setuptools import setup, find_packages
from setuptools.command.install import install
import mlagents_envs

VERSION = mlagents_envs.__version__
EXPECTED_TAG = mlagents_envs.__release_tag__

here = os.path.abspath(os.path.dirname(__file__))


class VerifyVersionCommand(install):
    """
    Custom command to verify that the git tag is the expected one for the release.
    Originally based on https://circleci.com/blog/continuously-deploying-python-packages-to-pypi-with-circleci/
    This differs slightly because our tags and versions are different.
    """

    description = "verify that the git tag matches our version"

    def run(self):
        tag = os.getenv("GITHUB_REF", "NO GITHUB TAG!").replace("refs/tags/", "")

        if tag != EXPECTED_TAG:
            info = "Git tag: {} does not match the expected tag of this app: {}".format(
                tag, EXPECTED_TAG
            )
            sys.exit(info)


# Get the long description from the README file
with open(os.path.join(here, "README.md"), encoding="utf-8") as f:
    long_description = f.read()

setup(
    name="mlagents_envs",
    version=VERSION,
    description="Unity Machine Learning Agents Interface",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/Unity-Technologies/ml-agents",
    author="Unity Technologies",
    author_email="ML-Agents@unity3d.com",
    classifiers=[
        "Intended Audience :: Developers",
        "Topic :: Scientific/Engineering :: Artificial Intelligence",
        "License :: OSI Approved :: Apache Software License",
        "Programming Language :: Python :: 3.10",
    ],
    packages=find_packages(
        exclude=["*.tests", "*.tests.*", "tests.*", "tests", "colabs", "*.ipynb"]
    ),
    zip_safe=False,
    install_requires=[
        "cloudpickle",
        "grpcio>=1.11.0,<=1.53.2",
        "Pillow>=4.2.1",
        "protobuf>=3.6,<3.21",
        "pyyaml>=3.1.0",
        "gym>=0.21.0",
        "pettingzoo==1.15.0",
        "numpy>=1.23.5,<1.24.0",
        "filelock>=3.4.0",
    ],
    python_requires=">=3.10.1,<=3.10.12",
    # TODO: Remove this once mypy stops having spurious setuptools issues.
    cmdclass={"verify": VerifyVersionCommand},  # type: ignore
)


--- ml-agents-envs/mlagents_envs/base_env.py ---
"""
Python Environment API for the ML-Agents Toolkit
The aim of this API is to expose Agents evolving in a simulation
to perform reinforcement learning on.
This API supports multi-agent scenarios and groups similar Agents (same
observations, actions spaces and behavior) together. These groups of Agents are
identified by their BehaviorName.
For performance reasons, the data of each group of agents is processed in a
batched manner. Agents are identified by a unique AgentId identifier that
allows tracking of Agents across simulation steps. Note that there is no
guarantee that the number or order of the Agents in the state will be
consistent across simulation steps.
A simulation steps corresponds to moving the simulation forward until at least
one agent in the simulation sends its observations to Python again. Since
Agents can request decisions at different frequencies, a simulation step does
not necessarily correspond to a fixed simulation time increment.
"""

from abc import ABC, abstractmethod
from collections.abc import Mapping
from typing import (
    List,
    NamedTuple,
    Tuple,
    Optional,
    Dict,
    Iterator,
    Any,
    Mapping as MappingType,
)
from enum import IntFlag, Enum
import numpy as np

from mlagents_envs.exception import UnityActionException

AgentId = int
GroupId = int
BehaviorName = str


class DecisionStep(NamedTuple):
    """
    Contains the data a single Agent collected since the last
    simulation step.
     - obs is a list of numpy arrays observations collected by the agent.
     - reward is a float. Corresponds to the rewards collected by the agent
     since the last simulation step.
     - agent_id is an int and an unique identifier for the corresponding Agent.
     - action_mask is an optional list of one dimensional array of booleans.
     Only available when using multi-discrete actions.
     Each array corresponds to an action branch. Each array contains a mask
     for each action of the branch. If true, the action is not available for
     the agent during this simulation step.
    """

    obs: List[np.ndarray]
    reward: float
    agent_id: AgentId
    action_mask: Optional[List[np.ndarray]]
    group_id: int
    group_reward: float


class DecisionSteps(Mapping):
    """
    Contains the data a batch of similar Agents collected since the last
    simulation step. Note that all Agents do not necessarily have new
    information to send at each simulation step. Therefore, the ordering of
    agents and the batch size of the DecisionSteps are not fixed across
    simulation steps.
     - obs is a list of numpy arrays observations collected by the batch of
     agent. Each obs has one extra dimension compared to DecisionStep: the
     first dimension of the array corresponds to the batch size of the batch.
     - reward is a float vector of length batch size. Corresponds to the
     rewards collected by each agent since the last simulation step.
     - agent_id is an int vector of length batch size containing unique
     identifier for the corresponding Agent. This is used to track Agents
     across simulation steps.
     - action_mask is an optional list of two dimensional array of booleans.
     Only available when using multi-discrete actions.
     Each array corresponds to an action branch. The first dimension of each
     array is the batch size and the second contains a mask for each action of
     the branch. If true, the action is not available for the agent during
     this simulation step.
    """

    def __init__(self, obs, reward, agent_id, action_mask, group_id, group_reward):
        self.obs: List[np.ndarray] = obs
        self.reward: np.ndarray = reward
        self.agent_id: np.ndarray = agent_id
        self.action_mask: Optional[List[np.ndarray]] = action_mask
        self.group_id: np.ndarray = group_id
        self.group_reward: np.ndarray = group_reward
        self._agent_id_to_index: Optional[Dict[AgentId, int]] = None

    @property
    def agent_id_to_index(self) -> Dict[AgentId, int]:
        """
        :returns: A Dict that maps agent_id to the index of those agents in
        this DecisionSteps.
        """
        if self._agent_id_to_index is None:
            self._agent_id_to_index = {}
            for a_idx, a_id in enumerate(self.agent_id):
                self._agent_id_to_index[a_id] = a_idx
        return self._agent_id_to_index

    def __len__(self) -> int:
        return len(self.agent_id)

    def __getitem__(self, agent_id: AgentId) -> DecisionStep:
        """
        returns the DecisionStep for a specific agent.
        :param agent_id: The id of the agent
        :returns: The DecisionStep
        """
        if agent_id not in self.agent_id_to_index:
            raise KeyError(f"agent_id {agent_id} is not present in the DecisionSteps")
        agent_index = self._agent_id_to_index[agent_id]  # type: ignore
        agent_obs = []
        for batched_obs in self.obs:
            agent_obs.append(batched_obs[agent_index])
        agent_mask = None
        if self.action_mask is not None:
            agent_mask = []
            for mask in self.action_mask:
                agent_mask.append(mask[agent_index])
        group_id = self.group_id[agent_index]
        return DecisionStep(
            obs=agent_obs,
            reward=self.reward[agent_index],
            agent_id=agent_id,
            action_mask=agent_mask,
            group_id=group_id,
            group_reward=self.group_reward[agent_index],
        )

    def __iter__(self) -> Iterator[Any]:
        yield from self.agent_id

    @staticmethod
    def empty(spec: "BehaviorSpec") -> "DecisionSteps":
        """
        Returns an empty DecisionSteps.
        :param spec: The BehaviorSpec for the DecisionSteps
        """
        obs: List[np.ndarray] = []
        for sen_spec in spec.observation_specs:
            obs += [np.zeros((0,) + sen_spec.shape, dtype=np.float32)]
        return DecisionSteps(
            obs=obs,
            reward=np.zeros(0, dtype=np.float32),
            agent_id=np.zeros(0, dtype=np.int32),
            action_mask=None,
            group_id=np.zeros(0, dtype=np.int32),
            group_reward=np.zeros(0, dtype=np.float32),
        )


class TerminalStep(NamedTuple):
    """
    Contains the data a single Agent collected when its episode ended.
     - obs is a list of numpy arrays observations collected by the agent.
     - reward is a float. Corresponds to the rewards collected by the agent
     since the last simulation step.
     - interrupted is a bool. Is true if the Agent was interrupted since the last
     decision step. For example, if the Agent reached the maximum number of steps for
     the episode.
     - agent_id is an int and an unique identifier for the corresponding Agent.
    """

    obs: List[np.ndarray]
    reward: float
    interrupted: bool
    agent_id: AgentId
    group_id: GroupId
    group_reward: float


class TerminalSteps(Mapping):
    """
    Contains the data a batch of Agents collected when their episode
    terminated. All Agents present in the TerminalSteps have ended their
    episode.
     - obs is a list of numpy arrays observations collected by the batch of
     agent. Each obs has one extra dimension compared to DecisionStep: the
     first dimension of the array corresponds to the batch size of the batch.
     - reward is a float vector of length batch size. Corresponds to the
     rewards collected by each agent since the last simulation step.
     - interrupted is an array of booleans of length batch size. Is true if the
     associated Agent was interrupted since the last decision step. For example, if the
     Agent reached the maximum number of steps for the episode.
     - agent_id is an int vector of length batch size containing unique
     identifier for the corresponding Agent. This is used to track Agents
     across simulation steps.
    """

    def __init__(self, obs, reward, interrupted, agent_id, group_id, group_reward):
        self.obs: List[np.ndarray] = obs
        self.reward: np.ndarray = reward
        self.interrupted: np.ndarray = interrupted
        self.agent_id: np.ndarray = agent_id
        self.group_id: np.ndarray = group_id
        self.group_reward: np.ndarray = group_reward
        self._agent_id_to_index: Optional[Dict[AgentId, int]] = None

    @property
    def agent_id_to_index(self) -> Dict[AgentId, int]:
        """
        :returns: A Dict that maps agent_id to the index of those agents in
        this TerminalSteps.
        """
        if self._agent_id_to_index is None:
            self._agent_id_to_index = {}
            for a_idx, a_id in enumerate(self.agent_id):
                self._agent_id_to_index[a_id] = a_idx
        return self._agent_id_to_index

    def __len__(self) -> int:
        return len(self.agent_id)

    def __getitem__(self, agent_id: AgentId) -> TerminalStep:
        """
        returns the TerminalStep for a specific agent.
        :param agent_id: The id of the agent
        :returns: obs, reward, done, agent_id and optional action mask for a
        specific agent
        """
        if agent_id not in self.agent_id_to_index:
            raise KeyError(f"agent_id {agent_id} is not present in the TerminalSteps")
        agent_index = self._agent_id_to_index[agent_id]  # type: ignore
        agent_obs = []
        for batched_obs in self.obs:
            agent_obs.append(batched_obs[agent_index])
        group_id = self.group_id[agent_index]
        return TerminalStep(
            obs=agent_obs,
            reward=self.reward[agent_index],
            interrupted=self.interrupted[agent_index],
            agent_id=agent_id,
            group_id=group_id,
            group_reward=self.group_reward[agent_index],
        )

    def __iter__(self) -> Iterator[Any]:
        yield from self.agent_id

    @staticmethod
    def empty(spec: "BehaviorSpec") -> "TerminalSteps":
        """
        Returns an empty TerminalSteps.
        :param spec: The BehaviorSpec for the TerminalSteps
        """
        obs: List[np.ndarray] = []
        for sen_spec in spec.observation_specs:
            obs += [np.zeros((0,) + sen_spec.shape, dtype=np.float32)]
        return TerminalSteps(
            obs=obs,
            reward=np.zeros(0, dtype=np.float32),
            interrupted=np.zeros(0, dtype=bool),
            agent_id=np.zeros(0, dtype=np.int32),
            group_id=np.zeros(0, dtype=np.int32),
            group_reward=np.zeros(0, dtype=np.float32),
        )


class _ActionTupleBase(ABC):
    """
    An object whose fields correspond to action data of continuous and discrete
    spaces. Dimensions are of (n_agents, continuous_size) and (n_agents, discrete_size),
    respectively. Note, this also holds when continuous or discrete size is
    zero.
    """

    def __init__(
        self,
        continuous: Optional[np.ndarray] = None,
        discrete: Optional[np.ndarray] = None,
    ):
        self._continuous: Optional[np.ndarray] = None
        self._discrete: Optional[np.ndarray] = None
        if continuous is not None:
            self.add_continuous(continuous)
        if discrete is not None:
            self.add_discrete(discrete)

    @property
    def continuous(self) -> np.ndarray:
        return self._continuous

    @property
    def discrete(self) -> np.ndarray:
        return self._discrete

    def add_continuous(self, continuous: np.ndarray) -> None:
        if continuous.dtype != np.float32:
            continuous = continuous.astype(np.float32, copy=False)
        if self._discrete is None:
            self._discrete = np.zeros(
                (continuous.shape[0], 0), dtype=self.discrete_dtype
            )
        self._continuous = continuous

    def add_discrete(self, discrete: np.ndarray) -> None:
        if discrete.dtype != self.discrete_dtype:
            discrete = discrete.astype(self.discrete_dtype, copy=False)
        if self._continuous is None:
            self._continuous = np.zeros((discrete.shape[0], 0), dtype=np.float32)
        self._discrete = discrete

    @property
    @abstractmethod
    def discrete_dtype(self) -> np.dtype:
        pass


class ActionTuple(_ActionTupleBase):
    """
    An object whose fields correspond to actions of different types.
    Continuous and discrete actions are numpy arrays of type float32 and
    int32, respectively and are type checked on construction.
    Dimensions are of (n_agents, continuous_size) and (n_agents, discrete_size),
    respectively. Note, this also holds when continuous or discrete size is
    zero.
    """

    @property
    def discrete_dtype(self) -> np.dtype:
        """
        The dtype of a discrete action.
        """
        return np.int32


class ActionSpec(NamedTuple):
    """
    A NamedTuple containing utility functions and information about the action spaces
    for a group of Agents under the same behavior.
    - num_continuous_actions is an int corresponding to the number of floats which
    constitute the action.
    - discrete_branch_sizes is a Tuple of int where each int corresponds to
    the number of discrete actions available to the agent on an independent action branch.
    """

    continuous_size: int
    discrete_branches: Tuple[int, ...]

    def __eq__(self, other):
        return (
            self.continuous_size == other.continuous_size
            and self.discrete_branches == other.discrete_branches
        )

    def __str__(self):
        return f"Continuous: {self.continuous_size}, Discrete: {self.discrete_branches}"

    # For backwards compatibility
    def is_discrete(self) -> bool:
        """
        Returns true if this Behavior uses discrete actions
        """
        return self.discrete_size > 0 and self.continuous_size == 0

    # For backwards compatibility
    def is_continuous(self) -> bool:
        """
        Returns true if this Behavior uses continuous actions
        """
        return self.discrete_size == 0 and self.continuous_size > 0

    @property
    def discrete_size(self) -> int:
        """
        Returns a an int corresponding to the number of discrete branches.
        """
        return len(self.discrete_branches)

    def empty_action(self, n_agents: int) -> ActionTuple:
        """
        Generates ActionTuple corresponding to an empty action (all zeros)
        for a number of agents.
        :param n_agents: The number of agents that will have actions generated
        """
        _continuous = np.zeros((n_agents, self.continuous_size), dtype=np.float32)
        _discrete = np.zeros((n_agents, self.discrete_size), dtype=np.int32)
        return ActionTuple(continuous=_continuous, discrete=_discrete)

    def random_action(self, n_agents: int) -> ActionTuple:
        """
        Generates ActionTuple corresponding to a random action (either discrete
        or continuous) for a number of agents.
        :param n_agents: The number of agents that will have actions generated
        """
        _continuous = np.random.uniform(
            low=-1.0, high=1.0, size=(n_agents, self.continuous_size)
        )
        _discrete = np.zeros((n_agents, self.discrete_size), dtype=np.int32)
        if self.discrete_size > 0:
            _discrete = np.column_stack(
                [
                    np.random.randint(
                        0,
                        self.discrete_branches[i],  # type: ignore
                        size=(n_agents),
                        dtype=np.int32,
                    )
                    for i in range(self.discrete_size)
                ]
            )
        return ActionTuple(continuous=_continuous, discrete=_discrete)

    def _validate_action(
        self, actions: ActionTuple, n_agents: int, name: str
    ) -> ActionTuple:
        """
        Validates that action has the correct action dim
        for the correct number of agents and ensures the type.
        """
        _expected_shape = (n_agents, self.continuous_size)
        if actions.continuous.shape != _expected_shape:
            raise UnityActionException(
                f"The behavior {name} needs a continuous input of dimension "
                f"{_expected_shape} for (<number of agents>, <action size>) but "
                f"received input of dimension {actions.continuous.shape}"
            )
        _expected_shape = (n_agents, self.discrete_size)
        if actions.discrete.shape != _expected_shape:
            raise UnityActionException(
                f"The behavior {name} needs a discrete input of dimension "
                f"{_expected_shape} for (<number of agents>, <action size>) but "
                f"received input of dimension {actions.discrete.shape}"
            )
        return actions

    @staticmethod
    def create_continuous(continuous_size: int) -> "ActionSpec":
        """
        Creates an ActionSpec that is homogenously continuous
        """
        return ActionSpec(continuous_size, ())

    @staticmethod
    def create_discrete(discrete_branches: Tuple[int]) -> "ActionSpec":
        """
        Creates an ActionSpec that is homogenously discrete
        """
        return ActionSpec(0, discrete_branches)

    @staticmethod
    def create_hybrid(
        continuous_size: int, discrete_branches: Tuple[int]
    ) -> "ActionSpec":
        """
        Creates a hybrid ActionSpace
        """
        return ActionSpec(continuous_size, discrete_branches)


class DimensionProperty(IntFlag):
    """
    The dimension property of a dimension of an observation.
    """

    UNSPECIFIED = 0
    """
    No properties specified.
    """

    NONE = 1
    """
    No Property of the observation in that dimension. Observation can be processed with
    Fully connected networks.
    """

    TRANSLATIONAL_EQUIVARIANCE = 2
    """
    Means it is suitable to do a convolution in this dimension.
    """

    VARIABLE_SIZE = 4
    """
    Means that there can be a variable number of observations in this dimension.
    The observations are unordered.
    """


class ObservationType(Enum):
    """
    An Enum which defines the type of information carried in the observation
    of the agent.
    """

    DEFAULT = 0
    """
    Observation information is generic.
    """

    GOAL_SIGNAL = 1
    """
    Observation contains goal information for current task.
    """


class ObservationSpec(NamedTuple):
    """
    A NamedTuple containing information about the observation of Agents.
    - shape is a Tuple of int : It corresponds to the shape of
    an observation's dimensions.
    - dimension_property is a Tuple of DimensionProperties flag, one flag for each
    dimension.
    - observation_type is an enum of ObservationType.
    """

    shape: Tuple[int, ...]
    dimension_property: Tuple[DimensionProperty, ...]
    observation_type: ObservationType

    # Optional name. For observations coming from com.unity.ml-agents, this
    # will be the ISensor name.
    name: str


class BehaviorSpec(NamedTuple):
    """
    A NamedTuple containing information about the observation and action
    spaces for a group of Agents under the same behavior.
    - observation_specs is a List of ObservationSpec NamedTuple containing
    information about the information of the Agent's observations such as their shapes.
    The order of the ObservationSpec is the same as the order of the observations of an
    agent.
    - action_spec is an ActionSpec NamedTuple.
    """

    observation_specs: List[ObservationSpec]
    action_spec: ActionSpec


class BehaviorMapping(Mapping):
    def __init__(self, specs: Dict[BehaviorName, BehaviorSpec]):
        self._dict = specs

    def __len__(self) -> int:
        return len(self._dict)

    def __getitem__(self, behavior: BehaviorName) -> BehaviorSpec:
        return self._dict[behavior]

    def __iter__(self) -> Iterator[Any]:
        yield from self._dict


class BaseEnv(ABC):
    @abstractmethod
    def step(self) -> None:
        """
        Signals the environment that it must move the simulation forward
        by one step.
        """

    @abstractmethod
    def reset(self) -> None:
        """
        Signals the environment that it must reset the simulation.
        """

    @abstractmethod
    def close(self) -> None:
        """
        Signals the environment that it must close.
        """

    @property
    @abstractmethod
    def behavior_specs(self) -> MappingType[str, BehaviorSpec]:
        """
        Returns a Mapping from behavior names to behavior specs.
        Agents grouped under the same behavior name have the same action and
        observation specs, and are expected to behave similarly in the
        environment.
        Note that new keys can be added to this mapping as new policies are instantiated.
        """

    @abstractmethod
    def set_actions(self, behavior_name: BehaviorName, action: ActionTuple) -> None:
        """
        Sets the action for all of the agents in the simulation for the next
        step. The Actions must be in the same order as the order received in
        the DecisionSteps.
        :param behavior_name: The name of the behavior the agents are part of
        :param action: ActionTuple tuple of continuous and/or discrete action.
        Actions are np.arrays with dimensions  (n_agents, continuous_size) and
        (n_agents, discrete_size), respectively.
        """

    @abstractmethod
    def set_action_for_agent(
        self, behavior_name: BehaviorName, agent_id: AgentId, action: ActionTuple
    ) -> None:
        """
        Sets the action for one of the agents in the simulation for the next
        step.
        :param behavior_name: The name of the behavior the agent is part of
        :param agent_id: The id of the agent the action is set for
        :param action: ActionTuple tuple of continuous and/or discrete action
        Actions are np.arrays with dimensions  (1, continuous_size) and
        (1, discrete_size), respectively. Note, this initial dimensions of 1 is because
        this action is meant for a single agent.
        """

    @abstractmethod
    def get_steps(
        self, behavior_name: BehaviorName
    ) -> Tuple[DecisionSteps, TerminalSteps]:
        """
        Retrieves the steps of the agents that requested a step in the
        simulation.
        :param behavior_name: The name of the behavior the agents are part of
        :return: A tuple containing :
         - A DecisionSteps NamedTuple containing the observations,
         the rewards, the agent ids and the action masks for the Agents
         of the specified behavior. These Agents need an action this step.
         - A TerminalSteps NamedTuple containing the observations,
         rewards, agent ids and interrupted flags of the agents that had their
         episode terminated last step.
        """


--- ml-agents-envs/colabs/Colab_PettingZoo.ipynb ---
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Agents PettingZoo Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install Rendering Dependencies { display-mode: \"form\" }\n",
    "#@markdown (You only need to run this code when using Colab's hosted runtime)\n",
    "\n",
    "import os\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def progress(value, max=100):\n",
    "    return HTML(\"\"\"\n",
    "        <progress\n",
    "            value='{value}'\n",
    "            max='{max}',\n",
    "            style='width: 100%'\n",
    "        >\n",
    "            {value}\n",
    "        </progress>\n",
    "    \"\"\".format(value=value, max=max))\n",
    "\n",
    "pro_bar = display(progress(0, 100), display_id=True)\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  INSTALL_XVFB = True\n",
    "except ImportError:\n",
    "  INSTALL_XVFB = 'COLAB_ALWAYS_INSTALL_XVFB' in os.environ\n",
    "\n",
    "if INSTALL_XVFB:\n",
    "  with open('frame-buffer', 'w') as writefile:\n",
    "    writefile.write(\"\"\"#taken from https://gist.github.com/jterrace/2911875\n",
    "XVFB=/usr/bin/Xvfb\n",
    "XVFBARGS=\":1 -screen 0 1024x768x24 -ac +extension GLX +render -noreset\"\n",
    "PIDFILE=./frame-buffer.pid\n",
    "case \"$1\" in\n",
    "  start)\n",
    "    echo -n \"Starting virtual X frame buffer: Xvfb\"\n",
    "    /sbin/start-stop-daemon --start --quiet --pidfile $PIDFILE --make-pidfile --background --exec $XVFB -- $XVFBARGS\n",
    "    echo \".\"\n",
    "    ;;\n",
    "  stop)\n",
    "    echo -n \"Stopping virtual X frame buffer: Xvfb\"\n",
    "    /sbin/start-stop-daemon --stop --quiet --pidfile $PIDFILE\n",
    "    rm $PIDFILE\n",
    "    echo \".\"\n",
    "    ;;\n",
    "  restart)\n",
    "    $0 stop\n",
    "    $0 start\n",
    "    ;;\n",
    "  *)\n",
    "        echo \"Usage: /etc/init.d/xvfb {start|stop|restart}\"\n",
    "        exit 1\n",
    "esac\n",
    "exit 0\n",
    "    \"\"\")\n",
    "  pro_bar.update(progress(5, 100))\n",
    "  !apt-get install daemon >/dev/null 2>&1\n",
    "  pro_bar.update(progress(10, 100))\n",
    "  !apt-get install wget >/dev/null 2>&1\n",
    "  pro_bar.update(progress(20, 100))\n",
    "  !wget http://security.ubuntu.com/ubuntu/pool/main/libx/libxfont/libxfont1_1.5.1-1ubuntu0.16.04.4_amd64.deb >/dev/null 2>&1\n",
    "  pro_bar.update(progress(30, 100))\n",
    "  !wget --output-document xvfb.deb http://security.ubuntu.com/ubuntu/pool/universe/x/xorg-server/xvfb_1.18.4-0ubuntu0.12_amd64.deb >/dev/null 2>&1\n",
    "  pro_bar.update(progress(40, 100))\n",
    "  !dpkg -i libxfont1_1.5.1-1ubuntu0.16.04.4_amd64.deb >/dev/null 2>&1\n",
    "  pro_bar.update(progress(50, 100))\n",
    "  !dpkg -i xvfb.deb >/dev/null 2>&1\n",
    "  pro_bar.update(progress(70, 100))\n",
    "  !rm libxfont1_1.5.1-1ubuntu0.16.04.4_amd64.deb\n",
    "  pro_bar.update(progress(80, 100))\n",
    "  !rm xvfb.deb\n",
    "  pro_bar.update(progress(90, 100))\n",
    "  !bash frame-buffer start\n",
    "  os.environ[\"DISPLAY\"] = \":1\"\n",
    "pro_bar.update(progress(100, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing ml-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import mlagents\n",
    "  print(\"ml-agents already installed\")\n",
    "except ImportError:\n",
    "  !git clone -b main --single-branch https://github.com/Unity-Technologies/ml-agents.git\n",
    "  !python -m pip install -q ./ml-agents/ml-agents-envs\n",
    "  !python -m pip install -q ./ml-agents/ml-agents\n",
    "  print(\"Installed ml-agents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "List of available environments:\n",
    "* Basic\n",
    "* ThreeDBall\n",
    "* ThreeDBallHard\n",
    "* GridWorld\n",
    "* Hallway\n",
    "* VisualHallway\n",
    "* CrawlerDynamicTarget\n",
    "* CrawlerStaticTarget\n",
    "* Bouncer\n",
    "* SoccerTwos\n",
    "* PushBlock\n",
    "* VisualPushBlock\n",
    "* WallJump\n",
    "* Tennis\n",
    "* Reacher\n",
    "* Pyramids\n",
    "* VisualPyramids\n",
    "* Walker\n",
    "* FoodCollector\n",
    "* VisualFoodCollector\n",
    "* StrikersVsGoalie\n",
    "* WormStaticTarget\n",
    "* WormDynamicTarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Environment with PettingZoo Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSf-WhxbqtLw"
   },
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# This code is used to close an env that might not have been closed before\n",
    "try:\n",
    "  env.close()\n",
    "except:\n",
    "  pass\n",
    "# -----------------\n",
    "\n",
    "import numpy as np\n",
    "from mlagents_envs.envs import StrikersVsGoalie # import unity environment\n",
    "env = StrikersVsGoalie.env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepping the environment\n",
    "\n",
    "Example of interacting with the environment in basic RL loop. It follows the same interface as described in [PettingZoo API page](https://pettingzoo.farama.org/api/aec/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhtl0mpeqxYi"
   },
   "outputs": [],
   "source": [
    "num_cycles = 10\n",
    "\n",
    "env.reset()\n",
    "for agent in env.agent_iter(env.num_agents * num_cycles):\n",
    "    prev_observe, reward, done, info = env.last()\n",
    "    if isinstance(prev_observe, dict) and 'action_mask' in prev_observe:\n",
    "        action_mask = prev_observe['action_mask']\n",
    "    if done:\n",
    "        action = None\n",
    "    else:\n",
    "        action = env.action_spaces[agent].sample() # randomly choose an action for example\n",
    "    env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Environment API\n",
    "\n",
    "All the API described in the `Additional Environment API` section in the [PettingZoo API page](https://pettingzoo.farama.org/api/aec/) are all supported. A few examples are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# `agents`: a list of the names of all current agents\n",
    "print(\"Agent names:\", env.agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `agent_selection`: the currently agent that an action can be taken for.\n",
    "print(\"Current agent:\", env.agent_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `observation_spaces`: a dict of the observation spaces of every agent, keyed by name.\n",
    "print(\"Observation space of current agent:\", env.observation_spaces[env.agent_selection])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `action_spaces`: a dict of the observation spaces of every agent, keyed by name.\n",
    "print(\"Action space of current agent:\", env.action_spaces[env.agent_selection])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the Environment to free the port it is using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7KatdThq7OV"
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Colab-UnityEnvironment-1-Run.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}


## Links discovered
- ["### Stepping the environment\n",
    "\n",
    "Example of interacting with the environment in basic RL loop. It follows the same interface as described in [PettingZoo API page](https://pettingzoo.farama.org/api/aec/)
- ["### Additional Environment API\n",
    "\n",
    "All the API described in the `Additional Environment API` section in the [PettingZoo API page](https://pettingzoo.farama.org/api/aec/)

--- ml-agents-envs/mlagents_envs/communicator.py ---
from typing import Callable, Optional
from mlagents_envs.communicator_objects.unity_output_pb2 import UnityOutputProto
from mlagents_envs.communicator_objects.unity_input_pb2 import UnityInputProto


# Function to call while waiting for a connection timeout.
# This should raise an exception if it needs to break from waiting for the timeout.
PollCallback = Callable[[], None]


class Communicator:
    def __init__(self, worker_id=0, base_port=5005):
        """
        Python side of the communication. Must be used in pair with the right Unity Communicator equivalent.

        :int worker_id: Offset from base_port. Used for training multiple environments simultaneously.
        :int base_port: Baseline port number to connect to Unity environment over. worker_id increments over this.
        """

    def initialize(
        self, inputs: UnityInputProto, poll_callback: Optional[PollCallback] = None
    ) -> UnityOutputProto:
        """
        Used to exchange initialization parameters between Python and the Environment
        :param inputs: The initialization input that will be sent to the environment.
        :param poll_callback: Optional callback to be used while polling the connection.
        :return: UnityOutput: The initialization output sent by Unity
        """

    def exchange(
        self, inputs: UnityInputProto, poll_callback: Optional[PollCallback] = None
    ) -> Optional[UnityOutputProto]:
        """
        Used to send an input and receive an output from the Environment
        :param inputs: The UnityInput that needs to be sent the Environment
        :param poll_callback: Optional callback to be used while polling the connection.
        :return: The UnityOutputs generated by the Environment
        """

    def close(self):
        """
        Sends a shutdown signal to the unity environment, and closes the connection.
        """


--- ml-agents-envs/tests/dummy_config.py ---
from typing import List, Tuple
from mlagents_envs.base_env import ObservationSpec, DimensionProperty, ObservationType
import pytest
import copy
import os
from mlagents.trainers.settings import (
    TrainerSettings,
    GAILSettings,
    CuriositySettings,
    RewardSignalSettings,
    NetworkSettings,
    RewardSignalType,
    ScheduleType,
)
from mlagents.trainers.ppo.trainer import PPOSettings, TRAINER_NAME as PPO_TRAINER_NAME
from mlagents.trainers.sac.trainer import SACSettings, TRAINER_NAME as SAC_TRAINER_NAME
from mlagents.trainers.poca.trainer import (
    POCASettings,
    TRAINER_NAME as POCA_TRAINER_NAME,
)

CONTINUOUS_DEMO_PATH = os.path.dirname(os.path.abspath(__file__)) + "/test.demo"
DISCRETE_DEMO_PATH = os.path.dirname(os.path.abspath(__file__)) + "/testdcvis.demo"

_PPO_CONFIG = TrainerSettings(
    trainer_type=PPO_TRAINER_NAME,
    hyperparameters=PPOSettings(
        learning_rate=5.0e-3,
        learning_rate_schedule=ScheduleType.CONSTANT,
        batch_size=16,
        buffer_size=64,
    ),
    network_settings=NetworkSettings(num_layers=1, hidden_units=32),
    summary_freq=500,
    max_steps=3000,
    threaded=False,
)

_SAC_CONFIG = TrainerSettings(
    trainer_type=SAC_TRAINER_NAME,
    hyperparameters=SACSettings(
        learning_rate=5.0e-3,
        learning_rate_schedule=ScheduleType.CONSTANT,
        batch_size=8,
        buffer_init_steps=100,
        buffer_size=5000,
        tau=0.01,
        init_entcoef=0.01,
    ),
    network_settings=NetworkSettings(num_layers=1, hidden_units=16),
    summary_freq=100,
    max_steps=1000,
    threaded=False,
)

_POCA_CONFIG = TrainerSettings(
    trainer_type=POCA_TRAINER_NAME,
    hyperparameters=POCASettings(
        learning_rate=5.0e-3,
        learning_rate_schedule=ScheduleType.CONSTANT,
        batch_size=16,
        buffer_size=64,
    ),
    network_settings=NetworkSettings(num_layers=1, hidden_units=32),
    summary_freq=500,
    max_steps=3000,
    threaded=False,
)


def ppo_dummy_config():
    return copy.deepcopy(_PPO_CONFIG)


def sac_dummy_config():
    return copy.deepcopy(_SAC_CONFIG)


def poca_dummy_config():
    return copy.deepcopy(_POCA_CONFIG)


@pytest.fixture
def gail_dummy_config():
    return {RewardSignalType.GAIL: GAILSettings(demo_path=CONTINUOUS_DEMO_PATH)}


@pytest.fixture
def curiosity_dummy_config():
    return {RewardSignalType.CURIOSITY: CuriositySettings()}


@pytest.fixture
def extrinsic_dummy_config():
    return {RewardSignalType.EXTRINSIC: RewardSignalSettings()}


def create_observation_specs_with_shapes(
    shapes: List[Tuple[int, ...]]
) -> List[ObservationSpec]:
    obs_specs: List[ObservationSpec] = []
    for i, shape in enumerate(shapes):
        dim_prop = (DimensionProperty.UNSPECIFIED,) * len(shape)
        if len(shape) == 2:
            dim_prop = (DimensionProperty.VARIABLE_SIZE, DimensionProperty.NONE)
        spec = ObservationSpec(
            name=f"observation {i} with shape {shape}",
            shape=shape,
            dimension_property=dim_prop,
            observation_type=ObservationType.DEFAULT,
        )
        obs_specs.append(spec)
    return obs_specs


--- ml-agents-envs/mlagents_envs/env_utils.py ---
import glob
import os
import subprocess
from sys import platform
from typing import Optional, List
from mlagents_envs.logging_util import get_logger, DEBUG
from mlagents_envs.exception import UnityEnvironmentException


logger = get_logger(__name__)


def get_platform():
    """
    returns the platform of the operating system : linux, darwin or win32
    """
    return platform


def validate_environment_path(env_path: str) -> Optional[str]:
    """
    Strip out executable extensions of the env_path
    :param env_path: The path to the executable
    """
    env_path = (
        env_path.strip()
        .replace(".app", "")
        .replace(".exe", "")
        .replace(".x86_64", "")
        .replace(".x86", "")
    )
    true_filename = os.path.basename(os.path.normpath(env_path))
    logger.debug(f"The true file name is {true_filename}")

    if not (glob.glob(env_path) or glob.glob(env_path + ".*")):
        return None

    cwd = os.getcwd()
    launch_string = None
    true_filename = os.path.basename(os.path.normpath(env_path))
    if get_platform() == "linux" or get_platform() == "linux2":
        candidates = glob.glob(os.path.join(cwd, env_path) + ".x86_64")
        if len(candidates) == 0:
            candidates = glob.glob(os.path.join(cwd, env_path) + ".x86")
        if len(candidates) == 0:
            candidates = glob.glob(env_path + ".x86_64")
        if len(candidates) == 0:
            candidates = glob.glob(env_path + ".x86")
        if len(candidates) == 0:
            if os.path.isfile(env_path):
                candidates = [env_path]
        if len(candidates) > 0:
            launch_string = candidates[0]

    elif get_platform() == "darwin":
        candidates = glob.glob(
            os.path.join(cwd, env_path + ".app", "Contents", "MacOS", true_filename)
        )
        if len(candidates) == 0:
            candidates = glob.glob(
                os.path.join(env_path + ".app", "Contents", "MacOS", true_filename)
            )
        if len(candidates) == 0:
            candidates = glob.glob(
                os.path.join(cwd, env_path + ".app", "Contents", "MacOS", "*")
            )
        if len(candidates) == 0:
            candidates = glob.glob(
                os.path.join(env_path + ".app", "Contents", "MacOS", "*")
            )
        if len(candidates) > 0:
            launch_string = candidates[0]
    elif get_platform() == "win32":
        candidates = glob.glob(os.path.join(cwd, env_path + ".exe"))
        if len(candidates) == 0:
            candidates = glob.glob(env_path + ".exe")
        if len(candidates) == 0:
            # Look for e.g. 3DBall\UnityEnvironment.exe
            crash_handlers = set(
                glob.glob(os.path.join(cwd, env_path, "UnityCrashHandler*.exe"))
            )
            candidates = [
                c
                for c in glob.glob(os.path.join(cwd, env_path, "*.exe"))
                if c not in crash_handlers
            ]
        if len(candidates) > 0:
            launch_string = candidates[0]
    return launch_string


def launch_executable(file_name: str, args: List[str]) -> subprocess.Popen:
    """
    Launches a Unity executable and returns the process handle for it.
    :param file_name: the name of the executable
    :param args: List of string that will be passed as command line arguments
    when launching the executable.
    """
    launch_string = validate_environment_path(file_name)
    if launch_string is None:
        raise UnityEnvironmentException(
            f"Couldn't launch the {file_name} environment. Provided filename does not match any environments."
        )
    else:
        logger.debug(f"The launch string is {launch_string}")
        logger.debug(f"Running with args {args}")
        # Launch Unity environment
        subprocess_args = [launch_string] + args
        # std_out_option = DEVNULL means the outputs will not be displayed on terminal.
        # std_out_option = None is default behavior: the outputs are displayed on terminal.
        std_out_option = subprocess.DEVNULL if logger.level > DEBUG else None
        try:
            return subprocess.Popen(
                subprocess_args,
                # start_new_session=True means that signals to the parent python process
                # (e.g. SIGINT from keyboard interrupt) will not be sent to the new process on POSIX platforms.
                # This is generally good since we want the environment to have a chance to shutdown,
                # but may be undesirable in come cases; if so, we'll add a command-line toggle.
                # Note that on Windows, the CTRL_C signal will still be sent.
                start_new_session=True,
                stdout=std_out_option,
                stderr=std_out_option,
            )
        except PermissionError as perm:
            # This is likely due to missing read or execute permissions on file.
            raise UnityEnvironmentException(
                f"Error when trying to launch environment - make sure "
                f"permissions are set correctly. For example "
                f'"chmod -R 755 {launch_string}"'
            ) from perm


--- ml-agents-envs/mlagents_envs/environment.py ---
import atexit
from distutils.version import StrictVersion

import numpy as np
import os
import subprocess
from typing import Dict, List, Optional, Tuple, Mapping as MappingType

import mlagents_envs

from mlagents_envs.logging_util import get_logger
from mlagents_envs.side_channel.side_channel import SideChannel
from mlagents_envs.side_channel import DefaultTrainingAnalyticsSideChannel
from mlagents_envs.side_channel.side_channel_manager import SideChannelManager
from mlagents_envs import env_utils

from mlagents_envs.base_env import (
    BaseEnv,
    DecisionSteps,
    TerminalSteps,
    BehaviorSpec,
    ActionTuple,
    BehaviorName,
    AgentId,
    BehaviorMapping,
)
from mlagents_envs.timers import timed, hierarchical_timer
from mlagents_envs.exception import (
    UnityEnvironmentException,
    UnityActionException,
    UnityTimeOutException,
    UnityCommunicatorStoppedException,
)

from mlagents_envs.communicator_objects.command_pb2 import STEP, RESET
from mlagents_envs.rpc_utils import behavior_spec_from_proto, steps_from_proto

from mlagents_envs.communicator_objects.unity_rl_input_pb2 import UnityRLInputProto
from mlagents_envs.communicator_objects.unity_rl_output_pb2 import UnityRLOutputProto
from mlagents_envs.communicator_objects.agent_action_pb2 import AgentActionProto
from mlagents_envs.communicator_objects.unity_output_pb2 import UnityOutputProto
from mlagents_envs.communicator_objects.capabilities_pb2 import UnityRLCapabilitiesProto
from mlagents_envs.communicator_objects.unity_rl_initialization_input_pb2 import (
    UnityRLInitializationInputProto,
)

from mlagents_envs.communicator_objects.unity_input_pb2 import UnityInputProto

from .rpc_communicator import RpcCommunicator
import signal

logger = get_logger(__name__)


class UnityEnvironment(BaseEnv):
    # Communication protocol version.
    # When connecting to C#, this must be compatible with Academy.k_ApiVersion.
    # We follow semantic versioning on the communication version, so existing
    # functionality will work as long the major versions match.
    # This should be changed whenever a change is made to the communication protocol.
    # Revision history:
    #  * 1.0.0 - initial version
    #  * 1.1.0 - support concatenated PNGs for compressed observations.
    #  * 1.2.0 - support compression mapping for stacked compressed observations.
    #  * 1.3.0 - support action spaces with both continuous and discrete actions.
    #  * 1.4.0 - support training analytics sent from python trainer to the editor.
    #  * 1.5.0 - support variable length observation training and multi-agent groups.
    API_VERSION = "1.5.0"

    # Default port that the editor listens on. If an environment executable
    # isn't specified, this port will be used.
    DEFAULT_EDITOR_PORT = 5004

    # Default base port for environments. Each environment will be offset from this
    # by it's worker_id.
    BASE_ENVIRONMENT_PORT = 5005

    # Command line argument used to pass the port to the executable environment.
    _PORT_COMMAND_LINE_ARG = "--mlagents-port"

    @staticmethod
    def _raise_version_exception(unity_com_ver: str) -> None:
        raise UnityEnvironmentException(
            f"The communication API version is not compatible between Unity and python. "
            f"Python API: {UnityEnvironment.API_VERSION}, Unity API: {unity_com_ver}.\n "
            f"Please find the versions that work best together from our release page.\n"
            "https://github.com/Unity-Technologies/ml-agents/releases"
        )

    @staticmethod
    def _check_communication_compatibility(
        unity_com_ver: str, python_api_version: str, unity_package_version: str
    ) -> bool:
        unity_communicator_version = StrictVersion(unity_com_ver)
        api_version = StrictVersion(python_api_version)
        if unity_communicator_version.version[0] == 0:
            if (
                unity_communicator_version.version[0] != api_version.version[0]
                or unity_communicator_version.version[1] != api_version.version[1]
            ):
                # Minor beta versions differ.
                return False
        elif unity_communicator_version.version[0] != api_version.version[0]:
            # Major versions mismatch.
            return False
        else:
            # Major versions match, so either:
            # 1) The versions are identical, in which case there's no compatibility issues
            # 2) The Unity version is newer, in which case we'll warn or fail on the Unity side if trying to use
            #    unsupported features
            # 3) The trainer version is newer, in which case new trainer features might be available but unused by C#
            # In any of the cases, there's no reason to warn about mismatch here.
            logger.info(
                f"Connected to Unity environment with package version {unity_package_version} "
                f"and communication version {unity_com_ver}"
            )
        return True

    @staticmethod
    def _get_capabilities_proto() -> UnityRLCapabilitiesProto:
        capabilities = UnityRLCapabilitiesProto()
        capabilities.baseRLCapabilities = True
        capabilities.concatenatedPngObservations = True
        capabilities.compressedChannelMapping = True
        capabilities.hybridActions = True
        capabilities.trainingAnalytics = True
        capabilities.variableLengthObservation = True
        capabilities.multiAgentGroups = True
        return capabilities

    @staticmethod
    def _warn_csharp_base_capabilities(
        caps: UnityRLCapabilitiesProto, unity_package_ver: str, python_package_ver: str
    ) -> None:
        if not caps.baseRLCapabilities:
            logger.warning(
                "WARNING: The Unity process is not running with the expected base Reinforcement Learning"
                " capabilities. Please be sure upgrade the Unity Package to a version that is compatible with this "
                "python package.\n"
                f"Python package version: {python_package_ver}, C# package version: {unity_package_ver}"
                f"Please find the versions that work best together from our release page.\n"
                "https://github.com/Unity-Technologies/ml-agents/releases"
            )

    def __init__(
        self,
        file_name: Optional[str] = None,
        worker_id: int = 0,
        base_port: Optional[int] = None,
        seed: int = 0,
        no_graphics: bool = False,
        no_graphics_monitor: bool = False,
        timeout_wait: int = 60,
        additional_args: Optional[List[str]] = None,
        side_channels: Optional[List[SideChannel]] = None,
        log_folder: Optional[str] = None,
        num_areas: int = 1,
    ):
        """
        Starts a new unity environment and establishes a connection with the environment.
        Notice: Currently communication between Unity and Python takes place over an open socket without authentication.
        Ensure that the network where training takes place is secure.

        :string file_name: Name of Unity environment binary. :int base_port: Baseline port number to connect to Unity
        environment over. worker_id increments over this. If no environment is specified (i.e. file_name is None),
        the DEFAULT_EDITOR_PORT will be used. :int worker_id: Offset from base_port. Used for training multiple
        environments simultaneously. :bool no_graphics: Whether to run the Unity simulator in no-graphics mode :bool
        no_graphics_monitor: Whether to run the main worker in graphics mode, with the remaining in no-graphics mode
        :int timeout_wait: Time (in seconds) to wait for connection from environment. :list args: Addition Unity
        command line arguments :list side_channels: Additional side channel for no-rl communication with Unity :str
        log_folder: Optional folder to write the Unity Player log file into.  Requires absolute path.
        """
        atexit.register(self._close)
        self._additional_args = additional_args or []
        self._no_graphics = no_graphics or no_graphics_monitor and worker_id != 0
        # If base port is not specified, use BASE_ENVIRONMENT_PORT if we have
        # an environment, otherwise DEFAULT_EDITOR_PORT
        if base_port is None:
            base_port = (
                self.BASE_ENVIRONMENT_PORT if file_name else self.DEFAULT_EDITOR_PORT
            )
        self._port = base_port + worker_id
        self._buffer_size = 12000
        # If true, this means the environment was successfully loaded
        self._loaded = False
        # The process that is started. If None, no process was started
        self._process: Optional[subprocess.Popen] = None
        self._timeout_wait: int = timeout_wait
        self._communicator = self._get_communicator(worker_id, base_port, timeout_wait)
        self._worker_id = worker_id
        if side_channels is None:
            side_channels = []
        default_training_side_channel: Optional[
            DefaultTrainingAnalyticsSideChannel
        ] = None
        if DefaultTrainingAnalyticsSideChannel.CHANNEL_ID not in [
            _.channel_id for _ in side_channels
        ]:
            default_training_side_channel = DefaultTrainingAnalyticsSideChannel()
            side_channels.append(default_training_side_channel)
        self._side_channel_manager = SideChannelManager(side_channels)
        self._log_folder = log_folder
        self.academy_capabilities: UnityRLCapabilitiesProto = None  # type: ignore

        # If the environment name is None, a new environment will not be launched
        # and the communicator will directly try to connect to an existing unity environment.
        # If the worker-id is not 0 and the environment name is None, an error is thrown
        if file_name is None and worker_id != 0:
            raise UnityEnvironmentException(
                "If the environment name is None, "
                "the worker-id must be 0 in order to connect with the Editor."
            )
        if file_name is not None:
            try:
                self._process = env_utils.launch_executable(
                    file_name, self._executable_args()
                )
            except UnityEnvironmentException:
                self._close(0)
                raise
        else:
            logger.info(
                f"Listening on port {self._port}. "
                f"Start training by pressing the Play button in the Unity Editor."
            )
        self._loaded = True

        rl_init_parameters_in = UnityRLInitializationInputProto(
            seed=seed,
            communication_version=self.API_VERSION,
            package_version=mlagents_envs.__version__,
            capabilities=UnityEnvironment._get_capabilities_proto(),
            num_areas=num_areas,
        )
        try:
            aca_output = self._send_academy_parameters(rl_init_parameters_in)
            aca_params = aca_output.rl_initialization_output
        except UnityTimeOutException:
            self._close(0)
            raise

        if not UnityEnvironment._check_communication_compatibility(
            aca_params.communication_version,
            UnityEnvironment.API_VERSION,
            aca_params.package_version,
        ):
            self._close(0)
            UnityEnvironment._raise_version_exception(aca_params.communication_version)

        UnityEnvironment._warn_csharp_base_capabilities(
            aca_params.capabilities,
            aca_params.package_version,
            UnityEnvironment.API_VERSION,
        )

        self._env_state: Dict[str, Tuple[DecisionSteps, TerminalSteps]] = {}
        self._env_specs: Dict[str, BehaviorSpec] = {}
        self._env_actions: Dict[str, ActionTuple] = {}
        self._is_first_message = True
        self._update_behavior_specs(aca_output)
        self.academy_capabilities = aca_params.capabilities
        if default_training_side_channel is not None:
            default_training_side_channel.environment_initialized()

    @staticmethod
    def _get_communicator(worker_id, base_port, timeout_wait):
        return RpcCommunicator(worker_id, base_port, timeout_wait)

    def _executable_args(self) -> List[str]:
        args: List[str] = []
        if self._no_graphics:
            args += ["-nographics", "-batchmode"]
        args += [UnityEnvironment._PORT_COMMAND_LINE_ARG, str(self._port)]

        # If the logfile arg isn't already set in the env args,
        # try to set it to an output directory
        logfile_set = "-logfile" in (arg.lower() for arg in self._additional_args)
        if self._log_folder and not logfile_set:
            log_file_path = os.path.join(
                self._log_folder, f"Player-{self._worker_id}.log"
            )
            args += ["-logFile", log_file_path]
        # Add in arguments passed explicitly by the user.
        args += self._additional_args
        return args

    def _update_behavior_specs(self, output: UnityOutputProto) -> None:
        init_output = output.rl_initialization_output
        for brain_param in init_output.brain_parameters:
            # Each BrainParameter in the rl_initialization_output should have at least one AgentInfo
            # Get that agent, because we need some of its observations.
            agent_infos = output.rl_output.agentInfos[brain_param.brain_name]
            if agent_infos.value:
                agent = agent_infos.value[0]
                new_spec = behavior_spec_from_proto(brain_param, agent)
                self._env_specs[brain_param.brain_name] = new_spec
                logger.info(f"Connected new brain: {brain_param.brain_name}")

    def _update_state(self, output: UnityRLOutputProto) -> None:
        """
        Collects experience information from all external brains in environment at current step.
        """
        for brain_name in self._env_specs.keys():
            if brain_name in output.agentInfos:
                agent_info_list = output.agentInfos[brain_name].value
                self._env_state[brain_name] = steps_from_proto(
                    agent_info_list, self._env_specs[brain_name]
                )
            else:
                self._env_state[brain_name] = (
                    DecisionSteps.empty(self._env_specs[brain_name]),
                    TerminalSteps.empty(self._env_specs[brain_name]),
                )
        self._side_channel_manager.process_side_channel_message(output.side_channel)

    def reset(self) -> None:
        if self._loaded:
            outputs = self._communicator.exchange(
                self._generate_reset_input(), self._poll_process
            )
            if outputs is None:
                raise UnityCommunicatorStoppedException("Communicator has exited.")
            self._update_behavior_specs(outputs)
            rl_output = outputs.rl_output
            self._update_state(rl_output)
            self._is_first_message = False
            self._env_actions.clear()
        else:
            raise UnityEnvironmentException("No Unity environment is loaded.")

    @timed
    def step(self) -> None:
        if self._is_first_message:
            return self.reset()
        if not self._loaded:
            raise UnityEnvironmentException("No Unity environment is loaded.")
        # fill the blanks for missing actions
        for group_name in self._env_specs:
            if group_name not in self._env_actions:
                n_agents = 0
                if group_name in self._env_state:
                    n_agents = len(self._env_state[group_name][0])
                self._env_actions[group_name] = self._env_specs[
                    group_name
                ].action_spec.empty_action(n_agents)
        step_input = self._generate_step_input(self._env_actions)
        with hierarchical_timer("communicator.exchange"):
            outputs = self._communicator.exchange(step_input, self._poll_process)
        if outputs is None:
            raise UnityCommunicatorStoppedException("Communicator has exited.")
        self._update_behavior_specs(outputs)
        rl_output = outputs.rl_output
        self._update_state(rl_output)
        self._env_actions.clear()

    @property
    def behavior_specs(self) -> MappingType[str, BehaviorSpec]:
        return BehaviorMapping(self._env_specs)

    def _assert_behavior_exists(self, behavior_name: str) -> None:
        if behavior_name not in self._env_specs:
            raise UnityActionException(
                f"The group {behavior_name} does not correspond to an existing "
                f"agent group in the environment"
            )

    def set_actions(self, behavior_name: BehaviorName, action: ActionTuple) -> None:
        self._assert_behavior_exists(behavior_name)
        if behavior_name not in self._env_state:
            return
        action_spec = self._env_specs[behavior_name].action_spec
        num_agents = len(self._env_state[behavior_name][0])
        action = action_spec._validate_action(action, num_agents, behavior_name)
        self._env_actions[behavior_name] = action

    def set_action_for_agent(
        self, behavior_name: BehaviorName, agent_id: AgentId, action: ActionTuple
    ) -> None:
        self._assert_behavior_exists(behavior_name)
        if behavior_name not in self._env_state:
            return
        action_spec = self._env_specs[behavior_name].action_spec
        action = action_spec._validate_action(action, 1, behavior_name)
        if behavior_name not in self._env_actions:
            num_agents = len(self._env_state[behavior_name][0])
            self._env_actions[behavior_name] = action_spec.empty_action(num_agents)
        try:
            index = np.where(self._env_state[behavior_name][0].agent_id == agent_id)[0][
                0
            ]
        except IndexError as ie:
            raise IndexError(
                "agent_id {} is did not request a decision at the previous step".format(
                    agent_id
                )
            ) from ie
        if action_spec.continuous_size > 0:
            self._env_actions[behavior_name].continuous[index] = action.continuous[0, :]
        if action_spec.discrete_size > 0:
            self._env_actions[behavior_name].discrete[index] = action.discrete[0, :]

    def get_steps(
        self, behavior_name: BehaviorName
    ) -> Tuple[DecisionSteps, TerminalSteps]:
        self._assert_behavior_exists(behavior_name)
        return self._env_state[behavior_name]

    def _poll_process(self) -> None:
        """
        Check the status of the subprocess. If it has exited, raise a UnityEnvironmentException
        :return: None
        """
        if not self._process:
            return
        poll_res = self._process.poll()
        if poll_res is not None:
            exc_msg = self._returncode_to_env_message(self._process.returncode)
            raise UnityEnvironmentException(exc_msg)

    def close(self):
        """
        Sends a shutdown signal to the unity environment, and closes the socket connection.
        """
        if self._loaded:
            self._close()
        else:
            raise UnityEnvironmentException("No Unity environment is loaded.")

    def _close(self, timeout: Optional[int] = None) -> None:
        """
        Close the communicator and environment subprocess (if necessary).

        :int timeout: [Optional] Number of seconds to wait for the environment to shut down before
            force-killing it.  Defaults to `self.timeout_wait`.
        """
        if timeout is None:
            timeout = self._timeout_wait
        self._loaded = False
        self._communicator.close()
        if self._process is not None:
            # Wait a bit for the process to shutdown, but kill it if it takes too long
            try:
                self._process.wait(timeout=timeout)
                logger.debug(self._returncode_to_env_message(self._process.returncode))
            except subprocess.TimeoutExpired:
                logger.warning("Environment timed out shutting down. Killing...")
                self._process.kill()
            # Set to None so we don't try to close multiple times.
            self._process = None

    @timed
    def _generate_step_input(
        self, vector_action: Dict[str, ActionTuple]
    ) -> UnityInputProto:
        rl_in = UnityRLInputProto()
        for b in vector_action:
            n_agents = len(self._env_state[b][0])
            if n_agents == 0:
                continue
            for i in range(n_agents):
                action = AgentActionProto()
                if vector_action[b].continuous is not None:
                    action.vector_actions_deprecated.extend(
                        vector_action[b].continuous[i]
                    )
                    action.continuous_actions.extend(vector_action[b].continuous[i])
                if vector_action[b].discrete is not None:
                    action.vector_actions_deprecated.extend(
                        vector_action[b].discrete[i]
                    )
                    action.discrete_actions.extend(vector_action[b].discrete[i])
                rl_in.agent_actions[b].value.extend([action])
                rl_in.command = STEP
        rl_in.side_channel = bytes(
            self._side_channel_manager.generate_side_channel_messages()
        )
        return self._wrap_unity_input(rl_in)

    def _generate_reset_input(self) -> UnityInputProto:
        rl_in = UnityRLInputProto()
        rl_in.command = RESET
        rl_in.side_channel = bytes(
            self._side_channel_manager.generate_side_channel_messages()
        )
        return self._wrap_unity_input(rl_in)

    def _send_academy_parameters(
        self, init_parameters: UnityRLInitializationInputProto
    ) -> UnityOutputProto:
        inputs = UnityInputProto()
        inputs.rl_initialization_input.CopyFrom(init_parameters)
        return self._communicator.initialize(inputs, self._poll_process)

    @staticmethod
    def _wrap_unity_input(rl_input: UnityRLInputProto) -> UnityInputProto:
        result = UnityInputProto()
        result.rl_input.CopyFrom(rl_input)
        return result

    @staticmethod
    def _returncode_to_signal_name(returncode: int) -> Optional[str]:
        """
        Try to convert return codes into their corresponding signal name.
        E.g. returncode_to_signal_name(-2) -> "SIGINT"
        """
        try:
            # A negative value -N indicates that the child was terminated by signal N (POSIX only).
            s = signal.Signals(-returncode)
            return s.name
        except Exception:
            # Should generally be a ValueError, but catch everything just in case.
            return None

    @staticmethod
    def _returncode_to_env_message(returncode: int) -> str:
        signal_name = UnityEnvironment._returncode_to_signal_name(returncode)
        signal_name = f" ({signal_name})" if signal_name else ""
        return f"Environment shut down with return code {returncode}{signal_name}."


--- ml-agents-envs/mlagents_envs/exception.py ---
class UnityException(Exception):
    """
    Any error related to ml-agents environment.
    """

    pass


class UnityEnvironmentException(UnityException):
    """
    Related to errors starting and closing environment.
    """

    pass


class UnityCommunicationException(UnityException):
    """
    Related to errors with the communicator.
    """

    pass


class UnityCommunicatorStoppedException(UnityException):
    """
    Raised when communicator has stopped gracefully.
    """

    pass


class UnityObservationException(UnityException):
    """
    Related to errors with receiving observations.
    """

    pass


class UnityActionException(UnityException):
    """
    Related to errors with sending actions.
    """

    pass


class UnityTimeOutException(UnityException):
    """
    Related to errors with communication timeouts.
    """

    pass


class UnitySideChannelException(UnityException):
    """
    Related to errors with side channels.
    """

    pass


class UnityWorkerInUseException(UnityException):
    """
    This error occurs when the port for a certain worker ID is already reserved.
    """

    MESSAGE_TEMPLATE = (
        "Couldn't start socket communication because worker number {} is still in use. "
        "You may need to manually close a previously opened environment "
        "or use a different worker number."
    )

    def __init__(self, worker_id):
        message = self.MESSAGE_TEMPLATE.format(str(worker_id))
        super().__init__(message)


class UnityPolicyException(UnityException):
    """
    Related to errors with the Trainer.
    """

    pass


--- ml-agents-envs/mlagents_envs/__init__.py ---
# Version of the library that will be used to upload to pypi
__version__ = "1.2.0.dev0"

# Git tag that will be checked to determine whether to trigger upload to pypi
__release_tag__ = None


--- ml-agents-trainer-plugin/setup.py ---
from setuptools import setup
from mlagents.plugins import ML_AGENTS_TRAINER_TYPE

setup(
    name="mlagents_trainer_plugin",
    version="0.0.1",
    # Example of how to add your own registration functions that will be called
    # by mlagents-learn.
    #
    # Here, the get_example_stats_writer() function in mlagents_plugin_examples/example_stats_writer.py
    # will get registered with the ML_AGENTS_STATS_WRITER plugin interface.
    entry_points={
        ML_AGENTS_TRAINER_TYPE: [
            "a2c=mlagents_trainer_plugin.a2c.a2c_trainer:get_type_and_setting",
            "dqn=mlagents_trainer_plugin.dqn.dqn_trainer:get_type_and_setting",
        ]
    },
)


--- ml-agents-trainer-plugin/mlagents_trainer_plugin/__init__.py ---


--- ml-agents-trainer-plugin/mlagents_trainer_plugin/a2c/a2c_optimizer.py ---
from typing import Dict, cast
import attr

from mlagents.torch_utils import torch, default_device

from mlagents.trainers.buffer import AgentBuffer, BufferKey, RewardSignalUtil

from mlagents_envs.timers import timed
from mlagents.trainers.policy.torch_policy import TorchPolicy
from mlagents.trainers.optimizer.torch_optimizer import TorchOptimizer
from mlagents.trainers.settings import (
    TrainerSettings,
    OnPolicyHyperparamSettings,
    ScheduleType,
)
from mlagents.trainers.torch_entities.networks import ValueNetwork
from mlagents.trainers.torch_entities.agent_action import AgentAction
from mlagents.trainers.torch_entities.utils import ModelUtils
from mlagents.trainers.trajectory import ObsUtil

from mlagents.trainers.exception import TrainerConfigError


@attr.s(auto_attribs=True)
class A2CSettings(OnPolicyHyperparamSettings):
    beta: float = 5.0e-3
    lambd: float = 0.95
    num_epoch: int = attr.ib(default=1)  # A2C does just one pass
    shared_critic: bool = False

    @num_epoch.validator
    def _check_num_epoch_one(self, attribute, value):
        if value != 1:
            raise TrainerConfigError("A2C requires num_epoch = 1")

    learning_rate_schedule: ScheduleType = ScheduleType.LINEAR
    beta_schedule: ScheduleType = ScheduleType.LINEAR


class A2COptimizer(TorchOptimizer):
    def __init__(self, policy: TorchPolicy, trainer_settings: TrainerSettings):
        """
        Takes a Policy and a Dict of trainer parameters and creates an Optimizer around the policy.
        The A2C optimizer has a value estimator and a loss function.
        :param policy: A TorchPolicy object that will be updated by this A2C Optimizer.
        :param trainer_params: Trainer parameters dictionary that specifies the
        properties of the trainer.
        """
        # Create the graph here to give more granular control of the TF graph to the Optimizer.

        super().__init__(policy, trainer_settings)
        self.hyperparameters: A2CSettings = cast(
            A2CSettings, trainer_settings.hyperparameters
        )

        params = list(self.policy.actor.parameters())
        if self.hyperparameters.shared_critic:
            self._critic = policy.actor
        else:

            self._critic = ValueNetwork(
                list(self.reward_signals.keys()),
                policy.behavior_spec.observation_specs,
                network_settings=trainer_settings.network_settings,
            )
        self._critic.to(default_device())
        params += list(self._critic.parameters())

        self.decay_learning_rate = ModelUtils.DecayedValue(
            self.hyperparameters.learning_rate_schedule,
            self.hyperparameters.learning_rate,
            1e-10,
            self.trainer_settings.max_steps,
        )

        self.decay_beta = ModelUtils.DecayedValue(
            self.hyperparameters.beta_schedule,
            self.hyperparameters.beta,
            1e-10,
            self.trainer_settings.max_steps,
        )

        self.optimizer = torch.optim.Adam(
            params, lr=self.trainer_settings.hyperparameters.learning_rate
        )
        self.stats_name_to_update_name = {
            "Losses/Value Loss": "value_loss",
            "Losses/Policy Loss": "policy_loss",
        }

        self.stream_names = list(self.reward_signals.keys())

    @property
    def critic(self):
        return self._critic

    @timed
    def update(self, batch: AgentBuffer, num_sequences: int) -> Dict[str, float]:
        """
        Performs update on model.
        :param batch: Batch of experiences.
        :param num_sequences: Number of sequences to process.
        :return: Results of update.
        """
        # Get decayed parameters
        decay_lr = self.decay_learning_rate.get_value(self.policy.get_current_step())
        decay_bet = self.decay_beta.get_value(self.policy.get_current_step())
        returns = {}
        for name in self.reward_signals:
            returns[name] = ModelUtils.list_to_tensor(
                batch[RewardSignalUtil.returns_key(name)]
            )

        n_obs = len(self.policy.behavior_spec.observation_specs)
        current_obs = ObsUtil.from_buffer(batch, n_obs)
        # Convert to tensors
        current_obs = [ModelUtils.list_to_tensor(obs) for obs in current_obs]

        act_masks = ModelUtils.list_to_tensor(batch[BufferKey.ACTION_MASK])
        actions = AgentAction.from_buffer(batch)

        memories = [
            ModelUtils.list_to_tensor(batch[BufferKey.MEMORY][i])
            for i in range(0, len(batch[BufferKey.MEMORY]), self.policy.sequence_length)
        ]
        if len(memories) > 0:
            memories = torch.stack(memories).unsqueeze(0)

        # Get value memories
        value_memories = [
            ModelUtils.list_to_tensor(batch[BufferKey.CRITIC_MEMORY][i])
            for i in range(
                0, len(batch[BufferKey.CRITIC_MEMORY]), self.policy.sequence_length
            )
        ]
        if len(value_memories) > 0:
            value_memories = torch.stack(value_memories).unsqueeze(0)

        run_out = self.policy.actor.get_stats(
            current_obs,
            masks=act_masks,
            actions=actions,
            memories=memories,
            sequence_length=self.policy.sequence_length,
        )

        log_probs = run_out["log_probs"]
        entropy = run_out["entropy"]

        values, _ = self.critic.critic_pass(
            current_obs,
            memories=value_memories,
            sequence_length=self.policy.sequence_length,
        )
        log_probs = log_probs.flatten()

        value_loss_per_head = []
        for name, head in values.items():
            returns_tensor = returns[name]
            be = (returns_tensor - head) ** 2
            value_loss_per_head.append(be)
        value_loss = torch.mean(torch.stack(value_loss_per_head))

        advantages = ModelUtils.list_to_tensor(batch[BufferKey.ADVANTAGES])
        policy_loss = -1 * torch.mean(torch.sum(log_probs, dim=1) * advantages)

        loss = policy_loss + 0.5 * value_loss - decay_bet * torch.mean(entropy)

        # Set optimizer learning rate
        ModelUtils.update_learning_rate(self.optimizer, decay_lr)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        update_stats = {
            # NOTE: abs() is not technically correct, but matches the behavior in TensorFlow.
            # TODO: After PyTorch is default, change to something more correct.
            "Losses/Policy Loss": torch.abs(policy_loss).item(),
            "Losses/Value Loss": value_loss.item(),
            "Policy/Learning Rate": decay_lr,
            "Policy/Beta": decay_bet,
        }

        return update_stats

    def get_modules(self):
        modules = {
            "Optimizer:value_optimizer": self.optimizer,
            "Optimizer:critic": self._critic,
        }
        for reward_provider in self.reward_signals.values():
            modules.update(reward_provider.get_modules())
        return modules


--- ml-agents-trainer-plugin/mlagents_trainer_plugin/a2c/a2c_trainer.py ---
# # Unity ML-Agents Toolkit
# ## ML-Agent Learning (A2C)
# Contains an implementation of A2C as described in: https://arxiv.org/abs/1707.06347

from typing import cast

import numpy as np

from mlagents_envs.base_env import BehaviorSpec
from mlagents_envs.logging_util import get_logger
from mlagents.trainers.buffer import BufferKey, RewardSignalUtil
from mlagents.trainers.trainer.on_policy_trainer import OnPolicyTrainer
from mlagents.trainers.optimizer.torch_optimizer import TorchOptimizer
from mlagents.trainers.trainer.trainer_utils import get_gae
from mlagents.trainers.policy.torch_policy import TorchPolicy
from .a2c_optimizer import A2COptimizer, A2CSettings
from mlagents.trainers.trajectory import Trajectory
from mlagents.trainers.behavior_id_utils import BehaviorIdentifiers
from mlagents.trainers.settings import TrainerSettings

from mlagents.trainers.torch_entities.networks import SimpleActor, SharedActorCritic

logger = get_logger(__name__)

TRAINER_NAME = "a2c"


class A2CTrainer(OnPolicyTrainer):
    """The A2CTrainer is an implementation of the A2C algorithm."""

    def __init__(
        self,
        behavior_name: str,
        reward_buff_cap: int,
        trainer_settings: TrainerSettings,
        training: bool,
        load: bool,
        seed: int,
        artifact_path: str,
    ):
        """
        Responsible for collecting experiences and training A2C model.
        :param behavior_name: The name of the behavior associated with trainer config
        :param reward_buff_cap: Max reward history to track in the reward buffer
        :param trainer_settings: The parameters for the trainer.
        :param training: Whether the trainer is set for training.
        :param load: Whether the model should be loaded.
        :param seed: The seed the model will be initialized with
        :param artifact_path: The directory within which to store artifacts from this trainer.
        """
        super().__init__(
            behavior_name,
            reward_buff_cap,
            trainer_settings,
            training,
            load,
            seed,
            artifact_path,
        )
        self.hyperparameters: A2CSettings = cast(
            A2CSettings, self.trainer_settings.hyperparameters
        )
        self.shared_critic = self.hyperparameters.shared_critic
        self.policy: TorchPolicy = None  # type: ignore

    def _process_trajectory(self, trajectory: Trajectory) -> None:
        """
        Takes a trajectory and processes it, putting it into the update buffer.
        Processing involves calculating value and advantage targets for model updating step.
        :param trajectory: The Trajectory tuple containing the steps to be processed.
        """
        super()._process_trajectory(trajectory)
        agent_id = trajectory.agent_id  # All the agents should have the same ID

        agent_buffer_trajectory = trajectory.to_agentbuffer()
        # Check if we used group rewards, warn if so.
        self._warn_if_group_reward(agent_buffer_trajectory)

        # Update the normalization
        if self.is_training:
            self.policy.actor.update_normalization(agent_buffer_trajectory)
            self.optimizer.critic.update_normalization(agent_buffer_trajectory)

        # Get all value estimates
        (
            value_estimates,
            value_next,
            value_memories,
        ) = self.optimizer.get_trajectory_value_estimates(
            agent_buffer_trajectory,
            trajectory.next_obs,
            trajectory.done_reached and not trajectory.interrupted,
        )
        if value_memories is not None:
            agent_buffer_trajectory[BufferKey.CRITIC_MEMORY].set(value_memories)

        for name, v in value_estimates.items():
            agent_buffer_trajectory[RewardSignalUtil.value_estimates_key(name)].extend(
                v
            )
            self._stats_reporter.add_stat(
                f"Policy/{self.optimizer.reward_signals[name].name.capitalize()} Value Estimate",
                np.mean(v),
            )

        # Evaluate all reward functions
        self.collected_rewards["environment"][agent_id] += np.sum(
            agent_buffer_trajectory[BufferKey.ENVIRONMENT_REWARDS]
        )
        for name, reward_signal in self.optimizer.reward_signals.items():
            evaluate_result = (
                reward_signal.evaluate(agent_buffer_trajectory) * reward_signal.strength
            )
            agent_buffer_trajectory[RewardSignalUtil.rewards_key(name)].extend(
                evaluate_result
            )
            # Report the reward signals
            self.collected_rewards[name][agent_id] += np.sum(evaluate_result)

        # Compute GAE and returns
        tmp_advantages = []
        tmp_returns = []
        for name in self.optimizer.reward_signals:
            bootstrap_value = value_next[name]

            local_rewards = agent_buffer_trajectory[
                RewardSignalUtil.rewards_key(name)
            ].get_batch()
            local_value_estimates = agent_buffer_trajectory[
                RewardSignalUtil.value_estimates_key(name)
            ].get_batch()

            local_advantage = get_gae(
                rewards=local_rewards,
                value_estimates=local_value_estimates,
                value_next=bootstrap_value,
                gamma=self.optimizer.reward_signals[name].gamma,
                lambd=self.hyperparameters.lambd,
            )
            local_return = local_advantage + local_value_estimates
            # This is later use as target for the different value estimates
            agent_buffer_trajectory[RewardSignalUtil.returns_key(name)].set(
                local_return
            )
            agent_buffer_trajectory[RewardSignalUtil.advantage_key(name)].set(
                local_advantage
            )
            tmp_advantages.append(local_advantage)
            tmp_returns.append(local_return)

        # Get global advantages
        global_advantages = list(
            np.mean(np.array(tmp_advantages, dtype=np.float32), axis=0)
        )
        global_returns = list(np.mean(np.array(tmp_returns, dtype=np.float32), axis=0))
        agent_buffer_trajectory[BufferKey.ADVANTAGES].set(global_advantages)
        agent_buffer_trajectory[BufferKey.DISCOUNTED_RETURNS].set(global_returns)

        self._append_to_update_buffer(agent_buffer_trajectory)

        # If this was a terminal trajectory, append stats and reset reward collection
        if trajectory.done_reached:
            self._update_end_episode_stats(agent_id, self.optimizer)

    def create_optimizer(self) -> TorchOptimizer:
        """
        Creates an Optimizer object
        """
        return A2COptimizer(  # type: ignore
            cast(TorchPolicy, self.policy), self.trainer_settings  # type: ignore
        )  # type: ignore

    def create_policy(
        self, parsed_behavior_id: BehaviorIdentifiers, behavior_spec: BehaviorSpec
    ) -> TorchPolicy:
        """
        Creates a policy with a PyTorch backend and PPO hyperparameters
        :param parsed_behavior_id:
        :param behavior_spec: specifications for policy construction
        :return policy
        """
        actor_cls = SimpleActor
        actor_kwargs = {"conditional_sigma": False, "tanh_squash": False}
        if self.shared_critic:
            reward_signal_configs = self.trainer_settings.reward_signals
            reward_signal_names = [
                key.value for key, _ in reward_signal_configs.items()
            ]
            actor_cls = SharedActorCritic
            actor_kwargs.update({"stream_names": reward_signal_names})

        policy = TorchPolicy(
            self.seed,
            behavior_spec,
            self.trainer_settings.network_settings,
            actor_cls,
            actor_kwargs,
        )
        return policy

    @staticmethod
    def get_settings_type():
        return A2CSettings

    @staticmethod
    def get_trainer_name() -> str:
        return TRAINER_NAME


def get_type_and_setting():
    return {A2CTrainer.get_trainer_name(): A2CTrainer}, {
        A2CTrainer.get_trainer_name(): A2CSettings
    }


--- ml-agents-trainer-plugin/mlagents_trainer_plugin/dqn/dqn_optimizer.py ---
from typing import cast
from mlagents.torch_utils import torch, nn, default_device
from mlagents.trainers.optimizer.torch_optimizer import TorchOptimizer
from mlagents.trainers.policy.torch_policy import TorchPolicy
from mlagents.trainers.buffer import AgentBuffer, BufferKey, RewardSignalUtil
from mlagents_envs.timers import timed
from typing import List, Dict, Tuple, Optional, Union, Any
from mlagents.trainers.torch_entities.networks import ValueNetwork, Actor
from mlagents_envs.base_env import ActionSpec, ObservationSpec
from mlagents.trainers.torch_entities.agent_action import AgentAction
from mlagents.trainers.torch_entities.utils import ModelUtils
from mlagents.trainers.trajectory import ObsUtil
from mlagents.trainers.settings import TrainerSettings, OffPolicyHyperparamSettings
from mlagents.trainers.settings import ScheduleType, NetworkSettings

from mlagents.trainers.torch_entities.networks import Critic
import numpy as np
import attr


# TODO: fix saving to onnx


@attr.s(auto_attribs=True)
class DQNSettings(OffPolicyHyperparamSettings):
    gamma: float = 0.99
    exploration_schedule: ScheduleType = ScheduleType.LINEAR
    exploration_initial_eps: float = 0.1
    exploration_final_eps: float = 0.05
    target_update_interval: int = 10000
    tau: float = 0.005
    steps_per_update: float = 1
    save_replay_buffer: bool = False
    reward_signal_steps_per_update: float = attr.ib()

    @reward_signal_steps_per_update.default
    def _reward_signal_steps_per_update_default(self):
        return self.steps_per_update


class DQNOptimizer(TorchOptimizer):
    def __init__(self, policy: TorchPolicy, trainer_settings: TrainerSettings):
        super().__init__(policy, trainer_settings)

        # initialize hyper parameters
        params = list(self.policy.actor.parameters())
        self.optimizer = torch.optim.Adam(
            params, lr=self.trainer_settings.hyperparameters.learning_rate
        )
        self.stream_names = list(self.reward_signals.keys())
        self.gammas = [_val.gamma for _val in trainer_settings.reward_signals.values()]
        self.use_dones_in_backup = {
            name: int(not self.reward_signals[name].ignore_done)
            for name in self.stream_names
        }

        self.hyperparameters: DQNSettings = cast(
            DQNSettings, trainer_settings.hyperparameters
        )
        self.tau = self.hyperparameters.tau
        self.decay_learning_rate = ModelUtils.DecayedValue(
            self.hyperparameters.learning_rate_schedule,
            self.hyperparameters.learning_rate,
            1e-10,
            self.trainer_settings.max_steps,
        )

        self.decay_exploration_rate = ModelUtils.DecayedValue(
            self.hyperparameters.exploration_schedule,
            self.hyperparameters.exploration_initial_eps,
            self.hyperparameters.exploration_final_eps,
            20000,
        )

        # initialize Target Q_network
        self.q_net_target = QNetwork(
            stream_names=self.reward_signals.keys(),
            observation_specs=policy.behavior_spec.observation_specs,
            network_settings=policy.network_settings,
            action_spec=policy.behavior_spec.action_spec,
        )
        ModelUtils.soft_update(self.policy.actor, self.q_net_target, 1.0)

        self.q_net_target.to(default_device())

    @property
    def critic(self):
        return self.q_net_target

    @timed
    def update(self, batch: AgentBuffer, num_sequences: int) -> Dict[str, float]:
        """
        Performs update on model.
        :param batch: Batch of experiences.
        :param num_sequences: Number of sequences to process.
        :return: Results of update.
        """
        # Get decayed parameters
        decay_lr = self.decay_learning_rate.get_value(self.policy.get_current_step())
        exp_rate = self.decay_exploration_rate.get_value(self.policy.get_current_step())
        self.policy.actor.exploration_rate = exp_rate
        rewards = {}
        for name in self.reward_signals:
            rewards[name] = ModelUtils.list_to_tensor(
                batch[RewardSignalUtil.rewards_key(name)]
            )

        n_obs = len(self.policy.behavior_spec.observation_specs)
        current_obs = ObsUtil.from_buffer(batch, n_obs)
        # Convert to tensors
        current_obs = [ModelUtils.list_to_tensor(obs) for obs in current_obs]

        next_obs = ObsUtil.from_buffer_next(batch, n_obs)
        # Convert to tensors
        next_obs = [ModelUtils.list_to_tensor(obs) for obs in next_obs]

        actions = AgentAction.from_buffer(batch)

        dones = ModelUtils.list_to_tensor(batch[BufferKey.DONE])

        current_q_values, _ = self.policy.actor.critic_pass(
            current_obs, sequence_length=self.policy.sequence_length
        )

        qloss = []
        with torch.no_grad():
            greedy_actions = self.policy.actor.get_greedy_action(current_q_values)
            next_q_values_list, _ = self.q_net_target.critic_pass(
                next_obs, sequence_length=self.policy.sequence_length
            )
        for name_i, name in enumerate(rewards.keys()):
            with torch.no_grad():
                next_q_values = torch.gather(
                    next_q_values_list[name], dim=1, index=greedy_actions
                ).squeeze()
                target_q_values = rewards[name] + (
                    (1.0 - self.use_dones_in_backup[name] * dones)
                    * self.gammas[name_i]
                    * next_q_values
                )
                target_q_values = target_q_values.reshape(-1, 1)
            curr_q = torch.gather(
                current_q_values[name], dim=1, index=actions.discrete_tensor
            )
            qloss.append(torch.nn.functional.smooth_l1_loss(curr_q, target_q_values))

        loss = torch.mean(torch.stack(qloss))
        ModelUtils.update_learning_rate(self.optimizer, decay_lr)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        ModelUtils.soft_update(self.policy.actor, self.q_net_target, self.tau)
        update_stats = {
            "Losses/Value Loss": loss.item(),
            "Policy/Learning Rate": decay_lr,
            "Policy/epsilon": exp_rate,
        }

        for reward_provider in self.reward_signals.values():
            update_stats.update(reward_provider.update(batch))
        return update_stats

    def get_modules(self):
        modules = {
            "Optimizer:value_optimizer": self.optimizer,
            "Optimizer:critic": self.critic,
        }
        for reward_provider in self.reward_signals.values():
            modules.update(reward_provider.get_modules())
        return modules


class QNetwork(nn.Module, Actor, Critic):
    MODEL_EXPORT_VERSION = 3

    def __init__(
        self,
        stream_names: List[str],
        observation_specs: List[ObservationSpec],
        network_settings: NetworkSettings,
        action_spec: ActionSpec,
        exploration_initial_eps: float = 1.0,
    ):
        self.exploration_rate = exploration_initial_eps
        nn.Module.__init__(self)
        output_act_size = max(sum(action_spec.discrete_branches), 1)
        self.network_body = ValueNetwork(
            stream_names,
            observation_specs,
            network_settings,
            outputs_per_stream=output_act_size,
        )

        # extra tensors for exporting to ONNX
        self.action_spec = action_spec
        self.version_number = torch.nn.Parameter(
            torch.Tensor([self.MODEL_EXPORT_VERSION]), requires_grad=False
        )
        self.continuous_act_size_vector = torch.nn.Parameter(
            torch.Tensor([int(self.action_spec.continuous_size)]), requires_grad=False
        )
        self.discrete_act_size_vector = torch.nn.Parameter(
            torch.Tensor([self.action_spec.discrete_branches]), requires_grad=False
        )
        self.memory_size_vector = torch.nn.Parameter(
            torch.Tensor([int(self.network_body.memory_size)]), requires_grad=False
        )

    def update_normalization(self, buffer: AgentBuffer) -> None:
        self.network_body.update_normalization(buffer)

    def critic_pass(
        self,
        inputs: List[torch.Tensor],
        memories: Optional[torch.Tensor] = None,
        sequence_length: int = 1,
    ) -> Tuple[Dict[str, torch.Tensor], torch.Tensor]:
        value_outputs, critic_mem_out = self.network_body(
            inputs, memories=memories, sequence_length=sequence_length
        )
        return value_outputs, critic_mem_out

    @property
    def memory_size(self) -> int:
        return self.network_body.memory_size

    def forward(
        self,
        inputs: List[torch.Tensor],
        masks: Optional[torch.Tensor] = None,
        memories: Optional[torch.Tensor] = None,
        sequence_length: int = 1,
    ) -> Tuple[Union[int, torch.Tensor], ...]:
        out_vals, memories = self.critic_pass(inputs, memories, sequence_length)

        # fixme random action tensor
        export_out = [self.version_number, self.memory_size_vector]

        disc_action_out = self.get_greedy_action(out_vals)
        deterministic_disc_action_out = self.get_random_action(out_vals)
        export_out += [
            disc_action_out,
            self.discrete_act_size_vector,
            deterministic_disc_action_out,
        ]
        return tuple(export_out)

    def get_random_action(self, inputs) -> torch.Tensor:
        action_out = torch.randint(
            0, self.action_spec.discrete_branches[0], (len(inputs), 1)
        )
        return action_out

    @staticmethod
    def get_greedy_action(q_values) -> torch.Tensor:
        all_q = torch.cat([val.unsqueeze(0) for val in q_values.values()])
        return torch.argmax(all_q.sum(dim=0), dim=1, keepdim=True)

    def get_action_and_stats(
        self,
        inputs: List[torch.Tensor],
        masks: Optional[torch.Tensor] = None,
        memories: Optional[torch.Tensor] = None,
        sequence_length: int = 1,
        deterministic=False,
    ) -> Tuple[AgentAction, Dict[str, Any], torch.Tensor]:
        run_out = {}
        if not deterministic and np.random.rand() < self.exploration_rate:
            action_out = self.get_random_action(inputs)
            action_out = AgentAction(None, [action_out])
            run_out["env_action"] = action_out.to_action_tuple()
        else:
            out_vals, _ = self.critic_pass(inputs, memories, sequence_length)
            action_out = self.get_greedy_action(out_vals)
            action_out = AgentAction(None, [action_out])
            run_out["env_action"] = action_out.to_action_tuple()
        return action_out, run_out, torch.Tensor([])


--- ml-agents-trainer-plugin/mlagents_trainer_plugin/dqn/dqn_trainer.py ---
from typing import cast

import numpy as np
from mlagents_envs.logging_util import get_logger
from mlagents.trainers.buffer import BufferKey
from mlagents.trainers.policy.torch_policy import TorchPolicy
from mlagents.trainers.trainer.off_policy_trainer import OffPolicyTrainer
from mlagents.trainers.optimizer.torch_optimizer import TorchOptimizer
from mlagents.trainers.trajectory import Trajectory, ObsUtil
from mlagents.trainers.behavior_id_utils import BehaviorIdentifiers
from mlagents_envs.base_env import BehaviorSpec
from mlagents.trainers.settings import TrainerSettings
from .dqn_optimizer import DQNOptimizer, DQNSettings, QNetwork

logger = get_logger(__name__)
TRAINER_NAME = "dqn"


class DQNTrainer(OffPolicyTrainer):
    """The DQNTrainer is an implementation of"""

    def __init__(
        self,
        behavior_name: str,
        reward_buff_cap: int,
        trainer_settings: TrainerSettings,
        training: bool,
        load: bool,
        seed: int,
        artifact_path: str,
    ):
        """
        Responsible for collecting experiences and training SAC model.
        :param behavior_name: The name of the behavior associated with trainer config
        :param reward_buff_cap: Max reward history to track in the reward buffer
        :param trainer_settings: The parameters for the trainer.
        :param training: Whether the trainer is set for training.
        :param load: Whether the model should be loaded.
        :param seed: The seed the model will be initialized with
        :param artifact_path: The directory within which to store artifacts from this trainer.
        """
        super().__init__(
            behavior_name,
            reward_buff_cap,
            trainer_settings,
            training,
            load,
            seed,
            artifact_path,
        )
        self.policy: TorchPolicy = None  # type: ignore
        self.optimizer: DQNOptimizer = None  # type: ignore

    def _process_trajectory(self, trajectory: Trajectory) -> None:
        """
        Takes a trajectory and processes it, putting it into the replay buffer.
        """
        super()._process_trajectory(trajectory)
        last_step = trajectory.steps[-1]
        agent_id = trajectory.agent_id  # All the agents should have the same ID

        agent_buffer_trajectory = trajectory.to_agentbuffer()
        # Check if we used group rewards, warn if so.
        self._warn_if_group_reward(agent_buffer_trajectory)

        # Update the normalization
        if self.is_training:
            self.policy.actor.update_normalization(agent_buffer_trajectory)
            self.optimizer.critic.update_normalization(agent_buffer_trajectory)

        # Evaluate all reward functions for reporting purposes
        self.collected_rewards["environment"][agent_id] += np.sum(
            agent_buffer_trajectory[BufferKey.ENVIRONMENT_REWARDS]
        )
        for name, reward_signal in self.optimizer.reward_signals.items():
            evaluate_result = (
                reward_signal.evaluate(agent_buffer_trajectory) * reward_signal.strength
            )

            # Report the reward signals
            self.collected_rewards[name][agent_id] += np.sum(evaluate_result)

        # Get all value estimates for reporting purposes
        (
            value_estimates,
            _,
            value_memories,
        ) = self.optimizer.get_trajectory_value_estimates(
            agent_buffer_trajectory, trajectory.next_obs, trajectory.done_reached
        )
        if value_memories is not None:
            agent_buffer_trajectory[BufferKey.CRITIC_MEMORY].set(value_memories)

        for name, v in value_estimates.items():
            self._stats_reporter.add_stat(
                f"Policy/{self.optimizer.reward_signals[name].name.capitalize()} Value",
                np.mean(v),
            )

        # Bootstrap using the last step rather than the bootstrap step if max step is reached.
        # Set last element to duplicate obs and remove dones.
        if last_step.interrupted:
            last_step_obs = last_step.obs
            for i, obs in enumerate(last_step_obs):
                agent_buffer_trajectory[ObsUtil.get_name_at_next(i)][-1] = obs
            agent_buffer_trajectory[BufferKey.DONE][-1] = False

        self._append_to_update_buffer(agent_buffer_trajectory)

        if trajectory.done_reached:
            self._update_end_episode_stats(agent_id, self.optimizer)

    def create_optimizer(self) -> TorchOptimizer:
        """
        Creates an Optimizer object
        """
        return DQNOptimizer(  # type: ignore
            cast(TorchPolicy, self.policy), self.trainer_settings  # type: ignore
        )  # type: ignore

    def create_policy(
        self, parsed_behavior_id: BehaviorIdentifiers, behavior_spec: BehaviorSpec
    ) -> TorchPolicy:
        """
        Creates a policy with a PyTorch backend and give DQN hyperparameters
        :param parsed_behavior_id:
        :param behavior_spec: specifications for policy construction
        :return policy
        """
        # initialize online Q-network which works as actor
        exploration_initial_eps = cast(
            DQNSettings, self.trainer_settings.hyperparameters
        ).exploration_initial_eps
        actor_kwargs = {
            "exploration_initial_eps": exploration_initial_eps,
            "stream_names": [
                signal.value for signal in self.trainer_settings.reward_signals.keys()
            ],
        }
        policy = TorchPolicy(
            self.seed,
            behavior_spec,
            self.trainer_settings.network_settings,
            actor_cls=QNetwork,
            actor_kwargs=actor_kwargs,
        )
        self.maybe_load_replay_buffer()
        return policy

    @staticmethod
    def get_settings_type():
        return DQNSettings

    @staticmethod
    def get_trainer_name() -> str:
        return TRAINER_NAME


def get_type_and_setting():
    return {DQNTrainer.get_trainer_name(): DQNTrainer}, {
        DQNTrainer.get_trainer_name(): DQNTrainer.get_settings_type()
    }


--- ml-agents-trainer-plugin/mlagents_trainer_plugin/a2c/__init__.py ---


--- ml-agents-trainer-plugin/mlagents_trainer_plugin/dqn/__init__.py ---


--- utils/__init__.py ---


--- utils/make_readme_table.py ---
"""
Generate the "Releases" table on the main readme. Update the versions lists, run this script, and copy the output
into the markdown file.
"""
from distutils.version import LooseVersion, StrictVersion
from datetime import datetime
from typing import NamedTuple
from collections import Counter

MAX_DAYS = 150  # do not print releases older than this many days


def table_line(version_info, bold=False):
    bold_str = "**" if bold else ""

    cells = [
        f"**{version_info.display_name}**",
        f"{bold_str}{version_info.release_date}{bold_str}",
        f"{bold_str}[source]({version_info.source_link}){bold_str}",
        f"{bold_str}[docs]({version_info.doc_link}){bold_str}",
        f"{bold_str}[download]({version_info.download_link}){bold_str}",
    ]
    if version_info.is_develop:
        cells.append("--")  # python
        cells.append("--")  # Unity
    else:
        cells.append(
            f"{bold_str}[{version_info.python_verion}]({version_info.pypi_link}){bold_str}"
        )
        cells.append(
            f"{bold_str}[{version_info.csharp_version}]({version_info.package_link}){bold_str}"
        )
    joined_cells = " | ".join(cells)
    return f"| {joined_cells} |"


class ReleaseInfo(NamedTuple):
    release_tag: str
    csharp_version: str
    python_verion: str
    release_date: str
    is_verified: bool = False

    @property
    def loose_version(self) -> LooseVersion:
        return LooseVersion(self.python_verion)

    @property
    def is_develop(self) -> bool:
        return self.release_tag == "develop"

    @property
    def release_datetime(self) -> datetime:
        if self.is_develop:
            return datetime.today()
        return datetime.strptime(self.release_date, "%B %d, %Y")

    @property
    def elapsed_days(self) -> int:
        """
        Days since this version was released.
        :return:
        """
        return (datetime.today() - self.release_datetime).days

    @property
    def display_name(self) -> str:
        """
        Clean up the tag name for display, e.g. "release_1" -> "Release 1"
        :return:
        """
        if self.is_verified:
            return f"Verified Package {self.csharp_version}"
        elif self.is_develop:
            return "develop (unstable)"
        else:
            return self.release_tag.replace("_", " ").title()

    @property
    def source_link(self):
        if self.is_verified:
            return f"https://github.com/Unity-Technologies/ml-agents/tree/com.unity.ml-agents_{self.csharp_version}"
        else:
            return f"https://github.com/Unity-Technologies/ml-agents/tree/{self.release_tag}"

    @property
    def download_link(self):
        if self.is_verified:
            tag = f"com.unity.ml-agents_{self.csharp_version}"
        else:
            tag = self.release_tag
        return f"https://github.com/Unity-Technologies/ml-agents/archive/{tag}.zip"

    @property
    def doc_link(self):
        if self.is_verified:
            return "https://github.com/Unity-Technologies/ml-agents/blob/release_2_verified_docs/docs/Readme.md"

        if self.csharp_version == "develop":
            return (
                "https://github.com/Unity-Technologies/ml-agents/tree/"
                "develop/com.unity.ml-agents/Documentation~/index.md"
            )

        # Prioritize Unity Package documentation over web docs
        try:
            StrictVersion(self.csharp_version).version
            return "https://docs.unity3d.com/Packages/com.unity.ml-agents@latest"
        except ValueError:
            return "https://unity-technologies.github.io/ml-agents/  (DEPRECATED)"

    @property
    def package_link(self):
        try:
            v = StrictVersion(self.csharp_version).version
            return f"https://docs.unity3d.com/Packages/com.unity.ml-agents@{v[0]}.{v[1]}/manual/index.html"
        except ValueError:
            return "--"

    @property
    def pypi_link(self):
        return f"https://pypi.org/project/mlagents/{self.python_verion}/"


versions = [
    ReleaseInfo("develop", "develop", "develop", "--"),
    ReleaseInfo("release_1", "1.0.0", "0.16.0", "April 30, 2020"),
    ReleaseInfo("release_2", "1.0.2", "0.16.1", "May 20, 2020"),
    ReleaseInfo("release_3", "1.1.0", "0.17.0", "June 10, 2020"),
    ReleaseInfo("release_4", "1.2.0", "0.18.0", "July 15, 2020"),
    ReleaseInfo("release_5", "1.2.1", "0.18.1", "July 31, 2020"),
    ReleaseInfo("release_6", "1.3.0", "0.19.0", "August 12, 2020"),
    ReleaseInfo("release_7", "1.4.0", "0.20.0", "September 16, 2020"),
    ReleaseInfo("release_8", "1.5.0", "0.21.0", "October 14, 2020"),
    ReleaseInfo("release_9", "1.5.0", "0.21.1", "November 4, 2020"),
    ReleaseInfo("release_10", "1.6.0", "0.22.0", "November 18, 2020"),
    ReleaseInfo("release_11", "1.7.0", "0.23.0", "December 21, 2020"),
    ReleaseInfo("release_12", "1.7.2", "0.23.0", "December 22, 2020"),
    ReleaseInfo("release_13", "1.8.0", "0.24.0", "February 17, 2021"),
    ReleaseInfo("release_14", "1.8.1", "0.24.1", "March 5, 2021"),
    ReleaseInfo("release_15", "1.9.0", "0.25.0", "March 17, 2021"),
    ReleaseInfo("release_16", "1.9.1", "0.25.1", "April 13, 2021"),
    ReleaseInfo("release_17", "2.0.0", "0.26.0", "April 22, 2021"),
    ReleaseInfo("release_18", "2.1.0", "0.27.0", "June 9, 2021"),
    ReleaseInfo("release_19", "2.2.1", "0.28.0", "January 14, 2022"),
    ReleaseInfo("release_20", "2.3.0", "0.30.0", "November 21, 2022"),
    ReleaseInfo("release_21", "3.0.0-exp.1", "1.0.0", "October 9, 2023"),
    ReleaseInfo("release_22", "3.0.0", "1.1.0", "October 5, 2024"),
    ReleaseInfo("release_23", "4.0.0", "1.1.0", "August 15, 2025"),
    # Verified releases
    # ReleaseInfo("", "1.0.8", "0.16.1", "May 26, 2021", is_verified=True),
    # ReleaseInfo("", "1.0.7", "0.16.1", "March 8, 2021", is_verified=True),
    # ReleaseInfo("", "1.0.6", "0.16.1", "November 16, 2020", is_verified=True),
    # ReleaseInfo("", "1.0.5", "0.16.1", "September 23, 2020", is_verified=True),
    # ReleaseInfo("", "1.0.4", "0.16.1", "August 20, 2020", is_verified=True),
]

sorted_versions = sorted(versions, key=lambda x: x.release_datetime, reverse=True)

highlight_versions = set()
# Highlight the most recent verified version
# disabling verified versions.
# TODO replace this table entry with released version according to
#  https://docs.unity3d.com/2022.3/Documentation/Manual/pack-safe.html
# highlight_versions.add([v for v in sorted_versions if v.is_verified][0])
# Highlight the most recent regular version
highlight_versions.add(
    [v for v in sorted_versions if (not v.is_verified and not v.is_develop)][0]
)

count_by_verified = Counter()

for version_info in sorted_versions:
    highlight = version_info in highlight_versions
    if version_info.elapsed_days > MAX_DAYS:
        # Make sure we always have at least regular and one verified entry
        if count_by_verified[version_info.is_verified] > 0:
            continue
    print(table_line(version_info, highlight))
    count_by_verified[version_info.is_verified] += 1

print("\n\n")


## Links discovered
- [f"**{version_info.display_name}**",
        f"{bold_str}{version_info.release_date}{bold_str}",
        f"{bold_str}[source](https://github.com/Unity-Technologies/ml-agents/blob/develop/utils/{version_info.source_link})
- [docs](https://github.com/Unity-Technologies/ml-agents/blob/develop/utils/{version_info.doc_link})
- [download](https://github.com/Unity-Technologies/ml-agents/blob/develop/utils/{version_info.download_link})
- [{version_info.python_verion}](https://github.com/Unity-Technologies/ml-agents/blob/develop/utils/{version_info.pypi_link})
- [{version_info.csharp_version}](https://github.com/Unity-Technologies/ml-agents/blob/develop/utils/{version_info.package_link})
