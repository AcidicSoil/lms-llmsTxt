# llms-full (private-aware)
> Built from GitHub files and website pages. Large files may be truncated.

--- build_files/build_environment/install_linux_packages.py ---
#!/usr/bin/env python3
# SPDX-FileCopyrightText: 2023 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

__all__ = (
    "main",
)

import logging
import os
import re
import subprocess
import sys
import time


DISTRO_ID_DEBIAN = "debian"
DISTRO_ID_FEDORA = "fedora"
DISTRO_ID_SUSE = "suse"
DISTRO_ID_ARCH = "arch"


MAYSUDO = subprocess.run("command -v sudo || command -v doas",
                         shell=True,
                         stdout=subprocess.PIPE,
                         stderr=subprocess.PIPE,
                         universal_newlines=True).stdout.rstrip('\n')


class LoggingColoredFormatter(logging.Formatter):
    """
    Logging colored formatter,.
    Based on https://alexandra-zaharia.github.io/posts/make-your-own-custom-color-formatter-with-python-logging/
    """
    GREY = '\x1b[38;21m'
    BLUE = '\x1b[38;5;39m'
    YELLOW = '\x1b[38;5;226m'
    RED = '\x1b[38;5;196m'
    BOLD_RED = '\x1b[31;1m'
    RESET = '\x1b[0m'

    def __init__(self, fmt=None):
        super().__init__(fmt=fmt)
        self.FORMATS = {
            logging.DEBUG: self.GREY + "DEBUG:    " + self.RESET + self._fmt,
            logging.INFO: self.BLUE + "INFO:     " + self.RESET + self._fmt,
            logging.WARNING: self.YELLOW + "WARNING: " + self.RESET + self._fmt,
            logging.ERROR: self.RED + "ERROR:    " + self.RESET + self._fmt,
            logging.CRITICAL: self.BOLD_RED + "CRITICAL: " + self.RESET + self._fmt,
        }

    def format(self, record):
        log_fmt = self.FORMATS.get(record.levelno)
        formatter = logging.Formatter(log_fmt)
        return formatter.format(record)


class Package:
    __slots__ = (
        # User-friendly name for the package.
        "name",
        # This is a fake package used to bulk-install a group of packages.
        # There is no version check performed here, and a single missing package will fail the whole thing.
        # Used for the basic sets of build packages and dependencies that can be assumed always available,
        # with stable enough API that the version does not matter (to some extent, it is expected to work with
        # any recent distribution version at least).
        "is_group",
        # Whether Blender can build without this package or not.
        # Note: In case of group packages, all sub-packages inherit from the value of the root group package.
        "is_mandatory",
        # Exact version currently used for pre-built libraries and build-bot builds.
        "version",
        # Ideal version of the package (if possible, prioritize a package of that version), `version` should match it.
        "version_short",
        # Minimal (included)/maximal (excluded) assumed supported version range.
        # Package outside of that range won't be installed.
        "version_min", "version_mex",
        # Actual installed package version.
        "version_installed",
        # Other Packages that depend/are only installed if the 'parent' one is valid.
        "sub_packages",
        # A mapping from distribution name key to distribution package name value.
        # Value may either be:
        #   - A package name string.
        #   - A callback taking the Package and an iterable of its parents as parameters, and returning a string.
        #   - None to indicate that there is no known package for that distribution.
        #   - ... to indicate that this package can be skipped for that distribution
        #     (typically, because it is included in a parent package already).
        "distro_package_names",
    )

    def __init__(self, name, is_group=False, is_mandatory=False,
                 version=None, version_short=None, version_min=None, version_mex=None,
                 sub_packages=(), distro_package_names={}):
        self.name = name
        self.is_group = is_group
        self.is_mandatory = is_mandatory
        self.version = version
        self.version_short = version_short
        self.version_min = version_min
        self.version_mex = version_mex
        self.version_installed = ...
        self.sub_packages = sub_packages
        self.distro_package_names = distro_package_names

    def __repr__(self):
        is_mandatory_repr = "[mandatory]" if self.is_mandatory else ""
        is_group_repr = "[group]" if self.is_group else ""
        return (
            f"{self.name} ({self.version_short}) {is_mandatory_repr}{is_group_repr}:\n"
            f"\t{self.version} ({self.version_min} ... {self.version_mex}) ==> {self.version_installed}"
        )


# Absolute minimal required tools to build Blender.
BUILD_MANDATORY_SUBPACKAGES = (
    Package(name="Build Essentials", is_group=True,
            sub_packages=(
                Package(name="GCC",
                        distro_package_names={DISTRO_ID_DEBIAN: ...,
                                              DISTRO_ID_FEDORA: "gcc",
                                              DISTRO_ID_SUSE: "gcc",
                                              DISTRO_ID_ARCH: ...,
                                              },
                        ),
                Package(name="GCC-C++",
                        distro_package_names={DISTRO_ID_DEBIAN: ...,
                                              DISTRO_ID_FEDORA: "gcc-c++",
                                              DISTRO_ID_SUSE: "gcc-c++",
                                              DISTRO_ID_ARCH: ...,
                                              },
                        ),
                Package(name="make",
                        distro_package_names={DISTRO_ID_DEBIAN: ...,
                                              DISTRO_ID_FEDORA: "make",
                                              DISTRO_ID_SUSE: "make",
                                              DISTRO_ID_ARCH: ...,
                                              },
                        ),
                Package(name="glibc",
                        distro_package_names={DISTRO_ID_DEBIAN: ...,
                                              DISTRO_ID_FEDORA: "glibc-devel",
                                              DISTRO_ID_SUSE: "glibc-devel",
                                              DISTRO_ID_ARCH: ...,
                                              },
                        ),
            ),
            distro_package_names={DISTRO_ID_DEBIAN: "build-essential",
                                  DISTRO_ID_FEDORA: ...,
                                  DISTRO_ID_SUSE: ...,
                                  DISTRO_ID_ARCH: "base-devel",
                                  },
            ),
    Package(name="Git", is_group=True,
            sub_packages=(
                Package(name="Git LFS",
                        distro_package_names={DISTRO_ID_DEBIAN: "git-lfs",
                                              DISTRO_ID_FEDORA: "git-lfs",
                                              DISTRO_ID_SUSE: "git-lfs",
                                              DISTRO_ID_ARCH: "git-lfs",
                                              },
                        ),
            ),
            distro_package_names={DISTRO_ID_DEBIAN: "git",
                                  DISTRO_ID_FEDORA: "git",
                                  DISTRO_ID_SUSE: "git",
                                  DISTRO_ID_ARCH: "git",
                                  },
            ),
    Package(name="CMake",
            distro_package_names={DISTRO_ID_DEBIAN: "cmake",
                                  DISTRO_ID_FEDORA: "cmake",
                                  DISTRO_ID_SUSE: "cmake",
                                  DISTRO_ID_ARCH: "cmake",
                                  },
            ),
)


# Fairly common additional tools useful to build Blender.
BUILD_OPTIONAL_SUBPACKAGES = (
    Package(name="Ninja Builder",
            distro_package_names={DISTRO_ID_DEBIAN: "ninja-build",
                                  DISTRO_ID_FEDORA: "ninja-build",
                                  DISTRO_ID_SUSE: "ninja",
                                  DISTRO_ID_ARCH: "ninja",
                                  },
            ),
    Package(name="CMake commandline GUI",
            distro_package_names={DISTRO_ID_DEBIAN: "cmake-curses-gui",
                                  DISTRO_ID_FEDORA: None,
                                  DISTRO_ID_SUSE: None,
                                  DISTRO_ID_ARCH: None,
                                  },
            ),
    Package(name="CMake GUI",
            distro_package_names={DISTRO_ID_DEBIAN: "cmake-gui",
                                  DISTRO_ID_FEDORA: "cmake-gui",
                                  DISTRO_ID_SUSE: "cmake-gui",
                                  DISTRO_ID_ARCH: None,
                                  },
            ),
    Package(name="Patch",
            distro_package_names={DISTRO_ID_DEBIAN: "patch",
                                  DISTRO_ID_FEDORA: "patch",
                                  DISTRO_ID_SUSE: "patch",
                                  DISTRO_ID_ARCH: "patch",
                                  },
            ),
)


# Library dependencies that are not provided by precompiled libraries.
DEPS_CRITICAL_SUBPACKAGES = (
    Package(name="X11 library",
            distro_package_names={DISTRO_ID_DEBIAN: "libx11-dev",
                                  DISTRO_ID_FEDORA: "libX11-devel",
                                  DISTRO_ID_SUSE: "libX11-devel",
                                  DISTRO_ID_ARCH: "libx11",
                                  },
            ),
    Package(name="Xxf86vm Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libxxf86vm-dev",
                                  DISTRO_ID_FEDORA: "libXxf86vm-devel",
                                  DISTRO_ID_SUSE: "libXxf86vm-devel",
                                  DISTRO_ID_ARCH: "libxxf86vm",
                                  },
            ),
    Package(name="XCursor Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libxcursor-dev",
                                  DISTRO_ID_FEDORA: "libXcursor-devel",
                                  DISTRO_ID_SUSE: "libXcursor-devel",
                                  DISTRO_ID_ARCH: "libxcursor",
                                  },
            ),
    Package(name="Xi Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libxi-dev",
                                  DISTRO_ID_FEDORA: "libXi-devel",
                                  DISTRO_ID_SUSE: "libXi-devel",
                                  DISTRO_ID_ARCH: "libxi",
                                  },
            ),
    Package(name="XRandr Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libxrandr-dev",
                                  DISTRO_ID_FEDORA: "libXrandr-devel",
                                  DISTRO_ID_SUSE: "libXrandr-devel",
                                  DISTRO_ID_ARCH: "libxrandr",
                                  },
            ),
    Package(name="Xinerama Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libxinerama-dev",
                                  DISTRO_ID_FEDORA: "libXinerama-devel",
                                  DISTRO_ID_SUSE: "libXinerama-devel",
                                  DISTRO_ID_ARCH: "libxinerama",
                                  },
            ),
    Package(name="XKbCommon Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libxkbcommon-dev",
                                  DISTRO_ID_FEDORA: "libxkbcommon-devel",
                                  DISTRO_ID_SUSE: "libxkbcommon-devel",
                                  DISTRO_ID_ARCH: "libxkbcommon",
                                  },
            ),
    Package(name="Wayland Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libwayland-dev",
                                  DISTRO_ID_FEDORA: "wayland-devel",
                                  DISTRO_ID_SUSE: "wayland-devel",
                                  DISTRO_ID_ARCH: "wayland",
                                  },
            ),
    Package(name="Decor Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libdecor-0-dev",
                                  DISTRO_ID_FEDORA: "libdecor-devel",
                                  DISTRO_ID_SUSE: "libdecor-devel",
                                  DISTRO_ID_ARCH: "libdecor",
                                  },
            ),
    Package(name="Wayland Protocols",
            distro_package_names={DISTRO_ID_DEBIAN: "wayland-protocols",
                                  DISTRO_ID_FEDORA: "wayland-protocols-devel",
                                  DISTRO_ID_SUSE: "wayland-protocols-devel",
                                  DISTRO_ID_ARCH: "wayland-protocols",
                                  },
            ),
    Package(name="DBus Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libdbus-1-dev",
                                  DISTRO_ID_FEDORA: "dbus-devel",
                                  DISTRO_ID_SUSE: "dbus-1-devel",
                                  DISTRO_ID_ARCH: "dbus",
                                  },
            ),
    Package(name="OpenGL Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libgl-dev",
                                  DISTRO_ID_FEDORA: "mesa-libGL-devel",
                                  DISTRO_ID_SUSE: "Mesa-libGL-devel",
                                  DISTRO_ID_ARCH: "libglvnd",
                                  },
            ),
    Package(name="EGL Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libegl-dev",
                                  DISTRO_ID_FEDORA: "mesa-libEGL-devel",
                                  DISTRO_ID_SUSE: "Mesa-libEGL-devel",
                                  DISTRO_ID_ARCH: None,  # Included in `libglvnd`.
                                  },
            ),
)


# Basic mandatory set of common libraries to build Blender, which are also available as pre-compiled libraries.
DEPS_MANDATORY_SUBPACKAGES = (
    Package(name="JPEG Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libjpeg-dev",
                                  DISTRO_ID_FEDORA: "libjpeg-turbo-devel",
                                  DISTRO_ID_SUSE: "libjpeg8-devel",
                                  DISTRO_ID_ARCH: "libjpeg-turbo",
                                  },
            ),
    Package(name="PNG Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libpng-dev",
                                  DISTRO_ID_FEDORA: "libpng-devel",
                                  DISTRO_ID_SUSE: "libpng16-compat-devel",
                                  DISTRO_ID_ARCH: "libpng",
                                  },
            ),
    Package(name="FreeType Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libfreetype-dev",
                                  DISTRO_ID_FEDORA: "freetype-devel",
                                  DISTRO_ID_SUSE: "freetype2-devel",
                                  DISTRO_ID_ARCH: "freetype2",
                                  },
            ),
    Package(name="FontConfig Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libfontconfig-dev",
                                  DISTRO_ID_FEDORA: "fontconfig",
                                  DISTRO_ID_SUSE: "fontconfig",
                                  DISTRO_ID_ARCH: "fontconfig",
                                  },
            ),
    Package(name="ZStandard Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libzstd-dev",
                                  DISTRO_ID_FEDORA: "libzstd-devel",
                                  DISTRO_ID_SUSE: "libzstd-devel",
                                  DISTRO_ID_ARCH: "zstd",
                                  },
            ),
    Package(name="BZ2 Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libbz2-dev",
                                  DISTRO_ID_FEDORA: "bzip2-devel",
                                  DISTRO_ID_SUSE: "libbz2-devel",
                                  DISTRO_ID_ARCH: "bzip2",
                                  },
            ),
    Package(name="LZMA Library",
            distro_package_names={DISTRO_ID_DEBIAN: "liblzma-dev",
                                  DISTRO_ID_FEDORA: "lzma-sdk-devel",  # ???
                                  DISTRO_ID_SUSE: "lzma-sdk-devel",  # ???
                                  DISTRO_ID_ARCH: "xz",  # ???
                                  },
            ),
    Package(name="SDL2 Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libsdl2-dev",
                                  DISTRO_ID_FEDORA: "SDL2-devel",
                                  DISTRO_ID_SUSE: "SDL2-devel",
                                  DISTRO_ID_ARCH: "sdl2",
                                  },
            ),
    Package(name="ShaderC Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libshaderc-dev",
                                  DISTRO_ID_FEDORA: "libshaderc-devel",
                                  DISTRO_ID_SUSE: "shaderc-devel",
                                  DISTRO_ID_ARCH: "shaderc",
                                  },
            ),
    Package(name="Epoxy Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libepoxy-dev",
                                  DISTRO_ID_FEDORA: "libepoxy-devel",
                                  DISTRO_ID_SUSE: "libepoxy-devel",
                                  DISTRO_ID_ARCH: "libepoxy",
                                  },
            ),
    Package(name="XML2 Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libxml2-dev",
                                  DISTRO_ID_FEDORA: "libxml2-devel",
                                  DISTRO_ID_SUSE: "libxml2-devel",
                                  DISTRO_ID_ARCH: "libxml2",
                                  },
            ),
    Package(name="Haru Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libhpdf-dev",
                                  DISTRO_ID_FEDORA: "libharu-devel",
                                  DISTRO_ID_SUSE: "libharu-devel",
                                  DISTRO_ID_ARCH: "libharu",
                                  },
            ),
    Package(name="PyString Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libpystring-dev",
                                  DISTRO_ID_FEDORA: "pystring-devel",
                                  DISTRO_ID_SUSE: "pystring-devel",
                                  DISTRO_ID_ARCH: "pystring",
                                  },
            ),
)


# Basic optional set of common libraries to build Blender, which are also available as pre-compiled libraries.
DEPS_OPTIONAL_SUBPACKAGES = (
    Package(name="OpenJPG Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libopenjp2-7-dev",
                                  DISTRO_ID_FEDORA: "openjpeg2-devel",
                                  DISTRO_ID_SUSE: "openjpeg2-devel",
                                  DISTRO_ID_ARCH: "openjpeg2",
                                  },
            ),
    Package(name="TIFF Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libtiff-dev",
                                  DISTRO_ID_FEDORA: "libtiff-devel",
                                  DISTRO_ID_SUSE: "libtiff-devel",
                                  DISTRO_ID_ARCH: "libtiff",
                                  },
            ),
    Package(name="Jack2 Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libjack-jackd2-dev",
                                  DISTRO_ID_FEDORA: "jack-audio-connection-kit-devel",
                                  DISTRO_ID_SUSE: None,
                                  DISTRO_ID_ARCH: "jack2",
                                  },
            ),
    Package(name="Pulse Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libpulse-dev",
                                  DISTRO_ID_FEDORA: "pulseaudio-libs-devel",
                                  DISTRO_ID_SUSE: "libpulse-devel",
                                  DISTRO_ID_ARCH: "libpulse",
                                  },
            ),
    Package(name="Pipewire Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libpipewire-0.3-dev",
                                  DISTRO_ID_FEDORA: "pipewire-devel",
                                  DISTRO_ID_SUSE: "pipewire-devel",
                                  DISTRO_ID_ARCH: "pipewire",
                                  },
            ),
    Package(name="OpenAL Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libopenal-dev",
                                  DISTRO_ID_FEDORA: "openal-soft-devel",
                                  DISTRO_ID_SUSE: None,
                                  DISTRO_ID_ARCH: "openal",
                                  },
            ),
    Package(name="SndFile Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libsndfile1-dev",
                                  DISTRO_ID_FEDORA: "libsndfile-devel",
                                  DISTRO_ID_SUSE: None,
                                  DISTRO_ID_ARCH: "libsndfile",
                                  },
            ),
    Package(name="Vulkan Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libvulkan-dev",
                                  DISTRO_ID_FEDORA: ...,
                                  DISTRO_ID_SUSE: ...,
                                  DISTRO_ID_ARCH: ...,
                                  },
            ),
    Package(name="Vulkan Headers",
            distro_package_names={DISTRO_ID_DEBIAN: ...,
                                  DISTRO_ID_FEDORA: "vulkan-headers",
                                  DISTRO_ID_SUSE: "vulkan-headers",
                                  DISTRO_ID_ARCH: "vulkan-headers",
                                  },
            ),
    Package(name="Vulkan ICD Loader",
            distro_package_names={DISTRO_ID_DEBIAN: ...,
                                  DISTRO_ID_FEDORA: "vulkan-loader-devel",
                                  DISTRO_ID_SUSE: ...,
                                  DISTRO_ID_ARCH: "vulkan-icd-loader",
                                  },
            ),
    Package(name="GMP Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libgmp-dev",
                                  DISTRO_ID_FEDORA: "gmp-devel",
                                  DISTRO_ID_SUSE: "gmp-devel",
                                  DISTRO_ID_ARCH: "gmp",
                                  },
            ),
    Package(name="PugiXML Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libpugixml-dev",
                                  DISTRO_ID_FEDORA: "pugixml-devel",
                                  DISTRO_ID_SUSE: "pugixml-devel",
                                  DISTRO_ID_ARCH: "pugixml",
                                  },
            ),
    Package(name="FFTW3 Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libfftw3-dev",
                                  DISTRO_ID_FEDORA: "fftw-devel",
                                  DISTRO_ID_SUSE: "fftw-devel",
                                  DISTRO_ID_ARCH: "fftw",
                                  },
            ),
    Package(name="POTrace Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libpotrace-dev",
                                  DISTRO_ID_FEDORA: "potrace-devel",
                                  DISTRO_ID_SUSE: "potrace-devel",
                                  DISTRO_ID_ARCH: "potrace",
                                  },
            ),
    Package(name="Yaml CPP Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libyaml-cpp-dev",
                                  DISTRO_ID_FEDORA: "yaml-cpp-devel",
                                  DISTRO_ID_SUSE: "yaml-cpp-devel",
                                  DISTRO_ID_ARCH: "yaml-cpp",
                                  },
            ),
    Package(name="Deflate Library",
            distro_package_names={DISTRO_ID_DEBIAN: "libdeflate-dev",
                                  DISTRO_ID_FEDORA: "libdeflate-devel",
                                  DISTRO_ID_SUSE: "libdeflate-devel",
                                  DISTRO_ID_ARCH: "libdeflate",
                                  },
            ),
)


# Python packages that should be available for Blender Pythons-scripts.
# SUSE uses names like `python310-Cython` for its python module packages...
def suse_pypackages_name_gen(name):
    def _gen(package, parent_packages):
        pp = parent_packages[-1]
        if pp is not None and pp.version_installed is not ...:
            v = "".join(str(i) for i in PackageInstaller.version_tokenize(pp.version_installed)[0][:2])
            return "python" + v + "-" + name
    return _gen


PYTHON_SUBPACKAGES = (
    Package(name="Cython",
            version="3.0.11", version_short="3.0", version_min="3.0", version_mex="4.0",
            distro_package_names={DISTRO_ID_DEBIAN: "cython3",
                                  DISTRO_ID_FEDORA: "python3-Cython",
                                  DISTRO_ID_SUSE: suse_pypackages_name_gen("Cython"),
                                  DISTRO_ID_ARCH: "cython",
                                  },
            ),
    Package(name="IDNA",
            version="3.10", version_short="3.10", version_min="2.0", version_mex="4.0",
            distro_package_names={DISTRO_ID_DEBIAN: "python3-idna",
                                  DISTRO_ID_FEDORA: "python3-idna",
                                  DISTRO_ID_SUSE: suse_pypackages_name_gen("idna"),
                                  DISTRO_ID_ARCH: "python-idna",
                                  },
            ),
    Package(name="Charset Normalizer",
            version="3.4.1", version_short="3.4", version_min="2.0.6", version_mex="4.0.0",
            distro_package_names={DISTRO_ID_DEBIAN: "python3-charset-normalizer",
                                  DISTRO_ID_FEDORA: "python3-charset-normalizer",
                                  DISTRO_ID_SUSE: suse_pypackages_name_gen("charset-normalizer"),
                                  DISTRO_ID_ARCH: "python-charset-normalizer",
                                  },
            ),
    Package(name="URLLib",
            version="2.4.0", version_short="2.4", version_min="1.0", version_mex="3.0",
            distro_package_names={DISTRO_ID_DEBIAN: "python3-urllib3",
                                  DISTRO_ID_FEDORA: "python3-urllib3",
                                  DISTRO_ID_SUSE: suse_pypackages_name_gen("urllib3"),
                                  DISTRO_ID_ARCH: "python-urllib3",
                                  },
            ),
    Package(name="Certifi",
            version="2025.4.26", version_short="2025.4", version_min="2021.0", version_mex="2026.0",
            distro_package_names={DISTRO_ID_DEBIAN: "python3-certifi",
                                  DISTRO_ID_FEDORA: "python3-certifi",
                                  DISTRO_ID_SUSE: suse_pypackages_name_gen("certifi"),
                                  DISTRO_ID_ARCH: "python-certifi",
                                  },
            ),
    Package(name="Requests",
            version="2.32.2", version_short="2.32", version_min="2.0", version_mex="3.0",
            distro_package_names={DISTRO_ID_DEBIAN: "python3-requests",
                                  DISTRO_ID_FEDORA: "python3-requests",
                                  DISTRO_ID_SUSE: suse_pypackages_name_gen("requests"),
                                  DISTRO_ID_ARCH: "python-requests",
                                  },
            ),
    Package(name="ZStandard",
            version="0.23.0", version_short="0.23", version_min="0.15.2", version_mex="1.0.0",
            distro_package_names={DISTRO_ID_DEBIAN: "python3-zstandard",
                                  DISTRO_ID_FEDORA: "python3-zstandard",
                                  DISTRO_ID_SUSE: suse_pypackages_name_gen("zstandard"),
                                  DISTRO_ID_ARCH: "python-zstandard",
                                  },
            ),
    Package(name="NumPy",
            version="1.26.4", version_short="1.26", version_min="1.14", version_mex="2.0",
            distro_package_names={DISTRO_ID_DEBIAN: "python3-numpy",
                                  DISTRO_ID_FEDORA: "python3-numpy",
                                  DISTRO_ID_SUSE: suse_pypackages_name_gen("numpy"),
                                  DISTRO_ID_ARCH: "python-numpy",
                                  },
            ),
    Package(name="NumPy Devel",
            version="1.26.4", version_short="1.26", version_min="1.14", version_mex="2.0",
            distro_package_names={DISTRO_ID_DEBIAN: ...,
                                  DISTRO_ID_FEDORA: ...,
                                  DISTRO_ID_SUSE: suse_pypackages_name_gen("numpy-devel"),
                                  DISTRO_ID_ARCH: ...,
                                  },
            ),
    Package(name="fastjsonschema",
            version="2.21.1", version_short="2.21", version_min="2.21", version_mex="3.0",
            distro_package_names={DISTRO_ID_DEBIAN: "python-fastjsonschema",
                                  DISTRO_ID_FEDORA: "python-fastjsonschema",
                                  DISTRO_ID_SUSE: suse_pypackages_name_gen("fastjsonschema"),
                                  DISTRO_ID_ARCH: "python-fastjsonschema",
                                  },
            ),
    Package(name="cattrs",
            version="25.1.1", version_short="25.1", version_min="25.1", version_mex="26.0",
            distro_package_names={DISTRO_ID_DEBIAN: "python-cattrs",
                                  DISTRO_ID_FEDORA: "python-cattrs",
                                  DISTRO_ID_SUSE: suse_pypackages_name_gen("cattrs"),
                                  DISTRO_ID_ARCH: "python-cattrs",
                                  },
            ),
)


# Packages required to build Blender, which are not included in the precompiled libraries.
PACKAGES_BASICS_BUILD = (
    Package(name="Basics Mandatory Build", is_group=True, is_mandatory=True, sub_packages=BUILD_MANDATORY_SUBPACKAGES),
    Package(name="Basics Optional Build", is_group=True, is_mandatory=False, sub_packages=BUILD_OPTIONAL_SUBPACKAGES),
    Package(name="Basic Critical Deps", is_group=True, is_mandatory=True, sub_packages=DEPS_CRITICAL_SUBPACKAGES),
)


# All packages, required or 'nice to have', to build Blender.
# Also covers (as best as possible) the dependencies provided by the precompiled libraries.
PACKAGES_ALL = (
    Package(name="Basics Mandatory Build", is_group=True, is_mandatory=True, sub_packages=BUILD_MANDATORY_SUBPACKAGES),
    Package(name="Basics Optional Build", is_group=True, is_mandatory=False, sub_packages=BUILD_OPTIONAL_SUBPACKAGES),
    Package(name="Basic Critical Deps", is_group=True, is_mandatory=True, sub_packages=DEPS_CRITICAL_SUBPACKAGES),
    Package(name="Basic Mandatory Deps", is_group=True, is_mandatory=True, sub_packages=DEPS_MANDATORY_SUBPACKAGES),
    Package(name="Basic Optional Deps", is_group=True, is_mandatory=False, sub_packages=DEPS_OPTIONAL_SUBPACKAGES),

    Package(name="Clang Format",
            version="10.0", version_short="10.0", version_min="6.0", version_mex="15.0",
            distro_package_names={DISTRO_ID_DEBIAN: "clang-format",
                                  DISTRO_ID_FEDORA: "clang",  # clang-format is part of the main clang package.
                                  DISTRO_ID_SUSE: "clang",  # clang-format is part of the main clang package.
                                  DISTRO_ID_ARCH: "clang",  # clang-format is part of the main clang package.
                                  },
            ),
    Package(name="Python", is_mandatory=True,
            version="3.11.11", version_short="3.11", version_min="3.11", version_mex="3.14",
            sub_packages=PYTHON_SUBPACKAGES,
            distro_package_names={DISTRO_ID_DEBIAN: "python3-dev",
                                  DISTRO_ID_FEDORA: "python3-devel",
                                  DISTRO_ID_SUSE: "python3-devel",
                                  DISTRO_ID_ARCH: "python",
                                  },
            ),
    Package(name="TBB Library", is_mandatory=True,
            version="2021.13.0", version_short="2021", version_min="2021.0.0", version_mex="2023.0.0",
            sub_packages=(),
            distro_package_names={DISTRO_ID_DEBIAN: "libtbb-dev",
                                  DISTRO_ID_FEDORA: "tbb-devel",
                                  DISTRO_ID_SUSE: "tbb-devel",
                                  DISTRO_ID_ARCH: "intel-oneapi-tbb",
                                  },
            ),
    Package(name="OpenColorIO Library", is_mandatory=False,
            version="2.4.1", version_short="2.4", version_min="2.0", version_mex="3.0",
            sub_packages=(),
            distro_package_names={DISTRO_ID_DEBIAN: "libopencolorio-dev",
                                  DISTRO_ID_FEDORA: "OpenColorIO-devel",
                                  DISTRO_ID_SUSE: "OpenColorIO-devel",
                                  DISTRO_ID_ARCH: "opencolorio",
                                  },
            ),
    Package(name="IMath Library", is_mandatory=False,
            version="3.2.1", version_short="3.2", version_min="3.0", version_mex="4.0",
            sub_packages=(),
            distro_package_names={DISTRO_ID_DEBIAN: "libimath-dev",
                                  DISTRO_ID_FEDORA: "imath-devel",
                                  DISTRO_ID_SUSE: "Imath-devel",
                                  DISTRO_ID_ARCH: "imath",
                                  },
            ),
    Package(name="OpenEXR Library", is_mandatory=False,
            version="3.3.2", version_short="3.3", version_min="3.0", version_mex="4.0",
            sub_packages=(),
            distro_package_names={DISTRO_ID_DEBIAN: "libopenexr-dev",
                                  DISTRO_ID_FEDORA: "openexr-devel",
                                  DISTRO_ID_SUSE: "openexr-devel",
                                  DISTRO_ID_ARCH: "openexr",
                                  },
            ),
    Package(name="OpenImageIO Library", is_mandatory=True,
            version="3.0.3.1", version_short="3.0", version_min="2.5.0", version_mex="3.1.0",
            sub_packages=(
                Package(name="OpenImageIO Tools", is_mandatory=False,
                        distro_package_names={DISTRO_ID_DEBIAN: "openimageio-tools",
                                              DISTRO_ID_FEDORA: "OpenImageIO-utils",
                                              DISTRO_ID_SUSE: "OpenImageIO",  # ???
                                              DISTRO_ID_ARCH: ...,
                                              },
                        ),
            ),
            distro_package_names={DISTRO_ID_DEBIAN: "libopenimageio-dev",
                                  DISTRO_ID_FEDORA: "OpenImageIO-devel",
                                  DISTRO_ID_SUSE: "OpenImageIO-devel",
                                  DISTRO_ID_ARCH: "openimageio",
                                  },
            ),
    Package(name="LLVM Library", is_mandatory=False,
            version="17.0.6", version_short="17.0", version_min="15.0", version_mex="18.0",
            sub_packages=(
                Package(name="Clang Compiler", is_mandatory=False,
                        distro_package_names={DISTRO_ID_DEBIAN: "clang",
                                              DISTRO_ID_FEDORA: "clang-devel",
                                              DISTRO_ID_SUSE: "clang-devel",
                                              DISTRO_ID_ARCH: "clang",
                                              },
                        ),
                Package(name="Clang Library", is_mandatory=False,
                        distro_package_names={DISTRO_ID_DEBIAN: "libclang-dev",
                                              DISTRO_ID_FEDORA: ...,
                                              DISTRO_ID_SUSE: ...,
                                              DISTRO_ID_ARCH: ...,
                                              },
                        ),
            ),
            distro_package_names={DISTRO_ID_DEBIAN: "llvm-dev",
                                  DISTRO_ID_FEDORA: "llvm-devel",
                                  DISTRO_ID_SUSE: "llvm-devel",
                                  DISTRO_ID_ARCH: "llvm",
                                  },
            ),
    Package(name="OpenShadingLanguage Library", is_mandatory=False,
            version="1.14.3.0", version_short="1.14", version_min="1.11", version_mex="2.0",
            sub_packages=(),
            distro_package_names={DISTRO_ID_DEBIAN: None,  # No package currently.
                                  DISTRO_ID_FEDORA: "openshadinglanguage-devel",
                                  DISTRO_ID_SUSE: "OpenShadingLanguage-devel",
                                  DISTRO_ID_ARCH: "openshadinglanguage",
                                  },
            ),
    Package(name="OpenSubDiv Library", is_mandatory=False,
            version="3.6.0", version_short="3.6", version_min="3.5", version_mex="4.0",
            sub_packages=(),
            distro_package_names={DISTRO_ID_DEBIAN: "libosd-dev",
                                  DISTRO_ID_FEDORA: "opensubdiv-devel",
                                  DISTRO_ID_SUSE: None,
                                  DISTRO_ID_ARCH: "opensubdiv",
                                  },
            ),
    Package(name="OpenVDB Library", is_mandatory=False,
            version="12.0.0", version_short="12.0", version_min="11.0", version_mex="13.0",
            sub_packages=(
                # Assume packaged versions of the dependencies are compatible with OpenVDB package.
                Package(name="OpenVDB Dependencies", is_mandatory=False, is_group=True,
                        sub_packages=(
                            Package(name="Blosc Library", is_mandatory=False,
                                    distro_package_names={DISTRO_ID_DEBIAN: "libblosc-dev",
                                                          DISTRO_ID_FEDORA: "blosc-devel",
                                                          DISTRO_ID_SUSE: "blosc-devel",
                                                          DISTRO_ID_ARCH: "blosc",
                                                          },
                                    ),
                            Package(name="NanoVDB Library", is_mandatory=False,
                                    distro_package_names={DISTRO_ID_DEBIAN: "libnanovdb-dev",
                                                          DISTRO_ID_FEDORA: ...,  # Part of OpenVDB package.
                                                          DISTRO_ID_SUSE: None,
                                                          DISTRO_ID_ARCH: ...,   # Part of OpenVDB package.
                                                          },
                                    ),
                        ),
                        ),
            ),
            distro_package_names={DISTRO_ID_DEBIAN: "libopenvdb-dev",
                                  DISTRO_ID_FEDORA: "openvdb-devel",
                                  DISTRO_ID_SUSE: None,  # No known package yet.
                                  DISTRO_ID_ARCH: "openvdb",
                                  },
            ),
    Package(name="Alembic Library", is_mandatory=False,
            version="1.8.3", version_short="1.8", version_min="1.7", version_mex="2.0",
            sub_packages=(),
            distro_package_names={DISTRO_ID_DEBIAN: None,
                                  DISTRO_ID_FEDORA: "alembic-devel",
                                  DISTRO_ID_SUSE: "alembic-devel",
                                  DISTRO_ID_ARCH: "alembic",
                                  },
            ),
    Package(name="MaterialX Library", is_mandatory=False,
            version="1.39.10", version_short="1.39", version_min="1.38", version_mex="1.40",
            sub_packages=(),
            distro_package_names={DISTRO_ID_DEBIAN: None,
                                  DISTRO_ID_FEDORA: None,
                                  DISTRO_ID_SUSE: None,
                                  DISTRO_ID_ARCH: "materialx-git",
                                  },
            ),
    Package(name="USD Library", is_mandatory=False,
            version="25.02", version_short="25.02", version_min="24.05", version_mex="26.00",
            sub_packages=(),
            distro_package_names={DISTRO_ID_DEBIAN: None,
                                  DISTRO_ID_FEDORA: "usd-devel",
                                  DISTRO_ID_SUSE: None,
                                  DISTRO_ID_ARCH: "usd",  # No official package, in AUR only currently.
                                  },
            ),
    Package(name="Embree Library", is_mandatory=False,
            version="4.4.0", version_short="4.4", version_min="4.3", version_mex="5.0",
            sub_packages=(),
            distro_package_names={DISTRO_ID_DEBIAN: "libembree-dev",
                                  DISTRO_ID_FEDORA: "embree-devel",
                                  DISTRO_ID_SUSE: "embree-devel",
                                  DISTRO_ID_ARCH: "embree",
                                  },
            ),
    Package(name="OpenImageDenoiser Library", is_mandatory=False,
            version="2.3.3", version_short="2.3", version_min="2.0.0", version_mex="3.0",
            sub_packages=(),
            distro_package_names={DISTRO_ID_DEBIAN: None,
                                  DISTRO_ID_FEDORA: "oidn-devel",
                                  DISTRO_ID_SUSE: "OpenImageDenoise-devel",
                                  DISTRO_ID_ARCH: "openimagedenoise",
                                  },
            ),
    Package(name="Level Zero Library", is_mandatory=False,
            version="1.19.2", version_short="1.19", version_min="1.7", version_mex="2.0",
            sub_packages=(),
            distro_package_names={DISTRO_ID_DEBIAN: "libze-dev",
                                  DISTRO_ID_FEDORA: "oneapi-level-zero-devel",
                                  DISTRO_ID_SUSE: "level-zero-devel",
                                  DISTRO_ID_ARCH: "level-zero-headers",  # ???
                                  },
            ),
    Package(name="OpenPGL Library", is_mandatory=False,
            version="0.6.0", version_short="0.6", version_min="0.5.0", version_mex="0.7",
            sub_packages=(),
            distro_package_names={DISTRO_ID_DEBIAN: None,
                                  DISTRO_ID_FEDORA: "openpgl-devel",
                                  DISTRO_ID_SUSE: None,
                                  DISTRO_ID_ARCH: "openpgl",
                                  },
            ),
    Package(name="XROpenXR Library", is_mandatory=False,
            version="1.0.22", version_short="1.0", version_min="1.0.8", version_mex="2.0",
            sub_packages=(),
            distro_package_names={DISTRO_ID_DEBIAN: "libopenxr-dev",
                                  DISTRO_ID_FEDORA: None,
                                  DISTRO_ID_SUSE: None,
                                  DISTRO_ID_ARCH: "openxr",
                                  },
            ),
    Package(name="FFMPEG Library", is_mandatory=False,
            version="7.1.1", version_short="7.1", version_min="4.0", version_mex="8.0",
            sub_packages=(
                Package(name="AVDevice FFMPEG Library", is_mandatory=False,
                        distro_package_names={DISTRO_ID_DEBIAN: "libavdevice-dev",
                                              DISTRO_ID_FEDORA: ...,
                                              DISTRO_ID_SUSE: ...,
                                              DISTRO_ID_ARCH: ...,
                                              },
                        ),
            ),
            distro_package_names={DISTRO_ID_DEBIAN: "ffmpeg",
                                  DISTRO_ID_FEDORA: "ffmpeg-free-devel",
                                  DISTRO_ID_SUSE: "ffmpeg-devel",
                                  DISTRO_ID_ARCH: "ffmpeg",
                                  },
            ),
    Package(name="harfbuzz", is_mandatory=False,
            version="10.0.1", version_short="10", version_min="5.1.0", version_mex="20.0.0",
            sub_packages=(),
            distro_package_names={DISTRO_ID_DEBIAN: "libharfbuzz-dev",
                                  DISTRO_ID_FEDORA: "harfbuzz-devel",
                                  DISTRO_ID_SUSE: "harfbuzz-devel",
                                  DISTRO_ID_ARCH: "harfbuzz",
                                  },
            ),
    Package(name="manifold", is_mandatory=False,
            version="3.1.0", version_short="3.1", version_min="3.1.0", version_mex="4.0.0",
            sub_packages=(),
            distro_package_names={DISTRO_ID_DEBIAN: None,
                                  DISTRO_ID_FEDORA: None,
                                  DISTRO_ID_SUSE: None,
                                  DISTRO_ID_ARCH: None,
                                  },
            ),
)


class ProgressBar:
    """Very basic progress bar printing in the console."""

    def __init__(self, min_value=0, max_value=100, print_len=80, is_known_limit=True):
        self.value = 0
        self.min_value = min_value
        self.max_value = max_value
        self.print_len = print_len
        self.is_known_limit = is_known_limit
        self.print_stdout()

    def update(self, steps=1):
        self.value += steps
        self.print_stdout()

    def finish(self):
        print("\033[2K\r", end="")

    def print_stdout(self):
        print("\r", self, end="")

    def __repr__(self):
        value_print_len = self.print_len - 2
        range_value = self.max_value - self.min_value
        diff_to_min = self.value - self.min_value
        value = (self.value % range_value) / range_value * value_print_len
        if (diff_to_min // range_value) % 2 == 0:
            value_str = "*" * int(value) + " " * (value_print_len - int(value))
        else:
            value_str = " " * int(value) + "*" * (value_print_len - int(value))
        if self.is_known_limit:
            return f"[{value_str}]"
        return f">{value_str}<"


class PackageInstaller:
    """Parent class of all package installers, does nothing but printing list of packages and defining the 'interface'.
    """
    _instance = None

    def __new__(cls, settings):
        if cls._instance is None:
            cls._instance = super(PackageInstaller, cls).__new__(cls)
        cls._instance.settings = settings
        return cls._instance

    def run_command(self, command):
        """Basic wrapper around ``subprocess.Popen``, mimicking ``subprocess.run`` with a basic progress bar."""
        # First dummy call to get user password for `sudo`. Otherwise the progress bar on actual commands
        # makes it impossible for users to enter their password.
        if not self.settings.no_sudo:
            subprocess.run([MAYSUDO, "echo"], capture_output=True)

        p = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        pbar = ProgressBar(is_known_limit=False)
        while p.poll() is None:
            pbar.update(steps=2)
            time.sleep(0.05)
        pbar.finish()
        return subprocess.CompletedProcess(
            args=command,
            returncode=p.returncode,
            stdout=p.stdout.read(),
            stderr=p.stderr.read())

    @property
    def can_install(self):
        return not self.settings.no_sudo and self.__class__ != PackageInstaller

    # Version utils, should not need to be redefined in each sub-class.
    # ----------
    _re_version_sanitize = re.compile(r"(?P<version>([0-9]+\.?)+([0-9]+)).*")

    @classmethod
    def version_sanitize(cls, version):
        """
        Sanitize a version string by removing 'extras' like `_RC2` in `1.2_RC2`.
        Note that the version string is expected to start with at least two numbers
        separated by a dot.
        """
        version = cls._re_version_sanitize.search(version)
        return version["version"] if version is not None else version

    @classmethod
    def version_tokenize(cls, *args):
        """
        Tokenize an iterable of sanitized version strings into tuples of integers of a same length,
        filling missing items with zero values.
        """
        versions = tuple(tuple(int(i) for i in cls.version_sanitize(v).split(".")) for v in args)
        maxlen = max(len(v) for v in versions)
        return tuple(v + (0,) * (maxlen - len(v)) for v in versions)

    @classmethod
    def version_match(cls, version, ref_version):
        """
        Return True if the ``version`` string falls into the version range covered by the ``ref_version`` string.
        ``version`` should be at least as long as ``ref_version`` (in term of version number items).
        E.g. 3.3.2:
          - matches 3.3
          - matches 3.3.2
          - does not match 3.4
          - does not match 3.3.0
          - does not match 3.3.2.5
        """
        version = cls.version_tokenize(version)[0]
        ref_version = cls.version_tokenize(ref_version)[0]
        len_ref_version = len(ref_version)
        return (len(version) >= len_ref_version) and version[:len_ref_version] == ref_version

    @classmethod
    def versions_range_gen(cls, package, versions_set):
        def do_yield(version, versions_set):
            if version not in versions_set:
                versions_set.add(version)
                yield version
        MEX_RANGE_DIFF = 5
        VERSION_FACTOR_MAX = 100
        VERSION_FACTOR_MINRANGE_MULTIPLIER = 2
        VERSION_FACTOR_STEP_DIVIDER = 10

        version = cls.version_tokenize(package.version)[0][:2]
        version_min = cls.version_tokenize(package.version_min)[0][:2]
        version_mex = cls.version_tokenize(package.version_mex)[0][:2]

        version_major = version[0]
        version_major_min = version_min[0]
        version_major_mex = version_mex[0]
        if version_major_mex - version_major_min > 1:
            yield from do_yield(str(version_major), versions_set)
            for i in range(1, MEX_RANGE_DIFF):
                if version_major + i < version_major_mex:
                    yield from do_yield(str(version_major + i), versions_set)
            for i in range(1, MEX_RANGE_DIFF):
                if version_major - i >= version_major_min:
                    yield from do_yield(str(version_major - i), versions_set)
            return
        if len(version) < 2:
            yield from do_yield(str(version_major), versions_set)
            return

        version_minor = version[1]
        version_minor_min = 0 if len(version_min) < 2 else version_min[1]
        version_minor_mex = 0 if len(version_mex) < 2 else version_mex[1]
        version_minor_fac = 1
        vfac = VERSION_FACTOR_MAX
        while vfac > 1:
            version_minor_minrange = vfac * VERSION_FACTOR_MINRANGE_MULTIPLIER
            is_vfac_in_range = (version_minor >= vfac or version_minor_min >= vfac or version_minor_mex >= vfac)
            is_version_range_big_enough = (version_major_min != version_major_mex or
                                           version_minor_mex - version_minor_min >= version_minor_minrange)
            if (is_vfac_in_range and is_version_range_big_enough and version_minor % vfac == 0):
                version_minor_fac = vfac
                break
            vfac = vfac // VERSION_FACTOR_STEP_DIVIDER
        yield from do_yield(str(version_major) + "." + str(version_minor), versions_set)
        yield from do_yield(str(version_major) + str(version_minor), versions_set)
        for i in range(1, MEX_RANGE_DIFF):
            i *= version_minor_fac
            if version_minor + i < version_minor_mex or version_major_mex > version_major:
                yield from do_yield(str(version_major) + "." + str(version_minor + i), versions_set)
                yield from do_yield(str(version_major) + str(version_minor + i), versions_set)
        for i in range(1, MEX_RANGE_DIFF):
            i *= version_minor_fac
            if version_minor - i >= version_minor_min or (version_minor - i >= 0 and version_major_min < version_major):
                yield from do_yield(str(version_major) + "." + str(version_minor - i), versions_set)
                yield from do_yield(str(version_major) + str(version_minor - i), versions_set)
        return

    # Generic package handling, should not need to be redefined in each sub-class.
    # Note that they may depend on some non-generic functions defined below though.
    # ----------

    def package_query_version_get(self, package_distro_name):
        """Return the available, potentially cached if already looked-up, version of the given package."""
        if not hasattr(self, "_package_versions_cache"):
            self._package_versions_cache = {}
        return self._package_versions_cache.setdefault(package_distro_name,
                                                       self.package_query_version_get_impl(package_distro_name))

    def package_query_version_match(self, package_distro_name, ref_version):
        """Check if given package name matches given reference version."""
        version = self.package_query_version_get(package_distro_name)
        if version is None:
            return False
        if version is ...:
            # Only from PackageInstaller base class.
            assert self.__class__ is PackageInstaller
            return True
        return self.version_match(version, ref_version)

    def package_query_version_ge_lt(self, package_distro_name, ref_version_min, ref_version_mex):
        """Check if given package name fits in between given minimal and maximal excluded versions."""
        version = self.package_query_version_get(package_distro_name)
        if version is None:
            return False
        if version is ...:
            # Only from PackageInstaller base class.
            assert self.__class__ is PackageInstaller
            return True
        version, ref_version_min, ref_version_mex = self.version_tokenize(version, ref_version_min, ref_version_mex)
        return ref_version_min <= version < ref_version_mex

    def packages_database_update(self):
        """Ensure that data-base of available packages is up-to-date."""
        if self._update_command is ...:
            # Only from PackageInstaller base class.
            assert self.__class__ is PackageInstaller
            return True

        if self.settings.no_sudo:
            self.settings.logger.debug("\t--no-sudo enabled, no update of packages info.")
            return True

        self.settings.logger.info("Trying to update packages info.")
        result = self.run_command(self._update_command)
        if result.returncode != 0:
            self.settings.logger.critical(f"\tFailed to update packages info:\n\t{repr(result)}\n")
            exit(1)
        self.settings.logger.info("Done.\n")
        self.settings.logger.debug(repr(result))
        return result.returncode == 0

    def package_find(self, package, package_distro_name):
        """
        Generic heuristics to try and find 'best matching version' for a given package.
        For most packages it just ensures given package name version matches the exact version from the ``package``,
        or at least fits within the [version_min, version_mex[ range.
        But some, like e.g. Python or LLVM, can have packages available for several versions,
        with complex naming (like ``python3.10``, ``llvm-9-dev``, etc.).
        This code attempts to find the best matching one possible, based on a set of 'possible names'
        generated by the distribution-specific ``package_name_version_gen`` generator.
        """
        # Check 'exact' version match on given name.
        if self.package_query_version_match(package_distro_name, package.version_short):
            return package_distro_name
        # Check exact version match on special 'versioned' names (like `python3.10-dev' e.g.).
        for pn in self.package_name_version_gen(package, package_distro_name):
            if self.package_query_version_match(pn, package.version_short):
                return pn
        # Check version in supported range.
        if self.package_query_version_ge_lt(package_distro_name, package.version_min, package.version_mex):
            return package_distro_name
        # Check version in supported range on special 'versioned' names (like `llvm-11-dev' e.g.).
        for pn in self.package_name_version_gen(package, package_distro_name, do_range_version_names=True):
            if self.package_query_version_ge_lt(pn, package.version_min, package.version_mex):
                return pn
        return None

    def package_distro_name(self, package, parent_packages):
        """
        Generate a collection of distro-specific package names from given package.
        Typically only one name, unless given package is a group one, in which case all its
        sub-packages' distro-specific names are returned in the list.
        """
        distro_id = self.settings.distro_id

        packages_distro_names = []
        if package.is_group:
            for p in package.sub_packages:
                p.is_mandatory = package.is_mandatory
                if distro_id is ...:
                    packages_distro_names.append(p.name)
                else:
                    package_name = p.distro_package_names[distro_id]
                    if callable(package_name):
                        package_name = package_name(p, parent_packages)
                    if package_name not in {None, ...}:
                        packages_distro_names.append(package_name)
                if p.is_group:
                    packages_distro_names += self.package_distro_name(p, parent_packages + (package,))
            return packages_distro_names

        if distro_id is ...:
            package_name = package.name
        else:
            package_name = package.distro_package_names[distro_id]
            if callable(package_name):
                package_name = package_name(package, parent_packages)
        return [package_name]

    def packages_install(self, packages, parent_packages=()):
        """
        Install all given packages and their sub-packages.
        This call is recursive, parent_packages is a tuple of the ancestors of current ``package``, in calling order
        (grand-parent, parent).
        """
        def package_info_name(package, parent_packages):
            packages = parent_packages + (package,)
            return " ".join(p.name for p in packages)

        distro_id = self.settings.distro_id
        for package in packages:
            if not package.name:
                continue
            info_name = package_info_name(package, parent_packages)

            if package.is_group:
                if self.can_install:
                    self.settings.logger.info(f"Trying to install group of packages {info_name}.")
                if not package.sub_packages:
                    self.settings.logger.critical(f"Invalid group of packages {info_name}")
                    exit(1)
                success = self.group_package_install(package, parent_packages)
                if self.can_install:
                    if not success:
                        self.settings.logger.info("Failed.\n")
                    else:
                        self.settings.logger.info("Done.\n")
                continue

            package_distro_name = self.package_distro_name(package, parent_packages)[0]
            if package_distro_name is None:
                if package.is_mandatory:
                    self.settings.logger.warning(
                        f"Mandatory package {info_name} is not defined for {distro_id} distribution, "
                        "Blender will likely not build at all without it.\n")
                else:
                    self.settings.logger.info(f"Package {info_name} is not defined for {distro_id} distribution.\n")
                continue
            if package_distro_name is ...:
                self.settings.logger.debug(f"Package {info_name} is not required for {distro_id} distribution.\n")
                continue

            # Inherit parent version info if needed and possible.
            if package.version is None:
                if not parent_packages:
                    self.settings.logger.critical(
                        f"Package {info_name} ({package_distro_name}) has no version information.")
                    exit(1)
                package.version = parent_packages[-1].version
                package.version_short = parent_packages[-1].version_short
                package.version_min = parent_packages[-1].version_min
                package.version_mex = parent_packages[-1].version_mex

            if self.can_install:
                self.settings.logger.info(f"Trying to install package {info_name} ({package_distro_name}).")
            success = self.single_package_install(package, parent_packages)
            if not success:
                if self.can_install:
                    self.settings.logger.info("Failed.\n")
                continue

            if self.can_install:
                self.settings.logger.info("Done.\n")
            if package.sub_packages:
                self.packages_install(package.sub_packages, parent_packages + (package,))

    def single_package_install(self, package, parent_packages=()):
        """Install a normal, single package."""
        package_distro_name = self.package_distro_name(package, parent_packages)[0]
        package_name = self.package_find(package, package_distro_name)
        if package_name is None:
            if package.is_mandatory:
                self.settings.logger.critical(
                    f"\tFailed to find a matching mandatory {package_distro_name} "
                    f"(within versions range [{package.version_min}, {package.version_mex}[).")
                exit(1)
            self.settings.logger.warning(
                f"\tFailed to find a matching {package_distro_name} "
                f"(within versions range [{package.version_min}, {package.version_mex}[).")
            return False

        if self._install_command is ...:
            # Only from PackageInstaller base class.
            assert self.__class__ is PackageInstaller
            self.settings.logger.info(f"\tWould install {package_distro_name}.")
            return True

        if self.settings.no_sudo:
            self.settings.logger.warning(f"\t--no-sudo enabled, impossible to run apt-get install for {package_name}.")
            return True

        package_version = self.package_query_version_get(package_name)
        self.settings.logger.info(f"\tInstalling package {package_name} ({package_version}).")
        cmd = self._install_command + [package_name]
        result = self.run_command(cmd)
        if result.returncode != 0:
            self.settings.logger.critical(f"\tFailed to install {package_name}:\n\t{repr(result)}")
            exit(1)

        package_version_installed = self.package_installed_version_get(package_name)
        if package_version_installed != package_version:
            self.settings.logger.critical(f"\tInstalled version of {package_name} does not match expected value "
                                          f"({package_version_installed} vs {package_version})")
            exit(1)
        self.settings.logger.debug(repr(result))
        package.version_installed = package_version_installed
        return result.returncode == 0

    def group_package_install(self, package, parent_packages=()):
        """Install a group package and all of its sub-packages."""
        packages_distro_names = self.package_distro_name(package, parent_packages)
        if self._install_command is ...:
            # Only from PackageInstaller base class.
            assert self.__class__ is PackageInstaller
            packages_info_names = ',\n\t\t\t'.join(packages_distro_names)
            self.settings.logger.info(
                f"\tWould install group of packages {package.name}:\n\t\t\t{packages_info_names}.")
            return True

        if self.settings.no_sudo:
            self.settings.logger.warning(
                f"\t--no-sudo enabled, impossible to run apt-get install for {packages_distro_names}.")
            return True

        if not packages_distro_names:
            return True

        self.settings.logger.info(f"\tInstalling packages {', '.join(packages_distro_names)}.")
        cmd = self._install_command + [*packages_distro_names]
        result = self.run_command(cmd)
        if result.returncode != 0:
            if package.is_mandatory:
                self.settings.logger.critical(f"\tFailed to install packages:\n\t{repr(result)}")
                exit(1)
            else:
                self.settings.logger.warning(
                    f"\tFailed to find install all of {packages_distro_names}:\n\t{repr(result)}")
        self.settings.logger.debug(repr(result))
        return result.returncode == 0

    # Implementation-specific, will most likely need to be re-defined in sub-classes.
    # ----------

    # Command and options to pass to install packages in specific distro (as a list, for `subprocess.run`).
    # Will be appended with package or list of packages to install.
    _install_command = ...

    # Command and options to pass to update packages data-base in specific distro (as a list, for `subprocess.run`).
    _update_command = ...

    def package_installed_version_get(self, package_distro_name):
        """Return the installed version of the given package."""
        return ...

    def package_query_version_get_impl(self, package_distro_name):
        """Return the available version of the given package."""
        return ...

    def package_name_version_gen(self, package, package_distro_name, version, suffix="", do_range_version_names=False):
        """Generator for all potential names for a given package 'base name'."""
        yield package_distro_name


class PackageInstallerDebian(PackageInstaller):
    """Debian-like package installer, using apt and dpkg-query."""
    _instance = None

    def __new__(cls, settings):
        if cls._instance is None:
            cls._instance = super(PackageInstallerDebian, cls).__new__(cls, settings)
        return cls._instance

    _version_regex_base_pattern = r"(?:[0-9]+:)?(?P<version>([0-9]+\.?)+([0-9]+)).*"
    _re_version = re.compile(_version_regex_base_pattern)
    _re_version_candidate = re.compile(r"Candidate:\s*" + _version_regex_base_pattern)

    _install_command = [MAYSUDO, "apt", "install", "-y"]
    _update_command = [MAYSUDO, "apt", "update"]

    def package_installed_version_get(self, package_distro_name):
        cmd = ["dpkg-query", "-W", "-f", "${Version}", package_distro_name]
        result = self.run_command(cmd)
        version = self._re_version.search(str(result.stdout))
        return version["version"] if version is not None else None

    def package_query_version_get_impl(self, package_distro_name):
        # `apt-cache policy` will do partial matching (so e.g. `python3.11` will also match `libpython3.11-stdlib`).
        # Use `apt show` first to ensure exact package name is available (stdout will be empty if no package of
        # requested name is known).
        cmd = ["apt", "show", package_distro_name]
        result = self.run_command(cmd)
        if not result.stdout:
            return None
        cmd = ["apt-cache", "policy", package_distro_name]
        result = self.run_command(cmd)
        version = self._re_version_candidate.search(str(result.stdout))
        return version["version"] if version is not None else None

    def package_name_version_gen(
            self,
            package,
            package_distro_name,
            version=...,
            suffix="",
            do_range_version_names=False):
        if version is ...:
            version = package.version_short
        # Generate versions variants with version between main name and '-dev' suffix, if any.
        tmp_package_name = package_distro_name.removesuffix("-dev")
        if tmp_package_name != package_distro_name:
            for pn in self.package_name_version_gen(
                    package,
                    tmp_package_name,
                    version,
                    suffix="-dev" + suffix,
                    do_range_version_names=do_range_version_names):
                yield pn
        # Strip any 'version-like' numbers at the end of the package name (already stripped of '-dev' suffix)
        # and generate versions variants out of it.
        tmp_package_name = tmp_package_name.rstrip("0123456789.-")
        if tmp_package_name != package_distro_name:
            for pn in self.package_name_version_gen(
                    package,
                    tmp_package_name,
                    version,
                    suffix=suffix,
                    do_range_version_names=do_range_version_names):
                yield pn
        # Generate version variants from given package name.
        versions = [version]
        if do_range_version_names:
            # Also search for major version numbers around the target one, within to allowed range.
            # Necessary for packages like llvm e.g.
            versions += [*self.versions_range_gen(package, set(versions))]
        for v in versions:
            yield package_distro_name + v + suffix
            yield package_distro_name + "-" + v + suffix


class PackageInstallerFedora(PackageInstaller):
    """Fedora-like package installer, using dnf."""
    _instance = None

    def __new__(cls, settings):
        if cls._instance is None:
            cls._instance = super(PackageInstallerFedora, cls).__new__(cls, settings)
        return cls._instance

    _re_version = re.compile(r"Version\s*:\s*(?:[0-9]+:)?(?P<version>([0-9]+\.?)+([0-9]+)).*")

    _install_command = [MAYSUDO, "dnf", "install", "-y"]
    _update_command = [MAYSUDO, "dnf", "check-update"]

    def package_version_get(self, command):
        result = self.run_command(command)
        version = self._re_version.search(str(result.stdout))
        return version["version"] if version is not None else None

    def package_installed_version_get(self, package_distro_name):
        return self.package_version_get([MAYSUDO, "dnf", "info", "--installed", package_distro_name])

    def package_query_version_get_impl(self, package_distro_name):
        return self.package_version_get([MAYSUDO, "dnf", "info", package_distro_name])

    def package_name_version_gen(
            self,
            package,
            package_distro_name,
            version=...,
            suffix="",
            do_range_version_names=False):
        if version is ...:
            version = package.version_short
        # Generate versions variants with version between main name and '-devel' suffix, if any.
        tmp_package_name = package_distro_name.removesuffix("-devel")
        if tmp_package_name != package_distro_name:
            for pn in self.package_name_version_gen(
                    package,
                    tmp_package_name,
                    version,
                    suffix="-devel" + suffix,
                    do_range_version_names=do_range_version_names):
                yield pn
        # Strip any 'version-like' numbers at the end of the package name (already stripped of '-devel' suffix)
        # and generate versions variants out of it.
        tmp_package_name = tmp_package_name.rstrip("0123456789.-")
        if tmp_package_name != package_distro_name:
            for pn in self.package_name_version_gen(
                    package,
                    tmp_package_name,
                    version,
                    suffix=suffix,
                    do_range_version_names=do_range_version_names):
                yield pn
        # Generate version variants from given package name.
        versions = [version]
        if do_range_version_names:
            # Also search for major version numbers around the target one, within to allowed range.
            # Necessary for packages like llvm e.g.
            versions += [*self.versions_range_gen(package, set(versions))]
        for v in versions:
            yield package_distro_name + v + suffix
            yield package_distro_name + "-" + v + suffix


class PackageInstallerSuse(PackageInstaller):
    """Suse-like package installer, using zypper."""
    _instance = None

    def __new__(cls, settings):
        if cls._instance is None:
            cls._instance = super(PackageInstallerSuse, cls).__new__(cls, settings)
        return cls._instance

    _re_version = re.compile(r"Version\s*:\s*(?:[0-9]+:)?(?P<version>([0-9]+\.?)+([0-9]+)).*")
    _re_installed = re.compile(r"Installed\s*:\s*Yes")

    _install_command = [MAYSUDO, "zypper", "--non-interactive", "install"]
    _update_command = [MAYSUDO, "zypper", "refresh"]

    def package_version_get(self, command_result):
        version = self._re_version.search(str(command_result.stdout))
        return version["version"] if version is not None else None

    def package_installed_version_get(self, package_distro_name):
        result = self.run_command([MAYSUDO, "zypper", "info", package_distro_name])
        is_installed = self._re_installed.search(str(result.stdout))
        return self.package_version_get(result) if is_installed is not None else None

    def package_query_version_get_impl(self, package_distro_name):
        result = self.run_command([MAYSUDO, "zypper", "info", package_distro_name])
        return self.package_version_get(result)

    def package_name_version_gen(
            self,
            package,
            package_distro_name,
            version=...,
            suffix="",
            do_range_version_names=False):
        if version is ...:
            version = package.version_short
        # Generate versions variants with version between main name and '-devel' suffix, if any.
        tmp_package_name = package_distro_name.removesuffix("-devel")
        if tmp_package_name != package_distro_name:
            for pn in self.package_name_version_gen(
                    package,
                    tmp_package_name,
                    version,
                    suffix="-devel" + suffix,
                    do_range_version_names=do_range_version_names):
                yield pn
        # Strip any 'version-like' numbers at the end of the package name (already stripped of '-devel' suffix)
        # and generate versions variants out of it.
        tmp_package_name = tmp_package_name.rstrip("0123456789.-")
        if tmp_package_name != package_distro_name:
            for pn in self.package_name_version_gen(
                    package,
                    tmp_package_name,
                    version,
                    suffix=suffix,
                    do_range_version_names=do_range_version_names):
                yield pn
        # Generate version variants from given package name.
        versions = [version]
        if do_range_version_names:
            # Also search for major version numbers around the target one, within to allowed range.
            # Necessary for packages like llvm e.g.
            versions += [*self.versions_range_gen(package, set(versions))]
        for v in versions:
            yield package_distro_name + v + suffix
            yield package_distro_name + "-" + v + suffix


class PackageInstallerArch(PackageInstaller):
    """Arch-like package installer, using pacman."""
    _instance = None

    def __new__(cls, settings):
        if cls._instance is None:
            cls._instance = super(PackageInstallerArch, cls).__new__(cls, settings)
        return cls._instance

    _re_version = re.compile(r"Version\s*:\s*(?:[0-9]+:)?(?P<version>([0-9]+\.?)+([0-9]+)).*")

    _install_command = [MAYSUDO, "pacman", "-S", "--needed", "--noconfirm"]
    _update_command = [MAYSUDO, "pacman", "-Sy"]

    def package_version_get(self, command):
        result = self.run_command(command)
        version = self._re_version.search(str(result.stdout))
        return version["version"] if version is not None else None

    def package_installed_version_get(self, package_distro_name):
        return self.package_version_get(["pacman", "-Qi", package_distro_name])

    def package_query_version_get_impl(self, package_distro_name):
        return self.package_version_get(["pacman", "-Si", package_distro_name])

    def package_name_version_gen(
            self,
            package,
            package_distro_name,
            version=...,
            suffix="",
            do_range_version_names=False):
        if version is ...:
            version = package.version_short
        # Generate versions variants with version after the main name.
        tmp_package_name = package_distro_name
        # Strip any 'version-like' numbers at the end of the package name (already stripped of '-dev' suffix)
        # and generate versions variants out of it.
        tmp_package_name = tmp_package_name.rstrip("0123456789.-")
        if tmp_package_name != package_distro_name:
            for pn in self.package_name_version_gen(
                    package,
                    tmp_package_name,
                    version,
                    suffix=suffix,
                    do_range_version_names=do_range_version_names):
                yield pn
        # Generate version variants from given package name.
        versions = [version]
        if do_range_version_names:
            # Also search for major version numbers around the target one, within to allowed range.
            # Necessary for packages like llvm e.g.
            versions += [*self.versions_range_gen(package, set(versions))]
        for v in versions:
            yield package_distro_name + v + suffix
            yield package_distro_name + "-" + v + suffix


DISTRO_IDS_INSTALLERS = {
    ...: PackageInstaller,
    DISTRO_ID_DEBIAN: PackageInstallerDebian,
    DISTRO_ID_FEDORA: PackageInstallerFedora,
    DISTRO_ID_SUSE: PackageInstallerSuse,
    DISTRO_ID_ARCH: PackageInstallerArch,
}


def get_distro(settings):
    if settings.distro_id is not ...:
        settings.logger.info(f"Distribution identifier forced by user to {settings.distro_id}.")
        return settings.distro_id
    import platform
    if hasattr(platform, "freedesktop_os_release"):
        info = platform.freedesktop_os_release()
        ids = [info["ID"]]
        if "ID_LIKE" in info:
            # ids are space separated and ordered by precedence.
            ids.extend(info["ID_LIKE"].split())
        for distro_id in ids:
            if distro_id in DISTRO_IDS_INSTALLERS:
                settings.distro_id = distro_id
                return distro_id
        settings.logger.warning(f"Distribution IDs do not match any supported one by this script ({ids})")

    settings.logger.warning("A valid distribution ID could not be found using `platform.freedesktop_os_release`, "
                            "now trying a lower-level check for specific files")
    if os.path.exists("/etc/debian_version"):
        distro_id = DISTRO_ID_DEBIAN
    elif os.path.exists("/etc/redhat-release"):
        distro_id = DISTRO_ID_FEDORA
    elif os.path.exists("/etc/SuSE-release"):
        distro_id = DISTRO_ID_SUSE
    elif os.path.exists("/etc/arch-release"):
        distro_id = DISTRO_ID_ARCH
    if distro_id in DISTRO_IDS_INSTALLERS:
        settings.distro_id = distro_id
        return distro_id

    settings.distro_id = ...
    return ...


def get_distro_package_installer(settings):
    distro_id = get_distro(settings)
    if distro_id is ...:
        settings.logger.warning("No valid distribution ID found, please try to set it using the `--distro-id` option")
    else:
        settings.logger.info(f"Distribution identified as '{distro_id}'")
    return DISTRO_IDS_INSTALLERS[distro_id](settings)


def argparse_create():
    import argparse

    # When --help or no args are given, print this help
    usage_text = (
        "Attempt to install dependencies to build Blender from current linux distribution's packages only.\n"
        "\n"
        "By default, only installs critical tools and dependencies to build Blender, excluding any library provided\n"
        "by the precompiled git-lfs repository.\n"
        "`make update` should then be ran after this script to download all precompiled libraries.\n"
        "\n"
        "When ran with the `--all` option, this tool will try to install all mandatory and optional dependencies\n"
        "from the distribution packages.\n"
        "\n"
        "NOTE: Many distributions do not provide packages for all libraries used by Blender, or have no\n"
        "version-compatible packages. In some cases, mandatory dependencies cannot be satisfied, and Blender\n"
        "won't be able to build at all.\n"
        "\n"
        "NOTE: To build with system package libraries instead of the precompiled ones when both are available,\n"
        "the `WITH_LIBS_PRECOMPILED` option must be disabled in CMake.\n"
        "\n"
        "See https://developer.blender.org/docs/handbook/building_blender/ for more details.\n"
        "\n"
    )

    parser = argparse.ArgumentParser(description=usage_text, formatter_class=argparse.RawDescriptionHelpFormatter)

    parser.add_argument(
        "--show-deps",
        dest="show_deps",
        action='store_true',
        help="Show main dependencies of Blender (including officially supported versions) and exit.",
    )
    parser.add_argument(
        "--no-sudo",
        dest="no_sudo",
        action='store_true',
        help=(
            "Disable use of `sudo` or `doas` "
            "(this script won't be able to do much then, will just print needed packages)."
        ),
    )
    parser.add_argument(
        "--all",
        dest="all",
        action='store_true',
        help="Install all dependencies from the distribution packages, including these also provided as "
             "precompiled libraries.",
    )
    parser.add_argument(
        "--distro-id",
        dest="distro_id",
        default=...,
        choices=set(DISTRO_IDS_INSTALLERS.keys()) - set((...,)),
        help="Force the linux distribution identifier to a specific value instead of relying on automatic detection.",
    )
    parser.add_argument(
        "--debug",
        dest="debug",
        action='store_true',
        help="Enable all debug info messages.",
    )

    return parser


def main():
    settings = argparse_create().parse_args()

    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG if settings.debug else logging.INFO)
    stdout_handler = logging.StreamHandler(stream=sys.stdout)
    stdout_handler.setFormatter(LoggingColoredFormatter())
    logger.addHandler(stdout_handler)
    settings.logger = logger

    if not settings.no_sudo and len(MAYSUDO) == 0:
        logger.critical("`sudo` or `doas` commands are needed to escalate privileges,"
                        " but they were not found.")
        exit(42)

    distro_package_installer = (PackageInstaller(settings) if settings.show_deps
                                else get_distro_package_installer(settings))
    distro_package_installer.packages_database_update()

    if settings.all:
        distro_package_installer.packages_install(PACKAGES_ALL)
    else:
        distro_package_installer.packages_install(PACKAGES_BASICS_BUILD)


if __name__ == "__main__":
    main()


## Links discovered
- [distro_id](https://github.com/blender/blender/blob/main/build_files/build_environment/settings.md)

--- doc/python_api/rst/info_overview.rst ---
.. _info_overview:

************
API Overview
************

The purpose of this document is to explain how Python and Blender fit together,
covering some of the functionality that may not be obvious from reading the API references
and example scripts.


Python in Blender
=================

Blender has an embedded Python interpreter which is loaded when Blender is started
and stays active while Blender is running. This interpreter runs scripts to draw the user interface
and is used for some of Blender's internal tools as well.

Blender's embedded interpreter provides a typical Python environment, so code from tutorials
on how to write Python scripts can also be run with Blender's interpreter. Blender provides its
Python modules, such as :mod:`bpy` and :mod:`mathutils`, to the embedded interpreter so they can
be imported into a script and give access to Blender's data, classes, and functions.
Scripts that deal with Blender data will need to import the modules to work.

Here is a simple example which moves a vertex attached to an object named "Cube":

.. code-block:: python

   import bpy
   bpy.data.objects["Cube"].data.vertices[0].co.x += 1.0

This modifies Blender's internal data directly.
When you run this in the interactive console you will see the 3D Viewport update.


The Default Environment
=======================

When developing your own scripts it may help to understand how Blender sets up its Python environment.
Many Python scripts come bundled with Blender and can be used as a reference
because they use the same API that script authors write tools in.
Typical usage for scripts include: user interface, import/export,
scene manipulation, automation, defining your own tool set and customization.

On startup Blender scans the ``scripts/startup/`` directory for Python modules and imports them.
The exact location of this directory depends on your installation.
See the :ref:`directory layout docs <blender_manual:blender-directory-layout>`.


Script Loading
==============

This may seem obvious, but it is important to note the difference between
executing a script directly and importing a script as a module.

Extending Blender by executing a script directly means the classes that the script defines
remain available inside Blender after the script finishes execution.
Using scripts this way makes future access to their classes
(to unregister them for example) more difficult compared to importing the scripts as modules.
When a script is imported as a module, its class instances will remain
inside the module and can be accessed later on by importing that module again.

For this reason it is preferable to avoid directly executing scripts that extend Blender by registering classes.

Here are some ways to run scripts directly in Blender:

- Loaded in the text editor and press *Run Script*.
- Typed or pasted into the interactive console.
- Execute a Python file from the command line with Blender, e.g:

  .. code-block:: sh

     blender --python /home/me/my_script.py


To run as modules:

- The obvious way, ``import some_module`` command from the text editor or interactive console.
- Open as a text data-block and check the *Register* option, this will load with the blend-file.
- Copy into one of the directories ``scripts/startup``, where they will be automatically imported on startup.
- Define as an add-on, enabling the add-on will load it as a Python module.


Add-ons
-------

Some of Blender's functionality is best kept optional,
alongside scripts loaded at startup there are add-ons which are kept in their own directory ``scripts/addons``,
They are only loaded on startup if selected from the user preferences.

The only difference between add-ons and built-in Python modules is that add-ons must contain a ``bl_info`` variable
which Blender uses to read metadata such as name, author, category and project link.
The User Preferences add-on listing uses ``bl_info`` to display information about each add-on.
`See Add-ons <https://developer.blender.org/docs/handbook/addons/guidelines/>`__
for details on the ``bl_info`` dictionary.


Integration through Classes
===========================

Running Python scripts in the text editor is useful for testing but you'll
want to extend Blender to make tools accessible like other built-in functionality.

The Blender Python API allows integration for:

- :class:`bpy.types.Panel`
- :class:`bpy.types.Menu`
- :class:`bpy.types.Operator`
- :class:`bpy.types.PropertyGroup`
- :class:`bpy.types.KeyingSet`
- :class:`bpy.types.RenderEngine`

This is intentionally limited. Currently, for more advanced features such as mesh modifiers,
object types, or shader nodes, C/C++ must be used.

For Python integration Blender defines methods which are common to all types.
This works by creating a Python subclass of a Blender class which contains variables and functions
specified by the parent class which are predefined to interface with Blender.

For example:

.. code-block:: python

   import bpy
   class SimpleOperator(bpy.types.Operator):
       bl_idname = "object.simple_operator"
       bl_label = "Tool Name"

       def execute(self, context):
           print("Hello World")
           return {'FINISHED'}

   bpy.utils.register_class(SimpleOperator)

First note that it defines a subclass as a member of :mod:`bpy.types`,
this is common for all classes which can be integrated with Blender and
is used to distinguish an Operator from a Panel when registering.

Both class properties start with a ``bl_`` prefix.
This is a convention used to distinguish Blender properties from those you add yourself.
Next see the execute function, which takes an instance of the operator and the current context.
A common prefix is not used for functions.
Lastly the register function is called, this takes the class and loads it into Blender. See `Class Registration`_.

Regarding inheritance, Blender doesn't impose restrictions on the kinds of class inheritance used,
the registration checks will use attributes and functions defined in parent classes.

Class mix-in example:

.. code-block:: python

   import bpy
   class BaseOperator:
       def execute(self, context):
           print("Hello World BaseClass")
           return {'FINISHED'}

   class SimpleOperator(bpy.types.Operator, BaseOperator):
       bl_idname = "object.simple_operator"
       bl_label = "Tool Name"

   bpy.utils.register_class(SimpleOperator)

.. note::

   Modal operators are an exception, keeping their instance variable as Blender runs, see modal operator template.

So once the class is registered with Blender, instancing the class and calling the functions is left up to Blender.
In fact you cannot instance these classes from the script as you would expect with most Python API's.
To run operators you can call them through the operator API, e.g:

.. code-block:: python

   import bpy
   bpy.ops.object.simple_operator()

User interface classes are given a context in which to draw, buttons, window, file header, toolbar, etc.,
then they are drawn when that area is displayed so they are never called by Python scripts directly.


.. _info_overview_class_construction_destruction:

Construction & Destruction
--------------------------

In the examples above, the classes don't define an ``__init__(self)`` function.
In general, defining custom constructors or destructors should not be needed, and is not recommended.

The lifetime of class instances is usually very short (also see the
:ref:`dedicated section <blender_py_objects_life_time>`), a panel for example will
have a new instance for every redraw.
Some other types, like :class:`bpy.types.Operator`, have an even more complex internal handling,
which can lead to several instantiations for a single operator execution.

There are a few cases where defining ``__init__()`` does make sense, e.g. when sub-classing a
:class:`bpy.types.RenderEngine`. When doing so, the parent matching function must always be called,
otherwise Blender's internal initialization won't happen properly:

.. code-block:: python

   import bpy
   class AwesomeRaytracer(bpy.types.RenderEngine):
      def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self.my_var = 42
         ...

.. warning::

   The Blender-defined parent constructor must be called before any data access to the object, including
   from other potential parent types ``__init__()`` functions.

.. warning::

   Calling the parent's ``__init__()`` function is a hard requirement since Blender 4.4.
   The 'generic' signature is the recommended one here, as Blender internal BPY code is typically
   the only caller of these functions. The actual arguments passed to the constructor are fully
   internal data, and may change depending on the implementation.

   Unfortunately, the error message, generated in case the expected constructor is not called, can
   be fairly cryptic and unhelping. Generally they should be about failure to create a (python)
   object:

      MemoryError: couldn't create bpy_struct object\_

   With Operators, it might be something like that:

      RuntimeError: could not create instance of <OPERATOR_OT_identifier> to call callback function execute

.. note::

   In case you are using complex/multi-inheritance, ``super()`` may not work (as the Blender-defined parent
   may not be the first type in the MRO). It is best then to first explicitly invoke the Blender-defined
   parent class constructor, before any other. For example:

   .. code-block:: python

      import bpy
      class FancyRaytracer(AwesomeRaytracer, bpy.types.RenderEngine):
         def __init__(self, *args, **kwargs):
            bpy.types.RenderEngine.__init__(self, *args, **kwargs)
            AwesomeRaytracer.__init__(self, *args, **kwargs)
            self.my_var = 42
            ...

.. note::

   Defining a custom ``__new__()`` function is strongly discouraged, not tested, and not considered
   as supported currently.
   Doing so presents a very high risk of crashes or otherwise corruption of Blender internal data.
   But if defined, it must take the same two generic positional and keyword arguments,
   and call the parent's ``__new__()`` with them if actually creating a new object.

.. note::

   Due to internal
   `CPython implementation details <https://discuss.python.org/t/cpython-usage-of-tp-finalize/64100>`__,
   C++-defined Blender types do not define or use a ``__del__()`` (aka ``tp_finalize()``) destructor
   currently.
   As this function
   `does not exist if not explicitly defined <https://stackoverflow.com/questions/36722390/python-3-super-del>`__,
   that means that calling ``super().__del__()`` in the ``__del__()`` function of a sub-class will
   fail with the following error:
   ``AttributeError: 'super' object has no attribute '__del__'``.
   If a call to the MRO 'parent' destructor is needed for some reason, the caller code must ensure
   that the destructor does exist, e.g. using something like that:
   ``getattr(super(), "__del__", lambda self: None)(self)``


.. _info_overview_registration:

Registration
============

Module Registration
-------------------

Blender modules loaded at startup require ``register()`` and ``unregister()`` functions.
These are the *only* functions that Blender calls from your code, which is otherwise a regular Python module.

A simple Blender Python module can look like this:

.. code-block:: python

   import bpy

   class SimpleOperator(bpy.types.Operator):
       """ See example above """

   def register():
       bpy.utils.register_class(SimpleOperator)

   def unregister():
       bpy.utils.unregister_class(SimpleOperator)

   if __name__ == "__main__":
       register()

These functions usually appear at the bottom of the script containing class registration sometimes adding menu items.
You can also use them for internal purposes setting up data for your own tools but take care
since register won't re-run when a new blend-file is loaded.

The register/unregister calls are used so it's possible to toggle add-ons and reload scripts while Blender runs.
If the register calls were placed in the body of the script, registration would be called on import,
meaning there would be no distinction between importing a module or loading its classes into Blender.
This becomes problematic when a script imports classes from another module
making it difficult to manage which classes are being loaded and when.

The last two lines are only for testing:

.. code-block:: python

   if __name__ == "__main__":
       register()

This allows the script to be run directly in the text editor to test changes.
This ``register()`` call won't run when the script is imported as a module
since ``__main__`` is reserved for direct execution.


Class Registration
------------------

Registering a class with Blender results in the class definition being loaded into Blender,
where it becomes available alongside existing functionality.
Once this class is loaded you can access it from :mod:`bpy.types`,
using the ``bl_idname`` rather than the classes original name.

.. note::

   There are some exceptions to this for class names which aren't guarantee to be unique.
   In this case use: :func:`bpy.types.Struct.bl_rna_get_subclass_py`.


When loading a class, Blender performs sanity checks making sure all required properties and functions are found,
that properties have the correct type, and that functions have the right number of arguments.

Mostly you will not need concern yourself with this but if there is a problem
with the class definition it will be raised on registering:

Using the function arguments ``def execute(self, context, spam)``, will raise an exception:

``ValueError: expected Operator, SimpleOperator class "execute" function to have 2 args, found 3``

Using ``bl_idname = 1`` will raise:

``TypeError: validating class error: Operator.bl_idname expected a string type, not int``


Inter-Class Dependencies
^^^^^^^^^^^^^^^^^^^^^^^^

When customizing Blender you may want to group your own settings together,
after all, they will likely have to co-exist with other scripts.
To group these properties classes need to be defined,
for groups within groups or collections within groups
you can't avoid having to deal with the order of registration/unregistration.

Custom properties groups are themselves classes which need to be registered.

For example, if you want to store material settings for a custom engine:

.. code-block:: python

   # Create new property:
   # bpy.data.materials[0].my_custom_props.my_float
   import bpy

   class MyMaterialProps(bpy.types.PropertyGroup):
       my_float: bpy.props.FloatProperty()

   def register():
       bpy.utils.register_class(MyMaterialProps)
       bpy.types.Material.my_custom_props = bpy.props.PointerProperty(type=MyMaterialProps)

   def unregister():
       del bpy.types.Material.my_custom_props
       bpy.utils.unregister_class(MyMaterialProps)

   if __name__ == "__main__":
       register()

.. note::

   The class **must be** registered before being used in a property, failing to do so will raise an error:

   ``ValueError: bpy_struct "Material" registration error: my_custom_props could not register``


.. code-block:: python

   # Create new property group with a sub property:
   # bpy.data.materials[0].my_custom_props.sub_group.my_float
   import bpy

   class MyMaterialSubProps(bpy.types.PropertyGroup):
       my_float: bpy.props.FloatProperty()

   class MyMaterialGroupProps(bpy.types.PropertyGroup):
       sub_group: bpy.props.PointerProperty(type=MyMaterialSubProps)

   def register():
       bpy.utils.register_class(MyMaterialSubProps)
       bpy.utils.register_class(MyMaterialGroupProps)
       bpy.types.Material.my_custom_props = bpy.props.PointerProperty(type=MyMaterialGroupProps)

   def unregister():
       del bpy.types.Material.my_custom_props
       bpy.utils.unregister_class(MyMaterialGroupProps)
       bpy.utils.unregister_class(MyMaterialSubProps)

   if __name__ == "__main__":
       register()

.. important::

   The lower most class needs to be registered first and that ``unregister()`` is a mirror of ``register()``.


Manipulating Classes
^^^^^^^^^^^^^^^^^^^^

Properties can be added and removed as Blender runs,
normally done on register or unregister but for some special cases
it may be useful to modify types as the script runs.

For example:

.. code-block:: python

   # Add a new property to an existing type.
   bpy.types.Object.my_float: bpy.props.FloatProperty()
   # Remove it.
   del bpy.types.Object.my_float

This works just as well for ``PropertyGroup`` subclasses you define yourself.

.. code-block:: python

   class MyPropGroup(bpy.types.PropertyGroup):
       pass
   MyPropGroup.my_float: bpy.props.FloatProperty()

This is equivalent to:

.. code-block:: python

   class MyPropGroup(bpy.types.PropertyGroup):
       my_float: bpy.props.FloatProperty()


Dynamic Class Definition (Advanced)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In some cases the specifier for data may not be in Blender, for example a external render engines shader definitions,
and it may be useful to define them as types and remove them on the fly.

.. code-block:: python

   for i in range(10):
       idname = "object.operator_{:d}".format(i)

       def func(self, context):
           print("Hello World", self.bl_idname)
           return {'FINISHED'}

       op_class = type(
           "DynOp{:d}".format(i),
           (bpy.types.Operator, ),
           {"bl_idname": idname, "bl_label": "Test", "execute": func},
       )
       bpy.utils.register_class(op_class)

.. note::

   ``type()`` is called to define the class.
   This is an alternative syntax for class creation in Python, better suited to constructing classes dynamically.


To call the operators from the previous example:

   >>> bpy.ops.object.operator_1()
   Hello World OBJECT_OT_operator_1
   {'FINISHED'}

   >>> bpy.ops.object.operator_2()
   Hello World OBJECT_OT_operator_2
   {'FINISHED'}


--- doc/python_api/rst/info_quickstart.rst ---
.. _info_quickstart:

**********
Quickstart
**********

This :abbr:`API (Application Programming Interface)` is generally stable
but some areas are still being extended and improved.

.. rubric:: Blender Python API features:

- Edit any data the user interface can (Scenes, Meshes, Particles etc.).
- Modify user preferences, keymaps and themes.
- Run tools with own settings.
- Create user interface elements such as menus, headers and panels.
- Create new tools.
- Create interactive tools.
- Create new rendering engines that integrate with Blender.
- Subscribe to changes to data and it's properties.
- Define new settings in existing Blender data.
- Draw in the 3D Viewport using Python.


.. rubric:: (Still) missing features:

- Create new space types.
- Assign custom properties to every type.


Before Starting
===============

This document is intended to familiarize you with Blender Python API
but not to fully cover each topic.

A quick list of helpful things to know before starting:

- Enable :ref:`Developer Extra <blender_manual:bpy.types.PreferencesView.show_developer_ui>`
  and :ref:`Python Tooltips <blender_manual:bpy.types.PreferencesView.show_tooltips_python>`.
- The :ref:`Python Console <blender_manual:bpy.types.SpaceConsole>`
  is great for testing one-liners; it has autocompletion so you can inspect the API quickly.
- Button tooltips show Python attributes and operator names (when enabled see above).
- The context menu of buttons directly links to this API documentation (when enabled see above).
- Many python examples can be found in the text editor's template menu.
- To examine further scripts distributed with Blender, see:

  - ``scripts/startup/bl_ui`` for the user interface.
  - ``scripts/startup/bl_operators`` for operators.

  Exact location depends on platform, see:
  :ref:`directory layout docs <blender_manual:blender-directory-layout>`.


Running Scripts
---------------

The two most common ways to execute Python scripts are using the built-in
text editor or entering commands in the Python console.
Both the *Text Editor* and *Python Console* are space types you can select from the header.
Rather than manually configuring your spaces for Python development,
you can use the *Scripting* workspace accessible from the Topbar tabs.

From the text editor you can open ``.py`` files or paste them from the clipboard, then test using *Run Script*.
The Python Console is typically used for typing in snippets and for testing to get immediate feedback,
but can also have entire scripts pasted into it.
Scripts can also run from the command line with Blender but to learn scripting in Blender this isn't essential.


Key Concepts
============

Data Access
-----------

Accessing Data-Blocks
^^^^^^^^^^^^^^^^^^^^^

You can access Blender's data with the Python API in the same way as the animation system or user interface;
this implies that any setting that can be changed via a button can also be changed with Python.
Accessing data from the currently loaded blend-file is done with the module :mod:`bpy.data`.
It gives access to library data, for example:

   >>> bpy.data.objects
   <bpy_collection[3], BlendDataObjects>

   >>> bpy.data.scenes
   <bpy_collection[1], BlendDataScenes>

   >>> bpy.data.materials
   <bpy_collection[1], BlendDataMaterials>


Accessing Collections
^^^^^^^^^^^^^^^^^^^^^

You will notice that an index as well as a string can be used to access members of the collection.
Unlike Python dictionaries, both methods are available;
however, the index of a member may change while running Blender.

   >>> list(bpy.data.objects)
   [bpy.data.objects["Cube"], bpy.data.objects["Plane"]]

   >>> bpy.data.objects['Cube']
   bpy.data.objects["Cube"]

   >>> bpy.data.objects[0]
   bpy.data.objects["Cube"]


Accessing Attributes
^^^^^^^^^^^^^^^^^^^^

Once you have a data-block, such as a material, object, collection, etc.,
its attributes can be accessed much like you would change a setting using the graphical interface.
In fact, the tooltip for each button also displays the Python attribute
which can help in finding what settings to change in a script.

   >>> bpy.data.objects[0].name
   'Camera'

   >>> bpy.data.scenes["Scene"]
   bpy.data.scenes['Scene']

   >>> bpy.data.materials.new("MyMaterial")
   bpy.data.materials['MyMaterial']


For testing what data to access it's useful to use the Python Console, which is its own space type.
This supports auto-complete, giving you a fast way to explore the data in your file.

Example of a data path that can be quickly found via the console:

   >>> bpy.data.scenes[0].render.resolution_percentage
   100
   >>> bpy.data.scenes[0].objects["Torus"].data.vertices[0].co.x
   1.0


Data Creation/Removal
^^^^^^^^^^^^^^^^^^^^^

When you are familiar with other Python APIs you may be surprised that
new data-blocks in the bpy API cannot be created by calling the class:

   >>> bpy.types.Mesh()
   Traceback (most recent call last):
     File "<blender_console>", line 1, in <module>
   TypeError: bpy_struct.__new__(type): expected a single argument


This is an intentional part of the API design.
The Blender Python API can't create Blender data that exists outside the main Blender database
(accessed through :mod:`bpy.data`), because this data is managed by Blender (save, load, undo, append, etc).

Data is added and removed via methods on the collections in :mod:`bpy.data`, e.g:

   >>> mesh = bpy.data.meshes.new(name="MyMesh")
   >>> print(mesh)
   <bpy_struct, Mesh("MyMesh.001")>

   >>> bpy.data.meshes.remove(mesh)


.. _info_quickstart-custom_properties:

Custom Properties
^^^^^^^^^^^^^^^^^

Python can access properties on any data-block that has an ID
(data that can be linked in and accessed from :mod:`bpy.data`).
When assigning a property, you can pick your own names,
these will be created when needed or overwritten if they already exist.

This data is saved with the blend-file and copied with objects, for example:

.. code-block:: python

   bpy.context.object["MyOwnProperty"] = 42

   if "SomeProp" in bpy.context.object:
       print("Property found")

   # Use the get function like a Python dictionary
   # which can have a fallback value.
   value = bpy.data.scenes["Scene"].get("test_prop", "fallback value")

   # Dictionaries can be assigned as long as they only use basic types.
   collection = bpy.data.collections.new("MyTestCollection")
   collection["MySettings"] = {"foo": 10, "bar": "spam", "baz": {}}

   del collection["MySettings"]


Note that these properties can only be assigned basic Python types:

- int, float, string
- array of ints or floats
- dictionary (only string keys are supported, values must be basic types too)

These properties are valid outside of Python. They can be animated by curves or used in driver paths.

For a list of types that support custom properties see:
:ref:`types supporting custom properties <bpy_types-custom_properties>`.


Context
-------

While it's useful to be able to access data directly by name or as a list,
it's more common to operate on the user's selection.
The context is always available from ``bpy.context`` and can be used to get the active object, scene,
tool settings along with many other attributes.

Some common use cases are:

   >>> bpy.context.object
   >>> bpy.context.selected_objects
   >>> bpy.context.visible_bones

Note that the context is read-only, which means that these values cannot be modified directly.
But they can be changed by running API functions or by using the data API.

So ``bpy.context.active_object = obj`` will raise an error.
But ``bpy.context.view_layer.objects.active = obj`` works as expected.

The context attributes change depending on where they are accessed.
The 3D Viewport has different context members than the Python Console,
so take care when accessing context attributes that the user state is known.

See :mod:`bpy.context` API reference.


Operators (Tools)
-----------------

Operators are tools generally accessed by the user from buttons, menu items or key shortcuts.
From the user perspective they are a tool but Python can run these with its own settings
through the :mod:`bpy.ops` module.

Examples:

   >>> bpy.ops.mesh.flip_normals()
   {'FINISHED'}
   >>> bpy.ops.mesh.hide(unselected=False)
   {'FINISHED'}
   >>> bpy.ops.object.transform_apply()
   {'FINISHED'}

.. tip::

   The :ref:`Operator Cheat Sheet <blender_manual:bpy.ops.wm.operator_cheat_sheet>`
   gives a list of all operators and their default values in Python syntax, along with the generated docs.
   This is a good way to get an overview of all Blender's operators.


Operator Poll()
^^^^^^^^^^^^^^^

Many operators have a "poll" function which checks if the cursor
is in a valid area or if the object is in the correct mode (Edit Mode, Weight Paint Mode, etc).
When an operator's poll function fails within Python, an exception is raised.

For example, calling ``bpy.ops.view3d.render_border()`` from the console raises the following error:

.. code-block:: python

   RuntimeError: Operator bpy.ops.view3d.render_border.poll() failed, context is incorrect

In this case the context must be the 3D Viewport with an active camera.

To avoid using try-except clauses wherever operators are called, you can call the operators
own ``poll()`` function to check if it can run the operator in the current context.

.. code-block:: python

   if bpy.ops.view3d.render_border.poll():
       bpy.ops.view3d.render_border()


Integration
===========

Python scripts can integrate with Blender in the following ways:

- By defining a render engine.
- By defining operators.
- By defining menus, headers and panels.
- By inserting new buttons into existing menus, headers and panels.

In Python, this is done by defining a class, which is a subclass of an existing type.


Example Operator
----------------

.. literalinclude:: __/__/__/scripts/templates_py/operator_simple.py

Once this script runs, ``SimpleOperator`` is registered with Blender
and can be called from Operator Search or added to the toolbar.

To run the script:

#. Start Blender and switch to the Scripting workspace.
#. Click the *New* button in the text editor to create a new text data-block.
#. Copy the code from above and paste it into the text editor.
#. Click on the *Run Script* button.
#. Move your cursor into the 3D Viewport,
   open the :ref:`Operator Search menu <blender_manual:bpy.ops.wm.search_menu>`,
   and type "Simple".
#. Click on the "Simple Operator" item found in search.

.. seealso::

   The class members with the ``bl_`` prefix are documented in the API reference :class:`bpy.types.Operator`.

.. note::

   The output from the ``main`` function is sent to the terminal;
   in order to see this, be sure to :ref:`use the terminal <use_the_terminal>`.


Example Panel
-------------

Panels are registered as a class, like an operator.
Notice the extra ``bl_`` variables used to set the context they display in.

.. literalinclude:: __/__/__/scripts/templates_py/ui_panel_simple.py

To run the script:

#. Start Blender and switch to the Scripting workspace.
#. Click the *New* button in the text editor to create a new text data-block.
#. Copy the code from above and paste it into the text editor.
#. Click on the *Run Script* button.

To view the results:

#. Select the default cube.
#. Click on the Object properties icon in the buttons panel (far right; appears as a tiny cube).
#. Scroll down to see a panel named "Hello World Panel".
#. Changing the object name also updates *Hello World Panel's* name: field.

Note the row distribution and the label and properties that are defined through the code.

.. seealso:: :class:`bpy.types.Panel`


Types
=====

Blender defines a number of Python types but also uses Python native types.
Blender's Python API can be split up into three categories.


Native Types
------------

In simple cases returning a number or a string as a custom type would be cumbersome,
so these are accessed as normal Python types.

- Blender float, int, boolean -> float, int, boolean
- Blender enumerator -> string

     >>> C.object.rotation_mode = 'AXIS_ANGLE'

- Blender enumerator (multiple) -> set of strings

  .. code-block:: python

     # Setting multiple snap targets.
     bpy.context.scene.tool_settings.snap_elements_base = {'VERTEX', 'EDGE'}

     # Passing as an operator argument for report types.
     self.report({'WARNING', 'INFO'}, "Some message!")


Internal Types
--------------

:class:`bpy.types.bpy_struct` is used for Blender data-blocks and collections.
Also for data that contains its own attributes: collections, meshes, bones, scenes, etc.

There are two main types that wrap Blender's data, one for data-blocks
(known internally as ``bpy_struct``), another for properties.

   >>> bpy.context.object
   bpy.data.objects['Cube']

   >>> C.scene.objects
   bpy.data.scenes['Scene'].objects

Note that these types reference Blender's data so modifying them is visible immediately.


Mathutils Types
---------------

Accessible from :mod:`mathutils` are vectors, quaternions, Euler angles, matrix and color types.
Some attributes such as :class:`bpy.types.Object.location`,
:class:`bpy.types.PoseBone.rotation_euler` and :class:`bpy.types.Scene.cursor_location`
can be accessed as special math types which can be used together and manipulated in various useful ways.

Example of a matrix, vector multiplication:

.. code-block:: python

   bpy.context.object.matrix_world @ bpy.context.object.data.verts[0].co

.. note::

   mathutils types keep a reference to Blender's internal data so changes can
   be applied back.

   Example:

   .. code-block:: python

      # Modifies the Z axis in place.
      bpy.context.object.location.z += 2.0

      # Location variable holds a reference to the object too.
      location = bpy.context.object.location
      location *= 2.0

      # Copying the value drops the reference so the value can be passed to
      # functions and modified without unwanted side effects.
      location = bpy.context.object.location.copy()


Animation
=========

There are two ways to add keyframes through Python.

The first is through key properties directly, which is like inserting a keyframe from the button as a user.
You can also manually create the curves and keyframe data, then set the path to the property.
Here are examples of both methods. Both insert a keyframe on the active object's Z axis.

Simple example:

.. code-block:: python

   obj = bpy.context.object
   obj.location[2] = 0.0
   obj.keyframe_insert(data_path="location", frame=10.0, index=2)
   obj.location[2] = 1.0
   obj.keyframe_insert(data_path="location", frame=20.0, index=2)

Using low-level functions:

.. code-block:: python

   obj = bpy.context.object

   # Create the action, with a slot for the object, a layer, and a keyframe strip:
   action = bpy.data.actions.new(name="MyAction")
   slot = action.slots.new(obj.id_type, obj.name)
   strip = action.layers.new("MyLayer").strips.new(type='KEYFRAME')

   # Create a channelbag to hold the F-Curves for the slot:
   channelbag = strip.channelbag(slot, ensure=True)

   # Create the F-Curve with two keyframes:
   fcu_z = channelbag.fcurves.new(data_path="location", index=2)
   fcu_z.keyframe_points.add(2)
   fcu_z.keyframe_points[0].co = 10.0, 0.0
   fcu_z.keyframe_points[1].co = 20.0, 1.0

   # Assign the action and the slot to the object:
   adt = obj.animation_data_create()
   adt.action = action
   adt.action_slot = slot


--- doc/guides/blender-guardedalloc.txt ---
MEMORY MANAGEMENT IN BLENDER (guardedalloc)
-------------------------------------------

NOTE: This file does not cover memutil and smart pointers and reference counted
      garbage collection, which are contained in the memutil module.

Blender takes care of dynamic memory allocation using a set of own functions
which are recognizable through their MEM_ prefix. All memory allocation and
deallocation in blender is done through these functions.

The following functions are available through MEM_guardedalloc.h:

For normal operation:
---------------------

void *MEM_[mc]allocN(unsigned int len, char * str);

- nearest ANSI counterpart: malloc()
- str must be a static string describing the memory block (used for debugging
memory management problems)
- returns a memory block of length len
- MEM_callocN clears the memory block to 0

void *MEM_dupallocN(void *vmemh);

- nearest ANSI counterpart: combination malloc() and memcpy()
- returns a pointer to a copy of the given memory area

short MEM_freeN(void *vmemh);

- nearest ANSI counterpart: free()
- frees the memory area given by the pointer
- returns 0 on success and !=0 on error

int MEM_allocN_len(void *vmemh);

- nearest ANSI counterpart: none known
- returns the length of the given memory area

For debugging:
--------------

void MEM_set_error_stream(FILE*);

- this sets the file the memory manager should use to output debugging messages
- if the parameter is NULL the messages are suppressed
- default is that messages are suppressed

void MEM_printmemlist(void);

- if err_stream is set by MEM_set_error_stream() this function dumps a list of all
currently allocated memory blocks with length and name to the stream

bool MEM_consistency_check(void);

- this function tests if the internal structures of the memory manager are intact
- returns 0 on success and !=0 on error


--- tools/check_docs/check_docs_code_layout.py ---
#!/usr/bin/env python3
# SPDX-FileCopyrightText: 2023 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

"""
This script is to validate the markdown page that documents Blender's file-structure, see:

    https://developer.blender.org/docs/features/code_layout/

It can run without any arguments, where it will download the markdown to Blender's source root:

You may pass the markdown text as an argument, e.g.

check_docs_code_layout.py --markdown=markdown.txt
"""
__all__ = (
    "main",
)

import os
import argparse

# -----------------------------------------------------------------------------
# Constants

CURRENT_DIR = os.path.abspath(os.path.dirname(__file__))
SOURCE_DIR = os.path.normpath(os.path.join(CURRENT_DIR, "..", ".."))

MARKDOWN_URL = (
    "https://projects.blender.org/blender/blender-developer-docs/raw/branch/main/docs/features/code_layout.md"
)


# -----------------------------------------------------------------------------
# HTML Utilities

def text_with_title_underline(text: str, underline: str = "=") -> str:
    return "\n{:s}\n{:s}\n".format(text, len(text) * underline)


def html_extract_markdown_from_url(url: str) -> str | None:
    """
    Download
    """
    import urllib.request

    req = urllib.request.Request(url=url)
    with urllib.request.urlopen(req) as fh:
        data = fh.read().decode('utf-8')

    # Quiet `mypy` checker warning.
    assert isinstance(data, str)
    return data


# -----------------------------------------------------------------------------
# markdown Text Parsing

def markdown_to_paths(markdown: str) -> list[str]:
    file_paths = []
    markdown = markdown.replace("<p>", "")
    markdown = markdown.replace("</p>", "")
    markdown = markdown.replace("<strong>", "")
    markdown = markdown.replace("</strong>", "")
    markdown = markdown.replace("</td>", "")

    path_prefix = "<td markdown>/"

    for line in markdown.splitlines():
        line = line.strip()
        if line.startswith(path_prefix):
            file_path = line[len(path_prefix):]
            file_path = file_path.rstrip("/")
            file_paths.append(file_path)

    return file_paths


# -----------------------------------------------------------------------------
# Reporting

def report_known_markdown_paths(file_paths: list[str]) -> None:
    heading = "Paths Found in markdown Table"
    print(text_with_title_underline(heading))
    for p in file_paths:
        print("-", p)


def report_missing_source(file_paths: list[str]) -> int:
    heading = "Missing in Source Dir"

    test = [p for p in file_paths if not os.path.exists(os.path.join(SOURCE_DIR, p))]

    amount = str(len(test)) if test else "none found"
    print(text_with_title_underline("{:s} ({:s})".format(heading, amount)))
    if not test:
        return 0

    print("The following paths were found in the markdown\n"
          "but were not found in Blender's source directory:\n")
    for p in test:
        print("-", p)

    return len(test)


def report_incomplete(file_paths: list[str]) -> int:
    heading = "Missing Documentation"

    test = []
    basedirs = {os.path.dirname(p) for p in file_paths}
    for base in sorted(basedirs):
        base_abs = os.path.join(SOURCE_DIR, base)
        if os.path.exists(base_abs):
            for p in os.listdir(base_abs):
                if not p.startswith("."):
                    p_abs = os.path.join(base_abs, p)
                    if os.path.isdir(p_abs):
                        p_rel = os.path.join(base, p)
                        if p_rel not in file_paths:
                            test.append(p_rel)

    amount = str(len(test)) if test else "none found"
    print(text_with_title_underline("{:s} ({:s})".format(heading, amount)))
    if not test:
        return 0

    print("The following paths were found in Blender's source directory\n"
          "but are missing from the markdown:\n")
    for p in sorted(test):
        print("-", p)

    return len(test)


def report_alphabetical_order(file_paths: list[str]) -> int:
    heading = "Non-Alphabetically Ordered"
    test = []

    p_prev = ""
    p_prev_dir = ""
    for p in file_paths:
        p_dir = os.path.dirname(p)
        if p_prev:
            if p_dir == p_prev_dir:
                if p < p_prev:
                    test.append((p_prev, p))
        p_prev_dir = p_dir
        p_prev = p

    amount = str(len(test)) if test else "none found"
    print(text_with_title_underline("{:s} ({:s})".format(heading, amount)))
    if not test:
        return 0

    for p_prev, p in test:
        print("-", p, "(should be before)\n ", p_prev)

    return len(test)


# -----------------------------------------------------------------------------
# Argument Parser

def create_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description=__doc__)

    parser.add_argument(
        "-m",
        "--markdown",
        dest="markdown",
        metavar='PATH',
        default=os.path.join(SOURCE_DIR, "markdown_file_structure.txt"),
        help="markdown text file path, NOTE: this will be downloaded if not found!",
    )
    return parser


# -----------------------------------------------------------------------------
# Main Function

def main() -> None:
    parser = create_parser()

    args = parser.parse_args()

    if os.path.exists(args.markdown):
        print("Using existing markdown text:", args.markdown)
    else:
        data = html_extract_markdown_from_url(MARKDOWN_URL)
        if data is not None:
            with open(args.markdown, 'w', encoding='utf-8') as fh:
                fh.write(data)
            print("Downloaded markdown text to:", args.markdown)
            print("Update and save to:", MARKDOWN_URL)
        else:
            print("Failed to downloaded or extract markdown text, aborting!")
            return

    with open(args.markdown, 'r', encoding='utf-8') as fh:
        file_paths = markdown_to_paths(fh.read())

    # Disable, mostly useful when debugging why paths might not be found.
    # report_known_markdown_paths()
    issues = 0
    issues += report_missing_source(file_paths)
    issues += report_incomplete(file_paths)
    issues += report_alphabetical_order(file_paths)

    if issues:
        print("Warning, found {:d} issues!\n".format(issues))
    else:
        print("Success! The markdown text is up to date with Blender's source tree!\n")


if __name__ == "__main__":
    main()


--- doc/guides/interface_API.txt ---
---------------------------------------------------
Blender interface.c API toolkit notes
(july 2003, Ton Roosendaal)
---------------------------------------------------

Contents

1 General notes
1.1 C and H files

2. Windows & Blocks
2.1 Memory allocation
2.2 And how it works internally

3. API for ui::Block
3.1 ui::Block Controlling functions
3.2 Internal function to know

4. API for uiButton
4.1 UiDefBut
	1. BUT
	2. TOG or TOGN
	   TOG|BIT|<nr>
	3. ROW
	4. NUMSLI or HSVSLI
	5. NUM
	6. TEX
	7. LABEL
	8  SEPR
	9. MENU
	10.	COL
4.2 Icon buttons
4.3 pulldown menus / block buttons
	14. BLOCK
4.4 specials
	15. KEYEVT
	16. LINK and INLINK
4.5 uiButton control functions


----------------1. General notes

- The API is built with Blender in mind, with some buttons acting on lists of Blender data.
  It was not meant to be available as a separate SDK, nor to be used for other applications.

- It works with only OpenGL calls, for the full 100%. This means that it has some quirks
  built-in to work with all OS's and OpenGL versions. Especially frontbuffer drawing is
  a continuous point of attention. Buttons can be drawn with any window matrix. However,
  errors can still occur when buttons are created in windows with non-standard glViewports.

- The code was written to replace the old 1.8 button system, but under high pressure. Quite
  some button methods from the old system were copied for that reason.

- I tried to design a unified GUI system, which equally works for pulldown menus, pop up menus,
  and normal button layouts. Although it gives nice features and freedom in design, the code
  looks quite hard to understand for that reason. Not all 'normal' pulldown menu features
  could be hacked in easily, they just differ too much from other UI elements. Could be
  looked at once...

- During the past period of NaN (beginning of 2002) someone tried to make a more 'high' level
  API for it, with less low level defines and structure info needed in calling code. I am not
  really sure if this was done well... or even finished. In the bottom of interface.c you can
  see the 'new' API which is now used in Blender code. It used to be so much more simple!
  Nevertheless, I will use that convention in this doc.

- Unfinished stuff: the code was scheduled to be expanded with 'floating blocks' which can
  serve as permanent little button-fields in Blender windows. Think for example of having
  an (optional) extra field in the 3d window displaying object loc/rot/size.
  After that, the existing button windows can be reorganized in such blocks as well, allowing
  a user to configure the general buttons layout (make vertical for example).


--------------1.1 C and H files

blender/source/blender/src/interface.c	/* almost all code */
blender/source/blender/include/interface.h 	/* internals for previous code */
blender/source/blender/include/BIF_interface.h 	/* externals for previous code */

(the previous 2 include files have not been separated fully yet)

Color and icons stuff has been put in: (unfinished code, under development)
blender/source/blender/src/resources.c
blender/source/blender/include/BIF_resources.h

Related code:
blender/source/blender/src/toolbox.c (extra GUI elements built on top of this API)


--------------2. Windows & Blocks

All GUI elements are collected in ui::Blocks, which in turn are linked together in a list that's
part of a Blender Area-window.

	ui::Block *block = uiNewBlock(&curarea->uiblocks, "stuff", UI_EMBOSSX, UI_HELV, curarea->win);

The next code example makes a new block, and puts it in the list of blocks of the current active
Area:

	uiDoBlocks(&curarea->uiblocks, event);

This code is usually available in each area-window event queue handler. You give uiDoBlocks
an event code, and the uiDoBlocks handles whatever is to be handled. Blocks can be
standard buttons or pull down menus. Can return immediately, or jump to an internal handling
loop.

2.1 Memory allocation

Important to know is that for this toolkit there's no difference in "creating blocks" or
"drawing blocks". In fact, for each window redraw all blocks are created again. Constructing
button interfaces in Blender always happens in the main drawing function itself.

Memory allocation is handled as follows:
- if in this window a ui::Block with the same name existed, it is freed
- when you close a window (or blender) the ui::Blocks get freed.
- when you duplicate (split) a window, the ui::Blocks get copied

2.2 And how it works internally

With a call to uiDoblocks, all blocks in the current active window are evaluated.
It walks through the lists in a rather complex manner:

- while(looping)

	/* the normal buttons handling */
	- for each block
		- call uiDoBlock (handles buttons for single block)
	- (end for)

	/* at this moment, a new block can be created, for a menu */
	/* so we create a 2nd loop for it */
	- while first block is a menu
		- if block is a menu and not initialized:
			- initialize 'saveunder'
			- draw it
		- get event from queue
		- call uiDoBlock (handles buttons for single block)
		/* here, a new block again can be created, for a sub menu */
		- if return "end" from uiDoBlock
			restore 'saveunder's
			free all menu blocks
			exit from loop
		- do tooltip if nothing has happened
	- (end while)

	- if there was menu, it does this loop once more
	  (when you click outside a menu, at another button)

- (end while)

- do tooltip if nothing has happened



-------------3. API for ui::Block

Create a new buttons block, and link it to the window:

ui::Block *uiNewBlock(ListBase *lb, char *name, short dt, short font, short win)
	ListBase *lb	pointer to list basis, where the block will be appended to (blenlib)
	char *name		unique name to identify the block. When the name exists in the list,
					the old ui::Block gets freed.
	short dt		drawtype. See below
	short font		font id number
	short win		blender area-window id

drawtype:
	UI_EMBOSSX		0	/* Rounded embossed button (standard in Blender) */
	UI_EMBOSSW		1	/* Simpler embossed button */
	UI_EMBOSSN		2	/* Button with no border */
	UI_EMBOSSF		3	/* Square embossed button (file select) */
	UI_EMBOSSM		4	/* Colored, for pulldown menus */
	UI_EMBOSSP		5	/* Simple borderless colored button (like blender sensors) */

font:
	UI_HELV			0	/* normal font */
	UI_HELVB		1	/* bold font */
With the new truetype option in Blender, this is used for all font families

When a ui::Block is created, each uiButton that is defined gets the ui::Block properties.
Changing Block properties in between will effect uiButtons defined thereafter.



----------3.1 ui::Block Controlling functions:

void uiDrawBlock(block)
	draws the block

void ui::BlockSetCol(ui::Block *block, int col)

col:
	BUTGREY,
	BUTGREEN,
	BUTBLUE,
	BUTSALMON,
	MIDGREY,
	BUTPURPLE,

void ui::BlockSetEmboss(ui::Block *block, int emboss)
	changes drawtype

void ui::BlockSetDirection(ui::Block *block, int direction)
	for pop-up and pulldown menus:

direction:
	UI_TOP
	UI_DOWN
	UI_LEFT
	UI_RIGHT

void ui::BlockSetXOfs(ui::Block *block, int xofs)
	for menus, offset from parent

void ui::BlockSetButmFunc(ui::Block *block, void (*menufunc)(void *arg, int event), void *arg)
	sets function to be handled when a menu-block is marked "OK"

void uiAutoBlock(ui::Block *block, float minx, float miny, float sizex, float sizey, BLOCK_ROWS)

	Sets the buttons in this block to automatically align, and fit within boundaries.
	Internally it allows multiple columns or rows as well. Only 'row order' has been implemented.
	The uiDefBut definitions don't need coordinates as input here, but instead:
	- first value (x1) to indicate row number
	- width and height values (if filled in) will be used to define a relative width/height.
	A call to uiDrawBlock will invoke the calculus to fit in all buttons.



---------- 3.2 Internal function to know:

These flags used to be accessible from outside of interface.c. Currently it is only
used elsewhere by toolbox.c, so it can be considered 'internal' somewhat.

void ui::BlockSetFlag(ui::Block *block, int flag)	/* block types, can be 'OR'ed */
	BLOCK_LOOP		1		a sublooping block, drawn in frontbuffer, i.e. menus
	BLOCK_REDRAW		2		block needs a redraw
	BLOCK_RET_1		4		block is closed when an event happens with value '1' (press key, not for mouse)
	BLOCK_BUSY		8		internal
	BLOCK_NUMSELECT	16		keys 1-2-...-9-0 can be used to select items
	BLOCK_ENTER_OK	32		enter key closes block with "OK"

(these values are being set within the interface.c and toolbox.c code.)


-------------4. API for uiButton

In Blender a button can do four things:

- directly visualize data, and write to it.
- put event codes (shorts) back in the queue, to be handled
- call a user-defined function pointer (while being pressed, etc)
- create and call another block (i.e. menu)

Internally, each button or menu item is a 'uiButton', with a generic API and handling:
ui_def_but(block, type, retval, str, x1, y1, x2, y2, poin, min, max, a1, a2, tip);

Because a lot of obscure generic (re-use) happens here, translation calls have been made
for each most button types individually.


-----------4.1 UiDefBut

Button *UiDefBut[CSIF](	ui::Block *block, int type, int retval, char *str,
				short x1, short y1, short x2, short y2, xxxx *poin,
				float min, float max, float a1, float a2,  char *tip)

UiDefButC	operates on char
UiDefButS	operates on short
UiDefButI	operates on int
UiDefButF	operates on float

*block:		current ui::Block pointer
type:		see below
retval:		return value, which is put back in queue
*str:		button name
x1, y1:		coordinates of left-lower corner
x2, y2:		width, height
*poin:		pointer to char, short, int, float
min, max	used for slider buttons
a1, a2		extra info for some buttons
*tip:		tooltip string

type:

1. BUT
	Activation button. (like "Render")
	Passing on a pointer is not needed

2. TOG or TOGN
	Toggle button (like "Lock")
	The pointer value is set either at 0 or 1
	If pressed, it calls the optional function with arguments provided.
	Type TOGN: works negative, when pressed it sets at 0

	"|BIT|<nr>"
	When added to type, it works on a single bit <nr> (lowest order bit: nr = '0')

3. ROW
	Button that's part of a row.
	in "min" you set a row-id number, in "max" the value you want *poin to be
	assigned when you press the button. Always pass on these values as floats.
	When this button is pressed, it sets the "max" value to *poin, and redraws
	all buttons with the same row-id number.

4. NUMSLI or HSVSLI
	Number-slider or hsv-slider button.
	"min" and "max" are to clamp the value to.
	If you want a button type "Col" to be updated, make 'a1' equal to 'retval'
	from the COL button.

5. NUM
	Number button
	Set the clamping values 'min' and 'max' always as float.
	For UiDefButF, set a 'step' in 'a1', in 1/100's. The step value is the increment or
	decrement when you click once on the right or left side of a button.
	The optional button function is additionally called for each change of the *poin value.

6. TEX
	Text string button.
	Pointertype is standard a char. Value 'max' is length of string (pass as float).
	When button is left with ESC, it doesn't put the 'retval' at the queue.

7. LABEL
	Label button.
	Only displays text.
	If 'min' is set at 1.0, the text is printed in white.

8  SEPR
	A separator line, typically used within pulldown menus.

9. MENU
	Menu button.
	The syntax of the string in *name defines the menu items:
		- %t means the previous text becomes the title
		- item separator is '|'
		- return values are indicated with %x[nr] (i.e: %x12).
			without returnvalues, the first item gets value 0 (incl. title!)
	Example: "Do something %t| turn left %2| turn right %1| nothing %0"

10.	COLOR
	A special button that only visualizes a RGB value
	In 'retval' you can put a code, which is used to identify for sliders if it needs
	redraws while using the sliders. Check button '5'.
	As *poin you input the pointer to the 'r' value, 'g' and 'b' are supposed to be
	next to that.


------------4.2 Icon buttons

Instead of a 'name', all buttons as described for uiDefBut also can have an icon:

Button *uiDefIconBut(ui::Block *block, int type, int retval, int icon,
			short x1, short y1, short x2, short y2, void *poin,
			float min, float max, float a1, float a2,  char *tip)

	Same syntax and types available as previous uiDefBut, but now with an icon code
	instead of a name. The icons are numbered in resources.c

Button *uiDefIconTextButF(ui::Block *block, int type, int retval, int icon, char *str,
			short x1, short y1, short x2, short y2, float *poin,
			float min, float max, float a1, float a2,  char *tip)

	Same again, but now with an icon and string as button name.


-----------4.3 pulldown menus / block buttons

14. BLOCK
void uiDefBlockBut(ui::Block *block, ui::BlockFuncFP func, void *arg, char *str,
	short x1, short y1, short x2, short y2, char *tip)

	This button creates a new block when pressed. The function argument 'func' is called
	to take care of this. An example func:

	static ui::Block *info_file_importmenu(void *arg_unused)
	{
		ui::Block *block;
		short yco = 0, xco = 20;

		block = uiNewBlock(&curarea->uiblocks, "importmenu", UI_EMBOSSW, UI_HELV, G.curscreen->mainwin);
		ui::BlockSetXOfs(block, -40);  // offset to parent button

		/* flags are defines */
		uiDefBut(block, LABEL, 0, "VRML 2.0 options", xco, yco, 125, 19, NULL, 0.0, 0.0, 0, 0, "");
		uiDefButS(block, TOG|BIT|0, 0, "SepLayers", xco, yco-=20, 75, 19, &U.vrmlflag, 0.0, 0.0, 0, 0, "");
		uiDefButS(block, TOG|BIT|1, 0, "Scale 1/100", xco, yco-=20, 75, 19, &U.vrmlflag, 0.0, 0.0, 0, 0, "");
		uiDefButS(block, TOG|BIT|2, 0, "Two Sided", xco, yco-=20, 75, 19, &U.vrmlflag, 0.0, 0.0, 0, 0, "");

		ui::BlockSetDirection(block, UI_RIGHT);
		uiTextBoundsBlock(block, 50);  /* checks for fontsize */

		return block;
	}

	The uiDef coordinates here are only relative. When this function is called, the interface
	code automatically makes sure the buttons fit in the menu nicely.

	Inside a menu ui::Block, other ui::Blocks can be invoked to make a hierarchical menu.



-----------4.4 specials

15. KEYEVT

void uiDefKeyevtButS(ui::Block *block, int retval, char *str,
		short x1, short y1, short x2, short y2, short *spoin, char *tip)

	A special button, which stores a keyvalue in *spoin. When the button is pressed,
	it displays the text 'Press any Key'. A keypress then stores the value.

16. LINK and INLINK

	These button present a method of linking data in Blender, by drawing a line from one
	icon to another. It consists of two button types:

	LINK, the 'linking from' part, can be:
	- a single pointer to data (only one line allowed)
	- an array of pointers to data. The LINK buttons system  keeps track of allocating
	  space for the array, and set the correct pointers in it.

	INLINK, the 'linking to' part activates creating a link, when a user releases the mouse
	cursor over it, while dragging a line from the LINK button.

	These buttons are defined as follows:


Button but= uiDefIconBut(block, LINK, 0, ICON_LINK,	x1, y1, w, h, NULL, 0, 0, 0, 0, "");
	/* create the LINK icon */

uiSetButLink(but, void **pt, void ***ppt, short *totlink, short fromcode, short tocode);
	**pt: pointer to pointer (only one link allowed)
	***ppt: pointer to pointerpointer (an array of pointers)
	(Either one of these values should be NULL)

	fromcode: (currently unused)
	tocode: a short indicating which blocks it can link to.


uiDefIconBut(block, INLINK, 0, ICON_INLINK, x1, y1, w, h, void *poin, short fromcode, 0, 0, 0, "");
	poin: the pointer of the datablock you want to create links to
	fromcode: a short identifying which LINK buttons can connect to it



------------- 4.5 uiButton control functions


void uiButSetFunc(Button *but, void (*func)(void *arg1, void *arg2), void *arg1, void *arg2)
	When the button is pressed and released, it calls this function, with the 2 arguments.

void uiButSetFlag(Button *but, int flag)
	set a flag for further control of button behavior:
	flag:
	UI_TEXT_LEFT

int uiButGetRetVal(Button *but)
	gives return value


</body>
<br><br><br>


--- doc/python_api/examples/aud.py ---
"""
Basic Sound Playback
++++++++++++++++++++

This script shows how to use the classes: :class:`Device`, :class:`Sound` and
:class:`Handle`.
"""
import aud

device = aud.Device()
# Load sound file (it can be a video file with audio).
sound = aud.Sound('music.ogg')

# Play the audio, this return a handle to control play/pause.
handle = device.play(sound)
# If the audio is not too big and will be used often you can buffer it.
sound_buffered = aud.Sound.cache(sound)
handle_buffered = device.play(sound_buffered)

# Stop the sounds (otherwise they play until their ends).
handle.stop()
handle_buffered.stop()


--- doc/python_api/examples/blf.py ---
"""
Hello World Text Example
++++++++++++++++++++++++

Example of using the blf module. For this module to work we
need to use the GPU module :mod:`gpu` as well.
"""
# Import stand alone modules.
import blf
import bpy

font_info = {
    "font_id": 0,
    "handler": None,
}


def init():
    """init function - runs once"""
    import os
    # Create a new font object, use external TTF file.
    font_path = bpy.path.abspath('//Zeyada.ttf')
    # Store the font index - to use later.
    if os.path.exists(font_path):
        font_info["font_id"] = blf.load(font_path)
    else:
        # Default font.
        font_info["font_id"] = 0

    # Set the font drawing routine to run every frame.
    font_info["handler"] = bpy.types.SpaceView3D.draw_handler_add(
        draw_callback_px, (None, None), 'WINDOW', 'POST_PIXEL')


def draw_callback_px(self, context):
    """Draw on the viewports"""
    # BLF drawing routine.
    font_id = font_info["font_id"]
    blf.position(font_id, 2, 80, 0)
    blf.size(font_id, 50.0)
    blf.draw(font_id, "Hello World")


if __name__ == '__main__':
    init()


--- doc/python_api/examples/blf.1.py ---
"""
Drawing Text to an Image
++++++++++++++++++++++++

Example showing how text can be draw into an image.
This can be done by binding an image buffer (:mod:`imbuf`) to the font's ID.
"""

import blf
import imbuf

image_size = 512, 512
font_size = 20

ibuf = imbuf.new(image_size)

font_id = blf.load("/path/to/font.ttf")

blf.color(font_id, 1.0, 1.0, 1.0, 1.0)
blf.size(font_id, font_size)
blf.position(font_id, 0, image_size[0] - font_size, 0)

blf.enable(font_id, blf.WORD_WRAP)
blf.word_wrap(font_id, image_size[0])

with blf.bind_imbuf(font_id, ibuf, display_name="sRGB"):
    blf.draw_buffer(font_id, "Lots of wrapped text. " * 50)

imbuf.write(ibuf, filepath="/path/to/image.png")


--- doc/python_api/examples/bmesh.ops.1.py ---
# This script uses bmesh operators to make 2 links of a chain.

import bpy
import bmesh
import math
import mathutils

# Make a new BMesh
bm = bmesh.new()

# Add a circle XXX, should return all geometry created, not just verts.
bmesh.ops.create_circle(
    bm,
    cap_ends=False,
    radius=0.2,
    segments=8)


# Spin and deal with geometry on side 'a'
edges_start_a = bm.edges[:]
geom_start_a = bm.verts[:] + edges_start_a
ret = bmesh.ops.spin(
    bm,
    geom=geom_start_a,
    angle=math.radians(180.0),
    steps=8,
    axis=(1.0, 0.0, 0.0),
    cent=(0.0, 1.0, 0.0))
edges_end_a = [ele for ele in ret["geom_last"]
               if isinstance(ele, bmesh.types.BMEdge)]
del ret


# Extrude and create geometry on side 'b'
ret = bmesh.ops.extrude_edge_only(
    bm,
    edges=edges_start_a)
geom_extrude_mid = ret["geom"]
del ret


# Collect the edges to spin XXX, 'extrude_edge_only' could return this.
verts_extrude_b = [ele for ele in geom_extrude_mid
                   if isinstance(ele, bmesh.types.BMVert)]
edges_extrude_b = [ele for ele in geom_extrude_mid
                   if isinstance(ele, bmesh.types.BMEdge) and ele.is_boundary]
bmesh.ops.translate(
    bm,
    verts=verts_extrude_b,
    vec=(0.0, 0.0, 1.0))


# Create the circle on side 'b'
ret = bmesh.ops.spin(
    bm,
    geom=verts_extrude_b + edges_extrude_b,
    angle=-math.radians(180.0),
    steps=8,
    axis=(1.0, 0.0, 0.0),
    cent=(0.0, 1.0, 1.0))
edges_end_b = [ele for ele in ret["geom_last"]
               if isinstance(ele, bmesh.types.BMEdge)]
del ret


# Bridge the resulting edge loops of both spins 'a & b'
bmesh.ops.bridge_loops(
    bm,
    edges=edges_end_a + edges_end_b)


# Now we have made a links of the chain, make a copy and rotate it
# (so this looks something like a chain)

ret = bmesh.ops.duplicate(
    bm,
    geom=bm.verts[:] + bm.edges[:] + bm.faces[:])
geom_dupe = ret["geom"]
verts_dupe = [ele for ele in geom_dupe if isinstance(ele, bmesh.types.BMVert)]
del ret

# position the new link
bmesh.ops.translate(
    bm,
    verts=verts_dupe,
    vec=(0.0, 0.0, 2.0))
bmesh.ops.rotate(
    bm,
    verts=verts_dupe,
    cent=(0.0, 1.0, 0.0),
    matrix=mathutils.Matrix.Rotation(math.radians(90.0), 3, 'Z'))

# Done with creating the mesh, simply link it into the scene so we can see it

# Finish up, write the bmesh into a new mesh
me = bpy.data.meshes.new("Mesh")
bm.to_mesh(me)
bm.free()


# Add the mesh to the scene
obj = bpy.data.objects.new("Object", me)
bpy.context.collection.objects.link(obj)

# Select and make active
bpy.context.view_layer.objects.active = obj
obj.select_set(True)


--- doc/python_api/examples/bpy.app.driver_namespace.py ---
"""
File Loading & Order of Initialization
   Since drivers may be evaluated immediately after loading a blend-file it is necessary
   to ensure the driver name-space is initialized beforehand.

   This can be done by registering text data-blocks to execute on startup,
   which executes the scripts before drivers are evaluated.
   See *Text -> Register* from Blender's text editor.

   .. hint::

      You may prefer to use external files instead of Blender's text-blocks.
      This can be done using a text-block which executes an external file.

      This example runs ``driver_namespace.py`` located in the same directory as the text-blocks blend-file:

      .. code-block::

         import os
         import bpy
         blend_dir = os.path.normalize(os.path.join(__file__, "..", ".."))
         bpy.utils.execfile(os.path.join(blend_dir, "driver_namespace.py"))

      Using ``__file__`` ensures the text resolves to the expected path even when library-linked from another file.

   Other methods of populating the drivers name-space can be made to work but tend to be error prone:

   Using The ``--python`` command line argument to populate name-space often fails to achieve the desired goal
   because the initial evaluation will lookup a function that doesn't exist yet,
   marking the driver as invalid - preventing further evaluation.

   Populating the driver name-space before the blend-file loads also doesn't work
   since opening a file clears the name-space.

   It is possible to run a script via the ``--python`` command line argument, before the blend file.
   This can register a load-post handler (:mod:`bpy.app.handlers.load_post`) that initialized the name-space.
   While this works for background tasks it has the downside that opening the file from the file selector
   won't setup the name-space.
"""


--- doc/python_api/examples/bpy.app.handlers.py ---
"""
Basic Handler Example
+++++++++++++++++++++

This script shows the most simple example of adding a handler.
"""

import bpy


def my_handler(scene):
    print("Frame Change", scene.frame_current)


bpy.app.handlers.frame_change_pre.append(my_handler)


--- doc/python_api/examples/bpy.app.handlers.1.py ---
"""
Persistent Handler Example
++++++++++++++++++++++++++

By default handlers are freed when loading new files, in some cases you may
want the handler stay running across multiple files (when the handler is
part of an add-on for example).

For this the :data:`bpy.app.handlers.persistent` decorator needs to be used.
"""

import bpy
from bpy.app.handlers import persistent


@persistent
def load_handler(dummy):
    print("Load Handler:", bpy.data.filepath)


bpy.app.handlers.load_post.append(load_handler)


--- doc/python_api/examples/bpy.app.handlers.2.py ---
"""
Note on Altering Data
+++++++++++++++++++++

Altering data from handlers should be done carefully. While rendering the
``frame_change_pre`` and ``frame_change_post`` handlers are called from one
thread and the viewport updates from a different thread. If the handler changes
data that is accessed by the viewport, this can cause a crash of Blender. In
such cases, lock the interface (Render  Lock Interface or
:data:`bpy.types.RenderSettings.use_lock_interface`) before starting a render.

Below is an example of a mesh that is altered from a handler:
"""


def frame_change_pre(scene):
    # A triangle that shifts in the z direction.
    zshift = scene.frame_current * 0.1
    vertices = [(-1, -1, zshift), (1, -1, zshift), (0, 1, zshift)]
    triangles = [(0, 1, 2)]

    object = bpy.data.objects["The Object"]
    object.data.clear_geometry()
    object.data.from_pydata(vertices, [], triangles)


--- doc/python_api/examples/bpy.app.timers.1.py ---
"""
Run a Function in x Seconds
---------------------------
"""
import bpy


def in_5_seconds():
    print("Hello World")


bpy.app.timers.register(in_5_seconds, first_interval=5)


--- doc/python_api/examples/bpy.app.timers.2.py ---
"""
Run a Function every x Seconds
------------------------------
"""
import bpy


def every_2_seconds():
    print("Hello World")
    return 2.0


bpy.app.timers.register(every_2_seconds)


--- scripts/modules/addon_utils.py ---
# SPDX-FileCopyrightText: 2011-2023 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

__all__ = (
    "paths",
    "modules",
    "check",
    "check_extension",
    "enable",
    "disable",
    "disable_all",
    "reset_all",
    "module_bl_info",
    "extensions_refresh",
    "stale_pending_remove_paths",
    "stale_pending_stage_paths",
)

import bpy as _bpy
_preferences = _bpy.context.preferences

error_encoding = False
# (name, file, path)
error_duplicates = []
addons_fake_modules = {}

# Global cached extensions, set before loading extensions on startup.
# `{addon_module_name: "Reason for incompatibility", ...}`
_extensions_incompatible = {}
# Global extension warnings, lazily calculated when displaying extensions.
# `{addon_module_name: "Warning", ...}`
_extensions_warnings = {}

# Filename used for stale files (which we can't delete).
_stale_filename = ".~stale~"


# called only once at startup, avoids calling 'reset_all', correct but slower.
def _initialize_once():
    for path in paths():
        _bpy.utils._sys_path_ensure_append(path)

    _stale_pending_check_and_remove_once()

    _initialize_extensions_repos_once()

    for addon in _preferences.addons:
        enable(
            addon.module,
            # Ensured by `_initialize_extensions_repos_once`.
            refresh_handled=True,
        )

    _initialize_ensure_extensions_addon()


def paths():
    import os

    paths = []
    for i, p in enumerate(_bpy.utils.script_paths()):
        # Bundled add-ons are always first.
        # Since this isn't officially part of the API, print an error so this never silently fails.
        addon_dir = os.path.join(p, "addons_core" if i == 0 else "addons")
        if os.path.isdir(addon_dir):
            paths.append(addon_dir)
        elif i == 0:
            print("Internal error:", addon_dir, "was not found!")
    return paths


# A version of `paths` that includes extension repositories returning a list `(path, package)` pairs.
#
# Notes on the ``package`` value.
#
# - For top-level modules (the "addons" directories, the value is an empty string)
#   because those add-ons can be imported directly.
# - For extension repositories the value is a module string (which can be imported for example)
#   where any modules within the `path` can be imported as a sub-module.
#   So for example, given a list value of: `("/tmp/repo", "bl_ext.temp_repo")`.
#
#   An add-on located at `/tmp/repo/my_handy_addon.py` will have a unique module path of:
#   `bl_ext.temp_repo.my_handy_addon`, which can be imported and will be the value of it's `Addon.module`.
def _paths_with_extension_repos():

    import os
    addon_paths = [(path, "") for path in paths()]
    for repo in _preferences.extensions.repos:
        if not repo.enabled:
            continue
        dirpath = repo.directory
        if not os.path.isdir(dirpath):
            continue
        addon_paths.append((dirpath, "{:s}.{:s}".format(_ext_base_pkg_idname, repo.module)))

    return addon_paths


def _fake_module(mod_name, mod_path, speedy=True):
    global error_encoding
    import os

    if _bpy.app.debug_python:
        print("fake_module", mod_path, mod_name)

    if mod_name.startswith(_ext_base_pkg_idname_with_dot):
        return _fake_module_from_extension(mod_name, mod_path)

    import ast
    ModuleType = type(ast)
    try:
        file_mod = open(mod_path, "r", encoding='UTF-8')
    except OSError as ex:
        print("Error opening file:", mod_path, ex)
        return None

    with file_mod:
        if speedy:
            lines = []
            line_iter = iter(file_mod)
            line = ""
            while not line.startswith("bl_info"):
                try:
                    line = line_iter.readline()
                except UnicodeDecodeError as ex:
                    if not error_encoding:
                        error_encoding = True
                        print("Error reading file as UTF-8:", mod_path, ex)
                    return None

                if len(line) == 0:
                    break
            while line.rstrip():
                lines.append(line)
                try:
                    line = line_iter.readline()
                except UnicodeDecodeError as ex:
                    if not error_encoding:
                        error_encoding = True
                        print("Error reading file as UTF-8:", mod_path, ex)
                    return None

            data = "".join(lines)

        else:
            data = file_mod.read()
    del file_mod

    try:
        ast_data = ast.parse(data, filename=mod_path)
    except Exception:
        print("Syntax error 'ast.parse' cannot read:", repr(mod_path))
        import traceback
        traceback.print_exc()
        ast_data = None

    body_info = None

    if ast_data:
        for body in ast_data.body:
            if body.__class__ == ast.Assign:
                if len(body.targets) == 1:
                    if getattr(body.targets[0], "id", "") == "bl_info":
                        body_info = body
                        break

    if body_info:
        try:
            mod = ModuleType(mod_name)
            mod.bl_info = ast.literal_eval(body.value)
            mod.__file__ = mod_path
            mod.__time__ = os.path.getmtime(mod_path)
        except Exception:
            print("AST error parsing bl_info for:", repr(mod_path))
            import traceback
            traceback.print_exc()
            return None

        return mod
    else:
        print("Warning: add-on missing 'bl_info', this can cause poor performance!:", repr(mod_path))
        return None


def modules_refresh(*, module_cache=addons_fake_modules):
    global error_encoding
    import os

    error_encoding = False
    error_duplicates.clear()

    modules_stale = set(module_cache.keys())

    for path, pkg_id in _paths_with_extension_repos():
        for mod_name, mod_path in _bpy.path.module_names(path, package=pkg_id):
            modules_stale.discard(mod_name)
            mod = module_cache.get(mod_name)
            if mod is not None:
                if mod.__file__ != mod_path:
                    print(
                        "multiple addons with the same name:\n"
                        "  {!r}\n"
                        "  {!r}".format(mod.__file__, mod_path)
                    )
                    error_duplicates.append((mod.bl_info["name"], mod.__file__, mod_path))

                elif (
                        (mod.__time__ != os.path.getmtime(metadata_path := mod_path)) if not pkg_id else
                        # Check the manifest time as this is the source of the cache.
                        (mod.__time_manifest__ != os.path.getmtime(metadata_path := mod.__file_manifest__))
                ):
                    print("reloading addon meta-data:", mod_name, repr(metadata_path), "(time-stamp change detected)")
                    del module_cache[mod_name]
                    mod = None

            if mod is None:
                mod = _fake_module(
                    mod_name,
                    mod_path,
                )
                if mod:
                    module_cache[mod_name] = mod

    # just in case we get stale modules, not likely
    for mod_stale in modules_stale:
        del module_cache[mod_stale]
    del modules_stale


def modules(*, module_cache=addons_fake_modules, refresh=True):
    if refresh or ((module_cache is addons_fake_modules) and modules._is_first):
        modules_refresh(module_cache=module_cache)
        modules._is_first = False

        # Dictionaries are ordered in more recent versions of Python,
        # re-create the dictionary from sorted items.
        # This avoids having to sort on every call to this function.
        module_cache_items = list(module_cache.items())
        # Sort by name with the module name as a tie breaker.
        module_cache_items.sort(key=lambda item: ((item[1].bl_info.get("name") or item[0]).casefold(), item[0]))
        module_cache.clear()
        module_cache.update((key, value) for key, value in module_cache_items)

    return module_cache.values()


modules._is_first = True


def check(module_name):
    """
    Returns the loaded state of the addon.

    :arg module_name: The name of the addon and module.
    :type module_name: str
    :return: (loaded_default, loaded_state)
    :rtype: tuple[bool, bool]
    """
    import sys
    loaded_default = module_name in _preferences.addons

    mod = sys.modules.get(module_name)
    loaded_state = (
        (mod is not None) and
        getattr(mod, "__addon_enabled__", Ellipsis)
    )

    if loaded_state is Ellipsis:
        print(
            "Warning: addon-module", module_name, "found module "
            "but without '__addon_enabled__' field, "
            "possible name collision from file:",
            repr(getattr(mod, "__file__", "<unknown>")),
        )

        loaded_state = False

    if mod and getattr(mod, "__addon_persistent__", False):
        loaded_default = True

    return loaded_default, loaded_state


def check_extension(module_name):
    """
    Return true if the module is an extension.
    """
    return module_name.startswith(_ext_base_pkg_idname_with_dot)


# utility functions


def _addon_ensure(module_name):
    addons = _preferences.addons
    addon = addons.get(module_name)
    if not addon:
        addon = addons.new()
        addon.module = module_name


def _addon_remove(module_name):
    addons = _preferences.addons

    while module_name in addons:
        addon = addons.get(module_name)
        if addon:
            addons.remove(addon)


def enable(module_name, *, default_set=False, persistent=False, refresh_handled=False, handle_error=None):
    """
    Enables an addon by name.

    :arg module_name: the name of the addon and module.
    :type module_name: str
    :arg default_set: Set the user-preference.
    :type default_set: bool
    :arg persistent: Ensure the addon is enabled for the entire session (after loading new files).
    :type persistent: bool
    :arg refresh_handled: When true, :func:`extensions_refresh` must have been called with ``module_name``
       included in ``addon_modules_pending``.
       This should be used to avoid many calls to refresh extensions when enabling multiple add-ons at once.
    :type refresh_handled: bool
    :arg handle_error: Called in the case of an error, taking an exception argument.
    :type handle_error: Callable[[Exception], None] | None
    :return: the loaded module or None on failure.
    :rtype: ModuleType
    """

    import os
    import sys
    import importlib
    from _bpy_restrict_state import RestrictBlend

    if handle_error is None:
        def handle_error(ex):
            if isinstance(ex, ImportError):
                # NOTE: checking "Add-on " prefix is rather weak,
                # it's just a way to avoid the noise of a full trace-back when
                # an add-on is simply missing on the file-system.
                if (type(msg := ex.msg) is str) and msg.startswith("Add-on "):
                    print(msg)
                    return
            import traceback
            traceback.print_exc()

    if (is_extension := module_name.startswith(_ext_base_pkg_idname_with_dot)):
        if not refresh_handled:
            extensions_refresh(
                addon_modules_pending=[module_name],
                handle_error=handle_error,
            )

        # Ensure the extensions are compatible.
        if _extensions_incompatible:
            if (error := _extensions_incompatible.get(
                    module_name[len(_ext_base_pkg_idname_with_dot):].partition(".")[0::2],
            )):
                try:
                    raise RuntimeError("Extension {:s} is incompatible ({:s})".format(module_name, error))
                except RuntimeError as ex:
                    handle_error(ex)
                    # No need to call `extensions_refresh` because incompatible extensions
                    # will not have their wheels installed.
                    return None

        # NOTE: from now on, before returning None, `extensions_refresh()` must be called
        # to ensure wheels setup in anticipation for this extension being used are removed upon failure.

    # reload if the mtime changes
    mod = sys.modules.get(module_name)
    # chances of the file _not_ existing are low, but it could be removed

    # Set to `mod.__file__` or None.
    mod_file = None

    if (
            (mod is not None) and
            (mod_file := mod.__file__) is not None and
            os.path.exists(mod_file)
    ):

        if getattr(mod, "__addon_enabled__", False):
            # This is an unlikely situation,
            # re-register if the module is enabled.
            # Note: the UI doesn't allow this to happen,
            # in most cases the caller should 'check()' first.
            try:
                mod.unregister()
            except Exception as ex:
                print("Exception in module unregister():", (mod_file or module_name))
                handle_error(ex)
                if is_extension and not refresh_handled:
                    extensions_refresh(handle_error=handle_error)
                return None

        mod.__addon_enabled__ = False
        mtime_orig = getattr(mod, "__time__", 0)
        mtime_new = os.path.getmtime(mod_file)
        if mtime_orig != mtime_new:
            print("module changed on disk:", repr(mod_file), "reloading...")

            try:
                importlib.reload(mod)
            except Exception as ex:
                handle_error(ex)
                del sys.modules[module_name]

                if is_extension and not refresh_handled:
                    extensions_refresh(handle_error=handle_error)
                return None
            mod.__addon_enabled__ = False

    # add the addon first it may want to initialize its own preferences.
    # must remove on fail through.
    if default_set:
        _addon_ensure(module_name)

    # Split registering up into 3 steps so we can undo
    # if it fails par way through.

    # Disable the context: using the context at all
    # while loading an addon is really bad, don't do it!
    with RestrictBlend():

        # 1) try import
        try:
            # Use instead of `__import__` so that sub-modules can eventually be supported.
            # This is also documented to be the preferred way to import modules.
            mod = importlib.import_module(module_name)
            if (mod_file := mod.__file__) is None:
                # This can happen when:
                # - The add-on has been removed but there are residual `.pyc` files left behind.
                # - An extension is a directory that doesn't contain an `__init__.py` file.
                #
                # Include a message otherwise the "cause:" for failing to load the module is left blank.
                # Include the `__path__` when available so there is a reference to the location that failed to load.
                raise ImportError(
                    "module loaded with no associated file, __path__={!r}, aborting!".format(
                        getattr(mod, "__path__", None)
                    ),
                    name=module_name,
                )
            mod.__time__ = os.path.getmtime(mod_file)
            mod.__addon_enabled__ = False
        except Exception as ex:
            # If the add-on doesn't exist, don't print full trace-back because the back-trace is in this case
            # is verbose without any useful details. A missing path is better communicated in a short message.
            # Account for `ImportError` & `ModuleNotFoundError`.
            if isinstance(ex, ImportError):
                if ex.name == module_name:
                    ex.msg = "Add-on not loaded: \"{:s}\", cause: {:s}".format(module_name, str(ex))

                # Issue with an add-on from an extension repository, report a useful message.
                elif is_extension and module_name.startswith(ex.name + "."):
                    repo_id = module_name[len(_ext_base_pkg_idname_with_dot):].rpartition(".")[0]
                    repo = next(
                        (repo for repo in _preferences.extensions.repos if repo.module == repo_id),
                        None,
                    )
                    if repo is None:
                        ex.msg = (
                            "Add-on not loaded: \"{:s}\", cause: extension repository \"{:s}\" doesn't exist".format(
                                module_name, repo_id,
                            )
                        )
                    elif not repo.enabled:
                        ex.msg = (
                            "Add-on not loaded: \"{:s}\", cause: extension repository \"{:s}\" is disabled".format(
                                module_name, repo_id,
                            )
                        )
                    else:
                        # The repository exists and is enabled, it should have imported.
                        ex.msg = "Add-on not loaded: \"{:s}\", cause: {:s}".format(module_name, str(ex))

            handle_error(ex)

            if default_set:
                _addon_remove(module_name)
            if is_extension and not refresh_handled:
                extensions_refresh(handle_error=handle_error)
            return None

        if is_extension:
            # Handle the case the an extension has `bl_info` (which is not used for extensions).
            # Note that internally a `bl_info` is added based on the extensions manifest - for compatibility.
            # So it's important not to use this one.
            bl_info = getattr(mod, "bl_info", None)
            if bl_info is not None:
                # Use `_init` to detect when `bl_info` was generated from the manifest, see: `_bl_info_from_extension`.
                if type(bl_info) is dict and "_init" not in bl_info:
                    # This print is noisy, hide behind a debug flag.
                    # Once `bl_info` is fully deprecated this should be changed to always print a warning.
                    if _bpy.app.debug_python:
                        print(
                            "Add-on \"{:s}\" has a \"bl_info\" which will be ignored in favor of \"{:s}\"".format(
                                module_name, _ext_manifest_filename_toml,
                            )
                        )
                # Always remove as this is not expected to exist and will be lazily initialized.
                del mod.bl_info

        # 2) Try register collected modules.
        # Removed register_module, addons need to handle their own registration now.

        from _bpy import _bl_owner_id_get, _bl_owner_id_set
        owner_id_prev = _bl_owner_id_get()
        _bl_owner_id_set(module_name)

        # 3) Try run the modules register function.
        try:
            mod.register()
        except Exception as ex:
            print("Exception in module register():", (mod_file or module_name))
            handle_error(ex)
            del sys.modules[module_name]
            if default_set:
                _addon_remove(module_name)
            if is_extension and not refresh_handled:
                extensions_refresh(handle_error=handle_error)
            return None
        finally:
            _bl_owner_id_set(owner_id_prev)

    # * OK loaded successfully! *
    mod.__addon_enabled__ = True
    mod.__addon_persistent__ = persistent

    if _bpy.app.debug_python:
        print("\taddon_utils.enable", mod.__name__)

    return mod


def disable(module_name, *, default_set=False, refresh_handled=False, handle_error=None):
    """
    Disables an addon by name.

    :arg module_name: The name of the addon and module.
    :type module_name: str
    :arg default_set: Set the user-preference.
    :type default_set: bool
    :arg handle_error: Called in the case of an error, taking an exception argument.
    :type handle_error: Callable[[Exception], None] | None
    """
    import sys

    if handle_error is None:
        def handle_error(_ex):
            import traceback
            traceback.print_exc()

    mod = sys.modules.get(module_name)

    # Possible this add-on is from a previous session and didn't load a
    # module this time. So even if the module is not found, still disable
    # the add-on in the user preferences.
    if mod and getattr(mod, "__addon_enabled__", False) is not False:
        mod.__addon_enabled__ = False
        mod.__addon_persistent__ = False

        try:
            mod.unregister()
        except Exception as ex:
            mod_path = getattr(mod, "__file__", module_name)
            print("Exception in module unregister():", repr(mod_path))
            del mod_path
            handle_error(ex)
    else:
        print(
            "addon_utils.disable: {:s} not {:s}".format(
                module_name,
                "loaded" if mod is None else "enabled",
            )
        )

    # could be in more than once, unlikely but better do this just in case.
    if default_set:
        _addon_remove(module_name)

    if not refresh_handled:
        extensions_refresh(handle_error=handle_error)

    if _bpy.app.debug_python:
        print("\taddon_utils.disable", module_name)


def reset_all(*, reload_scripts=False):
    """
    Sets the addon state based on the user preferences.
    """
    import sys

    # Ensures stale `addons_fake_modules` isn't used.
    modules._is_first = True
    addons_fake_modules.clear()

    # Update extensions compatibility (after reloading preferences).
    # Potentially refreshing wheels too.
    extensions_refresh()

    for path, pkg_id in _paths_with_extension_repos():
        if not pkg_id:
            _bpy.utils._sys_path_ensure_append(path)

        for mod_name, _mod_path in _bpy.path.module_names(path, package=pkg_id):
            is_enabled, is_loaded = check(mod_name)

            # first check if reload is needed before changing state.
            if reload_scripts:
                import importlib
                mod = sys.modules.get(mod_name)
                if mod:
                    importlib.reload(mod)

            if is_enabled == is_loaded:
                pass
            elif is_enabled:
                enable(mod_name, refresh_handled=True)
            elif is_loaded:
                print("\taddon_utils.reset_all unloading", mod_name)
                disable(mod_name)


def disable_all():
    import sys
    # Collect modules to disable first because dict can be modified as we disable.

    # NOTE: don't use `getattr(item[1], "__addon_enabled__", False)` because this runs on all modules,
    # including 3rd party modules unrelated to Blender.
    #
    # Some modules may have their own `__getattr__` and either:
    # - Not raise an `AttributeError` (is they should),
    #   causing `hasattr` & `getattr` to raise an exception instead of treating the attribute as missing.
    # - Generate modules dynamically, modifying `sys.modules` which is being iterated over,
    #   causing a RuntimeError: "dictionary changed size during iteration".
    #
    # Either way, running 3rd party logic here can cause undefined behavior.
    # Use direct `__dict__` access to bypass `__getattr__`, see: #111649.
    modules = sys.modules.copy()
    addon_modules = [
        item for item in modules.items()
        if type(mod_dict := getattr(item[1], "__dict__", None)) is dict
        if mod_dict.get("__addon_enabled__")
    ]
    # Check the enabled state again since it's possible the disable call
    # of one add-on disables others.
    for mod_name, mod in addon_modules:
        if getattr(mod, "__addon_enabled__", False):
            disable(mod_name, refresh_handled=True)


def _blender_manual_url_prefix():
    return "https://docs.blender.org/manual/{:s}/{:d}.{:d}".format(
        _bpy.utils.manual_language_code(),
        *_bpy.app.version[:2],
    )


def _bl_info_basis():
    return {
        "name": "",
        "author": "",
        "version": (),
        "blender": (),
        "location": "",
        "description": "",
        "doc_url": "",
        "support": 'COMMUNITY',
        "category": "",
        "warning": "",
        "show_expanded": False,
    }


def module_bl_info(mod, *, info_basis=None):
    if info_basis is None:
        info_basis = _bl_info_basis()

    addon_info = getattr(mod, "bl_info", {})

    # avoid re-initializing
    if "_init" in addon_info:
        return addon_info

    if not addon_info:
        if mod.__name__.startswith(_ext_base_pkg_idname_with_dot):
            addon_info, filepath_toml = _bl_info_from_extension(mod.__name__, mod.__file__)
            if addon_info is None:
                # Unexpected, this is a malformed extension if meta-data can't be loaded.
                print("module_bl_info: failed to extract meta-data from", filepath_toml)
                # Continue to initialize dummy data.
                addon_info = {}

        mod.bl_info = addon_info

    for key, value in info_basis.items():
        addon_info.setdefault(key, value)

    if not addon_info["name"]:
        addon_info["name"] = mod.__name__

    doc_url = addon_info["doc_url"]
    if doc_url:
        doc_url_prefix = "{BLENDER_MANUAL_URL}"
        if doc_url_prefix in doc_url:
            addon_info["doc_url"] = doc_url.replace(
                doc_url_prefix,
                _blender_manual_url_prefix(),
            )

    # Remove the maintainers email while it's not private, showing prominently
    # could cause maintainers to get direct emails instead of issue tracking systems.
    import re
    if "author" in addon_info:
        addon_info["author"] = re.sub(r"\s*<.*?>", "", addon_info["author"])

    addon_info["_init"] = None
    return addon_info


# -----------------------------------------------------------------------------
# Stale File Handling
#
# Notes:
# - On startup, a file exists that indicates cleanup is needed.
#   In the common case the file doesn't exist.
#   Otherwise module paths are scanned for files to remove.
# - Since errors resolving paths to remove could result in user data loss,
#   ensure the paths are always within the (extension/add-on/app-template) directory.
# - File locking isn't used, if multiple Blender instances start at the
#   same time and try to remove the same files, this won't cause errors.
#   Even so, remove the checking file immediately avoid unnecessary
#   file-system access overhead for other Blender instances.
#
# For more implementation details see `_bpy_internal.extensions.stale_file_manager`.
# This mainly impacts WIN32 which can't remove open file handles, see: #77837 & #125049.
#
# Use for all systems as the problem can impact any system if file removal fails
# for any reason (typically permissions or file-system error).

def _stale_pending_filepath():
    # When this file exists, stale file removal is pending.
    # Try to remove stale files next launch.
    import os
    return os.path.join(_bpy.utils.user_resource('CONFIG'), "stale-pending")


def _stale_pending_stage(debug):
    import os

    stale_pending_filepath = _stale_pending_filepath()
    dirpath = os.path.dirname(stale_pending_filepath)

    if os.path.exists(stale_pending_filepath):
        return

    try:
        os.makedirs(dirpath, exist_ok=True)
        with open(stale_pending_filepath, "wb") as _:
            pass
    except Exception as ex:
        print("Unable to set stale files pending:", str(ex))


def _stale_file_directory_iter():
    import os

    for repo in _preferences.extensions.repos:
        if not repo.enabled:
            continue
        if repo.source == 'SYSTEM':
            continue
        yield repo.directory

    # Skip `addons_core` because add-ons because these will never be uninstalled by the user.
    yield from paths()[1:]

    # The `local_dir`, for wheels.
    yield os.path.join(_bpy.utils.user_resource('EXTENSIONS'), ".local")

    # The `path_app_templates`, for user app-templates.
    yield _bpy.utils.user_resource(
        'SCRIPTS',
        path=os.path.join("startup", "bl_app_templates_user"),
        create=False,
    )


def _stale_pending_check_and_remove_once():
    # This runs on every startup, early exit if no stale data removal is staged.
    import os
    stale_pending_filepath = _stale_pending_filepath()
    if not os.path.exists(stale_pending_filepath):
        return

    # Some stale data needs to be removed, this is an exceptional case.
    # Allow for slower logic than is typically accepted on startup.
    from _bpy_internal.extensions.stale_file_manager import StaleFiles
    debug = _bpy.app.debug_python

    # Remove the pending file if all are removed.
    is_empty = True

    for dirpath in _stale_file_directory_iter():
        if not os.path.exists(os.path.join(dirpath, _stale_filename)):
            continue

        try:
            stale_handle = StaleFiles(
                base_directory=dirpath,
                stale_filename=_stale_filename,
                debug=debug,
            )
            stale_handle.state_load(check_exists=True)
            if not stale_handle.is_empty():
                stale_handle.state_remove_all()
                if not stale_handle.is_empty():
                    is_empty = False
            if stale_handle.is_modified():
                stale_handle.state_store(check_exists=False)
        except Exception as ex:
            print("Unexpected error clearing stale data, this is a bug!", str(ex))

    if is_empty:
        try:
            os.remove(stale_pending_filepath)
        except Exception as ex:
            if debug:
                print("Failed to remove stale-pending file:", str(ex))


def stale_pending_stage_paths(path_base, paths):
    # - `path_base` must a directory iterated over by `_stale_file_directory_iter`.
    #   Otherwise the stale files will never be removed.
    # - `paths` must be absolute paths which could not be removed.
    #   They must be located within `base_path` otherwise they cannot be removed.
    from _bpy_internal.extensions.stale_file_manager import StaleFiles

    debug = _bpy.app.debug_python

    stale_handle = StaleFiles(
        base_directory=path_base,
        stale_filename=_stale_filename,
        debug=debug,
    )
    # Already checked.
    if stale_handle.state_load_add_and_store(paths=paths):
        # Force clearing stale files on next restart.
        _stale_pending_stage(debug)


def stale_pending_remove_paths(path_base, paths):
    # The reverse of: `stale_pending_stage_paths`.
    from _bpy_internal.extensions.stale_file_manager import StaleFiles

    debug = _bpy.app.debug_python

    stale_handle = StaleFiles(
        base_directory=path_base,
        stale_filename=_stale_filename,
        debug=debug,
    )
    # Already checked.
    if stale_handle.state_load_remove_and_store(paths=paths):
        # Don't attempt to reverse the `_stale_pending_stage` call.
        # This is not trivial since other repositories may need to be cleared.
        # There will be a minor performance hit on restart but this is enough
        # of a corner case that it's not worth attempting to calculate if
        # removal of pending files is needed or not.
        pass


# -----------------------------------------------------------------------------
# Extension Pre-Flight Compatibility Check
#
# Check extension compatibility on startup so any extensions which are incompatible with Blender are marked as
# incompatible and wont be loaded. This cache avoids having to scan all extensions on startup on *every* startup.
#
# Implementation:
#
# The emphasis for this cache is to have minimum overhead for the common case where:
# - The simple case where there are no extensions enabled (running tests, background tasks etc).
# - The more involved case where extensions are enabled and have not changed since last time Blender started.
#   In this case do as little as possible since it runs on every startup, the following steps are unavoidable.
# - When reading compatibility cache, then run the following tests, regenerating when changes are detected.
#   - Compare with previous blender version/platform.
#   - Stat the manifests of all enabled extensions, testing that their modification-time and size are unchanged.
# - When any changes are detected,
#   regenerate compatibility information which does more expensive operations
#   (loading manifests, check version ranges etc).
#
# Other notes:
#
# - This internal format may change at any point, regenerating the cache should be reasonably fast
#   but may introduce a small but noticeable pause on startup for user configurations that contain many extensions.
# - Failure to load will simply ignore the file and regenerate the file as needed.
#
# Format:
#
# - The cache is ZLIB compressed pickled Python dictionary.
# - The dictionary keys are as follows:
#   `"blender": (bpy.app.version, platform.system(), platform.machine(), python_version, magic_number)`
#   `"filesystem": [(repo_module, pkg_id, manifest_time, manifest_size), ...]`
#   `"incompatible": {(repo_module, pkg_id): "Reason for being incompatible", ...}`
#


def _pickle_zlib_file_read(filepath):
    import pickle
    import gzip

    with gzip.GzipFile(filepath, "rb") as fh:
        data = pickle.load(fh)
    return data


def _pickle_zlib_file_write(filepath, data) -> None:
    import pickle
    import gzip

    with gzip.GzipFile(filepath, "wb", compresslevel=9) as fh:
        pickle.dump(data, fh)


def _extension_repos_module_to_directory_map():
    return {repo.module: repo.directory for repo in _preferences.extensions.repos if repo.enabled}


def _extension_compat_cache_update_needed(
        cache_data,  # `dict[str, Any]`
        blender_id,  # `tuple[Any, ...]`
        extensions_enabled,  # `set[tuple[str, str]]`
        print_debug,  # `Callable[[Any], None] | None`
):  # `-> bool`

    # Detect when Blender itself changes.
    if cache_data.get("blender") != blender_id:
        if print_debug is not None:
            print_debug("blender changed")
        return True

    # Detect when any of the extensions paths change.
    cache_filesystem = cache_data.get("filesystem", [])

    # Avoid touching the file-system if at all possible.
    # When the length is the same and all cached ID's are in this set, we can be sure they are a 1:1 patch.
    if len(cache_filesystem) != len(extensions_enabled):
        if print_debug is not None:
            print_debug("length changes ({:d} -> {:d}).".format(len(cache_filesystem), len(extensions_enabled)))
        return True

    from os import stat
    from os.path import join
    repos_module_to_directory_map = _extension_repos_module_to_directory_map()

    for repo_module, pkg_id, cache_stat_time, cache_stat_size in cache_filesystem:
        if (repo_module, pkg_id) not in extensions_enabled:
            if print_debug is not None:
                print_debug("\"{:s}.{:s}\" no longer enabled.".format(repo_module, pkg_id))
            return True

        if repo_directory := repos_module_to_directory_map.get(repo_module, ""):
            pkg_manifest_filepath = join(repo_directory, pkg_id, _ext_manifest_filename_toml)
        else:
            pkg_manifest_filepath = ""

        # It's possible an extension has been set as an add-on but cannot find the repository it came from.
        # In this case behave as if the file can't be found (because it can't) instead of ignoring it.
        # This is done because it's important to match.
        if pkg_manifest_filepath:
            try:
                statinfo = stat(pkg_manifest_filepath)
            except Exception:
                statinfo = None
        else:
            statinfo = None

        if statinfo is None:
            test_time = 0
            test_size = 0
        else:
            test_time = statinfo.st_mtime
            test_size = statinfo.st_size

        # Detect changes to any files manifest.
        if cache_stat_time != test_time:
            if print_debug is not None:
                print_debug("\"{:s}.{:s}\" time changed ({:g} -> {:g}).".format(
                    repo_module, pkg_id, cache_stat_time, test_time,
                ))
            return True
        if cache_stat_size != test_size:
            if print_debug is not None:
                print_debug("\"{:s}.{:s}\" size changed ({:d} -> {:d}).".format(
                    repo_module, pkg_id, cache_stat_size, test_size,
                ))
            return True

    return False


# This function should not run every startup, so it can afford to be slower,
# although users should not have to wait for it either.
def _extension_compat_cache_create(
        blender_id,  # `tuple[Any, ...]`
        extensions_enabled,  # `set[tuple[str, str]]`
        wheel_list,  # `list[tuple[str, list[str]]]`
        print_debug,  # `Callable[[Any], None] | None`
):  # `-> dict[str, Any]`
    import os
    from os.path import join

    filesystem = []
    incompatible = {}

    cache_data = {
        "blender": blender_id,
        "filesystem": filesystem,
        "incompatible": incompatible,
    }

    repos_module_to_directory_map = _extension_repos_module_to_directory_map()

    # Only import this module once (if at all).
    bl_pkg = None

    for repo_module, pkg_id in extensions_enabled:
        if repo_directory := repos_module_to_directory_map.get(repo_module, ""):
            pkg_manifest_filepath = join(repo_directory, pkg_id, _ext_manifest_filename_toml)
        else:
            pkg_manifest_filepath = ""
            if print_debug is not None:
                print_debug("directory for module \"{:s}\" not found!".format(repo_module))

        if pkg_manifest_filepath:
            try:
                statinfo = os.stat(pkg_manifest_filepath)
            except Exception:
                statinfo = None
                if print_debug is not None:
                    print_debug("unable to find \"{:s}\"".format(pkg_manifest_filepath))
        else:
            statinfo = None

        if statinfo is None:
            test_time = 0.0
            test_size = 0
        else:
            test_time = statinfo.st_mtime
            test_size = statinfo.st_size
            # Store the reason for failure, to print when attempting to load.

            # Only load the module once.
            if bl_pkg is None:
                # Without `bl_pkg.__time__` this will detect as having been changed and
                # reload the module when loading the add-on.
                import bl_pkg
                if getattr(bl_pkg, "__time__", 0) == 0:
                    try:
                        bl_pkg.__time__ = os.path.getmtime(bl_pkg.__file__)
                    except Exception as ex:
                        if print_debug is not None:
                            print_debug(str(ex))

            if (error := bl_pkg.manifest_compatible_with_wheel_data_or_error(
                    pkg_manifest_filepath,
                    repo_module,
                    pkg_id,
                    repo_directory,
                    wheel_list,
            )) is not None:
                incompatible[(repo_module, pkg_id)] = error

        filesystem.append((repo_module, pkg_id, test_time, test_size))

    return cache_data


def _initialize_extensions_compat_ensure_up_to_date(extensions_directory, extensions_enabled, print_debug):
    import os
    import platform
    import sys

    global _extensions_incompatible

    updated = False
    wheel_list = []

    # Number to bump to change this format and force re-generation.
    magic_number = 0

    blender_id = (_bpy.app.version, platform.system(), platform.machine(), sys.version_info[0:2], magic_number)

    filepath_compat = os.path.join(extensions_directory, ".cache", "compat.dat")

    # Cache data contains a dict of:
    # {
    #   "blender": (...)
    #   "paths": [path data to detect changes]
    #   "incompatible": {set of incompatible extensions}
    # }
    if os.path.exists(filepath_compat):
        try:
            cache_data = _pickle_zlib_file_read(filepath_compat)
        except Exception as ex:
            cache_data = None
            # While this should not happen continuously (that would point to writing invalid cache),
            # it is not a problem if there is some corruption with the cache and it needs to be re-generated.
            # Show a message since this should be a rare occurrence - if it happens often it's likely to be a bug.
            print("Extensions: reading cache failed ({:s}), creating...".format(str(ex)))
    else:
        cache_data = None
        if print_debug is not None:
            print_debug("doesn't exist, creating...")

    if cache_data is not None:
        # NOTE: the exception handling here is fairly paranoid and accounts for invalid values in the loaded cache.
        # An example would be values expected to be lists/dictionaries being other types (None or strings for example).
        # While this should not happen, some bad value should not prevent Blender from loading properly,
        # so report the error and regenerate cache.
        try:
            if _extension_compat_cache_update_needed(cache_data, blender_id, extensions_enabled, print_debug):
                cache_data = None
        except Exception:
            print("Extension: unexpected error reading cache, this is a bug! (regenerating)")
            import traceback
            traceback.print_exc()
            cache_data = None

    if cache_data is None:
        cache_data = _extension_compat_cache_create(blender_id, extensions_enabled, wheel_list, print_debug)
        try:
            os.makedirs(os.path.dirname(filepath_compat), exist_ok=True)
            _pickle_zlib_file_write(filepath_compat, cache_data)
            if print_debug is not None:
                print_debug("update written to disk.")
        except Exception as ex:
            # Should be rare but should not cause this function to fail.
            print("Extensions: writing cache failed ({:s}).".format(str(ex)))

        # Set to true even when not written to disk as the run-time data *has* been updated,
        # cache will attempt to be generated next time this is called.
        updated = True
    else:
        if print_debug is not None:
            print_debug("up to date.")

    _extensions_incompatible = cache_data["incompatible"]

    return updated, wheel_list


def _initialize_extensions_compat_ensure_up_to_date_wheels(extensions_directory, wheel_list, debug, error_fn):
    import os
    _extension_sync_wheels(
        local_dir=os.path.join(extensions_directory, ".local"),
        wheel_list=wheel_list,
        debug=debug,
        error_fn=error_fn,
    )


def _initialize_extensions_compat_data(
        extensions_directory,  # `str`
        *,
        ensure_wheels,  # `bool`
        addon_modules_pending,  # `Sequence[str] | None`
        use_startup_fastpath,  # `bool`
        error_fn,  # `Callable[[Exception], None] | None`
):
    # WARNING: this function must *never* raise an exception because it would interfere with low level initialization.
    # As the function deals with file IO, use what are typically over zealous exception checks so as to rule out
    # interfering with Blender loading properly in unexpected cases such as disk-full, read-only file-system
    # or any other rare but possible scenarios.

    _extensions_incompatible.clear()

    # Create a set of all extension ID's.
    extensions_enabled = set()
    extensions_prefix_len = len(_ext_base_pkg_idname_with_dot)
    for addon in _preferences.addons:
        module_name = addon.module
        if check_extension(module_name):
            extensions_enabled.add(module_name[extensions_prefix_len:].partition(".")[0::2])

    if addon_modules_pending is not None:
        for module_name in addon_modules_pending:
            if check_extension(module_name):
                extensions_enabled.add(module_name[extensions_prefix_len:].partition(".")[0::2])

    debug = _bpy.app.debug_python
    print_debug = (lambda *args, **kwargs: print("Extension version cache:", *args, **kwargs)) if debug else None

    # Early exit, use for automated tests.
    # Avoid (relatively) expensive file-system scanning if at all possible.
    #
    # - On startup when there are no extensions enabled, scanning and synchronizing wheels
    #   adds unnecessary overhead. Especially considering this will run for automated tasks.
    # - When disabling an add-on from the UI, there may be no extensions enabled afterwards,
    #   however the extension that was disabled may have had wheels installed which must be removed,
    #   so in this case it's important not to skip synchronizing wheels, see: #125958.
    if use_startup_fastpath and (not extensions_enabled):
        if print_debug is not None:
            print_debug("no extensions, skipping cache data.")
        return

    # While this isn't expected to fail, any failure here is a bug
    # but it should not cause Blender's startup to fail.
    try:
        updated, wheel_list = _initialize_extensions_compat_ensure_up_to_date(
            extensions_directory,
            extensions_enabled,
            print_debug,
        )
    except Exception:
        print("Extension: unexpected error detecting cache, this is a bug!")
        import traceback
        traceback.print_exc()
        updated = False

    if ensure_wheels:
        if updated:
            if error_fn is None:
                def error_fn(ex):
                    print("Error:", str(ex))

            try:
                _initialize_extensions_compat_ensure_up_to_date_wheels(
                    extensions_directory,
                    wheel_list,
                    debug,
                    error_fn=error_fn,
                )
            except Exception:
                print("Extension: unexpected error updating wheels, this is a bug!")
                import traceback
                traceback.print_exc()


# -----------------------------------------------------------------------------
# Extension Utilities

def _version_int_left_digits(x):
    # Parse as integer until the first non-digit.
    return int(x[:next((i for i, c in enumerate(x) if not c.isdigit()), len(x))] or "0")


def _bl_info_from_extension(mod_name, mod_path):
    # Extract the `bl_info` from an extensions manifest.
    # This is returned as a module which has a `bl_info` variable.
    # When support for non-extension add-ons is dropped (Blender v5.0 perhaps)
    # this can be updated not to use a fake module.
    import os
    import tomllib

    bl_info = _bl_info_basis()

    filepath_toml = os.path.join(os.path.dirname(mod_path), _ext_manifest_filename_toml)
    try:
        with open(filepath_toml, "rb") as fh:
            data = tomllib.load(fh)
    except FileNotFoundError:
        print("Warning: add-on missing manifest, this can cause poor performance!:", repr(filepath_toml))
        return None, filepath_toml
    except Exception as ex:
        print("Error:", str(ex), "in", filepath_toml)
        return None, filepath_toml

    # This isn't a full validation which happens on package install/update.
    if (value := data.get("name", None)) is None:
        print("Error: missing \"name\" in", filepath_toml)
        return None, filepath_toml
    if type(value) is not str:
        print("Error: \"name\" is not a string in", filepath_toml)
        return None, filepath_toml
    bl_info["name"] = value

    if (value := data.get("version", None)) is None:
        print("Error: missing \"version\" in", filepath_toml)
        return None, filepath_toml
    if type(value) is not str:
        print("Error: \"version\" is not a string in", filepath_toml)
        return None, filepath_toml
    try:
        value = tuple(
            (int if i < 2 else _version_int_left_digits)(x)
            for i, x in enumerate(value.split(".", 2))
        )
    except Exception as ex:
        print("Error: \"version\" is not a semantic version (X.Y.Z) in ", filepath_toml, str(ex))
        return None, filepath_toml
    bl_info["version"] = value

    if (value := data.get("blender_version_min", None)) is None:
        print("Error: missing \"blender_version_min\" in", filepath_toml)
        return None, filepath_toml
    if type(value) is not str:
        print("Error: \"blender_version_min\" is not a string in", filepath_toml)
        return None, filepath_toml
    try:
        value = tuple(int(x) for x in value.split("."))
    except Exception as ex:
        print("Error:", str(ex), "in \"blender_version_min\"", filepath_toml)
        return None, filepath_toml
    bl_info["blender"] = value

    # Only print warnings since description is not a mandatory field.
    if (value := data.get("tagline", None)) is None:
        print("Warning: missing \"tagline\" in", filepath_toml)
    elif type(value) is not str:
        print("Warning: \"tagline\" is not a string", filepath_toml)
    else:
        bl_info["description"] = value

    if (value := data.get("maintainer", None)) is None:
        print("Error: missing \"author\" in", filepath_toml)
        return None, filepath_toml
    if type(value) is not str:
        print("Error: \"maintainer\" is not a string", filepath_toml)
        return None, filepath_toml
    bl_info["author"] = value

    bl_info["category"] = "Development"  # Dummy, will be removed.

    return bl_info, filepath_toml


def _fake_module_from_extension(mod_name, mod_path):
    import os

    bl_info, filepath_toml = _bl_info_from_extension(mod_name, mod_path)
    if bl_info is None:
        return None

    ModuleType = type(os)
    mod = ModuleType(mod_name)
    mod.bl_info = bl_info
    mod.__file__ = mod_path
    mod.__time__ = os.path.getmtime(mod_path)

    # NOTE(@ideasman42): Add non-standard manifest variables to the "fake" module,
    # this isn't ideal as it moves further away from the return value being minimal fake-module
    # (where `__name__` and `__file__` are typically used).
    # A custom type could be used, however this needs to be done carefully
    # as all users of `addon_utils.modules(..)` need to be updated.
    mod.__file_manifest__ = filepath_toml
    mod.__time_manifest__ = os.path.getmtime(filepath_toml)
    return mod


def _extension_sync_wheels(
        *,
        local_dir,  # `str`
        wheel_list,  # `list[WheelSource]`
        debug,           # `bool`
        error_fn,  # `Callable[[Exception], None]`
):  # `-> None`
    import os
    import sys
    from _bpy_internal.extensions.wheel_manager import apply_action

    local_dir_site_packages = os.path.join(
        local_dir,
        "lib",
        "python{:d}.{:d}".format(*sys.version_info[0:2]),
        "site-packages",
    )

    paths_stale = []

    def remove_error_fn(filepath: str, _ex: Exception) -> None:
        paths_stale.append(filepath)

    apply_action(
        local_dir=local_dir,
        local_dir_site_packages=local_dir_site_packages,
        wheel_list=wheel_list,
        error_fn=error_fn,
        remove_error_fn=remove_error_fn,
        debug=debug,
    )

    if paths_stale:
        stale_pending_stage_paths(local_dir, paths_stale)

    if os.path.exists(local_dir_site_packages):
        if local_dir_site_packages not in sys.path:
            sys.path.append(local_dir_site_packages)


# -----------------------------------------------------------------------------
# Extensions

def _initialize_ensure_extensions_addon():
    module_name = "bl_pkg"
    if module_name not in _preferences.addons:
        enable(module_name, default_set=True, persistent=True)


# Module-like class, store singletons.
class _ext_global:
    __slots__ = ()

    # Store a map of `preferences.extensions.repos` -> `module_id`.
    # Only needed to detect renaming between `bpy.app.handlers.extension_repos_update_{pre & post}` events.
    #
    # The first dictionary is for enabled repositories, the second for disabled repositories
    # which can be ignored in most cases and is only needed for a module rename.
    idmap_pair = {}, {}

    # The base package created by `JunctionModuleHandle`.
    module_handle = None


# The name (in `sys.modules`) keep this short because it's stored as part of add-on modules name.
_ext_base_pkg_idname = "bl_ext"
_ext_base_pkg_idname_with_dot = _ext_base_pkg_idname + "."
_ext_manifest_filename_toml = "blender_manifest.toml"


def _extension_module_name_decompose(package):
    # Returns the repository module name and the extensions ID from an extensions module name (``__package__``).
    #
    # :arg module_name: The extensions module name.
    # :type module_name: str
    # :return: (repo_module_name, extension_id)
    # :rtype: tuple[str, str]

    if not package.startswith(_ext_base_pkg_idname_with_dot):
        raise ValueError("The \"package\" does not name an extension")

    repo_module, pkg_idname = package[len(_ext_base_pkg_idname_with_dot):].partition(".")[0::2]
    if not (repo_module and pkg_idname):
        raise ValueError("The \"package\" is expected to be a module name containing 3 components")

    if "." in pkg_idname:
        raise ValueError("The \"package\" is expected to be a module name containing 3 components, found {:d}".format(
            pkg_idname.count(".") + 3
        ))

    # Unlikely but possible.
    if not (repo_module.isidentifier() and pkg_idname.isidentifier()):
        raise ValueError("The \"package\" contains non-identifier characters")

    return repo_module, pkg_idname


def _extension_preferences_idmap():
    repos_idmap = {}
    repos_idmap_disabled = {}
    for repo in _preferences.extensions.repos:
        if repo.enabled:
            repos_idmap[repo.as_pointer()] = repo.module
        else:
            repos_idmap_disabled[repo.as_pointer()] = repo.module
    return repos_idmap, repos_idmap_disabled


def _extension_dirpath_from_preferences():
    repos_dict = {}
    for repo in _preferences.extensions.repos:
        if not repo.enabled:
            continue
        repos_dict[repo.module] = repo.directory
    return repos_dict


def _extension_dirpath_from_handle():
    repos_info = {}
    for module_id, module in _ext_global.module_handle.submodule_items():
        # Account for it being unset although this should never happen unless script authors
        # meddle with the modules.
        try:
            dirpath = module.__path__[0]
        except Exception:
            dirpath = ""
        repos_info[module_id] = dirpath
    return repos_info


# Ensure the add-ons follow changes to repositories, enabling, disabling and module renaming.
def _initialize_extension_repos_post_addons_prepare(
        module_handle,
        *,
        submodules_del,
        submodules_add,
        submodules_rename_module,
        submodules_del_disabled,
        submodules_rename_module_disabled,
):
    addons_to_enable = []
    if not (
            submodules_del or
            submodules_add or
            submodules_rename_module or
            submodules_del_disabled or
            submodules_rename_module_disabled
    ):
        return addons_to_enable

    # All preferences info.
    # Map: `repo_id -> {submodule_id -> addon, ...}`.
    addon_userdef_info = {}
    for addon in _preferences.addons:
        module = addon.module
        if not module.startswith(_ext_base_pkg_idname_with_dot):
            continue
        module_id, submodule_id = module[len(_ext_base_pkg_idname_with_dot):].partition(".")[0::2]
        try:
            addon_userdef_info[module_id][submodule_id] = addon
        except KeyError:
            addon_userdef_info[module_id] = {submodule_id: addon}

    # All run-time info.
    # Map: `module_id -> {submodule_id -> module, ...}`.
    addon_runtime_info = {}
    for module_id, repo_module in module_handle.submodule_items():
        extensions_info = {}
        for submodule_id in dir(repo_module):
            if submodule_id.startswith("_"):
                continue
            mod = getattr(repo_module, submodule_id)
            # Filter out non add-on, non-modules.
            if not hasattr(mod, "__addon_enabled__"):
                continue
            extensions_info[submodule_id] = mod
        addon_runtime_info[module_id] = extensions_info
        del extensions_info

    # Apply changes to add-ons.
    if submodules_add:
        # Re-enable add-ons that exist in the user preferences,
        # this lets the add-ons state be restored when toggling a repository.
        for module_id, _dirpath in submodules_add:
            repo_userdef = addon_userdef_info.get(module_id, {})
            repo_runtime = addon_runtime_info.get(module_id, {})

            for submodule_id, addon in repo_userdef.items():
                module_name_next = "{:s}.{:s}.{:s}".format(_ext_base_pkg_idname, module_id, submodule_id)
                # Only default & persistent add-ons are kept for re-activation.
                default_set = True
                persistent = True
                addons_to_enable.append((module_name_next, addon, default_set, persistent))

    for module_id_prev, module_id_next in submodules_rename_module:
        repo_userdef = addon_userdef_info.get(module_id_prev, {})
        repo_runtime = addon_runtime_info.get(module_id_prev, {})
        for submodule_id, mod in repo_runtime.items():
            if not getattr(mod, "__addon_enabled__", False):
                continue
            module_name_prev = "{:s}.{:s}.{:s}".format(_ext_base_pkg_idname, module_id_prev, submodule_id)
            module_name_next = "{:s}.{:s}.{:s}".format(_ext_base_pkg_idname, module_id_next, submodule_id)
            disable(module_name_prev, default_set=False, refresh_handled=True)
            addon = repo_userdef.get(submodule_id)
            default_set = addon is not None
            persistent = getattr(mod, "__addon_persistent__", False)
            addons_to_enable.append((module_name_next, addon, default_set, persistent))

    for module_id_prev, module_id_next in submodules_rename_module_disabled:
        repo_userdef = addon_userdef_info.get(module_id_prev, {})
        repo_runtime = addon_runtime_info.get(module_id_prev, {})
        for submodule_id, addon in repo_userdef.items():
            mod = repo_runtime.get(submodule_id)
            if mod is not None and getattr(mod, "__addon_enabled__", False):
                continue
            # Either there is no run-time data or the module wasn't enabled.
            # Rename the add-on without enabling it so the next time it's enabled it's preferences are kept.
            module_name_next = "{:s}.{:s}.{:s}".format(_ext_base_pkg_idname, module_id_next, submodule_id)
            addon.module = module_name_next

    if submodules_del:
        repo_module_map = {repo.module: repo for repo in _preferences.extensions.repos}
        for module_id in submodules_del:
            repo_userdef = addon_userdef_info.get(module_id, {})
            repo_runtime = addon_runtime_info.get(module_id, {})

            repo = repo_module_map.get(module_id)
            default_set = True
            if repo and not repo.enabled:
                # The repository exists but has been disabled, keep the add-on preferences
                # because the user may want to re-enable the repository temporarily.
                default_set = False

            for submodule_id, mod in repo_runtime.items():
                module_name_prev = "{:s}.{:s}.{:s}".format(_ext_base_pkg_idname, module_id, submodule_id)
                disable(module_name_prev, default_set=default_set, refresh_handled=True)
            del repo
        del repo_module_map

    if submodules_del_disabled:
        for module_id_prev in submodules_del_disabled:
            repo_userdef = addon_userdef_info.get(module_id_prev, {})
            for submodule_id in repo_userdef.keys():
                module_name_prev = "{:s}.{:s}.{:s}".format(_ext_base_pkg_idname, module_id_prev, submodule_id)
                disable(module_name_prev, default_set=True, refresh_handled=True)

    return addons_to_enable


# Enable add-ons after the modules have been manipulated.
def _initialize_extension_repos_post_addons_restore(addons_to_enable):
    if not addons_to_enable:
        return

    # Important to refresh wheels & compatibility data before enabling.
    extensions_refresh(addon_modules_pending=[module_name_next for (module_name_next, _, _, _) in addons_to_enable])

    any_failed = False
    for (module_name_next, addon, default_set, persistent) in addons_to_enable:
        # Ensure the preferences are kept.
        if addon is not None:
            addon.module = module_name_next
        if enable(module_name_next, default_set=default_set, persistent=persistent) is None:
            any_failed = True

    # Remove wheels for any add-ons that failed to enable.
    if any_failed:
        extensions_refresh()

    # Needed for module rename.
    _is_first_reset()


# Use `bpy.app.handlers.extension_repos_update_{pre/post}` to track changes to extension repositories
# and sync the changes to the Python module.


@_bpy.app.handlers.persistent
def _initialize_extension_repos_pre(*_):
    _ext_global.idmap_pair = _extension_preferences_idmap()


@_bpy.app.handlers.persistent
def _initialize_extension_repos_post(*_, is_first=False):

    # When enabling extensions for the first time, ensure the add-on is enabled.
    _initialize_ensure_extensions_addon()

    do_addons = not is_first

    # Map `module_id` -> `dirpath`.
    repos_info_prev = _extension_dirpath_from_handle()
    repos_info_next = _extension_dirpath_from_preferences()

    # Map `repo.as_pointer()` -> `module_id`.
    repos_idmap_prev, repos_idmap_prev_disabled = _ext_global.idmap_pair
    repos_idmap_next, repos_idmap_next_disabled = _extension_preferences_idmap()

    # Map `module_id` -> `repo.as_pointer()`.
    repos_idmap_next_reverse = {value: key for key, value in repos_idmap_next.items()}

    # Mainly needed when the state of repositories changes at run-time:
    # factory settings then load preferences for example.
    #
    # Filter `repos_idmap_prev` so only items which were also in the `repos_info_prev` are included.
    # This is an awkward situation, they should be in sync, however when enabling the experimental option
    # means the preferences wont have changed, but the module will not be in sync with the preferences.
    # Support this by removing items in `repos_idmap_prev` which aren't also initialized in the managed package.
    #
    # The only situation this would be useful to keep is if we want to support renaming a package
    # that manipulates all add-ons using it, when those add-ons are in the preferences but have not had
    # their package loaded. It's possible we want to do this but is also reasonably obscure.
    for repo_id_prev, module_id_prev in list(repos_idmap_prev.items()):
        if module_id_prev not in repos_info_prev:
            del repos_idmap_prev[repo_id_prev]

    submodules_add = []  # List of module names to add: `(module_id, dirpath)`.
    submodules_del = []  # List of module names to remove: `module_id`.
    submodules_rename_module = []  # List of module names: `(module_id_src, module_id_dst)`.
    submodules_rename_dirpath = []  # List of module names: `(module_id, dirpath)`.

    renamed_prev = set()
    renamed_next = set()

    # Detect rename modules & module directories.
    for module_id_next, dirpath_next in repos_info_next.items():
        # Lookup never fails, as the "next" values use: `preferences.extensions.repos`.
        repo_id = repos_idmap_next_reverse[module_id_next]
        # Lookup may fail if this is a newly added module.
        # Don't attempt to setup `submodules_add` though as it's possible
        # the module name persists while the underlying `repo_id` changes.
        module_id_prev = repos_idmap_prev.get(repo_id)
        if module_id_prev is None:
            continue

        # Detect rename.
        if module_id_next != module_id_prev:
            submodules_rename_module.append((module_id_prev, module_id_next))
            renamed_prev.add(module_id_prev)
            renamed_next.add(module_id_next)

        # Detect `dirpath` change.
        if dirpath_next != repos_info_prev[module_id_prev]:
            submodules_rename_dirpath.append((module_id_next, dirpath_next))

    # Detect added modules.
    for module_id, dirpath in repos_info_next.items():
        if (module_id not in repos_info_prev) and (module_id not in renamed_next):
            submodules_add.append((module_id, dirpath))
    # Detect deleted modules.
    for module_id, _dirpath in repos_info_prev.items():
        if (module_id not in repos_info_next) and (module_id not in renamed_prev):
            submodules_del.append(module_id)

    if do_addons:
        submodules_del_disabled = []  # A version of `submodules_del` for disabled repositories.
        submodules_rename_module_disabled = []  # A version of `submodules_rename_module` for disabled repositories.

        # Detect deleted modules.
        for repo_id_prev, module_id_prev in repos_idmap_prev_disabled.items():
            if (
                    (repo_id_prev not in repos_idmap_next_disabled) and
                    (repo_id_prev not in repos_idmap_next)
            ):
                submodules_del_disabled.append(module_id_prev)

        # Detect rename of disabled modules.
        for repo_id_next, module_id_next in repos_idmap_next_disabled.items():
            module_id_prev = repos_idmap_prev_disabled.get(repo_id_next)
            if module_id_prev is None:
                continue
            # Detect rename.
            if module_id_next != module_id_prev:
                submodules_rename_module_disabled.append((module_id_prev, module_id_next))

        addons_to_enable = _initialize_extension_repos_post_addons_prepare(
            _ext_global.module_handle,
            submodules_del=submodules_del,
            submodules_add=submodules_add,
            submodules_rename_module=submodules_rename_module,
            submodules_del_disabled=submodules_del_disabled,
            submodules_rename_module_disabled=submodules_rename_module_disabled,
        )
        del submodules_del_disabled, submodules_rename_module_disabled

        # Apply changes to the `_ext_base_pkg_idname` named module so it matches extension data from the preferences.
    module_handle = _ext_global.module_handle
    for module_id in submodules_del:
        module_handle.unregister_submodule(module_id)
    for module_id, dirpath in submodules_add:
        module_handle.register_submodule(module_id, dirpath)
    for module_id_prev, module_id_next in submodules_rename_module:
        module_handle.rename_submodule(module_id_prev, module_id_next)
    for module_id, dirpath in submodules_rename_dirpath:
        module_handle.rename_directory(module_id, dirpath)

    _ext_global.idmap_pair[0].clear()
    _ext_global.idmap_pair[1].clear()

    if do_addons:
        _initialize_extension_repos_post_addons_restore(addons_to_enable)

    # Force refreshing if directory paths change.
    if submodules_del or submodules_add or submodules_rename_dirpath:
        _is_first_reset()


def _initialize_extensions_site_packages(*, extensions_directory, create=False):
    # Add extension site-packages to `sys.path` (if it exists).
    # Use for wheels.
    import os
    import sys

    # NOTE: follow the structure of `~/.local/lib/python#.##/site-packages`
    # because some wheels contain paths pointing to parent directories,
    # referencing `../../../bin` for example - to install binaries into `~/.local/bin`,
    # so this can't simply be treated as a module directory unless those files would be excluded
    # which may interfere with the wheels functionality.
    site_packages = os.path.join(
        extensions_directory,
        ".local",
        "lib",
        "python{:d}.{:d}".format(sys.version_info.major, sys.version_info.minor),
        "site-packages",
    )
    if create:
        if not os.path.exists(site_packages):
            os.makedirs(site_packages)
        found = True
    else:
        found = os.path.exists(site_packages)

    if found:
        # Ensure the wheels `site-packages` are added before all other site-packages.
        # This is important for extensions modules get priority over system modules.
        # Without this, installing a module into the systems site-packages (`/usr/lib/python#.##/site-packages`)
        # could break an extension which already had a different version of this module installed locally.
        from site import getsitepackages
        index = None
        if builtin_site_packages := set(getsitepackages()):
            for i, dirpath in enumerate(sys.path):
                if dirpath in builtin_site_packages:
                    index = i
                    break
        if index is None:
            sys.path.append(site_packages)
        else:
            sys.path.insert(index, site_packages)
    else:
        try:
            sys.path.remove(site_packages)
        except ValueError:
            pass

    return site_packages if found else None


def _initialize_extensions_repos_once():
    from _bpy_internal.extensions.junction_module import JunctionModuleHandle
    module_handle = JunctionModuleHandle(_ext_base_pkg_idname)
    module_handle.register_module()
    _ext_global.module_handle = module_handle

    extensions_directory = _bpy.utils.user_resource('EXTENSIONS')

    # Ensure extensions wheels can be loaded (when found).
    _initialize_extensions_site_packages(extensions_directory=extensions_directory)

    # Ensure extension compatibility data has been loaded and matches the manifests.
    _initialize_extensions_compat_data(
        extensions_directory,
        ensure_wheels=True,
        addon_modules_pending=None,
        use_startup_fastpath=True,
        # Runs on startup, fall back to printing.
        error_fn=None,
    )

    # Setup repositories for the first time.
    # Intentionally don't call `_initialize_extension_repos_pre` as this is the first time,
    # the previous state is not useful to read.
    _initialize_extension_repos_post(is_first=True)

    # Internal handlers intended for Blender's own handling of repositories.
    _bpy.app.handlers._extension_repos_update_pre.append(_initialize_extension_repos_pre)
    _bpy.app.handlers._extension_repos_update_post.append(_initialize_extension_repos_post)


# -----------------------------------------------------------------------------
# Extension Public API

def extensions_refresh(
        ensure_wheels=True,
        addon_modules_pending=None,
        handle_error=None,
):
    """
    Ensure data relating to extensions is up to date.
    This should be called after extensions on the file-system have changed.

    :arg ensure_wheels: When true, refresh installed wheels with wheels used by extensions.
    :type ensure_wheels: bool
    :arg addon_modules_pending: Refresh these add-ons by listing their package names, as if they are enabled.
       This is needed so wheels can be setup before the add-on is enabled.
    :type addon_modules_pending: Sequence[str] | None
    :arg handle_error: Called in the case of an error, taking an exception argument.
    :type handle_error: Callable[[Exception], None] | None
    """

    # Ensure any changes to extensions refresh `_extensions_incompatible`.
    _initialize_extensions_compat_data(
        _bpy.utils.user_resource('EXTENSIONS'),
        ensure_wheels=ensure_wheels,
        addon_modules_pending=addon_modules_pending,
        use_startup_fastpath=False,
        error_fn=handle_error,
    )


def _extensions_warnings_get():
    if _extensions_warnings_get._is_first is False:
        return _extensions_warnings

    # Calculate warnings which are shown in the UI but not calculated at load time
    # because this incurs some overhead.
    #
    # Currently this checks for scripts violating policies:
    # - Adding their directories or sub-directories to `sys.path`.
    # - Loading any bundled scripts as modules directly into `sys.modules`.
    #
    # These warnings are shown:
    # - In the add-on UI.
    # - In the extension UI.
    # - When listing extensions via `blender -c extension list`.

    import sys
    import os

    _extensions_warnings_get._is_first = False
    _extensions_warnings.clear()

    # This could be empty, it just avoid a lot of redundant lookups to skip known module paths.
    dirs_skip_expected = (
        os.path.normpath(os.path.join(os.path.dirname(_bpy.__file__), "..")) + os.sep,
        os.path.normpath(os.path.join(os.path.dirname(__import__("bl_ui").__file__), "..")) + os.sep,
        os.path.normpath(os.path.dirname(os.__file__)) + os.sep,
        # Legacy add add-on paths.
        *(os.path.normpath(path) + os.sep for path in paths()),
    )

    extensions_directory_map = {}
    modules_other = []

    for module_name, module in sys.modules.items():

        if module_name == "__main__":
            continue

        module_file = getattr(module, "__file__", None) or ""
        if not module_file:
            # In most cases these are PY-CAPI modules.
            continue

        module_file = os.path.normpath(module_file)

        if module_file.startswith(dirs_skip_expected):
            continue

        if module_name.startswith(_ext_base_pkg_idname_with_dot):
            # Check this is a sub-module (an extension).
            if module_name.find(".", len(_ext_base_pkg_idname_with_dot)) != -1:
                # Ignore extension sub-modules because there is no need to handle their directories.
                # The extensions directory accounts for any paths which may be found in the sub-modules path.
                if module_name.count(".") > 2:
                    continue
                extensions_directory_map[module_name] = os.path.dirname(module_file) + os.sep
        else:
            # Any non extension module.
            modules_other.append((module_name, module_file))

    dirs_extensions = tuple(path for path in extensions_directory_map.values())
    dirs_extensions_noslash = set(path.rstrip(os.sep) for path in dirs_extensions)
    if dirs_extensions:
        for module_other_name, module_other_file in modules_other:
            if not module_other_file.startswith(dirs_extensions):
                continue

            # Need 2x lookups, not ideal but `str.startswith` doesn't let us know which argument matched.
            found = False
            for module_name, module_dirpath in extensions_directory_map.items():
                if not module_other_file.startswith(module_dirpath):
                    continue
                try:
                    warning_list = _extensions_warnings[module_name]
                except KeyError:
                    warning_list = _extensions_warnings[module_name] = []
                warning_list.append("Policy violation with top level module: {:s}".format(module_other_name))
                found = True
                break
            assert found

        for path in sys.path:
            path = os.path.normpath(path)
            if path.startswith(dirs_skip_expected):
                continue

            if not (path in dirs_extensions_noslash or path.startswith(dirs_extensions)):
                continue

            found = False
            for module_name, module_dirpath in extensions_directory_map.items():
                if not (path == module_dirpath.rstrip(os.sep) or path.startswith(module_dirpath)):
                    continue
                try:
                    warning_list = _extensions_warnings[module_name]
                except KeyError:
                    warning_list = _extensions_warnings[module_name] = []
                # Use an extension relative path as an absolute path may be too verbose for the UI.
                warning_list.append(
                    "Policy violation with sys.path: {:s}".format(
                        ".{:s}{:s}".format(os.sep, os.path.relpath(path, module_dirpath))
                    )
                )
                found = True
                break
            assert found

    return _extensions_warnings


_extensions_warnings_get._is_first = True


def _is_first_reset():
    # Reset all values which are lazily initialized,
    # use this to force re-creating extension warnings and cached modules.
    _extensions_warnings_get._is_first = True
    modules._is_first = True


--- scripts/modules/_animsys_refactor.py ---
# SPDX-FileCopyrightText: 2010-2023 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

"""
This module has utility functions for renaming
rna values in fcurves and drivers.

Currently unused, but might become useful later again.
"""
__all__ = (
    "update_data_paths",
)

import sys

import bpy

IS_TESTING = False


def classes_recursive(base_type, clss=None):
    if clss is None:
        clss = [base_type]
    else:
        clss.append(base_type)

    for base_type_iter in base_type.__bases__:
        if base_type_iter is not object:
            classes_recursive(base_type_iter, clss)

    return clss


class DataPathBuilder:
    """Dummy class used to parse fcurve and driver data paths."""
    __slots__ = ("data_path", )

    def __init__(self, attrs):
        self.data_path = attrs

    def __getattr__(self, attr):
        str_value = ".{:s}".format(attr)
        return DataPathBuilder(self.data_path + (str_value, ))

    def __getitem__(self, key):
        if type(key) is int:
            str_value = '[{:d}]'.format(key)
        elif type(key) is str:
            str_value = '["{:s}"]'.format(bpy.utils.escape_identifier(key))
        else:
            raise Exception("unsupported accessor {!r} of type {!r} (internal error)".format(key, type(key)))
        return DataPathBuilder(self.data_path + (str_value, ))

    def resolve(self, real_base, rna_update_from_map, fcurve, log):
        """Return (attribute, value) pairs."""
        pairs = []
        base = real_base
        for item in self.data_path:
            if base is not Ellipsis:
                base_new = Ellipsis
                # find the new name
                if item.startswith("."):
                    for class_name, item_new, options in (
                            rna_update_from_map.get(item[1:], []) +
                            [(None, item[1:], None)]
                    ):
                        if callable(item_new):
                            # No type check here, callback is assumed to know what it's doing.
                            base_new, item_new = item_new(base, class_name, item[1:], fcurve, options)
                            if base_new is not Ellipsis:
                                break  # found, don't keep looking
                        else:
                            # Type check!
                            type_ok = True
                            if class_name is not None:
                                type_ok = False
                                for base_type in classes_recursive(type(base)):
                                    if base_type.__name__ == class_name:
                                        type_ok = True
                                        break
                            if type_ok:
                                try:
                                    # print("base." + item_new)
                                    base_new = eval("base." + item_new)
                                    break  # found, don't keep looking
                                except Exception:
                                    pass
                    item_new = "." + item_new
                else:
                    item_new = item
                    try:
                        base_new = eval("base" + item_new)
                    except Exception:
                        pass

                if base_new is Ellipsis:
                    print("Failed to resolve data path:", self.data_path, file=log)
                base = base_new
            else:
                item_new = item

            pairs.append((item_new, base))
        return pairs


def id_iter():
    from bpy.types import bpy_prop_collection
    assert isinstance(bpy.data.objects, bpy_prop_collection)

    for attr in dir(bpy.data):
        data_iter = getattr(bpy.data, attr, None)
        if isinstance(data_iter, bpy_prop_collection):
            for id_data in data_iter:
                if id_data.library is None:
                    yield id_data


def anim_data_actions(anim_data) -> list[tuple[bpy.types.Action, bpy.types.ActionSlot]]:
    actions = []
    actions.append((anim_data.action, anim_data.action_slot))
    for track in anim_data.nla_tracks:
        for strip in track.strips:
            actions.append((strip.action, strip.action_slot))

    # Filter out None actions/slots, because if either is None, there is no animation.
    return [(act, slot) for (act, slot) in actions if act and slot]


def find_path_new(id_data, data_path, rna_update_from_map, fcurve, log):
    # note!, id_data can be ID type or a node tree
    # ignore ID props for now
    if data_path.startswith("["):
        return data_path

    # recursive path fixing, likely will be one in most cases.
    data_path_builder = eval("DataPathBuilder(tuple())." + data_path)
    data_resolve = data_path_builder.resolve(id_data, rna_update_from_map, fcurve, log)

    path_new = [pair[0] for pair in data_resolve]

    return "".join(path_new)[1:]  # skip the first "."


def update_data_paths(rna_update, log=sys.stdout):
    """
    rna_update triple [(class_name, from, to or to_callback, callback options), ...]
    to_callback is a function with this signature: update_cb(base, class_name, old_path, fcurve, options)
                where base is current object, class_name is the expected type name of base (callback has to handle
                this), old_path it the org name of base's property, fcurve is the affected fcurve (!),
                and options is an opaque data.
                class_name, fcurve and options may be None!
    """
    from bpy_extras import anim_utils

    rna_update_from_map = {}
    for ren_class, ren_from, ren_to, options in rna_update:
        rna_update_from_map.setdefault(ren_from, []).append((ren_class, ren_to, options))

    for id_data in id_iter():
        anim_data_ls: list[tuple[bpy.types.ID, bpy.types.AnimData | None]] = [
            (id_data, getattr(id_data, "animation_data", None))]
        # check node-trees too
        node_tree = getattr(id_data, "node_tree", None)
        if node_tree:
            anim_data_ls.append((node_tree, node_tree.animation_data))

        for anim_data_base, anim_data in anim_data_ls:
            if anim_data is None:
                continue

            for fcurve in anim_data.drivers:
                data_path = fcurve.data_path
                data_path_new = find_path_new(anim_data_base, data_path, rna_update_from_map, fcurve, log)
                # print(data_path_new)
                if data_path_new != data_path:
                    if not IS_TESTING:
                        fcurve.data_path = data_path_new
                        fcurve.driver.is_valid = True  # reset to allow this to work again
                    print(
                        "driver-fcurve ({:s}): {:s} -> {:s}".format(id_data.name, data_path, data_path_new),
                        file=log,
                    )

                for var in fcurve.driver.variables:
                    if var.type == 'SINGLE_PROP':
                        for tar in var.targets:
                            id_data_other = tar.id
                            data_path = tar.data_path

                            if id_data_other and data_path:
                                data_path_new = find_path_new(id_data_other, data_path, rna_update_from_map, None, log)
                                # print(data_path_new)
                                if data_path_new != data_path:
                                    if not IS_TESTING:
                                        tar.data_path = data_path_new
                                    print(
                                        "driver ({:s}): {:s} -> {:s}".format(
                                            id_data_other.name,
                                            data_path,
                                            data_path_new,
                                        ),
                                        file=log,
                                    )

            for action, action_slot in anim_data_actions(anim_data):
                channelbag = anim_utils.action_get_channelbag_for_slot(action, action_slot)
                if not channelbag:
                    continue
                for fcu in channelbag.fcurves:
                    data_path = fcu.data_path
                    data_path_new = find_path_new(anim_data_base, data_path, rna_update_from_map, fcu, log)
                    # print(data_path_new)
                    if data_path_new != data_path:
                        if not IS_TESTING:
                            fcu.data_path = data_path_new
                        print("fcurve ({:s}): {:s} -> {:s}".format(id_data.name, data_path, data_path_new), file=log)


if __name__ == "__main__":

    # Example, should be called externally
    # (class, from, to or to_callback, callback_options)
    replace_ls = [
        ("AnimVizMotionPaths", "frame_after", "frame_after", None),
        ("AnimVizMotionPaths", "frame_before", "frame_before", None),
        ("AnimVizOnionSkinning", "frame_after", "frame_after", None),
    ]

    update_data_paths(replace_ls)


--- scripts/modules/bl_app_template_utils.py ---
# SPDX-FileCopyrightText: 2017-2023 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

"""
Similar to ``addon_utils``, except we can only have one active at a time.

In most cases users of this module will simply call 'activate'.
"""

__all__ = (
    "activate",
    "import_from_path",
    "import_from_id",
    "reset",
)

import bpy as _bpy

# Normally matches 'preferences.app_template_id',
# but loading new preferences will get us out of sync.
_app_template = {
    "id": "",
}

# Instead of `sys.modules`
# note that we only ever have one template enabled at a time
# so it may not seem necessary to use this.
#
# However, templates may want to share between each-other,
# so any loaded modules are stored here?
#
# Note that the ID here is the app_template_id , not the modules __name__.
_modules = {}


def _enable(template_id, *, handle_error=None, ignore_not_found=False):
    from _bpy_restrict_state import RestrictBlend

    if handle_error is None:
        def handle_error(_ex):
            import traceback
            traceback.print_exc()

    # Split registering up into 2 steps so we can undo
    # if it fails par way through.

    # disable the context, using the context at all is
    # really bad while loading an template, don't do it!
    with RestrictBlend():

        # 1) try import
        try:
            mod = import_from_id(template_id, ignore_not_found=ignore_not_found)
        except Exception as ex:
            handle_error(ex)
            return None

        _modules[template_id] = mod
        if mod is None:
            return None
        mod.__template_enabled__ = False

        # 2) try run the modules register function
        try:
            mod.register()
        except Exception as ex:
            print("Exception in module register(): {!r}".format(getattr(mod, "__file__", template_id)))
            handle_error(ex)
            del _modules[template_id]
            return None

    # * OK loaded successfully! *
    mod.__template_enabled__ = True

    if _bpy.app.debug_python:
        print("\tapp_template_utils.enable", mod.__name__)

    return mod


def _disable(template_id, *, handle_error=None):
    """
    Disables a template by name.

    :arg template_id: The name of the template and module.
    :type template_id: str
    :arg handle_error: Called in the case of an error,
       taking an exception argument.
    :type handle_error: Callable[[Exception], None] | None
    """

    if handle_error is None:
        def handle_error(_ex):
            import traceback
            traceback.print_exc()

    mod = _modules.get(template_id, False)

    if mod is None:
        # Loaded but has no module, remove since there is no use in keeping it.
        del _modules[template_id]
    elif getattr(mod, "__template_enabled__", False) is not False:
        mod.__template_enabled__ = False

        try:
            mod.unregister()
        except Exception as ex:
            print("Exception in module unregister(): {!r}".format(getattr(mod, "__file__", template_id)))
            handle_error(ex)
    else:
        print(
            "\tapp_template_utils.disable: {:s} not {:s}.".format(
                template_id,
                "disabled" if mod is False else "loaded",
            )
        )

    if _bpy.app.debug_python:
        print("\tapp_template_utils.disable", template_id)


def import_from_path(path, *, ignore_not_found=False):
    import os
    from importlib import import_module
    base_module, template_id = path.rsplit(os.sep, 2)[-2:]
    module_name = base_module + "." + template_id

    try:
        return import_module(module_name)
    except ModuleNotFoundError as ex:
        if ignore_not_found and ex.name == module_name:
            return None
        raise ex


def import_from_id(template_id, *, ignore_not_found=False):
    import os
    path = next(iter(_bpy.utils.app_template_paths(path=template_id)), None)
    if path is None:
        if ignore_not_found:
            return None
        else:
            raise Exception("{!r} template not found!".format(template_id))
    else:
        if ignore_not_found:
            if not os.path.exists(os.path.join(path, "__init__.py")):
                return None
        return import_from_path(path, ignore_not_found=ignore_not_found)


def activate(*, template_id=None, reload_scripts=False):
    template_id_prev = _app_template["id"]

    # not needed but may as well avoids redundant
    # disable/enable for all add-ons on "File -> New".
    if not reload_scripts and template_id_prev == template_id:
        return

    if template_id_prev:
        _disable(template_id_prev)

    # ignore_not_found so modules that don't contain scripts don't raise errors
    _mod = _enable(template_id, ignore_not_found=True) if template_id else None

    _app_template["id"] = template_id


def reset(*, reload_scripts=False):
    """
    Sets default state.
    """
    template_id = _bpy.context.preferences.app_template
    if _bpy.app.debug_python:
        print("bl_app_template_utils.reset('{:s}')".format(template_id))

    activate(template_id=template_id, reload_scripts=reload_scripts)


--- tests/python/bl_bundled_modules.py ---
# SPDX-FileCopyrightText: 2009-2023 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

# Test that modules we ship with our Python installation are available,
# both for Blender itself and the bundled Python executable.

import os
import subprocess
import sys

app = "Blender" if sys.argv[-1] == "--inside-blender" else "Python"
sys.stderr.write(f"Testing bundled modules in {app} executable.\n")

# General purpose modules.
import bz2
import certifi
import ctypes
import cython
import lzma
import numpy
import requests
import sqlite3
import ssl
import urllib3
import zlib
import zstandard
import cattrs
import fastjsonschema

# Dynamically loaded modules, to ensure they have satisfactory dependencies.
import _blake2

# VFX platform modules.
from pxr import Usd
import MaterialX
import OpenImageIO
import PyOpenColorIO

# Test both old and new names, remove when all 4.4 libs have landed.
try:
    import pyopenvdb
except ModuleNotFoundError:
    import openvdb
    import oslquery

# Test modules in bundled Python standalone executable.
if app == "Blender":
    script_filepath = os.path.abspath(__file__)
    proc = subprocess.Popen([sys.executable, script_filepath])
    sys.exit(proc.wait())


--- tests/python/bl_load_py_modules.py ---
# SPDX-FileCopyrightText: 2011-2022 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

# simple script to enable all addons, and disable

"""
./blender.bin --background --factory-startup --python tests/python/bl_load_py_modules.py
"""

import bpy
import addon_utils

import sys
import os

# Modules to exclude relative to their location in `sys.path`.
# The trailing components of the path are used instead of the module name.
# Both script files & directories are supported which prevents searching inside the directory.
EXCLUDE_MODULE_PATHS = {
    "_bl_i18n_utils",
    "_bl_previews_utils",
    "cycles",

    # These tests which run stand-alone and aren't imported as modules.
    os.path.join("bl_pkg", "tests"),
}

# Some modules need to add to the `sys.path`.
MODULE_SYS_PATHS = {
    # Runs in a Python subprocess, so its expected its basedir can be imported.
    "io_blend_utils.blendfile_pack": ".",
}

if not bpy.app.build_options.freestyle:
    EXCLUDE_MODULE_PATHS.add("render_freestyle_svg")

if not bpy.app.build_options.xr_openxr:
    EXCLUDE_MODULE_PATHS.add("viewport_vr_preview")

EXCLUDE_MODULE_DIRS = (
    os.path.join(bpy.utils.user_resource('SCRIPTS')),
) + tuple(addon_utils.paths()[1:])


def module_names_recursive(mod_dir, *, parent=None):
    """
    a version of bpy.path.module_names that includes non-packages
    """

    is_package = os.path.exists(os.path.join(mod_dir, "__init__.py"))

    for n in os.listdir(mod_dir):
        if not n.startswith((".", "_")):
            submod_full = os.path.join(mod_dir, n)
            if os.path.isdir(submod_full):
                if not parent:
                    subparent = n
                else:
                    subparent = parent + "." + n
                yield from module_names_recursive(submod_full, parent=subparent)
            elif n.endswith(".py") and is_package is False:
                submod = n[:-3]
                if parent:
                    submod = parent + "." + submod
                yield submod, submod_full


def module_names_all(mod_dir):
    yield from bpy.path.module_names(mod_dir)
    yield from module_names_recursive(mod_dir)


def source_list(path, filename_check=None):
    from os.path import join
    for dirpath, dirnames, filenames in os.walk(path):
        # skip '.git'
        dirnames[:] = [d for d in dirnames if not d.startswith(".")]

        for filename in filenames:
            filepath = join(dirpath, filename)
            if filename_check is None or filename_check(filepath):
                yield filepath


def load_modules():
    VERBOSE = os.environ.get("BLENDER_VERBOSE") is not None

    modules = []
    module_paths = []

    # paths blender stores scripts in.
    paths = bpy.utils.script_paths()

    print("Paths:")
    for script_path in paths:
        print("\t'%s'" % script_path)

    #
    # find all sys.path we added
    for script_path in paths:
        for mod_dir in sys.path:
            if mod_dir.startswith(script_path):
                if not mod_dir.startswith(EXCLUDE_MODULE_DIRS):
                    if mod_dir not in module_paths:
                        if os.path.exists(mod_dir):
                            module_paths.append(mod_dir)

    #
    # collect modules from our paths.
    module_names = {}
    for mod_dir in module_paths:
        # print("mod_dir", mod_dir)
        for mod, mod_full in bpy.path.module_names(mod_dir):
            if mod in EXCLUDE_MODULE_PATHS:
                continue
            if mod in module_names:
                mod_dir_prev, mod_full_prev = module_names[mod]
                raise Exception("Module found twice %r.\n    (%r -> %r, %r -> %r)" %
                                (mod, mod_dir, mod_full, mod_dir_prev, mod_full_prev))

            modules.append(__import__(mod))

            module_names[mod] = mod_dir, mod_full
    del module_names

    #
    # test we tested all files except for presets and templates
    ignore_paths = [
        os.sep + "presets" + os.sep,
        os.sep + "templates_osl" + os.sep,
        os.sep + "templates_py" + os.sep,
        os.sep + "bl_app_templates_system" + os.sep,
    ] + ([(os.sep + f + os.sep) for f in EXCLUDE_MODULE_PATHS] +
         [(os.sep + f + ".py") for f in EXCLUDE_MODULE_PATHS])

    #
    # now submodules
    for m in modules:
        filepath = m.__file__
        if os.path.basename(filepath).startswith("__init__."):
            mod_dir = os.path.dirname(filepath)
            for submod, submod_full in module_names_all(mod_dir):
                # fromlist is ignored, ugh.
                mod_name_full = m.__name__ + "." + submod

                sys_path_back = sys.path[:]

                sys.path.extend([
                    os.path.normpath(os.path.join(mod_dir, f))
                    for f in MODULE_SYS_PATHS.get(mod_name_full, ())
                ])

                try:
                    __import__(mod_name_full)
                    mod_imp = sys.modules[mod_name_full]

                    sys.path[:] = sys_path_back

                    # check we load what we ask for.
                    assert os.path.samefile(mod_imp.__file__, submod_full)

                    modules.append(mod_imp)
                except Exception:
                    import traceback
                    # Module might fail to import, but we don't want whole test to fail here.
                    # Reasoning:
                    # - This module might be in ignored list (for example, preset or template),
                    #   so failing here will cause false-positive test failure.
                    # - If this is module which should not be ignored, it is not added to list
                    #   of successfully loaded modules, meaning the test will catch this
                    #   import failure.
                    # - We want to catch all failures of this script instead of stopping on
                    #   a first big failure.
                    do_print = True
                    if not VERBOSE:
                        for ignore in ignore_paths:
                            if ignore in submod_full:
                                do_print = False
                                break
                    if do_print:
                        traceback.print_exc()

    #
    # check which filepaths we didn't load
    source_files = []
    for mod_dir in module_paths:
        source_files.extend(source_list(mod_dir, filename_check=lambda f: f.endswith(".py")))

    source_files = list(set(source_files))
    source_files.sort()

    #
    # remove loaded files
    loaded_files = list({m.__file__ for m in modules})
    loaded_files.sort()

    for f in loaded_files:
        source_files.remove(f)

    for f in source_files:
        for ignore in ignore_paths:
            if ignore in f:
                break
        else:
            raise Exception("Source file %r not loaded in test" % f)

    print("loaded %d modules" % len(loaded_files))


def main():
    load_modules()


if __name__ == "__main__":
    main()


--- tests/python/bl_pyapi_bmesh.py ---
# SPDX-FileCopyrightText: 2025 Blender Authors
#
# SPDX-License-Identifier: Apache-2.0

# ./blender.bin --background --python tests/python/bl_pyapi_bmesh.py -- --verbose

__all__ = (
    "main",
)

import bmesh
import unittest


# ------------------------------------------------------------------------------
# Internal Utilities

def save_to_blend_file_for_testing(bm):
    """
    Useful for inspecting test data.
    """
    import bpy
    from bpy import context

    bpy.ops.wm.read_factory_settings(use_empty=True)
    me = bpy.data.meshes.new("test output")
    bm.to_mesh(me)
    ob = bpy.data.objects.new("", me)

    view_layer = context.view_layer
    layer_collection = context.layer_collection or view_layer.active_layer_collection
    scene_collection = layer_collection.collection

    scene_collection.objects.link(ob)
    ob.select_set(True)
    view_layer.objects.active = ob

    # Write to the $CWD.
    bpy.ops.wm.save_as_mainfile(filepath="bl_pyapi_bmesh.blend")


# ------------------------------------------------------------------------------
# Basic Tests

class TestBMeshBasic(unittest.TestCase):

    def test_create_uvsphere(self):
        bm = bmesh.new()
        bmesh.ops.create_uvsphere(
            bm,
            u_segments=8,
            v_segments=5,
            radius=1.0,
        )

        self.assertEqual(len(bm.verts), 34)
        self.assertEqual(len(bm.edges), 72)
        self.assertEqual(len(bm.faces), 40)

        bm.free()


# ------------------------------------------------------------------------------
# UV Selection

def bm_uv_select_check_or_empty(
        bm,
        sync=False,
        flush=False,
        contiguous=False,
):
    return bmesh.utils.uv_select_check(
        bm,
        sync=sync,
        flush=flush,
        contiguous=contiguous,
    ) or {}


def bm_uv_select_check_non_zero(
        bm, /, *,
        sync=False,
        flush=False,
        contiguous=False,
):
    """
    Remove all zero keys, so it's convenient to isolate failures.
    """
    result = bmesh.utils.uv_select_check(
        bm,
        sync=sync,
        flush=flush,
        contiguous=contiguous,
    )
    if result is not None:
        return {key: value for key, value in result.items() if value != 0}
    return {}


def bm_uv_select_set_all(bm, /, *, select):
    for f in bm.faces:
        f.uv_select = select
        for l in f.loops:
            l.uv_select_vert = select
            l.uv_select_edge = select


def bm_uv_layer_from_coords(bm, uv_layer):
    for face in bm.faces:
        for loop in face.loops:
            loop_uv = loop[uv_layer]
            # Use XY position of the vertex as a uv coordinate.
            loop_uv.uv = loop.vert.co.xy


def bm_loop_select_count_vert_edge_face(bm):
    """
    Return a tuple of UV selection counts (vert, edge, face).
    Use for tests.
    """
    mesh_vert = 0
    mesh_edge = 0
    mesh_face = 0

    uv_vert = 0
    uv_edge = 0
    uv_face = 0

    for v in bm.verts:
        if v.hide:
            continue
        if v.select:
            mesh_vert += 1
    for e in bm.edges:
        if e.hide:
            continue
        if e.select:
            mesh_edge += 1

    for f in bm.faces:
        if f.hide:
            continue
        if f.select:
            mesh_face += 1

        if f.uv_select:
            uv_face += 1
        for l in f.loops:
            if l.uv_select_vert:
                uv_vert += 1
            if l.uv_select_edge:
                uv_edge += 1

    return (uv_vert, uv_edge, uv_face), (mesh_vert, mesh_edge, mesh_face)


def bm_uv_select_reset(bm, /, *, select):
    bm_uv_select_set_all(bm, select=select)
    bm.uv_select_sync_to_mesh()


class TestBMeshUVSelectSimple(unittest.TestCase):

    def test_uv_grid(self):
        bm = bmesh.new()
        bmesh.ops.create_grid(
            bm,
            x_segments=3,
            y_segments=4,
            size=1.0,
        )
        self.assertEqual(len(bm.verts), 20)
        self.assertEqual(len(bm.edges), 31)
        self.assertEqual(len(bm.faces), 12)

        # Nothing selected.
        bm.uv_select_sync_valid = True
        self.assertEqual(bm_uv_select_check_or_empty(bm, sync=True), {})

        # All verts selected, no UV's selected.
        for v in bm.verts:
            v.select = True

        bm.uv_select_sync_valid = True
        self.assertEqual(bm_uv_select_check_or_empty(bm, sync=True).get(
            "count_uv_vert_none_selected_with_vert_selected", 0), 20)

        # No verts selected, all UV's selected.
        for v in bm.verts:
            v.select = False
        for f in bm.faces:
            for l in f.loops:
                l.uv_select_vert = True

        bm.uv_select_sync_valid = True
        self.assertTrue(
            bm_uv_select_check_or_empty(bm, sync=True).get("count_uv_vert_any_selected_with_vert_unselected", 0),
            48)

        bm.free()

    def test_uv_contiguous_verts(self):
        from mathutils import Vector
        bm = bmesh.new()
        bmesh.ops.create_grid(bm, x_segments=2, y_segments=2, size=1.0)
        self.assertEqual((len(bm.verts), len(bm.edges), len(bm.faces)), (9, 12, 4))

        faces = list(bm.faces)

        # Sort faces so the order is always predictable.
        vector_dot = Vector((0.95, 0.05, 0.0))
        faces.sort(key=lambda f: vector_dot.dot(f.calc_center_median()))

        # Checker de-select UV's, each face has an isolated selection.
        for i, f in enumerate(faces):
            do_select = bool(i % 2)
            for l in f.loops:
                l.uv_select_vert = do_select
                l.uv_select_edge = do_select

        bm.uv_select_sync_valid = True
        result = bm_uv_select_check_or_empty(bm, sync=True, flush=True, contiguous=False)
        self.assertTrue(result.get("count_uv_vert_any_selected_with_vert_unselected", 0), 48)

        bm.free()

    def test_uv_select_flush_mode(self):
        bm = bmesh.new()

        # Do a NOP empty mesh check.
        bm.uv_select_sync_valid = True
        bm.uv_select_flush_mode()
        bm.uv_select_sync_to_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((0, 0, 0), (0, 0, 0)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True), {})

        bmesh.ops.create_grid(bm, x_segments=3, y_segments=3, size=1.0)
        # Needed for methods that act on UV select.
        bm.uv_select_sync_valid = True

        # Do a NOP.
        bm.uv_select_flush_mode()
        bm.uv_select_sync_to_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((0, 0, 0), (0, 0, 0)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True), {})

        # Simple tests that selects all elements in a mode: `VERT`.
        bm.select_mode = {'VERT'}
        bm_uv_select_set_all(bm, select=False)
        # Select only verts.
        for f in bm.faces:
            for l in f.loops:
                l.uv_select_vert = True
        bm.uv_select_flush_mode()
        bm.uv_select_sync_to_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((36, 36, 9), (16, 24, 9)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True), {})

        # Simple tests that selects all elements in a mode: `EDGE`.
        bm.select_mode = {'EDGE'}
        bm_uv_select_set_all(bm, select=False)
        # Select only edges..
        for f in bm.faces:
            for l in f.loops:
                l.uv_select_edge_set(True)
        bm.uv_select_flush_mode()
        bm.uv_select_sync_to_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((36, 36, 9), (16, 24, 9)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True), {})

        # Simple tests that selects all elements in a mode: `FACE`.
        bm.select_mode = {'FACE'}
        bm_uv_select_set_all(bm, select=False)
        # Select only faces.
        for f in bm.faces:
            f.uv_select_set(True)
        bm.uv_select_flush_mode()
        bm.uv_select_sync_to_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((36, 36, 9), (16, 24, 9)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True), {})

        # TODO: Complex mixed selection.

    def test_uv_select_flush(self):
        from mathutils import Vector
        bm = bmesh.new()

        # Do a NOP empty mesh check.
        bm.uv_select_sync_valid = True
        bm.uv_select_flush(True)
        bm.uv_select_sync_to_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((0, 0, 0), (0, 0, 0)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True), {})

        bmesh.ops.create_grid(bm, x_segments=3, y_segments=3, size=1.0)
        # Needed for methods that act on UV select.
        bm.uv_select_sync_valid = True
        self.assertEqual((len(bm.verts), len(bm.edges), len(bm.faces)), (16, 24, 9))

        # Do a NOP check.
        bm.uv_select_flush(True)
        bm.uv_select_sync_to_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((0, 0, 0), (0, 0, 0)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True), {})

        faces = list(bm.faces)

        # Sort faces so the order is always predictable.
        vector_dot = Vector((0.95, 0.05, 0.0))
        faces.sort(key=lambda f: vector_dot.dot(f.calc_center_median()))

        f_center = faces[len(faces) // 2]
        self.assertEqual(f_center.calc_center_median().to_tuple(6), (0.0, 0.0, 0.0))

        uv_layer = bm.loops.layers.uv.new()
        bm_uv_layer_from_coords(bm, uv_layer)

        # Select 4 vertices.
        for l in f_center.loops:
            l.uv_select_vert = True
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((4, 0, 0), (0, 0, 0)))
        # Check.
        self.assertEqual(
            bm_uv_select_check_non_zero(bm, sync=True, flush=True),
            {
                "count_uv_edge_unselected_with_all_verts_selected": 4,
                "count_uv_face_unselected_with_all_verts_selected": 1,
                "count_uv_vert_any_selected_with_vert_unselected": 4,
            },
        )

        bm.uv_select_flush(True)
        bm.uv_select_sync_to_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((4, 4, 1), (4, 4, 1)))
        self.assertEqual(
            bm_uv_select_check_non_zero(bm, sync=True, flush=True, contiguous=True),
            # Not actually an error as the UV's have intentionally been selected in isolation.
            {
                "count_uv_vert_non_contiguous_selected": 5,
            },
        )

        # De-select those 4, ensure the selection remains false afterwards.
        for l in f_center.loops:
            l.uv_select_vert = False

        bm.uv_select_flush(False)
        bm.uv_select_sync_to_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((0, 0, 0), (0, 0, 0)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True), {})
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True, contiguous=True), {})

        # Select a single faces UV's bottom left hand corner (as well as adjacent UV's).
        for f in faces:
            for l in f.loops:
                xy = l.vert.co.xy[:]
                if xy[0] > 0.0 or xy[1] > 0.0:
                    continue
                l.uv_select_vert = True

        bm.uv_select_flush(True)
        bm.uv_select_sync_to_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((9, 6, 1), (4, 4, 1)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True, contiguous=True), {})

        # Ensure flushing de-selection does nothing when there is nothing to do.
        bm.uv_select_flush(False)
        bm.uv_select_sync_to_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((9, 6, 1), (4, 4, 1)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True), {})

        self.assertTrue(bm.uv_select_sync_valid)

        bm.free()

    def test_uv_select_sync_from_mesh(self):
        bm = bmesh.new()
        uv_layer = bm.loops.layers.uv.new()
        del uv_layer

        # Do a NOP empty mesh check.
        bm.select_flush(True)
        bm.uv_select_sync_from_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((0, 0, 0), (0, 0, 0)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True), {})

        bmesh.ops.create_grid(bm, x_segments=4, y_segments=4, size=2.0)
        # Needed for methods that act on UV select.
        bm.uv_select_sync_valid = True

        # Deselect all verts and flush back to the mesh.
        for v in bm.verts:
            v.select = False
        bm.select_flush(True)
        bm.uv_select_sync_from_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((0, 0, 0), (0, 0, 0)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True), {})

        # Select all verts and flush back to the mesh.
        for v in bm.verts:
            v.select = True
        bm.select_flush(True)
        bm.uv_select_sync_from_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((64, 64, 16), (25, 40, 16)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True), {})

        # TODO: Complex mixed selection.

    def test_uv_select_sync_to_mesh(self):
        # Even though this is called in other tests,
        # perform some additional checks here such as checking hide is respected.

        bm = bmesh.new()
        uv_layer = bm.loops.layers.uv.new()
        del uv_layer

        # Do a NOP empty mesh check.
        bm.select_flush(True)
        bm.uv_select_sync_from_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((0, 0, 0), (0, 0, 0)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True), {})

        bmesh.ops.create_grid(bm, x_segments=4, y_segments=4, size=2.0)
        # Needed for methods that act on UV select.
        bm.uv_select_sync_valid = True

        # Select a single faces UV's bottom left hand corner (as well as adjacent UV's).
        for f in bm.faces:
            for l in f.loops:
                l.uv_select_vert = True

        bm.uv_select_flush(True)
        bm.uv_select_sync_to_mesh()

        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((64, 64, 16), (25, 40, 16)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True), {})

        # Hide all geometry, then check syncing doesn't select them.
        for v in bm.verts:
            v.hide = True
        for e in bm.edges:
            e.hide = True
        for f in bm.faces:
            f.hide = True

        bm.uv_select_flush(True)
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((0, 0, 0), (0, 0, 0)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True), {})

        bm.uv_select_sync_to_mesh()
        # Nothing should be selected because the mesh is hidden.
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((0, 0, 0), (0, 0, 0)))

    def test_uv_select_foreach_set(self):
        # Select UV's directly, similar to selecting in the UV editor.
        bm = bmesh.new()
        uv_layer = bm.loops.layers.uv.new()
        bm.uv_select_sync_valid = True

        # Do a NOP empty mesh check.
        bm.uv_select_foreach_set(True)

        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((0, 0, 0), (0, 0, 0)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True), {})

        # Do a NOP empty mesh check with empty arguments.
        bm.uv_select_foreach_set(True, loop_verts=[], loop_edges=[], faces=[])

        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((0, 0, 0), (0, 0, 0)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True), {})

        # Use 3 segments to avoid central vertices (simplifies above/below tests when picking half the mesh).
        bmesh.ops.create_grid(bm, x_segments=3, y_segments=3, size=2.0)
        bm_uv_layer_from_coords(bm, uv_layer)

        # Select all vertices with X below 0.0.
        verts_x_pos = []
        verts_x_neg = []
        for v in bm.verts:
            (verts_x_pos if v.co.x > 0.0 else verts_x_neg).append(v)
        self.assertEqual((len(verts_x_neg), len(verts_x_pos)), (8, 8))

        verts_x_pos_as_set = set(verts_x_pos)

        # Other elements from the verts (to pass to selection).
        faces_x_pos = [
            f for f in bm.faces
            # Find the loop that spans positive edges.
            if len(set(l.vert for l in f.loops) & verts_x_pos_as_set) == 4
        ]
        self.assertEqual(len(faces_x_pos), 3)

        loop_edges_x_pos = [
            l for f in faces_x_pos
            for l in f.loops
        ]
        self.assertEqual(len(loop_edges_x_pos), 12)

        loop_verts_x_pos = [next(iter(v.link_loops)) for v in verts_x_pos]
        self.assertEqual(len(loop_verts_x_pos), 8)

        # NOTE: regarding allowing `count_uv_vert_non_contiguous_selected` when de-selecting edges & faces.
        # This occurs because of `uv_select_flush_mode` which doesn't take `contiguous` UV's into account.

        # ---------------
        # Select by Verts
        bm_uv_select_reset(bm, select=False)
        bm.uv_select_foreach_set(True, loop_verts=loop_verts_x_pos)
        bm.uv_select_flush(True)
        bm.uv_select_sync_to_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((18, 15, 3), (8, 10, 3)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True, contiguous=True), {})
        # ------------------
        # De-Select by Verts
        bm_uv_select_reset(bm, select=True)
        bm.uv_select_foreach_set(False, loop_verts=loop_verts_x_pos)
        bm.uv_select_flush(False)
        bm.uv_select_sync_to_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((18, 15, 3), (8, 10, 3)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True, contiguous=True), {})

        # ---------------
        # Select by Edges
        bm.select_mode = {'EDGE'}
        bm_uv_select_reset(bm, select=False)
        bm.uv_select_foreach_set(True, loop_edges=loop_edges_x_pos)
        bm.uv_select_flush_mode(flush_down=True)
        bm.uv_select_sync_to_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((18, 15, 3), (8, 10, 3)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True, contiguous=True), {})

        # ------------------
        # De-Select by Edges
        bm_uv_select_reset(bm, select=True)
        bm.uv_select_foreach_set(False, loop_edges=loop_edges_x_pos)
        bm.uv_select_flush_mode(flush_down=True)
        bm.uv_select_sync_to_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((24, 21, 3), (12, 14, 3)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True, contiguous=True), {
            "count_uv_vert_non_contiguous_selected": 4,  # Not an error, to be expected.
        })

        # ---------------
        # Select by Faces
        bm.select_mode = {'FACE'}
        bm_uv_select_reset(bm, select=False)
        bm.uv_select_foreach_set(True, faces=faces_x_pos)
        bm.uv_select_flush_mode(flush_down=True)
        bm.uv_select_sync_to_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((12, 12, 3), (8, 10, 3)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True, contiguous=True), {
            "count_uv_vert_non_contiguous_selected": 4,  # Not an error, to be expected.
        })

        # ------------------
        # De-Select by Faces
        bm_uv_select_reset(bm, select=True)
        bm.uv_select_foreach_set(False, faces=faces_x_pos)
        bm.uv_select_flush_mode(flush_down=True)
        bm.uv_select_sync_to_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((24, 24, 6), (12, 17, 6)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True, contiguous=True), {
            "count_uv_vert_non_contiguous_selected": 4,  # Not an error, to be expected.
        })

        # save_to_blend_file_for_testing(bm)

    def test_uv_select_foreach_set_from_mesh(self):
        """
        Select mesh elements, similar to selecting in the viewport,
        which is then flushed to the UV editor.
        """
        # Select geometry directly, similar to selecting in the 3D viewport.
        bm = bmesh.new()
        uv_layer = bm.loops.layers.uv.new()
        bm.uv_select_sync_valid = True

        # Do a NOP empty mesh check.
        bm.uv_select_foreach_set_from_mesh(True)

        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((0, 0, 0), (0, 0, 0)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True, contiguous=True), {})

        # Do a NOP empty mesh check with empty arguments.
        bm.uv_select_foreach_set_from_mesh(True, verts=[], edges=[], faces=[])

        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((0, 0, 0), (0, 0, 0)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True, contiguous=True), {})

        # Use 3 segments to avoid central vertices (simplifies above/below tests when picking half the mesh).
        bmesh.ops.create_grid(bm, x_segments=3, y_segments=3, size=2.0)
        bm_uv_layer_from_coords(bm, uv_layer)

        # Select all vertices with X below 0.0.
        verts_x_pos = []
        verts_x_neg = []
        for v in bm.verts:
            (verts_x_pos if v.co.x > 0.0 else verts_x_neg).append(v)
        self.assertEqual((len(verts_x_neg), len(verts_x_pos)), (8, 8))

        verts_x_pos_as_set = set(verts_x_pos)

        # Other elements from the verts (to pass to selection).
        faces_x_pos = [
            f for f in bm.faces
            # Find the loop that spans positive edges.
            if len(set(l.vert for l in f.loops) & verts_x_pos_as_set) == 4
        ]
        self.assertEqual(len(faces_x_pos), 3)

        edges_x_pos = [
            e for e in bm.edges
            if len(set(e.verts) & verts_x_pos_as_set) == 2
        ]
        self.assertEqual(len(edges_x_pos), 10)

        loop_verts_x_pos = [next(iter(v.link_loops)) for v in verts_x_pos]
        self.assertEqual(len(loop_verts_x_pos), 8)

        # NOTE: regarding allowing `count_uv_vert_non_contiguous_selected` when de-selecting edges & faces.
        # This occurs because of `uv_select_flush_mode` which doesn't take `contiguous` UV's into account.

        # ---------------
        # Select by Verts
        bm_uv_select_reset(bm, select=False)
        bm.uv_select_foreach_set_from_mesh(True, verts=verts_x_pos)
        bm.uv_select_flush(True)
        bm.uv_select_sync_to_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((18, 15, 3), (8, 10, 3)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True, contiguous=True), {})
        # ------------------
        # De-Select by Verts
        bm_uv_select_reset(bm, select=True)
        bm.uv_select_foreach_set_from_mesh(False, verts=verts_x_pos)
        bm.uv_select_flush(False)
        bm.uv_select_sync_to_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((18, 15, 3), (8, 10, 3)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True, contiguous=True), {})

        # ---------------
        # Select by Edges
        bm.select_mode = {'EDGE'}
        bm_uv_select_reset(bm, select=False)
        bm.uv_select_foreach_set_from_mesh(True, edges=edges_x_pos)
        bm.uv_select_flush_mode(flush_down=True)
        bm.uv_select_sync_to_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((18, 15, 3), (8, 10, 3)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True, contiguous=True), {})

        # ------------------
        # De-Select by Edges
        bm_uv_select_reset(bm, select=True)
        bm.uv_select_foreach_set_from_mesh(False, edges=edges_x_pos)
        bm.uv_select_flush_mode(flush_down=True)
        bm.uv_select_sync_to_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((24, 21, 3), (12, 14, 3)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True, contiguous=True), {
            "count_uv_vert_non_contiguous_selected": 4,  # Not an error, to be expected.
        })

        # ---------------
        # Select by Faces
        bm.select_mode = {'FACE'}
        bm_uv_select_reset(bm, select=False)
        bm.uv_select_foreach_set_from_mesh(True, faces=faces_x_pos)
        bm.uv_select_flush_mode(flush_down=True)
        bm.uv_select_sync_to_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((12, 12, 3), (8, 10, 3)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True, contiguous=True), {
            "count_uv_vert_non_contiguous_selected": 4,  # Not an error, to be expected.
        })

        # ------------------
        # De-Select by Faces
        bm_uv_select_reset(bm, select=True)
        bm.uv_select_foreach_set_from_mesh(False, faces=faces_x_pos)
        bm.uv_select_flush_mode(flush_down=True)
        bm.uv_select_sync_to_mesh()
        self.assertEqual(bm_loop_select_count_vert_edge_face(bm), ((24, 24, 6), (12, 17, 6)))
        self.assertEqual(bm_uv_select_check_non_zero(bm, sync=True, flush=True, contiguous=True), {
            "count_uv_vert_non_contiguous_selected": 4,  # Not an error, to be expected.
        })

        # save_to_blend_file_for_testing(bm)


def main():
    import sys
    sys.argv = [__file__] + (sys.argv[sys.argv.index("--") + 1:] if "--" in sys.argv else [])
    unittest.main()


if __name__ == "__main__":
    main()


--- tests/python/bl_pyapi_bpy_app.py ---
# SPDX-FileCopyrightText: 2026 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

# blender -b -P tests/python/bl_pyapi_bpy_app.py -- --verbose

__all__ = (
    "main",
)

import os.path
import sys
import unittest

import bpy


class AppCachedirTest(unittest.TestCase):
    def test_app_cachedir(self) -> None:
        match sys.platform:
            case 'darwin':
                expect = '$HOME/Library/Caches/Blender/'
            case 'win32':
                expect = '%USERPROFILE%\\AppData\\Local\\Blender Foundation\\Blender\\Cache\\'
            case _:  # Linux or other POSIX-ish system.
                expect = '$HOME/.cache/blender/'
        expect = os.path.expandvars(expect)

        self.assertEqual(expect, bpy.app.cachedir)


def main():
    import sys
    sys.argv = [__file__] + (sys.argv[sys.argv.index("--") + 1:] if "--" in sys.argv else [])
    unittest.main()


if __name__ == '__main__':
    main()


--- tests/python/bl_pyapi_bpy_app_tempdir.py ---
# SPDX-FileCopyrightText: 2025 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

# ./blender.bin --background --factory-startup --python tests/python/bl_pyapi_bpy_app_tempdir.py -- --verbose

# NOTE(ideasman42):
#
# - Creating a directory without write permissions across all supported platforms
#   is it not trivial, for the purpose of these tests simply create a file as a way of
#   pointing to a temporary path which can't have sub-directories created under it.
#
# - Internally, when no temporary path is set the hard coded path `/tmp/` is used.
#   For the purpose of the tests being complete - control over this path would be needed too.
#
# - When changing the temp directory & on exit
#   Blender's *session* temp directory is recursively removed.
#
#   These tests were added as part of a fix for a serious flaw (see #144042)
#   which would recursively delete the users `C:\` since the tests aren't sand-boxed
#   all of the following tests reference paths within a newly creating temporary directory
#   instead of referencing root paths to avoid risks for developers who run tests.
#
# - A related test exists `BLI_tempfile_test.cc` however this doesn't deal with
#   Blender's user preferences and fallbacks used in Blender.
#

__all__ = (
    "main",
)

import os
import unittest
import tempfile


is_win32 = os.name == "nt"
TEMP_ENV = "TEMP" if is_win32 else "TMPDIR"


def system_temp_set(path: str) -> None:
    os.environ[TEMP_ENV] = path


def prefs_temp_set(path: str) -> None:
    import bpy  # type: ignore
    bpy.context.preferences.filepaths.temporary_directory = path


def prefs_temp_get() -> str:
    import bpy
    result = bpy.context.preferences.filepaths.temporary_directory
    assert isinstance(result, str)
    return result


def blender_tempdir_session_get() -> str:
    import bpy
    result = bpy.app.tempdir
    assert isinstance(result, str)
    return result


def empty_file(path: str) -> None:
    with open(path, 'wb') as _fh:
        pass


def commonpath_safe(paths: list[str]) -> str:
    if is_win32:
        try:
            return os.path.commonpath(paths)
        except ValueError:
            # Workaround error on Windows.
            # ValueError: Paths don't have the same drive
            return ""

    return os.path.commonpath(paths)


class TestTempDir(unittest.TestCase):

    def setUp(self) -> None:
        print(prefs_temp_get())
        assert prefs_temp_get() == ""
        system_temp_set("")
        prefs_temp_set("")

    def tearDown(self) -> None:
        prefs_temp_set("")
        system_temp_set("")

    def test_fallback(self) -> None:
        # Set a file for preferences & system temp, ensure neither are used.
        with tempfile.TemporaryDirectory() as tempdir:

            temp_sys = os.path.join(tempdir, "a_sys")
            temp_bpy = os.path.join(tempdir, "b_bpy")

            empty_file(temp_sys)
            empty_file(temp_bpy)

            system_temp_set(temp_sys)
            prefs_temp_set(temp_bpy)

            temp_session = blender_tempdir_session_get()

            # Ensure neither are used.
            # This will try to use `/tmp/`.
            commonpath_test = commonpath_safe([temp_sys, temp_session])
            self.assertFalse(commonpath_test and os.path.samefile(temp_sys, commonpath_test))
            commonpath_test = commonpath_safe([temp_bpy, temp_session])
            self.assertFalse(commonpath_test and os.path.samefile(temp_bpy, commonpath_test))

    def test_system(self) -> None:
        # Set an file as the preferences temp directory, ensure the system path is used.
        with tempfile.TemporaryDirectory() as tempdir:

            temp_sys = os.path.join(tempdir, "a_sys")
            temp_bpy = os.path.join(tempdir, "b_bpy")

            os.mkdir(temp_sys)
            empty_file(temp_bpy)

            system_temp_set(temp_sys)
            prefs_temp_set(temp_bpy)

            temp_session = blender_tempdir_session_get()

            self.assertTrue(os.path.samefile(temp_sys, os.path.commonpath([temp_sys, temp_session])))

    def test_prefs(self) -> None:
        # Set an file as the system temp directory, ensure the preferences path is used.
        with tempfile.TemporaryDirectory() as tempdir:

            temp_sys = os.path.join(tempdir, "a_sys")
            temp_bpy = os.path.join(tempdir, "b_bpy")

            empty_file(temp_sys)
            os.mkdir(temp_bpy)

            system_temp_set(temp_sys)
            prefs_temp_set(temp_bpy)

            temp_session = blender_tempdir_session_get()

            self.assertTrue(os.path.samefile(temp_bpy, os.path.commonpath([temp_bpy, temp_session])))

    def test_system_to_prefs(self) -> None:
        with tempfile.TemporaryDirectory() as tempdir:
            temp_sys = os.path.join(tempdir, "a_sys")
            temp_bpy = os.path.join(tempdir, "b_bpy")

            os.mkdir(temp_sys)
            empty_file(temp_bpy)

            system_temp_set(temp_sys)
            prefs_temp_set(temp_bpy)

            temp_session = blender_tempdir_session_get()
            self.assertTrue(os.path.samefile(temp_sys, os.path.commonpath([temp_sys, temp_session])))

            # Now set the preferences and ensure the previous directory gets purged and the new one set.
            os.unlink(temp_bpy)
            os.mkdir(temp_bpy)

            # Ensure the preferences path is now used.
            prefs_temp_set(temp_bpy)

            self.assertFalse(os.path.exists(temp_session))
            temp_session = blender_tempdir_session_get()
            self.assertTrue(os.path.samefile(temp_bpy, os.path.commonpath([temp_bpy, temp_session])))

    def test_prefs_to_system(self) -> None:
        with tempfile.TemporaryDirectory() as tempdir:
            temp_sys = os.path.join(tempdir, "a_sys")
            temp_bpy = os.path.join(tempdir, "b_bpy")

            empty_file(temp_sys)
            os.mkdir(temp_bpy)

            system_temp_set(temp_sys)
            prefs_temp_set(temp_bpy)

            temp_session = blender_tempdir_session_get()
            self.assertTrue(os.path.samefile(temp_bpy, os.path.commonpath([temp_bpy, temp_session])))

            # Now set the preferences and ensure the previous directory gets purged and the new one set.
            os.unlink(temp_sys)
            os.mkdir(temp_sys)

            # Ensure the system path is now used.
            prefs_temp_set(temp_sys)

            self.assertFalse(os.path.exists(temp_session))
            temp_session = blender_tempdir_session_get()
            self.assertTrue(os.path.samefile(temp_sys, os.path.commonpath([temp_sys, temp_session])))


def main():
    import sys
    unittest.main(argv=[__file__] + (sys.argv[sys.argv.index("--") + 1:] if "--" in sys.argv else []))


if __name__ == "__main__":
    main()


--- tests/python/bl_pyapi_bpy_driver_secure_eval.py ---
# SPDX-FileCopyrightText: 2022-2023 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

# ./blender.bin --background --python tests/python/bl_pyapi_bpy_driver_secure_eval.py -- --verbose
import bpy
import unittest
import builtins


# -----------------------------------------------------------------------------
# Mock Environment


expect_unreachable_msg = "This function should _NEVER_ run!"
# Internal check, to ensure this actually runs as expected.
expect_unreachable_count = 0


def expect_os_unreachable():
    global expect_unreachable_count
    expect_unreachable_count += 1
    raise Exception(expect_unreachable_msg)


__import__("os").expect_os_unreachable = expect_os_unreachable


expect_open_unreachable_count = 0


def open_expect_unreachable(*args, **kwargs):
    global expect_open_unreachable_count
    expect_open_unreachable_count += 1
    raise Exception(expect_unreachable_msg)


mock_builtins = {**builtins.__dict__, **{"open": open_expect_unreachable}}


# -----------------------------------------------------------------------------
# Utility Functions


def is_expression_secure(expr_str, verbose):
    """
    Return (ok, code) where ok is true if expr_str is considered secure.
    """
    # Internal function only for testing (not part of the public API).
    from _bpy import _driver_secure_code_test
    expr_code = compile(expr_str, "<is_expression_secure>", 'eval')
    ok = _driver_secure_code_test(expr_code, verbose=verbose)
    return ok, expr_code


# -----------------------------------------------------------------------------
# Tests (Accept)


class _TestExprMixIn:
    """
    Sub-classes must define:
    - expressions_expect_secure: boolean, the expected secure state.
    - expressions: A sequence of expressions that must evaluate in the driver name-space.

    Optionally:
    - expressions_expect_unreachable:
      A boolean, when true, it's expected each expression should call
    ``expect_os_unreachable`` or ``expect_open_unreachable``.
    """

    # Sub-class may override.
    expressions_expect_unreachable = False

    def assertSecure(self, expect_secure, expr_str):
        is_secure, expr_code = is_expression_secure(
            expr_str,
            # Only verbose when secure as this is will result in an failure,
            # in that case it's useful to know which op-codes caused the test to unexpectedly fail.
            verbose=expect_secure,
        )
        if is_secure != expect_secure:
            raise self.failureException(
                "Expression \"%s\" was expected to be %s" %
                (expr_str, "secure" if expect_secure else "insecure"))
        # NOTE: executing is not essential, it's just better to ensure the expressions make sense.
        try:
            exec(
                expr_code,
                {"__builtins__": mock_builtins},
                {**bpy.app.driver_namespace, **{"__builtins__": mock_builtins}},
            )
            # exec(expr_code, {}, bpy.app.driver_namespace)
            ex = None
        except Exception as ex_test:
            ex = ex_test

        if self.expressions_expect_unreachable:
            if ex and ex.args == (expect_unreachable_msg,):
                ex = None
            elif not ex:
                raise self.failureException("Expression \"%s\" failed to run `os.expect_os_unreachable`" % (expr_str,))
            else:
                # An unknown exception was raised, use the exception below.
                pass

        if ex:
            raise self.failureException("Expression \"%s\" failed to evaluate with error: %r" % (expr_str, ex))

    def test_expr(self):
        expect_secure = self.expressions_expect_secure
        for expr_str in self.expressions:
            self.assertSecure(expect_secure, expr_str)


class TestExprMixIn_Accept(_TestExprMixIn):
    expressions_expect_secure = True


class TestExprMixIn_Reject(_TestExprMixIn):
    expressions_expect_secure = False


class TestAcceptLiteralNumbers(unittest.TestCase, TestExprMixIn_Accept):
    expressions = ("1", "1_1", "1.1", "1j", "0x1", "0o1", "0b1")


class TestAcceptLiteralStrings(unittest.TestCase, TestExprMixIn_Accept):
    expressions = ("''", "'_'", "r''", "r'_'", "'''_'''")


class TestAcceptSequencesEmpty(unittest.TestCase, TestExprMixIn_Accept):
    expressions = ("()", "[]", "{}", "[[]]", "(())")


class TestAcceptSequencesSimple(unittest.TestCase, TestExprMixIn_Accept):
    expressions = ("('', '')", "['', '_']", "{'', '_'}", "{'': '_'}")


class TestAcceptSequencesExpand(unittest.TestCase, TestExprMixIn_Accept):
    expressions = ("(*('', '_'),)", "[*(), *[]]", "{*{1, 2}}")


class TestAcceptSequencesComplex(unittest.TestCase, TestExprMixIn_Accept):
    expressions = ("[1, 2, 3][-1:0:-1][0]", "1 in (1, 2)", "False if 1 in {1, 2} else True")


class TestAcceptMathOperators(unittest.TestCase, TestExprMixIn_Accept):
    expressions = ("4 / 4", "4 * 4", "4 // 4", "2 ** 2", "4 ^ -1", "4 & 1", "4 % 1")


class TestAcceptMathFunctionsSimple(unittest.TestCase, TestExprMixIn_Accept):
    expressions = ("sin(pi)", "degrees(pi / 2)", "clamp(4, 0, 1)")


class TestAcceptMathFunctionsComplex(unittest.TestCase, TestExprMixIn_Accept):
    expressions = ("-(sin(pi) ** 2) / 2", "floor(22 / 7)", "ceil(pi + 1)")


# -----------------------------------------------------------------------------
# Tests (Reject)

class TestRejectLiteralFStrings(unittest.TestCase, TestExprMixIn_Reject):
    # F-String's are not supported as `BUILD_STRING` op-code is disabled,
    # while it may be safe to enable that needs to be double-checked.
    # Further it doesn't seem useful for typical math expressions used in drivers.
    expressions = ("f'{1}'", "f'{\"_\"}'")


class TestRejectModuleAccess(unittest.TestCase, TestExprMixIn_Reject):
    # Each of these commands _must_ run `expect_os_unreachable`,
    # and must also be rejected as insecure - otherwise we have problems.
    expressions_expect_unreachable = True
    expressions = (
        "__import__('os').expect_os_unreachable()",
        "exec(\"__import__('os').expect_os_unreachable()\")",
        "(globals().update(__import__('os').__dict__), expect_os_unreachable())",
        "__builtins__['getattr'](__builtins__['__import__']('os'), 'expect_os_unreachable')()",
    )

    # Ensure the functions are actually called.
    def setUp(self):
        self._count = expect_unreachable_count

    def tearDown(self):
        count_actual = expect_unreachable_count - self._count
        count_expect = len(self.expressions)
        if count_actual != count_expect:
            raise Exception(
                "Expected 'os.expect_os_unreachable' to be called %d times but was called %d times" %
                (count_expect, count_actual),
            )


class TestRejectOpenAccess(unittest.TestCase, TestExprMixIn_Reject):
    # Each of these commands _must_ run `expect_open_unreachable`,
    # and must also be rejected as insecure - otherwise we have problems.
    expressions_expect_unreachable = True
    expressions = (
        "open('file.txt', 'r')",
        "exec(\"open('file.txt', 'r')\")",
        "(globals().update({'fake_open': __builtins__['open']}), fake_open())",
    )

    # Ensure the functions are actually called.
    def setUp(self):
        self._count = expect_open_unreachable_count

    def tearDown(self):
        count_actual = expect_open_unreachable_count - self._count
        count_expect = len(self.expressions)
        if count_actual != count_expect:
            raise Exception(
                "Expected 'open' to be called %d times but was called %d times" %
                (count_expect, count_actual),
            )


if __name__ == '__main__':
    import sys
    sys.argv = [__file__] + (sys.argv[sys.argv.index("--") + 1:] if "--" in sys.argv else [])
    unittest.main()


## Links discovered
- ['getattr'](https://github.com/blender/blender/blob/main/tests/python/__builtins__['__import__']('os'.md)

--- tests/python/bl_pyapi_bpy_path.py ---
# SPDX-FileCopyrightText: 2015-2022 Blender Authors
#
# SPDX-License-Identifier: Apache-2.0

# ./blender.bin --background --python tests/python/bl_pyapi_bpy_path.py -- --verbose
import unittest


class TestBpyPath(unittest.TestCase):
    def test_ensure_ext(self):
        from bpy.path import ensure_ext

        # Should work with both strings and bytes.
        self.assertEqual(ensure_ext('demo', '.blend'), 'demo.blend')
        self.assertEqual(ensure_ext(b'demo', b'.blend'), b'demo.blend')

        # Test different cases.
        self.assertEqual(ensure_ext('demo.blend', '.blend'), 'demo.blend')
        self.assertEqual(ensure_ext('demo.BLEND', '.blend'), 'demo.BLEND')
        self.assertEqual(ensure_ext('demo.blend', '.BLEND'), 'demo.blend')

        # Test empty extensions, compound extensions etc.
        self.assertEqual(ensure_ext('demo', 'blend'), 'demoblend')
        self.assertEqual(ensure_ext('demo', ''), 'demo')
        self.assertEqual(ensure_ext('demo', '.json.gz'), 'demo.json.gz')
        self.assertEqual(ensure_ext('demo.json.gz', '.json.gz'), 'demo.json.gz')
        self.assertEqual(ensure_ext('demo.json', '.json.gz'), 'demo.json.json.gz')
        self.assertEqual(ensure_ext('', ''), '')
        self.assertEqual(ensure_ext('', '.blend'), '.blend')

        # Test case-sensitive behavior.
        self.assertEqual(ensure_ext('demo', '.blend', case_sensitive=True), 'demo.blend')
        self.assertEqual(ensure_ext('demo.BLEND', '.blend', case_sensitive=True), 'demo.BLEND.blend')
        self.assertEqual(ensure_ext('demo', 'Blend', case_sensitive=True), 'demoBlend')
        self.assertEqual(ensure_ext('demoBlend', 'blend', case_sensitive=True), 'demoBlendblend')
        self.assertEqual(ensure_ext('demo', '', case_sensitive=True), 'demo')


if __name__ == '__main__':
    import sys

    sys.argv = [__file__] + (sys.argv[sys.argv.index("--") + 1:] if "--" in sys.argv else [])
    unittest.main()


--- extern/audaspace/src/respec/filter_design.py ---
#!/usr/bin/python

################################################################################
# Copyright 2009-2023 Jrg Mller
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
################################################################################

# high quality:   sinc filter coefficients, Nz = 136, L = 2304, freq = 0.963904, Kaiser Window B = 16
# medium quality: sinc filter coefficients, Nz = 42, L = 500, freq = 0.916636, Kaiser Window B = 12
# low quality:    sinc filter coefficients, Nz = 16, L = 128, freq = 0.834068, Kaiser Window B = 10

import numpy as np
import scipy

L = 2304
Nz = 136
B = 16
freq = Nz / (Nz + B / np.pi)

print(f'// sinc filter coefficients, Nz = {Nz}, L = {L}, freq = {freq:.6f}, Kaiser Window B = {B}')

Nz = Nz / freq

a = freq * np.sinc(freq * np.arange(0, Nz, 1/L))

M = len(a)*2-1

b = scipy.signal.windows.kaiser(M, B)

b = b[len(a)-1:]

y = a * b

# print filter coefficients from y
if False:
	print(f'AUD_NAMESPACE_BEGIN')
	print(f'const int JOSResampleReader::m_len_PRESET = {int(L*Nz)};')
	print(f'const int JOSResampleReader::m_L_PRESET = {L};')
	print(f'const float JOSResampleReader::m_coeff_PRESET[m_len_PRESET + 1] = {{')
	for idx, val in enumerate(y):
		print(f'{val:.9e}f', end=', ')
		if (idx + 1) % 10 == 0:
			print("\n", end='')
	print(f'}};')
	print(f'AUD_NAMESPACE_END')

# visualize filter

import matplotlib.pyplot as plt

mid = len(y)
res = np.concatenate([y[:0:-1], y])

f1 = L
f2 = 1
Fs1 = L
Fs2 = 2

area = mid - 1
t = (np.arange(1, area*2+1) - area) / (Fs1 * f2)

plt.figure()
plt.plot(t, res[mid - area:mid + area])
plt.xlim([t[0], t[-1]])
plt.ylim(np.array([np.min(res), np.max(res)]) * 1.05)
plt.xlabel('Time [s]')
plt.ylabel('Amplitude')
plt.title('Response')

fftres = np.fft.fft(res / f1)

f = np.arange(len(fftres)) * Fs2 * f1 / len(fftres)

plt.figure()
plt.plot(f, np.log10(np.abs(fftres))*20)
plt.xlim([0, Fs2])
plt.ylim([-200, 0])
plt.xlabel('Frequency [Hz]')
plt.ylabel('Magnitude [dB]')
plt.title('Magnitude')

plt.figure()
plt.plot(f, np.log10(np.abs(fftres)/np.abs(fftres[0]))*20)
plt.xlim(np.array([0, Fs2/2])*1.1)
plt.ylim([-3, 1.5])
plt.xlabel('Frequency [Hz]')
plt.ylabel('Magnitude [dB]')
plt.title('Passband')

plt.figure()
plt.plot(f, np.log10(np.abs(fftres)/np.abs(fftres[0]))*20)
plt.xlim(np.array([0.8, 1.1])*Fs2/2)
plt.ylim([-100, 6])
plt.xlabel('Frequency [Hz]')
plt.ylabel('Magnitude [dB]')
plt.title('Transition')

phi = np.angle(fftres);
phi -= (phi > np.pi / 2) * np.pi;
phi += (phi < -np.pi / 2) * np.pi;

plt.figure()
plt.plot(f, phi * 180 / np.pi)
plt.xlim([0, Fs2/2])
plt.ylim([-180, 180])
plt.xlabel('Frequency [Hz]')
plt.ylabel('Phase [deg]')
plt.title('Phase')

plt.show()


--- release/darwin/README.md ---
Buildbot Configuration
======================

Files used by Buildbot's `package-code-binaires` step for the darwin platform.



--- release/lts/README.md ---
This folder contains a script to generate release notes and download URLs
for Blender LTS releases.

Ensure required Python modules are installed before running:

    pip3 install -r ./requirements.txt

Then run for example:

    ./create_release_notes.py --version 3.3.2 --format=html

Available arguments:

    --version VERSION  Version string in the form of {major}.{minor}.{build}
                       (e.g. 3.3.2)
    --issue ISSUE      Gitea issue that is contains the release notes
                       information (e.g. #77348)
    --format FORMAT    Format the result in `text`, `steam`, `wiki` or `html`


--- release/pypi/README.md ---
# PyPI Release Publishing

### Setup

Install Twine.

    pip3 install twine

Create ~/.pypirc with the following contents. Token is available in same place
as other credentials used for publishing releases.

    [distutils]
      index-servers =
        pypi
        bpy
    [pypi]
      username = __token__
      password = <SECRET_PYPI_TOKEN>
    [bpy]
      repository = https://upload.pypi.org/legacy/
      username = __token__
      password = <SECRET_PYPI_TOKEN>

### Release

Trigger release buildbot build with Python Module and Package Delivery enabled.
Check download page for Git hash.

Run checks:

    ./upload-release.py --version X.X.X --git-hash abcd1234 --check

Upload:

    ./upload-release.py --version X.X.X --git-hash abcd1234


--- release/text/readme.html ---
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta http-equiv="Content-Style-Type" content="text/css">
    <title>Blender Readme</title>
    <style type="text/css">
      p.p1 {margin: 0.0px 156.0px 22.0px 156.0px; text-align: center; font: 22.0px Helvetica}
      p.p2 {margin: 0.0px 0.0px 12.0px 0.0px; font: 12.0px Times; min-height: 14.0px}
      p.p3 {margin: 0.0px 156.0px 16.0px 156.0px; font: 16.0px Helvetica}
      p.p4 {margin: 0.0px 156.0px 14.0px 156.0px; font: 14.0px Helvetica}
      p.p5 {margin: 0.0px 156.0px 14.0px 156.0px; font: 14.0px Helvetica; color: #0000ee}
      p.p6 {margin: 0.0px 156.0px 14.0px 156.0px; text-align: center; font: 14.0px Helvetica}
      span.s1 {text-decoration: underline ; color: #0000ee}
      span.s2 {font: 14.0px 'Lucida Grande'}
      span.s3 {color: #000000}
      span.s4 {text-decoration: underline}
    </style>
  </head>
  <body>
    <p class="p1"><b>Blender @BLENDER_VERSION@</b></p>
    <p class="p2"><br></p>
    <p class="p3"><b>About</b></p>
    <p class="p4">
      Welcome to Blender, the free, open source 3D application for modeling, rigging, animation,
      simulation, rendering, compositing, motion tracking, and video editing.

      Blender is available for Linux, macOS and Windows and has a large world-wide community.
    </p>
    <p class="p4">
      Blender can be used freely for any purpose, including commercial use and distribution.
      It's free and open-source software, released under the GNU GPL licence.
      The entire source code is available on our website.
    </p>
    <p class="p4">
      For more information, visit
      <a href="https://www.blender.org/">
        <span class="s1">blender.org</span>
      </a>.
    </p>
    <p class="p2"><br></p>
    <p class="p3"><b>@BLENDER_VERSION@</b></p>
    <p class="p4">
      The Blender Foundation and online developer community is proud to present Blender
      @BLENDER_VERSION@.
      <a href="https://developer.blender.org/docs/release_notes/@BLENDER_VERSION@">
        <span class="s1">More information about this release</span>
      </a>.
    </p>
    <p class="p2"><br></p>
    <p class="p3"><b>Bugs</b></p>
    <p class="p4">
      Although this is considered a stable release, you may encounter a bug.
      If you do, please help us by posting it in the bug tracker or using Help
      <span class="s2"></span> Report a Bug from inside Blender. If it wasnt reported yet,
      please log in (or register) and fill in detailed information about the error. Please post
      detailed instructions on how to reproduce it or post a .blend file showcasing the bug.
    </p>
    <p class="p2"><br></p>
    <p class="p3"><b>Package Contents</b></p>
    <p class="p4">The downloaded Blender package includes:</p>
    <p class="p4"> The Blender application for the chosen operating system.</p>
    <p class="p4"> Add-ons to extend Blender functionality.</p>
    <p class="p4"> Readme and copyright files.</p>
    <p class="p2"><br></p>
    <p class="p3"><b>Installation</b></p>
    <p class="p4">
      <b>Windows: </b>
      The download .zip contains a Blender folder. You may put this anywhere on your hard drive.
      To launch Blender, double-click on Blender.exe.
    </p>
    <p class="p4">
      <b>Linux: </b>Unpack the archive, then run the Blender executable.
    </p>
    <p class="p4">
      <b>macOS: </b>
      The downloaded package includes Blender.app.
      Optionally copy this to your Applications folder,
      and add it to the dock by dragging it from there to the dock.
    </p>
    <p class="p2"><br></p>
    <p class="p4">
      <b>Installing Addons (all systems)</b>
      Addons can be installed from the user preferences addons section,
      download an addon as a .py or .zip file, then press the "Install Addon"
      button and select the file to install it.
    </p>
    <p class="p2"><br></p>
    <p class="p3"><b>Links</b></p>
    <p class="p4">Users:</p>
    <p class="p5">
      <span class="s3">
        General information
        <a href="https://www.blender.org/">
          <span class="s4">www.blender.org</span>
        </a><br>
        Release Notes
        <a href="https://developer.blender.org/docs/release_notes/@BLENDER_VERSION@">
          <span class="s4">https://developer.blender.org/docs/release_notes/@BLENDER_VERSION@</span>
        </a><br>
        Tutorials
        <a href="https://www.blender.org/support/tutorials/">
          <span class="s4">www.blender.org/support/tutorials/</span>
        </a><br>
        Manual
        <a href="https://docs.blender.org/manual/en/latest/">
          <span class="s4">https://docs.blender.org/manual/en/latest/</span>
        </a><br>
        User Community <a href="https://www.blender.org/community/">
          <span class="s4">https://www.blender.org/community/</span>
        </a><br>
        Chat
        <a href="https://chat.blender.org/#/room/#general:blender.org">
          <span class="s4">#general</span>
        </a>
      </span>
    </p>
    <p class="p4">Developers:</p>
    <p class="p5">
      <span class="s3">
        Development
        <a href="https://developer.blender.org/">
          <span class="s4">https://developer.blender.org/</span>
        </a><br>
        GIT and Bug Tracker
        <a href="https://projects.blender.org/">
          <span class="s4">projects.blender.org</span>
        </a><br>
        Chat
        <a href="https://chat.blender.org/#/room/#blender-coders:blender.org">
          <span class="s4">#blender-coders</span>
        </a> on chat.blender.org
      </span>
    </p>
    <p class="p2"><br></p>
    <p class="p2"><br></p>
    <p class="p6">Blender is open-source and free for all to use.</p>
    <p class="p2"><br></p>
    <p class="p6">Enjoy.</p>
    <p class="p2"><br></p>
  </body>
</html>


## Links discovered
- [<span class="s1">blender.org</span>](https://www.blender.org/)
- [<span class="s1">More information about this release</span>](https://developer.blender.org/docs/release_notes/@BLENDER_VERSION@)
- [<span class="s4">www.blender.org</span>](https://www.blender.org/)
- [<span class="s4">https://developer.blender.org/docs/release_notes/@BLENDER_VERSION@</span>](https://developer.blender.org/docs/release_notes/@BLENDER_VERSION@)
- [<span class="s4">www.blender.org/support/tutorials/</span>](https://www.blender.org/support/tutorials/)
- [<span class="s4">https://docs.blender.org/manual/en/latest/</span>](https://docs.blender.org/manual/en/latest/)
- [<span class="s4">https://www.blender.org/community/</span>](https://www.blender.org/community/)
- [<span class="s4">https://developer.blender.org/</span>](https://developer.blender.org/)
- [<span class="s4">projects.blender.org</span>](https://projects.blender.org/)

--- release/freedesktop/snap/README.md ---
Snap Configuration
===================

Files used by Buildbot's `package-code-store-snap` and `deliver-code-store-snap` steps.

Build pipeline snap tracks and channels

```
    <track>/stable            
        - Latest stable release for the specified track
    <track>/candidate         
        - Test builds for the upcoming stable release - *not used for now*
    <track>/beta              
        - Nightly automated builds provided by a release branch
    <track>/edge/<branch>
        - Nightly or on demand builds - will also make use of branch
```


--- release/windows/msix/README.md ---
Buildbot Configuration
======================

Files used by Buildbot's `package-code-store-windows` step.


--- release/extensions/system/readme.txt ---
System Extensions

Extensions extracted into this directory will be available from the
default "System" repository.

This allows extensions to be bundled with Blender outside of
user repositories.


--- release/datafiles/colormanagement/icc/README.md ---
ICC profiles to embed when writing images.

The names match the ASWF Color Interop Forum Recommendation.

From the Compact ICC Profiles project, with CC0-1.0 license.
https://github.com/saucecontrol/Compact-ICC-Profiles


--- intern/cycles/doc/license/readme.txt ---
The default license for Cycles source code is Apache 2.0.

There is also code adapted from other sources with different but compatible
licenses. This is marked with appropriate SPDX license identifiers and
copyright notices in source files.

See SPDX-license-identifiers.txt for a list of licenses used in the code.


--- doc/license/Apache2-license.txt ---

                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.


--- CMakeLists.txt ---
# SPDX-FileCopyrightText: 2006 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

# -----------------------------------------------------------------------------
# Early Initialization

# NOTE: We don't allow in-source builds. This causes no end of troubles because
# all out-of-source builds will use the CMakeCache.txt file there and even
# build the libs and objects in it.
if(${CMAKE_SOURCE_DIR} STREQUAL ${CMAKE_BINARY_DIR})
  if(NOT DEFINED WITH_IN_SOURCE_BUILD)
    message(FATAL_ERROR
      "CMake generation for blender is not allowed within the source directory!"
      "\n Remove \"${CMAKE_SOURCE_DIR}/CMakeCache.txt\""
      "\n then try again from another directory, e.g.:"
      "\n "
      "\n rm -rf CMakeCache.txt CMakeFiles"
      "\n cd .."
      "\n mkdir cmake-make"
      "\n cd cmake-make"
      "\n cmake ../blender"
      "\n "
      "\n Alternately define WITH_IN_SOURCE_BUILD to force this option (not recommended!)"
    )
  endif()
endif()

cmake_minimum_required(VERSION 3.10)

if(NOT EXECUTABLE_OUTPUT_PATH)
  set(FIRST_RUN TRUE)
else()
  set(FIRST_RUN FALSE)
endif()

# this starts out unset
list(APPEND CMAKE_MODULE_PATH "${CMAKE_SOURCE_DIR}/build_files/cmake/Modules")
list(APPEND CMAKE_MODULE_PATH "${CMAKE_SOURCE_DIR}/build_files/cmake/platform")

# Avoid having an empty `CMAKE_BUILD_TYPE`.
if(NOT DEFINED CMAKE_BUILD_TYPE_INIT)
  set(CMAKE_BUILD_TYPE_INIT "Release")
  # Internal logic caches this variable, avoid showing it by default
  # since it's easy to accidentally set instead of the build type.
endif()
mark_as_advanced(CMAKE_BUILD_TYPE_INIT)

# Omit superfluous "Up-to-date" messages.
if(NOT DEFINED CMAKE_INSTALL_MESSAGE)
  set(CMAKE_INSTALL_MESSAGE "LAZY")
endif()

# quiet output for Makefiles, `make -s` helps too
# set_property(GLOBAL PROPERTY RULE_MESSAGES OFF)

# -----------------------------------------------------------------------------
# Set policy

# see "cmake --help-policy CMP0003"
# So library linking is more sane
cmake_policy(SET CMP0003 NEW)

# So BUILDINFO and BLENDERPATH strings are automatically quoted
cmake_policy(SET CMP0005 NEW)

# So syntax problems are errors
cmake_policy(SET CMP0010 NEW)

# Input directories must have CMakeLists.txt
cmake_policy(SET CMP0014 NEW)

# Silence draco warning on macOS, new policy works fine.
if(POLICY CMP0068)
  cmake_policy(SET CMP0068 NEW)
endif()

# find_package() uses <PackageName>_ROOT variables.
if(POLICY CMP0074)
  cmake_policy(SET CMP0074 NEW)
endif()

# find_package() uses uppercase <PackageName>_ROOT variables.
if(POLICY CMP0144)
  cmake_policy(SET CMP0144 NEW)
endif()

# Install CODE|SCRIPT allow the use of generator expressions.
if(POLICY CMP0087)
  cmake_policy(SET CMP0087 NEW)
endif()

# Allow to specify language per file.
if(POLICY CMP0119)
  cmake_policy(SET CMP0119 NEW)
endif()

# Install DESTINATION paths are normalized.
if(POLICY CMP0177)
  cmake_policy(SET CMP0177 NEW)
endif()

# The target_sources() command converts relative paths to absolute.
cmake_policy(SET CMP0076 NEW)

# -----------------------------------------------------------------------------
# Load Blender's Local Macros

include(build_files/cmake/macros.cmake)

# -----------------------------------------------------------------------------
# Initialize Project

blender_project_hack_pre()

project(Blender)

blender_project_hack_post()

enable_testing()

# There are some circumstances where the git checkout doesn't grab
# the files from LFS but produces small pointer files. The build system
# does not pick up on this, and at runtime blender will throw a strange
# error while it is trying to load the startup.blend file. For now just
# check this file is over 1KB in size, and throw out a fatal error otherwise.

if(EXISTS ${CMAKE_SOURCE_DIR}/release/datafiles/startup.blend)
  file(SIZE ${CMAKE_SOURCE_DIR}/release/datafiles/startup.blend startup_size)
  if(${startup_size} LESS 1024)
    message(FATAL_ERROR "Detected incomplete startup blend, likely due to missing Git LFS checkout.\nRunning \"make update\" or \"git lfs pull\" may resolve the issue.")
  endif()
  unset(startup_size)
endif()


# -----------------------------------------------------------------------------
# Test Compiler Support
#
# Keep in sync with: https://developer.blender.org/docs/handbook/building_blender/

if(CMAKE_COMPILER_IS_GNUCC)
  if("${CMAKE_C_COMPILER_VERSION}" VERSION_LESS "11.0.0")
    message(FATAL_ERROR
      "The minimum supported version of GCC is 11.0.0, found C compiler: "
      "${CMAKE_C_COMPILER_VERSION}"
    )
  endif()
  if("${CMAKE_CXX_COMPILER_VERSION}" VERSION_LESS "11.0.0")
    message(FATAL_ERROR
      "The minimum supported version of GCC is 11.0.0, found C++ compiler: "
      "${CMAKE_CXX_COMPILER_VERSION}"
    )
  endif()
elseif(CMAKE_C_COMPILER_ID MATCHES "Clang")
  if(CMAKE_COMPILER_IS_GNUCC)
    if("${CMAKE_C_COMPILER_VERSION}" VERSION_LESS "8.0")
      message(FATAL_ERROR
        "The minimum supported version of CLANG is 8.0, found C compiler: "
        "${CMAKE_C_COMPILER_VERSION}"
      )
    endif()
    if("${CMAKE_CXX_COMPILER_VERSION}" VERSION_LESS "8.0")
      message(FATAL_ERROR
        "The minimum supported version of CLANG is 8.0, found C++ compiler: "
        "${CMAKE_CXX_COMPILER_VERSION}"
      )
    endif()
  endif()
elseif(CMAKE_CXX_COMPILER_ID STREQUAL "MSVC")
  if(MSVC_VERSION VERSION_LESS "1928")
    # MSVC_VERSION is an internal version number, it doesn't map to something
    # the end user would recognize as a version. Because of this, for MSVC we do
    # not show the found number. When using our make.bat the actual VS version
    # will be displayed on the console before starting the build, anyway.
    message(FATAL_ERROR "The minimum supported version of MSVC is 2019 (16.9.16)")
  endif()
endif()

# -----------------------------------------------------------------------------
# Test Compiler/Library Features

include(build_files/cmake/have_features.cmake)


# -----------------------------------------------------------------------------
# Redirect Output Files

set(EXECUTABLE_OUTPUT_PATH ${CMAKE_BINARY_DIR}/bin CACHE INTERNAL "" FORCE)
set(LIBRARY_OUTPUT_PATH ${CMAKE_BINARY_DIR}/lib CACHE INTERNAL "" FORCE)

get_property(GENERATOR_IS_MULTI_CONFIG GLOBAL PROPERTY GENERATOR_IS_MULTI_CONFIG)
if(GENERATOR_IS_MULTI_CONFIG)
  set(TESTS_OUTPUT_DIR ${EXECUTABLE_OUTPUT_PATH}/tests/$<CONFIG>/ CACHE INTERNAL "" FORCE)
else()
  set(TESTS_OUTPUT_DIR ${EXECUTABLE_OUTPUT_PATH}/tests/ CACHE INTERNAL "" FORCE)
endif()


# -----------------------------------------------------------------------------
# Set Default Configuration Options

get_blender_version()

if(WIN32)
  add_definitions(
    # This is the app ID used for file registration, given it's used from several modules
    # there really is no nice way to get this information consistent without a global define.
    -DBLENDER_WIN_APPID="blender.${BLENDER_VERSION_MAJOR}.${BLENDER_VERSION_MINOR}"
    # This is the name that will be shown in the taskbar and OpenWith windows UI
    -DBLENDER_WIN_APPID_FRIENDLY_NAME="Blender ${BLENDER_VERSION_MAJOR}.${BLENDER_VERSION_MINOR}"
  )
endif()


if(${CMAKE_VERSION} VERSION_GREATER_EQUAL "3.19")
  # This changes the default value from Off to On, but will still allow people to manually change
  # this setting through their CMakeCache.txt if they desire to do so.
  set(CMAKE_OPTIMIZE_DEPENDENCIES ON CACHE INTERNAL "")
endif()

# -----------------------------------------------------------------------------
# Declare Options

# Blender internal features
option(WITH_BLENDER "Build blender (disable to build only Cycles stand-alone)." ON)
mark_as_advanced(WITH_BLENDER)

if(WIN32)
  option(WITH_BLENDER_THUMBNAILER "\
Build \"BlendThumb.dll\" helper for Windows explorer integration to support extracting \
thumbnails from `.blend` files."
    ON
  )
elseif(UNIX AND NOT APPLE)
  option(WITH_BLENDER_THUMBNAILER "\
Build stand-alone \"blender-thumbnailer\" command-line thumbnail extraction utility, \
intended for use by file-managers to extract PNG images from `.blend` files."
    ON
  )
elseif(APPLE)
  option(WITH_BLENDER_THUMBNAILER "\
Build \"blender-thumbnailer.appex\" extension for Finder/ QuickLook thumbnail \
support for blend files."
    ON
  )
endif()

option(WITH_INTERNATIONAL "Enable I18N (International fonts and text)" ON)

option(WITH_PYTHON "Enable Embedded Python API (only disable for development)" ON)
option(WITH_PYTHON_SECURITY "Disables execution of scripts within blend files by default" ON)
# Don't want people disabling this unless they really know what they are doing.
mark_as_advanced(WITH_PYTHON)
# Some distributions see this as a security issue, rather than have them patch it,
# make a build option.
mark_as_advanced(WITH_PYTHON_SECURITY)

option(WITH_PYTHON_SAFETY "\
Enable internal API error checking to track invalid data to prevent crash on access \
(at the expense of some efficiency, only enable for development)."
  OFF
)
mark_as_advanced(WITH_PYTHON_SAFETY)
option(WITH_PYTHON_MODULE "\
Enable building as a python module which runs without a user interface, \
like running regular blender in background mode (only enable for development), \
installs to PYTHON_SITE_PACKAGES (or CMAKE_INSTALL_PREFIX if WITH_INSTALL_PORTABLE is enabled)."
  OFF
)

option(WITH_BUILDINFO "\
Include extra build details (only disable for development & faster builds)"
  ON
)
set(BUILDINFO_OVERRIDE_DATE "" CACHE STRING "\
Use instead of the current date for reproducible builds (empty string disables this option)"
)
set(BUILDINFO_OVERRIDE_TIME "" CACHE STRING "\
Use instead of the current time for reproducible builds (empty string disables this option)"
)
set(CPACK_OVERRIDE_PACKAGENAME "" CACHE STRING "\
Use instead of the standard packagename (empty string disables this option)"
)
mark_as_advanced(CPACK_OVERRIDE_PACKAGENAME)
mark_as_advanced(BUILDINFO_OVERRIDE_DATE)
mark_as_advanced(BUILDINFO_OVERRIDE_TIME)

# CMAKE 3.28.2 has issues with the combination of PCH and unity builds, disable for now.
# upstream ticket https://gitlab.kitware.com/cmake/cmake/-/issues/25650
if(${CMAKE_VERSION} VERSION_GREATER_EQUAL "3.16" AND NOT ${CMAKE_VERSION} VERSION_EQUAL "3.28.2")
  option(WITH_UNITY_BUILD "\
Enable unity build for modules that support it to improve compile times.\n\
WARNING: this option allows files to be built without all necessary headers!\n
This option should be disabled before manipulating or removing headers."
    ON
  )
  mark_as_advanced(WITH_UNITY_BUILD)
else()
  set(WITH_UNITY_BUILD OFF)
endif()

if(COMMAND target_precompile_headers)
  # Disabling is needed for `./tools/utils_maintenance/code_clean.py` to function.
  option(WITH_COMPILER_PRECOMPILED_HEADERS "\
Use pre-compiled headers to speed up compilation."
    ON
  )
  mark_as_advanced(WITH_COMPILER_PRECOMPILED_HEADERS)

  if(WITH_CLANG_TIDY AND (CMAKE_COMPILER_IS_GNUCC OR APPLE))
    if(WITH_COMPILER_PRECOMPILED_HEADERS)
      message(STATUS
        "Clang-Tidy and the current compiler's precompiled headers are incompatible, "
        "disabling precompiled headers."
      )
      set(WITH_COMPILER_PRECOMPILED_HEADERS OFF)
    endif()
  endif()

  if(NOT WITH_COMPILER_PRECOMPILED_HEADERS)
    set(CMAKE_DISABLE_PRECOMPILE_HEADERS ON)
  endif()
endif()

option(WITH_OPTIMIZED_BUILD_TOOLS "\
Enable optimizations even on Debug builds for executables used for the build process"
  ON
)
mark_as_advanced(WITH_OPTIMIZED_BUILD_TOOLS)

option(WITH_IK_ITASC "\
Enable ITASC IK solver (only disable for development & for incompatible C++ compilers)"
  ON
)
option(WITH_IK_SOLVER "\
Enable Legacy IK solver (only disable for development)"
  ON
)
option(WITH_FFTW3 "Enable FFTW3 support (Used for smoke, ocean sim, glare, and audio effects)" ON)
option(WITH_PUGIXML "Enable PugiXML support (Used for OpenImageIO, Grease Pencil SVG export)" ON)
option(WITH_BULLET "Enable Bullet (Physics Engine)" ON)
option(WITH_SYSTEM_BULLET "\
Use the systems bullet library (currently unsupported due to missing features in upstream!)"
  OFF
)
mark_as_advanced(WITH_SYSTEM_BULLET)
option(WITH_OPENCOLORIO "Enable OpenColorIO color management" ON)

set(_option_default ON)
if(APPLE)
  # There's no OpenXR runtime in sight for macOS, neither is code well
  # tested there -> disable it by default.
  set(_option_default OFF)
endif()
option(WITH_XR_OPENXR "Enable VR features through the OpenXR specification" ${_option_default})
if(APPLE)
  mark_as_advanced(WITH_XR_OPENXR)
endif()
unset(_option_default)

option(WITH_GMP "Enable features depending on GMP (Exact Boolean)" ON)
option(WITH_MANIFOLD "Enable features depending on Manifold (Fast Robust Boolean)" ON)

option(WITH_OPENIMAGEDENOISE "Enable the OpenImageDenoise compositing node" ON)

option(WITH_OPENSUBDIV "Enable OpenSubdiv for surface subdivision" ON)

option(WITH_POTRACE "Enable features relying on potrace" ON)
option(WITH_OPENVDB "Enable features relying on OpenVDB" ON)
option(WITH_OPENVDB_BLOSC "\
Enable blosc compression for OpenVDB, only enable if OpenVDB was built with blosc support"
  ON
)
option(WITH_OPENVDB_3_ABI_COMPATIBLE "\
Assume OpenVDB library has been compiled with version 3 ABI compatibility"
  OFF
)
mark_as_advanced(WITH_OPENVDB_3_ABI_COMPATIBLE)
option(WITH_NANOVDB "Enable usage of NanoVDB data structure for rendering on the GPU" ON)
option(WITH_HARU "Enable features relying on Libharu (Grease pencil PDF export)" ON)

# GHOST Windowing Library Options
option(WITH_GHOST_DEBUG "Enable debugging output for the GHOST library" OFF)
mark_as_advanced(WITH_GHOST_DEBUG)

option(WITH_GHOST_SDL "\
Enable building Blender against SDL for windowing rather than the native APIs"
  OFF
)
mark_as_advanced(WITH_GHOST_SDL)

if(UNIX AND NOT (APPLE OR HAIKU))
  option(WITH_GHOST_X11 "Enable building Blender against X11 for windowing" ON)
  mark_as_advanced(WITH_GHOST_X11)

  option(WITH_GHOST_WAYLAND "Enable building Blender against Wayland for windowing" ON)
  mark_as_advanced(WITH_GHOST_WAYLAND)

  if(WITH_GHOST_WAYLAND)
    option(WITH_GHOST_CSD "Enable client side decorations with Wayland" ON)
    mark_as_advanced(WITH_GHOST_CSD)

    option(WITH_GHOST_WAYLAND_DYNLOAD "Enable runtime dynamic WAYLAND libraries loading" ON)
    mark_as_advanced(WITH_GHOST_WAYLAND_DYNLOAD)

    set(WITH_GHOST_WAYLAND_APP_ID "" CACHE STRING "\
The application ID used for Blender (use default when an empty string), \
this can be used to differentiate Blender instances by version or branch for example."
    )
    mark_as_advanced(WITH_GHOST_WAYLAND_APP_ID)
  endif()
endif()

# Only used for WAYLAND, avoid having to check for WAYLAND everywhere by setting as disabled.
if(NOT DEFINED WITH_GHOST_CSD)
  set(WITH_GHOST_CSD OFF)
endif()

if(WITH_GHOST_X11)
  option(WITH_GHOST_XDND "Enable drag'n'drop support on X11 using XDND protocol" ON)
endif()

# Misc...
option(WITH_HEADLESS "Build without graphical support (renderfarm, server mode only)" OFF)
mark_as_advanced(WITH_HEADLESS)

option(WITH_QUADRIFLOW "Build with quadriflow remesher support" ON)

option(WITH_AUDASPACE "\
Build with blenders audio library (only disable if you know what you're doing!)"
  ON
)
option(WITH_SYSTEM_AUDASPACE "\
Build with external audaspace library installed on the system \
(only enable if you know what you're doing!)"
  OFF
)
option(WITH_RUBBERBAND "\
Build with Rubber Band for audio time-stretching and pitch-scaling (used by Audaspace)"
  ON
)
mark_as_advanced(WITH_AUDASPACE)
mark_as_advanced(WITH_SYSTEM_AUDASPACE)
mark_as_advanced(WITH_RUBBERBAND)

set_and_warn_dependency(WITH_AUDASPACE WITH_SYSTEM_AUDASPACE OFF)
set_and_warn_dependency(WITH_AUDASPACE WITH_RUBBERBAND OFF)
set_and_warn_incompatible(WITH_SYSTEM_AUDASPACE WITH_RUBBERBAND OFF)

if(WITH_GHOST_X11)
  option(WITH_X11_XINPUT "Enable X11 Xinput (tablet support and unicode input)" ON)
  option(WITH_X11_XFIXES "Enable X11 XWayland cursor warping workaround" ON)
endif()

if(UNIX AND NOT APPLE)
  option(WITH_SYSTEM_FREETYPE "Use the freetype library provided by the operating system" OFF)
  option(WITH_SYSTEM_EIGEN3 "Use the systems Eigen3 library" OFF)
else()
  set(WITH_SYSTEM_FREETYPE OFF)
  set(WITH_SYSTEM_EIGEN3 OFF)
endif()

if((NOT WITH_PYTHON_MODULE) AND (
      (WIN32 AND (CMAKE_SYSTEM_PROCESSOR STREQUAL "AMD64")) OR
      ((UNIX AND NOT APPLE) AND (CMAKE_SYSTEM_PROCESSOR STREQUAL "x86_64"))))
  option(WITH_CPU_CHECK "\
Report when a CPU is not compatible on startup \
instead of failing to start with an inscrutable error."
    ON
  )
  mark_as_advanced(WITH_CPU_CHECK)
else()
  set(WITH_CPU_CHECK OFF)
endif()

# Modifiers
option(WITH_MOD_FLUID "Enable Mantaflow Fluid Simulation Framework" ON)
option(WITH_MOD_REMESH "Enable Remesh Modifier" ON)
option(WITH_MOD_OCEANSIM "Enable Ocean Modifier" ON)

# UV solvers.
option(WITH_UV_SLIM "SLIM UV unwrapping solver" ON)

# Image format support
option(WITH_IMAGE_OPENEXR "Enable OpenEXR Support (http://www.openexr.com)" ON)
option(WITH_IMAGE_OPENJPEG "Enable OpenJpeg Support (http://www.openjpeg.org)" ON)
option(WITH_IMAGE_CINEON "Enable CINEON and DPX Image Support" ON)
option(WITH_IMAGE_WEBP "Enable WebP Image Support" ON)

# Audio/Video format support
option(WITH_CODEC_FFMPEG "Enable FFMPeg Support (http://ffmpeg.org)" ON)
option(WITH_CODEC_SNDFILE "Enable libsndfile Support (http://www.mega-nerd.com/libsndfile)" ON)

# Alembic support
option(WITH_ALEMBIC "Enable Alembic Support" ON)

# Universal Scene Description support
option(WITH_USD "Enable Universal Scene Description (USD) Support" ON)

# MaterialX
option(WITH_MATERIALX "Enable MaterialX Support" ON)

# Hydra render engine
option(WITH_HYDRA "Enable Hydra render engine" ON)

# RTL Languages, Complex Shaping, OpenType Features
option(WITH_FRIBIDI "Enable features relying on fribidi" OFF)
option(WITH_HARFBUZZ "Enable features relying on harfbuzz" OFF)

# 3D format support
option(WITH_IO_WAVEFRONT_OBJ "Enable Wavefront-OBJ 3D file format support (*.obj)" ON)
option(WITH_IO_PLY "Enable PLY 3D file format support (*.ply)" ON)
option(WITH_IO_STL "Enable STL 3D file format support (*.stl)" ON)
option(WITH_IO_FBX "Enable FBX 3D file format support (*.fbx)" ON)
option(WITH_IO_GREASE_PENCIL "Enable grease-pencil file format IO (*.svg, *.pdf)" ON)

# Sound output
option(WITH_SDL "Enable SDL for sound" OFF)
option(WITH_OPENAL "Enable OpenAL Support (http://www.openal.org)" ON)
if(APPLE)
  option(WITH_COREAUDIO "Enable CoreAudio for audio support on macOS" ON)
else()
  set(WITH_COREAUDIO OFF)
endif()
if(NOT WIN32)
  set(_option_default ON)
  if(APPLE)
    set(_option_default OFF)
  endif()
  option(WITH_JACK "Enable JACK Support (http://www.jackaudio.org)" ${_option_default})
  unset(_option_default)
  option(WITH_JACK_DYNLOAD "Enable runtime dynamic JACK libraries loading" OFF)
else()
  set(WITH_JACK OFF)
endif()
if(UNIX AND NOT APPLE)
  option(WITH_PULSEAUDIO "Enable PulseAudio for audio support on Linux" ON)
  option(WITH_PULSEAUDIO_DYNLOAD "Enable runtime dynamic PulseAudio libraries loading" OFF)
  option(WITH_PIPEWIRE "Enable Pipewire for audio support on Linux" ON)
  option(WITH_PIPEWIRE_DYNLOAD "Enable runtime dynamic Pipewire libraries loading" OFF)
else()
  set(WITH_PULSEAUDIO OFF)
  set(WITH_PIPEWIRE OFF)
endif()
if(WIN32)
  option(WITH_WASAPI "Enable Windows Audio Sessions API for audio support on Windows" ON)
else()
  set(WITH_WASAPI OFF)
endif()

# Compression
option(WITH_DRACO "Enable Draco mesh compression Python module (used for glTF)" ON)

# Camera/motion tracking
option(WITH_LIBMV "Enable Libmv structure from motion library" ON)
option(WITH_LIBMV_SCHUR_SPECIALIZATIONS "Enable fixed-size schur specializations." ON)
mark_as_advanced(WITH_LIBMV_SCHUR_SPECIALIZATIONS)

# Logging/unit test libraries.
option(WITH_SYSTEM_GFLAGS "Use system-wide Gflags instead of a bundled one" OFF)
option(WITH_SYSTEM_GLOG "Use system-wide Glog instead of a bundled one" OFF)
mark_as_advanced(WITH_SYSTEM_GFLAGS)
mark_as_advanced(WITH_SYSTEM_GLOG)

# Freestyle
option(WITH_FREESTYLE "Enable Freestyle (advanced edges rendering)" ON)

# Libraries.
if(UNIX AND NOT APPLE)
  # Optionally build without pre-compiled libraries.
  # NOTE: this could be supported on all platforms however in practice UNIX is the only platform
  # that has good support for detecting installed libraries.
  option(WITH_LIBS_PRECOMPILED "\
Detect and link against pre-compiled libraries (typically found under \"../lib/\"). \
Disabling this option will use the system libraries although cached paths \
that point to pre-compiled libraries will be left as-is."
    ON
  )
  mark_as_advanced(WITH_LIBS_PRECOMPILED)

  option(WITH_STATIC_LIBS "\
Try to link with static libraries, as much as possible, \
to make blender more portable across distributions"
    OFF
  )
endif()

# Misc
if(WIN32 OR APPLE OR ((UNIX AND (NOT HAIKU)) AND WITH_GHOST_WAYLAND))
  option(WITH_INPUT_IME "Enable Input Method Editor (IME) for complex Asian character input" ON)
else()
  set(WITH_INPUT_IME OFF)
endif()
option(WITH_INPUT_NDOF "Enable NDOF input devices (SpaceNavigator and friends)" ON)
# On Windows and for the Blender application on macOS, portable install
# is the only supported installation type, so there is no option.
if(UNIX AND (NOT APPLE OR WITH_PYTHON_MODULE))
  option(WITH_INSTALL_PORTABLE "\
Install redistributable runtime, otherwise install into CMAKE_INSTALL_PREFIX"
    ON
  )
endif()

option(WITH_PYTHON_INSTALL "Copy system python into the blender install directory" ON)

option(WITH_INSTALL_COPYRIGHT "\
Copy the official Blender Authors's copyright.txt into the Blender install directory"
  OFF
)
mark_as_advanced(WITH_INSTALL_COPYRIGHT)

if((WITH_AUDASPACE AND NOT WITH_SYSTEM_AUDASPACE) OR WITH_MOD_FLUID)
  option(WITH_PYTHON_NUMPY "Include NumPy in Blender (used by Audaspace and Mantaflow)" ON)
endif()

if(WIN32 OR APPLE)
  # Windows and macOS have this bundled with Python libraries.
elseif(WITH_PYTHON_INSTALL OR WITH_PYTHON_NUMPY)
  set(PYTHON_NUMPY_PATH "" CACHE PATH "\
Path to python site-packages or dist-packages containing 'numpy' module"
  )
  mark_as_advanced(PYTHON_NUMPY_PATH)
  set(PYTHON_NUMPY_INCLUDE_DIRS "" CACHE PATH "Path to the include directory of the NumPy module")
  mark_as_advanced(PYTHON_NUMPY_INCLUDE_DIRS)
endif()
if(WITH_PYTHON_INSTALL)
  option(WITH_PYTHON_INSTALL_NUMPY "Copy system NumPy into the blender install directory" ON)

  if(UNIX AND NOT APPLE)
    option(WITH_PYTHON_INSTALL_REQUESTS "\
Copy system requests into the blender install directory"
      ON
    )
    set(PYTHON_REQUESTS_PATH "" CACHE PATH "\
Path to python site-packages or dist-packages containing 'requests' module"
    )
    mark_as_advanced(PYTHON_REQUESTS_PATH)
  endif()

  option(WITH_PYTHON_INSTALL_ZSTANDARD "Copy zstandard into the blender install directory" ON)
  set(PYTHON_ZSTANDARD_PATH "" CACHE PATH "\
Path to python site-packages or dist-packages containing 'zstandard' module"
  )
  mark_as_advanced(PYTHON_ZSTANDARD_PATH)
endif()

option(WITH_COMPILER_SIMD "Add the recommended Blender default SIMD flags to the compiler's C_FLAGS/CXX_FLAGS" ON)
mark_as_advanced(WITH_COMPILER_SIMD)

# Cycles
option(WITH_CYCLES "Enable Cycles Render Engine" ON)
option(WITH_CYCLES_OSL "Build Cycles with OpenShadingLanguage support" ON)
option(WITH_CYCLES_PATH_GUIDING "Build Cycles with path guiding support" ON)
option(WITH_CYCLES_EMBREE "Build Cycles with Embree support" ON)
option(WITH_CYCLES_DEBUG "Build Cycles with options useful for debugging (e.g., MIS)" OFF)

option(WITH_CYCLES_STANDALONE "Build Cycles standalone application" OFF)
option(WITH_CYCLES_STANDALONE_GUI "Build Cycles standalone with GUI" OFF)
option(WITH_CYCLES_PRECOMPUTE "Build Cycles data precomputation tool" OFF)

option(WITH_CYCLES_HYDRA_RENDER_DELEGATE "Build Cycles Hydra render delegate" OFF)

option(WITH_CYCLES_PARALLEL_DEVICE_KERNEL_BUILD
       "Build Cycles kernels of difference devices in parallel (faster, but uses more memory)" OFF)
mark_as_advanced(WITH_CYCLES_PARALLEL_DEVICE_KERNEL_BUILD)

option(WITH_CYCLES_DEBUG_NAN "\
Build Cycles with additional asserts for detecting NaNs and invalid values"
  OFF
)
option(WITH_CYCLES_NATIVE_ONLY "\
Build Cycles with native kernel only (which fits current CPU, use for development only)"
  OFF
)
option(WITH_CYCLES_KERNEL_ASAN "\
Build Cycles kernels with address sanitizer when WITH_COMPILER_ASAN is on, even if it's very slow"
  OFF
)
set(CYCLES_TEST_DEVICES CPU CACHE STRING "\
Run regression tests on the specified device types \
(CPU CUDA OPTIX HIP HIP-RT METAL METAL-RT ONEAPI ONEAPI-RT)"
)
option(WITH_CYCLES_TEST_OSL "\
Run additional Cycles test with OSL enabled"
  OFF
)
mark_as_advanced(WITH_CYCLES_KERNEL_ASAN)
mark_as_advanced(WITH_CYCLES_DEBUG_NAN)
mark_as_advanced(WITH_CYCLES_NATIVE_ONLY)
mark_as_advanced(WITH_CYCLES_PRECOMPUTE)
mark_as_advanced(CYCLES_TEST_DEVICES)
mark_as_advanced(WITH_CYCLES_TEST_OSL)

# NVIDIA CUDA & OptiX
if(NOT APPLE AND NOT (WIN32 AND CMAKE_SYSTEM_PROCESSOR STREQUAL "ARM64"))
  option(WITH_CYCLES_DEVICE_CUDA "Enable Cycles NVIDIA CUDA compute support" ON)
  option(WITH_CYCLES_DEVICE_OPTIX "Enable Cycles NVIDIA OptiX support" ON)
  mark_as_advanced(WITH_CYCLES_DEVICE_CUDA)

  option(WITH_CYCLES_CUDA_BINARIES "Build Cycles NVIDIA CUDA binaries" OFF)
  set(CYCLES_CUDA_BINARIES_ARCH
    sm_50 sm_52 sm_60 sm_61 sm_70 sm_75 sm_86 sm_89 sm_120 compute_75
    CACHE STRING "CUDA architectures to build binaries for"
  )
  option(WITH_CYCLES_CUDA_BUILD_SERIAL "\
Build cubins one after another (useful on machines with limited RAM)"
    OFF
  )
  option(WITH_CUDA_DYNLOAD "\
Dynamically load CUDA libraries at runtime (for developers, makes cuda-gdb work)"
    ON
  )

  set(OPTIX_ROOT_DIR "" CACHE PATH "\
Path to the OptiX SDK root directory, for building Cycles OptiX kernels."
  )
  set(CYCLES_RUNTIME_OPTIX_ROOT_DIR "" CACHE PATH "\
Path to the OptiX SDK root directory. \
When set, this path will be used at runtime to compile OptiX kernels."
  )

  mark_as_advanced(CYCLES_CUDA_BINARIES_ARCH)
  mark_as_advanced(WITH_CYCLES_CUDA_BUILD_SERIAL)
  mark_as_advanced(WITH_CUDA_DYNLOAD)
  mark_as_advanced(OPTIX_ROOT_DIR)
  mark_as_advanced(CYCLES_RUNTIME_OPTIX_ROOT_DIR)
endif()

# AMD HIP
if(NOT APPLE AND NOT (WIN32 AND CMAKE_SYSTEM_PROCESSOR STREQUAL "ARM64"))
  option(WITH_CYCLES_DEVICE_HIP "Enable Cycles AMD HIP support" ON)
  option(WITH_CYCLES_HIP_BINARIES "Build Cycles AMD HIP binaries" OFF)
  # We only support RDNA1 (gfx101X) and newer.
  # Vega and older generations have rendering artifacts and crashing issues.
  set(CYCLES_HIP_BINARIES_ARCH
    gfx1010 gfx1011 gfx1012
    gfx1030 gfx1031 gfx1032 gfx1034 gfx1035 gfx1036
    gfx1100 gfx1101 gfx1102 gfx1103 gfx1150 gfx1151 gfx1152
    gfx1200 gfx1201
    CACHE STRING "AMD HIP architectures to build binaries for"
  )
  mark_as_advanced(WITH_CYCLES_DEVICE_HIP)
  mark_as_advanced(CYCLES_HIP_BINARIES_ARCH)

  option(WITH_CYCLES_DEVICE_HIPRT "Enable Cycles AMD HIPRT support" OFF)
  mark_as_advanced(WITH_CYCLES_DEVICE_HIPRT)
endif()

# Apple Metal
if(APPLE)
  option(WITH_CYCLES_DEVICE_METAL "Enable Cycles Apple Metal compute support" ON)
endif()

# oneAPI
if(NOT APPLE AND NOT (WIN32 AND CMAKE_SYSTEM_PROCESSOR STREQUAL "ARM64"))
  option(WITH_CYCLES_DEVICE_ONEAPI "Enable Cycles oneAPI compute support" OFF)
  option(WITH_CYCLES_ONEAPI_BINARIES "\
Enable Ahead-Of-Time compilation for Cycles oneAPI device"
    OFF
  )
  option(WITH_CYCLES_ONEAPI_HOST_TASK_EXECUTION "\
Switch target of oneAPI implementation from SYCL devices to Host Task (single thread on CPU). \
This option is only for debugging purposes."
    OFF
  )

  # https://www.intel.com/content/www/us/en/develop/documentation/oneapi-dpcpp-cpp-compiler-dev-guide-and-reference/top/compilation/ahead-of-time-compilation.html
  # The supported devices can be retrieved from `ocloc` output when running
  # `ocloc compile --help`.
  # If you have completed optimization work and now want to enable AoT for new Intel devices,
  # update the optimization status in OneapiDevice::architecture_information.
  set(CYCLES_ONEAPI_INTEL_BINARIES_ARCH dg2 mtl lnl bmg ptl CACHE STRING "\
oneAPI Intel GPU architectures to build binaries for"
  )
  set(CYCLES_ONEAPI_SYCL_TARGETS spir64 spir64_gen CACHE STRING "\
oneAPI targets to build AOT binaries for"
  )

  mark_as_advanced(WITH_CYCLES_ONEAPI_HOST_TASK_EXECUTION)
  mark_as_advanced(CYCLES_ONEAPI_INTEL_BINARIES_ARCH)
  mark_as_advanced(CYCLES_ONEAPI_SYCL_TARGETS)
endif()

# Draw Manager
option(WITH_DRAW_DEBUG "Add extra debug capabilities to Draw Manager" OFF)
mark_as_advanced(WITH_DRAW_DEBUG)

# LLVM
option(WITH_LLVM "Use LLVM" OFF)
option(LLVM_STATIC "Link with LLVM static libraries" OFF)
mark_as_advanced(LLVM_STATIC)
option(WITH_CLANG "Use Clang" OFF)

# currently only used for BLI_mempool
option(WITH_MEM_VALGRIND "Enable extended valgrind support for better reporting" OFF)
mark_as_advanced(WITH_MEM_VALGRIND)

option(WITH_ASSERT_ABORT "Call abort() when raising an assertion through BLI_assert()" ON)
mark_as_advanced(WITH_ASSERT_ABORT)

option(WITH_ASSERT_RELEASE "Build with asserts enabled even for non-debug configurations" OFF)
mark_as_advanced(WITH_ASSERT_RELEASE)

if(UNIX OR (CMAKE_GENERATOR MATCHES "^Visual Studio.+"))
  option(WITH_CLANG_TIDY "\
Use Clang Tidy to analyze the source code \
(only enable for development on Linux using Clang, or Windows using the Visual Studio IDE)"
    OFF
  )
  mark_as_advanced(WITH_CLANG_TIDY)
endif()

option(WITH_BOOST "Enable features depending on boost" ON)
option(WITH_TBB "\
Enable multi-threading. TBB is also required for features such as Cycles, OpenVDB and USD"
  ON
)

# TBB malloc is only supported:
# - On MS-Windows.
# - On Unix/Linux.
if((WIN32 AND NOT CMAKE_SYSTEM_PROCESSOR STREQUAL "ARM64") OR (UNIX AND NOT APPLE))
  option(WITH_TBB_MALLOC_PROXY "Enable the TBB malloc replacement" ON)
endif()

option(WITH_EXPERIMENTAL_FEATURES "Enable experimental features" ON)

# This should be turned off when Blender enter beta/rc/release
if("${BLENDER_VERSION_CYCLE}" STREQUAL "alpha" AND WITH_EXPERIMENTAL_FEATURES)
  set(WITH_EXPERIMENTAL_FEATURES ON)
else()
  set(WITH_EXPERIMENTAL_FEATURES OFF)
endif()

# Unit testing
option(WITH_GTESTS "Enable GTest unit testing" OFF)
option(WITH_GPU_RENDER_TESTS "\
Enable GPU render related unit testing (EEVEE, Workbench and Grease Pencil)"
  OFF
)
option(WITH_GPU_RENDER_TESTS_VULKAN "\
Enable GPU render related unit testing using Vulkan"
  OFF
)
mark_as_advanced(WITH_GPU_RENDER_TESTS_VULKAN)
option(WITH_GPU_RENDER_TESTS_HEADED "\
Enable GPU render related unit testing that doesn't work in background mode. \
These tests require an environment with a display server to run. \
Requires WITH_GPU_RENDER_TESTS "
  OFF
)
mark_as_advanced(WITH_GPU_RENDER_TESTS_HEADED)
option(WITH_GPU_BACKEND_TESTS "\
Enable GPU backend related unit testing"
  OFF
)
option(WITH_GPU_DRAW_TESTS "\
Enable GPU drawing related unit testing (draw manager)"
  OFF
)
option(WITH_GPU_COMPOSITOR_TESTS "Enable regression testing for GPU compositor" OFF)
option(WITH_GPU_MESH_PAINT_TESTS "\
Enable visual render-based regression testing for mesh painting"
  OFF
)
option(WITH_UI_TESTS "\
Enable user-interface tests (Experimental)"
  OFF
)
if(UNIX AND NOT (APPLE OR HAIKU))
  # Headless is preferable as opening windows on the developers computer
  # prevents the system being used while tests run. It's also unreliable,
  # where tiling compositors ignore the window size causing some tests to fail.
  set(_option_default OFF)
  if(WITH_GHOST_WAYLAND)
    set(_option_default ON)
  endif()
  option(WITH_UI_TESTS_HEADLESS "\
Enable user-interface tests using a headless display server. \
Currently this depends on WITH_GHOST_WAYLAND and the weston compositor \
(Experimental)"
    ${_option_default}
  )
  unset(_option_default)

  option(WITH_LINUX_OFFICIAL_RELEASE_TESTS "\
Enable sanity check tests for the linux official release. \
These test are only relevant for checking that our official Linux releases are portable and \
packaged properly. For example that we don't link to any unexpected system libraries."
    OFF
  )
else()
  set(WITH_UI_TESTS_HEADLESS OFF)
  # TODO: We should probably add more sanity checks for Windows and Mac as well
  set(WITH_LINUX_OFFICIAL_RELEASE_TESTS OFF)
endif()

option(WITH_TESTS_EXPERIMENTAL "\
Run tests marked as experimental. \
These tests are labeled as such due to long runtime, flakey results,
or other issues that make them unsuitable for blocking a build on."
  OFF
)
mark_as_advanced(WITH_TESTS_EXPERIMENTAL)

# Enabled by default for typical use cases to speed up development cycles. However, when looking
# into threading or memory related issues (in dependency graph, out-of-bounds, etc) forcing single
# test per Blender instance could give much better clues about the root of the problem.
option(WITH_TESTS_BATCHED "\
Run multiple tests in a single Blender invocation, for faster test execution"
  ON
)
mark_as_advanced(WITH_TESTS_BATCHED)

option(WITH_TESTS_SINGLE_BINARY "\
Link GTest tests into a single binary. \
For faster overall build and less disk space, but slower individual test build"
  ON
)
mark_as_advanced(WITH_TESTS_SINGLE_BINARY)

# NOTE: All callers of this must add `TEST_PYTHON_EXE_EXTRA_ARGS` before any other arguments.
set(TEST_PYTHON_EXE "" CACHE PATH "Python executable to run unit tests")
mark_as_advanced(TEST_PYTHON_EXE)


# System python tests.
option(WITH_SYSTEM_PYTHON_TESTS "\
Enable tests validating some build-related scripts against the 'system' version of Python \
(buildbots currently can use significantly older versions of Python than Blender's)"
  OFF
)
mark_as_advanced(WITH_SYSTEM_PYTHON_TESTS)
# We could use `find_package (Python3 COMPONENTS Interpreter)` to set that value automatically.
# However, on some buildbots this will give the default Python version of the current virtual
# environment, which may differ from the OS default Python version.
# And it would set that global `python3 exec path` CMake value for all CMake scripts,
# which could have unexpected and dangerous side effects.
# So this has to be set explicitly for all builders.
set(TEST_SYSTEM_PYTHON_EXE "" CACHE PATH "Python executable used to run 'system python' tests")
mark_as_advanced(TEST_SYSTEM_PYTHON_EXE)


# Documentation
if(UNIX AND NOT APPLE)
  option(WITH_DOC_MANPAGE "Create a manual page (Unix manpage)" OFF)
endif()


# GPU Module
option(WITH_RENDERDOC "Use Renderdoc API to capture frames" OFF)

option(WITH_GPU_SHADER_ASSERT "\
Globally enable in-shader asserts.
(Requires a debug build or setting GPU_FORCE_ENABLE_SHADER_PRINTF to 1)"
  OFF
)

mark_as_advanced(
  WITH_RENDERDOC
  WITH_GPU_SHADER_ASSERT
)

if(POLICY CMP0119)
  option(WITH_GPU_SHADER_CPP_COMPILATION "\
  Compiler shaders using C++. \
  Allows testing Metal compilation on other platform and enable C++ IDE support for shader code"
    OFF
  )
  mark_as_advanced(WITH_GPU_SHADER_CPP_COMPILATION)
else()
  set(WITH_GPU_SHADER_CPP_COMPILATION OFF)
endif()


# OpenGL
if(NOT APPLE)
  option(WITH_OPENGL_BACKEND "Enable OpenGL support as graphic backend" ON)
  mark_as_advanced(WITH_OPENGL_BACKEND)
else()
  set(WITH_OPENGL_BACKEND OFF)
endif()

# Vulkan
if(NOT APPLE)
  option(WITH_VULKAN_BACKEND "Enable Vulkan as graphics backend" ON)
  mark_as_advanced(WITH_VULKAN_BACKEND)
else()
  option(WITH_VULKAN_BACKEND "Enable Vulkan as graphics backend (development option)" OFF)
  mark_as_advanced(WITH_VULKAN_BACKEND)
endif()

# Metal
if(APPLE)
  option(WITH_METAL_BACKEND "\
Use Metal for graphics instead of (or as well as) OpenGL on macOS."
    ON
  )
  mark_as_advanced(WITH_METAL_BACKEND)
else()
  set(WITH_METAL_BACKEND OFF)
endif()

if(WIN32)
  getDefaultWindowsPrefixBase(CMAKE_GENERIC_PROGRAM_FILES)
  set(CPACK_INSTALL_PREFIX ${CMAKE_GENERIC_PROGRAM_FILES}/${})
endif()

option(WITH_STRSIZE_DEBUG "\
Ensure string operations on fixed size buffers \
(works well with \"WITH_COMPILER_ASAN\" & valgrind to detect incorrect buffer size arguments)"
  OFF)
mark_as_advanced(WITH_STRSIZE_DEBUG)

# Compiler tool-chain.
if(UNIX AND NOT APPLE)
  if(CMAKE_COMPILER_IS_GNUCC)
    option(WITH_LINKER_GOLD "Use ld.gold linker which is usually faster than ld.bfd" ON)
    mark_as_advanced(WITH_LINKER_GOLD)
  endif()
  if(CMAKE_COMPILER_IS_GNUCC OR CMAKE_C_COMPILER_ID MATCHES "Clang")
    option(WITH_LINKER_LLD "Use ld.lld linker which is usually faster than ld.gold" OFF)
    mark_as_advanced(WITH_LINKER_LLD)
    option(WITH_LINKER_MOLD "Use ld.mold linker which is usually faster than ld.gold & ld.lld" OFF)
    mark_as_advanced(WITH_LINKER_MOLD)
  endif()
endif()

option(WITH_COMPILER_ASAN "\
Build and link against address sanitizer (only for Debug & RelWithDebInfo targets)."
  OFF
)
mark_as_advanced(WITH_COMPILER_ASAN)
option(WITH_COMPILER_ASAN_EXTERN "\
Build `extern` dependencies with address sanitizer when WITH_COMPILER_ASAN is on. \
Can cause linking issues due to too large binary size."
  OFF
)
mark_as_advanced(WITH_COMPILER_ASAN_EXTERN)
# Note: Using tbbmalloc allocator seems to be fully compatible with current sanitizers
# (both clang/llvm and gcc ones).
set_and_warn_incompatible(WITH_COMPILER_ASAN WITH_MEM_VALGRIND OFF)
set_and_warn_incompatible(WITH_COMPILER_ASAN_EXTERN WITH_MEM_VALGRIND OFF)

option(WITH_COMPILER_CODE_COVERAGE "\
Build and link with code coverage support (only for Debug targets)."
  OFF
)
mark_as_advanced(WITH_COMPILER_CODE_COVERAGE)

if(WITH_COMPILER_CODE_COVERAGE)
  if(CMAKE_COMPILER_IS_GNUCC)
    set(_code_coverage_defaults "--coverage")
  elseif(CMAKE_C_COMPILER_ID MATCHES "Clang")
    get_filename_component(COMPILER_DIRECTORY ${CMAKE_CXX_COMPILER} DIRECTORY)
    find_program(LLVM_COV "llvm-cov" ${COMPILER_DIRECTORY} NO_DEFAULT_PATH)
    find_program(LLVM_PROFDATA "llvm-profdata" ${COMPILER_DIRECTORY} NO_DEFAULT_PATH)
    if(NOT LLVM_COV OR NOT LLVM_PROFDATA)
      message(WARNING
        "Could not find code coverage tools, disabling code coverage. "
        "You may explicitly specify LLVM_COV and LLVM_PROFDATA to work around this warning."
      )
      set(WITH_COMPILER_CODE_COVERAGE OFF)
    else()
      set(_code_coverage_defaults "-fprofile-instr-generate -fcoverage-mapping")
    endif()
  else()
    message(WARNING
      "Unsupported compiler ${CMAKE_C_COMPILER_ID} for WITH_COMPILER_CODE_COVERAGE, disabling."
    )
    set(WITH_COMPILER_CODE_COVERAGE OFF)
  endif()

  # The code above could have disabled the feature, so check again.
  if(WITH_COMPILER_CODE_COVERAGE)
    set(COMPILER_CODE_COVERAGE_CFLAGS
      ${_code_coverage_defaults} CACHE STRING
      "C flags for code coverage"
    )
    mark_as_advanced(COMPILER_CODE_COVERAGE_CFLAGS)
    set(COMPILER_CODE_COVERAGE_CXXFLAGS
      ${_code_coverage_defaults} CACHE STRING
      "C++ flags for code coverage"
    )
    mark_as_advanced(COMPILER_CODE_COVERAGE_CXXFLAGS)
    if(CMAKE_C_COMPILER_ID MATCHES "Clang")
      set(_code_coverage_dir_default "${CMAKE_BINARY_DIR}/coverage")
      set(COMPILER_CODE_COVERAGE_DATA_DIR ${_code_coverage_dir_default} CACHE STRING
        "Directory for code coverage data"
      )
      mark_as_advanced(COMPILER_CODE_COVERAGE_DATA_DIR)
      unset(_code_coverage_dir_default)
    endif()
    unset(_code_coverage_defaults)
  endif()
endif()

if(CMAKE_COMPILER_IS_GNUCC OR CMAKE_C_COMPILER_ID MATCHES "Clang")
  if(WITH_COMPILER_ASAN)
    set(_asan_defaults "\
-fsanitize=address \
-fsanitize=bool \
-fsanitize=bounds \
-fsanitize=enum \
-fsanitize=float-cast-overflow \
-fsanitize=float-divide-by-zero \
-fsanitize=nonnull-attribute \
-fsanitize=returns-nonnull-attribute \
-fsanitize=signed-integer-overflow \
-fsanitize=undefined \
-fsanitize=vla-bound \
-fno-sanitize=alignment \
")

    if(MSVC)
      # clang-cl doesn't support all sanitizers, but leak and object-size give errors/warnings.
      set(_asan_defaults "${_asan_defaults}")
    elseif(APPLE)
      # AppleClang doesn't support all sanitizers, but leak gives error.
      # Build type is not known for multi-config generator, so don't add object-size sanitizer.
      if(CMAKE_BUILD_TYPE MATCHES "Debug" OR GENERATOR_IS_MULTI_CONFIG)
        # Silence the warning that object-size is not effective in -O0.
        set(_asan_defaults "${_asan_defaults}")
      else()
        string(APPEND _asan_defaults " -fsanitize=object-size")
      endif()
    elseif(CMAKE_COMPILER_IS_GNUCC)
      string(APPEND _asan_defaults " -fsanitize=leak -fsanitize=object-size")
    else()
      string(APPEND _asan_defaults " -fsanitize=leak")
    endif()

    set(COMPILER_ASAN_CFLAGS "${_asan_defaults}" CACHE STRING "C flags for address sanitizer")
    mark_as_advanced(COMPILER_ASAN_CFLAGS)
    set(COMPILER_ASAN_CXXFLAGS "${_asan_defaults}" CACHE STRING "C++ flags for address sanitizer")
    mark_as_advanced(COMPILER_ASAN_CXXFLAGS)

    unset(_asan_defaults)

    if(MSVC)
      find_library(
        COMPILER_ASAN_LIBRARY NAMES clang_rt.asan-x86_64
        PATHS
        [HKEY_LOCAL_MACHINE\\SOFTWARE\\Wow6432Node\\LLVM\\LLVM;]/lib/clang/7.0.0/lib/windows
        [HKEY_LOCAL_MACHINE\\SOFTWARE\\Wow6432Node\\LLVM\\LLVM;]/lib/clang/6.0.0/lib/windows
      )
      mark_as_advanced(COMPILER_ASAN_LIBRARY)
    elseif(APPLE)
      execute_process(COMMAND ${CMAKE_CXX_COMPILER}
        -print-file-name=lib
        OUTPUT_VARIABLE CLANG_LIB_DIR
      )
      string(STRIP "${CLANG_LIB_DIR}" CLANG_LIB_DIR)
      find_library(
        COMPILER_ASAN_LIBRARY
        NAMES
          libclang_rt.asan_osx_dynamic.dylib
        PATHS
          "${CLANG_LIB_DIR}/darwin/"
      )
      unset(CLANG_LIB_DIR)
      mark_as_advanced(COMPILER_ASAN_LIBRARY)
    elseif(CMAKE_COMPILER_IS_GNUCC)
      find_library(
        COMPILER_ASAN_LIBRARY asan ${CMAKE_C_IMPLICIT_LINK_DIRECTORIES}
      )
      mark_as_advanced(COMPILER_ASAN_LIBRARY)
    endif()

  endif()
endif()

if(CMAKE_COMPILER_IS_GNUCC OR CMAKE_C_COMPILER_ID MATCHES "Clang")
  option(WITH_COMPILER_SHORT_FILE_MACRO "\
Make paths in macros like __FILE__ relative to top level source and build directories."
    ON
  )
  mark_as_advanced(WITH_COMPILER_SHORT_FILE_MACRO)
endif()

if(WIN32)
  # Use hardcoded paths or find_package to find externals
  option(WITH_WINDOWS_FIND_MODULES "Use find_package to locate libraries" OFF)
  mark_as_advanced(WITH_WINDOWS_FIND_MODULES)

  # The python debugger in Visual Studio for has been broken for years
  # but the upstream project over at https://github.com/microsoft/PTVS
  # show hopeful signs of life once in a while, so there is hope that
  # at one point this will start working again. That being said people
  # do keep turning this option on and end up disappointed it isn't
  # working and they spend a whole bunch of time on trying to get it to
  # work. So for now rather than removing this functionality
  # completely, just disable it.

  if(WINDOWS_PYTHON_DEBUG)
    # No need for a python version check here, anything over python 3.6 isn't working.
    message(WARNING "Unsupported python version for VS debugger, disabling WINDOWS_PYTHON_DEBUG")
    set(WINDOWS_PYTHON_DEBUG OFF)
  endif()

  # option(WINDOWS_PYTHON_DEBUG "\
  # Include the files needed for debugging python scripts with visual studio 2017+."
  #   OFF
  # )
  # mark_as_advanced(WINDOWS_PYTHON_DEBUG)

  option(WITH_WINDOWS_BUNDLE_CRT "Bundle the C runtime for install free distribution." ON)
  mark_as_advanced(WITH_WINDOWS_BUNDLE_CRT)
  if(WITH_WINDOWS_BUNDLE_CRT AND
     CMAKE_VERSION VERSION_LESS "4.2.0" AND
     MSVC_VERSION GREATER_EQUAL 1950)
    message(WARNING "VS2026 not supported on this CMAKE version: WITH_WINDOWS_BUNDLE_CRT will be disabled")
    set(WITH_WINDOWS_BUNDLE_CRT OFF)
  endif()
  option(WITH_WINDOWS_EXTERNAL_MANIFEST "Use external manifest files" OFF)
  mark_as_advanced(WITH_WINDOWS_EXTERNAL_MANIFEST)

  option(WITH_WINDOWS_SCCACHE "Use sccache to speed up builds (Ninja builder only)" OFF)
  mark_as_advanced(WITH_WINDOWS_SCCACHE)

  option(WITH_WINDOWS_RELEASE_PDB "\
Generate a pdb file for client side stacktraces for release builds"
    ON
  )
  mark_as_advanced(WITH_WINDOWS_RELEASE_PDB)

  option(WITH_WINDOWS_RELEASE_STRIPPED_PDB "Use a stripped PDB file for release builds" ON)
  mark_as_advanced(WITH_WINDOWS_RELEASE_STRIPPED_PDB)

endif()

if(WIN32 OR XCODE)
  option(IDE_GROUP_SOURCES_IN_FOLDERS "\
Organize the source files in filters matching the source directory."
    ON
  )
  mark_as_advanced(IDE_GROUP_SOURCES_IN_FOLDERS)

  option(IDE_GROUP_PROJECTS_IN_FOLDERS "\
Organize the projects according to source directory structure."
    ON
  )
  mark_as_advanced(IDE_GROUP_PROJECTS_IN_FOLDERS)

  if(IDE_GROUP_PROJECTS_IN_FOLDERS)
    set_property(GLOBAL PROPERTY USE_FOLDERS ON)
  endif()
endif()

if(UNIX)
  # See WITH_WINDOWS_SCCACHE for Windows.
  option(WITH_COMPILER_CCACHE "\
Use ccache to improve rebuild times (Works with Ninja, Makefiles and Xcode)"
    OFF
  )
  mark_as_advanced(WITH_COMPILER_CCACHE)
endif()

# The following only works with the Ninja generator in CMake >= 3.0.
if("${CMAKE_GENERATOR}" MATCHES "Ninja")
  option(WITH_NINJA_POOL_JOBS "\
Enable Ninja pools of jobs, to try to ease building on machines with 16GB of RAM or less \
(if not yet defined, will try to set best values based on detected machine specifications)."
    ON
  )
  mark_as_advanced(WITH_NINJA_POOL_JOBS)
endif()

# Installation process.
set(POSTINSTALL_SCRIPT "" CACHE FILEPATH "Run given CMake script after installation process")
mark_as_advanced(POSTINSTALL_SCRIPT)

set(POSTCONFIGURE_SCRIPT "" CACHE FILEPATH "\
Run given CMake script as the last step of CMake configuration"
)
mark_as_advanced(POSTCONFIGURE_SCRIPT)

# end option(...)



# By default we want to install to the directory we are compiling our executables
# unless specified otherwise, which we currently do not allow
if(CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT)
  if(WIN32)
    set(CMAKE_INSTALL_PREFIX ${EXECUTABLE_OUTPUT_PATH}/\${BUILD_TYPE} CACHE PATH "\
default install path"
      FORCE
    )
  elseif(APPLE)
    set(CMAKE_INSTALL_PREFIX ${EXECUTABLE_OUTPUT_PATH}/\${BUILD_TYPE} CACHE PATH "\
default install path"
      FORCE
    )
  else()
    if(WITH_INSTALL_PORTABLE)
      set(CMAKE_INSTALL_PREFIX ${EXECUTABLE_OUTPUT_PATH} CACHE PATH "default install path" FORCE)
    endif()
  endif()
endif()

# Effective install path including config directory, as a generator expression.
get_property(GENERATOR_IS_MULTI_CONFIG GLOBAL PROPERTY GENERATOR_IS_MULTI_CONFIG)
if(GENERATOR_IS_MULTI_CONFIG)
  string(
    REPLACE "\${BUILD_TYPE}" "$<CONFIG>"
    CMAKE_INSTALL_PREFIX_WITH_CONFIG ${CMAKE_INSTALL_PREFIX}
  )
else()
  string(
    REPLACE "\${BUILD_TYPE}" ""
    CMAKE_INSTALL_PREFIX_WITH_CONFIG ${CMAKE_INSTALL_PREFIX}
  )
endif()


# Apple
if(APPLE)
  include(platform_apple_xcode)

  option(WITH_LEGACY_MACOS_X64_LINKER "\
Use legacy macOS linker on x64 platform. Minutes slower, but emits fewer warnings."
    OFF
  )
  mark_as_advanced(WITH_LEGACY_MACOS_X64_LINKER)
endif()

# -----------------------------------------------------------------------------
# Check for Conflicting/Unsupported Configurations

option(WITH_STRICT_BUILD_OPTIONS "\
When requirements for a build option are not met, error instead of disabling the option."
  OFF
)

if(NOT WITH_BLENDER AND NOT WITH_CYCLES_STANDALONE AND NOT WITH_CYCLES_HYDRA_RENDER_DELEGATE)
  message(FATAL_ERROR
    "At least one of WITH_BLENDER or WITH_CYCLES_STANDALONE "
    "or WITH_CYCLES_HYDRA_RENDER_DELEGATE "
    "must be enabled, nothing to do!"
  )
endif()

# Doesn't make too much sense to check if the CPU supports our default SIMD options if they are
# turned off.
set_and_warn_dependency(WITH_COMPILER_SIMD WITH_CPU_CHECK OFF)

set_and_warn_dependency(WITH_AUDASPACE WITH_OPENAL OFF)
set_and_warn_dependency(WITH_AUDASPACE WITH_COREAUDIO OFF)
set_and_warn_dependency(WITH_AUDASPACE WITH_JACK OFF)
set_and_warn_dependency(WITH_AUDASPACE WITH_PULSEAUDIO OFF)
set_and_warn_dependency(WITH_AUDASPACE WITH_WASAPI OFF)

if(NOT WITH_SDL AND WITH_GHOST_SDL)
  message(FATAL_ERROR "WITH_GHOST_SDL requires WITH_SDL")
endif()

# python module, needs some different options
if(WITH_PYTHON_MODULE AND WITH_PYTHON_INSTALL)
  message(FATAL_ERROR "WITH_PYTHON_MODULE requires WITH_PYTHON_INSTALL to be OFF")
endif()

set_and_warn_dependency(WITH_PYTHON WITH_CYCLES        OFF)
set_and_warn_dependency(WITH_PYTHON WITH_DRACO         OFF)
set_and_warn_dependency(WITH_PYTHON WITH_MOD_FLUID     OFF)

# enable boost for cycles, audaspace or i18n
# otherwise if the user disabled

set_and_warn_dependency(WITH_BOOST WITH_OPENVDB        OFF)
set_and_warn_dependency(WITH_BOOST WITH_USD            OFF)
if(WITH_CYCLES)
  set_and_warn_dependency(WITH_BOOST   WITH_CYCLES_OSL          OFF)
  set_and_warn_dependency(WITH_PUGIXML WITH_CYCLES_OSL          OFF)
  set_and_warn_dependency(WITH_PUGIXML WITH_CYCLES_STANDALONE   OFF)
  set_and_warn_dependency(WITH_CYCLES_OSL WITH_CYCLES_TEST_OSL  OFF)
endif()

set_and_warn_dependency(WITH_TBB WITH_CYCLES            OFF)
set_and_warn_dependency(WITH_TBB WITH_USD               OFF)
set_and_warn_dependency(WITH_TBB WITH_OPENVDB           OFF)
set_and_warn_dependency(WITH_TBB WITH_MOD_FLUID         OFF)

# NanoVDB requires OpenVDB to convert the data structure
set_and_warn_dependency(WITH_OPENVDB WITH_NANOVDB       OFF)

# OpenVDB, Alembic and OSL uses `half` or `imath` from OpenEXR
set_and_warn_dependency(WITH_IMAGE_OPENEXR WITH_OPENVDB OFF)
set_and_warn_dependency(WITH_IMAGE_OPENEXR WITH_ALEMBIC OFF)
set_and_warn_dependency(WITH_IMAGE_OPENEXR WITH_CYCLES_OSL OFF)

# Hydra requires USD.
set_and_warn_dependency(WITH_USD WITH_HYDRA OFF)

# Ocean modifier and Rubberband require FFTW3.
set_and_warn_dependency(WITH_FFTW3 WITH_MOD_OCEANSIM OFF)
set_and_warn_dependency(WITH_FFTW3 WITH_RUBBERBAND OFF)

if(NOT WITH_CYCLES)
  set(WITH_CYCLES_OSL OFF)
endif()

# don't store paths to libs for portable distribution
if(WITH_INSTALL_PORTABLE)
  set(CMAKE_SKIP_BUILD_RPATH TRUE)
endif()

if(UNIX AND NOT (APPLE OR HAIKU))
  set_and_warn_incompatible(WITH_HEADLESS WITH_GHOST_WAYLAND OFF)
  set_and_warn_incompatible(WITH_HEADLESS WITH_GHOST_X11 OFF)
endif()
set_and_warn_incompatible(WITH_HEADLESS WITH_GHOST_SDL OFF)

if(WITH_INPUT_IME)
  set_and_warn_incompatible(WITH_HEADLESS WITH_INPUT_IME OFF)
  set_and_warn_incompatible(WITH_GHOST_SDL WITH_INPUT_IME OFF)
endif()

set_and_warn_incompatible(WITH_HEADLESS WITH_XR_OPENXR OFF)
set_and_warn_incompatible(WITH_GHOST_SDL WITH_XR_OPENXR OFF)

if(WITH_UI_TESTS_HEADLESS)
  set_and_warn_dependency(WITH_GHOST_WAYLAND WITH_UI_TESTS_HEADLESS OFF)
endif()

if(WITH_BUILDINFO)
  find_package(Git)
  set_and_warn_library_found("Git" GIT_FOUND WITH_BUILDINFO)
endif()

if(WITH_AUDASPACE)
  if(NOT WITH_SYSTEM_AUDASPACE)
    set(AUDASPACE_C_INCLUDE_DIRS
      "${CMAKE_SOURCE_DIR}/extern/audaspace/bindings/C"
      "${CMAKE_BINARY_DIR}/extern/audaspace"
    )
    set(AUDASPACE_PY_INCLUDE_DIRS
      "${CMAKE_SOURCE_DIR}/extern/audaspace/bindings"
    )
  endif()
endif()

# Auto-enable CUDA dynload if toolkit is not found.
if(WITH_CYCLES AND WITH_CYCLES_DEVICE_CUDA AND NOT WITH_CUDA_DYNLOAD)
  find_package(CUDA)
  if(NOT CUDA_FOUND)
    message(
      STATUS
      "CUDA toolkit not found, "
      "using dynamic runtime loading of libraries (WITH_CUDA_DYNLOAD) instead"
    )
    set(WITH_CUDA_DYNLOAD ON)
  endif()
endif()

if(WITH_CYCLES_DEVICE_HIP)
  # Currently HIP must be dynamically loaded, this may change in future toolkits
  set(WITH_HIP_DYNLOAD ON)
endif()


# -----------------------------------------------------------------------------
# Check if Sub-modules are Cloned

if(WITH_PYTHON)
  # While we have this as an '#error' in 'bpy_capi_utils.hh',
  # upgrading Python tends to cause confusion for users who build.
  # Give the error message early to make this more obvious.
  #
  # Do this before main 'platform_*' checks,
  # because UNIX will search for the old Python paths which may not exist.
  # giving errors about missing paths before this case is met.
  if(DEFINED PYTHON_VERSION AND "${PYTHON_VERSION}" VERSION_LESS "3.11")
    message(
      FATAL_ERROR
      "At least Python 3.11 is required to build, but found Python ${PYTHON_VERSION}"
    )
  endif()
endif()


# -----------------------------------------------------------------------------
# InitialIze Un-cached Vars, Avoid Unused Warning

# linux only, not cached
set(WITH_BINRELOC OFF)

# MACOSX only, set to avoid uninitialized
set(EXETYPE "")

# C/C++ flags
set(PLATFORM_CFLAGS)

# these are added to later on.
set(C_WARNINGS)
set(CXX_WARNINGS)

# NOTE: These flags are intended for situations where it's impractical to
# suppress warnings by modifying the code or for code which is maintained externally.
# For GCC this typically means adding `-Wno-*` arguments to negate warnings
# that are useful in the general case.
set(C_REMOVE_STRICT_FLAGS)
set(CXX_REMOVE_STRICT_FLAGS)

# Libraries to link to targets in setup_platform_linker_libs
set(PLATFORM_LINKLIBS "")

# Added to target linker flags in setup_platform_linker_flags
# - CMAKE_EXE_LINKER_FLAGS
# - CMAKE_EXE_LINKER_FLAGS_DEBUG
set(PLATFORM_LINKFLAGS "")
set(PLATFORM_LINKFLAGS_DEBUG "")
set(PLATFORM_LINKFLAGS_RELEASE "")
set(PLATFORM_LINKFLAGS_EXECUTABLE "")

if(WITH_COMPILER_CODE_COVERAGE)
  if(CMAKE_COMPILER_IS_GNUCC)
    # GCC only supports this on debug builds.
    string(APPEND CMAKE_C_FLAGS_DEBUG " ${COMPILER_CODE_COVERAGE_CFLAGS}")
    string(APPEND CMAKE_CXX_FLAGS_DEBUG " ${COMPILER_CODE_COVERAGE_CXXFLAGS}")
  elseif(CMAKE_C_COMPILER_ID MATCHES "Clang")
    # Clang will allow it on all builds.
    string(APPEND CMAKE_C_FLAGS " ${COMPILER_CODE_COVERAGE_CFLAGS}")
    string(APPEND CMAKE_CXX_FLAGS " ${COMPILER_CODE_COVERAGE_CXXFLAGS}")
  endif()
endif()

if(NOT CMAKE_BUILD_TYPE MATCHES "Release")
  if(WITH_COMPILER_ASAN)
    if(NOT APPLE)
      # Avoid passing address sanitizer compiler flags to `try_compile`.
      # Since linker flags are not set, all compiler checks and `find_package`
      # calls that rely on `try_compile` will fail.
      # See CMP0066 also.
      string(APPEND CMAKE_C_FLAGS_DEBUG " ${COMPILER_ASAN_CFLAGS}")
      string(APPEND CMAKE_C_FLAGS_RELWITHDEBINFO " ${COMPILER_ASAN_CFLAGS}")

      string(APPEND CMAKE_CXX_FLAGS_DEBUG " ${COMPILER_ASAN_CXXFLAGS}")
      string(APPEND CMAKE_CXX_FLAGS_RELWITHDEBINFO " ${COMPILER_ASAN_CXXFLAGS}")
    endif()
    if(MSVC)
      set(COMPILER_ASAN_LINKER_FLAGS "/FUNCTIONPADMIN:6")
    endif()

    if(APPLE AND COMPILER_ASAN_LIBRARY)
      string(REPLACE " " ";" _list_COMPILER_ASAN_CFLAGS ${COMPILER_ASAN_CFLAGS})
      set(_is_CONFIG_DEBUG "$<OR:$<CONFIG:Debug>,$<CONFIG:RelWithDebInfo>>")
      add_compile_options("$<${_is_CONFIG_DEBUG}:${_list_COMPILER_ASAN_CFLAGS}>")

      # Skip generation of the unwind tables, as they might require a lot of space when sanitizers
      # are enabled and not fit into the .eh_frame section. Disabling the unwind tables might have
      # side effects on code which does frame walking, such as
      #   - backtrace()
      #   - __attribute__((__cleanup__(f)))
      #   - __builtin_return_address(n), for n > 0
      #   - pthread_cleanup_push when it is implemented using __attribute__((__cleanup__(f)))
      # It should not have affect on debugging, since it uses -g flag which generates debugging
      # tables in the .debug_frame section.
      # At the time of adding these flags calling backtrace() from C code on Apple M2 did not
      # affect on the printed backtrace, and exception handling was correct as well.
      #
      # Related discussion:
      #  https://stackoverflow.com/questions/26300819
      add_compile_options("$<${_is_CONFIG_DEBUG}:-fno-unwind-tables>")
      add_compile_options("$<${_is_CONFIG_DEBUG}:-fno-asynchronous-unwind-tables>")

      add_compile_options("$<${_is_CONFIG_DEBUG}:-fno-omit-frame-pointer>")
      add_link_options("$<${_is_CONFIG_DEBUG}:-fno-omit-frame-pointer;-fsanitize=address>")
      unset(_list_COMPILER_ASAN_CFLAGS)
      unset(_is_CONFIG_DEBUG)
    elseif(COMPILER_ASAN_LIBRARY)
      set(PLATFORM_LINKLIBS "${PLATFORM_LINKLIBS};${COMPILER_ASAN_LIBRARY}")
      set(PLATFORM_LINKFLAGS "${COMPILER_ASAN_LIBRARY}")
      set(PLATFORM_LINKFLAGS_DEBUG "${COMPILER_ASAN_LIBRARY}")
      if(DEFINED COMPILER_ASAN_LINKER_FLAGS)
        set(PLATFORM_LINKFLAGS "${PLATFORM_LINKFLAGS} ${COMPILER_ASAN_LINKER_FLAGS}")
        set(PLATFORM_LINKFLAGS_DEBUG "${PLATFORM_LINKFLAGS_DEBUG} ${COMPILER_ASAN_LINKER_FLAGS}")
      endif()
    endif()
  endif()
endif()


# ----------------------------------------------------------------------------
# Main Platform Checks
#
# - UNIX
# - WIN32
# - APPLE

if(UNIX AND NOT APPLE)
  include(platform_unix)
elseif(WIN32)
  include(platform_win32)
elseif(APPLE)
  include(platform_apple)
endif()


# -----------------------------------------------------------------------------
# Common Checks for Compatible Options

# Add the default compiler SIMD flags if enabled
#
if(WITH_COMPILER_SIMD)
  set(COMPILER_SSE42_FLAG)
  get_sse_flags(COMPILER_SSE42_FLAG)
  string(APPEND CMAKE_CXX_FLAGS " ${COMPILER_SSE42_FLAG}")
  string(APPEND CMAKE_C_FLAGS " ${COMPILER_SSE42_FLAG}")
endif()

# Enable SSE2NEON SIMD support if found.
#
if(SUPPORTS_NEON_BUILD AND SSE2NEON_FOUND)
  include_directories(SYSTEM "${SSE2NEON_INCLUDE_DIRS}")
  add_definitions(-DWITH_SSE2NEON)
endif()

# Test endianness and set the endian define.
include(TestBigEndian)
test_big_endian(_SYSTEM_BIG_ENDIAN)
if(_SYSTEM_BIG_ENDIAN)
  message(FATAL_ERROR "Blender does not support building on Big Endian systems" )
else()
  add_definitions(-D__LITTLE_ENDIAN__)
endif()
unset(_SYSTEM_BIG_ENDIAN)

if(WITH_IMAGE_OPENJPEG)
  # Special handling of Windows platform where openjpeg is always static.
  if(WIN32)
    set(OPENJPEG_DEFINES "-DOPJ_STATIC")
  else()
    set(OPENJPEG_DEFINES "")
  endif()
endif()

if(NOT WITH_SYSTEM_EIGEN3)
  set(EIGEN3_INCLUDE_DIRS ${CMAKE_SOURCE_DIR}/extern/Eigen3)
endif()
if (WITH_COMPILER_ASAN AND NOT WITH_COMPILER_ASAN_EXTERN)
  # See also:
  # - `EIGEN_MALLOC_ALREADY_ALIGNED` definition and usages in
  #   `./extern/Eigen3/Eigen/src/Core/util/Memory.h`
  # - Report: https://projects.blender.org/blender/blender/issues/152010
  #
  # NOTE: It seems to only affect GCC and its sanitizer library - but best
  #       be on the safe side here, and do that for all compilers and systems.
  message(DEBUG
    "Forcing Eigen to use their own allocator code because `WITH_COMPILER_ASAN` is enabled but "
    "`WITH_COMPILER_ASAN_EXTERN` is disabled, which can cause mismatch usages of Eigen allocators."
  )
  add_definitions(-DEIGEN_MALLOC_ALREADY_ALIGNED=0)
endif()

# -----------------------------------------------------------------------------
# Configure Metal

if(WITH_METAL_BACKEND)
  add_definitions(-DWITH_METAL_BACKEND)

  # No need to add frameworks here, all the ones we need for Metal and
  # Metal-OpenGL Interoperability are already being added by
  # `build_files/cmake/platform/platform_apple.cmake`.
endif()


# -----------------------------------------------------------------------------
# Configure Bullet

if(WITH_BULLET)
  if(WITH_SYSTEM_BULLET)
    find_package(Bullet)
    set_and_warn_library_found("Bullet" BULLET_FOUND WITH_BULLET)
  else()
    set(BULLET_INCLUDE_DIRS "${CMAKE_SOURCE_DIR}/extern/bullet2/src")
    set(BULLET_LIBRARIES "extern_bullet")
  endif()
endif()


# -----------------------------------------------------------------------------
# Configure Python

# Not currently supported due to different required Python link flags.
set_and_warn_incompatible(WITH_PYTHON_MODULE WITH_GTESTS OFF)


# -----------------------------------------------------------------------------
# Configure `GLog/GFlags`

if(WITH_LIBMV OR WITH_GTESTS)
  if(WITH_SYSTEM_GFLAGS)
    find_package(Gflags)
    if(NOT GFLAGS_FOUND)
      message(FATAL_ERROR "System wide Gflags is requested but was not found")
    endif()
    # `FindGflags` does not define this, and we are not even sure what to use here.
    set(GFLAGS_DEFINES)
  else()
    set(GFLAGS_DEFINES
      -DGFLAGS_DLL_DEFINE_FLAG=
      -DGFLAGS_DLL_DECLARE_FLAG=
      -DGFLAGS_DLL_DECL=
    )
    set(GFLAGS_NAMESPACE "gflags")
    set(GFLAGS_LIBRARIES extern_gflags)
    set(GFLAGS_INCLUDE_DIRS "${PROJECT_SOURCE_DIR}/extern/gflags/src")
  endif()

  if(WITH_SYSTEM_GLOG)
    find_package(Glog)
    if(NOT GLOG_FOUND)
      message(FATAL_ERROR "System wide Glog is requested but was not found")
    endif()
    # `FindGlog` does not define this, and we are not even sure what to use here.
    set(GLOG_DEFINES)
  else()
    set(GLOG_DEFINES
      -DGOOGLE_GLOG_DLL_DECL=
    )
    set(GLOG_LIBRARIES extern_glog)
    if(WIN32)
      set(GLOG_INCLUDE_DIRS ${CMAKE_SOURCE_DIR}/extern/glog/src/windows)
    else()
      set(GLOG_INCLUDE_DIRS ${CMAKE_SOURCE_DIR}/extern/glog/include)
    endif()
  endif()
endif()

# -----------------------------------------------------------------------------
# Common dependency targets

include(dependency_targets)

# -----------------------------------------------------------------------------
# Ninja Job Limiting

# Extra limits to number of jobs running in parallel for some kind OS tasks.
# Only supported by Ninja build system currently.

if("${CMAKE_GENERATOR}" MATCHES "Ninja" AND WITH_NINJA_POOL_JOBS)
  message(STATUS "Using NINJA_POOL_JOBS:")

  if(NOT NINJA_MAX_NUM_PARALLEL_COMPILE_JOBS)
    set(NINJA_MAX_NUM_PARALLEL_COMPILE_JOBS "0" CACHE STRING "\
Define the maximum number of concurrent regular compilation jobs, for ninja build system. \
Keep at '0' to automatically compute optimal values for each build."
      FORCE
    )
    mark_as_advanced(NINJA_MAX_NUM_PARALLEL_COMPILE_JOBS)
  endif()

  if(NOT NINJA_MAX_NUM_PARALLEL_COMPILE_HEAVY_JOBS)
    set(NINJA_MAX_NUM_PARALLEL_COMPILE_HEAVY_JOBS "0" CACHE STRING "\
Define the maximum number of concurrent memory-heavy compilation jobs, for ninja build system. \
Keep at '0' to automatically compute optimal values for each build."
      FORCE
    )
    mark_as_advanced(NINJA_MAX_NUM_PARALLEL_COMPILE_HEAVY_JOBS)
  endif()

  if(NOT NINJA_MAX_NUM_PARALLEL_LINK_JOBS)
    set(NINJA_MAX_NUM_PARALLEL_LINK_JOBS "0" CACHE STRING "\
Define the maximum number of concurrent link jobs, for ninja build system. \
Keep at '0' to automatically compute optimal values for each build."
      FORCE
    )
    mark_as_advanced(NINJA_MAX_NUM_PARALLEL_LINK_JOBS)
  endif()

  # Try to compute good default values,
  # unless some are enforced in the user-exposed CMake cached variables.

  cmake_host_system_information(RESULT _NUM_CORES QUERY NUMBER_OF_LOGICAL_CORES)
  # Note: this gives mem in MB.
  cmake_host_system_information(RESULT _TOT_MEM QUERY TOTAL_PHYSICAL_MEMORY)

  set(_link_jobs 0)
  if(NINJA_MAX_NUM_PARALLEL_LINK_JOBS)
    set(_link_jobs ${NINJA_MAX_NUM_PARALLEL_LINK_JOBS})
    message(STATUS
      "  NINJA_MAX_NUM_PARALLEL_LINK_JOBS: "
      "max concurrent linking jobs (from settings): ${_link_jobs}"
    )
  endif()
  if(${_link_jobs} LESS 1)
    # Heuristics: Maximum amount of memory needed per linking task.
    # Note: These values are purposely over-estimated by 10-15% at least, to account
    #       for other types of jobs' memory usage, other system memory usages, etc.
    set(_link_mem 10000)
    if(WITH_COMPILER_ASAN)
      set(_link_mem 30000)
    elseif("${CMAKE_BUILD_TYPE}" MATCHES "Debug")
      set(_link_mem 20000)
    endif()

    # In practice, even when there is RAM available,
    # more than 2 linking tasks running in parallel gets slower (due to slow disks accesses).
    if(${_NUM_CORES} GREATER 7)
      set(_link_jobs_max 2)
    else()
      set(_link_jobs_max 1)
    endif()

    math(EXPR _link_jobs "${_TOT_MEM} / ${_link_mem}")
    if(${_link_jobs} GREATER ${_link_jobs_max})
      set(_link_jobs ${_link_jobs_max})
    elseif(${_link_jobs} LESS 1)
      set(_link_jobs 1)
    endif()

    message(STATUS
      "  NINJA_MAX_NUM_PARALLEL_LINK_JOBS: "
      "max concurrent linking jobs (computed): ${_link_jobs}"
    )
    set(_link_mem)
    set(_link_jobs_max)
  endif()
  set_property(
    GLOBAL APPEND PROPERTY
    JOB_POOLS link_job_pool=${_link_jobs}
  )
  set(CMAKE_JOB_POOL_LINK link_job_pool)

  set(_compile_heavy_jobs 0)
  if(NINJA_MAX_NUM_PARALLEL_COMPILE_HEAVY_JOBS)
    set(_compile_heavy_jobs ${NINJA_MAX_NUM_PARALLEL_COMPILE_HEAVY_JOBS})
    message(STATUS
      "  NINJA_MAX_NUM_PARALLEL_COMPILE_HEAVY_JOBS: "
      "max concurrent heavy compile jobs (from settings): ${_compile_heavy_jobs}"
    )
  endif()
  if(${_compile_heavy_jobs} LESS 1)
    # Heuristics: Maximum amount of memory needed per heavy compile task.
    # Note: These values are purposely over-estimated by 10-15% at least, to account
    #       for other types of jobs' memory usage, other system memory usages, etc.
    set(_compile_heavy_mem 2000)
    if(WITH_COMPILER_ASAN)
      set(_compile_heavy_mem 15000)
    elseif("${CMAKE_BUILD_TYPE}" MATCHES "Debug")
      set(_compile_heavy_mem 2000)
    endif()

    math(EXPR _compile_heavy_jobs "${_TOT_MEM} / ${_compile_heavy_mem}")
    if(${_NUM_CORES} GREATER 3)
      # Heuristics: Cap max number of heavy compile jobs to 80% the amount of available cores,
      # to ensure neither linking nor regular compile jobs are starved from cores.
      # It also ensures that even if there would be enough RAM, the machine never ends up
      # handling only heavy jobs at some point.
      # This can have annoying sides effects, like lack of output in the console for several
      # minutes, which can lead to a wrong detection of 'unresponsive' state by the build-bots e.g.
      math(EXPR _compile_heavy_jobs_max "(${_NUM_CORES} * 8) / 10")
    else()
      set(_compile_heavy_jobs_max ${_NUM_CORES})
    endif()
    if(${_compile_heavy_jobs} GREATER ${_compile_heavy_jobs_max})
      set(_compile_heavy_jobs ${_compile_heavy_jobs_max})
    elseif(${_compile_heavy_jobs} LESS 1)
      set(_compile_heavy_jobs 1)
    endif()

    message(STATUS
      "  NINJA_MAX_NUM_PARALLEL_COMPILE_HEAVY_JOBS: "
      "max concurrent heavy compile jobs (computed): ${_compile_heavy_jobs}"
    )

    set(_compile_heavy_mem)
    set(_compile_heavy_jobs_max)
  endif()
  set_property(
    GLOBAL APPEND PROPERTY
    JOB_POOLS compile_heavy_job_pool=${_compile_heavy_jobs}
  )

  set(_compile_jobs 0)
  if(NINJA_MAX_NUM_PARALLEL_COMPILE_JOBS)
    set(_compile_jobs ${NINJA_MAX_NUM_PARALLEL_COMPILE_JOBS})
    message(STATUS
      "  NINJA_MAX_NUM_PARALLEL_COMPILE_JOBS: "
      "max concurrent regular compile jobs (from settings): ${_compile_jobs}"
    )
  endif()
  if(${_compile_jobs} LESS 1)
    # Heuristics: Maximum amount of memory needed per regular compile task.
    # NOTE: These values are purposefully over-estimated by 10-15% at least,
    #       to account for other types of jobs' memory usage, other system memory usages, etc.
    #
    # FIXME:
    # There are a few files in 'normal' compile job pool now
    # that require a significant amount of RAM
    # (e.g. `blenkernel/intern/volume.cc` can require almost 5GB of RAM in debug + ASAN builds).
    # Until we can add individual files to the heavy compile pool job
    # (not possible currently with CMake), using overly-big
    # values in ASAN build cases is the best that can be done.
    # Alternative solution would be to put several whole targets (like `bf_blenkernel`)
    # into the heavy pool, but that is likely even worse of a workaround.
    set(_compile_mem 500)
    if(WITH_COMPILER_ASAN)
      set(_compile_mem 4000)
    elseif("${CMAKE_BUILD_TYPE}" MATCHES "Debug")
      set(_compile_mem 500)
    endif()

    math(EXPR _compile_jobs "${_TOT_MEM} / ${_compile_mem}")
    if(${_NUM_CORES} GREATER 3)
      # Heuristics: Cap max number of regular compile jobs
      # to less than the total available amount of cores,
      # to ensure neither linking nor heavy compile jobs are starved from cores.
      math(EXPR _compile_jobs_max "${_NUM_CORES} - ${_link_jobs} - (${_compile_heavy_jobs} / 8)")
    else()
      set(_compile_jobs_max ${_NUM_CORES})
    endif()
    if(${_compile_jobs} GREATER ${_compile_jobs_max})
      set(_compile_jobs ${_compile_jobs_max})
    elseif(${_compile_jobs} LESS 1)
      set(_compile_jobs 1)
    endif()

    message(STATUS
      "  NINJA_MAX_NUM_PARALLEL_COMPILE_JOBS: "
      "max concurrent regular compile jobs (computed): ${_compile_jobs}"
    )
    set(_compile_mem)
    set(_compile_jobs_max)
  endif()
  set_property(
    GLOBAL APPEND PROPERTY
    JOB_POOLS compile_job_pool=${_compile_jobs}
  )
  set(CMAKE_JOB_POOL_COMPILE compile_job_pool)

  set(_link_jobs)
  set(_compile_heavy_jobs)
  set(_compile_jobs)
  set(_NUM_CORES)
  set(_TOT_MEM)
endif()


# -----------------------------------------------------------------------------
# Extra Compile Flags

if(CMAKE_COMPILER_IS_GNUCC)

  add_check_c_compiler_flags(
    C_WARNINGS

    C_WARN_ALL -Wall
    C_WARN_ERROR_IMPLICIT_FUNCTION_DECLARATION -Werror=implicit-function-declaration

    # System headers sometimes do this, disable for now, was: `-Werror=strict-prototypes`.
    C_WARN_STRICT_PROTOTYPES -Wstrict-prototypes

    C_WARN_ERROR_RETURN_TYPE -Werror=return-type
    C_WARN_ERROR_VLA -Werror=vla
    C_WARN_MISSING_PROTOTYPES -Wmissing-prototypes
    C_WARN_NO_CHAR_SUBSCRIPTS -Wno-char-subscripts
    C_WARN_NO_UNKNOWN_PRAGMAS -Wno-unknown-pragmas
    C_WARN_POINTER_ARITH -Wpointer-arith
    C_WARN_UNUSED_PARAMETER -Wunused-parameter
    C_WARN_WRITE_STRINGS -Wwrite-strings
    C_WARN_LOGICAL_OP -Wlogical-op
    C_WARN_UNDEF -Wundef

    # Needs: `-Wuninitialized`.
    C_WARN_INIT_SELF -Winit-self

    C_WARN_MISSING_INCLUDE_DIRS -Wmissing-include-dirs
    C_WARN_NO_DIV_BY_ZERO -Wno-div-by-zero
    C_WARN_TYPE_LIMITS -Wtype-limits
    C_WARN_FORMAT_SIGN -Wformat-signedness
    C_WARN_RESTRICT -Wrestrict

    # Useful but too many false positives and inconvenient to suppress each occurrence.
    C_WARN_NO_STRINGOP_OVERREAD -Wno-stringop-overread
    C_WARN_NO_STRINGOP_OVERFLOW -Wno-stringop-overflow

    # C-only.
    C_WARN_NO_NULL -Wnonnull
    C_WARN_ABSOLUTE_VALUE -Wabsolute-value

    C_WARN_UNINITIALIZED -Wuninitialized
    C_WARN_REDUNDANT_DECLS -Wredundant-decls
    C_WARN_SHADOW -Wshadow

    # Disable because it gives warnings for `printf()` & friends.
    # C_WARN_DOUBLE_PROMOTION `-Wdouble-promotion -Wno-error=double-promotion`

    # Use `ATTR_FALLTHROUGH` macro to suppress.
    C_WARN_IMPLICIT_FALLTHROUGH -Wimplicit-fallthrough=5
  )

  if(NOT APPLE)
    add_check_c_compiler_flags(
      C_WARNINGS
      C_WARN_NO_ERROR_UNUSED_BUT_SET_VARIABLE -Wno-error=unused-but-set-variable
    )
  endif()

  add_check_cxx_compiler_flags(
    CXX_WARNINGS

    CXX_WARN_UNINITIALIZED -Wuninitialized
    CXX_WARN_REDUNDANT_DECLS -Wredundant-decls

    CXX_WARN_ALL -Wall
    CXX_WARN_NO_INVALID_OFFSETOF -Wno-invalid-offsetof
    CXX_WARN_NO_SIGN_COMPARE -Wno-sign-compare
    CXX_WARN_LOGICAL_OP -Wlogical-op

    # Needs: `-Wuninitialized`.
    CXX_WARN_INIT_SELF -Winit-self

    CXX_WARN_MISSING_INCLUDE_DIRS -Wmissing-include-dirs
    CXX_WARN_NO_DIV_BY_ZERO -Wno-div-by-zero
    CXX_WARN_TYPE_LIMITS -Wtype-limits
    CXX_WARN_ERROR_RETURN_TYPE -Werror=return-type
    CXX_WARN_NO_CHAR_SUBSCRIPTS -Wno-char-subscripts
    CXX_WARN_NO_UNKNOWN_PRAGMAS -Wno-unknown-pragmas
    CXX_WARN_POINTER_ARITH -Wpointer-arith
    CXX_WARN_UNUSED_PARAMETER -Wunused-parameter
    CXX_WARN_WRITE_STRINGS -Wwrite-strings
    CXX_WARN_UNDEF -Wundef
    CXX_WARN_COMMA_SUBSCRIPT -Wcomma-subscript
    CXX_WARN_FORMAT_SIGN -Wformat-signedness
    CXX_WARN_RESTRICT -Wrestrict
    CXX_WARN_NO_SUGGEST_OVERRIDE -Wno-suggest-override
    CXX_WARN_UNINITIALIZED -Wuninitialized

    # NOTE(@ideasman42): In GCC 13.2.1 on Linux this causes internal compiler errors.
    # The crashes can be resolved by disabling the flag per module (but not via pragmas).
    # However this also causes a type mix-up FreeStyle  (Blender & FreeStyle's `Curve`)
    # so it seems to impact GCC's the internal state enough that it's too risky to enable.
    # When this is resolved the check can be enabled for fixed GCC versions.
    #
    # Prevents linking errors with MSVC.
    # `CXX_WARN_MISMATCHED_TAGS -Wmismatched-tags`

    # Useful but too many false positives and inconvenient to suppress each occurrence.
    CXX_WARN_NO_STRINGOP_OVERREAD -Wno-stringop-overread
    CXX_WARN_NO_STRINGOP_OVERFLOW -Wno-stringop-overflow

    # Use `[[fallthrough]]` or `ATTR_FALLTHROUGH` macro to suppress.
    CXX_WARN_IMPLICIT_FALLTHROUGH -Wimplicit-fallthrough=5
  )

  if(${CMAKE_SYSTEM_PROCESSOR} STREQUAL "aarch64")
    # Silence warnings about GCC ABI breaking bug fixes on arm64
    add_check_cxx_compiler_flags(
      CXX_WARNINGS
      CXX_WARN_NO_PSABI -Wno-psabi
    )
  endif()

  # causes too many warnings
  if(NOT APPLE)
    add_check_cxx_compiler_flags(
      CXX_WARNINGS
      CXX_WARN_UNDEF -Wundef
      CXX_WARN_MISSING_DECLARATIONS -Wmissing-declarations
    )
  endif()

  # ---------------------
  # Suppress Strict Flags
  #
  # Exclude the following warnings from this list:
  # - `-Wno-address`:
  #   This can give useful hints that point to bugs/misleading logic.
  # - `-Wno-strict-prototypes`:
  #   No need to support older C-style prototypes.
  #
  # If code in `./extern/` needs to suppress these flags that can be done on a case-by-case basis.

  # flags to undo strict flags
  add_check_c_compiler_flags(
    C_REMOVE_STRICT_FLAGS

    C_WARN_NO_DEPRECATED_DECLARATIONS -Wno-deprecated-declarations
    C_WARN_NO_UNUSED_PARAMETER -Wno-unused-parameter
    C_WARN_NO_UNUSED_FUNCTION -Wno-unused-function
    C_WARN_NO_TYPE_LIMITS -Wno-type-limits
    C_WARN_NO_INT_IN_BOOL_CONTEXT -Wno-int-in-bool-context
    C_WARN_NO_FORMAT -Wno-format
    C_WARN_NO_SWITCH -Wno-switch
    C_WARN_NO_UNUSED_VARIABLE -Wno-unused-variable
    C_WARN_NO_UNUSED_VARIABLE -Wno-uninitialized
    C_WARN_NO_IMPLICIT_FALLTHROUGH -Wno-implicit-fallthrough
  )


  add_check_cxx_compiler_flags(
    CXX_REMOVE_STRICT_FLAGS

    CXX_WARN_NO_CLASS_MEMACCESS -Wno-class-memaccess
    CXX_WARN_NO_COMMENT -Wno-comment
    CXX_WARN_NO_UNUSED_TYPEDEFS -Wno-unused-local-typedefs
    CXX_WARN_NO_UNUSED_VARIABLE -Wno-unused-variable
    CXX_WARN_NO_UNINITIALIZED -Wno-uninitialized
    CXX_WARN_NO_REORDER -Wno-reorder
  )


  if(NOT APPLE)
    add_check_c_compiler_flags(
      C_REMOVE_STRICT_FLAGS
      C_WARN_NO_ERROR_UNUSED_BUT_SET_VARIABLE -Wno-error=unused-but-set-variable
    )
  endif()

elseif(CMAKE_C_COMPILER_ID MATCHES "Clang")
  # Matches both "Clang" & "AppleClang" on macOS.

  add_check_c_compiler_flags(
    C_WARNINGS

    # Strange, clang complains these are not supported, but then uses them.
    C_WARN_ALL -Wall
    C_WARN_ERROR_IMPLICIT_FUNCTION_DECLARATION -Werror=implicit-function-declaration
    C_WARN_ERROR_RETURN_TYPE -Werror=return-type
    C_WARN_NO_AUTOLOGICAL_COMPARE -Wno-tautological-compare
    C_WARN_NO_UNKNOWN_PRAGMAS -Wno-unknown-pragmas
    C_WARN_NO_CHAR_SUBSCRIPTS -Wno-char-subscripts
    C_WARN_STRICT_PROTOTYPES -Wstrict-prototypes
    C_WARN_MISSING_PROTOTYPES -Wmissing-prototypes
    C_WARN_UNUSED_PARAMETER -Wunused-parameter
    C_WARN_UNDEF -Wundef
    C_WARN_UNDEF_PREFIX -Wundef-prefix

    C_WARN_ERROR_UNGUARDED_AVAILABILITY_NEW -Werror=unguarded-availability-new
  )

  add_check_cxx_compiler_flags(
    CXX_WARNINGS

    CXX_WARN_ALL -Wall
    # Using C++20 features while having C++17 as the project language isn't allowed by MSVC.
    CXX_CXX20_DESIGNATOR -Wc++20-designator

    CXX_WARN_NO_AUTOLOGICAL_COMPARE -Wno-tautological-compare
    CXX_WARN_NO_UNKNOWN_PRAGMAS -Wno-unknown-pragmas
    CXX_WARN_NO_CHAR_SUBSCRIPTS -Wno-char-subscripts

    # We get a lot of these, if its a problem a developer needs to look into it.
    CXX_WARN_NO_OVERLOADED_VIRTUAL -Wno-overloaded-virtual

    CXX_WARN_NO_SIGN_COMPARE -Wno-sign-compare
    CXX_WARN_NO_INVALID_OFFSETOF -Wno-invalid-offsetof

    # Apple Clang (tested on version 12) doesn't support this flag while LLVM Clang 11 does.
    CXX_WARN_NO_SUGGEST_OVERRIDE -Wno-suggest-override

    CXX_WARN_UNDEF -Wundef
    CXX_WARN_UNDEF_PREFIX -Wundef-prefix
    CXX_WARN_UNUSED_PARAMETER -Wunused-parameter

    # Prevents linking errors with MSVC.
    CXX_WARN_MISMATCHED_TAGS -Wmismatched-tags

    # Gives too many unfixable warnings.
    # `C_WARN_UNUSED_MACROS -Wunused-macros`
    # `CXX_WARN_UNUSED_MACROS -Wunused-macros`

    CXX_WARN_ERROR_UNGUARDED_AVAILABILITY_NEW -Werror=unguarded-availability-new
  )
  if(MSVC_CLANG)
    # clang-cl produces an unhealthy amount of warnings in its default
    # configuration as it for reasons unknown decided to enable all
    # warnings known to mankind. Resulting in a 5.5GB build log containing
    # well over 11 million warnings. The code below disables every single
    # one of them indiscriminately. Someone with time on their hands,
    # could/should go over these and either fix them or describe why we
    # would want to disable the warning. The list below contains both C
    # and C++ warnings for all warnings since clang has seemingly no
    # easy way to tell if something is a C or C++ specific warning and
    # manually auditing every single one of them just isn't in the cards
    # right now.

    # /W3 is being removed, then added back again, this is because order
    # matters for clang and these flags are being placed before the
    # CMAKE_[LANGUAGE]_FLAGS which normally contain /W3, so we would
    # disable certain warnings here only for them to be re-enabled by /W3
    # later on.
    remove_cc_flag("/W3")

    add_check_c_compiler_flags(
      C_WARNINGS
      C_WARN_CLANG_CL_W3 /W3
      # The number behind each warn is the number of unique warning were
      # generated on 2024-04-24 (d2be9cecc28a03ff1f799e8c63f1f9f8eda7cce3)
      # especially the ones in the single and low double digits are likely
      # genuine problems that can be investigated.
      C_WARN_CLANG_CL_C++98_COMPAT -Wno-c++98-compat # 352692
      C_WARN_CLANG_CL_OLD_STYLE_CAST -Wno-old-style-cast # 178608
      C_WARN_CLANG_CL_UNSAFE_BUFFER_USAGE -Wno-unsafe-buffer-usage # 89032
      C_WARN_CLANG_CL_MISSING_PROTOTYPES -Wno-missing-prototypes # 25587
      C_WARN_CLANG_CL_SIGN_CONVERSION -Wno-sign-conversion # 20109
      C_WARN_CLANG_CL_MISSING_FIELD_INITIALIZERS -Wno-missing-field-initializers # 20060
      C_WARN_CLANG_CL_EXTRA_SEMI -Wno-extra-semi # 12513
      C_WARN_CLANG_CL_LANGUAGE_EXTENSION_TOKEN -Wno-language-extension-token # 11032
      C_WARN_CLANG_CL_IMPLICIT_FLOAT_CONVERSION -Wno-implicit-float-conversion # 11003
      C_WARN_CLANG_CL_C++98_COMPAT_PEDANTIC -Wno-c++98-compat-pedantic # 10336
      C_WARN_CLANG_CL_IMPLICIT_INT_FLOAT_CONVERSION -Wno-implicit-int-float-conversion # 7354
      C_WARN_CLANG_CL_DOUBLE_PROMOTION -Wno-double-promotion # 7350
      C_WARN_CLANG_CL_PRE_C++17_COMPAT -Wno-pre-c++17-compat # 7303
      C_WARN_CLANG_CL_SHORTEN_64_TO_32 -Wno-shorten-64-to-32 # 7085
      C_WARN_CLANG_CL_C++98_COMPAT_LOCAL_TYPE_TEMPLATE_ARGS -Wno-c++98-compat-local-type-template-args # 6906
      C_WARN_CLANG_CL_RESERVED_IDENTIFIER -Wno-reserved-identifier # 5886
      C_WARN_CLANG_CL_CAST_ALIGN -Wno-cast-align # 5513
      C_WARN_CLANG_CL_DOCUMENTATION -Wno-documentation # 5107
      C_WARN_CLANG_CL_DISABLED_MACRO_EXPANSION -Wno-disabled-macro-expansion # 4449
      C_WARN_CLANG_CL_EXTRA_SEMI_STMT -Wno-extra-semi-stmt # 4349
      C_WARN_CLANG_CL_ZERO_AS_NULL_POINTER_CONSTANT -Wno-zero-as-null-pointer-constant # 3209
      C_WARN_CLANG_CL_FLOAT_CONVERSION -Wno-float-conversion # 2869
      C_WARN_CLANG_CL_RESERVED_MACRO_IDENTIFIER -Wno-reserved-macro-identifier # 2862
      C_WARN_CLANG_CL_CAST_FUNCTION_TYPE_STRICT -Wno-cast-function-type-strict # 2663
      C_WARN_CLANG_CL_FLOAT_EQUAL -Wno-float-equal # 2153
      C_WARN_CLANG_CL_IMPLICIT_INT_CONVERSION -Wno-implicit-int-conversion # 2117
      C_WARN_CLANG_CL_SHADOW -Wno-shadow # 2068
      C_WARN_CLANG_CL_SHADOW_FIELD_IN_CONSTRUCTOR -Wno-shadow-field-in-constructor # 1829
      C_WARN_CLANG_CL_CAST_QUAL -Wno-cast-qual # 1742
      C_WARN_CLANG_CL_PRE_C++14_COMPAT -Wno-pre-c++14-compat # 1569
      C_WARN_CLANG_CL_GLOBAL_CONSTRUCTORS -Wno-global-constructors # 1402
      C_WARN_CLANG_CL_SWITCH_ENUM -Wno-switch-enum # 973
      C_WARN_CLANG_CL_EXIT_TIME_DESTRUCTORS -Wno-exit-time-destructors # 940
      C_WARN_CLANG_CL_CTAD_MAYBE_UNSUPPORTED -Wno-ctad-maybe-unsupported # 891
      C_WARN_CLANG_CL_UNDEFINED_FUNC_TEMPLATE -Wno-undefined-func-template # 863
      C_WARN_CLANG_CL_C++98_COMPAT_EXTRA_SEMI -Wno-c++98-compat-extra-semi # 848
      C_WARN_CLANG_CL_CAST_FUNCTION_TYPE -Wno-cast-function-type # 807
      C_WARN_CLANG_CL_NULLABILITY_EXTENSION -Wno-nullability-extension # 602
      C_WARN_CLANG_CL_SHADOW_FIELD -Wno-shadow-field # 585
      C_WARN_CLANG_CL_CONDITIONAL_UNINITIALIZED -Wno-conditional-uninitialized # 555
      C_WARN_CLANG_CL_UNUSED_PARAMETER -Wno-unused-parameter # 539
      C_WARN_CLANG_CL_SUGGEST_DESTRUCTOR_OVERRIDE -Wno-suggest-destructor-override # 356
      C_WARN_CLANG_CL_SHADOW_UNCAPTURED_LOCAL -Wno-shadow-uncaptured-local # 355
      C_WARN_CLANG_CL_UNUSED_MACROS -Wno-unused-macros # 289
      C_WARN_CLANG_CL_COVERED_SWITCH_DEFAULT -Wno-covered-switch-default # 233
      C_WARN_CLANG_CL_SIGNED_ENUM_BITFIELD -Wno-signed-enum-bitfield # 229
      C_WARN_CLANG_CL_DECLARATION_AFTER_STATEMENT -Wno-declaration-after-statement # 228
      C_WARN_CLANG_CL_IMPLICIT_FALLTHROUGH -Wno-implicit-fallthrough # 164
      C_WARN_CLANG_CL_NON_VIRTUAL_DTOR -Wno-non-virtual-dtor # 161
      C_WARN_CLANG_CL_NESTED_ANON_TYPES -Wno-nested-anon-types # 140
      C_WARN_CLANG_CL_GNU_ZERO_VARIADIC_MACRO_ARGUMENTS -Wno-gnu-zero-variadic-macro-arguments # 132
      C_WARN_CLANG_CL_UNREACHABLE_CODE_BREAK -Wno-unreachable-code-break # 115
      C_WARN_CLANG_CL_INCONSISTENT_MISSING_DESTRUCTOR_OVERRIDE -Wno-inconsistent-missing-destructor-override # 104
      C_WARN_CLANG_CL_FORMAT_PEDANTIC -Wno-format-pedantic # 97
      C_WARN_CLANG_CL_NONPORTABLE_SYSTEM_INCLUDE_PATH -Wno-nonportable-system-include-path # 95
      C_WARN_CLANG_CL_UNDEF -Wno-undef # 94
      C_WARN_CLANG_CL_IGNORED_QUALIFIERS -Wno-ignored-qualifiers # 93
      C_WARN_CLANG_CL_USED_BUT_MARKED_UNUSED -Wno-used-but-marked-unused # 83
      C_WARN_CLANG_CL_HEADER_HYGIENE -Wno-header-hygiene # 79
      C_WARN_CLANG_CL_CHAR_SUBSCRIPTS -Wno-char-subscripts # 76
      C_WARN_CLANG_CL_UNREACHABLE_CODE_RETURN -Wno-unreachable-code-return # 71
      C_WARN_CLANG_CL_UNUSED_TEMPLATE -Wno-unused-template # 66
      C_WARN_CLANG_CL_GNU_ANONYMOUS_STRUCT -Wno-gnu-anonymous-struct # 63
      C_WARN_CLANG_CL_DEPRECATED_COPY_WITH_USER_PROVIDED_DTOR -Wno-deprecated-copy-with-user-provided-dtor # 62
      C_WARN_CLANG_CL_INCONSISTENT_MISSING_OVERRIDE -Wno-inconsistent-missing-override # 54
      C_WARN_CLANG_CL_UNREACHABLE_CODE -Wno-unreachable-code # 52
      C_WARN_CLANG_CL_DEPRECATED_DYNAMIC_EXCEPTION_SPEC -Wno-deprecated-dynamic-exception-spec # 51
      C_WARN_CLANG_CL_BAD_FUNCTION_CAST -Wno-bad-function-cast # 50
      C_WARN_CLANG_CL_MICROSOFT_ENUM_VALUE -Wno-microsoft-enum-value # 47
      C_WARN_CLANG_CL_DEPRECATED_COPY_WITH_USER_PROVIDED_COPY -Wno-deprecated-copy-with-user-provided-copy # 41
      C_WARN_CLANG_CL_ZERO_LENGTH_ARRAY -Wno-zero-length-array # 39
      C_WARN_CLANG_CL_UNUSED_FUNCTION -Wno-unused-function # 38
      C_WARN_CLANG_CL_PEDANTIC -Wno-pedantic # 38
      C_WARN_CLANG_CL_DEPRECATED_COPY_WITH_DTOR -Wno-deprecated-copy-with-dtor # 37
      C_WARN_CLANG_CL_DOCUMENTATION_UNKNOWN_COMMAND -Wno-documentation-unknown-command # 34
      C_WARN_CLANG_CL_UNDEFINED_REINTERPRET_CAST -Wno-undefined-reinterpret-cast # 33
      C_WARN_CLANG_CL_FORMAT_NONLITERAL -Wno-format-nonliteral # 29
      C_WARN_CLANG_CL_COMMA -Wno-comma # 27
      C_WARN_CLANG_CL_DOCUMENTATION_DEPRECATED_SYNC -Wno-documentation-deprecated-sync # 26
      C_WARN_CLANG_CL_SHIFT_SIGN_OVERFLOW -Wno-shift-sign-overflow # 24
      C_WARN_CLANG_CL_PRE_C++17_COMPAT_PEDANTIC -Wno-pre-c++17-compat-pedantic # 24
      C_WARN_CLANG_CL_C++98_COMPAT_UNNAMED_TYPE_TEMPLATE_ARGS -Wno-c++98-compat-unnamed-type-template-args # 22
      C_WARN_CLANG_CL_SIGN_COMPARE -Wno-sign-compare # 21
      C_WARN_CLANG_CL_FORMAT -Wno-format # 21
      C_WARN_CLANG_CL_C++98_COMPAT_BIND_TO_TEMPORARY_COPY -Wno-c++98-compat-bind-to-temporary-copy # 21
      C_WARN_CLANG_CL_ENUM_ENUM_CONVERSION -Wno-enum-enum-conversion # 20
      C_WARN_CLANG_CL_ANON_ENUM_ENUM_CONVERSION -Wno-anon-enum-enum-conversion # 14
      C_WARN_CLANG_CL_RANGE_LOOP_BIND_REFERENCE -Wno-range-loop-bind-reference # 14
      C_WARN_CLANG_CL_ENUM_FLOAT_CONVERSION -Wno-enum-float-conversion # 12
      C_WARN_CLANG_CL_KEYWORD_MACRO -Wno-keyword-macro # 10
      C_WARN_CLANG_CL_DEPRECATED_COPY -Wno-deprecated-copy # 10
      C_WARN_CLANG_CL_UNUSED_MEMBER_FUNCTION -Wno-unused-member-function # 9
      C_WARN_CLANG_CL_MISSING_NORETURN -Wno-missing-noreturn # 8
      C_WARN_CLANG_CL_MISSING_VARIABLE_DECLARATIONS -Wno-missing-variable-declarations # 8
      C_WARN_CLANG_CL_DOCUMENTATION_HTML -Wno-documentation-html # 6
      C_WARN_CLANG_CL_GNU_REDECLARED_ENUM -Wno-gnu-redeclared-enum # 6
      C_WARN_CLANG_CL_DEPRECATED_DECLARATIONS -Wno-deprecated-declarations # 6
      C_WARN_CLANG_CL_OVERLOADED_VIRTUAL -Wno-overloaded-virtual # 5
      C_WARN_CLANG_CL_C++98_C++11_COMPAT_BINARY_LITERAL -Wno-c++98-c++11-compat-binary-literal # 4
      C_WARN_CLANG_CL_DEPRECATED_REDUNDANT_CONSTEXPR_STATIC_DEF -Wno-deprecated-redundant-constexpr-static-def # 4
      C_WARN_CLANG_CL_MISSING_BRACES -Wno-missing-braces # 4
      C_WARN_CLANG_CL_C99_EXTENSIONS -Wno-c99-extensions # 4
      C_WARN_CLANG_CL_STRICT_PROTOTYPES -Wno-strict-prototypes # 4
      C_WARN_CLANG_CL_UNREACHABLE_CODE_LOOP_INCREMENT -Wno-unreachable-code-loop-increment # 4
      C_WARN_CLANG_CL_GNU_CASE_RANGE -Wno-gnu-case-range # 4
      C_WARN_CLANG_CL_DUPLICATE_ENUM -Wno-duplicate-enum # 3
      C_WARN_CLANG_CL_NULL_POINTER_SUBTRACTION -Wno-null-pointer-subtraction # 2
      C_WARN_CLANG_CL_DEPRECATED_LITERAL_OPERATOR -Wno-deprecated-literal-operator # 2
      C_WARN_CLANG_CL_NEWLINE_EOF -Wno-newline-eof # 2
      C_WARN_CLANG_CL_MICROSOFT_CAST -Wno-microsoft-cast # 2
      C_WARN_CLANG_CL_DATE_TIME -Wno-date-time # 2
      C_WARN_CLANG_CL_DELETE_NON_ABSTRACT_NON_VIRTUAL_DTOR -Wno-delete-non-abstract-non-virtual-dtor # 2
      C_WARN_CLANG_CL_UNUSED_PRIVATE_FIELD -Wno-unused-private-field # 2
      C_WARN_CLANG_CL_FLEXIBLE_ARRAY_EXTENSIONS -Wno-flexible-array-extensions # 2
      C_WARN_CLANG_CL_STRING_CONVERSION -Wno-string-conversion # 2
      C_WARN_CLANG_CL_FINAL_DTOR_NON_FINAL_CLASS -Wno-final-dtor-non-final-class # 2
      C_WARN_CLANG_CL_MICROSOFT_UNQUALIFIED_FRIEND -Wno-microsoft-unqualified-friend # 2
      C_WARN_CLANG_CL_INVALID_NORETURN -Wno-invalid-noreturn # 1
      C_WARN_CLANG_CL_INVALID_UTF8 -Wno-invalid-utf8 # 1
      C_WARN_CLANG_CL_FOUR_CHAR_CONSTANTS -Wno-four-char-constants # 1
      C_WARN_CLANG_CL_PARENTHESES -Wno-parentheses # 1
      C_WARN_CLANG_CL_PESSIMIZING_MOVE -Wno-pessimizing-move # 1
      C_WARN_CLANG_CL_DEPRECATED_NON_PROTOTYPE -Wno-deprecated-non-prototype # 1
      C_WARN_CLANG_CL_BITFIELD_ENUM_CONVERSION -Wno-bitfield-enum-conversion # 1
      C_WARN_CLANG_CL_UNUSED_LAMBDA_CAPTURE -Wno-unused-lambda-capture # 1
      C_WARN_CLANG_CL_SHADOW_FIELD_IN_CONSTRUCTOR_MODIFIED -Wno-shadow-field-in-constructor-modified # 1
      # And some additional ones that came up when using LLVM 18.1.8 on Windows ARM64
      C_WARN_CLANG_CL_SWITCH_DEFAULT -Wno-switch-default
      C_WARN_CLANG_CL_NAN_INFINITY_DISABLED -Wno-nan-infinity-disabled
      # And another from 19.1.5
      C_WARN_CLANG_CL_PRE_C11_COMPAT -Wno-pre-c11-compat
    )

    add_check_cxx_compiler_flags(
      CXX_WARNINGS
      CXX_WARN_CLANG_CL_W3 /W3
      CXX_WARN_CLANG_CL_C++98_COMPAT -Wno-c++98-compat # 352692
      CXX_WARN_CLANG_CL_OLD_STYLE_CAST -Wno-old-style-cast # 178608
      CXX_WARN_CLANG_CL_UNSAFE_BUFFER_USAGE -Wno-unsafe-buffer-usage # 89032
      CXX_WARN_CLANG_CL_MISSING_PROTOTYPES -Wno-missing-prototypes # 25587
      CXX_WARN_CLANG_CL_SIGN_CONVERSION -Wno-sign-conversion # 20109
      CXX_WARN_CLANG_CL_MISSING_FIELD_INITIALIZERS -Wno-missing-field-initializers # 20060
      CXX_WARN_CLANG_CL_EXTRA_SEMI -Wno-extra-semi # 12513
      CXX_WARN_CLANG_CL_LANGUAGE_EXTENSION_TOKEN -Wno-language-extension-token # 11032
      CXX_WARN_CLANG_CL_IMPLICIT_FLOAT_CONVERSION -Wno-implicit-float-conversion # 11003
      CXX_WARN_CLANG_CL_C++98_COMPAT_PEDANTIC -Wno-c++98-compat-pedantic # 10336
      CXX_WARN_CLANG_CL_IMPLICIT_INT_FLOAT_CONVERSION -Wno-implicit-int-float-conversion # 7354
      CXX_WARN_CLANG_CL_DOUBLE_PROMOTION -Wno-double-promotion # 7350
      CXX_WARN_CLANG_CL_PRE_C++17_COMPAT -Wno-pre-c++17-compat # 7303
      CXX_WARN_CLANG_CL_SHORTEN_64_TO_32 -Wno-shorten-64-to-32 # 7085
      CXX_WARN_CLANG_CL_C++98_COMPAT_LOCAL_TYPE_TEMPLATE_ARGS -Wno-c++98-compat-local-type-template-args # 6906
      CXX_WARN_CLANG_CL_RESERVED_IDENTIFIER -Wno-reserved-identifier # 5886
      CXX_WARN_CLANG_CL_CAST_ALIGN -Wno-cast-align # 5513
      CXX_WARN_CLANG_CL_DOCUMENTATION -Wno-documentation # 5107
      CXX_WARN_CLANG_CL_DISABLED_MACRO_EXPANSION -Wno-disabled-macro-expansion # 4449
      CXX_WARN_CLANG_CL_EXTRA_SEMI_STMT -Wno-extra-semi-stmt # 4349
      CXX_WARN_CLANG_CL_ZERO_AS_NULL_POINTER_CONSTANT -Wno-zero-as-null-pointer-constant # 3209
      CXX_WARN_CLANG_CL_FLOAT_CONVERSION -Wno-float-conversion # 2869
      CXX_WARN_CLANG_CL_RESERVED_MACRO_IDENTIFIER -Wno-reserved-macro-identifier # 2862
      CXX_WARN_CLANG_CL_CAST_FUNCTION_TYPE_STRICT -Wno-cast-function-type-strict # 2663
      CXX_WARN_CLANG_CL_FLOAT_EQUAL -Wno-float-equal # 2153
      CXX_WARN_CLANG_CL_IMPLICIT_INT_CONVERSION -Wno-implicit-int-conversion # 2117
      CXX_WARN_CLANG_CL_SHADOW -Wno-shadow # 2068
      CXX_WARN_CLANG_CL_SHADOW_FIELD_IN_CONSTRUCTOR -Wno-shadow-field-in-constructor # 1829
      CXX_WARN_CLANG_CL_CAST_QUAL -Wno-cast-qual # 1742
      CXX_WARN_CLANG_CL_PRE_C++14_COMPAT -Wno-pre-c++14-compat # 1569
      CXX_WARN_CLANG_CL_GLOBAL_CONSTRUCTORS -Wno-global-constructors # 1402
      CXX_WARN_CLANG_CL_SWITCH_ENUM -Wno-switch-enum # 973
      CXX_WARN_CLANG_CL_EXIT_TIME_DESTRUCTORS -Wno-exit-time-destructors # 940
      CXX_WARN_CLANG_CL_CTAD_MAYBE_UNSUPPORTED -Wno-ctad-maybe-unsupported # 891
      CXX_WARN_CLANG_CL_UNDEFINED_FUNC_TEMPLATE -Wno-undefined-func-template # 863
      CXX_WARN_CLANG_CL_C++98_COMPAT_EXTRA_SEMI -Wno-c++98-compat-extra-semi # 848
      CXX_WARN_CLANG_CL_CAST_FUNCTION_TYPE -Wno-cast-function-type # 807
      CXX_WARN_CLANG_CL_NULLABILITY_EXTENSION -Wno-nullability-extension # 602
      CXX_WARN_CLANG_CL_SHADOW_FIELD -Wno-shadow-field # 585
      CXX_WARN_CLANG_CL_CONDITIONAL_UNINITIALIZED -Wno-conditional-uninitialized # 555
      CXX_WARN_CLANG_CL_UNUSED_PARAMETER -Wno-unused-parameter # 539
      CXX_WARN_CLANG_CL_SUGGEST_DESTRUCTOR_OVERRIDE -Wno-suggest-destructor-override # 356
      CXX_WARN_CLANG_CL_SHADOW_UNCAPTURED_LOCAL -Wno-shadow-uncaptured-local # 355
      CXX_WARN_CLANG_CL_UNUSED_MACROS -Wno-unused-macros # 289
      CXX_WARN_CLANG_CL_COVERED_SWITCH_DEFAULT -Wno-covered-switch-default # 233
      CXX_WARN_CLANG_CL_SIGNED_ENUM_BITFIELD -Wno-signed-enum-bitfield # 229
      CXX_WARN_CLANG_CL_DECLARATION_AFTER_STATEMENT -Wno-declaration-after-statement # 228
      CXX_WARN_CLANG_CL_IMPLICIT_FALLTHROUGH -Wno-implicit-fallthrough # 164
      CXX_WARN_CLANG_CL_NON_VIRTUAL_DTOR -Wno-non-virtual-dtor # 161
      CXX_WARN_CLANG_CL_NESTED_ANON_TYPES -Wno-nested-anon-types # 140
      CXX_WARN_CLANG_CL_GNU_ZERO_VARIADIC_MACRO_ARGUMENTS -Wno-gnu-zero-variadic-macro-arguments # 132
      CXX_WARN_CLANG_CL_UNREACHABLE_CODE_BREAK -Wno-unreachable-code-break # 115
      CXX_WARN_CLANG_CL_INCONSISTENT_MISSING_DESTRUCTOR_OVERRIDE -Wno-inconsistent-missing-destructor-override # 104
      CXX_WARN_CLANG_CL_FORMAT_PEDANTIC -Wno-format-pedantic # 97
      CXX_WARN_CLANG_CL_NONPORTABLE_SYSTEM_INCLUDE_PATH -Wno-nonportable-system-include-path # 95
      CXX_WARN_CLANG_CL_UNDEF -Wno-undef # 94
      CXX_WARN_CLANG_CL_IGNORED_QUALIFIERS -Wno-ignored-qualifiers # 93
      CXX_WARN_CLANG_CL_USED_BUT_MARKED_UNUSED -Wno-used-but-marked-unused # 83
      CXX_WARN_CLANG_CL_HEADER_HYGIENE -Wno-header-hygiene # 79
      CXX_WARN_CLANG_CL_CHAR_SUBSCRIPTS -Wno-char-subscripts # 76
      CXX_WARN_CLANG_CL_UNREACHABLE_CODE_RETURN -Wno-unreachable-code-return # 71
      CXX_WARN_CLANG_CL_UNUSED_TEMPLATE -Wno-unused-template # 66
      CXX_WARN_CLANG_CL_GNU_ANONYMOUS_STRUCT -Wno-gnu-anonymous-struct # 63
      CXX_WARN_CLANG_CL_DEPRECATED_COPY_WITH_USER_PROVIDED_DTOR -Wno-deprecated-copy-with-user-provided-dtor # 62
      CXX_WARN_CLANG_CL_INCONSISTENT_MISSING_OVERRIDE -Wno-inconsistent-missing-override # 54
      CXX_WARN_CLANG_CL_UNREACHABLE_CODE -Wno-unreachable-code # 52
      CXX_WARN_CLANG_CL_DEPRECATED_DYNAMIC_EXCEPTION_SPEC -Wno-deprecated-dynamic-exception-spec # 51
      CXX_WARN_CLANG_CL_BAD_FUNCTION_CAST -Wno-bad-function-cast # 50
      CXX_WARN_CLANG_CL_MICROSOFT_ENUM_VALUE -Wno-microsoft-enum-value # 47
      CXX_WARN_CLANG_CL_DEPRECATED_COPY_WITH_USER_PROVIDED_COPY -Wno-deprecated-copy-with-user-provided-copy # 41
      CXX_WARN_CLANG_CL_ZERO_LENGTH_ARRAY -Wno-zero-length-array # 39
      CXX_WARN_CLANG_CL_UNUSED_FUNCTION -Wno-unused-function # 38
      CXX_WARN_CLANG_CL_PEDANTIC -Wno-pedantic # 38
      CXX_WARN_CLANG_CL_DEPRECATED_COPY_WITH_DTOR -Wno-deprecated-copy-with-dtor # 37
      CXX_WARN_CLANG_CL_DOCUMENTATION_UNKNOWN_COMMAND -Wno-documentation-unknown-command # 34
      CXX_WARN_CLANG_CL_UNDEFINED_REINTERPRET_CAST -Wno-undefined-reinterpret-cast # 33
      CXX_WARN_CLANG_CL_FORMAT_NONLITERAL -Wno-format-nonliteral # 29
      CXX_WARN_CLANG_CL_COMMA -Wno-comma # 27
      CXX_WARN_CLANG_CL_DOCUMENTATION_DEPRECATED_SYNC -Wno-documentation-deprecated-sync # 26
      CXX_WARN_CLANG_CL_SHIFT_SIGN_OVERFLOW -Wno-shift-sign-overflow # 24
      CXX_WARN_CLANG_CL_PRE_C++17_COMPAT_PEDANTIC -Wno-pre-c++17-compat-pedantic # 24
      CXX_WARN_CLANG_CL_C++98_COMPAT_UNNAMED_TYPE_TEMPLATE_ARGS -Wno-c++98-compat-unnamed-type-template-args # 22
      CXX_WARN_CLANG_CL_SIGN_COMPARE -Wno-sign-compare # 21
      CXX_WARN_CLANG_CL_FORMAT -Wno-format # 21
      CXX_WARN_CLANG_CL_C++98_COMPAT_BIND_TO_TEMPORARY_COPY -Wno-c++98-compat-bind-to-temporary-copy # 21
      CXX_WARN_CLANG_CL_ENUM_ENUM_CONVERSION -Wno-enum-enum-conversion # 20
      CXX_WARN_CLANG_CL_ANON_ENUM_ENUM_CONVERSION -Wno-anon-enum-enum-conversion # 14
      CXX_WARN_CLANG_CL_RANGE_LOOP_BIND_REFERENCE -Wno-range-loop-bind-reference # 14
      CXX_WARN_CLANG_CL_ENUM_FLOAT_CONVERSION -Wno-enum-float-conversion # 12
      CXX_WARN_CLANG_CL_KEYWORD_MACRO -Wno-keyword-macro # 10
      CXX_WARN_CLANG_CL_DEPRECATED_COPY -Wno-deprecated-copy # 10
      CXX_WARN_CLANG_CL_UNUSED_MEMBER_FUNCTION -Wno-unused-member-function # 9
      CXX_WARN_CLANG_CL_MISSING_NORETURN -Wno-missing-noreturn # 8
      CXX_WARN_CLANG_CL_MISSING_VARIABLE_DECLARATIONS -Wno-missing-variable-declarations # 8
      CXX_WARN_CLANG_CL_DOCUMENTATION_HTML -Wno-documentation-html # 6
      CXX_WARN_CLANG_CL_GNU_REDECLARED_ENUM -Wno-gnu-redeclared-enum # 6
      CXX_WARN_CLANG_CL_DEPRECATED_DECLARATIONS -Wno-deprecated-declarations # 6
      CXX_WARN_CLANG_CL_OVERLOADED_VIRTUAL -Wno-overloaded-virtual # 5
      CXX_WARN_CLANG_CL_C++98_C++11_COMPAT_BINARY_LITERAL -Wno-c++98-c++11-compat-binary-literal # 4
      CXX_WARN_CLANG_CL_DEPRECATED_REDUNDANT_CONSTEXPR_STATIC_DEF -Wno-deprecated-redundant-constexpr-static-def # 4
      CXX_WARN_CLANG_CL_MISSING_BRACES -Wno-missing-braces # 4
      CXX_WARN_CLANG_CL_C99_EXTENSIONS -Wno-c99-extensions # 4
      CXX_WARN_CLANG_CL_STRICT_PROTOTYPES -Wno-strict-prototypes # 4
      CXX_WARN_CLANG_CL_UNREACHABLE_CODE_LOOP_INCREMENT -Wno-unreachable-code-loop-increment # 4
      CXX_WARN_CLANG_CL_GNU_CASE_RANGE -Wno-gnu-case-range # 4
      CXX_WARN_CLANG_CL_DUPLICATE_ENUM -Wno-duplicate-enum # 3
      CXX_WARN_CLANG_CL_NULL_POINTER_SUBTRACTION -Wno-null-pointer-subtraction # 2
      CXX_WARN_CLANG_CL_DEPRECATED_LITERAL_OPERATOR -Wno-deprecated-literal-operator # 2
      CXX_WARN_CLANG_CL_NEWLINE_EOF -Wno-newline-eof # 2
      CXX_WARN_CLANG_CL_MICROSOFT_CAST -Wno-microsoft-cast # 2
      CXX_WARN_CLANG_CL_DATE_TIME -Wno-date-time # 2
      CXX_WARN_CLANG_CL_DELETE_NON_ABSTRACT_NON_VIRTUAL_DTOR -Wno-delete-non-abstract-non-virtual-dtor # 2
      CXX_WARN_CLANG_CL_UNUSED_PRIVATE_FIELD -Wno-unused-private-field # 2
      CXX_WARN_CLANG_CL_FLEXIBLE_ARRAY_EXTENSIONS -Wno-flexible-array-extensions # 2
      CXX_WARN_CLANG_CL_STRING_CONVERSION -Wno-string-conversion # 2
      CXX_WARN_CLANG_CL_FINAL_DTOR_NON_FINAL_CLASS -Wno-final-dtor-non-final-class # 2
      CXX_WARN_CLANG_CL_MICROSOFT_UNQUALIFIED_FRIEND -Wno-microsoft-unqualified-friend # 2
      CXX_WARN_CLANG_CL_INVALID_NORETURN -Wno-invalid-noreturn # 1
      CXX_WARN_CLANG_CL_INVALID_UTF8 -Wno-invalid-utf8 # 1
      CXX_WARN_CLANG_CL_FOUR_CHAR_CONSTANTS -Wno-four-char-constants # 1
      CXX_WARN_CLANG_CL_PARENTHESES -Wno-parentheses # 1
      CXX_WARN_CLANG_CL_PESSIMIZING_MOVE -Wno-pessimizing-move # 1
      CXX_WARN_CLANG_CL_DEPRECATED_NON_PROTOTYPE -Wno-deprecated-non-prototype # 1
      CXX_WARN_CLANG_CL_BITFIELD_ENUM_CONVERSION -Wno-bitfield-enum-conversion # 1
      CXX_WARN_CLANG_CL_UNUSED_LAMBDA_CAPTURE -Wno-unused-lambda-capture # 1
      CXX_WARN_CLANG_CL_SHADOW_FIELD_IN_CONSTRUCTOR_MODIFIED -Wno-shadow-field-in-constructor-modified # 1
      # And some additional ones that came up when using LLVM 18.1.8 on Windows ARM64
      CXX_WARN_CLANG_CL_SWITCH_DEFAULT -Wno-switch-default
      CXX_WARN_CLANG_CL_NAN_INFINITY_DISABLED -Wno-nan-infinity-disabled
      # And another from 19.1.5
      CXX_WARN_CLANG_CL_PRE_C11_COMPAT -Wno-pre-c11-compat
    )
  endif()


  # ---------------------
  # Suppress Strict Flags

  # flags to undo strict flags

  add_check_c_compiler_flags(
    C_REMOVE_STRICT_FLAGS

    C_WARN_NO_UNUSED_PARAMETER -Wno-unused-parameter
    C_WARN_NO_UNUSED_VARIABLE -Wno-unused-variable
    C_WARN_NO_UNUSED_MACROS -Wno-unused-macros
    C_WARN_NO_MISLEADING_INDENTATION -Wno-misleading-indentation

    C_WARN_NO_MISSING_VARIABLE_DECLARATIONS -Wno-missing-variable-declarations
    C_WARN_NO_INCOMPAT_PTR_DISCARD_QUAL -Wno-incompatible-pointer-types-discards-qualifiers
    C_WARN_NO_UNUSED_FUNCTION -Wno-unused-function
    C_WARN_NO_INT_TO_VOID_POINTER_CAST -Wno-int-to-void-pointer-cast
    C_WARN_NO_MISSING_PROTOTYPES -Wno-missing-prototypes
    C_WARN_NO_DUPLICATE_ENUM -Wno-duplicate-enum
    C_WARN_NO_UNDEF -Wno-undef
    C_WARN_NO_MISSING_NORETURN -Wno-missing-noreturn
    C_WARN_NO_UNUSED_BUT_SET_VARIABLE -Wno-unused-but-set-variable
    C_WARN_NO_DEPRECATED_DECLARATIONS -Wno-deprecated-declarations
    C_WARN_NO_STRICT_PROTOTYPES -Wno-strict-prototypes
    C_WARN_NO_BITWISE_INSTEAD_OF_LOGICAL -Wno-bitwise-instead-of-logical
    C_WARN_NO_IMPLICIT_CONST_INT_FLOAT_CONVERSION -Wno-implicit-const-int-float-conversion
    C_WARN_NO_SINGLE_BIT_BITFIELD_CONSTANT_CONVERSION -Wno-single-bit-bitfield-constant-conversion
  )

  add_check_cxx_compiler_flags(
    CXX_REMOVE_STRICT_FLAGS

    CXX_WARN_NO_NONTRIVIAL_MEMCALL -Wno-nontrivial-memcall
    CXX_WARN_NO_UNUSED_PARAMETER -Wno-unused-parameter
    CXX_WARN_NO_UNUSED_PRIVATE_FIELD -Wno-unused-private-field
    CXX_WARN_NO_CXX11_NARROWING -Wno-c++11-narrowing
    CXX_WARN_NO_NON_VIRTUAL_DTOR -Wno-non-virtual-dtor
    CXX_WARN_NO_UNUSED_MACROS -Wno-unused-macros
    CXX_WARN_NO_UNUSED_VARIABLE -Wno-unused-variable
    CXX_WARN_NO_REORDER -Wno-reorder
    CXX_WARN_NO_COMMENT -Wno-comment
    CXX_WARN_NO_UNUSED_TYPEDEFS -Wno-unused-local-typedefs
    CXX_WARN_NO_UNDEFINED_VAR_TEMPLATE -Wno-undefined-var-template
    CXX_WARN_NO_INSTANTIATION_AFTER_SPECIALIZATION -Wno-instantiation-after-specialization
    CXX_WARN_NO_MISLEADING_INDENTATION -Wno-misleading-indentation
    CXX_WARN_NO_BITWISE_INSTEAD_OF_LOGICAL -Wno-bitwise-instead-of-logical
    CXX_WARN_NO_IMPLICIT_CONST_INT_FLOAT_CONVERSION -Wno-implicit-const-int-float-conversion
    CXX_WARN_NO_UNDEF -Wno-undef
    CXX_WARN_NO_UNDEF_PREFIX -Wno-undef-prefix
    CXX_WARN_NO_INCONSISTENT_MISSING_OVERRIDE -Wno-inconsistent-missing-override
  )

elseif(CMAKE_C_COMPILER_ID STREQUAL "Intel")

  add_check_c_compiler_flags(
    C_WARNINGS

    C_WARN_ALL -Wall
    C_WARN_POINTER_ARITH -Wpointer-arith
    C_WARN_NO_UNKNOWN_PRAGMAS -Wno-unknown-pragmas
  )

  add_check_cxx_compiler_flags(
    CXX_WARNINGS

    CXX_WARN_ALL -Wall
    CXX_WARN_NO_INVALID_OFFSETOF -Wno-invalid-offsetof
    CXX_WARN_NO_SIGN_COMPARE -Wno-sign-compare
  )

  # Disable numbered, false positives.
  string(APPEND C_WARNINGS " -wd188,186,144,913,556,858,597,177,1292,167,279,592,94,2722,3199")
  string(APPEND CXX_WARNINGS " -wd188,186,144,913,556,858,597,177,1292,167,279,592,94,2722,3199")
elseif(CMAKE_C_COMPILER_ID STREQUAL "MSVC")
  # Most MSVC warnings are C & C++.
  set(_WARNINGS
    # warning level:
    "/W3"
    "/w34062"  # switch statement contains 'default' but no 'case' labels
    "/w34100"  # 'identifier' : unreferenced formal parameter
    "/w34115"  # 'type' : named type definition in parentheses
    "/w34189"  # local variable is initialized but not referenced
    # see https://docs.microsoft.com/en-us/cpp/error-messages/compiler-warnings/c5038?view=vs-2017
    "/w35038"  # order of initialization in c++ constructors
    # disable:
    "/wd4018"  # signed/unsigned mismatch
    "/wd4146"  # unary minus operator applied to unsigned type, result still unsigned
    "/wd4065"  # switch statement contains 'default' but no 'case' labels
    "/wd4127"  # conditional expression is constant
    "/wd4181"  # qualifier applied to reference type; ignored
    "/wd4200"  # zero-sized array in struct/union
    "/wd4244"  # conversion from 'type1' to 'type2', possible loss of data
    "/wd4267"  # conversion from 'size_t' to 'type', possible loss of data
    "/wd4305"  # truncation from 'type1' to 'type2'
    "/wd4800"  # forcing value to bool 'true' or 'false'
    "/wd4828"  # The file contains a character that is illegal
    "/wd4996"  # identifier was declared deprecated
    "/wd4661"  # no suitable definition provided for explicit template instantiation request
    "/wd4848"  # 'no_unique_address' is a vendor extension in C++17
    # errors:
    "/we4013"  # 'function' undefined; assuming extern returning int
    "/we4133"  # incompatible pointer types
    "/we4431"  # missing type specifier - int assumed
    "/we4033"  # 'function' must return a value
  )

  string(REPLACE ";" " " _WARNINGS "${_WARNINGS}")
  set(C_WARNINGS "${_WARNINGS}")
  set(CXX_WARNINGS "${_WARNINGS}")
  unset(_WARNINGS)
endif()

# Xcode enables additional warning flags by default. Disable some to match
# command line build and other platforms more closely.
if(XCODE)
  set(CMAKE_XCODE_ATTRIBUTE_GCC_WARN_64_TO_32_BIT_CONVERSION NO)
endif()

# ensure python header is found since detection can fail, this could happen
# with _any_ library but since we used a fixed python version this tends to
# be most problematic.
if(WITH_PYTHON)
  if(NOT EXISTS "${PYTHON_INCLUDE_DIR}/Python.h")
    message(
      FATAL_ERROR
      "Missing: \"${PYTHON_INCLUDE_DIR}/Python.h\",\n"
      "Set the cache entry 'PYTHON_INCLUDE_DIR' to point "
      "to a valid python include path. Containing "
      "Python.h for python version \"${PYTHON_VERSION}\""
    )
  endif()

  if(WIN32)
    # Always use numpy bundled in precompiled libs.
  elseif((WITH_PYTHON_INSTALL AND WITH_PYTHON_INSTALL_NUMPY) OR WITH_PYTHON_NUMPY)
    if(("${PYTHON_NUMPY_PATH}" STREQUAL "") OR (${PYTHON_NUMPY_PATH} MATCHES NOTFOUND))
      set(_numpy_include "_core/include")
      if(PYTHON_VERSION VERSION_LESS "3.13")
        set(_numpy_include "core/include")
      endif()
      find_python_package(numpy "${_numpy_include}")
      unset(_numpy_include)
    endif()
  endif()

  if(WIN32 OR APPLE)
    # Always copy from precompiled libs.
  elseif(WITH_PYTHON_INSTALL_REQUESTS)
    find_python_package(requests "")
  endif()

  if(WIN32 OR APPLE)
    # Always copy from precompiled libs.
  elseif(WITH_PYTHON_INSTALL_ZSTANDARD)
    find_python_package(zstandard "")
  endif()
endif()

# Select C++17 as the standard for C++ projects.
set(CMAKE_CXX_STANDARD 17)
# If C++17 is not available, downgrading to an earlier standard is NOT OK.
set(CMAKE_CXX_STANDARD_REQUIRED ON)
# Do not enable compiler specific language extensions.
set(CMAKE_CXX_EXTENSIONS OFF)

# Visual Studio has all standards it supports available by default
# Clang on windows copies this behavior and does not support these switches
if(CMAKE_COMPILER_IS_GNUCC OR
   (CMAKE_C_COMPILER_ID MATCHES "Clang" AND (NOT MSVC)) OR
   (CMAKE_C_COMPILER_ID STREQUAL "Intel"))

  # Use C11 + GNU extensions, works with GCC, Clang, ICC
  string(APPEND CMAKE_C_FLAGS " -std=gnu11")
endif()

if(WITH_COMPILER_SHORT_FILE_MACRO)
  # Use `-fmacro-prefix-map` for Clang and GCC (MSVC doesn't support this).
  set(C_PREFIX_MAP_FLAGS "")
  set(CXX_PREFIX_MAP_FLAGS "")
  add_check_c_compiler_flags(
    C_PREFIX_MAP_FLAGS
    C_MACRO_PREFIX_MAP -fmacro-prefix-map=foo=bar
  )
  add_check_cxx_compiler_flags(
    CXX_PREFIX_MAP_FLAGS
    CXX_MACRO_PREFIX_MAP -fmacro-prefix-map=foo=bar
  )
  if(C_MACRO_PREFIX_MAP AND CXX_MACRO_PREFIX_MAP)
    if(APPLE)
      if(XCODE AND ${XCODE_VERSION} VERSION_LESS 12.0)
        # Developers may have say LLVM Clang-10.0.1 toolchain (which supports the flag)
        # with Xcode-11 (the Clang of which doesn't support the flag).
        message(
          WARNING
          "-fmacro-prefix-map flag is NOT supported by Clang shipped with Xcode-${XCODE_VERSION}."
          " Some Xcode functionality in Product menu may not work. "
          "Disabling WITH_COMPILER_SHORT_FILE_MACRO."
        )
        set(WITH_COMPILER_SHORT_FILE_MACRO OFF)
      endif()
    endif()
    if(WITH_COMPILER_SHORT_FILE_MACRO)
      path_ensure_trailing_slash(_src_dir "${CMAKE_SOURCE_DIR}")
      path_ensure_trailing_slash(_bin_dir "${CMAKE_BINARY_DIR}")
      # Keep this variable so it can be stripped from build-info.
      set(PLATFORM_CFLAGS_FMACRO_PREFIX_MAP
        "-fmacro-prefix-map=\"${_src_dir}\"=\"\" -fmacro-prefix-map=\"${_bin_dir}\"=\"\"")
      string(APPEND PLATFORM_CFLAGS " ${PLATFORM_CFLAGS_FMACRO_PREFIX_MAP}")
      unset(_src_dir)
      unset(_bin_dir)
    endif()
  else()
    message(
      WARNING
      "-fmacro-prefix-map flag is NOT supported by C/C++ compiler."
      " Disabling WITH_COMPILER_SHORT_FILE_MACRO."
    )
    set(WITH_COMPILER_SHORT_FILE_MACRO OFF)
  endif()
  unset(C_PREFIX_MAP_FLAGS)
  unset(CXX_PREFIX_MAP_FLAGS)
endif()

# Include warnings first, so its possible to disable them with user defined flags
# eg: -Wno-uninitialized
set(CMAKE_C_FLAGS "${C_WARNINGS} ${CMAKE_C_FLAGS} ${PLATFORM_CFLAGS}")
set(CMAKE_CXX_FLAGS "${CXX_WARNINGS} ${CMAKE_CXX_FLAGS} ${PLATFORM_CFLAGS}")

# defined above, platform specific but shared names
mark_as_advanced(
  CYCLES_OSL
  OSL_LIB_EXEC
  OSL_COMPILER
  OSL_LIB_COMP
  OSL_LIB_QUERY
  OSL_INCLUDE_DIR
)

mark_as_advanced(
  LLVM_CONFIG
  LLVM_ROOT_DIR
  LLVM_LIBRARY
  LLVM_VERSION
)


# -------------------------------------------------------------------------------
# Global Defines

if(WITH_ASSERT_ABORT)
  add_definitions(-DWITH_ASSERT_ABORT)
endif()

# NDEBUG is the standard C define to disable asserts.
if(WITH_ASSERT_RELEASE)
  # CMake seemingly be setting the NDEBUG flag on its own already on some configurations
  # therefore we need to remove the flags if they happen to be set.
  remove_cc_flag("-DNDEBUG") # GCC/CLang
  remove_cc_flag("/DNDEBUG") # MSVC
else()
  set_property(DIRECTORY APPEND PROPERTY COMPILE_DEFINITIONS
    $<$<CONFIG:Release>:NDEBUG>
    $<$<CONFIG:MinSizeRel>:NDEBUG>
    $<$<CONFIG:RelWithDebInfo>:NDEBUG>
  )
endif()

# message(STATUS "Using CFLAGS: ${CMAKE_C_FLAGS}")
# message(STATUS "Using CXXFLAGS: ${CMAKE_CXX_FLAGS}")

# -----------------------------------------------------------------------------
# Testing Functions

include(build_files/cmake/testing.cmake)

# -----------------------------------------------------------------------------
# Add Sub-Directories

if(WITH_BLENDER)
  add_subdirectory(intern)
  add_subdirectory(extern)

  # source after intern and extern to gather all
  # internal and external library information first, for test linking
  add_subdirectory(source)
elseif(WITH_CYCLES_STANDALONE OR WITH_CYCLES_HYDRA_RENDER_DELEGATE)
  add_subdirectory(intern/atomic)
  add_subdirectory(intern/guardedalloc)
  add_subdirectory(intern/libc_compat)
  add_subdirectory(intern/sky)

  add_subdirectory(intern/cycles)
  if(WITH_CUDA_DYNLOAD)
    add_subdirectory(extern/cuew)
  endif()
  if(WITH_HIP_DYNLOAD)
    add_subdirectory(extern/hipew)
  endif()
endif()


# -----------------------------------------------------------------------------
# Add Testing Directory

add_subdirectory(tests)


# -----------------------------------------------------------------------------
# Add Blender Application

if(WITH_BLENDER)
  add_subdirectory(source/creator)
endif()


# -----------------------------------------------------------------------------
# Define 'heavy' sub-modules (for Ninja builder when using pools)
setup_heavy_lib_pool()


# -----------------------------------------------------------------------------
# CPack for generating packages

include(build_files/cmake/packaging.cmake)


# -----------------------------------------------------------------------------
# OpenAPI-based Python code generator for data models

include(build_files/cmake/generate_datamodels.cmake)


# -----------------------------------------------------------------------------
# Print Final Configuration

if(FIRST_RUN)

  set(_config_msg "\nBlender Configuration\n=====================")

  function(info_cfg_option
    _setting
    )

    set(_msg "  - ${_setting}")
    string(LENGTH "${_msg}" _len)
    while("40" GREATER "${_len}")
      string(APPEND _msg " ")
      math(EXPR _len "${_len} + 1")
    endwhile()

    set(_config_msg "${_config_msg}\n${_msg}${${_setting}}" PARENT_SCOPE)
  endfunction()

  function(info_cfg_text
    _text
    )

    set(_config_msg "${_config_msg}\n\n  ${_text}" PARENT_SCOPE)
  endfunction()

  message(STATUS "C Compiler:   \"${CMAKE_C_COMPILER_ID}\"")
  message(STATUS "C++ Compiler: \"${CMAKE_CXX_COMPILER_ID}\"")

  info_cfg_text("Build Options:")
  info_cfg_option(WITH_ALEMBIC)
  info_cfg_option(WITH_BULLET)
  info_cfg_option(WITH_CLANG)
  info_cfg_option(WITH_CYCLES)
  info_cfg_option(WITH_FFTW3)
  info_cfg_option(WITH_FREESTYLE)
  info_cfg_option(WITH_GMP)
  info_cfg_option(WITH_HARU)
  info_cfg_option(WITH_IK_ITASC)
  info_cfg_option(WITH_IK_SOLVER)
  info_cfg_option(WITH_INPUT_NDOF)
  info_cfg_option(WITH_INPUT_IME)
  info_cfg_option(WITH_INTERNATIONAL)
  info_cfg_option(WITH_MANIFOLD)
  info_cfg_option(WITH_OPENCOLORIO)
  info_cfg_option(WITH_OPENIMAGEDENOISE)
  info_cfg_option(WITH_OPENSUBDIV)
  info_cfg_option(WITH_OPENVDB)
  info_cfg_option(WITH_POTRACE)
  info_cfg_option(WITH_PUGIXML)
  info_cfg_option(WITH_QUADRIFLOW)
  info_cfg_option(WITH_TBB)
  info_cfg_option(WITH_USD)
  info_cfg_option(WITH_MATERIALX)
  info_cfg_option(WITH_XR_OPENXR)

  info_cfg_text("Compiler Options:")
  info_cfg_option(WITH_BUILDINFO)
  info_cfg_option(WITH_OPTIMIZED_BUILD_TOOLS)

  info_cfg_text("System Options:")
  info_cfg_option(WITH_INSTALL_PORTABLE)
  info_cfg_option(WITH_TBB_MALLOC_PROXY)
  info_cfg_option(WITH_MEM_VALGRIND)

  info_cfg_text("GHOST Options:")
  info_cfg_option(WITH_GHOST_DEBUG)
  info_cfg_option(WITH_GHOST_SDL)
  if(UNIX AND NOT APPLE)
    info_cfg_option(WITH_GHOST_X11)
    info_cfg_option(WITH_GHOST_WAYLAND)
    if(WITH_GHOST_X11)
      info_cfg_option(WITH_GHOST_XDND)
      info_cfg_option(WITH_X11_XFIXES)
      info_cfg_option(WITH_X11_XINPUT)
    endif()
    if(WITH_GHOST_WAYLAND)
      info_cfg_option(WITH_GHOST_WAYLAND_DYNLOAD)
    endif()
  endif()

  info_cfg_text("Image Formats:")
  info_cfg_option(WITH_IMAGE_CINEON)
  info_cfg_option(WITH_IMAGE_OPENEXR)
  info_cfg_option(WITH_IMAGE_OPENJPEG)

  info_cfg_text("Audio:")
  info_cfg_option(WITH_AUDASPACE)
  info_cfg_option(WITH_RUBBERBAND)
  info_cfg_option(WITH_CODEC_FFMPEG)
  info_cfg_option(WITH_CODEC_SNDFILE)
  info_cfg_option(WITH_COREAUDIO)
  info_cfg_option(WITH_JACK)
  info_cfg_option(WITH_JACK_DYNLOAD)
  info_cfg_option(WITH_OPENAL)
  info_cfg_option(WITH_PULSEAUDIO)
  info_cfg_option(WITH_PULSEAUDIO_DYNLOAD)
  info_cfg_option(WITH_SDL)
  info_cfg_option(WITH_WASAPI)

  if(WITH_PYTHON)
    info_cfg_text("Python:")
    info_cfg_option(WITH_PYTHON_INSTALL)
    info_cfg_option(WITH_PYTHON_INSTALL_NUMPY)
    info_cfg_option(WITH_PYTHON_INSTALL_ZSTANDARD)
    info_cfg_option(WITH_PYTHON_MODULE)
    info_cfg_option(WITH_PYTHON_SAFETY)
  endif()

  info_cfg_text("Modifiers:")
  info_cfg_option(WITH_MOD_FLUID)
  info_cfg_option(WITH_MOD_OCEANSIM)
  info_cfg_option(WITH_MOD_REMESH)

  info_cfg_text("Rendering:")
  info_cfg_option(WITH_HYDRA)

  if(WITH_CYCLES)
    info_cfg_text("Rendering (Cycles):")
    info_cfg_option(WITH_CYCLES_OSL)
    info_cfg_option(WITH_CYCLES_EMBREE)
    info_cfg_option(WITH_CYCLES_PATH_GUIDING)
    if(NOT APPLE)
      info_cfg_option(WITH_CYCLES_DEVICE_OPTIX)
      info_cfg_option(WITH_CYCLES_DEVICE_CUDA)
      info_cfg_option(WITH_CYCLES_CUDA_BINARIES)
      info_cfg_option(WITH_CYCLES_DEVICE_ONEAPI)
      info_cfg_option(WITH_CYCLES_ONEAPI_BINARIES)
      info_cfg_option(WITH_CYCLES_DEVICE_HIP)
      info_cfg_option(WITH_CYCLES_HIP_BINARIES)
      info_cfg_option(WITH_CYCLES_DEVICE_HIPRT)
    endif()
  endif()

  info_cfg_text("Tests:")
  info_cfg_option(WITH_GTESTS)
  info_cfg_option(CYCLES_TEST_DEVICES)
  info_cfg_option(WITH_CYCLES_TEST_OSL)
  info_cfg_option(WITH_GPU_RENDER_TESTS)
  info_cfg_option(WITH_GPU_BACKEND_TESTS)
  info_cfg_option(WITH_GPU_DRAW_TESTS)
  info_cfg_option(WITH_GPU_COMPOSITOR_TESTS)
  info_cfg_option(WITH_GPU_MESH_PAINT_TESTS)
  info_cfg_option(WITH_UI_TESTS)
  info_cfg_option(WITH_UI_TESTS_HEADLESS)
  info_cfg_option(WITH_GPU_RENDER_TESTS_VULKAN)
  info_cfg_option(WITH_LINUX_OFFICIAL_RELEASE_TESTS)

  info_cfg_text("")

  message(STATUS "${_config_msg}")
endif()

if(0)
  print_all_vars()
endif()

# Should be the last step of configuration.
if(POSTCONFIGURE_SCRIPT)
  include(${POSTCONFIGURE_SCRIPT})
endif()


--- README.md ---
<!--
Keep this document short & concise,
linking to external resources instead of including content in-line.
See 'release/text/readme.html' for the end user read-me.
-->

Blender
=======

Blender is the free and open source 3D creation suite.
It supports the entirety of the 3D pipelinemodeling, rigging, animation, simulation, rendering, compositing,
motion tracking and video editing.

![Blender screenshot](https://code.blender.org/wp-content/uploads/2018/12/springrg.jpg "Blender screenshot")

Project Pages
-------------

- [Main Website](http://www.blender.org)
- [Reference Manual](https://docs.blender.org/manual/en/latest/index.html)
- [User Community](https://www.blender.org/community/)

Development
-----------

- [Build Instructions](https://developer.blender.org/docs/handbook/building_blender/)
- [Code Review & Bug Tracker](https://projects.blender.org)
- [Developer Forum](https://devtalk.blender.org)
- [Developer Documentation](https://developer.blender.org/docs/)


License
-------

Blender as a whole is licensed under the GNU General Public License, Version 3.
Individual files may have a different but compatible license.

See [blender.org/about/license](https://www.blender.org/about/license) for details.


## Links discovered
- [Main Website](http://www.blender.org)
- [Reference Manual](https://docs.blender.org/manual/en/latest/index.html)
- [User Community](https://www.blender.org/community/)
- [Build Instructions](https://developer.blender.org/docs/handbook/building_blender/)
- [Code Review & Bug Tracker](https://projects.blender.org)
- [Developer Forum](https://devtalk.blender.org)
- [Developer Documentation](https://developer.blender.org/docs/)
- [blender.org/about/license](https://www.blender.org/about/license)

--- assets/blender_assets.cats.txt ---
# This is an Asset Catalog Definition file for Blender.
#
# Empty lines and lines starting with `#` will be ignored.
# The first non-ignored line should be the version indicator.
# Other lines are of the format "UUID:catalog/path/for/assets:simple catalog name"

VERSION 1

753df37f-2da1-4c8e-bf3e-3cbf24387e66:Brushes:Brushes
8c4f15ab-755e-4929-9fd2-609157d8067f:Brushes/Curve Sculpt:Brushes-Curve Sculpt
851d0192-113d-4737-b5d7-ab054198bdaa:Brushes/Curve Sculpt/Deform:Brushes-Curve Sculpt-Deform
754a6925-6a06-476e-a714-25db953c80ea:Brushes/Curve Sculpt/Density:Brushes-Curve Sculpt-Density
2c511e86-866e-4379-93db-7d25ac850a02:Brushes/Curve Sculpt/General:Brushes-Curve Sculpt-General
00c402c1-8f55-45ab-bb26-b4c57c9e87ad:Brushes/Grease Pencil Draw:Brushes-Grease Pencil Draw
f14d7a24-428a-4753-bf88-ef136a66be73:Brushes/Grease Pencil Draw/Draw:Brushes-Grease Pencil Draw-Draw
db855991-aff2-45d1-9ba5-cd0dccea5fe0:Brushes/Grease Pencil Draw/Erase:Brushes-Grease Pencil Draw-Erase
bef62e73-63d2-43e6-89fd-e72245350bc7:Brushes/Grease Pencil Draw/Utilities:Brushes-Grease Pencil Draw-Utilities
98c3c158-b363-48d3-80cd-ce4846f5aed1:Brushes/Grease Pencil Paint:Brushes-Grease Pencil Paint
eb75aff6-7006-4ecf-a41d-48c14e09c32c:Brushes/Grease Pencil Sculpt:Brushes-Grease Pencil Sculpt
2afe28e4-1769-4262-ae62-7e8c17e64246:Brushes/Grease Pencil Sculpt/Contrast:Brushes-Grease Pencil Sculpt-Contrast
2cfbd59a-9b11-48f1-b35d-665b70c112cf:Brushes/Grease Pencil Sculpt/Transform:Brushes-Grease Pencil Sculpt-Transform
ca10d93d-0283-480d-b868-b4275f8bc79a:Brushes/Grease Pencil Sculpt/Utilities:Brushes-Grease Pencil Sculpt-Utilities
9601079f-86e9-4a6f-ba04-56ba1bef3a9f:Brushes/Grease Pencil Weight Paint:Brushes-Grease Pencil Weight Paint
19f885c8-9bd7-43f7-98c1-104118c06f60:Brushes/Mesh Sculpt:Brushes-Mesh Sculpt
194fda97-6d88-41b1-b26d-f41cd8426518:Brushes/Mesh Sculpt/General:Brushes-Mesh Sculpt-General
45d61c81-d318-4835-a37f-54af720391e6:Brushes/Mesh Sculpt/General/Add & Subtract:Brushes-Mesh Sculpt-General-Add & Subtract
650ad315-0880-451e-8f49-7339af60808a:Brushes/Mesh Sculpt/General/Contrast:Brushes-Mesh Sculpt-General-Contrast
a1131a99-c34b-4a28-b6d7-ce61fdbf2573:Brushes/Mesh Sculpt/General/Transform:Brushes-Mesh Sculpt-General-Transform
4900134b-ead6-413d-b6bf-3fb6ff43bbe4:Brushes/Mesh Sculpt/General/Utilities:Brushes-Mesh Sculpt-General-Utilities
eb98fe9e-0baa-417f-8ff1-d56dd79d2ce2:Brushes/Mesh Sculpt/General/Utilities:Brushes-Mesh Sculpt-General-Utilities
722259b1-4abf-4d15-8b17-7875c8efb248:Brushes/Mesh Sculpt/Paint:Brushes-Mesh Sculpt-Paint
abea2557-f13e-4e21-b4d8-11a1fffd7d57:Brushes/Mesh Sculpt/Simulation:Brushes-Mesh Sculpt-Simulation
a82f0b44-1d89-4ca7-ae21-86296257be99:Brushes/Mesh Texture Paint:Brushes-Mesh Texture Paint
d3581edf-cc21-4fb6-85e1-7c682a584517:Brushes/Mesh Texture Paint/Basic:Brushes-Mesh Texture Paint-Basic
63415de5-6391-412a-8cc5-419a11180480:Brushes/Mesh Texture Paint/Erase:Brushes-Mesh Texture Paint-Erase
5e82266d-6884-4d6c-b463-b3385ecb0244:Brushes/Mesh Texture Paint/Pixel Art:Brushes-Mesh Texture Paint-Pixel Art
f57862b2-0845-4ce7-a860-ce343dee67b9:Brushes/Mesh Texture Paint/Utilities:Brushes-Mesh Texture Paint-Utilities
27cbc8dc-80c8-45d5-863c-7ecc48bccf20:Brushes/Mesh Vertex Paint:Brushes-Mesh Vertex Paint
77635e98-1a5a-482e-9cb1-ba9ec4bc5b74:Brushes/Mesh Weight Paint:Brushes-Mesh Weight Paint
81249800-1dfd-4eec-9257-20019aa3ef5d:Camera & Lens Effects:Camera & Lens Effects
679c9c61-2227-41c9-b483-0112dcd65b71:Creative:Creative
007a2c8d-f113-421b-a39c-8fbc76c4f7fe:Generate:Generate
6ecdfe44-a0d7-40b9-8518-b92c97a1b4f2:Generate:Generate
a7b80fd1-5de6-4c15-9049-52f25f4376de:Geometry:Geometry
463e71f0-6db1-41af-bc96-6f875196aac7:Geometry/Operations:Geometry-Operations
5fa1e91c-e798-4051-8e4d-5d47f76f789b:Geometry/Selection:Geometry-Selection
f14d3c5d-8223-4e8c-b3b3-cbce81e4b3ee:Hair:Hair
87cbaad7-ae4e-404c-9b6b-4fe60ecc39dc:Hair/Deformation:Hair-Deformation
40aedbf9-be4b-4ddb-8eec-8a9cd37d0921:Hair/Generation:Hair-Generation
09af8c50-8c07-4039-821c-be801761d7cf:Hair/Guides:Hair-Guides
c676fe02-8d25-49a1-b672-11aee0918221:Hair/Read:Hair-Read
9dedc75a-afe5-472a-9e3f-a555a8df3dff:Hair/Utility:Hair-Utility
63a83f9c-5a95-476a-9bd5-fcb72414ea0b:Hair/Write:Hair-Write
a01301f0-5ae0-4627-91a4-b7fde91e5c5a:Instances:Instances
b9d488d7-441e-4f1a-905a-64ab84c31706:Mesh:Mesh
b6bb38bb-bfe1-4a12-b2bc-fc87d060864f:Mesh/Read:Mesh-Read
9e98eca8-e987-44d3-88a1-6633d0a8ad82:Normals:Normals
b8945cf7-3045-4675-aae3-0b0eba853415:Utilities:Utilities
8c0273f0-3645-449f-9d95-c4f17fb910db:Utilities/Vector:Utilities-Vector


--- .gitea/default_merge_message/REBASE_TEMPLATE.md ---
${CommitTitle}

${CommitBody}

Pull Request: https://projects.blender.org/blender/blender/pulls/${PullRequestIndex}


--- .gitea/default_merge_message/SQUASH_TEMPLATE.md ---
${PullRequestTitle}

Pull Request: https://projects.blender.org/blender/blender/pulls/${PullRequestIndex}


--- .github/README.md ---
<!--
Keep this document short & concise,
linking to external resources instead of including content in-line.
See 'release/text/readme.html' for the end user read-me.
-->

> [!IMPORTANT]
> Cloning from this [GitHub mirror](https://github.com/blender/blender) may cause Git LFS errors. To avoid this, use `GIT_LFS_SKIP_SMUDGE=1` when doing your initial clone.  
> See [the documentation](https://developer.blender.org/docs/handbook/contributing/using_git/#github-mirror) for full instructions.

Blender
=======

Blender is the free and open source 3D creation suite.
It supports the entirety of the 3D pipelinemodeling, rigging, animation, simulation, rendering, compositing,
motion tracking and video editing.

![Blender screenshot](https://code.blender.org/wp-content/uploads/2018/12/springrg.jpg "Blender screenshot")

Project Pages
-------------

- [Main Website](http://www.blender.org)
- [Reference Manual](https://docs.blender.org/manual/en/latest/index.html)
- [User Community](https://www.blender.org/community/)

Development
-----------

- [Build Instructions](https://developer.blender.org/docs/handbook/building_blender/)
- [Code Review & Bug Tracker](https://projects.blender.org)
- [Developer Forum](https://devtalk.blender.org)
- [Developer Documentation](https://developer.blender.org/docs/)


License
-------

Blender as a whole is licensed under the GNU General Public License, Version 3.
Individual files may have a different but compatible license.

See [blender.org/about/license](https://www.blender.org/about/license) for details.


## Links discovered
- [GitHub mirror](https://github.com/blender/blender)
- [the documentation](https://developer.blender.org/docs/handbook/contributing/using_git/#github-mirror)
- [Main Website](http://www.blender.org)
- [Reference Manual](https://docs.blender.org/manual/en/latest/index.html)
- [User Community](https://www.blender.org/community/)
- [Build Instructions](https://developer.blender.org/docs/handbook/building_blender/)
- [Code Review & Bug Tracker](https://projects.blender.org)
- [Developer Forum](https://devtalk.blender.org)
- [Developer Documentation](https://developer.blender.org/docs/)
- [blender.org/about/license](https://www.blender.org/about/license)

--- .github/pull_request_template.md ---
This repository is only used as a mirror. Blender development happens on projects.blender.org.

To get started with contributing code, please see:
https://developer.blender.org/docs/handbook/contributing/


--- build_files/buildbot/README.md ---
Buildbot Configuration
=====================

Files used by Buildbot's `compile-code` step.


--- build_files/config/README.md ---
Pipeline Config
===============

The `yaml` configuration file is used by buildbot build pipeline `update-code` step.

The file allows to set branches or specific commits for both git submodules and svn artifacts. Can also define various build package versions for use by build workers. Especially useful in experimental and release branches. 

NOTE:
* The configuration file is ```NOT``` used by the `../utils/make_update.py` script.
* That will implemented in the future.


--- build_files/utils/README.md ---
Make Utility Scripts
====================

Scripts used only by developers for now

Note: these scripts are assumed to be part of the deployment process, and thus
have to be able to run on older Python versions (3.6 at the moment of writing)
than the one bundled with Blender itself.


--- build_files/package_spec/build_archive.py ---
#!/usr/bin/env python3
# SPDX-FileCopyrightText: 2011-2022 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

__all__ = (
    "main",
)

import os
import shutil
import subprocess
import sys

# todo:
# strip executables


def main() -> int:
    # get parameters
    if len(sys.argv) < 5:
        sys.stderr.write('Excepted arguments: ./build_archive.py name extension install_dir output_dir')
        return 1

    package_name = sys.argv[1]
    extension = sys.argv[2]
    install_dir = sys.argv[3]
    output_dir = sys.argv[4]

    package_archive = os.path.join(output_dir, package_name + '.' + extension)
    package_dir = package_name

    # remove existing package with the same name
    try:
        if os.path.exists(package_archive):
            os.remove(package_archive)
        if os.path.exists(package_dir):
            shutil.rmtree(package_dir)
    except Exception as ex:
        sys.stderr.write('Failed to clean up old package files: ' + str(ex) + '\n')
        return 1

    # create temporary package dir
    try:
        shutil.copytree(install_dir, package_dir)

        for f in os.listdir(package_dir):
            if f.startswith('makes'):
                os.remove(os.path.join(package_dir, f))
    except Exception as ex:
        sys.stderr.write('Failed to copy install directory: ' + str(ex) + '\n')
        return 1

    # create archive
    try:
        if not os.path.exists(output_dir):
            os.mkdir(output_dir)

        archive_env = os.environ.copy()

        if extension == 'zip':
            archive_cmd = ['zip', '-9', '-r', package_archive, package_dir]
        elif extension == 'tar.xz':
            archive_cmd = ['tar', '-cf', package_archive, '--owner=0', '--group=0',
                           '--use-compress-program=xz', package_dir]
            archive_env['XZ_OPT'] = '-9'
        else:
            sys.stderr.write('Unknown archive extension: ' + extension)
            return 1

        subprocess.check_call(archive_cmd, env=archive_env)
    except Exception as ex:
        sys.stderr.write('Failed to create package archive: ' + str(ex) + '\n')
        return 1

    # empty temporary package dir
    try:
        shutil.rmtree(package_dir)
    except Exception as ex:
        sys.stderr.write('Failed to clean up package directory: ' + str(ex) + '\n')
        return 1

    return 0


if __name__ == "__main__":
    sys.exit(main())


--- build_files/cmake/cmake_print_build_options.py ---
# SPDX-FileCopyrightText: 2018-2023 Blender Authors
#
# SPDX-License-Identifier: Apache-2.0

# Simple utility that prints all `WITH_*` options in a `CMakeLists.txt`.
# Called by `make help_features`.

__all__ = (
    "main",
)

import re
import sys

from typing import (
    Optional,
)


def count_backslashes_before_pos(file_data: str, pos: int) -> int:
    slash_count = 0
    pos -= 1
    while pos >= 0:
        if file_data[pos] != '\\':
            break
        pos -= 1
        slash_count += 1
    return slash_count


def extract_cmake_string_at_pos(file_data: str, pos_beg: int) -> Optional[str]:
    assert file_data[pos_beg - 1] == '"'

    pos = pos_beg
    # Dummy assignment.
    pos_end = pos_beg
    while True:
        pos_next = file_data.find('"', pos)
        if pos_next == -1:
            raise Exception("Un-terminated string (parse error?)")

        count_slashes = count_backslashes_before_pos(file_data, pos_next)
        if (count_slashes % 2) == 0:
            pos_end = pos_next
            # Found the closing quote.
            break

        # The quote was back-slash escaped, step over it.
        pos = pos_next + 1
        file_data[pos_next]

    assert file_data[pos_end] == '"'

    if pos_beg == pos_end:
        return None

    # See: https://cmake.org/cmake/help/latest/manual/cmake-language.7.html#escape-sequences
    text = file_data[pos_beg: pos_end].replace(
        # Handle back-slash literals.
        "\\\\", "\\",
    ).replace(
        # Handle tabs.
        "\\t", "\t",
    ).replace(
        # Handle escaped quotes.
        "\\\"", "\"",
    ).replace(
        # Handle tabs.
        "\\;", ";",
    ).replace(
        # Handle trailing newlines.
        "\\\n", "",
    )

    return text


def main() -> int:
    cmakelists_file = sys.argv[-1]

    options = []
    with open(cmakelists_file, 'r', encoding="utf-8") as fh:
        file_data = fh.read()

    for m in re.finditer(r"^\s*option\s*\(\s*(WITH_[a-zA-Z0-9_]+)\s+(\")", file_data, re.MULTILINE):
        option_name = m.group(1)
        option_descr = extract_cmake_string_at_pos(file_data, m.span(2)[1])
        if option_descr is None:
            # Possibly a parsing error, at least show something.
            option_descr = "(UNDOCUMENTED)"
        options.append("{:s}: {:s}".format(option_name, option_descr))

    print('\n'.join(options))
    return 0


if __name__ == "__main__":
    main()


--- build_files/build_environment/CMakeLists.txt ---
# SPDX-FileCopyrightText: 2023 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

##################################################################################################
#
# This is a build system used by platform maintainers to build library dependencies.
# For users building Blender, we recommend using the pre-compiled libraries from `lib/`
# on all platforms.
#
# This CMake project is usually built through wrappers:
# * Windows: `build_files/build_environment/windows/build_deps.cmd`.
# * macOS and Linux: `make deps` from the blender directory.
#
##################################################################################################

cmake_minimum_required(VERSION 3.10)
project("BlenderDependencies")
if(POLICY CMP0135)
  cmake_policy(SET CMP0135 NEW) # CMake 3.24+ Set the date/time for extracted files to time of extraction
endif()
include(ExternalProject)

list(APPEND CMAKE_MODULE_PATH "${CMAKE_SOURCE_DIR}/../cmake/Modules")

include(cmake/check_software.cmake)
include(cmake/options.cmake)
# `versions.cmake` needs to be included after `options.cmake`
# due to the `BLENDER_PLATFORM_ARM` variable being needed.
include(cmake/versions.cmake)
include(cmake/download.cmake)
include(cmake/macros.cmake)
# `setup_msys2.cmake` to install `perl`,
# which HIP needs to function for the compiler detection to work properly,
# hence needs to be included before `check_compilers.cmake`.
if(ENABLE_MSYS2)
  include(cmake/setup_msys2.cmake)
else()
  set(mingw_LIBDIR ${LIBDIR})
endif()
include(cmake/check_compilers.cmake)
include(cmake/harvest.cmake)

include(cmake/ssl.cmake)
include(cmake/zlib.cmake)
include(cmake/zstd.cmake)
include(cmake/openal.cmake)
include(cmake/png.cmake)
include(cmake/jpeg.cmake)
include(cmake/blosc.cmake)
include(cmake/pthreads.cmake)
include(cmake/deflate.cmake)
include(cmake/imath.cmake)
include(cmake/openexr.cmake)
include(cmake/brotli.cmake)
include(cmake/freetype.cmake)
include(cmake/epoxy.cmake)
include(cmake/alembic.cmake)
include(cmake/opensubdiv.cmake)
include(cmake/sdl.cmake)
if(UNIX)
  include(cmake/nasm.cmake)
endif()
include(cmake/tiff.cmake)
if(WIN32)
  include(cmake/flexbison.cmake)
elseif(UNIX AND NOT APPLE)
  include(cmake/flex.cmake)
endif()
include(cmake/tbb.cmake)
include(cmake/python.cmake)
include(cmake/llvm.cmake)
include(cmake/osl.cmake)
include(cmake/cython.cmake)
include(cmake/numpy.cmake)
include(cmake/zstandard.cmake)
include(cmake/python_site_packages.cmake)
include(cmake/package_python.cmake)
include(cmake/openimageio.cmake)
include(cmake/usd.cmake)
include(cmake/materialx.cmake)
include(cmake/openvdb.cmake)
include(cmake/potrace.cmake)
include(cmake/haru.cmake)
include(cmake/pugixml.cmake)
include(cmake/fribidi.cmake)
include(cmake/harfbuzz.cmake)
if(NOT APPLE)
  include(cmake/xr_openxr.cmake)
  if(NOT BLENDER_PLATFORM_ARM)
    include(cmake/hiprt.cmake)
    include(cmake/dpcpp.cmake)
    include(cmake/dpcpp_deps.cmake)
    include(cmake/emhash.cmake)
    include(cmake/parallelhashmap.cmake)
    if(NOT WIN32)
      include(cmake/igc.cmake)
      include(cmake/gmmlib.cmake)
      include(cmake/ocloc.cmake)
    endif()
  endif()
endif()
include(cmake/ispc.cmake)
if(BLENDER_PLATFORM_WINDOWS_ARM)
  include(cmake/openpgl_windows_arm.cmake)
else()
  include(cmake/openpgl.cmake)
endif()
# EMBREE needs to be included after `dpcpp` as it uses it for compiling with GPU support
if(BLENDER_PLATFORM_WINDOWS_ARM)
  # WoA needs EMBREE to be built with the VS Generator + LLVM,
  # put it in its own file to avoid clutter.
  include(cmake/embree_windows_arm.cmake)
else()
  include(cmake/embree.cmake)
endif()
include(cmake/fmt.cmake)
include(cmake/robinmap.cmake)
include(cmake/xml2.cmake)
# OpenColorIO and dependencies.
include(cmake/expat.cmake)
include(cmake/pystring.cmake)
include(cmake/yamlcpp.cmake)
include(cmake/minizipng.cmake)
include(cmake/opencolorio.cmake)

if(BLENDER_PLATFORM_ARM)
  include(cmake/sse2neon.cmake)
endif()

include(cmake/webp.cmake)
if(NOT APPLE)
  include(cmake/level-zero.cmake)
endif()

if(NOT WIN32 OR ENABLE_MSYS2)
  include(cmake/gmp.cmake)
  include(cmake/openjpeg.cmake)
  include(cmake/sqlite.cmake)
  include(cmake/fftw.cmake)
  if(NOT WIN32 OR BUILD_MODE STREQUAL Release)
    include(cmake/openimagedenoise.cmake)
    include(cmake/lame.cmake)
    include(cmake/ogg.cmake)
    include(cmake/vorbis.cmake)
    include(cmake/theora.cmake)
    include(cmake/opus.cmake)
    include(cmake/vpx.cmake)
    include(cmake/x264.cmake)
    include(cmake/x265.cmake)
    include(cmake/aom.cmake)
    include(cmake/ffmpeg.cmake)
    include(cmake/flac.cmake)
    include(cmake/sndfile.cmake)
    if(UNIX)
      if(NOT APPLE)
        include(cmake/spnav.cmake)
      endif()
    endif()
  endif()
endif()

if(UNIX)
  include(cmake/bzip2.cmake)
  include(cmake/ffi.cmake)
  include(cmake/lzma.cmake)
endif()

if(UNIX AND NOT APPLE)
  include(cmake/libglu.cmake)
  include(cmake/mesa.cmake)
  include(cmake/wayland_protocols.cmake)
  # Can be removed when the build-bot upgrades to v1.20.x or newer.
  include(cmake/wayland.cmake)
  include(cmake/wayland_weston.cmake)
endif()
include(cmake/shaderc_deps.cmake)
include(cmake/shaderc.cmake)
include(cmake/vulkan.cmake)
include(cmake/vulkan-memory-allocator.cmake)
include(cmake/spirv-reflect.cmake)
include(cmake/pybind11.cmake)
include(cmake/nanobind.cmake)
include(cmake/manifold.cmake)
include(cmake/rubberband.cmake)
# Keep these last.
include(cmake/deps_html.cmake)
include(cmake/cve_check.cmake)

if(UNIX)
  # Strip all installed libraries.
  harvest_strip_all_libraries()
endif()


--- build_files/utils/make_bpy_wheel.py ---
#!/usr/bin/env python3
# SPDX-FileCopyrightText: 2022-2023 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

"""
Make Python wheel package (`*.whl`) file from Blender built with 'WITH_PYTHON_MODULE' enabled.

Example
=======

If the "bpy" module was build on Linux using the command:

   make bpy lite

The command to package it as a wheel is:

   ./build_files/utils/make_bpy_wheel.py ../build_linux_bpy_lite/bin --output-dir=./

This will create a `*.whl` file in the current directory.

WARNING:
Python 3.9 is used on the built-bot.
Take care *not* to use features from the Python version used by Blender!

NOTE:
Some type annotations are quoted to avoid errors in Python 3.9.
These can be unquoted eventually.
"""
__all__ = (
    "main",
)

import argparse
import make_utils
import os
import re
import platform
import string
import setuptools
import sys

from typing import (
    Tuple,
    # Proxies for `collections.abc`
    Iterator,
    Sequence,
)

# ------------------------------------------------------------------------------
# Long Description

long_description = """# Blender

[Blender](https://www.blender.org) is the free and open source 3D creation suite. It supports the entirety of the 3D pipeline: modeling, rigging, animation, simulation, rendering, compositing and motion tracking, even video editing.

This package provides Blender as a Python module for use in studio pipelines, web services, scientific research, and more.

### Archived Versions

Blender versions outside the current LTS window are removed from PyPI but are available at [https://download.blender.org/pypi/bpy/](https://download.blender.org/pypi/bpy/).

These versions can still be installed manually. For example, to install version 3.6.0:

```bash
pip install bpy==3.6.0 --extra-index-url https://download.blender.org/pypi/
```

## Documentation

* [Blender Python API](https://docs.blender.org/api/current/)
* [Blender as a Python Module](https://docs.blender.org/api/current/info_advanced_blender_as_bpy.html)

## Requirements

[System requirements](https://www.blender.org/download/requirements/) are the same as Blender.

Each Blender release supports one Python version, and the package is only compatible with that version.

## Source Code

* [Releases](https://download.blender.org/source/)
* Repository: [projects.blender.org/blender/blender.git](https://projects.blender.org/blender/blender)

## Credits

Created by the [Blender developer community](https://www.blender.org/about/credits/).

Thanks to Tyler Alden Gubala for maintaining the original version of this package."""

# ------------------------------------------------------------------------------
# Generic Functions


def find_dominating_file(
    path: str,
    search: Sequence[str],
) -> str:
    while True:
        for d in search:
            if os.path.exists(os.path.join(path, d)):
                return os.path.join(path, d)
        path_next = os.path.normpath(os.path.join(path, ".."))
        if path == path_next:
            break
        path = path_next
    return ""


# ------------------------------------------------------------------------------
# CMake Cache Access

def cmake_cache_var_iter(filepath_cmake_cache: str) -> Iterator[Tuple[str, str, str]]:
    re_cache = re.compile(r"([A-Za-z0-9_\-]+)?:?([A-Za-z0-9_\-]+)?=(.*)$")
    with open(filepath_cmake_cache, "r", encoding="utf-8") as cache_file:
        for l in cache_file:
            match = re_cache.match(l.strip())
            if match is not None:
                var, type_, val = match.groups()
                yield (var, type_ or "", val)


def cmake_cache_var(filepath_cmake_cache: str, var: str) -> "str | None":
    for var_iter, _type_iter, value_iter in cmake_cache_var_iter(filepath_cmake_cache):
        if var == var_iter:
            return value_iter
    return None


def cmake_cache_var_or_exit(filepath_cmake_cache: str, var: str) -> str:
    value = cmake_cache_var(filepath_cmake_cache, var)
    if value is None:
        sys.stderr.write("Unable to find %r in %r, abort!\n" % (var, filepath_cmake_cache))
        sys.exit(1)
    return value


# ------------------------------------------------------------------------------
# Argument Parser

def argparse_create() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
    parser.add_argument(
        "install_dir",
        metavar='INSTALL_DIR',
        type=str,
        help="The installation directory containing the \"bpy\" package.",
    )
    parser.add_argument(
        "--build-dir",
        metavar='BUILD_DIR',
        default=None,
        help="The build directory containing 'CMakeCache.txt' (search parent directories of INSTALL_DIR when omitted).",
        required=False,
    )
    parser.add_argument(
        "--output-dir",
        metavar='OUTPUT_DIR',
        default=None,
        help="The destination directory for the '*.whl' file (use INSTALL_DIR when omitted).",
        required=False,
    )

    return parser


# ------------------------------------------------------------------------------
# Main Function

def main() -> None:

    # Parse arguments.
    args = argparse_create().parse_args()

    install_dir = os.path.abspath(args.install_dir)
    output_dir = os.path.abspath(args.output_dir) if args.output_dir else install_dir

    if args.build_dir:
        build_dir = os.path.abspath(args.build_dir)
        filepath_cmake_cache = os.path.join(build_dir, "CMakeCache.txt")
        del build_dir
        if not os.path.exists(filepath_cmake_cache):
            sys.stderr.write("File not found %r, abort!\n" % filepath_cmake_cache)
            sys.exit(1)
    else:
        filepath_cmake_cache = find_dominating_file(install_dir, ("CMakeCache.txt",))
        if not filepath_cmake_cache:
            # Should never fail.
            sys.stderr.write("Unable to find CMakeCache.txt in or above %r, abort!\n" % install_dir)
            sys.exit(1)

    # Get the major and minor Python version.
    python_version = cmake_cache_var_or_exit(filepath_cmake_cache, "PYTHON_VERSION")
    python_version_number = (
        tuple(int("".join(c for c in digit if c in string.digits)) for digit in python_version.split(".")) +
        # Support version without a minor version "3" (add zero).
        tuple((0, 0, 0))
    )

    # Get Blender version.
    blender_version_str = str(make_utils.parse_blender_version())

    # Set platform tag following conventions.
    if sys.platform == "darwin":
        target = cmake_cache_var_or_exit(filepath_cmake_cache, "CMAKE_OSX_DEPLOYMENT_TARGET").split(".")
        # Minor version is expected to be always zero starting with macOS 11.
        # https://github.com/pypa/packaging/issues/435
        target_major = int(target[0])
        target_minor = 0  # int(target[1])
        machine = cmake_cache_var_or_exit(filepath_cmake_cache, "CMAKE_OSX_ARCHITECTURES")
        platform_tag = "macosx_%d_%d_%s" % (target_major, target_minor, machine)
    elif sys.platform == "win32":
        # Workaround for Python process running in a virtualized environment on Windows-on-Arm:
        # use the actual processor architecture instead of the virtualized one.
        #
        # The win_arm64 matches the behavior when native WoA Python is used, and also matches
        # sysconfig.get_platform() from a native Python build (although it returns win-arm64 with a
        # dash and not underscore).
        if "ARM" in os.environ.get("PROCESSOR_IDENTIFIER", ""):
            platform_tag = "win_arm64"
        else:
            platform_tag = "win_%s" % (platform.machine().lower())
    elif sys.platform == "linux":
        glibc = os.confstr("CS_GNU_LIBC_VERSION")
        if glibc is None:
            sys.stderr.write("Unable to find \"CS_GNU_LIBC_VERSION\", abort!\n")
            sys.exit(1)
        glibc = "%s_%s" % tuple(glibc.split()[1].split(".")[:2])
        platform_tag = "manylinux_%s_%s" % (glibc, platform.machine().lower())
    else:
        sys.stderr.write("Unsupported platform: %s, abort!\n" % (sys.platform))
        sys.exit(1)

    # Manually specify, otherwise it uses the version of the executable used to run
    # this script which may not match the Blender python version.
    python_tag = "py%d%d" % (python_version_number[0], python_version_number[1])

    os.chdir(install_dir)

    # Include all files recursively.
    def package_files(root_dir: str) -> list[str]:
        paths = []
        for path, dirs, files in os.walk(root_dir):
            paths += [os.path.join("..", path, f) for f in files]
        return paths

    # Ensure this wheel is marked platform specific.
    class BinaryDistribution(setuptools.dist.Distribution):
        def has_ext_modules(self) -> bool:
            return True

    # Build wheel.
    sys.argv = [sys.argv[0], "bdist_wheel"]

    setuptools.setup(
        name="bpy",
        version=blender_version_str,

        # `bpy` is not compatible with `numpy` 2+, as the VFX reference platform uses
        # 1.26 at the moment. This fix amended the install requirement package to specify
        # `numpy>=1.26,<2.0` to mitigate this issue.
        install_requires=["cython", "numpy>=1.26,<2.0", "requests", "zstandard"],

        python_requires="==%d.%d.*" % (python_version_number[0], python_version_number[1]),
        packages=["bpy"],
        package_data={"": package_files("bpy")},
        distclass=BinaryDistribution,
        options={"bdist_wheel": {"plat_name": platform_tag, "python_tag": python_tag}},

        description="Blender as a Python module",
        long_description=long_description,
        long_description_content_type='text/markdown',
        license="GPL-3.0",
        author="Blender Foundation",
        url="https://www.blender.org"
    )

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Move wheel to output directory.
    dist_dir = os.path.join(install_dir, "dist")
    for f in os.listdir(dist_dir):
        if f.endswith(".whl"):
            blender_py = "cp%d%d" % (python_version_number[0], python_version_number[1])

            # No apparent way to override this ABI version with setuptools, so rename.
            sys_py = "cp%d%d" % (sys.version_info.major, sys.version_info.minor)
            if hasattr(sys, "abiflags"):
                sys_py_abi = sys_py + sys.abiflags
                renamed_f = f.replace(sys_py_abi, blender_py).replace(sys_py, blender_py)
            else:
                renamed_f = f.replace(sys_py, blender_py)

            os.rename(os.path.join(dist_dir, f), os.path.join(output_dir, renamed_f))


if __name__ == "__main__":
    main()


## Links discovered
- [Blender](https://www.blender.org)
- [https://download.blender.org/pypi/bpy/](https://download.blender.org/pypi/bpy/)
- [Blender Python API](https://docs.blender.org/api/current/)
- [Blender as a Python Module](https://docs.blender.org/api/current/info_advanced_blender_as_bpy.html)
- [System requirements](https://www.blender.org/download/requirements/)
- [Releases](https://download.blender.org/source/)
- [projects.blender.org/blender/blender.git](https://projects.blender.org/blender/blender)
- [Blender developer community](https://www.blender.org/about/credits/)

--- build_files/utils/make_source_archive.py ---
#!/usr/bin/env python3
# SPDX-FileCopyrightText: 2021-2023 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

__all__ = (
    "main",
)

import argparse
import make_utils
import os
import subprocess
import sys
from pathlib import Path

from typing import (
    TextIO,
    Any,
    Tuple,
    Union,
    # Proxies for `collections.abc`
    Iterable,
    List,
)

# This script can run from any location,
# output is created in the $CWD
#
# NOTE: while the Python part of this script is portable,
# it relies on external commands typically found on GNU/Linux.
# Support for other platforms could be added by moving GNU `tar` & `md5sum` use to Python.
# This also relies on having a Unix shell (sh) to run some git commands.

SKIP_NAMES: Tuple[str, ...] = (
    ".gitignore",
    ".gitmodules",
    ".gitattributes",
    ".git-blame-ignore-revs",
    ".arcconfig",
    ".svn",
)

# Tuple with folder names relative to the main repository root that are to be excluded.
SKIP_FOLDERS: Tuple[str, ...] = (
)

# Generated list of Paths to be ignored based on the SKIP_FOLDERS and some rum-time rules like
# the type of package that is being created.
SKIP_PATHS: List[Path] = []


def main() -> None:
    blender_srcdir = Path(__file__).absolute().parent.parent.parent

    cli_parser = argparse.ArgumentParser(
        description="Create a tarball of the Blender sources, optionally including sources of dependencies.",
        epilog="This script is intended to be run by `make source_archive_complete`.",
    )
    group = cli_parser.add_mutually_exclusive_group()
    group.add_argument(
        "-p",
        "--include-packages",
        type=Path,
        default=None,
        metavar="PACKAGE_PATH",
        help="Include all source files from the given package directory as well.",
    )
    group.add_argument(
        "-t",
        "--package-test-data",
        action='store_true',
        help="Package all test data into its own archive",
    )

    cli_args = cli_parser.parse_args()

    print(f"Source dir: {blender_srcdir}")

    curdir = blender_srcdir.parent
    os.chdir(curdir)
    blender_srcdir = blender_srcdir.relative_to(curdir)

    # Update our SKIP_FOLDERS blacklist with the source directory name
    global SKIP_PATHS
    SKIP_PATHS = [blender_srcdir / entry for entry in SKIP_FOLDERS]

    print(f"Output dir: {curdir}")

    version = make_utils.parse_blender_version()
    tarball = tarball_path(curdir, version, cli_args)
    manifest = manifest_path(tarball)
    packages_dir = packages_path(curdir, cli_args)

    if cli_args.package_test_data:
        print("Creating an archive of all test data.")
        create_manifest(version, manifest, blender_srcdir / "tests/files", packages_dir)
    else:
        SKIP_PATHS.append(blender_srcdir / "tests/files")
        create_manifest(version, manifest, blender_srcdir, packages_dir)

    create_tarball(version, tarball, manifest, blender_srcdir, packages_dir)
    create_checksum_file(tarball)
    cleanup(manifest)
    print("Done!")


def tarball_path(output_dir: Path, version: make_utils.BlenderVersion, cli_args: Any) -> Path:
    extra = ""
    if cli_args.include_packages:
        extra = "-with-libraries"
    elif cli_args.package_test_data:
        extra = "-test-data"

    return output_dir / f"blender{extra}-{version}.tar.xz"


def manifest_path(tarball: Path) -> Path:
    """Return the manifest path for the given tarball path.

    >>> from pathlib import Path
    >>> tarball = Path("/home/user/workspace/blender-git/blender-test.tar.gz")
    >>> manifest_path(tarball).as_posix()
    '/home/user/workspace/blender-git/blender-test-manifest.txt'
    """
    # Note that `.tar.gz` is seen as two suffixes.
    without_suffix = tarball.with_suffix("").with_suffix("")
    name = without_suffix.name
    return without_suffix.with_name(f"{name}-manifest.txt")


def packages_path(current_directory: Path, cli_args: Any) -> Union[Path, None]:
    if not cli_args.include_packages:
        return None

    abspath = cli_args.include_packages.absolute()

    # `os.path.relpath()` can return paths like "../../packages", where
    # `Path.relative_to()` will not go up directories (so its return value never
    # has "../" in there).
    relpath = os.path.relpath(abspath, current_directory)

    return Path(relpath)

# -----------------------------------------------------------------------------
# Manifest creation


def create_manifest(
    version: make_utils.BlenderVersion,
    outpath: Path,
    blender_srcdir: Path,
    packages_dir: Union[Path, None],
    exclude: List[Path] = []
) -> None:
    print(f'Building manifest of files:  "{outpath}"...', end="", flush=True)
    with outpath.open("w", encoding="utf-8") as outfile:
        main_files_to_manifest(blender_srcdir, outfile)

        if packages_dir:
            packages_to_manifest(outfile, packages_dir)
    print("OK")


def main_files_to_manifest(blender_srcdir: Path, outfile: TextIO) -> None:
    assert not blender_srcdir.is_absolute()
    for git_repo in git_gather_all_folders_to_package(blender_srcdir):
        for path in git_ls_files(git_repo):
            print(path, file=outfile)


def packages_to_manifest(outfile: TextIO, packages_dir: Path) -> None:
    for path in packages_dir.glob("*"):
        if not path.is_file():
            continue
        if path.name in SKIP_NAMES:
            continue
        print(path, file=outfile)


# -----------------------------------------------------------------------------
# Higher-level functions


def create_tarball(
    version: make_utils.BlenderVersion,
    tarball: Path,
    manifest: Path,
    blender_srcdir: Path,
    packages_dir: Union[Path, None],
) -> None:
    print(f'Creating archive:            "{tarball}" ...', end="", flush=True)

    # Requires GNU `tar`, since `--transform` is used.
    if sys.platform == "darwin":
        # Provided by `brew install gnu-tar`.
        command = ["gtar"]
    else:
        command = ["tar"]

    if packages_dir:
        command += ["--transform", f"s,{packages_dir}/,packages/,g"]

    command += [
        "--transform",
        f"s,^{blender_srcdir.name}/,blender-{version}/,g",
        "--use-compress-program=xz -1",
        "--create",
        f"--file={tarball}",
        f"--files-from={manifest}",
        # Without owner/group args, extracting the files as root will
        # use ownership from the tar archive:
        "--owner=0",
        "--group=0",
    ]

    subprocess.run(command, check=True, timeout=3600)
    print("OK")


def create_checksum_file(tarball: Path) -> None:
    md5_path = tarball.with_name(tarball.name + ".md5sum")
    print(f'Creating checksum:           "{md5_path}" ...', end="", flush=True)
    command = [
        "md5sum",
        # The name is enough, as the tarball resides in the same dir as the MD5
        # file, and that's the current working directory.
        tarball.name,
    ]
    md5_cmd = subprocess.run(
        command, stdout=subprocess.PIPE, check=True, text=True, timeout=300
    )
    with md5_path.open("w", encoding="utf-8") as outfile:
        outfile.write(md5_cmd.stdout)
    print("OK")


def cleanup(manifest: Path) -> None:
    print("Cleaning up ...", end="", flush=True)
    if manifest.exists():
        manifest.unlink()
    print("OK")


# -----------------------------------------------------------------------------
# Low-level commands


def git_gather_all_folders_to_package(directory: Path = Path(".")) -> Iterable[Path]:
    """Generator, yields lines which represents each directory to gather git files from.

    Each directory represents either the top level git repository or a submodule.
    All submodules that have the 'update = none' setting will be excluded from this list.

    The directory path given to this function will be included in the yielded paths
    """

    # For each submodule (recurse into submodules within submodules if they exist)
    git_main_command = "submodule --quiet foreach --recursive"
    # Return the path to the submodule and what the value is of their "update" setting
    # If the "update" setting doesn't exist, only the path to the submodule is returned
    git_command_args = "'echo $displaypath $(git config --file \"$toplevel/.gitmodules\" --get submodule.$name.update)'"

    # Yield the root directory as this is our top level git repo
    yield directory

    for line in git_command(f"-C '{directory}' {git_main_command} {git_command_args}"):
        # Check if we shouldn't include the directory on this line
        split_line = line.rsplit(maxsplit=1)
        if len(split_line) > 1 and split_line[-1] == "none":
            continue
        path = directory / split_line[0]
        yield path


def is_path_ignored(file: Path) -> bool:
    for skip_folder in SKIP_PATHS:
        if file.is_relative_to(skip_folder):
            return True
    return False


def git_ls_files(directory: Path = Path(".")) -> Iterable[Path]:
    """Generator, yields lines of output from 'git ls-files'.

    Only lines that are actually files (so no directories, sockets, etc.) are
    returned, and never one from SKIP_NAMES.
    """
    for line in git_command(f"-C '{directory}' ls-files -z", "\x00"):
        path = directory / line
        if not path.is_file() or path.name in SKIP_NAMES:
            continue
        if not is_path_ignored(path):
            yield path


def git_command(cli_args: str, split_char: str = "\n") -> Iterable[str]:
    """Generator, yields lines of output from a Git command."""
    command = "git " + cli_args

    # import shlex
    # print(">", " ".join(shlex.quote(arg) for arg in command))

    git = subprocess.run(
        command, stdout=subprocess.PIPE, shell=True, check=True, text=True, timeout=30
    )
    for line in git.stdout.split(split_char):
        if line:
            yield line


if __name__ == "__main__":
    import doctest

    if doctest.testmod().failed:
        raise SystemExit("ERROR: Self-test failed, refusing to run")

    main()


--- build_files/utils/make_test.py ---
#!/usr/bin/env python3
# SPDX-FileCopyrightText: 2019-2023 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

"""
"make test" for all platforms, running automated tests.
"""

__all__ = (
    "main",
)

import argparse
import os
import sys

import make_utils
from make_utils import call


# Parse arguments.
def parse_arguments() -> argparse.Namespace:
    parser = argparse.ArgumentParser()
    parser.add_argument("--ctest-command", default="ctest")
    parser.add_argument("--git-command", default="git")
    parser.add_argument("--config", default="")
    parser.add_argument("build_directory")
    return parser.parse_args()


def main() -> int:
    args = parse_arguments()
    git_command = args.git_command
    ctest_command = args.ctest_command
    config = args.config
    build_dir = args.build_directory

    if make_utils.command_missing(ctest_command):
        sys.stderr.write("ctest not found, can't run tests\n")
        return 1

    if make_utils.command_missing(git_command):
        sys.stderr.write("git not found, can't run tests\n")
        return 1

    # Run tests
    tests_dir = os.path.join(build_dir, "tests")
    os.makedirs(tests_dir, exist_ok=True)

    os.chdir(build_dir)
    command = [ctest_command, ".", "--output-on-failure"]
    if len(config):
        command += ["-C", config]
        tests_log = "log_" + config + ".txt"
    else:
        tests_log = "log.txt"
    command += ["-O", os.path.join(tests_dir, tests_log)]
    call(command)
    return 0


if __name__ == "__main__":
    sys.exit(main())


--- build_files/utils/make_update.py ---
#!/usr/bin/env python3
# SPDX-FileCopyrightText: 2019-2023 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

"""
"make update" for all platforms, updating Git LFS submodules for libraries and
Blender git repository.

For release branches, this will check out the appropriate branches of
submodules and libraries.

WARNING:
- Python 3.6 is used on the Linux VM (Rocky8) to run "make update" to checkout LFS.
- Python 3.9 is used on the built-bot.

Take care *not* to use features from the Python version used by Blender!

NOTE:
Some type annotations are quoted to avoid errors in older Python versions.
These can be unquoted eventually.
"""

__all__ = (
    "main",
)


import argparse
import os
import platform
import shutil
import sys

import make_utils
from pathlib import Path
from make_utils import call, check_output
from urllib.parse import urljoin, urlsplit


def print_stage(text: str) -> None:
    print("")
    print(text)
    print("=" * len(text))
    print("")


def parse_arguments() -> argparse.Namespace:
    """
    Parse command line arguments.

    Returns parsed object from which the command line arguments can be accessed
    as properties. The name of the properties matches the command line argument,
    but with the leading dashed omitted and all remaining dashes replaced with
    underscore.
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("--no-libraries", action="store_true",
                        help="Don't fetch precompiled libraries for this system")
    parser.add_argument("--no-blender", action="store_true", help="Don't update the Blender code repository")
    parser.add_argument(
        "--no-lfs-fallback",
        action="store_true",
        help="Don't set up fallback URLs for fetching LFS files from projects.blender.org. These are only used when cloning repositories hosted elsewhere.")
    parser.add_argument(
        "--git-command",
        default="git",
        help="Path to the git binary. (Only useful if it is not in your PATH)")
    parser.add_argument("--architecture", type=str,
                        choices=("x86_64", "amd64", "arm64",))
    parser.add_argument("--prune-destructive", action="store_true",
                        help="Destructive! Detect and remove stale files from older checkouts")

    # Deprecated options, kept for compatibility with old configurations.
    parser.add_argument("--use-tests", action="store_true", help=argparse.SUPPRESS)
    parser.add_argument("--no-submodules", action="store_true", help=argparse.SUPPRESS)
    parser.add_argument("--use-linux-libraries", action="store_true", help=argparse.SUPPRESS)

    return parser.parse_args()


def get_blender_git_root(args: argparse.Namespace) -> Path:
    """
    Get root directory of the current Git directory.
    """
    return Path(
        check_output([args.git_command, "rev-parse", "--show-toplevel"]))


def get_effective_platform(args: argparse.Namespace) -> str:
    """
    Get platform of the host.

    The result string is normalized to the name used by Blender releases and
    library repository name prefixes: linux, macos, windows.
    """

    if sys.platform == "darwin":
        platform = "macos"
    elif sys.platform == "win32":
        platform = "windows"
    else:
        platform = sys.platform

    assert (platform in ("linux", "macos", "windows"))

    return platform


def get_effective_architecture(args: argparse.Namespace) -> str:
    """
    Get architecture of the host.

    The result string is normalized to the architecture name used by the Blender
    releases and library repository name suffixes: x64, arm64.

    NOTE: When cross-compiling the architecture is coming from the command line
    argument.
    """
    architecture: "str | None" = args.architecture
    if architecture:
        assert isinstance(architecture, str)
    elif "ARM64" in platform.version():
        # Check platform.version to detect arm64 with x86_64 python binary.
        architecture = "arm64"
    else:
        architecture = platform.machine().lower()

    # Normalize the architecture name.
    if architecture in {"x86_64", "amd64"}:
        architecture = "x64"
    if architecture == "aarch64":
        architecture = "arm64"

    assert isinstance(architecture, str)
    return architecture


# NOTE: unquote "tuple" once Python 3.6x is dropped.
def get_submodule_directories(args: argparse.Namespace) -> "tuple[Path, ...]":
    """
    Get list of all configured submodule directories.
    """

    blender_git_root = get_blender_git_root(args)
    dot_modules = blender_git_root / ".gitmodules"

    if not dot_modules.exists():
        return ()

    submodule_directories_output = check_output(
        [args.git_command, "config", "--file", str(dot_modules), "--get-regexp", "path"])
    return tuple([Path(line.split(' ', 1)[1]) for line in submodule_directories_output.strip().splitlines()])


def ensure_git_lfs(args: argparse.Namespace) -> None:
    # Use `--skip-repo` to avoid creating git hooks.
    # This is called from the `blender.git` checkout, so we don't need to install hooks there.
    call((args.git_command, "lfs", "install", "--skip-repo"), exit_on_error=True)


def switch_blender_git_remotes(args: argparse.Namespace) -> None:
    """
    Switch remote URLs from projects.blender.org to git.blender.org
    """
    remotes = make_utils.git_get_remotes(args.git_command)

    for remote in remotes:
        url = make_utils.git_get_remote_url(args.git_command, remote)
        new_url = url.replace("git@projects.blender.org", "git@git.blender.org")

        if new_url == url:
            continue

        print(f"Replacing {remote} URL from {url} to {new_url}")
        make_utils.git_set_config(args.git_command, f"remote.{remote}.url", new_url)


def prune_stale_files(args: argparse.Namespace) -> None:
    """
    Ensure files from previous Git configurations do not exist anymore
    """
    print_stage("Removing stale files")

    blender_git_root = get_blender_git_root(args)
    found_stale_files = False

    for relative_dir_to_remove in (
        Path("scripts") / "addons",
        Path("scripts") / "addons_contrib",
    ):
        dir_to_remove = blender_git_root / relative_dir_to_remove
        if not dir_to_remove.exists():
            continue
        if not dir_to_remove.is_dir():
            print(f"'{relative_dir_to_remove}' exists but is not a directory")
            continue
        print(f"Removing '{relative_dir_to_remove}'")
        make_utils.remove_directory(dir_to_remove)
        found_stale_files = True

    if not found_stale_files:
        print("Checkout looks pristine")


def initialize_precompiled_libraries(args: argparse.Namespace) -> str:
    """
    Configure submodule for precompiled libraries

    This function detects the current host architecture and enables
    corresponding submodule, and updates the submodule.

    NOTE: When cross-compiling the architecture is coming from the command line
    argument.
    """

    print_stage("Configuring Precompiled Libraries")

    platform = get_effective_platform(args)
    arch = get_effective_architecture(args)

    print(f"Detected platform     : {platform}")
    print(f"Detected architecture : {arch}")
    print()

    submodule_dir = f"lib/{platform}_{arch}"

    submodule_directories = get_submodule_directories(args)

    if platform == "macos" and arch == "x64":
        return ("WARNING: macOS x64/Intel support was dropped in Blender 5.0.\n"
                "         As such, pre-compiled dependencies are no longer provided.\n"
                "         You may build the dependencies yourself, or downgrade to Blender 4.5.\n"
                "         For more details, please see: https://devtalk.blender.org/t/38835")

    if Path(submodule_dir) not in submodule_directories:
        return "Skipping libraries update: no configured submodule\n"

    print(f"* Enabling precompiled libraries at {submodule_dir}")
    make_utils.git_enable_submodule(args.git_command, Path(submodule_dir))

    return ""


def git_update_skip(args: argparse.Namespace, check_remote_exists: bool = True) -> str:
    """Test if git repo can be updated."""

    if make_utils.command_missing(args.git_command):
        sys.stderr.write("git not found, can't update code\n")
        sys.exit(1)

    # Abort if a rebase is still progress.
    rebase_merge = check_output([args.git_command, 'rev-parse', '--git-path', 'rebase-merge'], exit_on_error=False)
    rebase_apply = check_output([args.git_command, 'rev-parse', '--git-path', 'rebase-apply'], exit_on_error=False)
    merge_head = check_output([args.git_command, 'rev-parse', '--git-path', 'MERGE_HEAD'], exit_on_error=False)
    if (
            os.path.exists(rebase_merge) or
            os.path.exists(rebase_apply) or
            os.path.exists(merge_head)
    ):
        return "rebase or merge in progress, complete it first"

    # Abort if uncommitted changes.
    changes = check_output([args.git_command, 'status', '--porcelain', '--untracked-files=no', '--ignore-submodules'])
    if len(changes) != 0:
        return "you have unstaged changes"

    # Test if there is an upstream branch configured
    if check_remote_exists:
        branch = check_output([args.git_command, "rev-parse", "--abbrev-ref", "HEAD"])
        remote = check_output([args.git_command, "config", "branch." + branch + ".remote"], exit_on_error=False)
        if len(remote) == 0:
            return "no remote branch to pull from"

    return ""


def use_upstream_workflow(args: argparse.Namespace) -> bool:
    return make_utils.git_remote_exist(args.git_command, "upstream")


def work_tree_update_upstream_workflow(args: argparse.Namespace, use_fetch: bool = True) -> str:
    """
    Update the Blender repository using the GitHub style of fork organization

    Returns true if the current local branch has been updated to the upstream state.
    Otherwise false is returned.
    """

    branch_name = make_utils.git_branch(args.git_command)

    if use_fetch:
        call((args.git_command, "fetch", "upstream"))

    upstream_branch = f"upstream/{branch_name}"
    if not make_utils.git_branch_exists(args.git_command, upstream_branch):
        return "no_branch"

    retcode = call((args.git_command, "merge", "--ff-only", upstream_branch), exit_on_error=False)
    if retcode != 0:
        return "Unable to fast forward\n"

    return ""


def work_tree_update(args: argparse.Namespace, use_fetch: bool = True) -> str:
    """
    Update the Git working tree using the best strategy

    This function detects whether it is a github style of fork remote organization is used, or
    is it a repository which origin is an upstream.
    """

    if use_upstream_workflow(args):
        message = work_tree_update_upstream_workflow(args, use_fetch)
        if message != "no_branch":
            return message

        # If there is upstream configured but the local branch is not in the upstream, try to
        # update the branch from the fork.

    update_command = [args.git_command, "pull", "--rebase"]

    call(update_command)

    return ""


# Update blender repository.
def blender_update(args: argparse.Namespace) -> str:
    print_stage("Updating Blender Git Repository")

    return work_tree_update(args)


# Extra LFS update for blender repository
def blender_lfs_update(args: argparse.Namespace) -> None:
    print_stage("Updating Blender Git LFS")

    # This seems to be required some times, e.g. on initial checkout from third party, non-lfs repository
    # (like the github one). The fallback repository set by `lfs_fallback_setup` is fetched, but running the
    # `update_command` above does not seem to do the actual checkout for these LFS-managed files.
    update_lfs_command = [args.git_command, "lfs", "pull"]
    call(update_lfs_command)


def resolve_external_url(blender_url: str, repo_name: str) -> str:
    return urljoin(blender_url + "/", "../" + repo_name)


def external_script_copy_old_submodule_over(
        args: argparse.Namespace,
        directory: Path,
        old_submodules_dir: Path,
) -> None:
    blender_git_root = get_blender_git_root(args)
    external_dir = blender_git_root / directory

    print(f"Moving {old_submodules_dir} to {directory} ...")
    shutil.move(blender_git_root / old_submodules_dir, external_dir)

    # Remove old ".git" which is a file with path to a submodule bare repo inside of main
    # repo .git/modules directory.
    (external_dir / ".git").unlink()

    bare_repo_relative_dir = Path(".git") / "modules" / old_submodules_dir
    print(f"Copying {bare_repo_relative_dir} to {directory}/.git ...")
    bare_repo_dir = blender_git_root / bare_repo_relative_dir
    shutil.copytree(bare_repo_dir, external_dir / ".git")

    git_config = external_dir / ".git" / "config"
    call((args.git_command, "config", "--file", str(git_config), "--unset", "core.worktree"))


def floating_checkout_initialize_if_needed(
        args: argparse.Namespace,
        repo_name: str,
        directory: Path,
        old_submodules_dir: "Path | None" = None,
) -> None:
    """Initialize checkout of an external repository"""

    blender_git_root = get_blender_git_root(args)
    blender_dot_git = blender_git_root / ".git"
    external_dir = blender_git_root / directory

    if external_dir.exists():
        return

    print(f"Initializing {directory} ...")

    if old_submodules_dir is not None:
        old_submodule_dot_git = blender_git_root / old_submodules_dir / ".git"
        if old_submodule_dot_git.exists() and blender_dot_git.is_dir():
            external_script_copy_old_submodule_over(args, directory, old_submodules_dir)
            return

    origin_name = "upstream" if use_upstream_workflow(args) else "origin"
    blender_url = make_utils.git_get_remote_url(args.git_command, origin_name)
    external_url = resolve_external_url(blender_url, repo_name)

    # When running `make update` from a freshly cloned fork check whether the fork of the submodule is
    # available, If not, switch to the submodule relative to the main blender repository.
    if origin_name == "origin" and not make_utils.git_is_remote_repository(args.git_command, external_url):
        external_url = resolve_external_url("https://projects.blender.org/blender/blender", repo_name)

    call((args.git_command, "clone", "--origin", origin_name, external_url, str(external_dir)))


def floating_checkout_add_origin_if_needed(
        args: argparse.Namespace,
        repo_name: str,
        directory: Path,
) -> None:
    """
    Add remote called 'origin' if there is a fork of the external repository available

    This is only done when using Github style upstream workflow in the main repository.
    """

    if not use_upstream_workflow(args):
        return

    cwd = os.getcwd()

    blender_git_root = get_blender_git_root(args)
    external_dir = blender_git_root / directory

    origin_blender_url = make_utils.git_get_remote_url(args.git_command, "origin")
    origin_external_url = resolve_external_url(origin_blender_url, repo_name)

    try:
        os.chdir(external_dir)

        if (make_utils.git_remote_exist(args.git_command, "origin") or
                not make_utils.git_remote_exist(args.git_command, "upstream")):
            return

        if not make_utils.git_is_remote_repository(args.git_command, origin_external_url):
            return

        print(f"Adding origin remote to {directory} pointing to fork ...")

        # Non-obvious tricks to introduce the new remote called "origin" to the existing
        # submodule configuration.
        #
        # This is all within the content of creating a fork of a submodule after `make update`
        # has been run and possibly local branches tracking upstream were added.
        #
        # The idea here goes as following:
        #
        #  - Rename remote "upstream" to "origin", which takes care of changing the names of
        #    remotes the local branches are tracking.
        #
        #  - Change the URL to the "origin", which was still pointing to upstream.
        #
        #  - Re-introduce the "upstream" remote, with the same URL as it had prior to rename.

        upstream_url = make_utils.git_get_remote_url(args.git_command, "upstream")

        call((args.git_command, "remote", "rename", "upstream", "origin"))
        make_utils.git_set_config(args.git_command, "remote.origin.url", origin_external_url)

        call((args.git_command, "remote", "add", "upstream", upstream_url))
    finally:
        os.chdir(cwd)

    return


def floating_checkout_update(
        args: argparse.Namespace,
        repo_name: str,
        directory: Path,
        branch: "str | None",
        old_submodules_dir: "Path | None" = None,
        only_update: bool = False,
) -> str:
    """Update a single external checkout with the given name in the scripts folder"""

    blender_git_root = get_blender_git_root(args)
    external_dir = blender_git_root / directory

    if only_update and not external_dir.exists():
        return ""

    floating_checkout_initialize_if_needed(args, repo_name, directory, old_submodules_dir)
    floating_checkout_add_origin_if_needed(args, repo_name, directory)

    blender_git_root = get_blender_git_root(args)
    external_dir = blender_git_root / directory

    print(f"* Updating {directory} ...")

    cwd = os.getcwd()

    # Update externals to appropriate given branch, falling back to main if none is given and/or
    # found in a sub-repository.
    branch_fallback = "main"
    if not branch:
        branch = branch_fallback

    skip_msg = ""

    try:
        os.chdir(external_dir)
        msg = git_update_skip(args, check_remote_exists=False)
        if msg:
            skip_msg += str(directory) + " skipped: " + msg + "\n"
        else:
            # Find a matching branch that exists.
            for remote in ("origin", "upstream"):
                if make_utils.git_remote_exist(args.git_command, remote):
                    call([args.git_command, "fetch", remote])

            submodule_branch = branch

            if make_utils.git_branch_exists(args.git_command, submodule_branch):
                pass
            elif make_utils.git_branch_exists(args.git_command, branch_fallback):
                submodule_branch = branch_fallback
            else:
                # Skip.
                submodule_branch = ""

            # Switch to branch and pull.
            if submodule_branch:
                if make_utils.git_branch(args.git_command) != submodule_branch:
                    # If the local branch exists just check out to it.
                    # If there is no local branch but only remote specify an explicit remote.
                    # Without this explicit specification Git attempts to set-up tracking
                    # automatically and fails when the branch is available in multiple remotes.
                    if make_utils.git_local_branch_exists(args.git_command, submodule_branch):
                        call([args.git_command, "checkout", submodule_branch])
                    else:
                        if make_utils.git_remote_branch_exists(args.git_command, "origin", submodule_branch):
                            call([args.git_command, "checkout", "-t", f"origin/{submodule_branch}"])
                        elif make_utils.git_remote_exist(args.git_command, "upstream"):
                            # For the Github style of upstream workflow create a local branch from
                            # the upstream, but do not track it, so that we stick to the paradigm
                            # that no local branches are tracking upstream, preventing possible
                            # accidental commit to upstream.
                            call([args.git_command, "checkout", "-b", submodule_branch,
                                 f"upstream/{submodule_branch}", "--no-track"])

                # Don't use extra fetch since all remotes of interest have been already fetched
                # some lines above.
                skip_msg += work_tree_update(args, use_fetch=False)
    finally:
        os.chdir(cwd)

    return skip_msg


def floating_libraries_update(args: argparse.Namespace, branch: "str | None") -> str:
    """Update libraries checkouts which are floating (not attached as Git submodules)"""
    msg = ""

    msg += floating_checkout_update(
        args,
        "benchmarks",
        Path("tests") / "benchmarks",
        branch,
        only_update=True,
    )

    return msg


def add_submodule_push_url(args: argparse.Namespace) -> None:
    """
    Add pushURL configuration for all locally activated submodules, pointing to SSH protocol.
    """

    blender_git_root = get_blender_git_root(args)
    modules = blender_git_root / ".git" / "modules"

    submodule_directories = get_submodule_directories(args)

    for submodule_path in submodule_directories:
        module_path = modules / submodule_path
        config = module_path / "config"

        if not config.exists():
            # Ignore modules which are not initialized
            continue

        push_url = check_output((args.git_command, "config", "--file", str(config),
                                "--get", "remote.origin.pushURL"), exit_on_error=False)

        # Don't modify PushURL if it is set.
        if push_url:
            if "projects.blender.org" in push_url:
                # Allow the code below to replace projects.blender.org with git.blender.org
                pass
            else:
                continue

        url = make_utils.git_get_config(args.git_command, "remote.origin.url", str(config))
        if not url.startswith("https:"):
            # Ignore non-URL URLs.
            continue

        url_parts = urlsplit(url)

        host = url_parts.hostname
        if host == "projects.blender.org":
            host = "git.blender.org"

        push_url = f"git@{host}:{url_parts.path[1:]}"

        print(f"Setting pushURL to {push_url} for {submodule_path}")
        make_utils.git_set_config(args.git_command, "remote.origin.pushURL", push_url, str(config))


def submodules_lib_update(args: argparse.Namespace, branch: "str | None") -> str:
    print_stage("Updating Libraries")

    msg = ""
    msg += floating_libraries_update(args, branch)

    submodule_directories = get_submodule_directories(args)
    for submodule_path in submodule_directories:
        if not make_utils.is_git_submodule_enabled(args.git_command, submodule_path):
            print(f"* Skipping {submodule_path}")
            continue

        print(f"* Updating {submodule_path} ...")

        if not make_utils.git_update_submodule(args.git_command, submodule_path):
            msg += f"Error updating Git submodule {submodule_path}\n"

    add_submodule_push_url(args)

    return msg


def lfs_fallback_setup(args: argparse.Namespace) -> None:
    """
    Set up an additional projects.blender.org remote, for LFS fetching fallback
    in case the fork does not include LFS files.
    """
    remotes = make_utils.git_get_remotes(args.git_command)
    add_fallback_remote = True
    fallback_remote = "lfs-fallback"

    for remote in remotes:
        url = make_utils.git_get_remote_url(args.git_command, remote)
        if "projects.blender.org" not in url and "git.blender.org" not in url:
            make_utils.git_set_config(args.git_command, "lfs.remote.searchall", "true")
        else:
            add_fallback_remote = False

    if add_fallback_remote and not make_utils.git_remote_exist(args.git_command, fallback_remote):
        print_stage("Adding Git LFS fallback remote")
        print("Used to fetch files from projects.blender.org if missing.")

        url = "https://projects.blender.org/blender/blender.git"
        push_url = "no_push"
        make_utils.git_add_remote(args.git_command, fallback_remote, url, push_url)

        # Fetch potentially missing files.
        call((args.git_command, "lfs", "fetch", fallback_remote))


def main() -> int:
    args = parse_arguments()

    blender_skip_msg = ""
    libraries_skip_msg = ""
    submodules_skip_msg = ""

    blender_version = make_utils. parse_blender_version()
    if blender_version.cycle != 'alpha':
        major = blender_version.version // 100
        minor = blender_version.version % 100
        branch = f"blender-v{major}.{minor}-release"
    else:
        branch = 'main'

    # Submodules and precompiled libraries require Git LFS.
    ensure_git_lfs(args)

    switch_blender_git_remotes(args)

    if args.prune_destructive:
        prune_stale_files(args)

    if not args.no_lfs_fallback:
        lfs_fallback_setup(args)

    if not args.no_blender:
        blender_skip_msg = git_update_skip(args)
        if not blender_skip_msg:
            blender_skip_msg = blender_update(args)
        if blender_skip_msg:
            blender_skip_msg = "Blender repository skipped: " + blender_skip_msg + "\n"
        blender_lfs_update(args)

    if not args.no_libraries:
        libraries_skip_msg += initialize_precompiled_libraries(args)
        libraries_skip_msg += submodules_lib_update(args, branch)

    # Report any skipped repositories at the end, so it's not as easy to miss.
    skip_msg = blender_skip_msg + libraries_skip_msg + submodules_skip_msg
    if skip_msg:
        print_stage("Update finished with the following messages")
        print(skip_msg.strip())

    if args.use_tests:
        print()
        print('NOTE: --use-tests is a deprecated command line argument, kept for compatibility purposes.')

    if args.no_submodules:
        print()
        print('NOTE: --no-submodules is a deprecated command line argument, kept for compatibility purposes.')

    if args.use_linux_libraries:
        print()
        print('NOTE: --use-linux-libraries is a deprecated command line argument, kept for compatibility purposes.')

    # For failed submodule update we throw an error, since not having correct
    # submodules can make Blender throw errors.
    # For Blender itself we don't and consider "make update" to be a command
    # you can use while working on uncommitted code.
    if submodules_skip_msg:
        return 1

    return 0


if __name__ == "__main__":
    sys.exit(main())


--- doc/manpage/blender.1.py ---
#!/usr/bin/env python3
# SPDX-FileCopyrightText: 2010-2023 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

"""
This script generates the ``blender.1`` man page, embedding the help text
from the Blender executable itself. Invoke it as follows:

    blender.1.py --blender <path-to-blender> --output <output-filename>

where <path-to-blender> is the path to the Blender executable,
and <output-filename> is where to write the generated man page.
"""

import argparse
import os
import subprocess
import time

from typing import (
    TextIO,
    Dict,
)


def man_format(data: str) -> str:
    data = data.replace("-", "\\-")
    data = data.replace("\t", "  ")
    # Single quotes prevent text rendering when found at the beginning of lines.
    data = data.replace("'", "\\(aq")
    return data


def blender_extract_info(blender_bin: str) -> Dict[str, str]:
    blender_env = {
        "ASAN_OPTIONS": (
            os.environ.get("ASAN_OPTIONS", "") +
            ":exitcode=0:check_initialization_order=0:strict_init_order=0"
        ).lstrip(":"),
    }

    # NOTE: in some ways it's more elegant to use `bpy.app.help_text()` which was done but had to be reverted.
    # however - this requires Blender to run with a full environment (initializing it's Python environment).
    # See #115056 & !115320 for details.

    blender_help = subprocess.run(
        [blender_bin, "--help"],
        env=blender_env,
        check=True,
        stdout=subprocess.PIPE,
    ).stdout.decode(encoding="utf-8")

    blender_version_output = subprocess.run(
        [blender_bin, "--version"],
        env=blender_env,
        check=True,
        stdout=subprocess.PIPE,
    ).stdout.decode(encoding="utf-8")

    # Extract information from the version string.
    # Note that some internal modules may print errors (e.g. color management),
    # check for each lines prefix to ensure these aren't included.
    blender_version = ""
    blender_date = ""

    # The full text (use to manipulate `blender_version_text`).
    blender_version_text = ""

    for l in blender_version_output.split("\n"):
        if l.startswith("Blender "):
            if blender_version_text == "":
                blender_version_text = l
            # Remove `Blender` prefix.
            blender_version = l.split(" ", 1)[1].strip()
        elif l.lstrip().startswith("build date:"):
            # Remove `build date:` prefix.
            blender_date = l.split(":", 1)[1].strip()
        if blender_version and blender_date:
            break

    # The `--help` text also contains the version, skip it so as not to include it twice.
    if blender_version_text:
        i = blender_help.find(blender_version_text)
        if i != -1:
            blender_help = blender_help[i + len(blender_version_text) + 1:]
        del i

    if not blender_date:
        # Happens when built without WITH_BUILD_INFO e.g.
        date_string = time.strftime("%B %d, %Y", time.gmtime(int(os.environ.get('SOURCE_DATE_EPOCH', time.time()))))
    else:
        date_string = time.strftime("%B %d, %Y", time.strptime(blender_date, "%Y-%m-%d"))

    return {
        "help": blender_help,
        "version": blender_version,
        "date": date_string,
    }


def man_page_from_blender_help(fh: TextIO, blender_bin: str, verbose: bool) -> None:
    if verbose:
        print("Extracting help text:", blender_bin)
    blender_info = blender_extract_info(blender_bin)

    # Header Content.
    fh.write(
        '.TH "BLENDER" "1" "{:s}" "Blender {:s}"\n'.format(
            blender_info["date"], blender_info["version"].replace(".", "\\&.")
        )
    )

    fh.write(r"""
.SH NAME
blender \- a full-featured 3D application""")

    fh.write(r"""
.SH SYNOPSIS
.B blender [args ...] [file] [args ...]""")

    fh.write(r"""
.br
.SH DESCRIPTION
.PP
.B blender
is a full-featured 3D application. It supports the entirety of the 3D pipeline - """
             """modeling, rigging, animation, simulation, rendering, compositing, motion tracking, and video editing.

Use Blender to create 3D images and animations, films and commercials, content for games, """
             r"""architectural and industrial visualizations, and scientific visualizations.

https://www.blender.org""")

    fh.write(r"""
.SH OPTIONS""")

    fh.write("\n\n")

    # Body Content.

    lines = [line.rstrip() for line in blender_info["help"].split("\n")]

    while lines:
        l = lines.pop(0)
        if l.startswith("Environment Variables:"):
            fh.write('.SH "ENVIRONMENT VARIABLES"\n')
        elif l.endswith(":"):  # One line.
            fh.write('.SS "{:s}"\n\n'.format(l))
        elif l.startswith("-") or l.startswith("/"):  # Can be multi line.
            fh.write('.TP\n')
            fh.write('.B {:s}\n'.format(man_format(l)))

            while lines:
                # line with no
                if lines[0].strip() and len(lines[0].lstrip()) == len(lines[0]):  # No white space.
                    break

                if not l:  # Second blank line.
                    fh.write('.IP\n')
                else:
                    fh.write('.br\n')

                l = lines.pop(0)
                if l:
                    assert l.startswith('\t')
                    l = l[1:]  # Remove first white-space (tab).

                fh.write('{:s}\n'.format(man_format(l)))

        else:
            if not l.strip():
                fh.write('.br\n')
            else:
                fh.write('{:s}\n'.format(man_format(l)))

    # Footer Content.

    fh.write(r"""
.br
.SH SEE ALSO
.B luxrender(1)

.br
.SH AUTHORS
This manpage was written for a Debian GNU/Linux system by Daniel Mester
<mester@uni-bremen.de> and updated by Cyril Brulebois
<cyril.brulebois@enst-bretagne.fr> and Dan Eicher <dan@trollwerks.org>.
""")


def create_argparse() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--output",
        required=True,
        help="The man page to write to."
    )
    parser.add_argument(
        "--blender",
        required=True,
        help="Path to the Blender binary."
    )
    parser.add_argument(
        "--verbose",
        default=False,
        required=False,
        action='store_true',
        help="Print additional progress."
    )

    return parser


def main() -> None:
    parser = create_argparse()
    args = parser.parse_args()

    output_filename = args.output
    blender_bin = args.blender
    verbose = args.verbose

    with open(output_filename, "w", encoding="utf-8") as fh:
        man_page_from_blender_help(fh, blender_bin, verbose)
        if verbose:
            print("Written:", output_filename)


if __name__ == "__main__":
    main()


--- doc/blender_file_format/BlendFileDnaExporter_25.py ---
#!/usr/bin/env python3
# SPDX-FileCopyrightText: 2010-2022 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

######################################################
#
#    Name:
#        dna.py
#
#    Description:
#        Creates a browsable DNA output to HTML.
#
#    Author:
#        Jeroen Bakker
#
#    Version:
#        v0.1 (12-05-2009) - migration of original source code to Python.
#           Added code to support blender 2.5 branch
#        v0.2 (25-05-2009) - integrated with BlendFileReader.py
#
#    Input:
#        blender build executable
#
#    Output:
#        dna.html
#        dna.css (will only be created when not existing)
#
#    Startup:
#        ./blender -P BlendFileDnaExporter.py
#
#    Process:
#        1: write blend file with SDNA info
#        2: read blend header from blend file
#        3: seek DNA1 file-block
#        4: read dna record from blend file
#        5: close and eventually delete temp blend file
#        6: export dna to html and css
#        7: quit blender
#
######################################################

import sys
from string import Template     # strings completion


# logs
import logging
log = logging.getLogger("BlendFileDnaExporter")

if '--dna-debug' in sys.argv:
    logging.basicConfig(level=logging.DEBUG)
else:
    logging.basicConfig(level=logging.INFO)


class DNACatalogHTML:
    '''
    DNACatalog is a catalog of all information in the DNA1 file-block
    '''

    def __init__(self, catalog, bpy_module=None):
        self.Catalog = catalog
        self.bpy = bpy_module

    def WriteToHTML(self, handle):

        dna_html_template = """
            <!DOCTYPE html PUBLIC -//W3C//DTD HTML 4.01 Transitional//EN http://www.w3.org/TR/html4/loose.dtd>
            <html>
            <head>
                <link rel="stylesheet" type="text/css" href="dna.css" media="screen, print" />
                <meta http-equiv="Content-Type" content="text/html"; charset="ISO-8859-1" />
                <title>The mystery of the blend</title>
            </head>
            <body>
                <div class=title>
                    Blender ${version}<br/>
                    Internal SDNA structures
                </div>
                Architecture: ${bitness} ${endianness}<br/>
                Build revision: <a href="https://svn.blender.org/svnroot/bf-blender/!svn/bc/${revision}/trunk/">${revision}</a><br/>
                File format reference: <a href="mystery_of_the_blend.html">The mystery of the blend</a> by Jeroen Bakker<br/>
                <h1>Index of blender structures</h1>
                <ul class=multicolumn>
                    ${structs_list}
                </ul>
                ${structs_content}
            </body>
            </html>"""

        header = self.Catalog.Header
        bpy = self.bpy

        # ${version} and ${revision}
        if bpy:
            version = '.'.join(map(str, bpy.app.version))
            revision = bpy.app.build_hash
        else:
            version = str(header.Version)
            revision = 'Unknown'

        # ${bitness}
        if header.PointerSize == 8:
            bitness = '64 bit'
        else:
            bitness = '32 bit'

        # ${endianness}
        if header.LittleEndianness:
            endianess = 'Little endianness'
        else:
            endianess = 'Big endianness'

        # ${structs_list}
        log.debug("Creating structs index")
        structs_list = ''
        list_item = '<li class="multicolumn">({0}) <a href="#{1}">{1}</a></li>\n'
        structureIndex = 0
        for structure in self.Catalog.Structs:
            structs_list += list_item.format(structureIndex, structure.Type.Name)
            structureIndex += 1

        # ${structs_content}
        log.debug("Creating structs content")
        structs_content = ''
        for structure in self.Catalog.Structs:
            log.debug(structure.Type.Name)
            structs_content += self.Structure(structure)

        d = dict(
            version=version,
            revision=revision,
            bitness=bitness,
            endianness=endianess,
            structs_list=structs_list,
            structs_content=structs_content
        )

        dna_html = Template(dna_html_template).substitute(d)
        dna_html = self.format(dna_html)
        handle.write(dna_html)

    def Structure(self, structure):
        struct_table_template = """
            <table><a name="${struct_name}"></a>
                <caption><a href="#${struct_name}">${struct_name}</a></caption>
                <thead>
                    <tr>
                        <th>reference</th>
                        <th>structure</th>
                        <th>type</th>
                        <th>name</th>
                        <th>offset</th>
                        <th>size</th>
                    </tr>
                </thead>
                <tbody>
                ${fields}
                </tbody>
            </table>
            <label>Total size: ${size} bytes</label><br/>
            <label>(<a href="#top">top</a>)</label><br/>"""

        d = dict(
            struct_name=structure.Type.Name,
            fields=self.StructureFields(structure, None, 0),
            size=str(structure.Type.Size)
        )

        struct_table = Template(struct_table_template).substitute(d)
        return struct_table

    def StructureFields(self, structure, parentReference, offset):
        fields = ''
        for field in structure.Fields:
            fields += self.StructureField(field, structure, parentReference, offset)
            offset += field.Size(self.Catalog.Header)
        return fields

    def StructureField(self, field, structure, parentReference, offset):
        structure_field_template = """
            <tr>
                <td>${reference}</td>
                <td>${struct}</td>
                <td>${type}</td>
                <td>${name}</td>
                <td>${offset}</td>
                <td>${size}</td>
            </tr>"""

        if field.Type.Structure is None or field.Name.IsPointer():

            # ${reference}
            reference = field.Name.AsReference(parentReference)

            # ${struct}
            if parentReference is not None:
                struct = '<a href="#{0}">{0}</a>'.format(structure.Type.Name)
            else:
                struct = structure.Type.Name

            # ${type}
            type = field.Type.Name

            # ${name}
            name = field.Name.Name

            # ${offset}
            # offset already set

            # ${size}
            size = field.Size(self.Catalog.Header)

            d = dict(
                reference=reference,
                struct=struct,
                type=type,
                name=name,
                offset=offset,
                size=size
            )

            structure_field = Template(structure_field_template).substitute(d)

        elif field.Type.Structure is not None:
            reference = field.Name.AsReference(parentReference)
            structure_field = self.StructureFields(field.Type.Structure, reference, offset)

        return structure_field

    def indent(self, input, dent):
        output = ''
        if dent < 0:
            for line in input.split('\n'):
                dent = abs(dent)
                output += line[dent:] + '\n'   # unindent of a desired amount
        elif dent == 0:
            for line in input.split('\n'):
                output += line.lstrip() + '\n'  # remove indentation completely
        elif dent > 0:
            for line in input.split('\n'):
                output += ' ' * dent + line + '\n'
        return output

    def format(self, input):
        diff = {
            '\n<!DOCTYPE': '<!DOCTYPE',
            '\n</ul>': '</ul>',
            '<a name': '\n<a name',
            '<tr>\n': '<tr>',
            '<tr>': '  <tr>',
            '</th>\n': '</th>',
            '</td>\n': '</td>',
            '<tbody>\n': '<tbody>'
        }
        output = self.indent(input, 0)
        for key, value in diff.items():
            output = output.replace(key, value)
        return output

    def WriteToCSS(self, handle):
        '''
        Write the Cascading style-sheet template to the handle
        It is expected that the handle is a File-handle.
        '''
        css = """
            @CHARSET "ISO-8859-1";

            body {
                font-family: verdana;
                font-size: small;
            }

            div.title {
                font-size: large;
                text-align: center;
            }

            h1 {
                page-break-before: always;
            }

            h1, h2 {
                background-color: #D3D3D3;
                color:#404040;
                margin-right: 3%;
                padding-left: 40px;
            }

            h1:hover{
                background-color: #EBEBEB;
            }

            h3 {
                padding-left: 40px;
            }

            table {
                border-width: 1px;
                border-style: solid;
                border-color: #000000;
                border-collapse: collapse;
                width: 94%;
                margin: 20px 3% 10px;
            }

            caption {
                margin-bottom: 5px;
            }

            th {
                background-color: #000000;
                color:#ffffff;
                padding-left:5px;
                padding-right:5px;
            }

            tr {
            }

            td {
                border-width: 1px;
                border-style: solid;
                border-color: #a0a0a0;
                padding-left:5px;
                padding-right:5px;
            }

            label {
                float:right;
                margin-right: 3%;
            }

            ul.multicolumn {
                list-style:none;
                float:left;
                padding-right:0px;
                margin-right:0px;
            }

            li.multicolumn {
                float:left;
                width:200px;
                margin-right:0px;
            }

            a {
                color:#a000a0;
                text-decoration:none;
            }

            a:hover {
                color:#a000a0;
                text-decoration:underline;
            }
            """

        css = self.indent(css, 0)

        handle.write(css)


def usage():
    print("\nUsage: \n\tblender2.5 --background --python BlendFileDnaExporter_25.py [-- [options]]")
    print("Options:")
    print("\t--dna-keep-blend:      doesn't delete the produced blend file DNA export to html")
    print("\t--dna-debug:           sets the logging level to DEBUG (lots of additional info)")
    print("\t--dna-versioned        saves version information in the html and blend filenames")
    print("\t--dna-overwrite-css    overwrite dna.css, useful when modifying css in the script")
    print("Examples:")
    print("\tdefault:       % blender2.5 --background --python BlendFileDnaExporter_25.py")
    print("\twith options:  % blender2.5 --background --python BlendFileDnaExporter_25.py -- --dna-keep-blend --dna-debug\n")


######################################################
# Main
######################################################

def main():

    import os
    import os.path

    try:
        bpy = __import__('bpy')

        # Files
        if '--dna-versioned' in sys.argv:
            blender_version = '_'.join(map(str, bpy.app.version))
            filename = 'dna-{0}-{1}_endian-{2}-{3}'.format(sys.arch, sys.byteorder, blender_version, bpy.app.build_hash)
        else:
            filename = 'dna'
        dir = os.path.dirname(__file__)
        Path_Blend = os.path.join(dir, filename + '.blend')  # temporary blend file
        Path_HTML = os.path.join(dir, filename + '.html')  # output html file
        Path_CSS = os.path.join(dir, 'dna.css')           # output css file

        # create a blend file for dna parsing
        if not os.path.exists(Path_Blend):
            log.info("1: write temp blend file with SDNA info")
            log.info("   saving to: " + Path_Blend)
            try:
                bpy.ops.wm.save_as_mainfile(filepath=Path_Blend, copy=True, compress=False)
            except Exception:
                log.error("Filename {0} does not exist and can't be created... quitting".format(Path_Blend))
                return
        else:
            log.info("1: found blend file with SDNA info")
            log.info("   " + Path_Blend)

        # read blend header from blend file
        log.info("2: read file:")

        if dir not in sys.path:
            sys.path.append(dir)
        import BlendFileReader

        handle = BlendFileReader.openBlendFile(Path_Blend)
        blendfile = BlendFileReader.BlendFile(handle)
        catalog = DNACatalogHTML(blendfile.Catalog, bpy)

        # close temp file
        handle.close()

        # deleting or not?
        if '--dna-keep-blend' in sys.argv:
            # Keep the blend, useful for studying HEX-dumps.
            log.info("5: closing blend file:")
            log.info("   {0}".format(Path_Blend))
        else:
            # delete the blend
            log.info("5: close and delete temp blend:")
            log.info("   {0}".format(Path_Blend))
            os.remove(Path_Blend)

        # export dna to xhtml
        log.info("6: export sdna to xhtml file: {!r}".format(Path_HTML))
        handleHTML = open(Path_HTML, "w")
        catalog.WriteToHTML(handleHTML)
        handleHTML.close()

        # only write the css when doesn't exist or at explicit request
        if not os.path.exists(Path_CSS) or '--dna-overwrite-css' in sys.argv:
            handleCSS = open(Path_CSS, "w")
            catalog.WriteToCSS(handleCSS)
            handleCSS.close()

        # quit blender
        if not bpy.app.background:
            log.info("7: quit blender")
            bpy.ops.wm.exit_blender()

    except ImportError:
        log.warning("  skipping, not running in Blender")
        usage()
        sys.exit(2)


if __name__ == '__main__':
    main()


## Links discovered
- [${revision}](https://svn.blender.org/svnroot/bf-blender/!svn/bc/${revision}/trunk/)
- [The mystery of the blend](https://github.com/blender/blender/blob/main/doc/blender_file_format/mystery_of_the_blend.html)

--- doc/blender_file_format/BlendFileReader.py ---
#!/usr/bin/env python3
# SPDX-FileCopyrightText: 2010-2023 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

######################################################
# Importing modules
######################################################

import os
import struct
import gzip
import tempfile

import logging
log = logging.getLogger("BlendFileReader")

######################################################
# module global routines
######################################################


def ReadString(handle, length):
    '''
    ReadString reads a String of given length or a zero terminating String
    from a file handle
    '''
    if length != 0:
        return handle.read(length).decode()
    else:
        # length == 0 means we want a zero terminating string
        result = ""
        s = ReadString(handle, 1)
        while s != "\0":
            result += s
            s = ReadString(handle, 1)
        return result


def Read(type, handle, fileheader):
    '''
    Reads the chosen type from a file handle
    '''
    def unpacked_bytes(type_char, size):
        return struct.unpack(fileheader.StructPre + type_char, handle.read(size))[0]

    if type == 'ushort':
        return unpacked_bytes("H", 2)   # unsigned short
    elif type == 'short':
        return unpacked_bytes("h", 2)   # short
    elif type == 'uint':
        return unpacked_bytes("I", 4)   # unsigned int
    elif type == 'int':
        return unpacked_bytes("i", 4)   # int
    elif type == 'float':
        return unpacked_bytes("f", 4)   # float
    elif type == 'ulong':
        return unpacked_bytes("Q", 8)   # unsigned long
    elif type == 'pointer':
        # The pointersize is given by the header (BlendFileHeader).
        if fileheader.PointerSize == 4:
            return Read('uint', handle, fileheader)
        if fileheader.PointerSize == 8:
            return Read('ulong', handle, fileheader)


def openBlendFile(filename):
    '''
    Open a filename, determine if the file is compressed and returns a handle
    '''
    handle = open(filename, 'rb')
    magic = ReadString(handle, 7)
    if magic in {"BLENDER", "BULLETf"}:
        log.debug("normal blendfile detected")
        handle.seek(0, os.SEEK_SET)
        return handle
    else:
        log.debug("gzip blendfile detected?")
        handle.close()
        log.debug("decompressing started")
        fs = gzip.open(filename, "rb")
        handle = tempfile.TemporaryFile()
        data = fs.read(1024 * 1024)
        while data:
            handle.write(data)
            data = fs.read(1024 * 1024)
        log.debug("decompressing finished")
        fs.close()
        log.debug("resetting decompressed file")
        handle.seek(0, os.SEEK_SET)
        return handle


def Align(handle):
    '''
    Aligns the file-handle on 4 bytes
    '''
    offset = handle.tell()
    trim = offset % 4
    if trim != 0:
        handle.seek(4 - trim, os.SEEK_CUR)


######################################################
# module classes
######################################################

class BlendFile:
    '''
    Reads a blend-file and store the header, all the file-blocks, and catalog
    structs found in the DNA file-block

    - BlendFile.Header  (BlendFileHeader instance)
    - BlendFile.Blocks  (list of BlendFileBlock instances)
    - BlendFile.Catalog (DNACatalog instance)
    '''

    def __init__(self, handle):
        log.debug("initializing reading blend-file")
        self.Header = BlendFileHeader(handle)
        self.Blocks = []
        fileblock = BlendFileBlock(handle, self)
        found_dna_block = False
        while not found_dna_block:
            if fileblock.Header.Code in {"DNA1", "SDNA"}:
                self.Catalog = DNACatalog(self.Header, handle)
                found_dna_block = True
            else:
                fileblock.Header.skip(handle)

            self.Blocks.append(fileblock)
            fileblock = BlendFileBlock(handle, self)

        # appending last fileblock, "ENDB"
        self.Blocks.append(fileblock)

    # seems unused?
    """
    def FindBlendFileBlocksWithCode(self, code):
        #result = []
        #for block in self.Blocks:
            #if block.Header.Code.startswith(code) or block.Header.Code.endswith(code):
                #result.append(block)
        #return result
    """


class BlendFileHeader:
    '''
    BlendFileHeader allocates the first 12 bytes of a blend file.
    It contains information about the hardware architecture.
    Header example: BLENDER_v254

    BlendFileHeader.Magic             (str)
    BlendFileHeader.PointerSize       (int)
    BlendFileHeader.LittleEndianness  (bool)
    BlendFileHeader.StructPre         (str)   see http://docs.python.org/py3k/library/struct.html#byte-order-size-and-alignment
    BlendFileHeader.Version           (int)
    '''

    def __init__(self, handle):
        log.debug("reading blend-file-header")

        self.Magic = ReadString(handle, 7)
        log.debug(self.Magic)

        pointersize = ReadString(handle, 1)
        log.debug(pointersize)
        if pointersize == "-":
            self.PointerSize = 8
        if pointersize == "_":
            self.PointerSize = 4

        endianness = ReadString(handle, 1)
        log.debug(endianness)
        if endianness == "v":
            self.LittleEndianness = True
            self.StructPre = "<"
        if endianness == "V":
            self.LittleEndianness = False
            self.StructPre = ">"

        version = ReadString(handle, 3)
        log.debug(version)
        self.Version = int(version)

        log.debug("{0} {1} {2} {3}".format(self.Magic, self.PointerSize, self.LittleEndianness, version))


class BlendFileBlock:
    '''
    BlendFileBlock.File     (BlendFile)
    BlendFileBlock.Header   (FileBlockHeader)
    '''

    def __init__(self, handle, blendfile):
        self.File = blendfile
        self.Header = FileBlockHeader(handle, blendfile.Header)

    def Get(self, handle, path):
        log.debug("find dna structure")
        dnaIndex = self.Header.SDNAIndex
        dnaStruct = self.File.Catalog.Structs[dnaIndex]
        log.debug("found " + dnaStruct.Type.Name)
        handle.seek(self.Header.FileOffset, os.SEEK_SET)
        return dnaStruct.GetField(self.File.Header, handle, path)


class FileBlockHeader:
    '''
    FileBlockHeader contains the information in a file-block-header.
    The class is needed for searching to the correct file-block (containing Code: DNA1)

    Code        (str)
    Size        (int)
    OldAddress  (pointer)
    SDNAIndex   (int)
    Count       (int)
    FileOffset  (= file pointer of data-block)
    '''

    def __init__(self, handle, fileheader):
        self.Code = ReadString(handle, 4).strip()
        if self.Code != "ENDB":
            self.Size = Read('uint', handle, fileheader)
            self.OldAddress = Read('pointer', handle, fileheader)
            self.SDNAIndex = Read('uint', handle, fileheader)
            self.Count = Read('uint', handle, fileheader)
            self.FileOffset = handle.tell()
        else:
            self.Size = Read('uint', handle, fileheader)
            self.OldAddress = 0
            self.SDNAIndex = 0
            self.Count = 0
            self.FileOffset = handle.tell()
        # self.Code += ' ' * (4 - len(self.Code))
        log.debug("found blend-file-block-fileheader {0} {1}".format(self.Code, self.FileOffset))

    def skip(self, handle):
        handle.read(self.Size)


class DNACatalog:
    '''
    DNACatalog is a catalog of all information in the DNA1 file-block

    Header = None
    Names = None
    Types = None
    Structs = None
    '''

    def __init__(self, fileheader, handle):
        log.debug("building DNA catalog")
        self.Names = []
        self.Types = []
        self.Structs = []
        self.Header = fileheader

        SDNA = ReadString(handle, 4)

        # names
        NAME = ReadString(handle, 4)
        numberOfNames = Read('uint', handle, fileheader)
        log.debug("building #{0} names".format(numberOfNames))
        for i in range(numberOfNames):
            name = ReadString(handle, 0)
            self.Names.append(DNAName(name))
        Align(handle)

        # types
        TYPE = ReadString(handle, 4)
        numberOfTypes = Read('uint', handle, fileheader)
        log.debug("building #{0} types".format(numberOfTypes))
        for i in range(numberOfTypes):
            type = ReadString(handle, 0)
            self.Types.append(DNAType(type))
        Align(handle)

        # type lengths
        TLEN = ReadString(handle, 4)
        log.debug("building #{0} type-lengths".format(numberOfTypes))
        for i in range(numberOfTypes):
            length = Read('ushort', handle, fileheader)
            self.Types[i].Size = length
        Align(handle)

        # structs
        STRC = ReadString(handle, 4)
        numberOfStructures = Read('uint', handle, fileheader)
        log.debug("building #{0} structures".format(numberOfStructures))
        for structureIndex in range(numberOfStructures):
            type = Read('ushort', handle, fileheader)
            Type = self.Types[type]
            structure = DNAStructure(Type)
            self.Structs.append(structure)

            numberOfFields = Read('ushort', handle, fileheader)
            for fieldIndex in range(numberOfFields):
                fTypeIndex = Read('ushort', handle, fileheader)
                fNameIndex = Read('ushort', handle, fileheader)
                fType = self.Types[fTypeIndex]
                fName = self.Names[fNameIndex]
                structure.Fields.append(DNAField(fType, fName))


class DNAName:
    '''
    DNAName is a C-type name stored in the DNA.

    Name = str
    '''

    def __init__(self, name):
        self.Name = name

    def AsReference(self, parent):
        if parent is None:
            result = ""
        else:
            result = parent + "."

        result = result + self.ShortName()
        return result

    def ShortName(self):
        result = self.Name
        result = result.replace("*", "")
        result = result.replace("(", "")
        result = result.replace(")", "")
        Index = result.find("[")
        if Index != -1:
            result = result[0:Index]
        return result

    def IsPointer(self):
        return self.Name.find("*") > -1

    def IsMethodPointer(self):
        return self.Name.find("(*") > -1

    def ArraySize(self):
        result = 1
        Temp = self.Name
        Index = Temp.find("[")

        while Index != -1:
            Index2 = Temp.find("]")
            result *= int(Temp[Index + 1:Index2])
            Temp = Temp[Index2 + 1:]
            Index = Temp.find("[")

        return result


class DNAType:
    '''
    DNAType is a C-type stored in the DNA

    Name = str
    Size = int
    Structure = DNAStructure
    '''

    def __init__(self, aName):
        self.Name = aName
        self.Structure = None


class DNAStructure:
    '''
    DNAType is a C-type structure stored in the DNA

    Type = DNAType
    Fields = [DNAField]
    '''

    def __init__(self, aType):
        self.Type = aType
        self.Type.Structure = self
        self.Fields = []

    def GetField(self, header, handle, path):
        splitted = path.partition(".")
        name = splitted[0]
        rest = splitted[2]
        offset = 0
        for field in self.Fields:
            if field.Name.ShortName() == name:
                log.debug("found " + name + "@" + str(offset))
                handle.seek(offset, os.SEEK_CUR)
                return field.DecodeField(header, handle, rest)
            else:
                offset += field.Size(header)

        log.debug("error did not find " + path)
        return None


class DNAField:
    '''
    DNAField is a coupled DNAType and DNAName.

    Type = DNAType
    Name = DNAName
    '''

    def __init__(self, aType, aName):
        self.Type = aType
        self.Name = aName

    def Size(self, header):
        if self.Name.IsPointer() or self.Name.IsMethodPointer():
            return header.PointerSize * self.Name.ArraySize()
        else:
            return self.Type.Size * self.Name.ArraySize()

    def DecodeField(self, header, handle, path):
        if path == "":
            if self.Name.IsPointer():
                return Read('pointer', handle, header)
            if self.Type.Name == "int":
                return Read('int', handle, header)
            if self.Type.Name == "short":
                return Read('short', handle, header)
            if self.Type.Name == "float":
                return Read('float', handle, header)
            if self.Type.Name == "char":
                return ReadString(handle, self.Name.ArraySize())
        else:
            return self.Type.Structure.GetField(header, handle, path)


--- doc/doxygen/footer.html ---
<hr class="footer"/>
<address class="footer">
  <small>Generated on $datetime for $projectname by&#160;
    <a href="http://www.doxygen.org/index.html"> doxygen</a>
    $doxygenversion
  </small>
</address>
</body>
</html>


## Links discovered
- [doxygen](http://www.doxygen.org/index.html)

--- doc/blender_file_format/mystery_of_the_blend.html ---
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
    <link rel="stylesheet" type="text/css" href="mystery_of_the_blend.css" media="screen, print">
    <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
    <title>The mystery of the blend</title>
</head>

<body>
<div class="title">The mystery of the blend</div>
<div class="subtitle">The blender file-format explained</div>
<div class="contact">
<label>Author</label> Jeroen Bakker<br>
<label>Email</label> <a href="mailto:j.bakker@atmind.nl">j.bakker@atmind.nl</a><br>
<label>Website</label> <a href="http://www.atmind.nl/blender/">http://www.atmind.nl/blender</a><br>
<label>Version</label> 06-10-2010<br>
</div>

<a name="introduction" href="#introduction" ><h2>Introduction</h2></a>
</a>

<p>In this article I will describe the
 blend-file-format with a request to tool-makers to support blend-file. 
   
</p>
<p>First I'll describe how Blender works with blend-files. You'll notice
 why the blend-file-format is not that well documented, as from 
Blender's perspective this is not needed.
We look at the global file-structure of a blend-file (the file-header 
and file-blocks).
After this is explained, we go deeper to the core of the blend-file, the
 DNA-structures. They hold the blue-prints of the blend-file and the key
 asset of understanding blend-files.
When that's done we can use these DNA-structures to read information 
from elsewhere in the blend-file.

</p>
<p>
In this article we'll be using the default blend-file from Blender 2.54,
 with the goal to read the output resolution from the Scene. 
The article is written to be programming language independent and I've 
setup a web-site for support.
</p>

<a name="loading-and-saving-in-blender" href="#loading-and-saving-in-blender">
<h2>Loading and saving in Blender</h2>
</a>

<p>
Loading and saving in Blender is very fast and Blender is known to 
have excellent downward and upward compatibility. Ton Roosendaal 
demonstrated that in December 2008 by loading a 1.0 blend-file using 
Blender 2.48a [ref: <a href="http://www.blendernation.com/2008/12/01/blender-dna-rna-and-backward-compatibility/">http://www.blendernation.com/2008/12/01/blender-dna-rna-and-backward-compatibility/</a>].
</p>

<p>
Saving complex scenes in Blender is done within seconds. Blender 
achieves this by saving data in memory to disk without any 
transformations or translations. Blender only adds file-block-headers to
 this data. A file-block-header contains clues on how to interpret the 
data. After the data, all internally Blender structures are stored. 
These structures will act as blue-prints when Blender loads the file. 
Blend-files can be different when stored on different hardware platforms
 or Blender releases. There is no effort taken to make blend-files 
binary the same. Blender creates the blend-files in this manner since 
release 1.0. Backward and upwards compatibility is not implemented when 
saving the file, this is done during loading.
</p>

<p>
When Blender loads a blend-file, the DNA-structures are read first. 
Blender creates a catalog of these DNA-structures. Blender uses this 
catalog together with the data in the file, the internal Blender 
structures of the Blender release you're using and a lot of 
transformation and translation logic to implement the backward and 
upward compatibility. In the source code of blender there is actually 
logic which can transform and translate every structure used by a 
Blender release to the one of the release you're using [ref: <a href="http://download.blender.org/source/blender-2.48a.tar.gz">http://download.blender.org/source/blender-2.48a.tar.gz</a>
 <a href="https://svn.blender.org/svnroot/bf-blender/tags/blender-2.48-release/source/blender/blenloader/intern/readfile.c">blender/blenloader/intern/readfile.c</a> lines 
4946-7960]. The more difference between releases the more logic is 
executed. 
</p>

<p>
The blend-file-format is not well documented, as it does not differ from
 internally used structures and the file can really explain itself.
</p>

<a name="global-file-structure" href="#global-file-structure">
<h2>Global file-structure</h2>
</a>

<p>
This section explains how the global file-structure can be read.
</p>

<ul>
<li>A blend-file always start with the <b>file-header</b></li>
<li>After the file-header, follows a list of <b>file-blocks</b> (the default blend file of Blender 2.48 contains more than 400 of these file-blocks).</li>
<li>Each file-block has a <b>file-block header</b> and <b>file-block data</b></li>
<li>At the end of the blend-file there is a section called "<a href="#structure-DNA" style="font-weight:bold">Structure DNA</a>", which lists all the internal structures of the Blender release the file was created in</li>
<li>The blend-file ends with a file-block called 'ENDB'</li>
</ul>

<!-- file scheme -->
<div class="box-solid" style="width:20%; margin-left:35%; font-size:0.8em;">

    <p class="code"><b>File.blend</b></p>

    <div class="box"><p class="code">File-header</p></div>

    <div class="box-solid"><p class="code">File-block</p>
        <div class="box"><p class="code">Header</p></div>
        <div class="box"><p class="code">Data</p></div>
    </div>
    
    <div class="box" style="border-style:dashed"><p class="code">File-block</p></div>
    <div class="box" style="border-style:dashed"><p class="code">File-block</p></div>

    <div class="box-solid"><p class="code">File-block 'Structure DNA'</p>
        <div class="box"><p class="code">Header ('DNA1')</p></div>
        <div class="box-solid">
            <p class="code">Data ('SDNA')</p>
            <div class="box">
                <p class="code">Names ('NAME')</p>
            </div>
            <div class="box">
                <p class="code">Types ('TYPE')</p>
            </div>
            <div class="box">
                <p class="code">Lengths ('TLEN')</p>
            </div>
            <div class="box">
                <p class="code">Structures ('STRC')</p>
            </div>
        </div>
    </div>

    <div class="box-solid"><p class="code">File-Block 'ENDB'</p></div>

</div><!-- end of file scheme -->

<a name="file-header" href="#file-header">
<h3>File-Header</h3>
</a>

<p>
The first 12 bytes of every blend-file is the file-header. The 
file-header has information on Blender (version-number) and the PC the 
blend-file was saved on (pointer-size and endianness). This is required 
as all data inside the blend-file is ordered in that  way, because no  
translation or transformation  is done during saving. 
The next table describes the information in the file-header.
</p>

<table>
<caption>File-header</caption>
<thead>
<tr><th>reference</th>
    <th>structure</th>
    <th>type</th>
    <th>offset</th>
    <th>size</th></tr>
</thead>
<tbody>
<tr><td>identifier</td>
    <td>char[7]</td>
    <td>File identifier (always 'BLENDER')</td>
    <td>0</td>
    <td>7</td></tr>
<tr><td>pointer-size</td>
    <td>char</td>
    <td>Size of a pointer; all pointers in the file are stored in this format. '_' means 4 bytes or 32 bit and '-' means 8 bytes or 64 bits.</td>
    <td>7</td>
    <td>1</td></tr>
<tr><td>endianness</td>
    <td>char</td>
    <td>Type of byte ordering used; 'v' means little endian and 'V' means big endian.</td>
    <td>8</td>
    <td>1</td></tr>
<tr><td>version-number</td>
    <td>char[3]</td>
    <td>Version of Blender the file was created in; '254' means version 2.54</td>
    <td>9</td>
    <td>3</td></tr>
</tbody>
</table>

<p>
<a href="https://en.wikipedia.org/wiki/Endianness">Endianness</a> addresses the way values are ordered in a sequence of bytes(see the <a href="#example-endianess">example</a> below):
</p>

<ul>
    <li>in a big endian ordering, the largest part of the value is placed on the first byte and 
    the lowest part of the value is placed on the last byte,</li>
    <li>in a little endian ordering, largest part of the value is placed on the last byte
    and the smallest part of the value is placed on the first byte.</li>
</ul>

<p> 
Nowadays, little-endian is the most commonly used.
</p>

<a name="example-endianess"></a>
<div class="box">
<p onclick="location.href='#example-endianess'" class="box-title">
Endianness Example
</p>
<p>
Writing the integer <code class="evidence">0x4A3B2C1Dh</code>, will be ordered:
<ul>
<li>in big endian as <code class="evidence">0x4Ah</code>, <code class="evidence">0x3Bh</code>, <code class="evidence">0x2Ch</code>, <code class="evidence">0x1Dh</code></li>
<li>in little endian as <code class="evidence">0x1Dh</code>, <code class="evidence">0x2Ch</code>, <code class="evidence">0x3Bh</code>, <code class="evidence">0x4Ah</code></li>
</ul>
</p>
</div>

<p>
Blender supports little-endian and big-endian.<br>
This means that when the endianness 
is different between the blend-file and the PC your using, Blender changes it to the byte ordering 
of your PC.
</p>

<a name="example-file-header"></a>
<div class="box">
<p onclick="location.href='#example-file-header'" class="box-title">
File-header Example
</p>

<p>
This hex-dump describes a file-header created with <code>blender</code> <code>2.54.0</code> on <code>little-endian</code> hardware with a <code>32 bits</code> pointer length.
<code class="block">                               <span class="descr">pointer-size  version-number
                                     |             |</span>
0000 0000: [42 4C 45 4E 44 45 52]  [5F]  [76]  [32 35 34]        BLENDER_v254  <span class="descr">
                      |                   |
                  identifier          endianness</span></code>
</p>
</div>

<a name="file-blocks" href="#file-blocks"><h3>File-blocks</h3></a>

<p>
File-blocks contain a "<a href="#file-block-header">file-block header</a>" and "file-block data". 
</p>

<a name="file-block-header" href="#file-block-header"><h3>File-block headers</h3></a>

<p>
The file-block-header describes:
</p>

<ul>
<li>the type of information stored in the 
file-block</li>
<li>the total length of the data</li>
<li>the old memory 
pointer at the moment the data was written to disk</li>
<li>the number of items of this information</li>
</ul>

<p>
As we can see below, depending on the pointer-size stored in the file-header, a file-block-header 
can be 20 or 24 bytes long, hence it is always aligned at 4 bytes.
</p>

<table>
<caption>File-block-header</caption>
<thead>
<tr>
<th>reference</th>
    <th>structure</th>
    <th>type</th>
    <th>offset</th>
    <th>size</th></tr></thead>
<tbody>
<tr><td>code</td>
    <td>char[4]</td>
    <td>File-block identifier</td>
    <td>0</td>
    <td>4</td></tr>
<tr><td>size</td>
    <td>integer</td>
    <td>Total length of the data after the file-block-header</td>
    <td>4</td>
    <td>4</td></tr>
<tr><td>old memory address</td>
    <td>void*</td>
    <td>Memory address the structure was located when written to disk</td>
    <td>8</td>
    <td>pointer-size (4/8)</td></tr>
<tr><td>SDNA index</td>
    <td>integer</td>
    <td>Index of the SDNA structure</td>
    <td>8+pointer-size</td>
    <td>4</td></tr>
<tr><td>count</td>
    <td>integer</td>
    <td>Number of structure located in this file-block</td>
    <td>12+pointer-size</td>
    <td>4</td></tr>
</tbody>
</table>

<p>
The above table describes how a file-block-header is structured:
</p>

<ul>
<li><code>Code</code> describes different types of file-blocks. The code determines with what logic the data must be read. <br>  
These codes also allows fast finding of data like Library, Scenes, Object or Materials as they all have a specific code. </li>
<li><code>Size</code> contains the total length of data after the file-block-header. 
After the data a new file-block starts. The last file-block in the file 
has code 'ENDB'.</li>
<li><code>Old memory address</code> contains the memory address when the structure 
was last stored. When loading the file the structures can be placed on 
different memory addresses. Blender updates pointers to these structures
 to the new memory addresses.</li>
<li><code>SDNA index</code> contains the index in the DNA structures to be used when 
reading this file-block-data. <br>
More information about this subject will be explained in the <a href="#reading-scene-information">Reading scene information section</a>.</li>
<li><code>Count</code> tells how many elements of the specific SDNA structure can be found in the data.</li> 
</ul>

<a name="example-file-block-header"></a>
<div class="box">
<p onclick="location.href='#example-file-block-header'" class="box-title">
Example
</p>
<p>
This hex-dump describes a File-block (= <span class="header">File-block header</span> + <span class="data">File-block data</span>) created with <code>blender</code> <code>2.54</code> on <code>little-endian</code> hardware with a <code>32 bits</code> pointer length.<br>
<code class="block"><span class="descr">             file-block 
           identifier='SC'  data size=1404  old pointer   SDNA index=150
                 |              |              |              |</span>
0000 4420: <span class="header">[53 43 00 00]  [7C 05 00 00]  [68 34 FB 0B]  [96 00 00 00]</span>    SC.. `... ./.. .... 
0000 4430: <span class="header">[01 00 00 00]</span>  <span class="data">[xx xx xx xx    xx xx xx xx    xx xx xx xx</span>     .... xxxx xxxx xxxx<span class="descr">
                 |              |
              count=1      file-block data (next 1404 bytes)</span>
</code>
</p>

<ul>
<li>The code <code>'SC'+0x00h</code> identifies that it is a Scene. </li>
<li>Size of the data is 1404 bytes (0x0000057Ch = 0x7Ch + 0x05h * 256 = 124 + 1280)</li>
<li>The old pointer is 0x0BFB3468h</li>
<li>The SDNA index is 150 (0x00000096h = 6 + 9 * 16 = 6 + 144)</li>
<li>The section contains a single scene (count = 1).</li>
</ul>

<p>
Before we can interpret the data of this file-block we first have to read the DNA structures in the file. 
The section "<a href="#structure-DNA">Structure DNA</a>" will show how to do that. 
</p>
</div>

<a name="structure-DNA" href="#structure-DNA"><h2>Structure DNA</h2></a>

<a name="DNA1-file-block" href="#DNA1-file-block"><h3>The DNA1 file-block</h3></a>

<p>
Structure DNA is stored in a file-block with code 'DNA1'. It can be just before the 'ENDB' file-block.
</p>

<p>
The 'DNA1' file-block contains all internal structures of the Blender release the 
file was created in. <br>
These structure can be described as C-structures: they can hold fields, arrays and 
pointers to other structures, just like a normal C-structure.

<p>
<code class="block">struct SceneRenderLayer {
    struct SceneRenderLayer *next, *prev;
    char name[32];
    struct Material *mat_override;
    struct Group *light_override;
    unsigned int lay;
    unsigned int lay_zmask;
    int layflag;
    int pad;
    int passflag;
    int pass_xor;
};
</code>
</p>

<p>
For example,a blend-file created with Blender 2.54 the 'DNA1' file-block is 57796 bytes long and contains 398 structures.
</p>

<a name="DNA1-file-block-header" href="#DNA1-file-block-header"><h3>DNA1 file-block-header</h3></a>

<p>
The DNA1 file-block header follows the same rules of any other file-block, see the example below.
</p>

<a name="example-DNA1-file-block-header"></a>
<div class="box">
<p onclick="location.href='#example-DNA1-file-block-header'" class="box-title">
Example
</p>
<p>
This hex-dump describes the file-block 'DNA1' header created with <code>blender</code> <code>2.54.0</code> on <code>little-endian</code> hardware with a <code>32 bits</code> pointer length.<br>
<code class="block"><span class="descr">      (file-block 
       identifier='DNA1') data size=57796   old pointer   SDNA index=0
                  |              |              |              |</span>
0004 B060   <span class="header">[44 4E 41 31]  [C4 E1 00 00]  [C8 00 84 0B]  [00 00 00 00]</span>  DNA1............
0004 B070   <span class="header">[01 00 00 00]</span>  <span class="fade">[53 44 4E 41    4E 41 4D 45    CB 0B 00 00</span>   ....<span class="fade">SDNANAME....</span><span class="descr">
                  |              |
              count=1    'DNA1' file-block data (next 57796 bytes)</span>
</code>
</p>
</div>

<a name="DNA1-file-block-data" href="#DNA1-file-block-data"><h3>DNA1 file-block data</h3></a>
<p>
The next section describes how this information is ordered in the <b>data</b> of the 'DNA1' file-block.
</p>

<table>
<caption>Structure of the DNA file-block-data</caption>
<thead>
    <tr><th colspan="2">repeat condition</th>
    <th>name</th>
    <th>type</th>
    <th>length</th>
    <th>description</th></tr>
</thead>
<tbody>
<tr><td></td>
    <td></td>
    <td>identifier</td>
    <td>char[4]</td>
    <td>4</td>
    <td>'SDNA'</td></tr>
<tr><td></td>
    <td></td>
    <td>name identifier</td>
    <td>char[4]</td>
    <td>4</td>
    <td>'NAME'</td></tr>
<tr><td></td>
    <td></td>
    <td>#names</td>
    <td>integer</td>
    <td>4</td>
    <td>Number of names follows</td></tr>
<tr><td>for(#names)</td>
    <td></td>
    <td>name</td>
    <td>char[]</td>
    <td>?</td>
    <td>Zero terminating string of name, also contains pointer and simple array definitions (e.g. '*vertex[3]\0')</td></tr>
<tr><td></td>
    <td></td>
    <td>type identifier</td>
    <td>char[4]</td>
    <td>4</td>
    <td>'TYPE' this field is aligned at 4 bytes</td></tr>
<tr><td></td>
    <td></td>
    <td>#types</td>
    <td>integer</td>
    <td>4</td>
    <td>Number of types follows</td></tr>
<tr><td>for(#types)</td>
    <td></td>
    <td>type</td>
    <td>char[]</td>
    <td>?</td>
    <td>Zero terminating string of type (e.g. 'int\0')</td></tr>
<tr><td></td>
    <td></td>
    <td>length identifier</td>
    <td>char[4]</td>
    <td>4</td>
    <td>'TLEN' this field is aligned at 4 bytes</td></tr>
<tr><td>for(#types)</td>
    <td></td>
    <td>length</td>
    <td>short</td>
    <td>2</td>
    <td>Length in bytes of type (e.g. 4)</td></tr>
<tr><td></td>
    <td></td>
    <td>structure identifier</td>
    <td>char[4]</td>
    <td>4</td>
    <td>'STRC' this field is aligned at 4 bytes</td></tr>
<tr><td></td>
    <td></td>
    <td>#structures</td>
    <td>integer</td>
    <td>4</td>
    <td>Number of structures follows</td></tr>
<tr><td>for(#structures)</td>
    <td></td>
    <td>structure type</td>
    <td>short</td>
    <td>2</td>
    <td>Index in types containing the name of the structure</td></tr>
<tr><td>..</td>
    <td></td>
    <td>#fields</td>
    <td>short</td>
    <td>2</td>
    <td>Number of fields in this structure</td></tr>
<tr><td>..</td>
    <td>for(#field)</td>
    <td>field type</td>
    <td>short</td>
    <td>2</td>
    <td>Index in type</td></tr>
<tr><td>for end</td>
    <td>for end</td>
    <td>field name</td>
    <td>short</td>
    <td>2</td>
    <td>Index in name</td></tr>
</tbody>
</table>

<p>
As you can see, the structures are stored in 4 arrays: names, types, 
lengths and structures. Every structure also contains an array of 
fields. A field is the combination of a type and a name. From this 
information a catalog of all structures can be constructed. 
The names are stored as how a C-developer defines them. This means that 
the name also defines pointers and arrays.
(When a name starts with '*' it is used as a pointer. when the name 
contains for example '[3]' it is used as a array of 3 long.)
In the types you'll find simple-types (like: 'integer', 'char', 
'float'), but also complex-types like 'Scene' and 'MetaBall'. 
'TLEN' part describes the length of the types. A 'char' is 1 byte, an 
'integer' is 4 bytes and a 'Scene' is 1376 bytes long.
</p>

<div class="box">
<p class="box-title">
Note
</p>
<p>
All identifiers, are arrays of 4 chars, hence they are all aligned at 4 bytes.
</p>
</div>

<a name="example-DNA1-file-block-data"></a>
<div class="box">
<p onclick="location.href='#example-DNA1-file-block-data'" class="box-title">
Example
</p>
<p>
Created with <code>blender</code> <code>2.54.0</code> on <code>little-endian</code> hardware with a <code>32 bits</code> pointer length.
</p>

<a name="DNA1-data-array-names" href="#DNA1-data-array-names"><h4>The names array</h4></a>
<p>
The first names are: *next, *prev, *data, *first, *last, x, y, xmin, xmax, ymin, ymax, *pointer, group, val, val2, type, subtype, flag, name[32], ...
<code class="block"><span class="descr">    file-block-data identifier='SDNA'  array-id='NAME'     number of names=3019
                                |              |             |</span>
0004 B070    <span class="fade">01 00  00 00 [53 44  4E 41]</span><span class="data">[4E  41 4D 45] [CB 0B 00  00]</span>  <span class="fade">....SDNA</span>NAME....
0004 B080   <span class="data">[2A 6E  65 78  74 00][2A 70  72  65 76 00] [2A 64 61  74</span>   *next.*prev.*dat<span class="descr">
                      |                    |                   | 
                  '*next\0'            '*prev\0'            '*dat'</span><span class="fade">
                                  ....
                                  .... (3019 names)</span>
</code>
</p>

<div class="box">
<p class="box-title">
Note
</p>
<p>
While reading the DNA you'll will come across some strange 
names like '(*doit)()'. These are method pointers and Blender updates 
them to the correct methods.
</p>
</div>

<a name="DNA1-data-array-types" href="#DNA1-data-array-types"><h4>The types array</h4></a>
<p>
The first types are: char, uchar, short, ushort, int, long, ulong, float, double, void, Link, LinkData, ListBase, vec2s, vec2f, ...
<code class="block"><span class="descr">                                                      array-id='TYPE'
                                                            |</span>
0005 2440    <span class="fade">6F 6C 64 5B   34 5D 5B 34 5D 00 00 00</span>   [54 59 50  45]  <span class="fade">old[4][4]...</span>TYPE
0005 2450   [C9 01 00 00] [63 68 61 72 00]  [75 63 68 61 72 00][73   ....char.uchar.s<span class="descr">
                  |                |                 |           |
       number of types=457     'char\0'          'uchar\0'      's'</span><span class="fade">
                                  ....
                                  .... (457 types)</span>
</code>
</p>

<a name="DNA1-data-array-lengths" href="#DNA1-data-array-lengths"><h4>The lengths array</h4></a>
<p>
<code class="block"><span class="descr">                                                 char    uchar    ushort   short
                                 array-id       length   length   length   length
                                   'TLEN'          1        1        2        2</span>
0005 3AA0    <span class="fade">45 00    00 00</span>   [54 4C    45 4E]  [01 00]  [01 00]  [02 00]  [02 00]  <span class="fade">E...</span>TLEN........
                                  <span class="fade">....</span>
0005 3AC0   [08 00]  [04 00]  [08 00]  [10 00]  [10 00]  [14 00]  [4C 00]  [34 00]  ............L.4.<span class="descr">
               8        4        8       
           ListBase   vec2s    vec2f    ... etc
           length     len      length   </span><span class="fade">
                                  ....
                                  .... (457 lengths, same as number of types)</span>
</code>
</p>

<a name="DNA1-data-array-structures" href="#DNA1-data-array-structures"><h4>The structures array</h4></a>
<p>
<code class="block"><span class="descr">                                                                 array-id='STRC'
                                                                        |</span>
0005 3E30    <span class="fade">40 00 38 00   60 00   00 00     00 00     00 00</span>    [53 54     52 43]  <span class="fade">@.8.`.......</span>STRC
0005 3E40   [8E 01 00 00] [0A 00] [02 00]   [0A 00]   [00 00]   [0A 00]   [01 00]  ................<span class="descr">
                 398         10      2         10        0         10        0  
              number of    index    fields    index     index     index     index
             structures   in <a href="#DNA1-data-array-types">types</a>           in <a href="#DNA1-data-array-types">types</a>  in <a href="#DNA1-data-array-names">names</a>  in <a href="#DNA1-data-array-types">types</a>   in <a href="#DNA1-data-array-names">names</a></span><span class="fade">
                          '                  '----------------'  '-----------------'                                    '
                          '                      field 0                field 1    '
                          '--------------------------------------------------------'
                                             structure 0
                                  ....
                                  .... (398 structures, each one describeing own type, and type/name for each field)</span>
</code>
</p>
</div>

<p>
The DNA structures inside a Blender 2.48 blend-file can be found at <a href="http://www.atmind.nl/blender/blender-sdna.html">http://www.atmind.nl/blender/blender-sdna.html</a>.

If we understand the DNA part of the file it is now possible to read 
information from other parts file-blocks. The next section will tell us 
how.
</p>

<a name="reading-scene-information" href="#reading-scene-information"><h2>Reading scene information</h2></a>

<p>
Let us look at <a href="#example-file-block-header">the file-block header we have seen earlier</a>:<br>
</p>
<ul>
<li>the file-block identifier is <code>'SC'+0x00h</code></li>
<li>the SDNA index is 150</li>
<li>the file-block size is 1404 bytes</li>
</ul>
<p>
Now note that:
<ul>
<li>the structure at index 150 in the DNA is a structure of type 'Scene' (counting from 0).</li>
<li>the associated type ('Scene') in the DNA has the length of 1404 bytes.</li>
</ul>
</p>

<p> 
We can map the Scene structure on the data of the file-blocks. 
But before we can do that, we have to flatten the Scene-structure.

<code class="block">struct Scene {
    ID id;              <span class="descr">// 52 bytes long (ID is different a structure)</span>
    AnimData *adt;      <span class="descr">// 4 bytes long (pointer to an AnimData structure)</span>
    Object *camera;     <span class="descr">// 4 bytes long (pointer to an Object structure)</span>
    World *world;       <span class="descr">// 4 bytes long (pointer to an Object structure)</span>
    ...
    float cursor[3];    <span class="descr">// 12 bytes long (array of 3 floats)</span>
    ...
};
</code>

The first field in the Scene-structure is of type 'ID' with the name 'id'.
Inside the list of DNA structures there is a structure defined for type 'ID' (structure index 17).
 
<code class="block">struct ID {
    void *next, *prev;
    struct ID *newid;
    struct Library *lib;
    char name[24];
    short us;
    short flag;
    int icon_id;
    IDProperty *properties;
};
</code>

The first field in this structure has type 'void' and name '*next'. <br>
Looking in the structure list there is no structure defined for type 'void': it is a simple type and therefore the data should be read. 
The name '*next' describes a pointer.
As we see, the first 4 bytes of the data can be mapped to 'id.next'.
</p>

<p>
Using this method we'll map a structure to its data. If we want to 
read a specific field we know at which offset in the data it is located 
and how much space it takes.<br>
The next table shows the output of this flattening process for some 
parts of the Scene-structure. Not all rows are described in the table as
 there is a lot of information in a Scene-structure.
</p>

<table>
<caption>Flattened SDNA structure 150: Scene</caption>
<thead>
<tr><th>reference</th>
    <th>structure</th>
    <th>type</th><th>name</th>
    <th>offset</th><th>size</th>
    <th>description</th></tr>
</thead>
<tbody>
<tr><td>id.next</td><td><a href="#struct:ID">ID</a></td>
    <td>void</td><td>*next</td>
    <td>0</td>
    <td>4</td>
    <td>Refers to the next scene</td></tr>
<tr><td>id.prev</td><td><a href="#struct:ID">ID</a></td>
    <td>void</td><td>*prev</td>
    <td>4</td>
    <td>4</td>
    <td>Refers to the previous scene</td></tr>
<tr><td>id.newid</td><td><a href="#struct:ID">ID</a></td>
    <td>ID</td><td>*newid</td>
    <td>8</td>
    <td>4</td>
    <td></td></tr>
<tr><td>id.lib</td><td><a href="#struct:ID">ID</a></td>
    <td>Library</td><td>*lib</td>
    <td>12</td>
    <td>4</td>
    <td></td></tr>
<tr><td>id.name</td><td><a href="#struct:ID">ID</a></td>
    <td>char</td><td>name[24]</td>
    <td>16</td>
    <td>24</td>
    <td>'SC'+the name of the scene as displayed in Blender</td></tr>
<tr><td>id.us</td><td><a href="#struct:ID">ID</a></td>
    <td>short</td><td>us</td>
    <td>40</td>
    <td>2</td>
    <td></td></tr>
<tr><td>id.flag</td><td><a href="#struct:ID">ID</a></td>
    <td>short</td><td>flag</td><td>42</td><td>2</td>
    <td></td></tr>
<tr><td>id.icon_id</td><td><a href="#struct:ID">ID</a></td>
    <td>int</td><td>icon_id</td><td>44</td>
    <td>4</td>
    <td></td></tr>
<tr><td>id.properties</td><td><a href="#struct:ID">ID</a></td>
    <td>IDProperty</td><td>*properties</td>
    <td>48</td>
    <td>4</td>
    <td></td></tr>
<tr><td>adt</td><td>Scene</td><td>AnimData</td>
    <td>*adt</td>
    <td>52</td>
    <td>4</td>
    <td></td></tr>
<tr><td>camera</td><td>Scene</td>
    <td>Object</td>
    <td>*camera</td>
    <td>56</td>
    <td>4</td>
    <td>Pointer to the current camera</td></tr>
<tr><td>world</td><td>Scene</td>
    <td>World</td>
    <td>*world</td>
    <td>60</td>
    <td>4</td>
    <td>Pointer to the current world</td></tr>

<tr><td class="skip" colspan="7">Skipped rows</td></tr>

<tr><td>r.xsch</td><td><a href="#struct:RenderData">RenderData</a>
    </td><td>short</td><td>xsch</td><td>382</td><td>2</td>
    <td>X-resolution of the output when rendered at 100%</td></tr>
<tr><td>r.ysch</td><td><a href="#struct:RenderData">RenderData</a>
    </td><td>short</td><td>ysch</td><td>384</td><td>2</td>
    <td>Y-resolution of the output when rendered at 100%</td></tr>
<tr><td>r.xparts</td><td><a href="#struct:RenderData">RenderData</a>
    </td><td>short</td><td>xparts</td><td>386</td><td>2</td>
    <td>Number of x-part used by the renderer</td></tr>
<tr><td>r.yparts</td><td><a href="#struct:RenderData">RenderData</a>
    </td><td>short</td><td>yparts</td><td>388</td><td>2</td>
    <td>Number of x-part used by the renderer</td></tr>

<tr><td class="skip" colspan="7">Skipped rows</td></tr>

<tr><td>gpd</td><td>Scene</td><td>bGPdata</td><td>*gpd</td><td>1376</td><td>4</td>
    <td></td></tr>
<tr><td>physics_settings.gravity</td><td><a href="#struct:PhysicsSettings">PhysicsSettings</a>
    </td><td>float</td><td>gravity[3]</td><td>1380</td><td>12</td>
    <td></td></tr>
<tr><td>physics_settings.flag</td><td><a href="#struct:PhysicsSettings">PhysicsSettings</a>
    </td><td>int</td><td>flag</td><td>1392</td><td>4</td>
    <td></td></tr>
<tr><td>physics_settings.quick_cache_step</td><td><a href="#struct:PhysicsSettings">PhysicsSettings</a>
    </td><td>int</td><td>quick_cache_step</td><td>1396</td><td>4</td>
    <td></td></tr>
<tr><td>physics_settings.rt</td><td><a href="#struct:PhysicsSettings">PhysicsSettings</a>
    </td><td>int</td><td>rt</td><td>1400</td><td>4</td>
    <td></td></tr>
</tbody>
</table>

<p>
We can now read the X and Y resolution of the Scene:
<ul>
<li>the X-resolution is located on offset 382 of the file-block-data and must be read as a 
short.</li>
<li>the Y-resolution is located on offset 384 and is also a short</li>
</ul>
</p>

<div class="box">
<p class="box-title">
Note
</p>
<p>
An array of chars can mean 2 things. The field contains readable 
text or it contains an array of flags (not humanly readable).
</p>
</div>

<div class="box">
<p class="box-title">
Note
</p>
<p>
A file-block containing a list refers to the DNA structure and has a count larger
than 1. For example Vertices and Faces are stored in this way.
</p>
</div>

</body>
</html>



## Links discovered
- [http://www.atmind.nl/blender](http://www.atmind.nl/blender/)
- [http://www.blendernation.com/2008/12/01/blender-dna-rna-and-backward-compatibility/](http://www.blendernation.com/2008/12/01/blender-dna-rna-and-backward-compatibility/)
- [http://download.blender.org/source/blender-2.48a.tar.gz](http://download.blender.org/source/blender-2.48a.tar.gz)
- [blender/blenloader/intern/readfile.c](https://svn.blender.org/svnroot/bf-blender/tags/blender-2.48-release/source/blender/blenloader/intern/readfile.c)
- [Endianness](https://en.wikipedia.org/wiki/Endianness)
- [http://www.atmind.nl/blender/blender-sdna.html](http://www.atmind.nl/blender/blender-sdna.html)

--- extern/audaspace/README.md ---
audaspace
=========

Audaspace (pronounced "outer space") is a high level audio library written in C++ with language bindings for Python for example. It started out as the audio engine of the 3D modelling application Blender and is now released as a standalone library.

Documentation and Community
---------------------------

The documentation including guides for building and installing, demos, tutorials as well as the API reference for C++, C and python can be found on https://audaspace.github.io.

Bug reports and feature requests should go to the [issue tracker](https://github.com/audaspace/audaspace/issues).

For any other discussions about audaspace there is a [mailing list](https://groups.google.com/forum/#!forum/audaspace) and there is also the IRC channel #audaspace on irc.freenode.net.

Features
--------

The following (probably incomplete) features are supported by audaspace:

* input/output devices
 * input from microphones, line in, etc.
 * output devices including 3D audio support
* file reading/writing
* filters like low-/highpass and effects like delay, reverse or fading
* generators for simple waveforms like silence, sine and triangle
* respecification - this term is used for changing stream parameters which are
 * channel count - channel remapping
 * sample format - the library internally uses 32 bit floats
 * sample rate - resampling
* simple (superposition, joining and ping-pong aka forward-reverse) and more complex (non-linear audio editing) sequencing of sounds

License
-------

> Copyright  2009-2025 Jrg Mller. All rights reserved.
>
> Licensed under the Apache License, Version 2.0 (the "License");
> you may not use this file except in compliance with the License.
> You may obtain a copy of the License at
>
>   http://www.apache.org/licenses/LICENSE-2.0
>
> Unless required by applicable law or agreed to in writing, software
> distributed under the License is distributed on an "AS IS" BASIS,
> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
> See the License for the specific language governing permissions and
> limitations under the License.


## Links discovered
- [issue tracker](https://github.com/audaspace/audaspace/issues)
- [mailing list](https://groups.google.com/forum/#!forum/audaspace)

--- extern/ceres/README.md ---
[![Android](https://github.com/ceres-solver/ceres-solver/actions/workflows/android.yml/badge.svg)](https://github.com/ceres-solver/ceres-solver/actions/workflows/android.yml)
[![Linux](https://github.com/ceres-solver/ceres-solver/actions/workflows/linux.yml/badge.svg)](https://github.com/ceres-solver/ceres-solver/actions/workflows/linux.yml)
[![macOS](https://github.com/ceres-solver/ceres-solver/actions/workflows/macos.yml/badge.svg)](https://github.com/ceres-solver/ceres-solver/actions/workflows/macos.yml)
[![Windows](https://github.com/ceres-solver/ceres-solver/actions/workflows/windows.yml/badge.svg)](https://github.com/ceres-solver/ceres-solver/actions/workflows/windows.yml)

Ceres Solver
============

Ceres Solver is an open source C++ library for modeling and solving
large, complicated optimization problems. It is a feature rich, mature
and performant library which has been used in production at Google
since 2010. Ceres Solver can solve two kinds of problems.

1. Non-linear Least Squares problems with bounds constraints.
2. General unconstrained optimization problems.

Please see [ceres-solver.org](http://ceres-solver.org/) for more
information.


## Links discovered
- [![Android](https://github.com/ceres-solver/ceres-solver/actions/workflows/android.yml/badge.svg)
- [![Linux](https://github.com/ceres-solver/ceres-solver/actions/workflows/linux.yml/badge.svg)
- [![macOS](https://github.com/ceres-solver/ceres-solver/actions/workflows/macos.yml/badge.svg)
- [![Windows](https://github.com/ceres-solver/ceres-solver/actions/workflows/windows.yml/badge.svg)
- [ceres-solver.org](http://ceres-solver.org/)

--- extern/fast_float/README.md ---

## fast_float number parsing library: 4x faster than strtod
[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/fast_float.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:fast_float)
[![VS17-CI](https://github.com/fastfloat/fast_float/actions/workflows/vs17-ci.yml/badge.svg)](https://github.com/fastfloat/fast_float/actions/workflows/vs17-ci.yml)
[![Ubuntu 22.04 CI (GCC 11)](https://github.com/fastfloat/fast_float/actions/workflows/ubuntu22.yml/badge.svg)](https://github.com/fastfloat/fast_float/actions/workflows/ubuntu22.yml)

The fast_float library provides fast header-only implementations for the C++ from_chars
functions for `float` and `double` types.  These functions convert ASCII strings representing
decimal values (e.g., `1.3e10`) into binary types. We provide exact rounding (including
round to even). In our experience, these `fast_float` functions many times faster than comparable number-parsing functions from existing C++ standard libraries.

Specifically, `fast_float` provides the following two functions with a C++17-like syntax (the library itself only requires C++11):

```C++
from_chars_result from_chars(const char* first, const char* last, float& value, ...);
from_chars_result from_chars(const char* first, const char* last, double& value, ...);
```

The return type (`from_chars_result`) is defined as the struct:
```C++
struct from_chars_result {
    const char* ptr;
    std::errc ec;
};
```

It parses the character sequence [first,last) for a number. It parses floating-point numbers expecting
a locale-independent format equivalent to the C++17 from_chars function.
The resulting floating-point value is the closest floating-point values (using either float or double),
using the "round to even" convention for values that would otherwise fall right in-between two values.
That is, we provide exact parsing according to the IEEE standard.


Given a successful parse, the pointer (`ptr`) in the returned value is set to point right after the
parsed number, and the `value` referenced is set to the parsed value. In case of error, the returned
`ec` contains a representative error, otherwise the default (`std::errc()`) value is stored.

The implementation does not throw and does not allocate memory (e.g., with `new` or `malloc`).

It will parse infinity and nan values.

Example:

``` C++
#include "fast_float/fast_float.h"
#include <iostream>

int main() {
    const std::string input =  "3.1416 xyz ";
    double result;
    auto answer = fast_float::from_chars(input.data(), input.data()+input.size(), result);
    if(answer.ec != std::errc()) { std::cerr << "parsing failure\n"; return EXIT_FAILURE; }
    std::cout << "parsed the number " << result << std::endl;
    return EXIT_SUCCESS;
}
```


Like the C++17 standard, the `fast_float::from_chars` functions take an optional last argument of
the type `fast_float::chars_format`. It is a bitset value: we check whether
`fmt & fast_float::chars_format::fixed` and `fmt & fast_float::chars_format::scientific` are set
to determine whether we allow the fixed point and scientific notation respectively.
The default is  `fast_float::chars_format::general` which allows both `fixed` and `scientific`.

The library seeks to follow the C++17 (see [20.19.3](http://eel.is/c++draft/charconv.from.chars).(7.1))  specification.
* The `from_chars` function does not skip leading white-space characters.
* [A leading `+` sign](https://en.cppreference.com/w/cpp/utility/from_chars) is forbidden.
* It is generally impossible to represent a decimal value exactly as binary floating-point number (`float` and `double` types). We seek the nearest value. We round to an even mantissa when we are in-between two binary floating-point numbers.

Furthermore, we have the following restrictions:
* We only support `float` and `double` types at this time.
* We only support the decimal format: we do not support hexadecimal strings.
* For values that are either very large or very small (e.g., `1e9999`), we represent it using the infinity or negative infinity value and the returned `ec` is set to `std::errc::result_out_of_range`.

We support Visual Studio, macOS, Linux, freeBSD. We support big and little endian. We support 32-bit and 64-bit systems.

We assume that the rounding mode is set to nearest (`std::fegetround() == FE_TONEAREST`).

## C++20: compile-time evaluation (constexpr)

In C++20, you may use `fast_float::from_chars` to parse strings
at compile-time, as in the following example:

```C++
// consteval forces compile-time evaluation of the function in C++20.
consteval double parse(std::string_view input) {
  double result;
  auto answer = fast_float::from_chars(input.data(), input.data()+input.size(), result);
  if(answer.ec != std::errc()) { return -1.0; }
  return result;
}

// This function should compile to a function which
// merely returns 3.1415.
constexpr double constexptest() {
  return parse("3.1415 input");
}
```

## Non-ASCII Inputs

We also support UTF-16 and UTF-32 inputs, as well as ASCII/UTF-8, as in the following example:

``` C++
#include "fast_float/fast_float.h"
#include <iostream>

int main() {
    const std::u16string input =  u"3.1416 xyz ";
    double result;
    auto answer = fast_float::from_chars(input.data(), input.data()+input.size(), result);
    if(answer.ec != std::errc()) { std::cerr << "parsing failure\n"; return EXIT_FAILURE; }
    std::cout << "parsed the number " << result << std::endl;
    return EXIT_SUCCESS;
}
```

## Using commas as decimal separator


The C++ standard stipulate that `from_chars` has to be locale-independent. In
particular, the decimal separator has to be the period (`.`). However,
some users still want to use the `fast_float` library with in a locale-dependent
manner. Using a separate function called `from_chars_advanced`, we allow the users
to pass a `parse_options` instance which contains a custom decimal separator (e.g.,
the comma). You may use it as follows.

```C++
#include "fast_float/fast_float.h"
#include <iostream>

int main() {
    const std::string input =  "3,1416 xyz ";
    double result;
    fast_float::parse_options options{fast_float::chars_format::general, ','};
    auto answer = fast_float::from_chars_advanced(input.data(), input.data()+input.size(), result, options);
    if((answer.ec != std::errc()) || ((result != 3.1416))) { std::cerr << "parsing failure\n"; return EXIT_FAILURE; }
    std::cout << "parsed the number " << result << std::endl;
    return EXIT_SUCCESS;
}
```

You can parse delimited numbers:
```C++
  const std::string input =   "234532.3426362,7869234.9823,324562.645";
  double result;
  auto answer = fast_float::from_chars(input.data(), input.data()+input.size(), result);
  if(answer.ec != std::errc()) {
    // check error
  }
  // we have result == 234532.3426362.
  if(answer.ptr[0] != ',') {
    // unexpected delimiter
  }
  answer = fast_float::from_chars(answer.ptr + 1, input.data()+input.size(), result);
  if(answer.ec != std::errc()) {
    // check error
  }
  // we have result == 7869234.9823.
  if(answer.ptr[0] != ',') {
    // unexpected delimiter
  }
  answer = fast_float::from_chars(answer.ptr + 1, input.data()+input.size(), result);
  if(answer.ec != std::errc()) {
    // check error
  }
  // we have result == 324562.645.
```


## Relation With Other Work

The fast_float library is part of:

- GCC (as of version 12): the `from_chars` function in GCC relies on fast_float.
- [WebKit](https://github.com/WebKit/WebKit), the engine behind Safari (Apple's web browser)


The fastfloat algorithm is part of the [LLVM standard libraries](https://github.com/llvm/llvm-project/commit/87c016078ad72c46505461e4ff8bfa04819fe7ba).

There is a [derived implementation part of AdaCore](https://github.com/AdaCore/VSS).


The fast_float library provides a performance similar to that of the [fast_double_parser](https://github.com/lemire/fast_double_parser) library but using an updated algorithm reworked from the ground up, and while offering an API more in line with the expectations of C++ programmers. The fast_double_parser library is part of the [Microsoft LightGBM machine-learning framework](https://github.com/microsoft/LightGBM).

## References

- Daniel Lemire, [Number Parsing at a Gigabyte per Second](https://arxiv.org/abs/2101.11408), Software: Practice and Experience 51 (8), 2021.
- Noble Mushtak, Daniel Lemire, [Fast Number Parsing Without Fallback](https://arxiv.org/abs/2212.06644), Software: Practice and Experience (to appear)

## Other programming languages

- [There is an R binding](https://github.com/eddelbuettel/rcppfastfloat) called `rcppfastfloat`.
- [There is a Rust port of the fast_float library](https://github.com/aldanor/fast-float-rust/) called `fast-float-rust`.
- [There is a Java port of the fast_float library](https://github.com/wrandelshofer/FastDoubleParser) called `FastDoubleParser`. It used for important systems such as [Jackson](https://github.com/FasterXML/jackson-core).
- [There is a C# port of the fast_float library](https://github.com/CarlVerret/csFastFloat) called `csFastFloat`.


## Users

The fast_float library is used by [Apache Arrow](https://github.com/apache/arrow/pull/8494) where it multiplied the number parsing speed by two or three times. It is also used by [Yandex ClickHouse](https://github.com/ClickHouse/ClickHouse) and by [Google Jsonnet](https://github.com/google/jsonnet).


## How fast is it?

It can parse random floating-point numbers at a speed of 1 GB/s on some systems. We find that it is often twice as fast as the best available competitor, and many times faster than many standard-library implementations.

<img src="http://lemire.me/blog/wp-content/uploads/2020/11/fastfloat_speed.png" width="400">

```
$ ./build/benchmarks/benchmark
# parsing random integers in the range [0,1)
volume = 2.09808 MB
netlib                                  :   271.18 MB/s (+/- 1.2 %)    12.93 Mfloat/s
doubleconversion                        :   225.35 MB/s (+/- 1.2 %)    10.74 Mfloat/s
strtod                                  :   190.94 MB/s (+/- 1.6 %)     9.10 Mfloat/s
abseil                                  :   430.45 MB/s (+/- 2.2 %)    20.52 Mfloat/s
fastfloat                               :  1042.38 MB/s (+/- 9.9 %)    49.68 Mfloat/s
```

See https://github.com/lemire/simple_fastfloat_benchmark for our benchmarking code.


## Video

[![Go Systems 2020](http://img.youtube.com/vi/AVXgvlMeIm4/0.jpg)](http://www.youtube.com/watch?v=AVXgvlMeIm4)<br />

## Using as a CMake dependency

This library is header-only by design. The CMake file provides the `fast_float` target
which is merely a pointer to the `include` directory.

If you drop the `fast_float` repository in your CMake project, you should be able to use
it in this manner:

```cmake
add_subdirectory(fast_float)
target_link_libraries(myprogram PUBLIC fast_float)
```

Or you may want to retrieve the dependency automatically if you have a sufficiently recent version of CMake (3.11 or better at least):

```cmake
FetchContent_Declare(
  fast_float
  GIT_REPOSITORY https://github.com/lemire/fast_float.git
  GIT_TAG tags/v1.1.2
  GIT_SHALLOW TRUE)

FetchContent_MakeAvailable(fast_float)
target_link_libraries(myprogram PUBLIC fast_float)

```

You should change the `GIT_TAG` line so that you recover the version you wish to use.

## Using as single header

The script `script/amalgamate.py` may be used to generate a single header
version of the library if so desired.
Just run the script from the root directory of this repository.
You can customize the license type and output file if desired as described in
the command line help.

You may directly download automatically generated single-header files:

https://github.com/fastfloat/fast_float/releases/download/v3.4.0/fast_float.h

## Credit

Though this work is inspired by many different people, this work benefited especially from exchanges with
Michael Eisel, who motivated the original research with his key insights, and with Nigel Tao who provided
invaluable feedback. Rmy Oudompheng first implemented a fast path we use in the case of long digits.

The library includes code adapted from Google Wuffs (written by Nigel Tao) which was originally published
under the Apache 2.0 license.

## License

<sup>
Licensed under either of <a href="LICENSE-APACHE">Apache License, Version
2.0</a> or <a href="LICENSE-MIT">MIT license</a> or <a href="LICENSE-BOOST">BOOST license</a> .
</sup>

<br>

<sub>
Unless you explicitly state otherwise, any contribution intentionally submitted
for inclusion in this repository by you, as defined in the Apache-2.0 license,
shall be triple licensed as above, without any additional terms or conditions.
</sub>


## Links discovered
- [![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/fast_float.svg)
- [![VS17-CI](https://github.com/fastfloat/fast_float/actions/workflows/vs17-ci.yml/badge.svg)
- [![Ubuntu 22.04 CI (GCC 11)](https://github.com/fastfloat/fast_float/actions/workflows/ubuntu22.yml/badge.svg)
- [first,last) for a number. It parses floating-point numbers expecting
a locale-independent format equivalent to the C++17 from_chars function.
The resulting floating-point value is the closest floating-point values (using either float or double),
using the "round to even" convention for values that would otherwise fall right in-between two values.
That is, we provide exact parsing according to the IEEE standard.


Given a successful parse, the pointer (`ptr`) in the returned value is set to point right after the
parsed number, and the `value` referenced is set to the parsed value. In case of error, the returned
`ec` contains a representative error, otherwise the default (`std::errc()`) value is stored.

The implementation does not throw and does not allocate memory (e.g., with `new` or `malloc`).

It will parse infinity and nan values.

Example:

``` C++
#include "fast_float/fast_float.h"
#include <iostream>

int main() {
    const std::string input =  "3.1416 xyz ";
    double result;
    auto answer = fast_float::from_chars(input.data(), input.data()+input.size(), result);
    if(answer.ec != std::errc()) { std::cerr << "parsing failure\n"; return EXIT_FAILURE; }
    std::cout << "parsed the number " << result << std::endl;
    return EXIT_SUCCESS;
}
```


Like the C++17 standard, the `fast_float::from_chars` functions take an optional last argument of
the type `fast_float::chars_format`. It is a bitset value: we check whether
`fmt & fast_float::chars_format::fixed` and `fmt & fast_float::chars_format::scientific` are set
to determine whether we allow the fixed point and scientific notation respectively.
The default is  `fast_float::chars_format::general` which allows both `fixed` and `scientific`.

The library seeks to follow the C++17 (see [20.19.3](http://eel.is/c++draft/charconv.from.chars)
- [A leading `+` sign](https://en.cppreference.com/w/cpp/utility/from_chars)
- [WebKit](https://github.com/WebKit/WebKit)
- [LLVM standard libraries](https://github.com/llvm/llvm-project/commit/87c016078ad72c46505461e4ff8bfa04819fe7ba)
- [derived implementation part of AdaCore](https://github.com/AdaCore/VSS)
- [fast_double_parser](https://github.com/lemire/fast_double_parser)
- [Microsoft LightGBM machine-learning framework](https://github.com/microsoft/LightGBM)
- [Number Parsing at a Gigabyte per Second](https://arxiv.org/abs/2101.11408)
- [Fast Number Parsing Without Fallback](https://arxiv.org/abs/2212.06644)
- [There is an R binding](https://github.com/eddelbuettel/rcppfastfloat)
- [There is a Rust port of the fast_float library](https://github.com/aldanor/fast-float-rust/)
- [There is a Java port of the fast_float library](https://github.com/wrandelshofer/FastDoubleParser)
- [Jackson](https://github.com/FasterXML/jackson-core)
- [There is a C# port of the fast_float library](https://github.com/CarlVerret/csFastFloat)
- [Apache Arrow](https://github.com/apache/arrow/pull/8494)
- [Yandex ClickHouse](https://github.com/ClickHouse/ClickHouse)
- [Google Jsonnet](https://github.com/google/jsonnet)
- [0,1)
volume = 2.09808 MB
netlib                                  :   271.18 MB/s (+/- 1.2 %)    12.93 Mfloat/s
doubleconversion                        :   225.35 MB/s (+/- 1.2 %)    10.74 Mfloat/s
strtod                                  :   190.94 MB/s (+/- 1.6 %)     9.10 Mfloat/s
abseil                                  :   430.45 MB/s (+/- 2.2 %)    20.52 Mfloat/s
fastfloat                               :  1042.38 MB/s (+/- 9.9 %)    49.68 Mfloat/s
```

See https://github.com/lemire/simple_fastfloat_benchmark for our benchmarking code.


## Video

[![Go Systems 2020](http://img.youtube.com/vi/AVXgvlMeIm4/0.jpg)
- [Apache License, Version 2.0](https://github.com/blender/blender/blob/main/extern/fast_float/LICENSE-APACHE.md)
- [MIT license](https://github.com/blender/blender/blob/main/extern/fast_float/LICENSE-MIT.md)
- [BOOST license](https://github.com/blender/blender/blob/main/extern/fast_float/LICENSE-BOOST.md)

--- extern/gflags/README.md ---
24 March 2015
-------------

I've just released gflags 2.1.2.

This release completes the namespace change fixes. In particular,
it restores binary ABI compatibility with release version 2.0.
The deprecated "google" namespace is by default still kept as
primary namespace while symbols are imported into the new "gflags" namespace.
This can be overridden using the CMake variable GFLAGS_NAMESPACE.

Other fixes of the build configuration are related to the (patched)
CMake modules FindThreads.cmake and CheckTypeSize.cmake. These have
been removed and instead the C language is enabled again even though
gflags is written in C++ only.

This release also marks the complete move of the gflags project
from Google Code to GitHub. Email addresses of original issue
reporters got lost in the process. Given the age of most issue reports,
this should be negligable.

Please report any further issues using the GitHub issue tracker.


30 March 2014
-------------

I've just released gflags 2.1.1.

This release fixes a few bugs in the configuration of gflags\_declare.h
and adds a separate GFLAGS\_INCLUDE\_DIR CMake variable to the build configuration.
Setting GFLAGS\_NAMESPACE to "google" no longer changes also the include
path of the public header files. This allows the use of the library with
other Google projects such as glog which still use the deprecated "google"
namespace for the gflags library, but include it as "gflags/gflags.h".

20 March 2014
-------------

I've just released gflags 2.1.

The major changes are the use of CMake for the build configuration instead
of the autotools and packaging support through CPack. The default namespace
of all C++ symbols is now "gflags" instead of "google". This can be
configured via the GFLAGS\_NAMESPACE variable.

This release compiles with all major compilers without warnings and passed
the unit tests on  Ubuntu 12.04, Windows 7 (Visual Studio 2008 and 2010,
Cygwin, MinGW), and Mac OS X (Xcode 5.1).

The SVN repository on Google Code is now frozen and replaced by a Git
repository such that it can be used as Git submodule by projects. The main
hosting of this project remains at Google Code. Thanks to the distributed
character of Git, I can push (and pull) changes from both GitHub and Google Code
in order to keep the two public repositories in sync.
When fixing an issue for a pull request through either of these hosting
platforms, please reference the issue number as
[described here](https://code.google.com/p/support/wiki/IssueTracker#Integration_with_version_control).
For the further development, I am following the
[Git branching model](http://nvie.com/posts/a-successful-git-branching-model/)
with feature branch names prefixed by "feature/" and bugfix branch names
prefixed by "bugfix/", respectively.

Binary and source [packages](https://github.com/schuhschuh/gflags/releases) are available on GitHub.


14 January 2014
---------------

The migration of the build system to CMake is almost complete.
What remains to be done is rewriting the tests in Python such they can be
executed on non-Unix platforms and splitting them up into separate CTest tests.
Though merging these changes into the master branch yet remains to be done,
it is recommended to already start using the
[cmake-migration](https://github.com/schuhschuh/gflags/tree/cmake-migration) branch.


20 April 2013
-------------

More than a year has past since I (Andreas) took over the maintenance for
`gflags`. Only few minor changes have been made since then, much to my regret.
To get more involved and stimulate participation in the further
development of the library, I moved the project source code today to
[GitHub](https://github.com/schuhschuh/gflags).
I believe that the strengths of [Git](http://git-scm.com/) will allow for better community collaboration
as well as ease the integration of changes made by others. I encourage everyone
who would like to contribute to send me pull requests.
Git's lightweight feature branches will also provide the right tool for more
radical changes which should only be merged back into the master branch
after these are complete and implement the desired behavior.

The SVN repository remains accessible at Google Code and I will keep the
master branch of the Git repository hosted at GitHub and the trunk of the
Subversion repository synchronized. Initially, I was going to simply switch the
Google Code project to Git, but in this case the SVN repository would be
frozen and force everyone who would like the latest development changes to
use Git as well. Therefore I decided to host the public Git repository at GitHub
instead.

Please continue to report any issues with gflags on Google Code. The GitHub project will
only be used to host the Git repository.

One major change of the project structure I have in mind for the next weeks
is the migration from autotools to [CMake](http://www.cmake.org/).
Check out the (unstable!)
[cmake-migration](https://github.com/schuhschuh/gflags/tree/cmake-migration)
branch on GitHub for details.


25 January 2012
---------------

I've just released gflags 2.0.

The `google-gflags` project has been renamed to `gflags`.  I
(csilvers) am stepping down as maintainer, to be replaced by Andreas
Schuh.  Welcome to the team, Andreas!  I've seen the energy you have
around gflags and the ideas you have for the project going forward,
and look forward to having you on the team.

I bumped the major version number up to 2 to reflect the new community
ownership of the project.  All the [changes](ChangeLog.txt)
are related to the renaming.  There are no functional changes from
gflags 1.7.  In particular, I've kept the code in the namespace
`google`, though in a future version it should be renamed to `gflags`.
I've also kept the `/usr/local/include/google/` subdirectory as
synonym of `/usr/local/include/gflags/`, though the former name has
been obsolete for some time now.


18 January 2011
---------------

The `google-gflags` Google Code page has been renamed to
`gflags`, in preparation for the project being renamed to
`gflags`.  In the coming weeks, I'll be stepping down as
maintainer for the gflags project, and as part of that Google is
relinquishing ownership of the project; it will now be entirely
community run.  The name change reflects that shift.


20 December 2011
----------------

I've just released gflags 1.7.  This is a minor release; the major
change is that `CommandLineFlagInfo` now exports the address in memory
where the flag is located.  There has also been a bugfix involving
very long --help strings, and some other minor [changes](ChangeLog.txt).

29 July 2011
------------

I've just released gflags 1.6.  The major new feature in this release
is support for setting version info, so that --version does something
useful.

One minor change has required bumping the library number:
`ReparseCommandlineFlags` now returns `void` instead of `int` (the int
return value was always meaningless).  Though I doubt anyone ever used
this (meaningless) return value, technically it's a change to the ABI
that requires a version bump.  A bit sad.

There's also a procedural change with this release: I've changed the
internal tools used to integrate Google-supplied patches for gflags
into the opensource release.  These new tools should result in more
frequent updates with better change descriptions.  They will also
result in future `ChangeLog` entries being much more verbose (for better
or for worse).

See the [ChangeLog](ChangeLog.txt) for a full list of changes for this release.

24 January 2011
---------------

I've just released gflags 1.5.  This release has only minor changes
from 1.4, including some slightly better reporting in --help, and
an new memory-cleanup function that can help when running gflags-using
libraries under valgrind.  The major change is to fix up the macros
(`DEFINE_bool` and the like) to work more reliably inside namespaces.

If you have not had a problem with these macros, and don't need any of
the other changes described, there is no need to upgrade.  See the
[ChangeLog](ChangeLog.txt) for a full list of changes for this release.

11 October 2010
---------------

I've just released gflags 1.4.  This release has only minor changes
from 1.3, including some documentation tweaks and some work to make
the library smaller.  If 1.3 is working well for you, there's no
particular reason to upgrade.

4 January 2010
--------------

I've just released gflags 1.3.  gflags now compiles under MSVC, and
all tests pass.  I **really** never thought non-unix-y Windows folks
would want gflags, but at least some of them do.

The major news, though, is that I've separated out the python package
into its own library, [python-gflags](http://code.google.com/p/python-gflags).
If you're interested in the Python version of gflags, that's the place to
get it now.

10 September 2009
-----------------

I've just released gflags 1.2.  The major change from gflags 1.1 is it
now compiles under MinGW (as well as cygwin), and all tests pass.  I
never thought Windows folks would want unix-style command-line flags,
since they're so different from the Windows style, but I guess I was
wrong!

The other changes are minor, such as support for --htmlxml in the
python version of gflags.

15 April 2009
-------------

I've just released gflags 1.1.  It has only minor changes fdrom gflags
1.0 (see the [ChangeLog](ChangeLog.txt) for details).
The major change is that I moved to a new system for creating .deb and .rpm files.
This allows me to create x86\_64 deb and rpm files.

In the process of moving to this new system, I noticed an
inconsistency: the tar.gz and .rpm files created libraries named
libgflags.so, but the deb file created libgoogle-gflags.so.  I have
fixed the deb file to create libraries like the others.  I'm no expert
in debian packaging, but I believe this has caused the package name to
change as well.  Please let me know (at
[[mailto:google-gflags@googlegroups.com](mailto:google-gflags@googlegroups.com)
google-gflags@googlegroups.com]) if this causes problems for you --
especially if you know of a fix!  I would be happy to change the deb
packages to add symlinks from the old library name to the new
(libgoogle-gflags.so -> libgflags.so), but that is beyond my knowledge
of how to make .debs.

If you've tried to install a .rpm or .deb and it doesn't work for you,
let me know.  I'm excited to finally have 64-bit package files, but
there may still be some wrinkles in the new system to iron out.

1 October 2008
--------------

gflags 1.0rc2 was out for a few weeks without any issues, so gflags
1.0 is now released.  This is much like gflags 0.9.  The major change
is that the .h files have been moved from `/usr/include/google` to
`/usr/include/gflags`.  While I have backwards-compatibility
forwarding headeds in place, please rewrite existing code to say
```
   #include <gflags/gflags.h>
```
instead of
```
   #include <google/gflags.h>
```

I've kept the default namespace to google.  You can still change with
with the appropriate flag to the configure script (`./configure
--help` to see the flags).  If you have feedback as to whether the
default namespace should change to gflags, which would be a
non-backwards-compatible change, send mail to
`google-gflags@googlegroups.com`!

Version 1.0 also has some neat new features, like support for bash
commandline-completion of help flags.  See the [ChangeLog](ChangeLog.txt)
for more details.

If I don't hear any bad news for a few weeks, I'll release 1.0-final.


## Links discovered
- [described here](https://code.google.com/p/support/wiki/IssueTracker#Integration_with_version_control)
- [Git branching model](http://nvie.com/posts/a-successful-git-branching-model/)
- [packages](https://github.com/schuhschuh/gflags/releases)
- [cmake-migration](https://github.com/schuhschuh/gflags/tree/cmake-migration)
- [GitHub](https://github.com/schuhschuh/gflags)
- [Git](http://git-scm.com/)
- [CMake](http://www.cmake.org/)
- [changes](https://github.com/blender/blender/blob/main/extern/gflags/ChangeLog.txt)
- [ChangeLog](https://github.com/blender/blender/blob/main/extern/gflags/ChangeLog.txt)
- [python-gflags](http://code.google.com/p/python-gflags)

--- extern/gmock/README.md ---
# Googletest Mocking (gMock) Framework

### Overview

Google's framework for writing and using C++ mock classes. It can help you
derive better designs of your system and write better tests.

It is inspired by:

*   [jMock](http://www.jmock.org/),
*   [EasyMock](http://www.easymock.org/), and
*   [Hamcrest](http://code.google.com/p/hamcrest/),

and designed with C++'s specifics in mind.

gMock:

-   provides a declarative syntax for defining mocks,
-   can define partial (hybrid) mocks, which are a cross of real and mock
    objects,
-   handles functions of arbitrary types and overloaded functions,
-   comes with a rich set of matchers for validating function arguments,
-   uses an intuitive syntax for controlling the behavior of a mock,
-   does automatic verification of expectations (no record-and-replay needed),
-   allows arbitrary (partial) ordering constraints on function calls to be
    expressed,
-   lets a user extend it by defining new matchers and actions.
-   does not use exceptions, and
-   is easy to learn and use.

Details and examples can be found here:

*   [gMock for Dummies](docs/for_dummies.md)
*   [Legacy gMock FAQ](docs/gmock_faq.md)
*   [gMock Cookbook](docs/cook_book.md)
*   [gMock Cheat Sheet](docs/cheat_sheet.md)

Please note that code under scripts/generator/ is from the [cppclean
project](http://code.google.com/p/cppclean/) and under the Apache
License, which is different from Google Mock's license.

Google Mock is a part of
[Google Test C++ testing framework](http://github.com/google/googletest/) and a
subject to the same requirements.


## Links discovered
- [jMock](http://www.jmock.org/)
- [EasyMock](http://www.easymock.org/)
- [Hamcrest](http://code.google.com/p/hamcrest/)
- [gMock for Dummies](https://github.com/blender/blender/blob/main/extern/gmock/docs/for_dummies.md)
- [Legacy gMock FAQ](https://github.com/blender/blender/blob/main/extern/gmock/docs/gmock_faq.md)
- [gMock Cookbook](https://github.com/blender/blender/blob/main/extern/gmock/docs/cook_book.md)
- [gMock Cheat Sheet](https://github.com/blender/blender/blob/main/extern/gmock/docs/cheat_sheet.md)
- [cppclean
project](http://code.google.com/p/cppclean/)
- [Google Test C++ testing framework](http://github.com/google/googletest/)

--- extern/gtest/README.md ---

# Google Test #

[![Build Status](https://travis-ci.org/google/googletest.svg?branch=master)](https://travis-ci.org/google/googletest)
[![Build status](https://ci.appveyor.com/api/projects/status/4o38plt0xbo1ubc8/branch/master?svg=true)](https://ci.appveyor.com/project/GoogleTestAppVeyor/googletest/branch/master)

**Future Plans**:
* 1.8.x Release - the 1.8.x will be the last release that works with pre-C++11 compilers. The 1.8.x will not accept any requests for any new features and any bugfix requests will only be accepted if proven "critical"
* Post 1.8.x - work to improve/cleanup/pay technical debt. When this work is completed there will be a 1.9.x tagged release
* Post 1.9.x googletest will follow [Abseil Live at Head philosophy](https://abseil.io/about/philosophy)


Welcome to **Google Test**, Google's C++ test framework!

This repository is a merger of the formerly separate GoogleTest and
GoogleMock projects. These were so closely related that it makes sense to
maintain and release them together.

Please see the project page above for more information as well as the
mailing list for questions, discussions, and development.  There is
also an IRC channel on [OFTC](https://webchat.oftc.net/) (irc.oftc.net) #gtest available.  Please
join us!

Getting started information for **Google Test** is available in the
[Google Test Primer](googletest/docs/primer.md) documentation.

**Google Mock** is an extension to Google Test for writing and using C++ mock
classes.  See the separate [Google Mock documentation](googlemock/README.md).

More detailed documentation for googletest (including build instructions) are
in its interior [googletest/README.md](googletest/README.md) file.

## Features ##

  * An [xUnit](https://en.wikipedia.org/wiki/XUnit) test framework.
  * Test discovery.
  * A rich set of assertions.
  * User-defined assertions.
  * Death tests.
  * Fatal and non-fatal failures.
  * Value-parameterized tests.
  * Type-parameterized tests.
  * Various options for running the tests.
  * XML test report generation.

## Platforms ##

Google test has been used on a variety of platforms:

  * Linux
  * Mac OS X
  * Windows
  * Cygwin
  * MinGW
  * Windows Mobile
  * Symbian

## Who Is Using Google Test? ##

In addition to many internal projects at Google, Google Test is also used by
the following notable projects:

  * The [Chromium projects](http://www.chromium.org/) (behind the Chrome
    browser and Chrome OS).
  * The [LLVM](http://llvm.org/) compiler.
  * [Protocol Buffers](https://github.com/google/protobuf), Google's data
    interchange format.
  * The [OpenCV](http://opencv.org/) computer vision library.
  * [tiny-dnn](https://github.com/tiny-dnn/tiny-dnn): header only, dependency-free deep learning framework in C++11.

## Related Open Source Projects ##

[GTest Runner](https://github.com/nholthaus/gtest-runner) is a Qt5 based automated test-runner and Graphical User Interface with powerful features for Windows and Linux platforms.

[Google Test UI](https://github.com/ospector/gtest-gbar) is test runner that runs
your test binary, allows you to track its progress via a progress bar, and
displays a list of test failures. Clicking on one shows failure text. Google
Test UI is written in C#.

[GTest TAP Listener](https://github.com/kinow/gtest-tap-listener) is an event
listener for Google Test that implements the
[TAP protocol](https://en.wikipedia.org/wiki/Test_Anything_Protocol) for test
result output. If your test runner understands TAP, you may find it useful.

[gtest-parallel](https://github.com/google/gtest-parallel) is a test runner that
runs tests from your binary in parallel to provide significant speed-up.

[GoogleTest Adapter](https://marketplace.visualstudio.com/items?itemName=DavidSchuldenfrei.gtest-adapter) is a VS Code extension allowing to view Google Tests in a tree view, and run/debug your tests.

## Requirements ##

Google Test is designed to have fairly minimal requirements to build
and use with your projects, but there are some.  Currently, we support
Linux, Windows, Mac OS X, and Cygwin.  We will also make our best
effort to support other platforms (e.g. Solaris, AIX, and z/OS).
However, since core members of the Google Test project have no access
to these platforms, Google Test may have outstanding issues there.  If
you notice any problems on your platform, please notify
[googletestframework@googlegroups.com](https://groups.google.com/forum/#!forum/googletestframework). Patches for fixing them are
even more welcome!

### Linux Requirements ###

These are the base requirements to build and use Google Test from a source
package (as described below):

  * GNU-compatible Make or gmake
  * POSIX-standard shell
  * POSIX(-2) Regular Expressions (regex.h)
  * A C++98-standard-compliant compiler

### Windows Requirements ###

  * Microsoft Visual C++ 2015 or newer

### Cygwin Requirements ###

  * Cygwin v1.5.25-14 or newer

### Mac OS X Requirements ###

  * Mac OS X v10.4 Tiger or newer
  * Xcode Developer Tools

## Contributing change

Please read the [`CONTRIBUTING.md`](CONTRIBUTING.md) for details on
how to contribute to this project.

Happy testing!


## Links discovered
- [![Build Status](https://travis-ci.org/google/googletest.svg?branch=master)
- [![Build status](https://ci.appveyor.com/api/projects/status/4o38plt0xbo1ubc8/branch/master?svg=true)
- [Abseil Live at Head philosophy](https://abseil.io/about/philosophy)
- [OFTC](https://webchat.oftc.net/)
- [Google Test Primer](https://github.com/blender/blender/blob/main/extern/gtest/googletest/docs/primer.md)
- [Google Mock documentation](https://github.com/blender/blender/blob/main/extern/gtest/googlemock/README.md)
- [googletest/README.md](https://github.com/blender/blender/blob/main/extern/gtest/googletest/README.md)
- [xUnit](https://en.wikipedia.org/wiki/XUnit)
- [Chromium projects](http://www.chromium.org/)
- [LLVM](http://llvm.org/)
- [Protocol Buffers](https://github.com/google/protobuf)
- [OpenCV](http://opencv.org/)
- [tiny-dnn](https://github.com/tiny-dnn/tiny-dnn)
- [GTest Runner](https://github.com/nholthaus/gtest-runner)
- [Google Test UI](https://github.com/ospector/gtest-gbar)
- [GTest TAP Listener](https://github.com/kinow/gtest-tap-listener)
- [TAP protocol](https://en.wikipedia.org/wiki/Test_Anything_Protocol)
- [gtest-parallel](https://github.com/google/gtest-parallel)
- [GoogleTest Adapter](https://marketplace.visualstudio.com/items?itemName=DavidSchuldenfrei.gtest-adapter)
- [googletestframework@googlegroups.com](https://groups.google.com/forum/#!forum/googletestframework)
- [`CONTRIBUTING.md`](https://github.com/blender/blender/blob/main/extern/gtest/CONTRIBUTING.md)

--- extern/mantaflow/README.md ---
# Mantaflow #

Mantaflow is an open-source framework targeted at fluid simulation research in Computer Graphics.
Its parallelized C++ solver core, python scene definition interface and plugin system allow for quickly prototyping and testing new algorithms.

In addition, it provides a toolbox of examples for deep learning experiments with fluids. E.g., it contains examples
how to build convolutional neural network setups in conjunction with the [tensorflow framework](https://www.tensorflow.org).

For more information on how to install, run and code with Mantaflow, please head over to our home page at
[http://mantaflow.com](http://mantaflow.com)

## Debugging ##

You could export openVDB volume into mantaflow, by running Blender with:

    blender --debug-value 3001

And then select `Domain` -> `Fluid` -> `Cache` - > `Advanced` -> `Export Mantaflow Script`.


## Links discovered
- [tensorflow framework](https://www.tensorflow.org)
- [http://mantaflow.com](http://mantaflow.com)

--- extern/quadriflow/README.md ---
# QuadriFlow: A Scalable and Robust Method for Quadrangulation

Source code for the paper:

Jingwei Huang, Yichao Zhou, Matthias Niessner, Jonathan Shewchuk and Leonidas Guibas. [**QuadriFlow: A Scalable and Robust Method for Quadrangulation**](http://stanford.edu/~jingweih/papers/quadriflow/quadriflow.pdf), The Eurographics Symposium on Geometry Processing (SGP) 2018.

<!-- ## Processing Result -->
![QuadriFlow Results](https://github.com/hjwdzh/quadriflow/raw/master/img/result.jpg)

## WebGL Application
Our 3D WebGL Apps for QuadriFlow are online!  Without any installation, you are able to
*  [**Compare**](https://yichaozhou.com/publication/1805quadriflow/#demo) QuadriFlow with previous methods;
*  [**Quadrangulate**](https://yichaozhou.com/publication/1805quadriflow/#tool) your own meshes and
    download the result!

## Desktop Software
The software supports cmake build for Linux/Mac/Windows systems. For linux and mac users, run **`sh demo.sh`** to build and try the QuadriFlow example, which converts `examples/Gargoyle_input.obj` to `examples/Gargoyle_quadriflow.obj`.

### Install

```
git clone git://github.com/hjwdzh/quadriflow
cd quadriflow
mkdir build
cd build
cmake .. -DCMAKE_BUILD_TYPE=release
make -j
```

### QuadriFlow Software

We take a manifold triangle mesh `input.obj` and generate a manifold quad mesh `output.obj`. The face number increases linearly with the resolution controled by the user.

```
./quadriflow -i input.obj -o output.obj -f [resolution]
```

Here, the resolution is the desired number of faces in the quad mesh.

## Advanced Functions

### Min-cost Flow
By default, `quadriflow` uses the Boykov maximum flow solver from boost becuase it is faster.  To
enable the adaptive network simplex minimum-cost flow solver, you can enable the `-mcf` option:

```
./quadriflow -mcf -i input.obj -o output.obj -f [resolution]
```

### Sharp Preserving
By default, `quadriflow` does not explicitly detect and perserve the sharp edges in the model. To
enable this feature, uses

```
./quadriflow -sharp -i input.obj -o output.obj -f [resolution]
```

### SAT Flip Removal (Unix Only)
By default, `quadriflow` does not use the SAT solver to remove the flips in the integer offsets
map.  To remove the flips and guarantee a watertight result mesh, you can enable the SAT solver.
First, make sure that `minisat` and `timeout` is properly installed under your `${PATH}`.  The
former can be done by building `3rd/MapleCOMSPS_LRB/CMakeLists.txt` and copying `minisat` to `/usr/bin`.
In addition, `timeout` is included in coreutils. If you are using Mac, you can install it using
homebrew:
```
brew install coreutils
export PATH="/usr/local/opt/coreutils/libexec/gnubin:$PATH"
```

You can verify if those binaries are properly installed by executing
```
which minisat
which timeout
```

After that, you can enable SAT flip removal procedure by executing
```
./quadriflow -sat -i input.obj -o output.obj -f [resolution]
```

When using the SAT flip removal, we also suggest you enabling the verbose logging to understand
what is going on. You can build quadriflow with the following options:
```
cmake .. -DCMAKE_BUILD_TYPE=release -DBUILD_LOG=ON
```

### GUROBI Support (For Benchmark Purpose)

To use the Gurobi integer programming to solve the integer offset problem, you can build QuadriFlow with
```
cmake .. -DCMAKE_BUILD_TYPE=release -DBUILD_GUROBI=ON -DBUILD_LOG=ON
```
This override other solvers and should only be used for benchmark purpose.

## External Dependencies
* Boost
* Eigen
* OpenMP (optional in CMake)
* TBB (optional in CMake)
* GUROBI (for benchmark purpose only)

## Licenses

QuadriFlow is released under [MIT License](LICENSE.txt).
For 3rd dependencies,
* Boost and Lemon are released under [Boost Software License](https://lemon.cs.elte.hu/trac/lemon/wiki/License)
* Most part of Eigen is released under [MPL2](https://www.mozilla.org/en-US/MPL/2.0/FAQ/)
    * Sparse Cholesky Decomposition algorithms are released under LGPL
    * To replace it using Sparse LU decomposition with a more permissive MPL2 license (a little slower), enable `BUILD_FREE_LICENSE` in CMake (e.g., `-DBUILD_FREE_LICENSE=ON`).
* `pcg32.h` is released under the Apache License, Version 2.0
* `parallel_stable_sort.h` is released under the MIT License

## Authors
- [Jingwei Huang](mailto:jingweih@stanford.edu)
- [Yichao Zhou](mailto:zyc@berkeley.edu)

&copy; 2018 Jingwei Huang and Yichao Zhou All Rights Reserved

**IMPORTANT**: If you use this software please cite the following in any resulting publication:
```
@article {10.1111:cgf.13498,
    journal = {Computer Graphics Forum},
    title = {{QuadriFlow: A Scalable and Robust Method for Quadrangulation}},
    author = {Huang, Jingwei and Zhou, Yichao and Niessner, Matthias and Shewchuk, Jonathan Richard and Guibas, Leonidas J.},
    year = {2018},
    publisher = {The Eurographics Association and John Wiley & Sons Ltd.},
    ISSN = {1467-8659},
    DOI = {10.1111/cgf.13498}
}
```

## Triangle Manifold

In case you are dealing with a triangle mesh that is not a manifold, we implemented the software that converts any triangle mesh to watertight manifold. Please visit https://github.com/hjwdzh/Manifold for details.


## Links discovered
- [**QuadriFlow: A Scalable and Robust Method for Quadrangulation**](http://stanford.edu/~jingweih/papers/quadriflow/quadriflow.pdf)
- [QuadriFlow Results](https://github.com/hjwdzh/quadriflow/raw/master/img/result.jpg)
- [**Compare**](https://yichaozhou.com/publication/1805quadriflow/#demo)
- [**Quadrangulate**](https://yichaozhou.com/publication/1805quadriflow/#tool)
- [MIT License](https://github.com/blender/blender/blob/main/extern/quadriflow/LICENSE.txt)
- [Boost Software License](https://lemon.cs.elte.hu/trac/lemon/wiki/License)
- [MPL2](https://www.mozilla.org/en-US/MPL/2.0/FAQ/)

--- extern/vulkan_memory_allocator/README.md ---
# Vulkan Memory Allocator

Easy to integrate Vulkan memory allocation library.

**Documentation:** Browse online: [Vulkan Memory Allocator](https://gpuopen-librariesandsdks.github.io/VulkanMemoryAllocator/html/) (generated from Doxygen-style comments in [include/vk_mem_alloc.h](include/vk_mem_alloc.h))

**License:** MIT. See [LICENSE.txt](LICENSE.txt)

**Changelog:** See [CHANGELOG.md](CHANGELOG.md)

**Product page:** [Vulkan Memory Allocator on GPUOpen](https://gpuopen.com/gaming-product/vulkan-memory-allocator/)

[![Average time to resolve an issue](http://isitmaintained.com/badge/resolution/GPUOpen-LibrariesAndSDKs/VulkanMemoryAllocator.svg)](http://isitmaintained.com/project/GPUOpen-LibrariesAndSDKs/VulkanMemoryAllocator "Average time to resolve an issue")

# Problem

Memory allocation and resource (buffer and image) creation in Vulkan is difficult (comparing to older graphics APIs, like D3D11 or OpenGL) for several reasons:

- It requires a lot of boilerplate code, just like everything else in Vulkan, because it is a low-level and high-performance API.
- There is additional level of indirection: `VkDeviceMemory` is allocated separately from creating `VkBuffer`/`VkImage` and they must be bound together.
- Driver must be queried for supported memory heaps and memory types. Different GPU vendors provide different types of it.
- It is recommended to allocate bigger chunks of memory and assign parts of them to particular resources, as there is a limit on maximum number of memory blocks that can be allocated.

# Features

This library can help game developers to manage memory allocations and resource creation by offering some higher-level functions:

1. Functions that help to choose correct and optimal memory type based on intended usage of the memory.
   - Required or preferred traits of the memory are expressed using higher-level description comparing to Vulkan flags.
2. Functions that allocate memory blocks, reserve and return parts of them (`VkDeviceMemory` + offset + size) to the user.
   - Library keeps track of allocated memory blocks, used and unused ranges inside them, finds best matching unused ranges for new allocations, respects all the rules of alignment and buffer/image granularity.
3. Functions that can create an image/buffer, allocate memory for it and bind them together - all in one call.

Additional features:

- Well-documented - description of all functions and structures provided, along with chapters that contain general description and example code.
- Thread-safety: Library is designed to be used in multithreaded code. Access to a single device memory block referred by different buffers and textures (binding, mapping) is synchronized internally. Memory mapping is reference-counted.
- Configuration: Fill optional members of `VmaAllocatorCreateInfo` structure to provide custom CPU memory allocator, pointers to Vulkan functions and other parameters.
- Customization and integration with custom engines: Predefine appropriate macros to provide your own implementation of all external facilities used by the library like assert, mutex, atomic.
- Support for memory mapping, reference-counted internally. Support for persistently mapped memory: Just allocate with appropriate flag and access the pointer to already mapped memory.
- Support for non-coherent memory. Functions that flush/invalidate memory. `nonCoherentAtomSize` is respected automatically.
- Support for resource aliasing (overlap).
- Support for sparse binding and sparse residency: Convenience functions that allocate or free multiple memory pages at once.
- Custom memory pools: Create a pool with desired parameters (e.g. fixed or limited maximum size) and allocate memory out of it.
- Linear allocator: Create a pool with linear algorithm and use it for much faster allocations and deallocations in free-at-once, stack, double stack, or ring buffer fashion.
- Support for Vulkan 1.0...1.4.
- Support for extensions (and equivalent functionality included in new Vulkan versions):
   - VK_KHR_dedicated_allocation: Just enable it and it will be used automatically by the library.
   - VK_KHR_bind_memory2.
   - VK_KHR_maintenance4.
   - VK_KHR_maintenance5, including `VkBufferUsageFlags2CreateInfoKHR`.
   - VK_EXT_memory_budget: Used internally if available to query for current usage and budget. If not available, it falls back to an estimation based on memory heap sizes.
   - VK_KHR_buffer_device_address: Flag `VK_MEMORY_ALLOCATE_DEVICE_ADDRESS_BIT_KHR` is automatically added to memory allocations where needed.
   - VK_EXT_memory_priority: Set `priority` of allocations or custom pools and it will be set automatically using this extension.
   - VK_AMD_device_coherent_memory.
   - VK_KHR_external_memory_win32.
- Defragmentation of GPU and CPU memory: Let the library move data around to free some memory blocks and make your allocations better compacted.
- Statistics: Obtain brief or detailed statistics about the amount of memory used, unused, number of allocated blocks, number of allocations etc. - globally, per memory heap, and per memory type.
- Debug annotations: Associate custom `void* pUserData` and debug `char* pName` with each allocation.
- JSON dump: Obtain a string in JSON format with detailed map of internal state, including list of allocations, their string names, and gaps between them.
- Convert this JSON dump into a picture to visualize your memory. See [tools/GpuMemDumpVis](tools/GpuMemDumpVis/README.md).
- Debugging incorrect memory usage: Enable initialization of all allocated memory with a bit pattern to detect usage of uninitialized or freed memory. Enable validation of a magic number after every allocation to detect out-of-bounds memory corruption.
- Support for interoperability with OpenGL.
- Virtual allocator: Interface for using core allocation algorithm to allocate any custom data, e.g. pieces of one large buffer.

# Prerequisites

- Self-contained C++ library in single header file. No external dependencies other than standard C and C++ library and of course Vulkan. Some features of C++14 used. STL containers, RTTI, or C++ exceptions are not used.
- Public interface in C, in same convention as Vulkan API. Implementation in C++.
- Error handling implemented by returning `VkResult` error codes - same way as in Vulkan.
- Interface documented using Doxygen-style comments.
- Platform-independent, but developed and tested on Windows using Visual Studio. Continuous integration setup for Windows and Linux. Used also on Android, MacOS, and other platforms.

# Example

Basic usage of this library is very simple. Advanced features are optional. After you created global `VmaAllocator` object, a complete code needed to create a buffer may look like this:

```cpp
VkBufferCreateInfo bufferInfo = { VK_STRUCTURE_TYPE_BUFFER_CREATE_INFO };
bufferInfo.size = 65536;
bufferInfo.usage = VK_BUFFER_USAGE_VERTEX_BUFFER_BIT | VK_BUFFER_USAGE_TRANSFER_DST_BIT;

VmaAllocationCreateInfo allocInfo = {};
allocInfo.usage = VMA_MEMORY_USAGE_AUTO;

VkBuffer buffer;
VmaAllocation allocation;
vmaCreateBuffer(allocator, &bufferInfo, &allocInfo, &buffer, &allocation, nullptr);
```

With this one function call:

1. `VkBuffer` is created.
2. `VkDeviceMemory` block is allocated if needed.
3. An unused region of the memory block is bound to this buffer.

`VmaAllocation` is an object that represents memory assigned to this buffer. It can be queried for parameters like `VkDeviceMemory` handle and offset.

# How to build

On Windows it is recommended to use [CMake GUI](https://cmake.org/runningcmake/).

Alternatively you can generate/open a Visual Studio from the command line:

```sh
# By default CMake picks the newest version of Visual Studio it can use
cmake -S .  -B build -D VMA_BUILD_SAMPLES=ON
cmake --open build
```

On Linux:

```sh
cmake -S . -B build
# Since VMA has no source files, you can skip to installation immediately
cmake --install build --prefix build/install
```

## How to use

After calling either `find_package` or `add_subdirectory` simply link the library.
This automatically handles configuring the include directory. Example:

```cmake
find_package(VulkanMemoryAllocator CONFIG REQUIRED)
target_link_libraries(YourGameEngine PRIVATE GPUOpen::VulkanMemoryAllocator)
```

For more info on using CMake visit the official [CMake documentation](https://cmake.org/cmake/help/latest/index.html).

## Building using vcpkg

You can download and install VulkanMemoryAllocator using the [vcpkg](https://github.com/Microsoft/vcpkg) dependency manager:

    git clone https://github.com/Microsoft/vcpkg.git
    cd vcpkg
    ./bootstrap-vcpkg.sh
    ./vcpkg integrate install
    ./vcpkg install vulkan-memory-allocator

The VulkanMemoryAllocator port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please [create an issue or pull request](https://github.com/Microsoft/vcpkg) on the vcpkg repository.

# Binaries

The release comes with precompiled binary executable for "VulkanSample" application which contains test suite. It is compiled using Visual Studio 2022, so it requires appropriate libraries to work, including "MSVCP140.dll", "VCRUNTIME140.dll", "VCRUNTIME140_1.dll". If the launch fails with error message telling about those files missing, please download and install [Microsoft Visual C++ Redistributable](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads), "X64" version.

# Read more

See **[Documentation](https://gpuopen-librariesandsdks.github.io/VulkanMemoryAllocator/html/)**.

# Software using this library

- **[Blender](https://www.blender.org)**
- **[Qt Project](https://github.com/qt)**
- **[Baldur's Gate III](https://www.mobygames.com/game/150689/baldurs-gate-iii/credits/windows/?autoplatform=true)**
- **[Cyberpunk 2077](https://www.mobygames.com/game/128136/cyberpunk-2077/credits/windows/?autoplatform=true)**
- **[X-Plane](https://x-plane.com/)**
- **[Detroit: Become Human](https://gpuopen.com/learn/porting-detroit-3/)**
- **[Vulkan Samples](https://github.com/LunarG/VulkanSamples)** - official Khronos Vulkan samples. License: Apache-style.
- **[GFXReconstruct](https://github.com/LunarG/gfxreconstruct)** - a tools for the capture and replay of graphics API calls. License: MIT.
- **[Anvil](https://github.com/GPUOpen-LibrariesAndSDKs/Anvil)** - cross-platform framework for Vulkan. License: MIT.
- **[Filament](https://github.com/google/filament)** - physically based rendering engine for Android, Windows, Linux and macOS, from Google. Apache License 2.0.
- **[Atypical Games - proprietary game engine](https://developer.samsung.com/galaxy-gamedev/gamedev-blog/infinitejet.html)**
- **[Flax Engine](https://flaxengine.com/)**
- **[Godot Engine](https://github.com/godotengine/godot/)** - multi-platform 2D and 3D game engine. License: MIT.
- **[Lightweight Java Game Library (LWJGL)](https://www.lwjgl.org/)** - includes binding of the library for Java. License: BSD.
- **[LightweightVK](https://github.com/corporateshark/lightweightvk)** - lightweight C++ bindless Vulkan 1.3 wrapper. License: MIT.
- **[PowerVR SDK](https://github.com/powervr-graphics/Native_SDK)** - C++ cross-platform 3D graphics SDK, from Imagination. License: MIT.
- **[Skia](https://github.com/google/skia)** - complete 2D graphic library for drawing Text, Geometries, and Images, from Google.
- **[The Forge](https://github.com/ConfettiFX/The-Forge)** - cross-platform rendering framework. Apache License 2.0.
- **[VK9](https://github.com/disks86/VK9)** - Direct3D 9 compatibility layer using Vulkan. Zlib license.
- **[vkDOOM3](https://github.com/DustinHLand/vkDOOM3)** - Vulkan port of GPL DOOM 3 BFG Edition. License: GNU GPL.
- **[vkQuake2](https://github.com/kondrak/vkQuake2)** - vanilla Quake 2 with Vulkan support. License: GNU GPL.
- **[Vulkan Best Practice for Mobile Developers](https://github.com/ARM-software/vulkan_best_practice_for_mobile_developers)** from ARM. License: MIT.
- **[RPCS3](https://github.com/RPCS3/rpcs3)** - PlayStation 3 emulator/debugger. License: GNU GPLv2.
- **[PPSSPP](https://github.com/hrydgard/ppsspp)** - Playstation Portable emulator/debugger. License: GNU GPLv2+.
- **[Wicked Engine](https://github.com/turanszkij/WickedEngine)** - 3D engine with modern graphics 

[Many other projects on GitHub](https://github.com/search?q=AMD_VULKAN_MEMORY_ALLOCATOR_H&type=Code) and some game development studios that use Vulkan in their games.

# See also

- **[D3D12 Memory Allocator](https://github.com/GPUOpen-LibrariesAndSDKs/D3D12MemoryAllocator)** - equivalent library for Direct3D 12. License: MIT.
- **[Awesome Vulkan](https://github.com/vinjn/awesome-vulkan)** - a curated list of awesome Vulkan libraries, debuggers and resources.
- **[vcpkg](https://github.com/Microsoft/vcpkg)** dependency manager from Microsoft also offers a port of this library.
- **[VulkanMemoryAllocator-Hpp](https://github.com/YaaZ/VulkanMemoryAllocator-Hpp)** - C++ binding for this library. License: CC0-1.0.
- **[PyVMA](https://github.com/realitix/pyvma)** - Python wrapper for this library. Author: Jean-Sbastien B. (@realitix). License: Apache 2.0.
- **[vk-mem](https://github.com/gwihlidal/vk-mem-rs)** - Rust binding for this library. Author: Graham Wihlidal. License: Apache 2.0 or MIT.
- **[Haskell bindings](https://hackage.haskell.org/package/VulkanMemoryAllocator)**, **[github](https://github.com/expipiplus1/vulkan/tree/master/VulkanMemoryAllocator)** - Haskell bindings for this library. Author: Ellie Hermaszewska (@expipiplus1). License BSD-3-Clause.
- **[vma_sample_sdl](https://github.com/rextimmy/vma_sample_sdl)** - SDL port of the sample app of this library (with the goal of running it on multiple platforms, including MacOS). Author: @rextimmy. License: MIT.
- **[vulkan-malloc](https://github.com/dylanede/vulkan-malloc)** - Vulkan memory allocation library for Rust. Based on version 1 of this library. Author: Dylan Ede (@dylanede). License: MIT / Apache 2.0.


## Links discovered
- [Vulkan Memory Allocator](https://gpuopen-librariesandsdks.github.io/VulkanMemoryAllocator/html/)
- [include/vk_mem_alloc.h](https://github.com/blender/blender/blob/main/extern/vulkan_memory_allocator/include/vk_mem_alloc.h)
- [LICENSE.txt](https://github.com/blender/blender/blob/main/extern/vulkan_memory_allocator/LICENSE.txt)
- [CHANGELOG.md](https://github.com/blender/blender/blob/main/extern/vulkan_memory_allocator/CHANGELOG.md)
- [Vulkan Memory Allocator on GPUOpen](https://gpuopen.com/gaming-product/vulkan-memory-allocator/)
- [![Average time to resolve an issue](http://isitmaintained.com/badge/resolution/GPUOpen-LibrariesAndSDKs/VulkanMemoryAllocator.svg)
- [tools/GpuMemDumpVis](https://github.com/blender/blender/blob/main/extern/vulkan_memory_allocator/tools/GpuMemDumpVis/README.md)
- [CMake GUI](https://cmake.org/runningcmake/)
- [CMake documentation](https://cmake.org/cmake/help/latest/index.html)
- [vcpkg](https://github.com/Microsoft/vcpkg)
- [create an issue or pull request](https://github.com/Microsoft/vcpkg)
- [Microsoft Visual C++ Redistributable](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)
- [Documentation](https://gpuopen-librariesandsdks.github.io/VulkanMemoryAllocator/html/)
- [Blender](https://www.blender.org)
- [Qt Project](https://github.com/qt)
- [Baldur's Gate III](https://www.mobygames.com/game/150689/baldurs-gate-iii/credits/windows/?autoplatform=true)
- [Cyberpunk 2077](https://www.mobygames.com/game/128136/cyberpunk-2077/credits/windows/?autoplatform=true)
- [X-Plane](https://x-plane.com/)
- [Detroit: Become Human](https://gpuopen.com/learn/porting-detroit-3/)
- [Vulkan Samples](https://github.com/LunarG/VulkanSamples)
- [GFXReconstruct](https://github.com/LunarG/gfxreconstruct)
- [Anvil](https://github.com/GPUOpen-LibrariesAndSDKs/Anvil)
- [Filament](https://github.com/google/filament)
- [Atypical Games - proprietary game engine](https://developer.samsung.com/galaxy-gamedev/gamedev-blog/infinitejet.html)
- [Flax Engine](https://flaxengine.com/)
- [Godot Engine](https://github.com/godotengine/godot/)
- [Lightweight Java Game Library (LWJGL)](https://www.lwjgl.org/)
- [LightweightVK](https://github.com/corporateshark/lightweightvk)
- [PowerVR SDK](https://github.com/powervr-graphics/Native_SDK)
- [Skia](https://github.com/google/skia)
- [The Forge](https://github.com/ConfettiFX/The-Forge)
- [VK9](https://github.com/disks86/VK9)
- [vkDOOM3](https://github.com/DustinHLand/vkDOOM3)
- [vkQuake2](https://github.com/kondrak/vkQuake2)
- [Vulkan Best Practice for Mobile Developers](https://github.com/ARM-software/vulkan_best_practice_for_mobile_developers)
- [RPCS3](https://github.com/RPCS3/rpcs3)
- [PPSSPP](https://github.com/hrydgard/ppsspp)
- [Wicked Engine](https://github.com/turanszkij/WickedEngine)
- [Many other projects on GitHub](https://github.com/search?q=AMD_VULKAN_MEMORY_ALLOCATOR_H&type=Code)
- [D3D12 Memory Allocator](https://github.com/GPUOpen-LibrariesAndSDKs/D3D12MemoryAllocator)
- [Awesome Vulkan](https://github.com/vinjn/awesome-vulkan)
- [VulkanMemoryAllocator-Hpp](https://github.com/YaaZ/VulkanMemoryAllocator-Hpp)
- [PyVMA](https://github.com/realitix/pyvma)
- [vk-mem](https://github.com/gwihlidal/vk-mem-rs)
- [Haskell bindings](https://hackage.haskell.org/package/VulkanMemoryAllocator)
- [github](https://github.com/expipiplus1/vulkan/tree/master/VulkanMemoryAllocator)
- [vma_sample_sdl](https://github.com/rextimmy/vma_sample_sdl)
- [vulkan-malloc](https://github.com/dylanede/vulkan-malloc)

--- extern/bullet2/readme.txt ---
This is the new refactored version of Bullet physics library version 2.x

Questions? mail blender at erwincoumans.com, or check the bf-blender mailing list.
Thanks,
Erwin

Apply patches/blender.patch to fix a few build errors and warnings and dd original
vertex access for BMesh convex hull operator.

Documentation is available at:
http://code.google.com/p/bullet/source/browse/trunk/Bullet_User_Manual.pdf
and:
https://github.com/bulletphysics/bullet3/tree/master/docs


--- intern/mikktspace/README.md ---
# MikkTSpace
A common standard for tangent space used in baking tools to produce normal maps.

More information can be found at http://www.mikktspace.com/.


--- intern/dualcon/intern/readme.txt ---
PolyMender program for robustly repairing a polygonal model.

Author: Tao Ju (jutao@cs.wustl.edu)

Version: 1.6 (Updated: Oct. 12, 2006)

Platform: Windows


I. What's new in v1.6:


> Removal of disconnected components

> Topologically manifold dual contouring

> Output signed octree with geometry



II. Introduction


PolyMender is based on the algorithm presented in the paper "Robust Repair of Polygonal Models" (SIGGRAPH 2004). The program reads in a polygonal model (i.e., a bag of polygons) and produces a closed surface that approximates the original model. PolyMender consumes a small amount of time and memory space, and can accurately reproduce the original geometry. PolyMender is suitable for repairing CAD models and gigantic polygonal models. Alternatively, PolyMender can also be used to generate a signed volume from any polygonal models.



III. How to run


The executable package contains three programs:

1. PolyMender, PolyMender-clean

Purpose: repairs general purpose models, such as those created from range scanners. The repaired surface is constructed using Marching Cubes. Consumes minimal memory and time and generates closed, manifold triangular surfaces. The -clean option removes isolated pieces.

2. PolyMender-qd, PolyMender-qd-clean

Purpose: same as PolyMender and PolyMender-clean, but outputs a quad-mesh.

3. PolyMender-dc, PolyMender-dc-clean

Purpose: repairs models containing sharp features, such as CAD models. The repaired surface is constructed using Dual Contouring with a manifold topology, which is capable of reproducing sharp edges and corners. However, more memory is required. Generates closed triangular and quadrilateral surfaces. The -clean option removes isolated pieces.


Type the program names (e.g., PolyMender) on the DOS prompt and you will see their usages:

Usage:   PolyMender <input_file> <octree_depth> <scale> <output_file>

Example: PolyMender bunny.ply 6 0.9 closedbunny.ply

Description:

<input_file>    Polygonal file of format STL (binary only), ASC, or PLY.

<octree_depth>  Integer depth of octree. The dimension of the volumetric
                grid is 2^<octree_depth> on each side.

<scale>         Floating point number between 0 and 1 denoting the ratio of
                the largest dimension of the model over the size of the grid.

<output_file>   Output in polygonal format PLY or signed-octree format SOF (or SOG).


Additional notes:

1. STL(binary) is preferred input format, since the program does not need to store the model in memory at all. ASC or PLY formats require additional storage of vertices, due to their topology-geometry file structure.

2. The running time and memory consumption of the program depends on several factors: the number of input polygons, the depth of the octree, and the surface area of the model (hence the number of leaf nodes on the octree). To give an idea, processing the David model with 56 million triangles at depth 13 takes 45 minutes using 500 MB RAM (excluding the mem allocated for storing vertices when reading PLY format) on a PC with AMD 1.5Hz CPU.

3. The number of output polygons can be finely controlled using the scale argument. The large the scale, the more polygons are generated, since the model occupies a larger portion of the volume grid.

4. As an alternative of output repaired models, the intermediate signed octree can be generated as a SOF or SOG file. The signed octree can be used for generating signed distance field, extracting isosurfaces, or multiresolution spatial representation of the polygonal model.


IV SOF format

SOF (Signed Octree Format) records an octree grid with signes attached to the 8 corners of each leaf node. All leaf nodes appear at the same depth that is specified by the <octree_depth> argument to the program. The tree is recorded in SOF file using pre-order traversal. Here is the structure of a SOF file (binary):

<header>

<node>

<header> is a 4-bytes integer that equals 2 ^ octree_depth. The first byte of a <node> is either 0 (denoting an intermediate node) or 1 (denoting an empty node) or 2 (denoting a leaf node). After the first byte, an intermediate node <node> contains (after the first byte) eight <node> structures for its eight children; an empty node <node> contains one byte of value 0 or 1 denoting if it is inside or outside; and a leaf node contains one byte whose eight bits correspond to the signs at its eight corners (0 for inside and 1 for outside). The order of enumeration of the eight children nodes in an intermediate nodeis the following (expressed in coordinates <x,y,z> ): <0,0,0>,<0,0,1>,<0,1,0>,<0,1,1>,<1,0,0>,<1,0,1>,<1,1,0>,<1,1,1>. The enumeration of the eight corners in a leaf node follows the same order (e.g., the lowest bit records the sign at <0,0,0>).



V SOG format

SOF (Signed Octree with Geometry) has the same data structure with SOG, with the addition of following features:

1. The file starts with a 128-byte long header. Currently, the header begins with the string "SOG.Format 1.0" followed by 3 floats representing the lower-left-near corner of the octree follwed by 1 float denoting the length of the octree (in one direction). The locations and lengths are in the input model's coordinate space. The rest of the header is left empty.

2. Each leaf node has additioanl three floats {x,y,z} (following the signs) denoting the geometric location of a feature vertex within the cell.



VI Test data

Three models are included in the testmodels package. (Suggested arguments are provided in the parathesis).

bunny.ply (octree depth: 7, scale: 0.9)

- The Stanford Bunny (containing big holes at the bottom)

horse.stl (octree depth: 8, scale: 0.9)

- The horse model with 1/3 of all polygons removed and vertices randomly perturbed.

mechanic.asc (octree depth: 6, scale: 0.9)

- A mechanic part with hanging triangles


--- intern/CMakeLists.txt ---
# SPDX-FileCopyrightText: 2006 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

# add_subdirectory(atomic)  # header only
add_subdirectory(atomic)
add_subdirectory(clog)
add_subdirectory(ghost)
add_subdirectory(guardedalloc)
add_subdirectory(libmv)
add_subdirectory(memutil)
add_subdirectory(opensubdiv)
add_subdirectory(mikktspace)
add_subdirectory(eigen)
add_subdirectory(sky)

add_subdirectory(openvdb)

if(WITH_MOD_REMESH)
  add_subdirectory(dualcon)
endif()

if(WITH_IK_SOLVER)
  add_subdirectory(iksolver)
endif()

if(WITH_IK_ITASC)
  add_subdirectory(itasc)
endif()

if(WITH_CYCLES)
  add_subdirectory(cycles)
endif()

if(WITH_BULLET)
  add_subdirectory(rigidbody)
endif()


if(WIN32)
  # Only windows needs utf16 converter.
  add_subdirectory(utfconv)

  # Only used for Windows for now.
  add_subdirectory(uriconvert)
endif()

if(WITH_MOD_FLUID)
  add_subdirectory(mantaflow)
endif()

if(WITH_UV_SLIM)
  add_subdirectory(slim)
endif()

if(WITH_QUADRIFLOW)
  add_subdirectory(quadriflow)
endif()

if(UNIX AND NOT APPLE)
  add_subdirectory(libc_compat)
endif()

if(WITH_RENDERDOC)
  add_subdirectory(renderdoc_dynload)
endif()

if(UNIX AND NOT APPLE)
  # Important this comes after "ghost" as it uses includes defined by GHOST's CMake.
  if(WITH_GHOST_WAYLAND AND WITH_GHOST_WAYLAND_DYNLOAD)
    add_subdirectory(wayland_dynload)
  endif()
endif()


--- intern/atomic/CMakeLists.txt ---
# SPDX-FileCopyrightText: 2020 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

set(INC
  .
)

set(INC_SYS
)
set(LIB
)

add_library(bf_intern_atomic INTERFACE)

target_include_directories(bf_intern_atomic INTERFACE .)
add_library(bf::intern::atomic ALIAS bf_intern_atomic)

# CMake 3.19+ allows one to populate the interface library with
# source files to show in the IDE, for people on older CMake versions
# these headers will be visible in the bf_intern_guardedalloc project
# where they historically have been.
if(${CMAKE_VERSION} VERSION_GREATER_EQUAL "3.19")
  set(SRC
    atomic_ops.h
    intern/atomic_ops_ext.h
    intern/atomic_ops_msvc.h
    intern/atomic_ops_unix.h
    intern/atomic_ops_utils.h
  )
  target_sources(bf_intern_atomic PRIVATE ${SRC})
  blender_source_group(bf_intern_atomic ${SRC})
endif()

if(WITH_GTESTS)
  set(TEST_SRC
    tests/atomic_test.cc
  )
  set(TEST_INC
  )
  set(TEST_LIB
    PRIVATE bf_intern_atomic
  )
  blender_add_test_executable(atomic "${TEST_SRC}" "${INC};${TEST_INC}" "${INC_SYS}" "${LIB};${TEST_LIB}")
endif()


--- intern/clog/CMakeLists.txt ---
# SPDX-FileCopyrightText: 2002-2022 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

set(INC
  PUBLIC .
)

set(INC_SYS

)

set(SRC
  clog.cc

  CLG_log.h
)

set(LIB
  PRIVATE bf::intern::guardedalloc
  PRIVATE bf::intern::atomic
)

# Disabled for `makesdna` & `makesrna`.
add_definitions(-DWITH_CLOG_PTHREADS)

blender_add_lib(bf_intern_clog "${SRC}" "${INC}" "${INC_SYS}" "${LIB}")
add_library(bf::intern::clog ALIAS bf_intern_clog)


--- intern/cycles/CMakeLists.txt ---
# SPDX-FileCopyrightText: 2011-2022 Blender Foundation
#
# SPDX-License-Identifier: Apache-2.0

# Standalone or with Blender
if(NOT WITH_BLENDER)
  set(CYCLES_INSTALL_PATH ${CMAKE_INSTALL_PREFIX})
else()
  set(WITH_CYCLES_BLENDER ON)
  # WINDOWS_PYTHON_DEBUG needs to write into the user addons folder since it will
  # be started with --env-system-scripts pointing to the release folder, which will
  # lack the cycles addon, and we don't want to write into it.
  if(NOT WINDOWS_PYTHON_DEBUG)
    set(CYCLES_INSTALL_PATH "scripts/addons_core/cycles")
  else()
    set(CYCLES_INSTALL_PATH
      "$ENV{appdata}/blender foundation/blender/${BLENDER_VERSION}/scripts/addons_core/cycles"
    )
  endif()
endif()

# External Libraries

if(NOT CYCLES_STANDALONE_REPOSITORY)
  include(cmake/external_libs.cmake)
  include(cmake/macros.cmake)
endif()

# Build Flags
# TODO: this code could be refactored a bit to avoid duplication.
# NOTE: CXX_HAS_SSE42 is needed in case passing SSE flags fails altogether (`gcc-arm`).

# Determine vectorization support and required build flags
if(WITH_CYCLES_NATIVE_ONLY)
  set(CXX_HAS_SSE42 FALSE)
  set(CXX_HAS_AVX2 FALSE)
  add_definitions(
    -DWITH_KERNEL_NATIVE
  )

  if(NOT MSVC)
    add_check_cxx_compiler_flags(
      CMAKE_CXX_FLAGS
      _has_march_native "-march=native"
    )
    if(_has_march_native)
      string(APPEND CMAKE_CXX_FLAGS " -march=native")
    else()
      string(APPEND CMAKE_CXX_FLAGS "")
    endif()
    unset(_has_march_native)
  else()
    if(NOT MSVC_NATIVE_ARCH_FLAGS)
      try_run(
        arch_run_result
        arch_compile_result
        ${CMAKE_CURRENT_BINARY_DIR}/
        ${CMAKE_CURRENT_SOURCE_DIR}/cmake/msvc_arch_flags.c
        COMPILE_OUTPUT_VARIABLE arch_compile_output
        RUN_OUTPUT_VARIABLE arch_run_output
      )
      if(arch_compile_result AND "${arch_run_result}" EQUAL "0")
        string(STRIP ${arch_run_output} arch_run_output)
        set(MSVC_NATIVE_ARCH_FLAGS ${arch_run_output} CACHE STRING "MSVC Native architecture flags")
      endif()
    endif()
    string(APPEND CMAKE_CXX_FLAGS " ${MSVC_NATIVE_ARCH_FLAGS}")
  endif()
elseif(SUPPORTS_NEON_BUILD AND SSE2NEON_FOUND)
  # On ARM we use SSE2NEON to compile the cycles SSE kernels into native NEON instructions
  # Disable the CXX_HAS_* flags so we don't add any SSE or AVX compiler flags.
  set(CXX_HAS_SSE42 FALSE)
  set(CXX_HAS_AVX2 FALSE)
elseif(WIN32 AND MSVC AND NOT CMAKE_CXX_COMPILER_ID MATCHES "Clang")
  set(CXX_HAS_SSE42 TRUE)
  set(CXX_HAS_AVX2 TRUE)

  # /arch:AVX for VC2012 and above
  if(NOT MSVC_VERSION LESS 1700)
    set(CYCLES_AVX2_FLAGS "/arch:AVX /arch:AVX2")
  elseif(NOT CMAKE_CL_64)
    set(CYCLES_AVX2_FLAGS "/arch:SSE2")
  endif()

  # there is no /arch:SSE3, but intrinsics are available anyway
  if(CMAKE_CL_64)
    set(CYCLES_SSE42_FLAGS "")
  else()
    set(CYCLES_SSE42_FLAGS "/arch:SSE2")
  endif()
elseif(CMAKE_COMPILER_IS_GNUCC OR (CMAKE_CXX_COMPILER_ID MATCHES "Clang"))
  check_cxx_compiler_flag(-msse4.2 CXX_HAS_SSE42)
  check_cxx_compiler_flag(-mavx2 CXX_HAS_AVX2)

  if(CXX_HAS_SSE42)
    set(CYCLES_SSE42_FLAGS "-msse -msse2 -msse3 -mssse3 -msse4.1 -msse4.2")
    if(CXX_HAS_AVX2)
      set(CYCLES_AVX2_FLAGS "${CYCLES_SSE42_FLAGS} -mavx -mavx2 -mfma -mlzcnt -mbmi -mbmi2 -mf16c")
    endif()
  endif()

elseif(WIN32 AND CMAKE_CXX_COMPILER_ID STREQUAL "Intel")
  check_cxx_compiler_flag(/QxSSE4.2 CXX_HAS_SSE42)
  check_cxx_compiler_flag(/QxCORE-AVX2 CXX_HAS_AVX2)

  if(CXX_HAS_SSE42)
    set(CYCLES_SSE42_FLAGS "/QxSSE4.2")

    if(CXX_HAS_AVX2)
      set(CYCLES_AVX2_FLAGS "/QxCORE-AVX2")
    endif()
  endif()
elseif(CMAKE_CXX_COMPILER_ID STREQUAL "Intel")
  check_cxx_compiler_flag(-xsse4.2 CXX_HAS_SSE42)
  check_cxx_compiler_flag(-xcore-avx2 CXX_HAS_AVX2)

  if(CXX_HAS_SSE42)
    set(CYCLES_SSE42_FLAGS "-xsse4.2")

    if(CXX_HAS_AVX2)
      set(CYCLES_AVX2_FLAGS "-xcore-avx2")
    endif()
  endif()
endif()

if(CXX_HAS_SSE42)
  add_definitions(
    -DWITH_KERNEL_SSE42
  )
  # We require SSE4.2 as a minimum, so make use of it
  string(APPEND CMAKE_CXX_FLAGS " ${CYCLES_SSE42_FLAGS}")
endif()

if(CXX_HAS_AVX2)
  add_definitions(-DWITH_KERNEL_AVX2)
endif()

# Enable math optimizations

if(WIN32 AND MSVC AND NOT CMAKE_CXX_COMPILER_ID MATCHES "Clang")
  # Unlike GCC/clang we still use fast math, because there is no fine
  # grained control and the speedup we get here is too big to ignore.
  string(APPEND CMAKE_CXX_FLAGS " /fp:fast -D_CRT_SECURE_NO_WARNINGS /GS-")
  string(APPEND CMAKE_CXX_FLAGS_RELEASE " /Ox")
  string(APPEND CMAKE_CXX_FLAGS_RELWITHDEBINFO " /Ox")
  string(APPEND CMAKE_CXX_FLAGS_MINSIZEREL " /Ox")

  # `jumptablerdata` improves performance when there is contention in large switch statements
  # such as in `svm.h`.
  # This flag is supported starting with MSVC 17.7 preview 3:
  # https://learn.microsoft.com/en-us/cpp/build/reference/jump-table-rdata
  # This is an x64 specific optimization.
  if(CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 19.37.32820
     AND CMAKE_SYSTEM_PROCESSOR STREQUAL "AMD64")
    string(APPEND CMAKE_CXX_FLAGS " /jumptablerdata")
  endif()
elseif(CMAKE_COMPILER_IS_GNUCC OR (CMAKE_CXX_COMPILER_ID MATCHES "Clang"))
  # Assume no signal trapping for better code generation.
  list(APPEND CYCLES_MATH_FLAGS "-fno-trapping-math")
  # Avoid overhead of setting `errno` for NaNs.
  list(APPEND CYCLES_MATH_FLAGS "-fno-math-errno")
  # Let compiler optimize 0.0 - x without worrying about signed zeros.
  list(APPEND CYCLES_MATH_FLAGS "-fno-signed-zeros")
  # Disable FMA auto-vectorization across different statements,
  # which creates havoc in "intern/cycles/blender/camera.cpp"
  # with GCC-15.2.1, for something like:
  #   `const float2 dv = 2.0f * (aspectratio * bcam->shift + bcam->offset * aspect * 2.0f);`
  # (probably due to https://gcc.gnu.org/bugzilla/show_bug.cgi?id=106902).
  list(APPEND CYCLES_MATH_FLAGS "-ffp-contract=on")
  # Let the compiler replace x/y with x*(1/y)
  list(APPEND CYCLES_MATH_FLAGS "-freciprocal-math")
  # Let the compiler reorder terms to save operations
  # NOTE: Disabled for now due to problems with bsdf_D for GGX on Linux (#130389)
  # list(APPEND CYCLES_MATH_FLAGS "-fassociative-math")
  # Don't enable `-ffinite-math-only` since the BVH code relies on NaNs.
  # Otherwise, we could just use `-ffast-math`.

  if(CMAKE_COMPILER_IS_GNUCC)
    # Assume no signal trapping for better code generation.
    list(APPEND CYCLES_MATH_FLAGS "-fno-signaling-nans")
    # Assume a fixed rounding mode for better constant folding.
    list(APPEND CYCLES_MATH_FLAGS "-fno-rounding-math")

    if(CXX_HAS_SSE42)
      list(APPEND CYCLES_MATH_FLAGS "-mfpmath=sse")
    endif()
  endif()

  if(WIN32 AND MSVC)
    # Pass clang flags directly to clang otherwise. Clang-cl doesn't recognize
    # these flags by default
    list(TRANSFORM CYCLES_MATH_FLAGS PREPEND "/clang:")
  endif()

  list(JOIN CYCLES_MATH_FLAGS " " CYCLES_MATH_FLAGS)
  string(APPEND CMAKE_CXX_FLAGS " ${CYCLES_MATH_FLAGS}")
endif()

# Definitions and Includes

add_definitions(
  ${BOOST_DEFINITIONS}
)

add_definitions(
  -DCCL_NAMESPACE_BEGIN=namespace\ ccl\ {
  -DCCL_NAMESPACE_END=}
)

include_directories(
  SYSTEM
  ${BOOST_INCLUDE_DIR}
  ${OPENIMAGEIO_INCLUDE_DIRS}
  ${IMATH_INCLUDE_DIRS}
  ${OPENEXR_INCLUDE_DIRS}
  ${PUGIXML_INCLUDE_DIR}
)

if(WITH_CYCLES_DEBUG)
  add_definitions(-DWITH_CYCLES_DEBUG)
endif()
if(WITH_CYCLES_STANDALONE_GUI)
  add_definitions(-DWITH_CYCLES_STANDALONE_GUI)
endif()

if(WITH_CYCLES_PTEX)
  add_definitions(-DWITH_PTEX)
endif()

if(WITH_CYCLES_OSL)
  add_definitions(-DWITH_OSL)
  include_directories(
    SYSTEM
    ${OSL_INCLUDE_DIR}
  )
endif()

if(WITH_CYCLES_DEVICE_CUDA OR WITH_CYCLES_DEVICE_OPTIX)
  add_definitions(-DWITH_CUDA)

  if(WITH_CUDA_DYNLOAD)
    include_directories(
      ../../extern/cuew/include
    )
    add_definitions(-DWITH_CUDA_DYNLOAD)
  else()
    include_directories(
      SYSTEM
      ${CUDA_TOOLKIT_INCLUDE}
    )
  endif()
endif()

if(WITH_CYCLES_DEVICE_HIP)
  add_definitions(-DWITH_HIP)

  if(WITH_CYCLES_DEVICE_HIPRT)
    include_directories(
      ${HIPRT_INCLUDE_DIR}
    )
    add_definitions(-DWITH_HIPRT)
  endif()

  if(WITH_HIP_DYNLOAD)
    include_directories(
      ../../extern/hipew/include
    )
    add_definitions(-DWITH_HIP_DYNLOAD)
  endif()
endif()

if(WITH_CYCLES_DEVICE_OPTIX)
  find_package(OptiX 8.0.0)

  if(OPTIX_FOUND)
    add_definitions(-DWITH_OPTIX)
    include_directories(
      SYSTEM
      ${OPTIX_INCLUDE_DIR}
    )
  else()
    set_and_warn_library_found("OptiX" OPTIX_FOUND WITH_CYCLES_DEVICE_OPTIX)
  endif()
endif()

if(WITH_CYCLES_DEVICE_METAL)
  add_definitions(-DWITH_METAL)
endif()

if(WITH_CYCLES_DEVICE_ONEAPI)
  add_definitions(-DWITH_ONEAPI)
endif()

if(WITH_CYCLES_EMBREE)
  add_definitions(-DWITH_EMBREE)
  if(WITH_CYCLES_DEVICE_ONEAPI AND EMBREE_SYCL_SUPPORT)
    # NOTE: The debug version of Embree is built without SYCL support on Windows
    # since 7fb480095e371f8f5ac4f647f0ba2fd78da486f7. This is not reflected in
    # EMBREE_SYCL_SUPPORT which is coming from Embree headers that aren't
    # differentiated for release and debug, so we handle this case here by
    # disabling its use when embree4_sycl_d.lib doesn't exist.
    set(EMBREE_SYCL_DEBUG_LIBRARY ${EMBREE_LIBRARIES})
    list(FILTER EMBREE_SYCL_DEBUG_LIBRARY INCLUDE REGEX "_sycl_d\\.lib$")
    if(WIN32 AND NOT EMBREE_SYCL_DEBUG_LIBRARY)
      add_compile_definitions("$<$<CONFIG:Release>:WITH_EMBREE_GPU>")
      add_compile_definitions("$<$<CONFIG:RelWithDebInfo>:WITH_EMBREE_GPU>")
      add_compile_definitions("$<$<CONFIG:MinSizeRel>:WITH_EMBREE_GPU>")
      if(CMAKE_BUILD_TYPE MATCHES "Debug" OR GENERATOR_IS_MULTI_CONFIG)
        message(STATUS
          "The use of Embree GPU is disabled for the Debug configuration "
          "as embree${EMBREE_MAJOR_VERSION}_sycl_d.lib is not found."
        )
      endif()
    else()
      add_definitions(-DWITH_EMBREE_GPU)
    endif()
  endif()
  add_definitions(-DEMBREE_MAJOR_VERSION=${EMBREE_MAJOR_VERSION})
  include_directories(
    SYSTEM
    ${EMBREE_INCLUDE_DIRS}
  )
endif()

if(WITH_OPENIMAGEDENOISE)
  add_definitions(-DWITH_OPENIMAGEDENOISE)
  include_directories(
    SYSTEM
    ${OPENIMAGEDENOISE_INCLUDE_DIRS}
  )
endif()

# Includes that might be overrides by USD last, to avoid compiling
# against the wrong versions of other libraries.
include_directories(
  SYSTEM
  ${TBB_INCLUDE_DIRS}
)

if(WITH_OPENVDB)
  add_definitions(-DWITH_OPENVDB ${OPENVDB_DEFINITIONS})
  include_directories(
    SYSTEM
    ${OPENVDB_INCLUDE_DIRS}
  )
endif()

if(WITH_NANOVDB)
  add_definitions(-DWITH_NANOVDB)
  include_directories(
    SYSTEM
    ${NANOVDB_INCLUDE_DIR}
  )
endif()

if(WITH_OPENSUBDIV)
  add_definitions(-DWITH_OPENSUBDIV)
  include_directories(
    SYSTEM
    ${OPENSUBDIV_INCLUDE_DIRS}
  )
endif()

if(WITH_OPENCOLORIO)
  add_definitions(-DWITH_OCIO)
  include_directories(
    SYSTEM
    ${OPENCOLORIO_INCLUDE_DIRS}
  )
endif()

if(WITH_CYCLES_PATH_GUIDING)
  add_definitions(-DWITH_PATH_GUIDING)

  # The level of the guiding integration.
  # Different levels can be selected to measure the overhead of different stages.
  # 1 = recording the path segments
  # 2 = 1 + generating (not storing) sample data from the segments
  # 3 = 2 + storing the generates sample data
  # 4 = 3 + training the guiding fields
  # 5 = 4 + querying the trained guiding for sampling (full path guiding)
  add_definitions(-DPATH_GUIDING_LEVEL=5)

  include_directories(
    SYSTEM
    ${OPENPGL_INCLUDE_DIR}
  )
endif()

# NaN debugging
if(WITH_CYCLES_DEBUG_NAN)
  add_definitions(-DWITH_CYCLES_DEBUG_NAN)
endif()

if(WITH_PUGIXML OR OPENIMAGEIO_PUGIXML_FOUND)
  add_definitions(-DWITH_PUGIXML)
  if((NOT OPENIMAGEIO_PUGIXML_FOUND) OR WIN32)
    add_definitions(-DWITH_SYSTEM_PUGIXML)
  endif()
endif()

if(CYCLES_STANDALONE_REPOSITORY)
  include_directories(../third_party/atomic)
else()
  include_directories(../atomic)
endif()

# Warnings
if(CMAKE_COMPILER_IS_GNUCXX OR CMAKE_C_COMPILER_ID MATCHES "Clang")
  add_check_cxx_compiler_flags(
    CMAKE_CXX_FLAGS
    _has_no_error_unused_macros "-Wno-error=unused-macros"
  )
  unset(_has_no_error_unused_macros)
endif()

if(WITH_USD)
  add_definitions(-DWITH_USD)
endif()

if(WITH_CYCLES_HYDRA_RENDER_DELEGATE AND (NOT WITH_USD))
  set_and_warn_library_found("USD" WITH_USD WITH_CYCLES_HYDRA_RENDER_DELEGATE)
endif()
if(WITH_CYCLES_HYDRA_RENDER_DELEGATE AND (NOT WITH_BLENDER) AND (NOT WITH_CYCLES_STANDALONE))
  set(CYCLES_INSTALL_PATH ${CYCLES_INSTALL_PATH}/hdCycles/resources)
endif()

if(WITH_CYCLES_CUDA_BINARIES)
  if(MSVC)
    set(MAX_MSVC 1800)
    if(${CUDA_VERSION} EQUAL "8.0")
      set(MAX_MSVC 1900)
    elseif(${CUDA_VERSION} EQUAL "9.0")
      set(MAX_MSVC 1910)
    elseif(${CUDA_VERSION} EQUAL "9.1")
      set(MAX_MSVC 1911)
    elseif(${CUDA_VERSION} VERSION_GREATER_EQUAL 10.0)
      set(MAX_MSVC 1999)
    endif()
    unset(MAX_MSVC)
  endif()
endif()

# Subdirectories

if(WITH_CYCLES_BLENDER)
  add_definitions(-DWITH_BLENDER_GUARDEDALLOC)
  add_subdirectory(blender)
endif()

add_subdirectory(app)
add_subdirectory(bvh)
add_subdirectory(device)
add_subdirectory(doc)
add_subdirectory(graph)
add_subdirectory(integrator)
add_subdirectory(kernel)
add_subdirectory(scene)
add_subdirectory(session)
add_subdirectory(subd)
add_subdirectory(util)

# TODO(sergey): Make this to work with standalone repository.
if(WITH_GTESTS)
  add_subdirectory(test)
endif()

if(WITH_CYCLES_HYDRA_RENDER_DELEGATE OR (WITH_CYCLES_STANDALONE AND WITH_USD))
  add_subdirectory(hydra)
endif()

if(NOT WITH_BLENDER)
  if(CYCLES_STANDALONE_REPOSITORY)
    delayed_do_install()
  else()
    delayed_do_install(${CMAKE_BINARY_DIR}/bin)
  endif()
endif()


--- intern/dualcon/CMakeLists.txt ---
# SPDX-FileCopyrightText: 2002-2022 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

set(INC
  .
  intern
  ../guardedalloc
)

set(INC_SYS
)

set(SRC
  intern/manifold_table.cpp
  intern/marching_cubes_table.cpp
  intern/octree.cpp
  intern/Projections.cpp

  intern/cubes.h
  intern/GeoCommon.h
  intern/manifold_table.h
  intern/marching_cubes_table.h
  intern/MemoryAllocator.h
  intern/ModelReader.h
  intern/octree.h
  intern/Projections.h
  intern/Queue.h

  intern/dualcon_c_api.cpp
  dualcon.h
)

set(LIB
  PRIVATE bf::dependencies::eigen
)

blender_add_lib(bf_intern_dualcon "${SRC}" "${INC}" "${INC_SYS}" "${LIB}")


--- intern/eigen/CMakeLists.txt ---
# SPDX-FileCopyrightText: 2015 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

set(INC
  .
)

set(INC_SYS
)

set(SRC
  eigen_capi.h

  intern/eigenvalues.cc
  intern/linear_solver.cc
  intern/matrix.cc
  intern/svd.cc

  intern/eigenvalues.h
  intern/linear_solver.h
  intern/matrix.h
  intern/svd.h
)

set(LIB
  PRIVATE bf::dependencies::eigen
)

blender_add_lib(bf_intern_eigen "${SRC}" "${INC}" "${INC_SYS}" "${LIB}")


--- intern/ghost/CMakeLists.txt ---
# SPDX-FileCopyrightText: 2006 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

set(INC
  PUBLIC .
  ../guardedalloc
)

set(INC_SYS
)

set(SRC
  intern/GHOST_Buttons.cc
  intern/GHOST_C-api.cc
  intern/GHOST_CallbackEventConsumer.cc
  intern/GHOST_Context.cc
  intern/GHOST_ContextNone.cc
  intern/GHOST_EventManager.cc
  intern/GHOST_ISystem.cc
  intern/GHOST_ISystemPaths.cc
  intern/GHOST_ModifierKeys.cc
  intern/GHOST_Path-api.cc
  intern/GHOST_PathUtils.cc
  intern/GHOST_Rect.cc
  intern/GHOST_System.cc
  intern/GHOST_TimerManager.cc
  intern/GHOST_Window.cc
  intern/GHOST_WindowManager.cc

  GHOST_C-api.h
  GHOST_IContext.hh
  GHOST_IEvent.hh
  GHOST_IEventConsumer.hh
  GHOST_ISystem.hh
  GHOST_ISystemPaths.hh
  GHOST_ITimerTask.hh
  GHOST_IWindow.hh
  GHOST_Path-api.hh
  GHOST_Rect.hh
  GHOST_Types.h

  intern/GHOST_Buttons.hh
  intern/GHOST_CallbackEventConsumer.hh
  intern/GHOST_Context.hh
  intern/GHOST_ContextNone.hh
  intern/GHOST_Debug.hh
  intern/GHOST_Event.hh
  intern/GHOST_EventButton.hh
  intern/GHOST_EventCursor.hh
  intern/GHOST_EventDragnDrop.hh
  intern/GHOST_EventKey.hh
  intern/GHOST_EventManager.hh
  intern/GHOST_EventString.hh
  intern/GHOST_EventTrackpad.hh
  intern/GHOST_EventWheel.hh
  intern/GHOST_ModifierKeys.hh
  intern/GHOST_PathUtils.hh
  intern/GHOST_System.hh
  intern/GHOST_SystemPaths.hh
  intern/GHOST_TimerManager.hh
  intern/GHOST_TimerTask.hh
  intern/GHOST_Util.hh
  intern/GHOST_Window.hh
  intern/GHOST_WindowManager.hh
  intern/GHOST_utildefines.hh
  intern/GHOST_utildefines_variadic.hh
)

set(LIB
  PRIVATE bf::blenlib
  PRIVATE bf::dna
  PRIVATE bf::gpu
  PRIVATE bf::imbuf
  PRIVATE bf::intern::clog
)

if(WITH_INPUT_IME)
  add_definitions(-DWITH_INPUT_IME)
endif()

if(WITH_OPENGL_BACKEND)
  list(APPEND LIB
    PRIVATE bf::dependencies::epoxy
  )
  add_definitions(-DWITH_OPENGL_BACKEND)
endif()

if(WITH_VULKAN_BACKEND)
  list(APPEND SRC
    intern/GHOST_ContextVK.cc

    intern/GHOST_ContextVK.hh
  )
  list(APPEND LIB
    PUBLIC bf::dependencies::optional::vulkan
    PRIVATE bf::extern::vulkan_memory_allocator
  )

  add_definitions(-DWITH_VULKAN_BACKEND)
endif()

if(WITH_GHOST_DEBUG)
  list(APPEND SRC
    intern/GHOST_EventPrinter.cc

    intern/GHOST_EventPrinter.hh
  )
  add_definitions(-DWITH_GHOST_DEBUG)
endif()

if(WITH_INPUT_NDOF)
  add_definitions(-DWITH_INPUT_NDOF)

  list(APPEND SRC
    intern/GHOST_NDOFManager.cc

    intern/GHOST_EventNDOF.hh
    intern/GHOST_NDOFManager.hh
  )

  # Only some platforms define this.
  if(DEFINED NDOF_INCLUDE_DIRS)
    list(APPEND INC_SYS
      ${NDOF_INCLUDE_DIRS}
    )
  endif()
  if(DEFINED NDOF_LIBRARIES)
    list(APPEND LIB
      ${NDOF_LIBRARIES}
    )
  endif()
endif()

list(APPEND SRC
  intern/GHOST_SystemHeadless.hh
  intern/GHOST_WindowNULL.hh
)

if(WITH_HEADLESS)
  add_definitions(-DWITH_HEADLESS)
elseif(WITH_GHOST_SDL)
  list(APPEND SRC
    intern/GHOST_ContextSDL.cc
    intern/GHOST_SystemSDL.cc
    intern/GHOST_WindowSDL.cc

    intern/GHOST_ContextSDL.hh
    intern/GHOST_SystemSDL.hh
    intern/GHOST_WindowSDL.hh
  )
  add_definitions(-DWITH_GHOST_SDL)

  list(APPEND LIB
    bf::dependencies::optional::sdl
  )
elseif(APPLE AND NOT WITH_GHOST_X11)
  list(APPEND SRC
    intern/GHOST_SystemCocoa.mm
    intern/GHOST_WindowCocoa.mm

    intern/GHOST_SystemCocoa.hh
    intern/GHOST_WindowCocoa.hh
    intern/GHOST_WindowViewCocoa.hh
  )

  if(WITH_INPUT_NDOF)
    list(APPEND SRC
      intern/GHOST_NDOFManagerCocoa.mm

      intern/GHOST_NDOFManagerCocoa.hh
    )
  endif()

elseif(WITH_GHOST_X11 OR WITH_GHOST_WAYLAND)
  if(WITH_GHOST_X11)
    list(APPEND INC_SYS
      ${X11_X11_INCLUDE_PATH}
    )

    list(APPEND LIB
      ${X11_X11_LIB}
      ${X11_Xrender_LIB}
    )

    list(APPEND SRC
      intern/GHOST_SystemX11.cc
      intern/GHOST_WindowX11.cc

      intern/GHOST_IconX11.hh
      intern/GHOST_SystemX11.hh
      intern/GHOST_WindowX11.hh
    )

    if(WITH_OPENGL_BACKEND)
      list(APPEND SRC
        intern/GHOST_ContextGLX.cc

        intern/GHOST_ContextGLX.hh
      )
    endif()

    if(WITH_GHOST_XDND)
      add_definitions(-DWITH_XDND)

      list(APPEND LIB
        extern_xdnd
      )

      list(APPEND INC
        ../../extern/xdnd
      )

      list(APPEND SRC
        intern/GHOST_DropTargetX11.cc

        intern/GHOST_DropTargetX11.hh
      )
    endif()

    if(X11_XF86keysym_INCLUDE_PATH)
      add_definitions(-DWITH_XF86KEYSYM)
      list(APPEND INC_SYS
        ${X11_XF86keysym_INCLUDE_PATH}
      )
    endif()

    if(WITH_X11_XFIXES)
      add_definitions(-DWITH_X11_XFIXES)
      list(APPEND INC_SYS
        ${X11_Xfixes_INCLUDE_PATH}
      )
      list(APPEND LIB
        ${X11_Xfixes_LIB}
      )
    endif()

    if(WITH_X11_XINPUT)
      add_definitions(-DWITH_X11_XINPUT)
      list(APPEND INC_SYS
        ${X11_Xinput_INCLUDE_PATH}
      )
      list(APPEND LIB
        ${X11_Xinput_LIB}
      )
    endif()

    add_definitions(-DWITH_GHOST_X11)
  endif()

  if(WITH_GHOST_WAYLAND)
    list(APPEND INC_SYS
      ${wayland-client_INCLUDE_DIRS}
      ${wayland-egl_INCLUDE_DIRS}
      ${xkbcommon_INCLUDE_DIRS}
      ${wayland-cursor_INCLUDE_DIRS}
    )
    list(APPEND LIB
      ${xkbcommon_LINK_LIBRARIES}
    )

    if(WITH_GHOST_WAYLAND_DYNLOAD)
      list(APPEND INC_SYS
        ../wayland_dynload/extern
      )
      list(APPEND LIB
        bf_intern_wayland_dynload
      )
      add_definitions(-DWITH_GHOST_WAYLAND_DYNLOAD)
    else()
      list(APPEND LIB
        ${wayland-client_LINK_LIBRARIES}
        ${wayland-egl_LINK_LIBRARIES}
        ${wayland-cursor_LINK_LIBRARIES}
      )
    endif()

    if(WITH_GHOST_CSD)
      list(APPEND SRC
        intern/GHOST_WindowWaylandCSD.cc

        intern/GHOST_WindowWaylandCSD.hh
      )
      add_definitions(-DWITH_GHOST_CSD)
    endif()

    include(CheckSymbolExists)
    set(CMAKE_REQUIRED_DEFINITIONS "-D_GNU_SOURCE")
    check_symbol_exists(memfd_create "sys/mman.h" HAVE_MEMFD_CREATE)
    unset(CMAKE_REQUIRED_DEFINITIONS)
    if(HAVE_MEMFD_CREATE)
      add_definitions(-DHAVE_MEMFD_CREATE)
    endif()

    check_symbol_exists(poll "poll.h" HAVE_POLL)
    if(HAVE_POLL)
      add_definitions(-DHAVE_POLL)
    endif()

    list(APPEND SRC
      intern/GHOST_SystemWayland.cc
      intern/GHOST_WindowWayland.cc

      intern/GHOST_SystemWayland.hh
      intern/GHOST_WaylandUtils.hh
      intern/GHOST_WindowWayland.hh
    )

    set(INC_DST ${CMAKE_CURRENT_BINARY_DIR}/libwayland)

    # Generate protocols bindings.
    macro(generate_protocol_bindings PROT_DEF)
      # File name without directory or extension (use for header name).
      get_filename_component(_name ${PROT_DEF} NAME_WLE)
      add_custom_command(
        OUTPUT ${INC_DST}/${_name}-client-protocol.h
        COMMAND ${CMAKE_COMMAND} -E make_directory ${INC_DST}
        COMMAND ${WAYLAND_SCANNER} client-header ${PROT_DEF} ${INC_DST}/${_name}-client-protocol.h
      )
      add_custom_command(
        OUTPUT ${INC_DST}/${_name}-client-protocol.c
        COMMAND ${CMAKE_COMMAND} -E make_directory ${INC_DST}
        COMMAND ${WAYLAND_SCANNER} private-code ${PROT_DEF} ${INC_DST}/${_name}-client-protocol.c
        DEPENDS ${INC_DST}/${_name}-client-protocol.h
      )

      if(CMAKE_C_COMPILER_ID MATCHES "Clang")
        # Prevent warnings/failure to compile with generated `WL_PRIVATE` declarations.
        set_source_files_properties(
          "${INC_DST}/${_name}-client-protocol.c"
          PROPERTIES COMPILE_FLAGS "-Wno-missing-variable-declarations"
        )
      endif()

      list(APPEND SRC
        ${INC_DST}/${_name}-client-protocol.c
        ${INC_DST}/${_name}-client-protocol.h
      )
      unset(_name)
    endmacro()

    list(APPEND INC_SYS
      ${INC_DST}
    )

    # `xdg-shell`.
    generate_protocol_bindings(
      "${WAYLAND_PROTOCOLS_DIR}/stable/xdg-shell/xdg-shell.xml"
    )
    # `xdg-decoration`.
    generate_protocol_bindings(
      "${WAYLAND_PROTOCOLS_DIR}/unstable/xdg-decoration/xdg-decoration-unstable-v1.xml"
    )
    # `xdg-output`.
    generate_protocol_bindings(
      "${WAYLAND_PROTOCOLS_DIR}/unstable/xdg-output/xdg-output-unstable-v1.xml"
    )
    # `xdg-activation`.
    generate_protocol_bindings(
      "${WAYLAND_PROTOCOLS_DIR}/staging/xdg-activation/xdg-activation-v1.xml"
    )
    # Fractional scale.
    generate_protocol_bindings(
      "${WAYLAND_PROTOCOLS_DIR}/staging/fractional-scale/fractional-scale-v1.xml"
    )
    # ViewPorter (only required when fractional scale is in use).
    generate_protocol_bindings(
      "${WAYLAND_PROTOCOLS_DIR}/stable/viewporter/viewporter.xml"
    )
    # Pointer-constraints.
    generate_protocol_bindings(
      "${WAYLAND_PROTOCOLS_DIR}/unstable/pointer-constraints/pointer-constraints-unstable-v1.xml"
    )
    # Relative-pointer.
    generate_protocol_bindings(
      "${WAYLAND_PROTOCOLS_DIR}/unstable/relative-pointer/relative-pointer-unstable-v1.xml"
    )
    # Pointer-gestures (multi-touch).
    generate_protocol_bindings(
      "${WAYLAND_PROTOCOLS_DIR}/unstable/pointer-gestures/pointer-gestures-unstable-v1.xml"
    )
    # Tablet.
    generate_protocol_bindings(
      "${WAYLAND_PROTOCOLS_DIR}/stable/tablet/tablet-v2.xml"
    )
    # Primary-selection.
    generate_protocol_bindings(
      "${WAYLAND_PROTOCOLS_DIR}/unstable/primary-selection/primary-selection-unstable-v1.xml"
    )
    if(WITH_INPUT_IME)
      generate_protocol_bindings(
        "${WAYLAND_PROTOCOLS_DIR}/unstable/text-input/text-input-unstable-v3.xml"
      )
    endif()
    # Desktop compositor controlled cursor rendering.
    generate_protocol_bindings(
      "${WAYLAND_PROTOCOLS_DIR}/staging/cursor-shape/cursor-shape-v1.xml"
    )

    unset(INC_DST)

    add_definitions(-DWITH_GHOST_WAYLAND)
    if(NOT WITH_GHOST_WAYLAND_APP_ID STREQUAL "")
      add_definitions(-DWITH_GHOST_WAYLAND_APP_ID=${WITH_GHOST_WAYLAND_APP_ID})
    endif()

  endif()

  if(WITH_INPUT_NDOF)
    list(APPEND SRC
      intern/GHOST_NDOFManagerUnix.cc

      intern/GHOST_NDOFManagerUnix.hh
    )
  endif()

  if(NOT WITH_INSTALL_PORTABLE)
    add_definitions(-DPREFIX="${CMAKE_INSTALL_PREFIX}")
  endif()


elseif(WIN32)
  # # Warnings as errors, this is too strict!
  # if(MSVC)
  #   string(APPEND CMAKE_CXX_FLAGS " /WX")
  # endif()

  list(APPEND INC_SYS
    ${WINTAB_INC}
  )

  list(APPEND SRC
    intern/GHOST_ContextD3D.cc
    intern/GHOST_DropTargetWin32.cc
    intern/GHOST_SystemWin32.cc
    intern/GHOST_TrackpadWin32.cc
    intern/GHOST_WindowWin32.cc
    intern/GHOST_Wintab.cc

    intern/GHOST_ContextD3D.hh
    intern/GHOST_DropTargetWin32.hh
    intern/GHOST_SystemWin32.hh
    intern/GHOST_TaskbarWin32.hh
    intern/GHOST_TrackpadWin32.hh
    intern/GHOST_WindowWin32.hh
    intern/GHOST_Wintab.hh
  )

  list(APPEND SRC
    intern/GHOST_ContextWGL.cc

    intern/GHOST_ContextWGL.hh
  )

  if(WITH_INPUT_IME)
    list(APPEND SRC
      intern/GHOST_ImeWin32.cc

      intern/GHOST_ImeWin32.hh
    )
  endif()

  if(WITH_INPUT_NDOF)
    list(APPEND SRC
      intern/GHOST_NDOFManagerWin32.cc

      intern/GHOST_NDOFManagerWin32.hh
    )
  endif()
endif()

if(UNIX AND NOT APPLE)
  if(WITH_OPENGL_BACKEND)
    list(APPEND SRC
      intern/GHOST_ContextEGL.cc

      intern/GHOST_ContextEGL.hh
    )
  endif()
endif()

if(APPLE)
  if(WITH_METAL_BACKEND)
    list(APPEND SRC
      intern/GHOST_ContextMTL.mm

      intern/GHOST_ContextMTL.hh
    )
  endif()
endif()

if(APPLE)
  list(APPEND SRC
    intern/GHOST_SystemPathsCocoa.hh
    intern/GHOST_SystemPathsCocoa.mm
  )

elseif(UNIX)
  list(APPEND SRC
    intern/GHOST_SystemPathsUnix.cc
    intern/GHOST_SystemPathsUnix.hh
  )

  if(NOT WITH_INSTALL_PORTABLE)
    add_definitions(-DPREFIX="${CMAKE_INSTALL_PREFIX}")
  endif()

elseif(WIN32)
  list(APPEND SRC
    intern/GHOST_SystemPathsWin32.cc
    intern/GHOST_SystemPathsWin32.hh
  )

  list(APPEND INC
    ../utfconv
  )

endif()

if(WITH_XR_OPENXR)
  list(APPEND SRC
    intern/GHOST_Xr.cc
    intern/GHOST_XrAction.cc
    intern/GHOST_XrContext.cc
    intern/GHOST_XrControllerModel.cc
    intern/GHOST_XrEvent.cc
    intern/GHOST_XrGraphicsBinding.cc
    intern/GHOST_XrSession.cc
    intern/GHOST_XrSwapchain.cc

    GHOST_IXrContext.hh
    intern/GHOST_IXrGraphicsBinding.hh
    intern/GHOST_XrAction.hh
    intern/GHOST_XrContext.hh
    intern/GHOST_XrControllerModel.hh
    intern/GHOST_XrException.hh
    intern/GHOST_XrSession.hh
    intern/GHOST_XrSwapchain.hh
    intern/GHOST_Xr_intern.hh
    intern/GHOST_Xr_openxr_includes.hh

    # Header only library.
    ../../extern/tinygltf/tiny_gltf.h
  )
  if(APPLE)
    list(APPEND SRC
      intern/GHOST_XrGraphicsBindingMetal.mm

      intern/GHOST_XrGraphicsBindingMetal.hh
    )
  endif()

  if(WIN32)
    list(APPEND SRC
      intern/GHOST_XrGraphicsBindingD3D.cc

      intern/GHOST_XrGraphicsBindingD3D.hh
    )
  endif()

  list(APPEND INC_SYS
    ../../extern/json/include
    ../../extern/tinygltf
  )
  list(APPEND INC_SYS
    ${XR_OPENXR_SDK_INCLUDE_DIR}
  )
  list(APPEND LIB
    ${XR_OPENXR_SDK_LIBRARIES}
    PRIVATE bf::dependencies::eigen
  )

  set(XR_PLATFORM_DEFINES
  )
  if(WITH_OPENGL_BACKEND)
    list(APPEND XR_PLATFORM_DEFINES -DXR_USE_GRAPHICS_API_OPENGL)
  endif()
  if(WITH_VULKAN_BACKEND)
    list(APPEND XR_PLATFORM_DEFINES -DXR_USE_GRAPHICS_API_VULKAN)
    list(APPEND SRC
      intern/GHOST_XrGraphicsBindingVulkan.cc

      intern/GHOST_XrGraphicsBindingVulkan.hh
    )
  endif()
  if(WITH_METAL_BACKEND)
    list(APPEND XR_PLATFORM_DEFINES -DXR_USE_GRAPHICS_API_METAL)
  endif()

  # Add compiler defines as required by the OpenXR specification.
  if(WIN32)
    list(APPEND XR_PLATFORM_DEFINES
      -DXR_USE_PLATFORM_WIN32
      -DXR_USE_GRAPHICS_API_D3D11
    )
    list(APPEND LIB
      shlwapi
    )
  elseif(UNIX AND NOT APPLE)
    list(APPEND XR_PLATFORM_DEFINES -DXR_OS_LINUX)
    if(WITH_GHOST_WAYLAND)
      list(APPEND XR_PLATFORM_DEFINES -DXR_USE_PLATFORM_WAYLAND)
    endif()
    if(WITH_GHOST_X11)
      list(APPEND XR_PLATFORM_DEFINES -DXR_USE_PLATFORM_EGL)
      list(APPEND XR_PLATFORM_DEFINES -DXR_USE_PLATFORM_XLIB)
    endif()
  endif()

  add_definitions(-DWITH_XR_OPENXR ${XR_PLATFORM_DEFINES})

  unset(XR_PLATFORM_DEFINES)
endif()

blender_add_lib(bf_intern_ghost "${SRC}" "${INC}" "${INC_SYS}" "${LIB}")


--- intern/guardedalloc/CMakeLists.txt ---
# SPDX-FileCopyrightText: 2006 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

if(HAVE_MALLOC_STATS_H)
  add_definitions(-DHAVE_MALLOC_STATS_H)
endif()

if(WITH_MEM_VALGRIND)
  add_definitions(-DWITH_MEM_VALGRIND)
endif()

set(INC
  PUBLIC .
)

set(INC_SYS

)

set(SRC
  ./intern/leak_detector.cc
  ./intern/mallocn.cc
  ./intern/mallocn_guarded_impl.cc
  ./intern/mallocn_lockfree_impl.cc
  ./intern/memory_usage.cc

  MEM_guardedalloc.h
  ./intern/mallocn_inline.hh
  ./intern/mallocn_intern.hh
  ./intern/mallocn_intern_function_pointers.hh

  # only so the header is known by cmake
  ../atomic/atomic_ops.h
  ../atomic/intern/atomic_ops_ext.h
  ../atomic/intern/atomic_ops_msvc.h
  ../atomic/intern/atomic_ops_unix.h
  ../atomic/intern/atomic_ops_utils.h
)

set(LIB
  PRIVATE bf::intern::atomic
  PRIVATE bf::dependencies::pthreads
)

blender_add_lib(bf_intern_guardedalloc "${SRC}" "${INC}" "${INC_SYS}" "${LIB}")
add_library(bf::intern::guardedalloc ALIAS bf_intern_guardedalloc)

if(WITH_GTESTS)
  set(TEST_SRC
    tests/guardedalloc_alignment_test.cc
    tests/guardedalloc_overflow_test.cc
    tests/guardedalloc_test_base.h
  )
  set(TEST_INC
    ../../source/blender/blenlib
  )
  set(TEST_LIB
    bf_intern_guardedalloc
    bf_blenlib
  )
  blender_add_test_suite_executable(guardedalloc "${TEST_SRC}" "${INC};${TEST_INC}" "${INC_SYS}" "${LIB};${TEST_LIB}")
endif()


--- scripts/addons_core/node_wrangler/README.md ---
# Running Tests

```
./utils/paths_test.py
```


--- scripts/addons_core/bl_pkg/readme.rst ---

##########
Extensions
##########

Extensions Source Code Overview
===============================

Add-on: Blender Modules
-----------------------

- ``__init__.py``
  Add-on containing, preferences, ``bpy.app.handlers`` that respond to adding/removing repositories.

- ``bl_extension_ui.py``
  Defines the extensions UI, select between add-ons, themes, tag-filtering.

- ``bl_extension_ops.py``
  Defines extension operators, this is the main entry point for extension logic (except notifications, see below).

  This module defines a mechanism for a modal operator to run commands
  defined in ``cli/blender_ext.py`` as sub-processes, monitoring their progress
  (via STDOUT, see :ref:`Inter process communication (IPC) <IPC>`),
  see the ``_ExtCmdMixIn`` class.
  Actions such a as downloading, installing, updating are supported by calling into lower level functions,
  ``cli/blender_ext.py`` does the actual work.

  There are also some operators for the UI (changing tags, allowing online access).

- ``bl_extension_notify.py``

  This module checks for updates and is intended to run in the background.

  Checking for updates uses ``bl_extension_utils.CommandBatch`` from a timer.
  Checking for updates from a modal-operator is avoided since Blender may cancel
  modal operators when loading a file for example.

  The status bar is refreshed if/when updates are found.

- ``bl_extension_cli.py``

  The command line interface to support: ``blender -c extension ...``

  Some commands operate on Blender's preferences (for adding/removing repositories),
  other commands such as building packages are forwarded to ``cli/blender_ext.py``.

Add-on: Other Scripts
---------------------

- ``bl_extension_utils.py``

  This module contains various utilities,

  Note that use of ``bpy`` is intentionally avoided here,
  state from preferences or operators is passed in.

  - Generic shared utility functions.

  - Command line sub-process supervisor (``CommandBatch``)
    used by Blender operators to call ``cli/blender_ext.py`` and its sub-commands (see doc-string for details).

  - A view on the repositories JSON/TOML data what abstracts the file IO (``RepoCacheStore``).

  - A locking context to prevent multiple Blender instances operating on the same repository at once.

- ``cli/blender_ext.py``
  This command is responsible for operations on the repository,
  primarily downloading & installing extensions.

  It also contains functions to build & validate packages & create a static repository.

  This script typically runs as an external process using the ``subprocess`` module
  called from ``bl_extension_utils.py``.
  In some cases ``bl_extension_cli.py`` forwards sub-commands directly to this script.

  :ref:`Inter process communication (IPC) <IPC>` via Python's ``subprocess`` module
  which runs this script using Blender's bundled Python executable.
  Signals are used to interrupt the process, pipes are used to read it's output.

  This is done so Blender's UI can show the status of each command.

- ``extensions_map_from_legacy_addons``

  This is more of a data file used so users with legacy add-ons can update them to extensions.

Other Modules
-------------

Some functionality that's relevant to the extensions system is implemented in other modules,
as it relates to how extensions are loaded by Blender.

*Paths are relative to Blender's source tree.*

``./scripts/modules/_bpy_internal/extensions/junction_module.py``
   This stand-alone module allows extensions to appear as if they are all loaded
   from a single *package* independent of their file-system location.

   This is done so extensions don't pollute the module name-space
   (avoiding naming collisions with packages downloaded from ``https://pypi.org``).

   So each extension's add-on ID follows this format: ``bl_ext.{repository_id}.{extension_id}``.

``./scripts/modules/_bpy_internal/extensions/wheel_manager.py``
   Extensions may include Python modules as wheels,
   these are extracted into an a site-packages directory that is specific to the extensions for this version of Blender.
   ``~/.config/blender/X.X/extensions/.local/lib/python3.XX/site-packages/``.

   Once extensions have been installed the list of wheels from each extensions ``blender_manifest.toml``
   is combined and passed in to the main "apply_action" function which will install/uninstall wheels as needed.

   Unfortunately there is no special handling for version conflicts.
   When different versions of the same wheel are found, the latest version is installed.
   This may break any extensions depending on the old version of a wheel.

``./scripts/modules/_bpy_internal/extensions/stale_file_manager.py``
   On MS-Windows it's common that files are locked and can't be deleted
   (any DLL's loaded into memory),
   although it can also happen if other processes are scanning the file system.

   In this case, the file is marked as stale and queued for removal when Blender next starts.

   Unfortunately **upgrading** extensions that use DLL's on MS-Windows isn't
   reliable because it is necessary to remove then re-create the extension.
   This is an area that could use further development it may be necessary to
   support installing on restart.

``./scripts/modules/_bpy_internal/extensions/{tags,permissions}.py``
   These are definition lists used when building packages,
   they are ``https://extensions.blender.org`` specific.


C++ Sources
-----------

``./source/blender/makesdna/DNA_userdef_types.h``
   The repository definition (``bUserExtensionRepo``).

``./source/blender/editors/space_userpref/userpref_ops.cc``
   Operators for adding/removing repositories as well as dropping URL's to initiate installation.

``./source/blender/python/intern/bpy_app_handlers.cc``
   Handlers for extensions ``bpy.app.handlers._extension_repos_*``,
   *note the leading underscore as they are not part of the public API.*

   Unfortunately these handlers were needed as a way for Python to hook into lower level code paths,
   so it's possible (for example) to refresh the extensions from an RNA update function
   (``rna_userdef.cc`` and the operators in some cases).

``wmWindowManager::extensions_updates`` & ``extensions_blocked``
   Status bar drawing uses these values set by ``bl_extension_notify.py``.


Functionality Described
=======================

This section describes how functionality has been implemented.


Extension Pre-Flight Compatibility Check
----------------------------------------

Since extensions may be from a shared system directory or imported from an older installation
it's necessary to ensure the extension is compatible with Blender on startup.

An extension may be incompatible for various reasons,

- Unsupported Blender version.
- Unsupported platform.
- Binary incompatibility (from it's wheels).
- The extension may also have been block-listed.

Since this runs on every startup, expensive checks are avoided if at all possible.

In ``./scripts/modules/addon_utils.py`` the private function ``_initialize_extensions_compat_ensure_up_to_date``
is responsible for ensuring extensions are compatible before loading.

- On startup run: ``_initialize_extensions_repos_once`` which sets up repositories and handlers.
- If the "compatibility cache" doesn't exist it is created (each extension's TOML file is inspected for compatibility).
  A dictionary of incompatible extensions is stored in the compatibility cache which is checked
  whenever ``addon_utils.enable(..)`` is used to enable an extension.
- If the "compatibility cache" exists it is validated by each extensions TOML modification-time & size,
  re-generating upon any changes.
- The compatibility data stores the reason the extension is disabled,
  this is reported if the user attempts to enable it.


The details of the compatibility cache are documented in ``addon_utils``,
it's a simple format that stores the Blender version & a magic number that can be bumped at any time,
changes to these files cause the cache to be re-generated.


Dragging & Dropping a URL
-------------------------

Extensions drag & drop is handled with Blender's drop-boxes.
This works in much the same way as dropping images in the 3D viewport or blend files.

There are two drop-boxes used, one for file-paths another for URL's.
Both check the path contains a ``.zip`` extension,
where the URL logic needs to strips the query string and the fragment from the URL.

The drop action runs the operator ``extensions.package_install`` (from ``bl_extension_ops.py``)
which checks if the ``url`` property has been set.
If so, the code-path for dropping a URL is activated.

Once drop is activated:

- A URL is scanned for blender version range & platform compatibility
  to prevent downloading & attempting to install extensions which aren't compatible.

  Note that the query parameters in the URL are optional and serve as a convenient way to fail-early,
  if they're not present the user may be prompted to add a new repository only to discover
  later that extension they dropped isn't compatible with their Blender version.

  Both ``extensions.blender.org`` and static sites
  generated by ``blender -c extension server-generate`` set these parameters.

- A file-path is considered "local" so its manifest is inspected to check it's compatible.

Other checks are performed to ensure the repository exists locally.
If the extension isn't found to be incompatible, the user may install it.

Dropping a URL may prompt the user for actions that need to be done before the extensions may be installed.

- Dropping a URL with "Online Access" disabled prompts the user to enable online-access.
- Dropping a URL from an unknown remote repository prompts the user to add the repository.
- Otherwise, dropping the URL of a compatible extension will prompt the user to install the extension.

Unfortunately chaining popups together (setup wizard) or merging popups together is not well supported in Blender.
Causing some fairly bad worst-case scenarios when dropping a URL which isn't part of a known repository.


Implementation Details
======================

Extension Format
----------------

Extensions are intended to be created with the ``blender -c extension build``
command which creates a ZIP file and performs some checks to catch errors early.

The ZIP file must contain a ``blender_manifest.toml`` (which may be in a directory),
as well as files for a Python package for add-ons or an XML for themes.


Repositories
------------

Information about repositories is stored in user preferences.
The main values are a unique name, module path & optionally a remote URL.

There are 2 kinds of repositories:

- **Remote** which can be synchronized for updates.
- **Local** where the repository is a file-system location.

  Local repositories may define a source:

  - **User** the user may add/remove extensions to this location.
  - **System** this treated as read-only and may be used when extensions
    are shared on a network file-system for example.

Synchronizing a **Remote** repository simply downloads the JSON listing from the remote URL.

Once extensions have been installed their TOML files are compared with the repository to check for updates.

.. _IPC:

Inter Process Communication (IPC)
---------------------------------

- Commands that manipulate extensions (such as updating/installing/removing)
  are performed by the stand-alone script ``cli/blender_ext.py``.
- Using IPC means these commands can run in the background without blocking Blender's GUI.
- The state of the extensions repository (repository location, blender-version, API tokens etc)
  are passed in via command line arguments.
- This can be configured to only output JSON messages to the STDOUT which Blender parses and uses
  to send feedback to the user.
- Progress (such as percentage of a file downloaded) is sent to the STDOUT
  so the GUI and command-line interface can show progress.
- Input is limited to the request to cancel
  (if the user cancels the operator or presses Control-C on the command line).
- Internally functions are responsible for checking if the user has requested to exit.
  This is especially important before IO or anything that could cause the process to wait
  so as to avoid "hanging" once the user has requested to exit.

All IPC is handled by ``bl_extension_utils.CommandBatch`` which can run multiple commands,
a common case is running multiple updates at once.

The caller can use methods on the CommandBatch to access the status and report any problems.


Tooling
=======

The tests are not yet integrated into CTest
because some tests depend on the ``wheel`` module (not distributed with Blender's Python).

Tests can be run via the ``Makefile`` using the local Python::

   make -C scripts/addons_core/bl_pkg test

Run the ``help`` target for a list of convenience targets to run checkers & tests.


--- scripts/templates_py/addon_add_object.py ---
# To make this add-on installable, create an extension with it:
# https://docs.blender.org/manual/en/latest/advanced/extensions/getting_started.html

import bpy
from bpy.types import Operator
from bpy.props import FloatVectorProperty
from bpy_extras.object_utils import AddObjectHelper, object_data_add
from mathutils import Vector


def add_object(self, context):
    scale_x = self.scale.x
    scale_y = self.scale.y

    verts = [
        Vector((-1 * scale_x, 1 * scale_y, 0)),
        Vector((1 * scale_x, 1 * scale_y, 0)),
        Vector((1 * scale_x, -1 * scale_y, 0)),
        Vector((-1 * scale_x, -1 * scale_y, 0)),
    ]

    edges = []
    faces = [[0, 1, 2, 3]]

    mesh = bpy.data.meshes.new(name="New Object Mesh")
    mesh.from_pydata(verts, edges, faces)
    # useful for development when the mesh may be invalid.
    # mesh.validate(verbose=True)
    object_data_add(context, mesh, operator=self)


class OBJECT_OT_add_object(Operator, AddObjectHelper):
    """Create a new Mesh Object"""
    bl_idname = "mesh.add_object"
    bl_label = "Add Mesh Object"
    bl_options = {'REGISTER', 'UNDO'}

    scale: FloatVectorProperty(
        name="scale",
        default=(1.0, 1.0, 1.0),
        subtype='TRANSLATION',
        description="scaling",
    )

    def execute(self, context):

        add_object(self, context)

        return {'FINISHED'}


# Registration

def add_object_button(self, context):
    self.layout.operator(
        OBJECT_OT_add_object.bl_idname,
        text="Add Object",
        icon='PLUGIN',
    )


# This allows you to right click on a button and link to documentation
def add_object_manual_map():
    url_manual_prefix = "https://docs.blender.org/manual/en/latest/"
    url_manual_mapping = (
        ("bpy.ops.mesh.add_object", "scene_layout/object/types.html"),
    )
    return url_manual_prefix, url_manual_mapping


def register():
    bpy.utils.register_class(OBJECT_OT_add_object)
    bpy.utils.register_manual_map(add_object_manual_map)
    bpy.types.VIEW3D_MT_mesh_add.append(add_object_button)


def unregister():
    bpy.utils.unregister_class(OBJECT_OT_add_object)
    bpy.utils.unregister_manual_map(add_object_manual_map)
    bpy.types.VIEW3D_MT_mesh_add.remove(add_object_button)


if __name__ == "__main__":
    register()


--- scripts/templates_py/background_job.py ---
# This script is an example of how you can run blender from the command line
# (in background mode with no interface) to automate tasks, in this example it
# creates a text object, camera and light, then renders and/or saves it.
# This example also shows how you can parse command line options to scripts.
#
# Example usage for this test.
#  blender --background --factory-startup --python $HOME/background_job.py -- \
#          --text="Hello World" \
#          --render="/tmp/hello" \
#          --save="/tmp/hello.blend"
#
# Notice:
# `--factory-startup` is used to avoid the user default settings from
#                     interfering with automated scene generation.
#
# `--` causes blender to ignore all following arguments so python can use them.
#
# See blender --help for details.


import bpy


def example_function(text, save_path, render_path):
    # Clear existing objects.
    bpy.ops.wm.read_factory_settings(use_empty=True)

    scene = bpy.context.scene

    txt_data = bpy.data.curves.new(name="MyText", type='FONT')

    # Text Object
    txt_ob = bpy.data.objects.new(name="MyText", object_data=txt_data)
    scene.collection.objects.link(txt_ob)   # add the data to the scene as an object
    txt_data.body = text         # the body text to the command line arg given
    txt_data.align_x = 'CENTER'  # center text

    # Camera
    cam_data = bpy.data.cameras.new("MyCam")
    cam_ob = bpy.data.objects.new(name="MyCam", object_data=cam_data)
    scene.collection.objects.link(cam_ob)  # instance the camera object in the scene
    scene.camera = cam_ob       # set the active camera
    cam_ob.location = 0.0, 0.0, 10.0

    # Light
    light_data = bpy.data.lights.new("MyLight", 'POINT')
    light_ob = bpy.data.objects.new(name="MyLight", object_data=light_data)
    scene.collection.objects.link(light_ob)
    light_ob.location = 2.0, 2.0, 5.0

    bpy.context.view_layer.update()

    if save_path:
        bpy.ops.wm.save_as_mainfile(filepath=save_path)

    if render_path:
        render = scene.render
        render.use_file_extension = True
        render.filepath = render_path
        bpy.ops.render.render(write_still=True)


def main():
    import sys       # to get command line args
    import argparse  # to parse options for us and print a nice help message

    # get the args passed to blender after "--", all of which are ignored by
    # blender so scripts may receive their own arguments
    argv = sys.argv

    if "--" not in argv:
        argv = []  # as if no args are passed
    else:
        argv = argv[argv.index("--") + 1:]  # get all args after "--"

    # When --help or no args are given, print this help
    usage_text = (
        "Run blender in background mode with this script:"
        "  blender --background --python " + __file__ + " -- [options]"
    )

    parser = argparse.ArgumentParser(description=usage_text)

    # Example utility, add some text and renders or saves it (with options)
    # Possible types are: string, int, long, choice, float and complex.
    parser.add_argument(
        "-t", "--text", dest="text", type=str, required=True,
        help="This text will be used to render an image",
    )

    parser.add_argument(
        "-s", "--save", dest="save_path", metavar='FILE',
        help="Save the generated file to the specified path",
    )
    parser.add_argument(
        "-r", "--render", dest="render_path", metavar='FILE',
        help="Render an image to the specified path",
    )

    args = parser.parse_args(argv)  # In this example we won't use the args

    if not argv:
        parser.print_help()
        return

    if not args.text:
        print("Error: --text=\"some string\" argument not given, aborting.")
        parser.print_help()
        return

    # Run the example function
    example_function(args.text, args.save_path, args.render_path)

    print("batch job finished, exiting")


if __name__ == "__main__":
    main()


--- scripts/templates_py/batch_export.py ---
# exports each selected object into its own file

import bpy
import os

# export to blend file location
basedir = os.path.dirname(bpy.data.filepath)

if not basedir:
    raise Exception("Blend file is not saved")

view_layer = bpy.context.view_layer

obj_active = view_layer.objects.active
selection = bpy.context.selected_objects

bpy.ops.object.select_all(action='DESELECT')

for obj in selection:

    obj.select_set(True)

    # some exporters only use the active object
    view_layer.objects.active = obj

    name = bpy.path.clean_name(obj.name)
    fn = os.path.join(basedir, name)

    bpy.ops.export_scene.fbx(filepath=fn + ".fbx", use_selection=True)

    # Can be used for multiple formats
    # bpy.ops.export_scene.x3d(filepath=fn + ".x3d", use_selection=True)

    obj.select_set(False)

    print("written:", fn)


view_layer.objects.active = obj_active

for obj in selection:
    obj.select_set(True)


--- scripts/templates_py/bmesh_simple.py ---
# This example assumes we have a mesh object selected

import bpy
import bmesh

# Get the active mesh
me = bpy.context.object.data


# Get a BMesh representation
bm = bmesh.new()   # create an empty BMesh
bm.from_mesh(me)   # fill it in from a Mesh


# Modify the BMesh, can do anything here...
for v in bm.verts:
    v.co.x += 1.0


# Finish up, write the bmesh back to the mesh
bm.to_mesh(me)
bm.free()  # free and prevent further access


--- scripts/templates_py/bmesh_simple_editmode.py ---
# This example assumes we have a mesh object in edit-mode

import bpy
import bmesh

# Get the active mesh
obj = bpy.context.edit_object
me = obj.data


# Get a BMesh representation
bm = bmesh.from_edit_mesh(me)

bm.faces.active = None

# Modify the BMesh, can do anything here...
for v in bm.verts:
    v.co.x += 1.0


# Show the updates in the viewport
# and recalculate n-gon tessellation.
bmesh.update_edit_mesh(me, loop_triangles=True)


--- scripts/templates_py/builtin_keyingset.py ---
import bpy


class BUILTIN_KSI_hello(bpy.types.KeyingSetInfo):
    bl_label = "Hello World KeyingSet"

    # poll - test for whether Keying Set can be used at all
    def poll(ksi, context):
        return context.active_object or context.selected_objects

    # iterator - go over all relevant data, calling generate()
    def iterator(ksi, context, ks):
        for ob in context.selected_objects:
            ksi.generate(context, ks, ob)

    # generator - populate Keying Set with property paths to use
    def generate(ksi, context, ks, data):
        id_block = data.id_data

        ks.paths.add(id_block, "location")

        for i in range(5):
            ks.paths.add(id_block, "layers", i, group_method='NAMED', group_name="5x Hello Layers")

        ks.paths.add(id_block, "show_in_front", group_method='NONE')


def register():
    bpy.utils.register_class(BUILTIN_KSI_hello)


def unregister():
    bpy.utils.unregister_class(BUILTIN_KSI_hello)


if __name__ == '__main__':
    register()


--- scripts/templates_py/custom_nodes.py ---
import bpy
from bpy.types import NodeTree, Node, NodeSocket, NodeTreeInterfaceSocket
from bl_ui import node_add_menu

# Implementation of custom nodes from Python


# Derived from the NodeTree base type, similar to Menu, Operator, Panel, etc.
class MyCustomTree(NodeTree):
    # Description string
    '''A custom node tree type that will show up in the editor type list'''
    # Optional identifier string. If not explicitly defined, the python class name is used.
    bl_idname = 'CustomTreeType'
    # Label for nice name display
    bl_label = "Custom Node Tree"
    # Icon identifier
    bl_icon = 'NODETREE'


# Custom socket type
class MyCustomSocket(NodeSocket):
    # Description string
    """Custom node socket type"""
    # Optional identifier string. If not explicitly defined, the python class name is used.
    bl_idname = 'CustomSocketType'
    # Label for nice name display
    bl_label = "Custom Node Socket"

    input_value: bpy.props.FloatProperty(
        name="Value",
        description="Value when the socket is not connected",
    )

    # Optional function for drawing the socket input value
    def draw(self, context, layout, node, text):
        if self.is_output or self.is_linked:
            layout.label(text=text)
        else:
            layout.prop(self, "input_value", text=text)

    # Socket color
    @classmethod
    def draw_color_simple(cls):
        return (1.0, 0.4, 0.216, 0.5)


# Customizable interface properties to generate a socket from.
class MyCustomInterfaceSocket(NodeTreeInterfaceSocket):
    # The type of socket that is generated.
    bl_socket_idname = 'CustomSocketType'

    default_value: bpy.props.FloatProperty(default=1.0, description="Default input value for new sockets",)

    def draw(self, context, layout):
        # Display properties of the interface.
        layout.prop(self, "default_value")

    # Set properties of newly created sockets
    def init_socket(self, node, socket, data_path):
        socket.input_value = self.default_value

    # Use an existing socket to initialize the group interface
    def from_socket(self, node, socket):
        # Current value of the socket becomes the default
        self.default_value = socket.input_value


# Mix-in class for all custom nodes in this tree type.
# Defines a poll function to enable instantiation.
class MyCustomTreeNode:
    @classmethod
    def poll(cls, ntree):
        return ntree.bl_idname == 'CustomTreeType'


# Derived from the Node base type.
class MyCustomNode(MyCustomTreeNode, Node):
    # === Basics ===
    # Description string
    '''A custom node'''
    # Optional identifier string. If not explicitly defined, the python class name is used.
    bl_idname = 'CustomNodeType'
    # Label for nice name display
    bl_label = "Custom Node"
    # Icon identifier
    bl_icon = 'SOUND'

    # === Custom Properties ===
    # These work just like custom properties in ID data blocks
    # Extensive information can be found under
    # https://docs.blender.org/api/current/bpy.props.html
    my_string_prop: bpy.props.StringProperty()
    my_float_prop: bpy.props.FloatProperty(default=3.1415926)

    # === Optional Functions ===
    # Initialization function, called when a new node is created.
    # This is the most common place to create the sockets for a node, as shown below.
    # NOTE: this is not the same as the standard __init__ function in Python, which is
    #       a purely internal Python method and unknown to the node system!
    def init(self, context):
        self.inputs.new('CustomSocketType', "Hello")
        self.inputs.new('NodeSocketFloat', "World")
        self.inputs.new('NodeSocketVector', "!", use_multi_input=True)

        self.outputs.new('NodeSocketColor', "How")
        self.outputs.new('NodeSocketColor', "are")
        self.outputs.new('NodeSocketFloat', "you")

    # Copy function to initialize a copied node from an existing one.
    def copy(self, node):
        print("Copying from node ", node)

    # Free function to clean up on removal.
    def free(self):
        print("Removing node ", self, ", Goodbye!")

    # Additional buttons displayed on the node.
    def draw_buttons(self, context, layout):
        layout.label(text="Node settings")
        layout.prop(self, "my_float_prop")

    # Detail buttons in the sidebar.
    # If this function is not defined, the draw_buttons function is used instead
    def draw_buttons_ext(self, context, layout):
        layout.prop(self, "my_float_prop")
        # my_string_prop button will only be visible in the sidebar
        layout.prop(self, "my_string_prop")

    # Optional: custom label
    # Explicit user label overrides this, but here we can define a label dynamically
    def draw_label(self):
        return "I am a custom node"


# Add custom nodes to the Add & Swap menu.
def draw_add_menu(self, context):
    layout = self.layout
    if context.space_data.tree_type != MyCustomTree.bl_idname:
        # Avoid adding nodes to built-in node tree
        return
    # Add nodes to the layout. Can use submenus, separators, etc. as in any other menu.
    self.node_operator(layout, "CustomNodeType")


classes = (
    MyCustomTree,
    MyCustomSocket,
    MyCustomInterfaceSocket,
    MyCustomNode,
)


def register():
    from bpy.utils import register_class
    for cls in classes:
        register_class(cls)

    bpy.types.NODE_MT_add.append(draw_add_menu)
    bpy.types.NODE_MT_swap.append(draw_add_menu)


def unregister():
    bpy.types.NODE_MT_add.remove(draw_add_menu)
    bpy.types.NODE_MT_swap.remove(draw_add_menu)

    from bpy.utils import unregister_class
    for cls in reversed(classes):
        unregister_class(cls)


if __name__ == "__main__":
    register()


--- scripts/templates_py/driver_functions.py ---
# This script defines functions to be used directly in driver expressions to
# extend the built-in set of python functions.
#
# This can be executed on manually or set to "Register" to
# initialize the functions on file load.


# two sample functions
def invert(f):
    """ Simple function call:

            invert(val)
    """
    return 1.0 - f


uuid_store = {}


def slow_value(value, fac, uuid):
    """ Delay the value by a factor, use a unique string to allow
        use in multiple drivers without conflict:

            slow_value(val, 0.5, "my_value")
    """
    value_prev = uuid_store.get(uuid, value)
    uuid_store[uuid] = value_new = (value_prev * fac) + (value * (1.0 - fac))
    return value_new


import bpy

# Add functions defined in this script into the drivers namespace.
bpy.app.driver_namespace["invert"] = invert
bpy.app.driver_namespace["slow_value"] = slow_value


--- source/CMakeLists.txt ---
# SPDX-FileCopyrightText: 2006 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

if(WITH_LEGACY_OPENGL)
  add_definitions(-DWITH_LEGACY_OPENGL)
endif()

if(WITH_CLANG_TIDY AND NOT MSVC)
  find_package(ClangTidy REQUIRED)
  set(CLANG_TIDY_EXTRA_ARGS --extra-arg=-Wno-error=unknown-warning-option)

  set(CMAKE_C_CLANG_TIDY ${CLANG_TIDY_EXECUTABLE};${CLANG_TIDY_EXTRA_ARGS})
  set(CMAKE_CXX_CLANG_TIDY ${CLANG_TIDY_EXECUTABLE};${CLANG_TIDY_EXTRA_ARGS})
  set(CMAKE_OBJC_CLANG_TIDY ${CLANG_TIDY_EXECUTABLE};${CLANG_TIDY_EXTRA_ARGS})
  set(CMAKE_OBJCXX_CLANG_TIDY ${CLANG_TIDY_EXECUTABLE};${CLANG_TIDY_EXTRA_ARGS})
endif()

add_subdirectory(blender)


--- source/blender/CMakeLists.txt ---
# SPDX-FileCopyrightText: 2006 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

set(SRC_DNA_INC
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_ID.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_ID_enums.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_action_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_anim_enums.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_anim_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_armature_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_asset_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_attribute_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_boid_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_brush_enums.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_brush_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_cachefile_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_camera_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_cloth_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_collection_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_color_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_colorband_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_constraint_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_curve_enums.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_curve_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_curveprofile_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_curves_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_customdata_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_defs.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_documentation.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_dynamicpaint_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_effect_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_fileglobal_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_fluid_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_freestyle_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_genfile.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_gpencil_legacy_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_gpencil_modifier_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_gpu_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_grease_pencil_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_image_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_key_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_lattice_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_layer_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_light_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_lightprobe_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_lineart_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_linestyle_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_listBase.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_mask_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_material_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_mesh_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_meshdata_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_meta_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_modifier_enums.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_modifier_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_movieclip_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_nla_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_node_tree_interface_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_node_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_object_enums.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_object_fluidsim_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_object_force_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_object_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_outliner_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_packedFile_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_particle_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_pointcache_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_pointcloud_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_rigidbody_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_scene_enums.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_scene_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_screen_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_sdna_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_sequence_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_session_uid_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_shader_fx_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_sound_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_space_enums.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_space_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_speaker_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_text_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_texture_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_theme_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_tracking_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_userdef_enums.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_userdef_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_uuid_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_vec_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_vfont_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_view2d_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_view3d_enums.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_view3d_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_viewer_path_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_volume_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_windowmanager_enums.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_windowmanager_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_workspace_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_world_types.h
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_xr_types.h
)

# Utility & other headers.
set(SRC_DNA_OTHER_INC
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_array_utils.hh
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_print.hh
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_sdna_type_ids.hh
  ${CMAKE_CURRENT_SOURCE_DIR}/makesdna/DNA_vec_defaults.h
)

add_subdirectory(datatoc)
add_subdirectory(gpu/shader_tool)
add_subdirectory(editors)
add_subdirectory(windowmanager)
add_subdirectory(animrig)
add_subdirectory(asset_system)
add_subdirectory(blenkernel)
add_subdirectory(blenlib)
add_subdirectory(bmesh)
add_subdirectory(draw)
add_subdirectory(draw/intern/shaders)
add_subdirectory(draw/engines/eevee/shaders)
add_subdirectory(draw/engines/gpencil/shaders)
add_subdirectory(draw/engines/overlay/shaders)
add_subdirectory(draw/engines/workbench/shaders)
add_subdirectory(render)
add_subdirectory(blenfont)
add_subdirectory(blentranslation)
add_subdirectory(blenloader)
add_subdirectory(blenloader_core)
add_subdirectory(depsgraph)
add_subdirectory(ikplugin)
add_subdirectory(simulation)
add_subdirectory(geometry)
add_subdirectory(gpu)
add_subdirectory(gpu/shaders)
add_subdirectory(gpu/tests/shaders)
add_subdirectory(imbuf)
add_subdirectory(imbuf/intern/oiio)
add_subdirectory(nodes)
add_subdirectory(modifiers)
add_subdirectory(sequencer)
add_subdirectory(shader_fx)
add_subdirectory(io)
add_subdirectory(functions)
add_subdirectory(makesdna)
add_subdirectory(makesrna)
add_subdirectory(compositor)
add_subdirectory(compositor/shaders)

if(WITH_BLENDER_THUMBNAILER)
  add_subdirectory(blendthumb)
endif()


if(WITH_IMAGE_OPENEXR)
  add_subdirectory(imbuf/intern/openexr)
endif()

if(WITH_IMAGE_CINEON)
  add_subdirectory(imbuf/intern/cineon)
endif()

if(WITH_PYTHON)
  add_subdirectory(python)
endif()

if(WITH_FREESTYLE)
  add_subdirectory(freestyle)
endif()

if(WITH_CPU_CHECK)
  add_subdirectory(cpucheck)
endif()


--- source/creator/CMakeLists.txt ---
# SPDX-FileCopyrightText: 2006 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

set(INC
  ../blender/editors/include
  ../blender/io/usd
  ../blender/makesrna
)

set(LIB
  PRIVATE bf::blenkernel
  PRIVATE bf::blenlib
  PRIVATE bf::bmesh
  PRIVATE bf::depsgraph
  PRIVATE bf::dna
  PRIVATE bf::gpu
  PRIVATE bf::imbuf
  PRIVATE bf::imbuf::movie
  PRIVATE bf::intern::clog
  PRIVATE bf::intern::guardedalloc
  PRIVATE bf::render
  PRIVATE bf::sequencer
  PRIVATE bf::windowmanager
  PRIVATE bf::dependencies::optional::opencolorio
)

if(HAVE_FEENABLEEXCEPT)
  add_definitions(-DHAVE_FEENABLEEXCEPT)
endif()

if(WITH_STRSIZE_DEBUG)
  add_definitions(-DWITH_STRSIZE_DEBUG)
endif()

if(WITH_TBB)
  # Force TBB libraries to be in front of MKL (part of `OpenImageDenoise`), so
  # that it is initialized before MKL and static library initialization order issues are avoided.
  #
  # This isn't fully robust but seems to work.
  list(INSERT LIB 0 ${TBB_LIBRARIES})
  list(INSERT LIB 0 bf_blenkernel)
endif()

if(WIN32)
  list(APPEND INC ../../intern/utfconv)
endif()

if(WITH_LIBMV)
  list(APPEND INC ../../intern/libmv)
  add_definitions(-DWITH_LIBMV)
endif()

if(WITH_CYCLES)
  add_definitions(-DWITH_CYCLES)
  list(APPEND INC ../../intern/cycles/blender)
endif()

if(WITH_OPENGL_BACKEND)
  add_definitions(-DWITH_OPENGL_BACKEND)
endif()

if(WITH_VULKAN_BACKEND)
  add_definitions(-DWITH_VULKAN_BACKEND)
endif()

if(WITH_RENDERDOC)
  add_definitions(-DWITH_RENDERDOC)
endif()

if(WITH_CODEC_FFMPEG)
  add_definitions(-DWITH_FFMPEG)
endif()

if(WITH_TBB)
  list(APPEND INC ${TBB_INCLUDE_DIRS})
  if(WIN32)
    # For `pragma` that links `tbbmalloc_proxy.lib`.
    link_directories(${LIBDIR}/tbb/lib)
  endif()
endif()

if(WIN32)
  # Windows.h will define min/max macros that will collide with the STL versions.
  add_definitions(-DNOMINMAX)
endif()

if(WITH_USD)
  # USD links libMaterialX, when using pre-compiled libraries
  # ensures `usd_ms` can find `MaterialXRender` and friends.
  #
  # NOTE: This is _only_ needed when linking blender before the install target runs.
  # Once MATERIALX libraries have been copied into `TARGETDIR_LIB` then Blender will link.
  # Don't rely on this though as failing on a fresh build is no good and the library
  # files could get outdated too.
  if(DEFINED LIBDIR)
    link_directories(${LIBDIR}/materialx/lib)
  endif()
endif()

if(WITH_PYTHON)
  list(APPEND INC ../blender/python)
  add_definitions(-DWITH_PYTHON)

  if(WITH_PYTHON_SECURITY)
    add_definitions(-DWITH_PYTHON_SECURITY)
  endif()
endif()

if(WITH_HEADLESS)
  add_definitions(-DWITH_HEADLESS)
endif()

if(WITH_SDL)
  add_definitions(-DWITH_SDL)
endif()

if(WITH_BINRELOC)
  list(APPEND INC ${BINRELOC_INCLUDE_DIRS})
  add_definitions(-DWITH_BINRELOC)
endif()

if(WITH_FREESTYLE)
  list(APPEND INC ../blender/freestyle)
  add_definitions(-DWITH_FREESTYLE)
endif()

if(WITH_XR_OPENXR)
  add_definitions(-DWITH_XR_OPENXR)
endif()

if(WITH_GMP)
  list(APPEND INC ${GMP_INCLUDE_DIRS})
  add_definitions(-DWITH_GMP)
endif()

if(WITH_TBB_MALLOC_PROXY)
  add_definitions(-DWITH_TBB_MALLOC)
endif()

# Setup the EXE sources and `buildinfo`.
set(SRC
  creator.cc
  creator_args.cc
  creator_signals.cc

  creator_intern.h
)

if(CMAKE_GENERATOR MATCHES "^Visual Studio.+")
  # This helps visual studio find the debugger visualizers
  list(APPEND SRC ${CMAKE_SOURCE_DIR}/tools/utils_ide/natvis/Blender.natvis)
endif()

# MSVC 2010 gives linking errors with the manifest.
if(WIN32 AND NOT UNIX)
  add_definitions(
    -DBLEN_VER_RC_STR="${BLENDER_VERSION}"
    -DBLEN_VER_RC_1=${BLENDER_VERSION_MAJOR}
    -DBLEN_VER_RC_2=${BLENDER_VERSION_MINOR}
    -DBLEN_VER_RC_3=${BLENDER_VERSION_PATCH}
    -DBLEN_VER_RC_4=0
  )

  list(APPEND SRC
    ${CMAKE_SOURCE_DIR}/release/windows/icons/winblender.rc
  )

  if(NOT WITH_WINDOWS_EXTERNAL_MANIFEST)
    list(APPEND SRC
      ${CMAKE_BINARY_DIR}/blender.exe.manifest
    )
  endif()
endif()

if(WITH_BUILDINFO)
  add_definitions(-DWITH_BUILDINFO)
  # --------------------------------------------------------------------------
  # These defines could all be moved into the header below

  # Write strings into a separate header since we can escape C-strings
  # in a way that's not practical when passing defines.
  set(BUILD_PLATFORM "${CMAKE_SYSTEM_NAME}")
  set(BUILD_TYPE "${CMAKE_BUILD_TYPE}")
  set(BUILD_CFLAGS "${CMAKE_C_FLAGS}")
  set(BUILD_CXXFLAGS "${CMAKE_CXX_FLAGS}")
  set(BUILD_LINKFLAGS "${PLATFORM_LINKFLAGS}")
  set(BUILD_SYSTEM "CMake")

  if(WITH_COMPILER_SHORT_FILE_MACRO)
    # It's not necessary to include path information
    # about the system building Blender in the executable.
    string(REPLACE "${PLATFORM_CFLAGS_FMACRO_PREFIX_MAP}" " " BUILD_CFLAGS "${BUILD_CFLAGS}")
    string(REPLACE "${PLATFORM_CFLAGS_FMACRO_PREFIX_MAP}" " " BUILD_CXXFLAGS "${BUILD_CXXFLAGS}")
  endif()

  # Use `configure_file` instead of definitions since properly
  # escaping the multiple command line arguments which themselves
  # contain strings and spaces becomes overly error-prone & complicated.
  configure_file(
    "${CMAKE_SOURCE_DIR}/build_files/cmake/buildinfo_static.h.in"
    "${CMAKE_CURRENT_BINARY_DIR}/buildinfo_static.h"
    ESCAPE_QUOTES
    @ONLY
  )

  unset(BUILD_PLATFORM)
  unset(BUILD_TYPE)
  unset(BUILD_CFLAGS)
  unset(BUILD_CXXFLAGS)
  unset(BUILD_LINKFLAGS)
  unset(BUILD_SYSTEM)

  # --------------------------------------------------------------------------
  # Write header for values that change each build
  #
  # NOTE: generated file is in build directory `source/creator`
  # except when used as an include path.

  add_definitions(-DWITH_BUILDINFO_HEADER)

  # Include the output directory, where the `buildinfo.h` file is generated.
  include_directories(${CMAKE_CURRENT_BINARY_DIR})


  # XXX: `${buildinfo_h_fake}` is used here,
  # because we rely on that file being detected as missing
  # every build so that the real header `buildinfo.h` is updated.
  #
  # Keep this until we find a better way to resolve!

  set(buildinfo_h_real "${CMAKE_CURRENT_BINARY_DIR}/buildinfo.h")
  set(buildinfo_h_fake "${CMAKE_CURRENT_BINARY_DIR}/buildinfo.h_fake")

  if(EXISTS ${buildinfo_h_fake})
    message(
      FATAL_ERROR
      "File \"${buildinfo_h_fake}\" found, this should never be created, remove!"
    )
  endif()

  # From the CMAKE documentation "If the output of the custom command is not actually created as a
  # file on disk it should be marked with the SYMBOLIC source file property."
  #
  # Not doing this leads to build warnings for the not generated file on
  # MS-Windows when using `msbuild`.
  set_source_files_properties(${buildinfo_h_fake} PROPERTIES SYMBOLIC TRUE)

  # a custom target that is always built
  add_custom_target(
    buildinfo ALL
    DEPENDS ${buildinfo_h_fake}
  )

  # Creates `buildinfo.h` using CMAKE script.
  add_custom_command(
    OUTPUT
      ${buildinfo_h_fake}  # ensure we always run
      ${buildinfo_h_real}
    COMMAND
      ${CMAKE_COMMAND}
      -DSOURCE_DIR=${CMAKE_SOURCE_DIR}
      # Overrides only used when non-empty strings.
      -DBUILD_DATE=${BUILDINFO_OVERRIDE_DATE}
      -DBUILD_TIME=${BUILDINFO_OVERRIDE_TIME}
      -P ${CMAKE_SOURCE_DIR}/build_files/cmake/buildinfo.cmake
  )

  # `buildinfo.h` is a generated file.
  set_source_files_properties(
    ${buildinfo_h_real}
    PROPERTIES GENERATED TRUE
    HEADER_FILE_ONLY TRUE)

  unset(buildinfo_h_real)
  unset(buildinfo_h_fake)

  # Add dependencies below, after adding Blender
  # -------------- done with header values.

  list(APPEND SRC
    buildinfo.c
  )

  # make an object library so can load with it in tests
  add_library(buildinfoobj OBJECT buildinfo.c)
  add_dependencies(buildinfoobj buildinfo)
endif()

add_cc_flags_custom_test(blender)

# message(STATUS "Configuring blender")
if(WITH_PYTHON_MODULE)
  add_definitions(-DWITH_PYTHON_MODULE)

  # Creates `./bpy/__init__.so` which can be imported as a Python module.
  #
  # Note that 'SHARED' works on Linux and Windows, but not MACOS which _must_ be 'MODULE'.
  add_library(blender MODULE ${SRC})


  get_property(GENERATOR_IS_MULTI_CONFIG GLOBAL PROPERTY GENERATOR_IS_MULTI_CONFIG)
  if(GENERATOR_IS_MULTI_CONFIG)
    set(BPY_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin/$<CONFIG>/bpy)
  else()
    set(BPY_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin/bpy)
  endif()

  set_target_properties(
    blender
    PROPERTIES
      PREFIX ""
      OUTPUT_NAME __init__
      LIBRARY_OUTPUT_DIRECTORY ${BPY_OUTPUT_DIRECTORY}
      RUNTIME_OUTPUT_DIRECTORY ${BPY_OUTPUT_DIRECTORY}
  )
  unset(BPY_OUTPUT_DIRECTORY)

  if(APPLE)
    set_target_properties(blender PROPERTIES MACOSX_BUNDLE TRUE)
    if(WITH_BLENDER_THUMBNAILER)
      set_target_properties(blender-thumbnailer PROPERTIES MACOSX_BUNDLE TRUE)
    endif()
  endif()

  if(WIN32)
    # Python modules use this.
    set_target_properties(
      blender
      PROPERTIES
      SUFFIX ".pyd"
    )
  endif()

else()
  add_executable(blender ${EXETYPE} ${SRC})
  if(WITH_CPU_CHECK)
    # blender_cpu_check *NEEDS* to be linked first, there can be no exceptions
    # to this, this is to ensure this will be the first code to run once the
    # blender binary has been loaded by the OS.
    target_link_libraries(blender PRIVATE blender_cpu_check)
  endif()
  if(WIN32)
    add_executable(blender-launcher WIN32
      blender_launcher_win32.c
      ${CMAKE_SOURCE_DIR}/release/windows/icons/winblender.rc
    )
    if(NOT WITH_WINDOWS_EXTERNAL_MANIFEST)
      target_sources(blender-launcher PRIVATE
        ${CMAKE_BINARY_DIR}/blender.exe.manifest
      )
    endif()
    target_compile_definitions(blender-launcher PRIVATE -D_UNICODE -DUNICODE)
    target_link_libraries(blender-launcher Pathcch.lib)
  endif()
endif()

if(WITH_BUILDINFO)
  # Explicitly say that the executable depends on the `buildinfo`.
  add_dependencies(blender buildinfo)
endif()


set(BLENDER_TEXT_FILES
  # Generate this file:
  # `${CMAKE_SOURCE_DIR}/release/text/readme.html`
)

if(WITH_INSTALL_COPYRIGHT)
  list(APPEND BLENDER_TEXT_FILES
    ${CMAKE_SOURCE_DIR}/release/text/copyright.txt
  )
endif()

# -----------------------------------------------------------------------------
# Platform specific target destinations
#
# Setup version directory, libraries, `bpy` & text files.

if(UNIX AND NOT APPLE)
  if(WITH_PYTHON_MODULE)
    if(WITH_INSTALL_PORTABLE)
      set(TARGETDIR_BPY "bpy")
      set(TARGETDIR_VER "bpy/${BLENDER_VERSION}")
      set(TARGETDIR_LIB "bpy/lib")
    else()
      set(TARGETDIR_BPY ${PYTHON_SITE_PACKAGES}/bpy)
      set(TARGETDIR_VER ${PYTHON_SITE_PACKAGES}/bpy/${BLENDER_VERSION})
      set(TARGETDIR_LIB ${PYTHON_SITE_PACKAGES}/bpy/lib)
    endif()
  else()
    if(WITH_INSTALL_PORTABLE)
      set(TARGETDIR_VER "${BLENDER_VERSION}")
      set(TARGETDIR_TEXT ".")
      set(TARGETDIR_LIB "lib")
    else()
      set(TARGETDIR_VER "share/blender/${BLENDER_VERSION}")
      set(TARGETDIR_TEXT "share/doc/blender")
    endif()
  endif()
  set(TARGETDIR_SITE_PACKAGES "${TARGETDIR_VER}/python/lib/python${PYTHON_VERSION}/site-packages")
elseif(WIN32)
  if(WITH_PYTHON_MODULE)
    set(TARGETDIR_BPY ${CMAKE_INSTALL_PREFIX_WITH_CONFIG}/bpy)
    set(TARGETDIR_VER ${CMAKE_INSTALL_PREFIX_WITH_CONFIG}/bpy/${BLENDER_VERSION})
    # Important the DLL's are next to `__init__.pyd` otherwise it won't load.
    set(TARGETDIR_LIB ${CMAKE_INSTALL_PREFIX_WITH_CONFIG}/bpy)
    set(TARGETDIR_EXE ${CMAKE_INSTALL_PREFIX_WITH_CONFIG}/bpy)
  else()
    set(TARGETDIR_VER "${BLENDER_VERSION}")
    set(TARGETDIR_TEXT ".")
    set(TARGETDIR_LIB "blender.shared")
    set(TARGETDIR_EXE ".")
  endif()
  set(TARGETDIR_SITE_PACKAGES "${TARGETDIR_VER}/python/lib/site-packages")
elseif(APPLE)
  if(WITH_PYTHON_MODULE)
    if(WITH_INSTALL_PORTABLE)
      set(TARGETDIR_BPY "bpy")
      set(TARGETDIR_VER "bpy/${BLENDER_VERSION}")
      set(TARGETDIR_LIB "bpy/lib")
    else()
      # Paths defined in terms of `site-packages` since the `site-packages`
      # directory can be a symbolic-link (brew for example).
      set(TARGETDIR_BPY ${PYTHON_SITE_PACKAGES}/bpy)
      set(TARGETDIR_VER ${PYTHON_SITE_PACKAGES}/bpy/${BLENDER_VERSION})
      set(TARGETDIR_LIB ${PYTHON_SITE_PACKAGES}/bpy/lib)
    endif()
  else()
    set(TARGETDIR_VER "Blender.app/Contents/Resources/${BLENDER_VERSION}")
    set(TARGETDIR_LIB "Blender.app/Contents/Resources/lib")
    set(TARGETDIR_TEXT "Blender.app/Contents/Resources/text")
  endif()
  set(TARGETDIR_SITE_PACKAGES "${TARGETDIR_VER}/python/lib/python${PYTHON_VERSION}/site-packages")
  # Skip re-linking on CPACK / install.
  set_target_properties(blender PROPERTIES BUILD_WITH_INSTALL_RPATH true)
  if(WITH_BLENDER_THUMBNAILER)
    set_target_properties(blender-thumbnailer PROPERTIES BUILD_WITH_INSTALL_RPATH true)
  endif()
endif()


# -----------------------------------------------------------------------------
# Install Targets (Generic, All Platforms)

if(WITH_PYTHON)
  # install(CODE "message(\"copying blender scripts...\")")

  # do not install freestyle dir if disabled
  if(NOT WITH_FREESTYLE)
    set(FREESTYLE_EXCLUDE_CONDITIONAL "freestyle/*")
  else()
    set(FREESTYLE_EXCLUDE_CONDITIONAL "_freestyle/*")  # Dummy, won't do anything.
  endif()

  install(
    DIRECTORY ${CMAKE_SOURCE_DIR}/scripts
    DESTINATION ${TARGETDIR_VER}
    PATTERN ".git" EXCLUDE
    PATTERN ".gitignore" EXCLUDE
    PATTERN ".gitea" EXCLUDE
    PATTERN ".github" EXCLUDE
    PATTERN ".arcconfig" EXCLUDE
    PATTERN "__pycache__" EXCLUDE
    PATTERN "site" EXCLUDE
    PATTERN "${FREESTYLE_EXCLUDE_CONDITIONAL}" EXCLUDE

    # Exclude extensions development files.
    PATTERN "addons_core/bl_pkg/Makefile" EXCLUDE
    PATTERN "addons_core/bl_pkg/tests" EXCLUDE
    PATTERN "addons_core/bl_pkg/example_extension" EXCLUDE
  )

  if(WITH_PYTHON_MODULE)
    install(
      FILES ${CMAKE_SOURCE_DIR}/scripts/site/sitecustomize.py
      DESTINATION ${TARGETDIR_VER}/scripts/startup
      # Rename to avoid conflict with system `sitecustomize.py`.
      RENAME bpy_site_customize.py
    )
  elseif(WITH_PYTHON_INSTALL)
    install(
      FILES ${CMAKE_SOURCE_DIR}/scripts/site/sitecustomize.py
      DESTINATION ${TARGETDIR_SITE_PACKAGES}
    )
  endif()
  unset(FREESTYLE_EXCLUDE_CONDITIONAL)

  if(WITH_DRACO)
    install(
      PROGRAMS $<TARGET_FILE:extern_draco>
      DESTINATION ${TARGETDIR_VER}/scripts/addons_core/io_scene_gltf2
    )
  endif()

endif()

# fonts
install(
  DIRECTORY ${CMAKE_SOURCE_DIR}/release/datafiles/fonts
  DESTINATION ${TARGETDIR_VER}/datafiles
)

# localization
if(WITH_INTERNATIONAL)
  set(_locale_dir "${CMAKE_SOURCE_DIR}/locale")
  set(_locale_target_dir ${TARGETDIR_VER}/datafiles/locale)

  file(GLOB _po_files "${_locale_dir}/po/*.po")
  foreach(_po_file ${_po_files})
    msgfmt_simple(${_po_file} _all_mo_files)
  endforeach()

  # Create a custom target which will compile all `*.po` to `*.mo`.
  add_custom_target(
    locales
    DEPENDS ${_all_mo_files}
  )
  add_dependencies(blender locales)

  # Generate INSTALL rules.
  install(
    FILES ${_locale_dir}/languages
    DESTINATION ${_locale_target_dir}
  )

  foreach(_mo_file ${_all_mo_files})
    get_filename_component(_locale_name ${_mo_file} NAME_WE)
    install(
      FILES ${_mo_file}
      DESTINATION ${_locale_target_dir}/${_locale_name}/LC_MESSAGES
      RENAME blender.mo
    )
    unset(_locale_name)
  endforeach()

  unset(_all_mo_files)
  unset(_po_files)
  unset(_po_file)
  unset(_mo_file)
  unset(_locale_target_dir)

  unset(_locale_dir)
endif()

# Color management.
if(WITH_OPENCOLORIO)
  install(
    DIRECTORY ${CMAKE_SOURCE_DIR}/release/datafiles/colormanagement
    DESTINATION ${TARGETDIR_VER}/datafiles
  )
endif()
if(WIN32)
  if(EXISTS ${LIBDIR}/osl/lib/python${PYTHON_VERSION}/site-packages/oslquery) # 4.4+
    install(
      DIRECTORY ${LIBDIR}/osl/lib/python${PYTHON_VERSION}/site-packages/oslquery
      DESTINATION ${TARGETDIR_SITE_PACKAGES}
      CONFIGURATIONS Release;RelWithDebInfo;MinSizeRel
    )
    install(
      DIRECTORY ${LIBDIR}/osl/lib/python${PYTHON_VERSION}_debug/site-packages/oslquery
      DESTINATION ${TARGETDIR_SITE_PACKAGES}
      CONFIGURATIONS Debug
    )
  endif()
  if(EXISTS ${LIBDIR}/osl/bin/oslquery.dll) # 4.1+
    windows_install_shared_manifest(
      FILES
        ${LIBDIR}/osl/bin/oslquery.dll
        ${LIBDIR}/osl/bin/oslcomp.dll
        ${LIBDIR}/osl/bin/oslexec.dll
        ${LIBDIR}/osl/bin/oslnoise.dll
      RELEASE
    )
    windows_install_shared_manifest(
      FILES
        ${LIBDIR}/osl/bin/oslquery_d.dll
        ${LIBDIR}/osl/bin/oslcomp_d.dll
        ${LIBDIR}/osl/bin/oslexec_d.dll
        ${LIBDIR}/osl/bin/oslnoise_d.dll
      DEBUG
    )
  endif()
  if(EXISTS ${LIBDIR}/opencolorio/bin/opencolorio_2_4.dll) # 4.4
    windows_install_shared_manifest(
      FILES ${LIBDIR}/opencolorio/bin/opencolorio_2_4.dll
      RELEASE
    )
    windows_install_shared_manifest(
      FILES ${LIBDIR}/opencolorio/bin/opencolorio_d_2_4.dll
      DEBUG
    )
  endif()
  if(EXISTS ${LIBDIR}/opencolorio/bin/opencolorio_2_3.dll) # 4.1
    windows_install_shared_manifest(
      FILES ${LIBDIR}/opencolorio/bin/opencolorio_2_3.dll
      RELEASE
    )
    windows_install_shared_manifest(
      FILES ${LIBDIR}/opencolorio/bin/opencolorio_d_2_3.dll
      DEBUG
    )
  endif()
  install(
    DIRECTORY ${LIBDIR}/opencolorio/lib/site-packages-debug/PyOpenColorIO
    DESTINATION ${TARGETDIR_SITE_PACKAGES}
    CONFIGURATIONS Debug
  )
  install(
    DIRECTORY ${LIBDIR}/opencolorio/lib/site-packages/PyOpenColorIO
    DESTINATION ${TARGETDIR_SITE_PACKAGES}
    CONFIGURATIONS Release;RelWithDebInfo;MinSizeRel
  )
  if(EXISTS ${LIBDIR}/OpenImageDenoise/bin/openimagedenoise.dll) # 4.0
    windows_install_shared_manifest(
      FILES
        ${LIBDIR}/OpenImageDenoise/bin/OpenImageDenoise.dll
        ${LIBDIR}/OpenImageDenoise/bin/OpenImageDenoise_core.dll
        ${LIBDIR}/OpenImageDenoise/bin/OpenImageDenoise_device_cpu.dll
    )
  endif()
  # Platforms that have SyCL support.
  if(EXISTS ${LIBDIR}/OpenImageDenoise/bin/OpenImageDenoise_device_sycl.dll)
    windows_install_shared_manifest(
      FILES
        ${LIBDIR}/OpenImageDenoise/bin/OpenImageDenoise_device_sycl.dll
    )
  endif()
  if(EXISTS ${LIBDIR}/OpenImageDenoise/bin/OpenImageDenoise_device_hip.dll) # 4.1
    windows_install_shared_manifest(
      FILES
        ${LIBDIR}/OpenImageDenoise/bin/OpenImageDenoise_device_hip.dll
    )
  endif()
  if(EXISTS ${LIBDIR}/OpenImageDenoise/bin/OpenImageDenoise_device_cuda.dll) # 4.1
    windows_install_shared_manifest(
      FILES
        ${LIBDIR}/OpenImageDenoise/bin/OpenImageDenoise_device_cuda.dll
    )
  endif()
endif()

# Show helpful tip.
set(_install_cmd "")
if("${CMAKE_GENERATOR}" MATCHES ".*Makefiles.*")
  set(_install_cmd "make install")
elseif("${CMAKE_GENERATOR}" MATCHES "Ninja")
  set(_install_cmd "ninja install")
endif()
if(NOT ("${_install_cmd}" STREQUAL ""))
  # Message to display after building.
  get_filename_component(_install_dst ${TARGETDIR_VER} ABSOLUTE BASE_DIR ${CMAKE_INSTALL_PREFIX})
  add_custom_command(
    TARGET blender POST_BUILD
    COMMAND
      ${CMAKE_COMMAND} -E echo
      "Run: \\\"${_install_cmd}\\\" to copy runtime files and scripts to: ${_install_dst}"
  )
  unset(_install_dst)
endif()
unset(_install_cmd)

# macro to help install files without dragging in unnecessary data.
macro(install_dir from to)
  install(
    DIRECTORY ${from}
    DESTINATION ${to}
    # Irrelevant files and caches.
    PATTERN ".git" EXCLUDE
    PATTERN ".gitignore" EXCLUDE
    PATTERN ".gitea" EXCLUDE
    PATTERN ".github" EXCLUDE
    PATTERN "*.pyc" EXCLUDE
    PATTERN "*.pyo" EXCLUDE
    PATTERN "*.orig" EXCLUDE
    PATTERN "*.rej" EXCLUDE
    PATTERN "__pycache__" EXCLUDE
    PATTERN "__MACOSX" EXCLUDE
    PATTERN ".DS_Store" EXCLUDE
    # Unneeded Python files.
    PATTERN "config-${PYTHON_VERSION}/*.a" EXCLUDE  # static lib
    PATTERN "lib2to3" EXCLUDE                   # ./lib2to3
    PATTERN "tkinter" EXCLUDE                   # ./tkinter
    PATTERN "lib-dynload/_tkinter.*" EXCLUDE    # ./lib-dynload/_tkinter.co
    PATTERN "idlelib" EXCLUDE                   # ./idlelib
    PATTERN "test" EXCLUDE                      # ./test
    PATTERN "turtledemo" EXCLUDE                # ./turtledemo
    PATTERN "turtle.py" EXCLUDE                 # ./turtle.py
    PATTERN "wininst*.exe" EXCLUDE              # from distutils, avoid malware false positive
  )
endmacro()

# -----------------------------------------------------------------------------
# Install Targets (Platform Specific)

if(UNIX AND NOT APPLE)

  if(PLATFORM_BUNDLED_LIBRARIES AND TARGETDIR_LIB)
    install(
      FILES ${PLATFORM_BUNDLED_LIBRARIES}
      DESTINATION ${TARGETDIR_LIB}
    )
  endif()

  # There are a few differences between portable and system install.
  if(WITH_PYTHON_MODULE)
    if(WITH_INSTALL_PORTABLE)
      install(
        TARGETS blender
        DESTINATION ${TARGETDIR_BPY}
      )
    else()
      install(
        TARGETS blender
        LIBRARY DESTINATION ${TARGETDIR_BPY}
      )
    endif()

    # none of the other files are needed currently
  elseif(WITH_INSTALL_PORTABLE)
    set(BLENDER_BIN "blender")
    install(
      TARGETS blender
      DESTINATION "."
    )

    if(WITH_CPU_CHECK)
      install(
        TARGETS blender_cpu_check
        DESTINATION "./lib"
      )
    endif()

    install(
      FILES
        ${CMAKE_SOURCE_DIR}/release/freedesktop/blender.desktop
        ${CMAKE_SOURCE_DIR}/release/freedesktop/icons/scalable/apps/blender.svg
        ${CMAKE_SOURCE_DIR}/release/freedesktop/icons/symbolic/apps/blender-symbolic.svg
      DESTINATION "."
    )

    if(WITH_BLENDER_THUMBNAILER)
      install(
        TARGETS blender-thumbnailer
        DESTINATION "."
      )
    endif()

    # NOTE: there is a bug in CMake 3.25.1 where `LIBDIR` is reported as undefined.
    if(NOT DEFINED LIBDIR)
      # Pass.
    elseif(EXISTS ${LIBDIR}/mesa)
      install(
        # Trailing slash is needed to install contents instead of directory itself.
        DIRECTORY ${LIBDIR}/mesa/lib/
        DESTINATION "./lib/mesa"
      )

      install(
        PROGRAMS
        ${CMAKE_SOURCE_DIR}/release/bin/blender-launcher
        ${CMAKE_SOURCE_DIR}/release/bin/blender-softwaregl
        DESTINATION "."
      )

      # Remove from old location, so existing builds don't start with software
      # OpenGL now that the `./lib/` directory is used for other libraries.
      install(
        CODE "\
file(REMOVE ${CMAKE_BINARY_DIR}/bin/lib/libGL.so)\n
file(REMOVE ${CMAKE_BINARY_DIR}/bin/lib/libGL.so.1)\n
file(REMOVE ${CMAKE_BINARY_DIR}/bin/lib/libGL.so.1.5.0)\n
file(REMOVE ${CMAKE_BINARY_DIR}/bin/lib/libGLU.so)\n
file(REMOVE ${CMAKE_BINARY_DIR}/bin/lib/libGLU.so.1)\n
file(REMOVE ${CMAKE_BINARY_DIR}/bin/lib/libGLU.so.1.3.1)\n
file(REMOVE ${CMAKE_BINARY_DIR}/bin/lib/libglapi.so)\n
file(REMOVE ${CMAKE_BINARY_DIR}/bin/lib/libglapi.so.0)\n
file(REMOVE ${CMAKE_BINARY_DIR}/bin/lib/libglapi.so.0.0.0)\n
"
      )
    endif()
  else()
    # main blender binary
    set(BLENDER_BIN "bin/blender")
    install(
      TARGETS blender
      DESTINATION "./bin"
    )
    # Misc files.
    install(
      FILES ${CMAKE_SOURCE_DIR}/release/freedesktop/blender.desktop
      DESTINATION "./share/applications"
    )
    install(
      FILES ${CMAKE_SOURCE_DIR}/release/freedesktop/org.blender.Blender.metainfo.xml
      DESTINATION "./share/metainfo"
    )
    install(
      FILES ${CMAKE_SOURCE_DIR}/release/freedesktop/icons/scalable/apps/blender.svg
      DESTINATION "./share/icons/hicolor/scalable/apps"
    )
    install(
      FILES ${CMAKE_SOURCE_DIR}/release/freedesktop/icons/symbolic/apps/blender-symbolic.svg
      DESTINATION "./share/icons/hicolor/symbolic/apps"
    )
    if(WITH_BLENDER_THUMBNAILER)
      install(
        TARGETS blender-thumbnailer
        DESTINATION "./bin"
      )
    endif()
  endif()

  if(WITH_PYTHON AND WITH_PYTHON_INSTALL)
    # Install executable
    install(
      PROGRAMS ${PYTHON_EXECUTABLE}
      DESTINATION ${TARGETDIR_VER}/python/bin
    )

    if(DEFINED LIBDIR)
      # Pre-compiled libraries, copy over complete lib directory.
      install_dir(
        ${PYTHON_LIBPATH}
        ${TARGETDIR_VER}/python
      )
    else()
      # System libraries.
      install(
        PROGRAMS ${PYTHON_EXECUTABLE}
        DESTINATION ${TARGETDIR_VER}/python/bin
      )

      # On some platforms (like openSUSE) Python is linked to be used from `lib64` directory.
      # determine this from Python's libraries path.
      # Ugh, its possible `lib64` is just a symbolic-link to `lib`
      # which causes incorrect use of `lib64`.
      get_filename_component(_pypath_real ${PYTHON_LIBPATH} REALPATH)
      if(${_pypath_real} MATCHES "lib64$")
        set(_target_LIB "lib64")
      else()
        set(_target_LIB "lib")
      endif()
      unset(_pypath_real)

      # Copy the systems python into the install directory:
      # install(CODE "message(\"copying a subset of the systems python...\")")
      install(
        DIRECTORY ${PYTHON_LIBPATH}/python${PYTHON_VERSION}
        DESTINATION ${TARGETDIR_VER}/python/${_target_LIB}
        PATTERN "__pycache__" EXCLUDE               # * any cache *
        PATTERN "config-${PYTHON_VERSION}/*.a" EXCLUDE  # static lib
        PATTERN "lib2to3" EXCLUDE                   # ./lib2to3
        PATTERN "site-packages/*" EXCLUDE           # ./site-packages/*
        PATTERN "tkinter" EXCLUDE                   # ./tkinter
        PATTERN "lib-dynload/_tkinter.*" EXCLUDE    # ./lib-dynload/_tkinter.co
        PATTERN "idlelib" EXCLUDE                   # ./idlelib
        PATTERN "test" EXCLUDE                      # ./test
        PATTERN "turtledemo" EXCLUDE                # ./turtledemo
        PATTERN "turtle.py" EXCLUDE                 # ./turtle.py
        PATTERN "wininst*.exe" EXCLUDE              # from distutils, avoid malware false positive
      )

      # Needed for `distutils/pip`.
      # Get the last part of the include dir, will be `python{version}{abiflag}`.
      get_filename_component(_py_inc_suffix ${PYTHON_INCLUDE_DIR} NAME)
      install(
        FILES ${PYTHON_INCLUDE_DIR}/pyconfig.h
        DESTINATION ${TARGETDIR_VER}/python/include/${_py_inc_suffix}
      )
      unset(_py_inc_suffix)

      if(WITH_PYTHON_INSTALL_NUMPY)
        # Install to the same directory as the source, so debian-like
        # distributions are happy with their policy.
        set(_suffix "site-packages")
        if(${PYTHON_NUMPY_PATH} MATCHES "dist-packages")
          set(_suffix "dist-packages")
        endif()
        install(
          DIRECTORY ${PYTHON_NUMPY_PATH}/numpy
          DESTINATION ${TARGETDIR_VER}/python/${_target_LIB}/python${PYTHON_VERSION}/${_suffix}
          PATTERN "__pycache__" EXCLUDE           # * any cache *
          PATTERN "*.pyc" EXCLUDE                 # * any cache *
          PATTERN "*.pyo" EXCLUDE                 # * any cache *
          PATTERN "oldnumeric" EXCLUDE            # ./oldnumeric
          PATTERN "doc" EXCLUDE                   # ./doc
          PATTERN "tests" EXCLUDE                 # ./tests
          PATTERN "f2py" EXCLUDE                  # ./f2py - fortran/python interface code, not for blender.
          PATTERN "include" EXCLUDE               # include dirs all over, we won't use NumPy/CAPI
          PATTERN "*.h" EXCLUDE                   # some includes are not in include dirs
          PATTERN "*.a" EXCLUDE                   # ./core/lib/libnpymath.a - for linking, we don't need.
        )
        install(
          DIRECTORY ${PYTHON_NUMPY_PATH}/Cython
          DESTINATION ${TARGETDIR_VER}/python/${_target_LIB}/python${PYTHON_VERSION}/${_suffix}
          PATTERN "__pycache__" EXCLUDE           # * any cache *
          PATTERN "*.pyc" EXCLUDE                 # * any cache *
          PATTERN "*.pyo" EXCLUDE                 # * any cache *
        )
        install(
          FILES ${PYTHON_NUMPY_PATH}/cython.py
          DESTINATION ${TARGETDIR_VER}/python/${_target_LIB}/python${PYTHON_VERSION}/${_suffix}
        )
        unset(_suffix)
      endif()

      if(WITH_USD)
        # Install to the same directory as the source, so debian-like
        # distributions are happy with their policy.
        set(_suffix "site-packages")
        if(0) # TODO: `PYTHON_USD_PATH` isn't defined anywhere.
          if(${PYTHON_USD_PATH} MATCHES "dist-packages")
            set(_suffix "dist-packages")
          endif()
        endif()
        install(
          DIRECTORY ${USD_LIBRARY_DIR}/python/
          DESTINATION ${TARGETDIR_VER}/python/${_target_LIB}/python${PYTHON_VERSION}/${_suffix}
          PATTERN "__pycache__" EXCLUDE           # * any cache *
          PATTERN "*.pyc" EXCLUDE                 # * any cache *
          PATTERN "*.pyo" EXCLUDE                 # * any cache *
        )
        unset(_suffix)
      endif()

      if(WITH_PYTHON_INSTALL_ZSTANDARD)
        # Install to the same directory as the source, so debian-like
        # distributions are happy with their policy.
        set(_suffix "site-packages")
        if(${PYTHON_ZSTANDARD_PATH} MATCHES "dist-packages")
          set(_suffix "dist-packages")
        endif()
        install(
          DIRECTORY ${PYTHON_ZSTANDARD_PATH}/zstandard
          DESTINATION ${TARGETDIR_VER}/python/${_target_LIB}/python${PYTHON_VERSION}/${_suffix}
          PATTERN "__pycache__" EXCLUDE           # * any cache *
          PATTERN "*.pyc" EXCLUDE                 # * any cache *
          PATTERN "*.pyo" EXCLUDE                 # * any cache *
        )
        unset(_suffix)
      endif()

      # Copy requests, we need to generalize site-packages.
      if(WITH_PYTHON_INSTALL_REQUESTS)
        set(_suffix "site-packages")
        if(${PYTHON_REQUESTS_PATH} MATCHES "dist-packages")
          set(_suffix "dist-packages")
        endif()
        install(
          DIRECTORY ${PYTHON_REQUESTS_PATH}/requests
          DESTINATION ${TARGETDIR_VER}/python/${_target_LIB}/python${PYTHON_VERSION}/${_suffix}
          PATTERN "__pycache__" EXCLUDE           # * any cache *
          PATTERN "*.pyc" EXCLUDE                 # * any cache *
          PATTERN "*.pyo" EXCLUDE                 # * any cache *
        )
        # On some platforms requests does have extra dependencies.
        #
        # Either `chardet` or `charset_normalizer` is used, depending on the version of Python.
        # The code below silently skips the one that's not available, so we can list both here.
        set(_requests_deps "certifi" "chardet" "charset_normalizer" "idna" "urllib3")
        foreach(_requests_dep ${_requests_deps})
          if(EXISTS ${PYTHON_REQUESTS_PATH}/${_requests_dep})
            install(
              DIRECTORY ${PYTHON_REQUESTS_PATH}/${_requests_dep}
              DESTINATION ${TARGETDIR_VER}/python/${_target_LIB}/python${PYTHON_VERSION}/${_suffix}
              PATTERN "__pycache__" EXCLUDE           # * any cache *
              PATTERN "*.pyc" EXCLUDE                 # * any cache *
              PATTERN "*.pyo" EXCLUDE                 # * any cache *
            )
          endif()
        endforeach()
        if(EXISTS ${PYTHON_REQUESTS_PATH}/six.py)
          install(
            FILES ${PYTHON_REQUESTS_PATH}/six.py
            DESTINATION ${TARGETDIR_VER}/python/${_target_LIB}/python${PYTHON_VERSION}/${_suffix}
          )
        endif()
        unset(_requests_dep)
        unset(_requests_deps)
        unset(_suffix)
      endif()
      unset(_target_LIB)
    endif()

    if(WITH_INSTALL_PORTABLE)
      # The script uses the version of Python installed with Blender,
      # so it must only be inside installed if `WITH_PYTHON AND WITH_PYTHON_INSTALL`
      # are active.
      get_filename_component(PYTHON_EXECUTABLE_NAME_ONLY ${PYTHON_EXECUTABLE} NAME)
      configure_file(
        ${CMAKE_SOURCE_DIR}/release/freedesktop/scripts/blender-system-info.sh.in
        ${CMAKE_BINARY_DIR}/release/freedesktop/scripts/blender-system-info.sh
        @ONLY
      )
      unset(PYTHON_EXECUTABLE_NAME_ONLY)
      install(
        PROGRAMS ${CMAKE_BINARY_DIR}/release/freedesktop/scripts/blender-system-info.sh
        DESTINATION "."
      )
    endif()
  endif()

elseif(WIN32)
  if(WITH_WINDOWS_EXTERNAL_MANIFEST)
    install(
      FILES ${CMAKE_BINARY_DIR}/blender.exe.manifest
      DESTINATION "."
    )
    install(
      FILES ${CMAKE_BINARY_DIR}/blender.exe.manifest
      DESTINATION "."
      RENAME blender-launcher.exe.manifest
    )
  endif()
  windows_install_shared_manifest(
    FILES ${LIBDIR}/epoxy/bin/epoxy-0.dll
    ALL
  )

  if(WITH_VULKAN_BACKEND)
    windows_install_shared_manifest(
      FILES ${LIBDIR}/vulkan/bin/vulkan-1.dll
      ALL
    )
  endif()

  # 4.1 FFTW libs need to be installed, in 4.2 FFTW got turned into a static lib
  # and the files below no longer exist.
  if(EXISTS ${LIBDIR}/fftw3/lib/fftw3.dll)
    windows_install_shared_manifest(
      FILES
        ${LIBDIR}/fftw3/lib/fftw3.dll
        ${LIBDIR}/fftw3/lib/fftw3f.dll
      ALL
    )
  endif()
  if(MSVC_ASAN)
    # The ASAN DLL's can be found in the same directory as the compiler,
    # this is the easiest way to find these.
    string(
      REPLACE "cl.exe" "clang_rt.asan_dynamic-x86_64.dll"
      ASAN_DLL ${CMAKE_C_COMPILER})
    string(
      REPLACE "cl.exe" "clang_rt.asan_dbg_dynamic-x86_64.dll"
      ASAN_DEBUG_DLL ${CMAKE_C_COMPILER}
    )
    if(NOT EXISTS "${ASAN_DLL}")
      message(
        FATAL_ERROR
        "ASAN is enabled, but the ASAN runtime is not detected, "
        "this is an optional component during the MSVC install, please install it"
      )
    endif()
    windows_install_shared_manifest(
      FILES ${ASAN_DLL}
      RELEASE
    )
    windows_install_shared_manifest(
      FILES ${ASAN_DEBUG_DLL}
      DEBUG
    )
    unset(ASAN_DLL)
    unset(ASAN_DEBUG_DLL)
  endif()
  if(EXISTS ${LIBDIR}/openexr/bin/Iex.dll)
    windows_install_shared_manifest(
      FILES
        ${LIBDIR}/openexr/bin/Iex.dll
        ${LIBDIR}/openexr/bin/IlmThread.dll
        ${LIBDIR}/openexr/bin/OpenEXRCore.dll
        ${LIBDIR}/openexr/bin/OpenEXRUtil.dll
        ${LIBDIR}/openexr/bin/OpenEXR.dll
        ${LIBDIR}/imath/bin/imath.dll
      RELEASE
    )
    windows_install_shared_manifest(
      FILES
        ${LIBDIR}/openexr/bin/Iex_d.dll
        ${LIBDIR}/openexr/bin/IlmThread_d.dll
        ${LIBDIR}/openexr/bin/OpenEXRCore_d.dll
        ${LIBDIR}/openexr/bin/OpenEXRUtil_d.dll
        ${LIBDIR}/openexr/bin/OpenEXR_d.dll
        ${LIBDIR}/imath/bin/imath_d.dll
      DEBUG
    )
  endif()
  if(EXISTS ${LIBDIR}/openimageio/bin/openimageio.dll)
    windows_install_shared_manifest(
      FILES
        ${LIBDIR}/openimageio/bin/openimageio.dll
        ${LIBDIR}/openimageio/bin/openimageio_util.dll
      RELEASE
    )
    windows_install_shared_manifest(
      FILES
        ${LIBDIR}/openimageio/bin/openimageio_d.dll
        ${LIBDIR}/openimageio/bin/openimageio_util_d.dll
      DEBUG
    )
  endif()

  if(EXISTS ${LIBDIR}/gmp/lib/gmp-10.dll)
    set(GMP_DLL ${LIBDIR}/gmp/lib/gmp-10.dll)
  else()
    set(GMP_DLL ${LIBDIR}/gmp/lib/libgmp-10.dll)
  endif()

  windows_install_shared_manifest(
    FILES ${GMP_DLL}
    ALL
  )
  unset(GMP_DLL)

  windows_install_shared_manifest(
    FILES ${LIBDIR}/gmp/lib/libgmpxx.dll
    RELEASE
  )
  windows_install_shared_manifest(
    FILES ${LIBDIR}/gmp/lib/libgmpxx_d.dll
    DEBUG
  )

  if(WITH_WINDOWS_RELEASE_PDB)
    if(MSVC_CLANG AND WITH_WINDOWS_RELEASE_STRIPPED_PDB)
      # clang-cl doesn't support stripped pdbs yet, it takes the /PDBSTRIPPED argument
      # without errors, but will just proceed to not do anything with it. So for now
      # just call pdbcopy manually to create the stripped pdb.
      if(CMAKE_SYSTEM_PROCESSOR STREQUAL "ARM64")
        set(SDK_SEARCH_DIR $ENV{WindowsSdkDir}/Debuggers/arm64)
      else()
        set(SDK_SEARCH_DIR $ENV{WindowsSdkDir}/Debuggers/x64)
      endif()
      find_program(PDBCOPY_EXECUTABLE
        NAMES
          pdbcopy
        HINTS
          ${SDK_SEARCH_DIR}
      )
      if(PDBCOPY_EXECUTABLE)
        add_custom_command(
          TARGET blender POST_BUILD
          COMMAND ${CMAKE_COMMAND} -E rm -f ${CMAKE_CURRENT_BINARY_DIR}/${CMAKE_BUILD_TYPE}/blender_public.pdb
          COMMAND ${PDBCOPY_EXECUTABLE} $<TARGET_PDB_FILE:blender> ${CMAKE_CURRENT_BINARY_DIR}/${CMAKE_BUILD_TYPE}/blender_public.pdb -p
          COMMENT "Stripping PDB file..."
        )
      else()
        message(WARNING "pdbcopy not found in ${SDK_SEARCH_DIR}. PDB stripping will be skipped.")
        set(WITH_WINDOWS_RELEASE_STRIPPED_PDB off)
      endif()
    endif()

    if(WITH_WINDOWS_RELEASE_STRIPPED_PDB)
      # Icky hack for older CMAKE from https://stackoverflow.com/a/21198501
      # `$<CONFIG>` will work in newer CMAKE but the version currently (3.12)
      # on the build-bot does not support this endeavor.
      install(
        FILES ${CMAKE_CURRENT_BINARY_DIR}/\${CMAKE_INSTALL_CONFIG_NAME}/blender_public.pdb
        DESTINATION "."
        RENAME blender.pdb
        CONFIGURATIONS Release
      )
    else()
      install(
        FILES $<TARGET_PDB_FILE:blender>
        DESTINATION "."
        RENAME blender.pdb
        CONFIGURATIONS Release
      )
    endif()
  endif()

  windows_install_shared_manifest(
    FILES ${LIBDIR}/openvdb/bin/openvdb.dll
    RELEASE
  )
  windows_install_shared_manifest(
    FILES ${LIBDIR}/openvdb/bin/openvdb_d.dll
    DEBUG
  )

  windows_install_shared_manifest(
    FILES
      ${LIBDIR}/materialx/bin/MaterialXCore.dll
      ${LIBDIR}/materialx/bin/MaterialXFormat.dll
      ${LIBDIR}/materialx/bin/MaterialXGenGlsl.dll
      ${LIBDIR}/materialx/bin/MaterialXGenMdl.dll
      ${LIBDIR}/materialx/bin/MaterialXGenOsl.dll
      ${LIBDIR}/materialx/bin/MaterialXGenShader.dll
    RELEASE
  )
  if(EXISTS ${LIBDIR}/materialx/bin/MaterialXRender.dll) # 3.6+
    windows_install_shared_manifest(
      FILES
        ${LIBDIR}/materialx/bin/MaterialXRender.dll
        ${LIBDIR}/materialx/bin/MaterialXRenderGlsl.dll
        ${LIBDIR}/materialx/bin/MaterialXRenderHw.dll
        ${LIBDIR}/materialx/bin/MaterialXRenderOsl.dll
      RELEASE
    )
    windows_install_shared_manifest(
      FILES
        ${LIBDIR}/materialx/bin/MaterialXRender_d.dll
        ${LIBDIR}/materialx/bin/MaterialXRenderGlsl_d.dll
        ${LIBDIR}/materialx/bin/MaterialXRenderHw_d.dll
        ${LIBDIR}/materialx/bin/MaterialXRenderOsl_d.dll
      DEBUG
    )
  endif()
  if(EXISTS ${LIBDIR}/materialx/bin/MaterialXGenMsl.dll) # 4.1+
    windows_install_shared_manifest(
      FILES
        ${LIBDIR}/materialx/bin/MaterialXGenMsl.dll
      RELEASE
    )
    windows_install_shared_manifest(
      FILES
        ${LIBDIR}/materialx/bin/MaterialXGenMsl_d.dll
      DEBUG
    )
  endif()
  windows_install_shared_manifest(
    FILES
      ${LIBDIR}/materialx/bin/MaterialXCore_d.dll
      ${LIBDIR}/materialx/bin/MaterialXFormat_d.dll
      ${LIBDIR}/materialx/bin/MaterialXGenGlsl_d.dll
      ${LIBDIR}/materialx/bin/MaterialXGenMdl_d.dll
      ${LIBDIR}/materialx/bin/MaterialXGenOsl_d.dll
      ${LIBDIR}/materialx/bin/MaterialXGenShader_d.dll
    DEBUG
  )

  if(WITH_PYTHON)
    string(REPLACE "." "" _PYTHON_VERSION_NO_DOTS ${PYTHON_VERSION})

    if(NOT WITH_PYTHON_MODULE)
      if(NOT CMAKE_COMPILER_IS_GNUCC)
        install(
          FILES
            ${LIBDIR}/python/${_PYTHON_VERSION_NO_DOTS}/bin/python${_PYTHON_VERSION_NO_DOTS}.dll
            ${LIBDIR}/python/${_PYTHON_VERSION_NO_DOTS}/bin/python3.dll
          DESTINATION ${TARGETDIR_EXE}
          CONFIGURATIONS Release;RelWithDebInfo;MinSizeRel
        )

        install(
          FILES
            ${LIBDIR}/python/${_PYTHON_VERSION_NO_DOTS}/bin/python${_PYTHON_VERSION_NO_DOTS}_d.dll
            ${LIBDIR}/python/${_PYTHON_VERSION_NO_DOTS}/bin/python3_d.dll
          DESTINATION ${TARGETDIR_EXE}
          CONFIGURATIONS Debug
        )
      endif()
    endif()

    # VFX libs are bundled with both Blender executable and Python module.
    if(WITH_PYTHON_INSTALL OR WITH_PYTHON_MODULE)
      # NOTE: as far as python is concerned `RelWithDebInfo`
      # is not debug since its without debug flags.

      install(DIRECTORY DESTINATION ${TARGETDIR_VER}/python)
      install(DIRECTORY DESTINATION ${TARGETDIR_VER}/python/lib)

      install(
        DIRECTORY ${LIBDIR}/python/${_PYTHON_VERSION_NO_DOTS}/lib
        DESTINATION ${BLENDER_VERSION}/python/
        CONFIGURATIONS Release;RelWithDebInfo;MinSizeRel
        PATTERN "*_d.*" EXCLUDE                 # * debug libraries *
        PATTERN "__pycache__" EXCLUDE           # * any cache *
        PATTERN "*.pyc" EXCLUDE                 # * any cache *
        PATTERN "*.pyo" EXCLUDE                 # * any cache *
      )

      install(
        DIRECTORY ${LIBDIR}/python/${_PYTHON_VERSION_NO_DOTS}/lib
        DESTINATION ${BLENDER_VERSION}/python/
        CONFIGURATIONS Debug
        PATTERN "__pycache__" EXCLUDE           # * any cache *
        PATTERN "*.pyc" EXCLUDE                 # * any cache *
        PATTERN "*.pyo" EXCLUDE                 # * any cache *
      )

      install(
        DIRECTORY ${LIBDIR}/python/${_PYTHON_VERSION_NO_DOTS}/DLLs
        DESTINATION ${BLENDER_VERSION}/python
        CONFIGURATIONS Release;RelWithDebInfo;MinSizeRel
        PATTERN "*.pdb" EXCLUDE
        PATTERN "*_d.*" EXCLUDE
      )

      install(
        DIRECTORY ${LIBDIR}/python/${_PYTHON_VERSION_NO_DOTS}/DLLs
        DESTINATION ${BLENDER_VERSION}/python
        CONFIGURATIONS Debug
      )

      install(
        FILES
          ${LIBDIR}/python/${_PYTHON_VERSION_NO_DOTS}/bin/python${_PYTHON_VERSION_NO_DOTS}.dll
          ${LIBDIR}/python/${_PYTHON_VERSION_NO_DOTS}/bin/python3.dll
          ${LIBDIR}/python/${_PYTHON_VERSION_NO_DOTS}/bin/python.exe
        DESTINATION ${BLENDER_VERSION}/python/bin
        CONFIGURATIONS Release;RelWithDebInfo;MinSizeRel
      )
      install(
        FILES
          ${LIBDIR}/python/${_PYTHON_VERSION_NO_DOTS}/bin/python${_PYTHON_VERSION_NO_DOTS}_d.dll
          ${LIBDIR}/python/${_PYTHON_VERSION_NO_DOTS}/bin/python3_d.dll
          ${LIBDIR}/python/${_PYTHON_VERSION_NO_DOTS}/bin/python_d.exe
        DESTINATION ${BLENDER_VERSION}/python/bin
        CONFIGURATIONS Debug
      )

      # This will only exist for 3.5+.
      if(EXISTS ${LIBDIR}/openimageio/lib/python${PYTHON_VERSION}/site-packages)
        install(
          DIRECTORY ${LIBDIR}/openimageio/lib/python${PYTHON_VERSION}/site-packages/
          DESTINATION ${TARGETDIR_SITE_PACKAGES}/
          CONFIGURATIONS Release;RelWithDebInfo;MinSizeRel
          PATTERN "__pycache__" EXCLUDE           # * any cache *
          PATTERN "*.pyc" EXCLUDE                 # * any cache *
          PATTERN "*.pyo" EXCLUDE                 # * any cache *
        )
      endif()
      if(EXISTS ${LIBDIR}/openimageio/lib/python${PYTHON_VERSION}_debug/site-packages)
        install(
          DIRECTORY ${LIBDIR}/openimageio/lib/python${PYTHON_VERSION}_debug/site-packages/
          DESTINATION ${TARGETDIR_SITE_PACKAGES}/
          CONFIGURATIONS Debug
          PATTERN "__pycache__" EXCLUDE           # * any cache *
          PATTERN "*.pyc" EXCLUDE                 # * any cache *
          PATTERN "*.pyo" EXCLUDE                 # * any cache *
        )
      endif()

      # This will not exist for 3.4 and earlier `./lib` directory
      # to ease the transition, support both 3.4 and 3.5 `./lib` directories.
      if(EXISTS ${USD_LIBRARY_DIR}/python/)
        install(
          DIRECTORY ${USD_LIBRARY_DIR}/python/
          DESTINATION ${TARGETDIR_SITE_PACKAGES}
          CONFIGURATIONS Release;RelWithDebInfo;MinSizeRel
          PATTERN "__pycache__" EXCLUDE           # * any cache *
          PATTERN "*.pyc" EXCLUDE                 # * any cache *
          PATTERN "*.pyo" EXCLUDE                 # * any cache *
        )
      endif()
      if(EXISTS ${USD_LIBRARY_DIR}/debug/python/)
        install(
          DIRECTORY ${USD_LIBRARY_DIR}/debug/python/
          DESTINATION ${TARGETDIR_SITE_PACKAGES}
          CONFIGURATIONS Debug
          PATTERN "__pycache__" EXCLUDE           # * any cache *
          PATTERN "*.pyc" EXCLUDE                 # * any cache *
          PATTERN "*.pyo" EXCLUDE                 # * any cache *
        )
      endif()

      # This will not exist for 3.4 and earlier `./lib` directories
      # to ease the transition, support both 3.4 and 3.5 `./lib` directories.
      if(EXISTS ${LIBDIR}/openvdb/python/pyopenvdb_d.pyd)
        install(
          FILES ${LIBDIR}/openvdb/python/pyopenvdb_d.pyd
          DESTINATION ${TARGETDIR_SITE_PACKAGES}
          CONFIGURATIONS Debug
        )
        install(
          FILES ${LIBDIR}/openvdb/python/pyopenvdb.pyd
          DESTINATION ${TARGETDIR_SITE_PACKAGES}
          CONFIGURATIONS Release;RelWithDebInfo;MinSizeRel
        )
      endif()

      # This will exist for 4.1 `./lib` directories.
      if(CMAKE_SYSTEM_PROCESSOR STREQUAL "ARM64")
        set(_openvdb_arch arm64)
      else()
        set(_openvdb_arch amd64)
      endif()

      # 4.3
      if(EXISTS ${LIBDIR}/openvdb/python/pyopenvdb_d.cp${_PYTHON_VERSION_NO_DOTS}-win_${_openvdb_arch}.pyd)
        install(
          FILES ${LIBDIR}/openvdb/python/pyopenvdb_d.cp${_PYTHON_VERSION_NO_DOTS}-win_${_openvdb_arch}.pyd
          DESTINATION ${TARGETDIR_SITE_PACKAGES}
          CONFIGURATIONS Debug
        )
        install(
          FILES ${LIBDIR}/openvdb/python/pyopenvdb.cp${_PYTHON_VERSION_NO_DOTS}-win_${_openvdb_arch}.pyd
          DESTINATION ${TARGETDIR_SITE_PACKAGES}
          CONFIGURATIONS Release;RelWithDebInfo;MinSizeRel
        )
      endif()
      # 4.4
      if(EXISTS ${LIBDIR}/openvdb/python/openvdb_d.cp${_PYTHON_VERSION_NO_DOTS}-win_${_openvdb_arch}.pyd)
        install(
          FILES ${LIBDIR}/openvdb/python/openvdb_d.cp${_PYTHON_VERSION_NO_DOTS}-win_${_openvdb_arch}.pyd
          DESTINATION ${TARGETDIR_SITE_PACKAGES}
          CONFIGURATIONS Debug
        )
        install(
          FILES ${LIBDIR}/openvdb/python/openvdb.cp${_PYTHON_VERSION_NO_DOTS}-win_${_openvdb_arch}.pyd
          DESTINATION ${TARGETDIR_SITE_PACKAGES}
          CONFIGURATIONS Release;RelWithDebInfo;MinSizeRel
        )
      endif()
      # MaterialX python bindings.
      # Check they exist so building against non `LIBDIR` Python works as expected.
      if(EXISTS ${LIBDIR}/materialx/python/Release/MaterialX)
        install(
          DIRECTORY ${LIBDIR}/materialx/python/Release/MaterialX
          DESTINATION ${TARGETDIR_SITE_PACKAGES}/
          CONFIGURATIONS Release;RelWithDebInfo;MinSizeRel
          PATTERN "__pycache__" EXCLUDE           # * any cache *
          PATTERN "*.pyc" EXCLUDE                 # * any cache *
          PATTERN "*.pyo" EXCLUDE                 # * any cache *
        )
      endif()
      if(EXISTS ${LIBDIR}/materialx/python/Debug/MaterialX)
        install(
          DIRECTORY ${LIBDIR}/materialx/python/Debug/MaterialX
          DESTINATION ${TARGETDIR_SITE_PACKAGES}/
          CONFIGURATIONS Debug
          PATTERN "__pycache__" EXCLUDE           # * any cache *
          PATTERN "*.pyc" EXCLUDE                 # * any cache *
          PATTERN "*.pyo" EXCLUDE                 # * any cache *
        )
      endif()
    endif()

    if(WITH_PYTHON_INSTALL)
      if(WINDOWS_PYTHON_DEBUG)
        install(
          FILES
            ${LIBDIR}/python/${_PYTHON_VERSION_NO_DOTS}/libs/python${_PYTHON_VERSION_NO_DOTS}.pdb
          DESTINATION "."
          CONFIGURATIONS Release;RelWithDebInfo;MinSizeRel
        )

        install(
          FILES
            ${LIBDIR}/python/${_PYTHON_VERSION_NO_DOTS}/libs/python${_PYTHON_VERSION_NO_DOTS}_d.pdb
          DESTINATION "."
          CONFIGURATIONS Debug
        )
      endif()
    endif()

  endif()

  # Filenames change slightly between FFMPEG versions check both 6.0 and fallback to 5.0
  # to ease the transition between versions.
  if(EXISTS "${LIBDIR}/ffmpeg/lib/avfilter-10.dll")
    windows_install_shared_manifest(
      FILES
        ${LIBDIR}/ffmpeg/lib/avfilter-10.dll
      ALL
    )
  endif()
  if(EXISTS "${LIBDIR}/ffmpeg/lib/avcodec-61.dll")
    windows_install_shared_manifest(
      FILES
        ${LIBDIR}/ffmpeg/lib/avcodec-61.dll
        ${LIBDIR}/ffmpeg/lib/avformat-61.dll
        ${LIBDIR}/ffmpeg/lib/avdevice-61.dll
        ${LIBDIR}/ffmpeg/lib/avutil-59.dll
        ${LIBDIR}/ffmpeg/lib/swscale-8.dll
        ${LIBDIR}/ffmpeg/lib/swresample-5.dll
      ALL
    )
  elseif(EXISTS "${LIBDIR}/ffmpeg/lib/avcodec-60.dll")
    windows_install_shared_manifest(
      FILES
        ${LIBDIR}/ffmpeg/lib/avcodec-60.dll
        ${LIBDIR}/ffmpeg/lib/avformat-60.dll
        ${LIBDIR}/ffmpeg/lib/avdevice-60.dll
        ${LIBDIR}/ffmpeg/lib/avutil-58.dll
        ${LIBDIR}/ffmpeg/lib/swscale-7.dll
        ${LIBDIR}/ffmpeg/lib/swresample-4.dll
      ALL
    )
  else()
    windows_install_shared_manifest(
      FILES
        ${LIBDIR}/ffmpeg/lib/avcodec-59.dll
        ${LIBDIR}/ffmpeg/lib/avformat-59.dll
        ${LIBDIR}/ffmpeg/lib/avdevice-59.dll
        ${LIBDIR}/ffmpeg/lib/avutil-57.dll
        ${LIBDIR}/ffmpeg/lib/swscale-6.dll
        ${LIBDIR}/ffmpeg/lib/swresample-4.dll
      ALL
    )
  endif()
  if(EXISTS ${LIBDIR}/tbb/bin/tbb12.dll) # 4.4
    windows_install_shared_manifest(
      FILES
        ${LIBDIR}/tbb/bin/tbb12.dll
      RELEASE
    )
    windows_install_shared_manifest(
      FILES
        ${LIBDIR}/tbb/bin/tbb12_debug.dll
      DEBUG
    )
  else()
    windows_install_shared_manifest(
      FILES
        ${LIBDIR}/tbb/bin/tbb.dll
      RELEASE
    )
    windows_install_shared_manifest(
      FILES
        ${LIBDIR}/tbb/bin/tbb_debug.dll
      DEBUG
    )
  endif()
  if(WITH_TBB_MALLOC_PROXY)
    windows_install_shared_manifest(
      FILES
        ${LIBDIR}/tbb/bin/tbbmalloc.dll
        ${LIBDIR}/tbb/bin/tbbmalloc_proxy.dll
      RELEASE
    )
    windows_install_shared_manifest(
      FILES
        ${LIBDIR}/tbb/bin/tbbmalloc_debug.dll
        ${LIBDIR}/tbb/bin/tbbmalloc_proxy_debug.dll
      DEBUG
    )
    list(APPEND LIB ${TBB_MALLOC_LIBRARIES})
  endif()

  if(EXISTS ${LIBDIR}/sndfile/lib/sndfile.dll)
    set(SNDFILE_DLL ${LIBDIR}/sndfile/lib/sndfile.dll)
  else()
    set(SNDFILE_DLL ${LIBDIR}/sndfile/lib/libsndfile-1.dll)
  endif()

  windows_install_shared_manifest(
    FILES ${SNDFILE_DLL}
    ALL
  )
  unset(SNDFILE_DLL)

  windows_install_shared_manifest(
    FILES ${LIBDIR}/shaderc/bin/shaderc_shared.dll
    ALL
  )

  windows_install_shared_manifest(
    FILES
      ${LIBDIR}/openal/lib/OpenAL32.dll
    ALL
  )

  windows_install_shared_manifest(
    FILES ${LIBDIR}/sdl/lib/SDL2.dll
    ALL
  )

  if(WITH_SYSTEM_AUDASPACE)
    install(
      FILES
        ${LIBDIR}/audaspace/lib/audaspace.dll
        ${LIBDIR}/audaspace/lib/audaspace-c.dll
        ${LIBDIR}/audaspace/lib/audaspace-py.dll
      DESTINATION "."
    )
  endif()


  if(NOT WITH_PYTHON_MODULE)
    get_filename_component(PYTHON_EXECUTABLE_NAME_ONLY ${PYTHON_EXECUTABLE} NAME)
    # Configure then generate file. Note that this copies the literal generator
    # for the Python executable name. So the file needs to be generated afterwards
    # to get the correct name.
    configure_file(
      ${CMAKE_SOURCE_DIR}/release/windows/batch/blender_system_info.cmd.in
      ${CMAKE_BINARY_DIR}/release/windows/batch/blender_system_info.with_vars.cmd.in
      @ONLY
    )
    unset(PYTHON_EXECUTABLE_NAME_ONLY)
    # Replace Python executable generator with actual executable name.
    file(GENERATE
      OUTPUT ${CMAKE_BINARY_DIR}/release/windows/batch/blender_system_info_$<CONFIG>.cmd
      INPUT ${CMAKE_BINARY_DIR}/release/windows/batch/blender_system_info.with_vars.cmd.in
    )
    install(
      FILES
      ${CMAKE_BINARY_DIR}/release/windows/batch/blender_system_info_$<CONFIG>.cmd
      DESTINATION ${TARGETDIR_EXE}
      RENAME blender_system_info.cmd
    )
    if(WITH_CYCLES)
      install(
        FILES
          ${CMAKE_SOURCE_DIR}/release/windows/batch/blender_debug_cycles.cmd
        DESTINATION ${TARGETDIR_EXE}
      )
    endif()
    install(
      FILES
        ${CMAKE_SOURCE_DIR}/release/windows/batch/blender_debug_gpu.cmd
        ${CMAKE_SOURCE_DIR}/release/windows/batch/blender_debug_gpu_glitchworkaround.cmd
        ${CMAKE_SOURCE_DIR}/release/windows/batch/blender_debug_log.cmd
        ${CMAKE_SOURCE_DIR}/release/windows/batch/blender_factory_startup.cmd
        ${CMAKE_SOURCE_DIR}/release/windows/batch/blender_factory_startup_vulkan.cmd
        ${CMAKE_SOURCE_DIR}/release/windows/batch/blender_startup_opengl.cmd
        ${CMAKE_SOURCE_DIR}/release/windows/batch/blender_startup_vulkan.cmd
        ${CMAKE_SOURCE_DIR}/release/windows/batch/blender_oculus.cmd
        ${CMAKE_SOURCE_DIR}/release/windows/batch/oculus.json
      DESTINATION ${TARGETDIR_EXE}
    )
  endif()

  if(WITH_BLENDER_THUMBNAILER)
    install(
      TARGETS BlendThumb
      DESTINATION "."
    )
  endif()

  if(WITH_PYTHON_MODULE AND TARGETDIR_BPY)
    install(
      TARGETS blender
      LIBRARY DESTINATION ${TARGETDIR_BPY}
    )
  endif()

  if(PLATFORM_BUNDLED_LIBRARIES)
    windows_process_platform_bundled_libraries("${PLATFORM_BUNDLED_LIBRARIES}")
  endif()
elseif(APPLE)
  if(NOT WITH_PYTHON_MODULE)
    # Uppercase name for app bundle.
    set_target_properties(blender PROPERTIES OUTPUT_NAME Blender)
  endif()

  set(OSX_APP_SOURCEDIR ${CMAKE_SOURCE_DIR}/release/darwin/Blender.app)

  # Setup `Info.plist`.
  execute_process(
    COMMAND date "+%Y-%m-%d"
    OUTPUT_VARIABLE BLENDER_DATE
    OUTPUT_STRIP_TRAILING_WHITESPACE
  )

  set_target_properties(blender PROPERTIES
    MACOSX_BUNDLE_INFO_PLIST ${OSX_APP_SOURCEDIR}/Contents/Info.plist
    MACOSX_BUNDLE_SHORT_VERSION_STRING "${BLENDER_VERSION}.${BLENDER_VERSION_PATCH}"
    MACOSX_BUNDLE_LONG_VERSION_STRING "${BLENDER_VERSION}.${BLENDER_VERSION_PATCH} ${BLENDER_DATE}"
  )

  # Xcode Archiving support for distributing the application (Testflight, App Store Connect, etc...)
  set_target_properties(blender PROPERTIES
    XCODE_ATTRIBUTE_SKIP_INSTALL "NO"
    XCODE_ATTRIBUTE_INSTALL_PATH "$(LOCAL_APPS_DIR)"
  )

  if(WITH_BLENDER_THUMBNAILER)
    set(OSX_THUMBNAILER_SOURCEDIR ${OSX_APP_SOURCEDIR}/Contents/PlugIns/blender-thumbnailer.appex)
    set_target_properties(blender-thumbnailer PROPERTIES
      BUNDLE_EXTENSION appex
      MACOSX_BUNDLE_INFO_PLIST ${OSX_THUMBNAILER_SOURCEDIR}/Contents/Info.plist
      MACOSX_BUNDLE_SHORT_VERSION_STRING "${BLENDER_VERSION}.${BLENDER_VERSION_PATCH}"
      MACOSX_BUNDLE_LONG_VERSION_STRING "${BLENDER_VERSION}.${BLENDER_VERSION_PATCH} ${BLENDER_DATE}"
    )
  endif()

  # Gather the date in finder-style.
  execute_process(
    COMMAND date "+%m/%d/%Y/%H:%M"
    OUTPUT_VARIABLE SETFILE_DATE
    OUTPUT_STRIP_TRAILING_WHITESPACE
  )

  # Give the bundle actual creation/modification date.
  #
  # Note that the directory might not yet exist, which happens when CMAKE is first run.
  if(NOT EXISTS ${EXECUTABLE_OUTPUT_PATH}/Blender.app)
    file(MAKE_DIRECTORY ${EXECUTABLE_OUTPUT_PATH}/Blender.app)
  endif()
  execute_process(
    COMMAND SetFile -d ${SETFILE_DATE} -m ${SETFILE_DATE} ${EXECUTABLE_OUTPUT_PATH}/Blender.app
  )

  set(BLENDER_BIN "bin/blender")
  install(
    TARGETS blender
    DESTINATION "."
  )

  install(
    FILES ${OSX_APP_SOURCEDIR}/Contents/PkgInfo
    DESTINATION "Blender.app/Contents"
  )

  install_dir(
    ${OSX_APP_SOURCEDIR}/Contents/Resources
    "Blender.app/Contents"
  )

  if(WITH_BLENDER_THUMBNAILER)
    install(
      TARGETS blender-thumbnailer
      DESTINATION "./Blender.app/Contents/PlugIns"
    )
  endif()

  if(PLATFORM_BUNDLED_LIBRARIES AND TARGETDIR_LIB)
    install(
      FILES ${PLATFORM_BUNDLED_LIBRARIES}
      DESTINATION ${TARGETDIR_LIB}
    )
  endif()

  if(WITH_VULKAN_BACKEND)
    install(
      FILES ${VULKAN_LIBRARY}
      DESTINATION ${TARGETDIR_LIB}
    )
  endif()

  # Python.
  if(WITH_PYTHON AND NOT WITH_PYTHON_MODULE AND NOT WITH_PYTHON_FRAMEWORK)
    # Copy the python libraries into the install directory.
    install_dir(
      ${PYTHON_LIBPATH}/python${PYTHON_VERSION}
      ${TARGETDIR_VER}/python/lib
    )

    # Install Python executable.
    install(
      PROGRAMS ${PYTHON_EXECUTABLE}
      DESTINATION ${TARGETDIR_VER}/python/bin
    )

    # Needed for `distutils/pip`.
    # Get the last part of the include dir, will be `python{version}{abiflag}`.
    get_filename_component(_py_inc_suffix ${PYTHON_INCLUDE_DIR} NAME)
    install(
      FILES ${PYTHON_INCLUDE_DIR}/pyconfig.h
      DESTINATION ${TARGETDIR_VER}/python/include/${_py_inc_suffix}
    )
    unset(_py_inc_suffix)
  endif()

  if(WITH_PYTHON_MODULE AND TARGETDIR_BPY)
    install(
      TARGETS blender
      LIBRARY DESTINATION ${TARGETDIR_BPY}
    )
  endif()

  if(NOT WITH_PYTHON_MODULE AND WITH_PYTHON AND WITH_PYTHON_INSTALL)
    get_filename_component(PYTHON_EXECUTABLE_NAME_ONLY ${PYTHON_EXECUTABLE} NAME)
    configure_file(
      ${CMAKE_SOURCE_DIR}/release/darwin/scripts/blender-system-info.sh.in
      ${CMAKE_BINARY_DIR}/release/darwin/scripts/blender-system-info.sh
      @ONLY
    )
    unset(PYTHON_EXECUTABLE_NAME_ONLY)
    install(
      PROGRAMS ${CMAKE_BINARY_DIR}/release/darwin/scripts/blender-system-info.sh
      DESTINATION "Blender.app/Contents/Resources"
    )
  endif()

endif()

# Windows already installs these, for Unix we need to pick just the VFX libs from
# the site-packages directory.
if(WITH_PYTHON_MODULE AND LIBDIR AND NOT WIN32)
  # It's possible for a build using LIBDIR to reference a non-standard Python installation.
  path_is_prefix(LIBDIR PYTHON_INCLUDE_DIR _is_python_in_libdir)
  if(_is_python_in_libdir)
    install(
      DIRECTORY ${LIBDIR}/python/lib/python${PYTHON_VERSION}/site-packages/MaterialX
      DESTINATION ${TARGETDIR_SITE_PACKAGES}
    )
    install(
      DIRECTORY ${LIBDIR}/python/lib/python${PYTHON_VERSION}/site-packages/OpenImageIO
      DESTINATION ${TARGETDIR_SITE_PACKAGES}
    )
    install(
      DIRECTORY ${LIBDIR}/python/lib/python${PYTHON_VERSION}/site-packages/PyOpenColorIO
      DESTINATION ${TARGETDIR_SITE_PACKAGES}
    )
    install(
      DIRECTORY ${LIBDIR}/python/lib/python${PYTHON_VERSION}/site-packages/oslquery
      DESTINATION ${TARGETDIR_SITE_PACKAGES} OPTIONAL
    )
    install(
      DIRECTORY ${LIBDIR}/python/lib/python${PYTHON_VERSION}/site-packages/pxr
      DESTINATION ${TARGETDIR_SITE_PACKAGES}
    )
    if(APPLE)
      set(_openvdb_filename openvdb.cpython-${PYTHON_VERSION_NO_DOTS}-darwin.so)
    else()
      set(_openvdb_filename openvdb.cpython-${PYTHON_VERSION_NO_DOTS}-${CMAKE_SYSTEM_PROCESSOR}-linux-gnu.so)
    endif()
    install(
      FILES ${LIBDIR}/python/lib/python${PYTHON_VERSION}/site-packages/${_openvdb_filename}
      DESTINATION ${TARGETDIR_SITE_PACKAGES}
    )
  endif()
  unset(_is_python_in_libdir)
endif()

# -----------------------------------------------------------------------------
# Generic Install, for all targets

if(DEFINED TARGETDIR_TEXT)

  configure_file(
    ${CMAKE_SOURCE_DIR}/release/text/readme.html
    ${CMAKE_BINARY_DIR}/release/text/readme.html
    @ONLY
  )
  list(APPEND BLENDER_TEXT_FILES
    ${CMAKE_BINARY_DIR}/release/text/readme.html
  )

  install(
    FILES ${BLENDER_TEXT_FILES}
    DESTINATION "${TARGETDIR_TEXT}"
  )

  install(
    DIRECTORY ${CMAKE_SOURCE_DIR}/release/license
    DESTINATION "${TARGETDIR_TEXT}"
  )
endif()

# Create a system extensions directory (users or administrators may populate this).
# This only contains a `readme.txt` explaining it's purpose.
install(
  DIRECTORY ${CMAKE_SOURCE_DIR}/release/extensions
  DESTINATION ${TARGETDIR_VER}
)

# Install more files specified elsewhere.
delayed_do_install(${TARGETDIR_VER})

unset(BLENDER_TEXT_FILES)
unset(TARGETDIR_TEXT)


# -----------------------------------------------------------------------------
# Geometry Icons

# Geometry icons.
get_property(_icon_names GLOBAL PROPERTY ICON_GEOM_NAMES)
set(_icon_files)
foreach(_f ${_icon_names})
  list(APPEND _icon_files
    "${CMAKE_SOURCE_DIR}/release/datafiles/icons/${_f}.dat"
  )
endforeach()
install(
  FILES ${_icon_files}
  DESTINATION ${TARGETDIR_VER}/datafiles/icons
)

unset(_icon_names)
unset(_icon_files)
unset(_f)


# -----------------------------------------------------------------------------
# Studio Lights

install(
  DIRECTORY ${CMAKE_SOURCE_DIR}/release/datafiles/studiolights
  DESTINATION ${TARGETDIR_VER}/datafiles
)


# -----------------------------------------------------------------------------
# Bundle assets

set(ASSET_BUNDLE_DIR ${CMAKE_SOURCE_DIR}/assets/)

if(EXISTS "${ASSET_BUNDLE_DIR}")
  install(
    DIRECTORY ${ASSET_BUNDLE_DIR}
    DESTINATION ${TARGETDIR_VER}/datafiles/assets
  )
endif()


# -----------------------------------------------------------------------------
# Setup link libraries

add_dependencies(blender makesdna)
target_link_libraries(blender PRIVATE ${LIB})
unset(LIB)

setup_platform_linker_flags(blender)
setup_platform_linker_libs(blender)

if(DEFINED PLATFORM_SYMBOLS_MAP)
  set_target_properties(blender PROPERTIES LINK_DEPENDS ${PLATFORM_SYMBOLS_MAP})
endif()

blender_target_include_dirs(blender ${INC})

# -----------------------------------------------------------------------------
# USD registry.

# USD requires a set of JSON files that define the standard schemas.
# These files are required at runtime.
if(WITH_USD)
  add_definitions(-DWITH_USD)
  absolute_include_dirs(../blender/io/usd)
endif()

# Always install USD shared library and `datafiles` regardless if Blender
# itself uses them, the bundled Python module still needs it.
if((DEFINED LIBDIR) AND TARGETDIR_LIB)
  # On windows the USD library sits in `./blender.shared` copy the files
  # relative to the location of the USD DLL, if the DLL does not exist
  # assume we are linking against the static 3.5 lib.
  if(WITH_USD)
    if(WIN32 AND
        (
          EXISTS ${LIBDIR}/usd/lib/usd_usd_ms.dll OR  # USD 22.03
          EXISTS ${LIBDIR}/usd/lib/usd_ms.dll         # USD 22.11
        )
      )
      install(DIRECTORY
        ${USD_LIBRARY_DIR}/usd
        DESTINATION ${TARGETDIR_LIB}
      )
      install(DIRECTORY
        ${LIBDIR}/usd/plugin/usd/hdStorm
        ${LIBDIR}/usd/plugin/usd/usdShaders
        ${LIBDIR}/usd/plugin/usd/hioOiio
        DESTINATION "blender.shared/usd"
      )
    elseif(USD_PYTHON_SUPPORT)
      install(DIRECTORY
        ${USD_LIBRARY_DIR}/usd
        DESTINATION ${TARGETDIR_LIB}
      )
      install(DIRECTORY
        ${LIBDIR}/usd/plugin/usd/hdStorm
        ${LIBDIR}/usd/plugin/usd/usdShaders
        DESTINATION ${TARGETDIR_LIB}/usd
      )
    else()
      install(DIRECTORY
        ${USD_LIBRARY_DIR}/usd
        DESTINATION "${TARGETDIR_VER}/datafiles"
      )
      install(DIRECTORY
        ${LIBDIR}/usd/plugin/usd/hdStorm
        ${LIBDIR}/usd/plugin/usd/usdShaders
        DESTINATION "${TARGETDIR_VER}/datafiles/usd"
      )
    endif()
  endif()
  if(WIN32)
    # If this file exists we are building against a 3.5 22.03 library directory
    # that needs these DLL's installed.
    if(EXISTS ${LIBDIR}/usd/lib/usd_usd_ms.dll)
      windows_install_shared_manifest(FILES
        ${LIBDIR}/usd/lib/usd_usd_ms.dll
        RELEASE
      )
      windows_install_shared_manifest(FILES
        ${LIBDIR}/usd/lib/usd_usd_ms_d.dll
        DEBUG
      )
    endif()
    # If this file exists we are building against a 3.5 22.11 library directory
    # that needs these DLL's installed.
    if(EXISTS ${LIBDIR}/usd/lib/usd_ms.dll)
      windows_install_shared_manifest(FILES
        ${LIBDIR}/usd/lib/usd_ms.dll
        RELEASE
      )
      windows_install_shared_manifest(FILES
        ${LIBDIR}/usd/lib/usd_ms_d.dll
        DEBUG
      )
    endif()
  endif()
endif()

# Always install MaterialX files regardless if Blender itself uses them, the
# bundled Python module still needs it.
if((DEFINED LIBDIR) AND TARGETDIR_LIB AND WITH_MATERIALX)
  install(
    DIRECTORY ${LIBDIR}/materialx/libraries
    DESTINATION "${TARGETDIR_LIB}/materialx"
  )
endif()

if(WIN32 AND DEFINED BOOST_LIBPATH)
  if(EXISTS ${BOOST_LIBPATH})
    set(BOOST_COMPONENTS atomic chrono date_time filesystem
      iostreams locale program_options regex
      serialization system thread wave wserialization
      python${_PYTHON_VERSION_NO_DOTS} numpy${_PYTHON_VERSION_NO_DOTS}
    )
    foreach(component ${BOOST_COMPONENTS})
      if(EXISTS ${BOOST_LIBPATH}/${BOOST_PREFIX}boost_${component}-${BOOST_POSTFIX}.dll)
        windows_install_shared_manifest(
          FILES ${BOOST_LIBPATH}/${BOOST_PREFIX}boost_${component}-${BOOST_POSTFIX}.dll
          RELEASE
        )
        windows_install_shared_manifest(
          FILES ${BOOST_LIBPATH}/${BOOST_PREFIX}boost_${component}-${BOOST_DEBUG_POSTFIX}.dll
          DEBUG
        )
      endif()
    endforeach()
  endif()
endif()

if(WIN32)
  if(WITH_CYCLES_DEVICE_HIPRT)
    if(EXISTS ${LIBDIR}/hiprt/bin/hiprt64.dll)
      install(
        FILES ${LIBDIR}/hiprt/bin/hiprt64.dll
        DESTINATION "./"
      )
    endif()
  endif()
endif()

# `vcpkg` substitutes our libraries with theirs, which will cause issues when you run
# these builds on other systems due to missing DLL's. So we opt out the use of `vcpkg`.
if(WIN32)
  set_target_properties(blender PROPERTIES VS_GLOBAL_VcpkgEnabled "false")
  set_target_properties(blender PROPERTIES
    PDB_NAME "blender_private"
    PDB_OUTPUT_DIRECTORY "${CMAKE_CURRENT_BINARY_DIR}/$<CONFIG>"
  )
  # If compiling with clang-cl we skip PDBSTRIPPED.
  # There's doesn't seem to be a switch for it currently
  if(WITH_WINDOWS_RELEASE_PDB AND WITH_WINDOWS_RELEASE_STRIPPED_PDB)
    # This is slightly messy, but single target generators like ninja will not have the
    # `CMAKE_CFG_INTDIR` variable and multi-target generators like `msbuild` will not have
    # `CMAKE_BUILD_TYPE`. This can be simplified by `target_link_options` and the `$<CONFIG>`
    # generator expression in newer CMAKE (2.13+) but until that time this fill have suffice.
    if(CMAKE_BUILD_TYPE)
      set_property(
        TARGET blender APPEND_STRING PROPERTY LINK_FLAGS_RELEASE
        " /PDBSTRIPPED:${CMAKE_CURRENT_BINARY_DIR}/${CMAKE_BUILD_TYPE}/blender_public.pdb"
      )
    else()
      set_property(
        TARGET blender APPEND_STRING PROPERTY LINK_FLAGS_RELEASE
        " /PDBSTRIPPED:${CMAKE_CURRENT_BINARY_DIR}/${CMAKE_CFG_INTDIR}/blender_public.pdb"
      )
    endif()
  endif()
endif()

# -----------------------------------------------------------------------------
# Setup launcher

if(WIN32 AND NOT WITH_PYTHON_MODULE)
  set(BLENDER_BIN "blender.exe")
  install(
    TARGETS blender blender-launcher
    COMPONENT Blender
    DESTINATION "."
  )
  if(WITH_CPU_CHECK)
    install(
      TARGETS blender_cpu_check
      COMPONENT Blender
      DESTINATION "."
    )
  endif()
  set_target_properties(
    blender
    PROPERTIES
      VS_USER_PROPS "blender.Cpp.user.props"
  )
endif()

# -----------------------------------------------------------------------------
# Windows shared library manifest
if(WIN32)
  windows_generate_shared_manifest()
endif()

# -----------------------------------------------------------------------------
# Steps that Run Blender
#
# As executing Blender is needed - it's important this operation runs after the shared
# libraries have been installed to their destination.

if(UNIX AND NOT APPLE)
  if(NOT WITH_PYTHON_MODULE)
    if(WITH_DOC_MANPAGE)
      # Only run the command to generate the man-page when it may be outdated.
      # The `IS_NEWER_THAN` checks always run when files are missing.

      # NOTE: While `${EXECUTABLE_OUTPUT_PATH}/blender` may work in some cases
      # Blender's requirement of libraries mean the installation path must be used.
      install(
        CODE "\
set(BLENDER_BIN \"${CMAKE_INSTALL_PREFIX}/${BLENDER_BIN}\")\n\
if(DEFINED ENV\{DESTDIR\})\n\
  set(BLENDER_BIN \"$ENV\{DESTDIR\}/$\{BLENDER_BIN\}\")\n\
endif()\n\
set(PYTHON_EXECUTABLE \"${PYTHON_EXECUTABLE}\")\n\
set(MANPAGE_GEN \"${CMAKE_SOURCE_DIR}/doc/manpage/blender.1.py\")\n\
set(MANPAGE_OUT \"${CMAKE_CURRENT_BINARY_DIR}/blender.1\")\n\
if(\n\
  ($\{BLENDER_BIN\} IS_NEWER_THAN $\{MANPAGE_OUT\}) OR\n\
  ($\{MANPAGE_GEN\} IS_NEWER_THAN $\{MANPAGE_OUT\})\n\
)\n\
  set(ENV{ASAN_OPTIONS} \"$ENV{ASAN_OPTIONS}:\
exitcode=0:\
check_initialization_order=0:\
strict_init_order=0:\
detect_leaks=0\"\n\
  )\n\
  execute_process(\n\
    COMMAND\n\
    $\{PYTHON_EXECUTABLE\} $\{MANPAGE_GEN\}\n\
    --blender $\{BLENDER_BIN\}\n\
    --output $\{MANPAGE_OUT\}\n\
  )\n\
endif()\n\
"
        DEPENDS blender
      )

      if(WITH_INSTALL_PORTABLE)
        install(
          FILES ${CMAKE_CURRENT_BINARY_DIR}/blender.1
          DESTINATION "."
        )
      else()
        # Manual page (only with `blender` binary).
        install(
          FILES ${CMAKE_CURRENT_BINARY_DIR}/blender.1
          DESTINATION "./share/man/man1"
        )
      endif()
    endif()
  endif()
endif()

# -----------------------------------------------------------------------------
# Post-install script

if(POSTINSTALL_SCRIPT)
  install(SCRIPT ${POSTINSTALL_SCRIPT})
endif()


--- source/blender/animrig/CMakeLists.txt ---
# SPDX-FileCopyrightText: 2023 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

set(INC
  PUBLIC .
  intern

  ../editors/include
  ../makesrna
  # RNA_prototypes.hh
  ${CMAKE_BINARY_DIR}/source/blender/makesrna
)

set(INC_SYS
)

set(SRC
  intern/action.cc
  intern/action_iterators.cc
  intern/action_legacy.cc
  intern/action_runtime.cc
  intern/action_selection.cc
  intern/anim_rna.cc
  intern/animdata.cc
  intern/armature.cc
  intern/bone_collections.cc
  intern/bonecolor.cc
  intern/driver.cc
  intern/evaluation.cc
  intern/fcurve.cc
  intern/keyframing.cc
  intern/keyframing_auto.cc
  intern/keyingsets.cc
  intern/nla.cc
  intern/pose.cc
  intern/versioning.cc
  intern/visualkey.cc

  ANIM_action.hh
  ANIM_action_iterators.hh
  ANIM_action_legacy.hh
  ANIM_animdata.hh
  ANIM_armature.hh
  ANIM_armature_iter.hh
  ANIM_bone_collections.hh
  ANIM_bonecolor.hh
  ANIM_driver.hh
  ANIM_evaluation.hh
  ANIM_fcurve.hh
  ANIM_keyframing.hh
  ANIM_keyingsets.hh
  ANIM_nla.hh
  ANIM_pose.hh
  ANIM_rna.hh
  ANIM_versioning.hh
  ANIM_visualkey.hh
  intern/action_runtime.hh
  intern/bone_collections_internal.hh
  intern/evaluation_internal.hh
)

set(LIB
  PRIVATE bf::blenkernel
  bf::blenlib
  PRIVATE bf::blenloader
  PRIVATE bf::blentranslation
  PRIVATE bf::depsgraph
  bf::dna
  PRIVATE bf::intern::guardedalloc
  PRIVATE bf::intern::atomic
  PRIVATE bf::intern::clog
  PRIVATE bf::windowmanager
  PRIVATE bf::extern::fmtlib
)

blender_add_lib(bf_animrig "${SRC}" "${INC}" "${INC_SYS}" "${LIB}")
add_library(bf::animrig ALIAS bf_animrig)

if(WITH_GTESTS)
  set(TEST_INC
  )
  set(TEST_SRC
    intern/action_iterators_test.cc
    intern/action_legacy_test.cc
    intern/action_test.cc
    intern/bone_collections_test.cc
    intern/evaluation_test.cc
    intern/keyframing_test.cc
    intern/nla_test.cc
    intern/pose_test.cc
    intern/versioning_test.cc
  )
  set(TEST_LIB
    PRIVATE bf::animrig
  )
  blender_add_test_suite_lib(animrig "${TEST_SRC}" "${INC};${TEST_INC}" "${INC_SYS}" "${LIB};${TEST_LIB}")
endif()


--- source/blender/asset_system/CMakeLists.txt ---
# SPDX-FileCopyrightText: 2023 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

set(INC
  PUBLIC .
  intern
  intern/library_types
)

set(INC_SYS
)

set(SRC
  intern/asset_catalog.cc
  intern/asset_catalog_collection.cc
  intern/asset_catalog_definition_file.cc
  intern/asset_catalog_path.cc
  intern/asset_catalog_tree.cc
  intern/asset_library.cc
  intern/asset_library_service.cc
  intern/asset_representation.cc
  intern/disk_file_hash_service.cc
  intern/library_types/all_library.cc
  intern/library_types/essentials_library.cc
  intern/library_types/on_disk_library.cc
  intern/library_types/preferences_on_disk_library.cc
  intern/library_types/runtime_library.cc
  intern/utils.cc

  AS_asset_catalog.hh
  AS_asset_catalog_path.hh
  AS_asset_catalog_tree.hh
  AS_asset_library.hh
  AS_asset_representation.hh
  AS_disk_file_hash_service.hh
  AS_essentials_library.hh
  intern/asset_catalog_collection.hh
  intern/asset_catalog_definition_file.hh
  intern/asset_library_service.hh
  intern/library_types/all_library.hh
  intern/library_types/essentials_library.hh
  intern/library_types/on_disk_library.hh
  intern/library_types/preferences_on_disk_library.hh
  intern/library_types/runtime_library.hh
  intern/utils.hh
)

set(LIB
  PRIVATE bf::blenkernel
  PRIVATE bf::blenlib
  PRIVATE bf::dna
  PRIVATE bf::imbuf
  PRIVATE bf::intern::clog
  PRIVATE bf::intern::guardedalloc
)

if(WITH_PYTHON)
  list(APPEND INC
    ../python
  )
  add_definitions(-DWITH_PYTHON)
endif()

blender_add_lib(bf_asset_system "${SRC}" "${INC}" "${INC_SYS}" "${LIB}")
add_library(bf::asset_system ALIAS bf_asset_system)

if(WITH_GTESTS)
  set(TEST_INC
    ../editors/asset
  )
  set(TEST_SRC
    tests/asset_catalog_path_test.cc
    tests/asset_catalog_test.cc
    tests/asset_catalog_tree_test.cc
    tests/asset_library_service_test.cc
    tests/asset_library_test.cc
    tests/asset_representation_test.cc
  )
  set(TEST_COMMON_SRC
    tests/asset_library_test_common.hh
  )
  set(TEST_LIB
    bf_asset_system
    PRIVATE bf_editor_asset
  )
  blender_add_test_suite_lib(asset_system
    "${TEST_SRC}" "${INC};${TEST_INC}" "${INC_SYS}" "${LIB};${TEST_LIB}" "${TEST_COMMON_SRC}"
  )
endif()


--- source/blender/blendthumb/CMakeLists.txt ---
# SPDX-FileCopyrightText: 2006 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

# -----------------------------------------------------------------------------
# Shared Thumbnail Extraction Logic

include_directories(
  ../blenlib
  ../blenloader_core
  ../makesdna
  ../../../intern/guardedalloc
)

include_directories(
  SYSTEM
  ${ZLIB_INCLUDE_DIRS}
)

set(SRC
  src/blendthumb.hh
  src/blendthumb_extract.cc
  src/blendthumb_png.cc
)

if(WITH_BUILDINFO)
  list(APPEND SRC
    "$<TARGET_OBJECTS:buildinfoobj>"
  )
endif()

if(WIN32)
  # -----------------------------------------------------------------------------
  # Build `BlendThumb.dll`

  set(SRC_WIN32
    src/blendthumb_win32.cc
    src/blendthumb_win32.def
    src/blendthumb_win32.rc
    src/blendthumb_win32_dll.cc
  )

  add_definitions(-DNOMINMAX)

  add_library(BlendThumb SHARED ${SRC} ${SRC_WIN32})

  setup_platform_linker_flags(BlendThumb)
  setup_platform_linker_libs(BlendThumb)

  target_link_libraries(BlendThumb PRIVATE bf_blenlib bf_blenloader_core dbghelp.lib Version.lib Comctl32.lib)
  # `blenlib` drags in a whole bunch of dependencies on shared libraries, none of which are used
  # by `blenthumb`, but will cause load issues since the debug linker will not eliminate them.
  # Link with /OPT:ref to force elimination of those unused dependencies this is already
  # enabled by default on the release mode flags.
  set_target_properties(BlendThumb PROPERTIES LINK_FLAGS "/OPT:ref")
  set_target_properties(BlendThumb PROPERTIES LINK_FLAGS_DEBUG "/NODEFAULTLIB:msvcrt")
  set_target_properties(BlendThumb PROPERTIES VS_GLOBAL_VcpkgEnabled "false")

elseif(APPLE)
  # -----------------------------------------------------------------------------
  # Build `blender-thumbnailer.appex` app extension.
  set(SRC_APPEX
    src/thumbnail_provider.h
    src/thumbnail_provider.mm
  )

  add_executable(blender-thumbnailer MACOSX_BUNDLE ${SRC} ${SRC_APPEX})
  setup_platform_linker_flags(blender-thumbnailer)
  setup_platform_linker_libs(blender-thumbnailer)
  target_link_libraries(blender-thumbnailer
    bf_blenlib
    bf_blenloader_core
    # Avoid linker error about undefined _main symbol.
    "-e _NSExtensionMain"
    "-framework QuickLookThumbnailing"
  )
  # The RPATH here points to the main Blender Resources/lib directory.
  # Avoid duplicating the large `dylibs` (~300MB).
  set_target_properties(blender-thumbnailer PROPERTIES
    INSTALL_RPATH "@loader_path/../../../../Resources/lib"
    # Prevent Xcode from overwriting the signature.
    XCODE_ATTRIBUTE_CODE_SIGN_IDENTITY ""
  )
  # CMake needs the target defined in the same file as add_custom_command.
  # It needs to be code-signed (ad-hoc in this case)
  # even on developer machine to generate thumbnails.
  # Command taken from XCode build process.
  add_custom_command(
    TARGET blender-thumbnailer POST_BUILD
    COMMAND codesign --deep --force --sign -
      --entitlements "${CMAKE_SOURCE_DIR}/release/darwin/thumbnailer_entitlements.plist"
      --timestamp=none $<TARGET_BUNDLE_DIR:blender-thumbnailer>
  )
elseif(UNIX)
  # -----------------------------------------------------------------------------
  # Build `blender-thumbnailer` executable

  set(SRC_CMD
    src/blender_thumbnailer.cc
  )

  add_executable(blender-thumbnailer ${SRC} ${SRC_CMD})
  setup_platform_linker_flags(blender-thumbnailer)
  setup_platform_linker_libs(blender-thumbnailer)
  target_link_libraries(blender-thumbnailer PRIVATE bf_blenlib bf_blenloader_core)
  target_link_libraries(blender-thumbnailer PRIVATE bf::dependencies::pthreads)
endif()


--- source/blender/blenfont/CMakeLists.txt ---
# SPDX-FileCopyrightText: 2008 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

set(INC
  PUBLIC .
  ../makesrna
)

set(INC_SYS
)

set(SRC
  intern/blf.cc
  intern/blf_default.cc
  intern/blf_dir.cc
  intern/blf_font.cc
  intern/blf_font_default.cc
  intern/blf_glyph.cc
  intern/blf_glyph_curves.cc
  intern/blf_thumbs.cc
  BLF_api.hh
  BLF_enums.hh
  intern/blf_internal.hh
  intern/blf_internal_types.hh
)

set(LIB
  PRIVATE bf::blenkernel
  PRIVATE bf::blenlib
  PRIVATE bf::blentranslation
  PRIVATE bf::dna
  PRIVATE bf::gpu
  PRIVATE bf::imbuf
  PRIVATE bf::intern::guardedalloc
  PRIVATE bf::dependencies::freetype
)

if(WIN32)
  list(APPEND SRC
    intern/blf_font_win32_compat.cc
  )
endif()

if(WITH_PYTHON)
  add_definitions(-DWITH_PYTHON)
  list(APPEND INC
    ../python
  )
endif()

if(WITH_HEADLESS)
  add_definitions(-DWITH_HEADLESS)
else()
  # SVG icons.
  list(APPEND LIB
    PRIVATE bf::extern::nanosvg
    PRIVATE bf::editor::datafiles
  )

endif()

blender_add_lib(bf_blenfont "${SRC}" "${INC}" "${INC_SYS}" "${LIB}")
add_library(bf::blenfont ALIAS bf_blenfont)

if(WITH_GTESTS)
  set(TEST_SRC
    tests/BLF_tests.cc
  )
  set(TEST_INC
  )
  set(TEST_LIB
    bf::blenfont
  )
  blender_add_test_suite_lib(blenfont "${TEST_SRC}" "${INC};${TEST_INC}" "${INC_SYS}" "${LIB};${TEST_LIB}")
endif()


--- source/blender/blenkernel/CMakeLists.txt ---
# SPDX-FileCopyrightText: 2006 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

set(INC
  PUBLIC .
  ../ikplugin
  ../makesrna
  ../modifiers
  ../nodes/geometry/include
  ../shader_fx
  ../simulation
  ../windowmanager
  ../../../intern/eigen
  ../../../intern/ghost
  ../../../intern/iksolver/extern
  ../../../intern/libmv
  ../../../intern/mantaflow/extern
  ../../../intern/memutil
  ../../../intern/mikktspace
  ../../../intern/opensubdiv

  # RNA_prototypes.hh
  ${CMAKE_BINARY_DIR}/source/blender/makesrna
)

set(INC_SYS
)

set(SRC
  intern/action.cc
  intern/action_bones.cc
  intern/action_mirror.cc
  intern/addon.cc
  intern/anim_data.cc
  intern/anim_data_bmain_utils.cc
  intern/anim_path.cc
  intern/anim_sys.cc
  intern/anim_visualization.cc
  intern/anonymous_attribute_id.cc
  intern/appdir.cc
  intern/armature.cc
  intern/armature_deform.cc
  intern/armature_selection.cc
  intern/armature_update.cc
  intern/asset.cc
  intern/asset_edit.cc
  intern/asset_weak_reference.cc
  intern/attribute.cc
  intern/attribute_access.cc
  intern/attribute_legacy_convert.cc
  intern/attribute_math.cc
  intern/attribute_storage.cc
  intern/attribute_storage_access.cc
  intern/autoexec.cc
  intern/bake_data_block_map.cc
  intern/bake_geometry_nodes_modifier.cc
  intern/bake_geometry_nodes_modifier_pack.cc
  intern/bake_items.cc
  intern/bake_items_paths.cc
  intern/bake_items_serialize.cc
  intern/bake_items_socket.cc
  intern/blender.cc
  intern/blender_cli_command.cc
  intern/blender_copybuffer.cc
  intern/blender_undo.cc
  intern/blender_user_menu.cc
  intern/blendfile.cc
  intern/blendfile_link_append.cc
  intern/boids.cc
  intern/bpath.cc
  intern/brush.cc
  intern/bvhutils.cc
  intern/cachefile.cc
  intern/callbacks.cc
  intern/camera.cc
  intern/cloth.cc
  intern/collection.cc
  intern/collision.cc
  intern/colorband.cc
  intern/colortools.cc
  intern/compositor.cc
  intern/compute_contexts.cc
  intern/constraint.cc
  intern/context.cc
  intern/cpp_types.cc
  intern/crazyspace.cc
  intern/cryptomatte.cc
  intern/curve.cc
  intern/curve_bevel.cc
  intern/curve_bezier.cc
  intern/curve_catmull_rom.cc
  intern/curve_convert.cc
  intern/curve_decimate.cc
  intern/curve_deform.cc
  intern/curve_legacy_convert.cc
  intern/curve_nurbs.cc
  intern/curve_poly.cc
  intern/curve_to_mesh_convert.cc
  intern/curveprofile.cc
  intern/curves.cc
  intern/curves_attributes.cc
  intern/curves_geometry.cc
  intern/curves_utils.cc
  intern/customdata.cc
  intern/customdata_file.cc
  intern/data_transfer.cc
  intern/deform.cc
  intern/displist.cc
  intern/dynamicpaint.cc
  intern/editlattice.cc
  intern/editmesh.cc
  intern/editmesh_bvh.cc
  intern/editmesh_cache.cc
  intern/editmesh_tangent.cc
  intern/effect.cc
  intern/fcurve.cc
  intern/fcurve_cache.cc
  intern/fcurve_driver.cc
  intern/file_handler.cc
  intern/fluid.cc
  intern/fmodifier.cc
  intern/freestyle.cc
  intern/geometry_compare.cc
  intern/geometry_component_curves.cc
  intern/geometry_component_edit_data.cc
  intern/geometry_component_grease_pencil.cc
  intern/geometry_component_instances.cc
  intern/geometry_component_mesh.cc
  intern/geometry_component_pointcloud.cc
  intern/geometry_component_volume.cc
  intern/geometry_fields.cc
  intern/geometry_set.cc
  intern/geometry_set_instances.cc
  intern/gpencil_geom_legacy.cc
  intern/gpencil_legacy.cc
  intern/gpencil_modifier_legacy.cc
  intern/grease_pencil.cc
  intern/grease_pencil_attributes.cc
  intern/grease_pencil_convert_legacy.cc
  intern/grease_pencil_vertex_groups.cc
  intern/icons.cc
  intern/icons_rasterize.cc
  intern/id_hash.cc
  intern/idprop.cc
  intern/idprop_create.cc
  intern/idprop_serialize.cc
  intern/idprop_utils.cc
  intern/idtype.cc
  intern/image.cc
  intern/image_format.cc
  intern/image_gen.cc
  intern/image_gpu.cc
  intern/image_partial_update.cc
  intern/image_save.cc
  intern/instances.cc
  intern/instances_attributes.cc
  intern/kelvinlet.cc
  intern/key.cc
  intern/keyconfig.cc
  intern/lattice.cc
  intern/lattice_deform.cc
  intern/layer.cc
  intern/layer_utils.cc
  intern/lib_id.cc
  intern/lib_id_delete.cc
  intern/lib_id_eval.cc
  intern/lib_id_remapper.cc
  intern/lib_override.cc
  intern/lib_override_proxy_conversion.cc
  intern/lib_query.cc
  intern/lib_remap.cc
  intern/library.cc
  intern/light.cc
  intern/light_linking.cc
  intern/lightprobe.cc
  intern/linestyle.cc
  intern/main.cc
  intern/main_idmap.cc
  intern/main_invariants.cc
  intern/main_namemap.cc
  intern/mask.cc
  intern/mask_evaluate.cc
  intern/mask_rasterize.cc
  intern/material.cc
  intern/mball.cc
  intern/mball_tessellate.cc
  intern/mesh.cc
  intern/mesh_attributes.cc
  intern/mesh_calc_edges.cc
  intern/mesh_convert.cc
  intern/mesh_data_update.cc
  intern/mesh_debug.cc
  intern/mesh_evaluate.cc
  intern/mesh_fair.cc
  intern/mesh_flip_faces.cc
  intern/mesh_iterators.cc
  intern/mesh_legacy_convert.cc
  intern/mesh_mapping.cc
  intern/mesh_merge_customdata.cc
  intern/mesh_mirror.cc
  intern/mesh_normals.cc
  intern/mesh_remap.cc
  intern/mesh_remesh_voxel.cc
  intern/mesh_runtime.cc
  intern/mesh_sample.cc
  intern/mesh_tangent.cc
  intern/mesh_tessellate.cc
  intern/mesh_topology_state.cc
  intern/mesh_validate.cc
  intern/mesh_wrapper.cc
  intern/modifier.cc
  intern/movieclip.cc
  intern/multires.cc
  intern/multires_reshape.cc
  intern/multires_reshape_apply_base.cc
  intern/multires_reshape_ccg.cc
  intern/multires_reshape_smooth.cc
  intern/multires_reshape_subdivide.cc
  intern/multires_reshape_util.cc
  intern/multires_reshape_vertcos.cc
  intern/multires_subdiv.cc
  intern/multires_unsubdivide.cc
  intern/multires_versioning.cc
  intern/nla.cc
  intern/node.cc
  intern/node_enum_definition.cc
  intern/node_runtime.cc
  intern/node_socket_value.cc
  intern/node_tree_dot_export.cc
  intern/node_tree_field_inferencing.cc
  intern/node_tree_interface.cc
  intern/node_tree_reference_lifetimes.cc
  intern/node_tree_structure_type_inferencing.cc
  intern/node_tree_update.cc
  intern/node_tree_zones.cc
  intern/object.cc
  intern/object_deform.cc
  intern/object_dupli.cc
  intern/object_update.cc
  intern/ocean.cc
  intern/ocean_spectrum.cc
  intern/outliner_treehash.cc
  intern/packedFile.cc
  intern/paint.cc
  intern/paint_canvas.cc
  intern/paint_runtime.cc
  intern/particle.cc
  intern/particle_child.cc
  intern/particle_distribute.cc
  intern/particle_system.cc
  intern/path_templates.cc
  intern/pbvh.cc
  intern/pbvh_bmesh.cc
  intern/pbvh_pixels.cc
  intern/pbvh_pixels_copy.cc
  intern/pbvh_uv_islands.cc
  intern/pointcache.cc
  intern/pointcloud.cc
  intern/pointcloud_attributes.cc
  intern/pose_backup.cc
  intern/preferences.cc
  intern/preview_image.cc
  intern/report.cc
  intern/rigidbody.cc
  intern/scene.cc
  intern/screen.cc
  intern/shader_fx.cc
  intern/shrinkwrap.cc
  intern/softbody.cc
  intern/sound.cc
  intern/speaker.cc
  intern/studiolight.cc
  intern/subdiv.cc
  intern/subdiv_ccg.cc
  intern/subdiv_ccg_mask.cc
  intern/subdiv_converter.cc
  intern/subdiv_converter_mesh.cc
  intern/subdiv_deform.cc
  intern/subdiv_displacement.cc
  intern/subdiv_displacement_multires.cc
  intern/subdiv_eval.cc
  intern/subdiv_foreach.cc
  intern/subdiv_mesh.cc
  intern/subdiv_modifier.cc
  intern/subdiv_stats.cc
  intern/subdiv_topology.cc
  intern/text.cc
  intern/text_suggestions.cc
  intern/texture.cc
  intern/tracking.cc
  intern/tracking_auto.cc
  intern/tracking_detect.cc
  intern/tracking_plane_tracker.cc
  intern/tracking_region_tracker.cc
  intern/tracking_solver.cc
  intern/tracking_stabilize.cc
  intern/tracking_util.cc
  intern/type_conversions.cc
  intern/undo_system.cc
  intern/unit.cc
  intern/uvproject.cc
  intern/vfont.cc
  intern/vfont_curve.cc
  intern/vfontdata_freetype.cc
  intern/viewer_path.cc
  intern/volume.cc
  intern/volume_grid.cc
  intern/volume_grid_fields.cc
  intern/volume_grid_file_cache.cc
  intern/volume_render.cc
  intern/volume_to_mesh.cc
  intern/wm_runtime.cc
  intern/workspace.cc
  intern/world.cc

  BKE_action.hh
  BKE_addon.h
  BKE_anim_data.hh
  BKE_anim_path.h
  BKE_anim_visualization.h
  BKE_animsys.h
  BKE_anonymous_attribute_id.hh
  BKE_anonymous_attribute_make.hh
  BKE_appdir.hh
  BKE_armature.hh
  BKE_asset.hh
  BKE_asset_edit.hh
  BKE_attribute.h
  BKE_attribute.hh
  BKE_attribute_filter.hh
  BKE_attribute_filters.hh
  BKE_attribute_legacy_convert.hh
  BKE_attribute_math.hh
  BKE_attribute_storage.hh
  BKE_attribute_storage_blend_write.hh
  BKE_autoexec.hh
  BKE_bake_data_block_id.hh
  BKE_bake_data_block_map.hh
  BKE_bake_geometry_nodes_modifier.hh
  BKE_bake_geometry_nodes_modifier_pack.hh
  BKE_bake_items.hh
  BKE_bake_items_paths.hh
  BKE_bake_items_serialize.hh
  BKE_bake_items_socket.hh
  BKE_blender.hh
  BKE_blender_cli_command.hh
  BKE_blender_copybuffer.hh
  BKE_blender_undo.hh
  BKE_blender_user_menu.hh
  BKE_blender_version.h
  BKE_blendfile.hh
  BKE_blendfile_link_append.hh
  BKE_boids.h
  BKE_bpath.hh
  BKE_brush.hh
  BKE_bvhutils.hh
  BKE_cachefile.hh
  BKE_callbacks.hh
  BKE_camera.h
  BKE_ccg.hh
  BKE_cloth.hh
  BKE_collection.hh
  BKE_collision.h
  BKE_colorband.hh
  BKE_colortools.hh
  BKE_compositor.hh
  BKE_compute_context_cache.hh
  BKE_compute_context_cache_fwd.hh
  BKE_compute_contexts.hh
  BKE_constraint.h
  BKE_context.hh
  BKE_cpp_types.hh
  BKE_crazyspace.hh
  BKE_cryptomatte.h
  BKE_cryptomatte.hh
  BKE_curve.hh
  BKE_curve_legacy_convert.hh
  BKE_curve_to_mesh.hh
  BKE_curveprofile.h
  BKE_curves.h
  BKE_curves.hh
  BKE_curves_utils.hh
  BKE_customdata.hh
  BKE_customdata_file.h
  BKE_data_transfer.h
  BKE_deform.hh
  BKE_displist.h
  BKE_duplilist.hh
  BKE_dynamicpaint.h
  BKE_editlattice.h
  BKE_editmesh.hh
  BKE_editmesh_bvh.hh
  BKE_editmesh_cache.hh
  BKE_editmesh_tangent.hh
  BKE_effect.h
  BKE_fcurve.hh
  BKE_fcurve_driver.h
  BKE_file_handler.hh
  BKE_fluid.h
  BKE_freestyle.h
  BKE_geometry_compare.hh
  BKE_geometry_fields.hh
  BKE_geometry_nodes_gizmos_transforms.hh
  BKE_geometry_nodes_reference_set.hh
  BKE_geometry_set.hh
  BKE_geometry_set_instances.hh
  BKE_global.hh
  BKE_gpencil_geom_legacy.h
  BKE_gpencil_legacy.h
  BKE_gpencil_modifier_legacy.h
  BKE_grease_pencil.h
  BKE_grease_pencil.hh
  BKE_grease_pencil_legacy_convert.hh
  BKE_grease_pencil_vertex_groups.hh
  BKE_icons.hh
  BKE_id_hash.hh
  BKE_idprop.hh
  BKE_idtype.hh
  BKE_image.hh
  BKE_image_format.hh
  BKE_image_partial_update.hh
  BKE_image_save.hh
  BKE_image_wrappers.hh
  BKE_instances.hh
  BKE_kelvinlet.h
  BKE_key.hh
  BKE_keyconfig.h
  BKE_lattice.hh
  BKE_layer.hh
  BKE_lib_id.hh
  BKE_lib_override.hh
  BKE_lib_query.hh
  BKE_lib_remap.hh
  BKE_library.hh
  BKE_light.h
  BKE_light_linking.h
  BKE_lightprobe.h
  BKE_linestyle.h
  BKE_main.hh
  BKE_main_idmap.hh
  BKE_main_invariants.hh
  BKE_main_namemap.hh
  BKE_mask.hh
  BKE_material.hh
  BKE_mball.hh
  BKE_mball_tessellate.hh
  BKE_mesh.h
  BKE_mesh.hh
  BKE_mesh_fair.hh
  BKE_mesh_iterators.hh
  BKE_mesh_legacy_convert.hh
  BKE_mesh_mapping.hh
  BKE_mesh_mirror.hh
  BKE_mesh_remap.hh
  BKE_mesh_remesh_voxel.hh
  BKE_mesh_runtime.hh
  BKE_mesh_sample.hh
  BKE_mesh_tangent.hh
  BKE_mesh_topology_state.hh
  BKE_mesh_types.hh
  BKE_mesh_wrapper.hh
  BKE_modifier.hh
  BKE_movieclip.hh
  BKE_multires.hh
  BKE_nla.hh
  BKE_node.hh
  BKE_node_enum.hh
  BKE_node_legacy_types.hh
  BKE_node_runtime.hh
  BKE_node_socket_value.hh
  BKE_node_socket_value_fwd.hh
  BKE_node_tree_dot_export.hh
  BKE_node_tree_interface.hh
  BKE_node_tree_reference_lifetimes.hh
  BKE_node_tree_update.hh
  BKE_node_tree_zones.hh
  BKE_object.hh
  BKE_object_deform.h
  BKE_object_types.hh
  BKE_ocean.h
  BKE_outliner_treehash.hh
  BKE_packedFile.hh
  BKE_paint.hh
  BKE_paint_bvh.hh
  BKE_paint_bvh_pixels.hh
  BKE_paint_types.hh
  BKE_particle.h
  BKE_path_templates.hh
  BKE_pointcache.h
  BKE_pointcloud.hh
  BKE_pose_backup.h
  BKE_preferences.h
  BKE_preview_image.hh
  BKE_report.hh
  BKE_rigidbody.h
  BKE_scene.hh
  BKE_scene_runtime.hh
  BKE_screen.hh
  BKE_shader_fx.hh
  BKE_shrinkwrap.hh
  BKE_softbody.h
  BKE_sound.hh
  BKE_speaker.hh
  BKE_studiolight.h
  BKE_subdiv.hh
  BKE_subdiv_ccg.hh
  BKE_subdiv_deform.hh
  BKE_subdiv_eval.hh
  BKE_subdiv_foreach.hh
  BKE_subdiv_mesh.hh
  BKE_subdiv_modifier.hh
  BKE_subdiv_topology.hh
  BKE_text.h
  BKE_text_suggestions.h
  BKE_texture.h
  BKE_tracking.hh
  BKE_type_conversions.hh
  BKE_undo_system.hh
  BKE_unit.hh
  BKE_uvproject.h
  BKE_vfont.hh
  BKE_vfontdata.hh
  BKE_viewer_path.hh
  BKE_volume.hh
  BKE_volume_enums.hh
  BKE_volume_grid.hh
  BKE_volume_grid_fields.hh
  BKE_volume_grid_file_cache.hh
  BKE_volume_grid_fwd.hh
  BKE_volume_grid_process.hh
  BKE_volume_grid_type_traits.hh
  BKE_volume_openvdb.hh
  BKE_volume_render.hh
  BKE_volume_to_mesh.hh
  BKE_wm_runtime.hh
  BKE_workspace.hh
  BKE_world.h

  nla_private.h
  particle_private.h

  intern/attribute_access_intern.hh
  intern/attribute_storage_access.hh
  intern/data_transfer_intern.hh
  intern/lib_intern.hh
  intern/multires_inline.hh
  intern/multires_reshape.hh
  intern/multires_unsubdivide.hh
  intern/ocean_intern.h
  intern/pbvh_intern.hh
  intern/pbvh_pixels_copy.hh
  intern/pbvh_uv_islands.hh
  intern/subdiv_converter.hh
  intern/subdiv_inline.hh
  intern/tracking_private.hh
)

set(LIB
  PRIVATE bf::animrig
  PRIVATE bf::asset_system
  PRIVATE bf::blenfont
  PRIVATE bf::blenlib
  PRIVATE bf::blenloader
  PRIVATE bf::blentranslation
  PRIVATE bf::bmesh
  PRIVATE bf::depsgraph
  PRIVATE bf::dna
  PRIVATE bf::draw
  PRIVATE bf::extern::curve_fit_nd
  PRIVATE bf::functions
  PRIVATE bf::gpu
  bf_ikplugin
  PRIVATE bf::imbuf
  PRIVATE bf::imbuf::movie
  PRIVATE bf::intern::clog
  bf_intern_ghost
  PRIVATE bf::intern::guardedalloc
  PUBLIC bf::intern::optional::openvdb  # Contains intern-openvdb items in public headers
  bf_intern_libmv  # Uses stub when disabled.
  bf_intern_mikktspace
  bf_intern_opensubdiv  # Uses stub when disabled.
  bf_modifiers
  PRIVATE bf::nodes
  PRIVATE bf::render
  bf_rna
  PRIVATE bf::sequencer
  bf_shader_fx
  bf_simulation
  PRIVATE bf::extern::fmtlib
  PRIVATE bf::extern::xxhash
  PRIVATE bf::intern::atomic
  PRIVATE bf::dependencies::zlib
  PRIVATE bf::dependencies::zstd
  PRIVATE bf::dependencies::freetype  # For `vfontdata_freetype.c`.
  PRIVATE bf::dependencies::optional::fftw3
  PRIVATE bf::dependencies::optional::opensubdiv
  PRIVATE bf::dependencies::optional::audaspace
)

if(WITH_BINRELOC)
  list(APPEND LIB
    PRIVATE bf::extern::binreloc
  )
  add_definitions(-DWITH_BINRELOC)
endif()


if(WIN32)
  list(APPEND INC
    ../../../intern/utfconv
  )
endif()

if(WITH_BULLET)
  list(APPEND INC
    ../../../intern/rigidbody
  )
  list(APPEND LIB
    bf_intern_rigidbody
    PRIVATE bf::dependencies::optional::bullet
  )
endif()

if(WITH_IMAGE_OPENEXR)
  add_definitions(-DWITH_IMAGE_OPENEXR)
endif()

if(WITH_IMAGE_OPENJPEG)
  add_definitions(-DWITH_IMAGE_OPENJPEG)
endif()

if(WITH_IMAGE_CINEON)
  add_definitions(-DWITH_IMAGE_CINEON)
endif()

if(WITH_IMAGE_WEBP)
  add_definitions(-DWITH_IMAGE_WEBP)
endif()

if(WITH_CODEC_FFMPEG)
  add_definitions(-DWITH_FFMPEG)
endif()

if(WITH_PYTHON)
  list(APPEND INC
    ../python
  )
  list(APPEND LIB
    bf_python
    bf_python_bmesh
  )
  add_definitions(-DWITH_PYTHON)

  if(WITH_PYTHON_MODULE)
    add_definitions(-DWITH_PYTHON_MODULE)
  endif()

  if(WITH_PYTHON_SAFETY)
    add_definitions(-DWITH_PYTHON_SAFETY)
  endif()

  if(WITH_PYTHON_SECURITY)
    add_definitions(-DWITH_PYTHON_SECURITY)
  endif()


  if(PYTHON_EXECUTABLE)
    get_filename_component(_python_exe_name ${PYTHON_EXECUTABLE} NAME)
    add_definitions(-DPYTHON_EXECUTABLE_NAME=${_python_exe_name})
    unset(_python_exe_name)
  endif()
endif()

if(WITH_MOD_FLUID)
  list(APPEND LIB
    bf_intern_mantaflow
  )
  add_definitions(-DWITH_FLUID)
endif()

if(WITH_MOD_OCEANSIM)
  add_definitions(-DWITH_OCEANSIM)
endif()

if(WITH_JACK)
  add_definitions(-DWITH_JACK)
endif()

if(WITH_LIBMV)
  add_definitions(-DWITH_LIBMV)
endif()

if(WITH_FREESTYLE)
  add_definitions(-DWITH_FREESTYLE)
endif()

if(WITH_ALEMBIC)
  list(APPEND INC
    ../io/alembic
  )
  add_definitions(-DWITH_ALEMBIC)
endif()

if(WITH_USD)
  list(APPEND INC
    ../io/usd
  )
  add_definitions(-DWITH_USD)
endif()

if(WITH_QUADRIFLOW)
  list(APPEND INC
    ../../../intern/quadriflow
  )
  list(APPEND LIB
    bf_intern_quadriflow
  )
  add_definitions(-DWITH_QUADRIFLOW)
endif()

if(WITH_XR_OPENXR)
  add_definitions(-DWITH_XR_OPENXR)
endif()

if(WITH_INPUT_IME)
  add_definitions(-DWITH_INPUT_IME)
endif()

# # Warnings as errors, this is too strict!
# if(MSVC)
#    string(APPEND CMAKE_C_FLAGS " /WX")
# endif()

blender_add_lib(bf_blenkernel "${SRC}" "${INC}" "${INC_SYS}" "${LIB}")
add_library(bf::blenkernel ALIAS bf_blenkernel)

# RNA_prototypes.hh
add_dependencies(bf_blenkernel bf_rna)


if(WITH_GTESTS)
  set(TEST_SRC
    intern/anim_data_test.cc
    intern/armature_deform_test.cc
    intern/armature_test.cc
    intern/asset_metadata_test.cc
    intern/attribute_storage_test.cc
    intern/bpath_test.cc
    intern/brush_test.cc
    intern/cryptomatte_test.cc
    intern/curves_geometry_test.cc
    intern/deform_test.cc
    intern/fcurve_test.cc
    intern/file_handler_test.cc
    intern/grease_pencil_test.cc
    intern/idprop_serialize_test.cc
    intern/idprop_test.cc
    intern/image_partial_update_test.cc
    intern/image_test.cc
    intern/key_test.cc
    intern/lattice_deform_test.cc
    intern/layer_test.cc
    intern/lib_id_remapper_test.cc
    intern/lib_id_test.cc
    intern/lib_query_test.cc
    intern/lib_remap_test.cc
    intern/main_test.cc
    intern/nla_test.cc
    intern/path_templates_test.cc
    intern/scene_test.cc
    intern/subdiv_ccg_test.cc
    intern/tracking_test.cc
    intern/volume_test.cc
  )
  set(TEST_INC
    # WARNING: this is a bad-level include which is only acceptable for tests
    # and even then it would be good if the dependency could be removed.
    ../editors/include
  )
  set(TEST_LIB
    ${LIB}
    bf_rna  # RNA_prototypes.hh
  )
  blender_add_test_suite_lib(blenkernel "${TEST_SRC}" "${INC};${TEST_INC}" "${INC_SYS}" "${TEST_LIB}")
endif()


--- source/blender/blenlib/CMakeLists.txt ---
# SPDX-FileCopyrightText: 2006 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

if(HAVE_EXECINFO_H)
  add_definitions(-DHAVE_EXECINFO_H)
endif()

set(INC
  PUBLIC .
  ../../../intern/eigen
)

set(INC_SYS
)

set(SRC
  intern/BLI_assert.cc
  intern/BLI_color.cc
  intern/BLI_colorspace.cc
  intern/BLI_dial_2d.cc
  intern/BLI_dynstr.cc
  intern/BLI_filelist.cc
  intern/BLI_ghash.cc
  intern/BLI_ghash_utils.cc
  intern/BLI_heap.cc
  intern/BLI_heap_simple.cc
  intern/BLI_kdopbvh.cc
  intern/BLI_linklist.cc
  intern/BLI_linklist_lockfree.cc
  intern/BLI_memarena.cc
  intern/BLI_memblock.cc
  intern/BLI_memiter.cc
  intern/BLI_mempool.cc
  intern/BLI_mmap.cc
  intern/BLI_subprocess.cc
  intern/BLI_timer.cc
  intern/array_store.cc
  intern/array_store_rle.cc
  intern/array_store_utils.cc
  intern/array_utils.cc
  intern/array_utils_c.cc
  intern/astar.cc
  intern/atomic_disjoint_set.cc
  intern/bit_bool_conversion.cc
  intern/bit_ref.cc
  intern/bit_span.cc
  intern/bitmap.cc
  intern/bitmap_draw_2d.cc
  intern/boxpack_2d.cc
  intern/cache_mutex.cc
  intern/compression.cc
  intern/compute_context.cc
  intern/convexhull_2d.cc
  intern/cpp_type.cc
  intern/cpp_types.cc
  intern/csv_parse.cc
  intern/delaunay_2d.cc
  intern/dot_export.cc
  intern/easing.cc
  intern/endian_switch.cc
  intern/expr_pylike_eval.cc
  intern/fftw.cc
  intern/fileops.cc
  intern/fileops_c.cc
  intern/filereader_file.cc
  intern/filereader_gzip.cc
  intern/filereader_memory.cc
  intern/filereader_zstd.cc
  intern/fnmatch.cc
  intern/generic_vector_array.cc
  intern/generic_virtual_array.cc
  intern/generic_virtual_vector_array.cc
  intern/gsqueue.cc
  intern/hash_md5.cc
  intern/hash_mm2a.cc
  intern/hash_mm3.cc
  intern/hash_tables.cc
  intern/implicit_sharing.cc
  intern/index_mask.cc
  intern/index_mask_expression.cc
  intern/index_range.cc
  intern/jitter_2d.cc
  intern/lasso_2d.cc
  intern/lazy_threading.cc
  intern/length_parameterize.cc
  intern/listbase.cc
  intern/math_base.cc
  intern/math_base_inline.cc
  intern/math_base_safe_inline.cc
  intern/math_basis_types.cc
  intern/math_bits_inline.cc
  intern/math_boolean.cc
  intern/math_color.cc
  intern/math_color_blend_inline.cc
  intern/math_color_inline.cc
  intern/math_filter.cc
  intern/math_geom.cc
  intern/math_geom_inline.cc
  intern/math_half.cc
  intern/math_interp.cc
  intern/math_matrix.cc
  intern/math_matrix_c.cc
  intern/math_rotation.cc
  intern/math_rotation_c.cc
  intern/math_solvers.cc
  intern/math_statistics.cc
  intern/math_time.cc
  intern/math_vec.cc
  intern/math_vector.cc
  intern/math_vector_inline.cc
  intern/memory_cache.cc
  intern/memory_cache_file_load.cc
  intern/memory_counter.cc
  intern/memory_utils.cc
  intern/mesh_boolean.cc
  intern/mesh_intersect.cc
  intern/noise.cc
  intern/noise_c.cc
  intern/offset_indices.cc
  intern/ordered_edge.cc
  intern/path_utils.cc
  intern/polyfill_2d.cc
  intern/polyfill_2d_beautify.cc
  intern/quadric.cc
  intern/radial_tiling_wrapper.cc
  intern/rand.cc
  intern/rct.cc
  intern/resource_scope.cc
  intern/scanfill.cc
  intern/scanfill_utils.cc
  intern/serialize.cc
  intern/session_uid.cc
  intern/smaa_textures.cc
  intern/sort.cc
  intern/sort_utils.cc
  intern/stack.cc
  intern/storage.cc
  intern/string.cc
  intern/string_cursor_utf8.cc
  intern/string_ref.cc
  intern/string_search.cc
  intern/string_utf8.cc
  intern/string_utils.cc
  intern/system.cc
  intern/task_graph.cc
  intern/task_iterator.cc
  intern/task_pool.cc
  intern/task_range.cc
  intern/task_scheduler.cc
  intern/tempfile.cc
  intern/threads.cc
  intern/time.cc
  intern/timecode.cc
  intern/timeit.cc
  intern/uuid.cc
  intern/vector.cc
  intern/virtual_array.cc
  intern/voxel.cc
  intern/winstuff.cc
  intern/winstuff_dir.cc
  intern/winstuff_registration.cc
  # Private headers.
  intern/BLI_mempool_private.h

  # Header as source (included in C files above).
  intern/list_sort_impl.h
  intern/radial_tiling_shared.hh

  BLI_alloca.h
  BLI_allocator.hh
  BLI_any.hh
  BLI_array.hh
  BLI_array_state.hh
  BLI_array_store.h
  BLI_array_store_utils.h
  BLI_array_utils.h
  BLI_array_utils.hh
  BLI_asan.h
  BLI_assert.h
  BLI_astar.h
  BLI_atomic_disjoint_set.hh
  BLI_binary_search.hh
  BLI_bit_bool_conversion.hh
  BLI_bit_group_vector.hh
  BLI_bit_ref.hh
  BLI_bit_span.hh
  BLI_bit_span_ops.hh
  BLI_bit_span_to_index_ranges.hh
  BLI_bit_vector.hh
  BLI_bitmap.h
  BLI_bitmap_draw_2d.h
  BLI_bounds.hh
  BLI_bounds_types.hh
  BLI_boxpack_2d.h
  BLI_build_config.h
  BLI_cache_mutex.hh
  BLI_color.hh
  BLI_color_mix.hh
  BLI_colorspace.hh
  BLI_compiler_attrs.h
  BLI_compiler_compat.h
  BLI_compiler_typecheck.h
  BLI_compression.hh
  BLI_compute_context.hh
  BLI_concurrent_map.hh
  BLI_console.h
  BLI_convexhull_2d.hh
  BLI_cpp_type.hh
  BLI_cpp_type_make.hh
  BLI_cpp_types.hh
  BLI_cpp_types_make.hh
  BLI_csv_parse.hh
  BLI_delaunay_2d.hh
  BLI_devirtualize_parameters.hh
  BLI_dial_2d.h
  BLI_disjoint_set.hh
  BLI_dot_export.hh
  BLI_dot_export_attribute_enums.hh
  BLI_dynstr.h
  BLI_easing.h
  BLI_endian_defines.h
  BLI_endian_switch.h
  BLI_endian_switch_inline.h
  BLI_enum_flags.hh
  BLI_enumerable_thread_specific.hh
  BLI_expr_pylike_eval.h
  BLI_fftw.hh
  BLI_fileops.h
  BLI_fileops.hh
  BLI_fileops_types.h
  BLI_filereader.h
  BLI_fixed_width_int.hh
  BLI_fixed_width_int_str.hh
  BLI_fnmatch.h
  BLI_function_ref.hh
  BLI_generic_array.hh
  BLI_generic_key.hh
  BLI_generic_key_string.hh
  BLI_generic_pointer.hh
  BLI_generic_span.hh
  BLI_generic_value_map.hh
  BLI_generic_vector_array.hh
  BLI_generic_virtual_array.hh
  BLI_generic_virtual_vector_array.hh
  BLI_ghash.h
  BLI_gsqueue.h
  BLI_hash.h
  BLI_hash.hh
  BLI_hash_fwd.hh
  BLI_hash_md5.hh
  BLI_hash_mm2a.hh
  BLI_hash_mm3.hh
  BLI_hash_tables.hh
  BLI_heap.h
  BLI_heap_simple.h
  BLI_implicit_sharing.h
  BLI_implicit_sharing.hh
  BLI_implicit_sharing_ptr.hh
  BLI_index_mask.hh
  BLI_index_mask_expression.hh
  BLI_index_mask_fwd.hh
  BLI_index_range.hh
  BLI_index_ranges_builder.hh
  BLI_index_ranges_builder_fwd.hh
  BLI_inplace_priority_queue.hh
  BLI_iterator.h
  BLI_jitter_2d.h
  BLI_kdopbvh.hh
  BLI_kdtree.hh
  BLI_kdtree_types.hh
  BLI_lasso_2d.hh
  BLI_lazy_threading.hh
  BLI_length_parameterize.hh
  BLI_linear_allocator.hh
  BLI_linear_allocator_chunked_list.hh
  BLI_link_utils.h
  BLI_linklist.h
  BLI_linklist_lockfree.h
  BLI_linklist_stack.h
  BLI_listbase.h
  BLI_listbase_iterator.hh
  BLI_listbase_wrapper.hh
  BLI_map.hh
  BLI_map_slots.hh
  BLI_math_angle_types.hh
  BLI_math_axis_angle.hh
  BLI_math_axis_angle_types.hh
  BLI_math_base.h
  BLI_math_base.hh
  BLI_math_base_safe.h
  BLI_math_basis_types.hh
  BLI_math_bits.h
  BLI_math_boolean.hh
  BLI_math_color.h
  BLI_math_color.hh
  BLI_math_color_blend.h
  BLI_math_constants.h
  BLI_math_euler.hh
  BLI_math_euler_types.hh
  BLI_math_filter.hh
  BLI_math_geom.h
  BLI_math_half.hh
  BLI_math_inline.h
  BLI_math_interp.hh
  BLI_math_matrix.h
  BLI_math_matrix.hh
  BLI_math_matrix_types.hh
  BLI_math_mpq.hh
  BLI_math_numbers.hh
  BLI_math_quaternion.hh
  BLI_math_quaternion_types.hh
  BLI_math_rotation.h
  BLI_math_rotation.hh
  BLI_math_rotation_legacy.hh
  BLI_math_rotation_types.hh
  BLI_math_solvers.h
  BLI_math_statistics.h
  BLI_math_time.h
  BLI_math_vector.h
  BLI_math_vector.hh
  BLI_math_vector_mpq_types.hh
  BLI_math_vector_types.hh
  BLI_math_vector_unroll.hh
  BLI_memarena.h
  BLI_memblock.h
  BLI_memiter.h
  BLI_memory_cache.hh
  BLI_memory_cache_file_load.hh
  BLI_memory_counter.hh
  BLI_memory_counter_fwd.hh
  BLI_memory_utils.h
  BLI_memory_utils.hh
  BLI_mempool.h
  BLI_mesh_boolean.hh
  BLI_mesh_intersect.hh
  BLI_mmap.h
  BLI_multi_value_map.hh
  BLI_mutex.hh
  BLI_noise.h
  BLI_noise.hh
  BLI_offset_indices.hh
  BLI_offset_span.hh
  BLI_ordered_edge.hh
  BLI_parameter_pack_utils.hh
  BLI_path_utils.hh
  BLI_polyfill_2d.h
  BLI_polyfill_2d_beautify.h
  BLI_pool.hh
  BLI_probing_strategies.hh
  BLI_quadric.h
  BLI_radial_tiling.hh
  BLI_rand.h
  BLI_rand.hh
  BLI_random_access_iterator_mixin.hh
  BLI_rect.h
  BLI_resource_scope.hh
  BLI_scanfill.h
  BLI_serialize.hh
  BLI_session_uid.h
  BLI_set.hh
  BLI_set_slots.hh
  BLI_shared_cache.hh
  BLI_simd.hh
  BLI_smaa_textures.h
  BLI_sort.h
  BLI_sort.hh
  BLI_sort_utils.h
  BLI_span.hh
  BLI_stack.h
  BLI_stack.hh
  BLI_strict_flags.h
  BLI_string.h
  BLI_string_cursor_utf8.h
  BLI_string_ref.hh
  BLI_string_search.hh
  BLI_string_utf8.h
  BLI_string_utf8_symbols.h
  BLI_string_utils.hh
  BLI_struct_equality_utils.hh
  BLI_sub_frame.hh
  BLI_subprocess.hh
  BLI_sys_types.h
  BLI_system.h
  BLI_task.h
  BLI_task.hh
  BLI_task_size_hints.hh
  BLI_tempfile.h
  BLI_threads.h
  BLI_time.h
  BLI_time_utildefines.h
  BLI_timecode.h
  BLI_timeit.hh
  BLI_timer.h
  BLI_unique_sorted_indices.hh
  BLI_unroll.hh
  BLI_utildefines.h
  BLI_utildefines_iter.h
  BLI_utildefines_stack.h
  BLI_utildefines_variadic.h
  BLI_utility_mixins.hh
  BLI_uuid.h
  BLI_vector.hh
  BLI_vector_list.hh
  BLI_vector_set.hh
  BLI_vector_set_slots.hh
  BLI_virtual_array.hh
  BLI_virtual_array_fwd.hh
  BLI_virtual_vector_array.hh
  BLI_voxel.h
  BLI_winstuff.h
  BLI_winstuff_com.hh

  # Without these files listed, they aren't known to CMake.
  ../../../extern/json/include/json.hpp
)

set(LIB
  PUBLIC bf::dna
  PRIVATE bf::extern::fmtlib
  PRIVATE bf::extern::json
  PRIVATE bf::extern::xxhash
  bf_intern_eigen
  PRIVATE bf::intern::guardedalloc
  PRIVATE bf::extern::wcwidth
  PRIVATE bf::intern::atomic
  PRIVATE bf::extern::fmtlib
  PRIVATE bf::dependencies::eigen
  PUBLIC bf::dependencies::optional::tbb
  PRIVATE bf::dependencies::zlib
  PRIVATE bf::dependencies::zstd
  PRIVATE bf::dependencies::optional::gmp
  PRIVATE bf::dependencies::optional::fftw3
)

if(NOT WITH_PYTHON_MODULE)
  list(APPEND SRC
    intern/BLI_args.cc

    BLI_args.h
  )
endif()

if(WITH_MEM_VALGRIND)
  add_definitions(-DWITH_MEM_VALGRIND)
endif()

if(WIN32)
  if(WITH_BLENDER_THUMBNAILER)
    # Needed for querying the `thumbnailer .dll` in `winstuff.c`.
    add_definitions(-DWITH_BLENDER_THUMBNAILER)
  endif()
  list(APPEND INC
    ../../../intern/uriconvert
    ../../../intern/utfconv
  )
  list(APPEND LIB
    bf_intern_uriconvert
    bf_intern_utfconv
    dxgi
  )
  list(APPEND SRC
    intern/system_win32.cc
  )
endif()


if(APPLE)
  list(APPEND SRC
    intern/fileops_apple.mm
    intern/storage_apple.mm
  )
endif()

if(UNIX AND NOT APPLE)
  list(APPEND LIB
    bf_intern_libc_compat
  )
endif()

# no need to compile object files for inline headers.
set_source_files_properties(
  intern/math_base_inline.cc
  intern/math_base_safe_inline.cc
  intern/math_bits_inline.cc
  intern/math_color_blend_inline.cc
  intern/math_color_inline.cc
  intern/math_geom_inline.cc
  intern/math_vector_inline.cc
  PROPERTIES HEADER_FILE_ONLY TRUE
)

blender_add_lib(bf_blenlib "${SRC}" "${INC}" "${INC_SYS}" "${LIB}")
add_library(bf::blenlib ALIAS bf_blenlib)

if(WITH_GTESTS)
  set(TEST_SRC
    tests/BLI_any_test.cc
    tests/BLI_array_state_test.cc
    tests/BLI_array_store_test.cc
    tests/BLI_array_test.cc
    tests/BLI_array_utils_test.cc
    tests/BLI_binary_search_test.cc
    tests/BLI_bit_group_vector_test.cc
    tests/BLI_bit_ref_test.cc
    tests/BLI_bit_span_test.cc
    tests/BLI_bit_vector_test.cc
    tests/BLI_bitmap_test.cc
    tests/BLI_bounds_test.cc
    tests/BLI_build_config_test.cc
    tests/BLI_color_test.cc
    tests/BLI_compression_test.cc
    tests/BLI_convexhull_2d_test.cc
    tests/BLI_cpp_type_test.cc
    tests/BLI_csv_parse_test.cc
    tests/BLI_delaunay_2d_test.cc
    tests/BLI_disjoint_set_test.cc
    tests/BLI_expr_pylike_eval_test.cc
    tests/BLI_fileops_test.cc
    tests/BLI_fixed_width_int_test.cc
    tests/BLI_function_ref_test.cc
    tests/BLI_generic_array_test.cc
    tests/BLI_generic_span_test.cc
    tests/BLI_generic_vector_array_test.cc
    tests/BLI_ghash_test.cc
    tests/BLI_hash_mm2a_test.cc
    tests/BLI_heap_simple_test.cc
    tests/BLI_heap_test.cc
    tests/BLI_implicit_sharing_test.cc
    tests/BLI_index_mask_expression_test.cc
    tests/BLI_index_mask_test.cc
    tests/BLI_index_range_test.cc
    tests/BLI_index_ranges_builder_test.cc
    tests/BLI_inplace_priority_queue_test.cc
    tests/BLI_kdopbvh_test.cc
    tests/BLI_kdtree_test.cc
    tests/BLI_length_parameterize_test.cc
    tests/BLI_linear_allocator_chunked_list_test.cc
    tests/BLI_linear_allocator_test.cc
    tests/BLI_linklist_lockfree_test.cc
    tests/BLI_listbase_test.cc
    tests/BLI_map_test.cc
    tests/BLI_math_base_safe_test.cc
    tests/BLI_math_base_test.cc
    tests/BLI_math_bits_test.cc
    tests/BLI_math_color_test.cc
    tests/BLI_math_geom_test.cc
    tests/BLI_math_half_test.cc
    tests/BLI_math_interp_test.cc
    tests/BLI_math_matrix_test.cc
    tests/BLI_math_matrix_types_test.cc
    tests/BLI_math_rotation_test.cc
    tests/BLI_math_rotation_types_test.cc
    tests/BLI_math_solvers_test.cc
    tests/BLI_math_time_test.cc
    tests/BLI_math_vector_test.cc
    tests/BLI_math_vector_types_test.cc
    tests/BLI_memiter_test.cc
    tests/BLI_memory_cache_test.cc
    tests/BLI_memory_counter_test.cc
    tests/BLI_memory_utils_test.cc
    tests/BLI_mesh_boolean_test.cc
    tests/BLI_mesh_intersect_test.cc
    tests/BLI_multi_value_map_test.cc
    tests/BLI_offset_indices_test.cc
    tests/BLI_path_utils_test.cc
    tests/BLI_polyfill_2d_test.cc
    tests/BLI_pool_test.cc
    tests/BLI_random_access_iterator_mixin_test.cc
    tests/BLI_ressource_strings.h
    tests/BLI_serialize_test.cc
    tests/BLI_session_uid_test.cc
    tests/BLI_set_test.cc
    tests/BLI_span_test.cc
    tests/BLI_stack_cxx_test.cc
    tests/BLI_stack_test.cc
    tests/BLI_string_ref_test.cc
    tests/BLI_string_search_test.cc
    tests/BLI_string_test.cc
    tests/BLI_string_utf8_test.cc
    tests/BLI_string_utils_test.cc
    tests/BLI_task_graph_test.cc
    tests/BLI_task_test.cc
    tests/BLI_tempfile_test.cc
    tests/BLI_unique_sorted_indices_test.cc
    tests/BLI_utildefines_test.cc
    tests/BLI_uuid_test.cc
    tests/BLI_vector_list_test.cc
    tests/BLI_vector_set_test.cc
    tests/BLI_vector_test.cc
    tests/BLI_virtual_array_test.cc

    tests/BLI_exception_safety_test_utils.hh
  )
  set(TEST_INC
    ../imbuf
  )
  set(TEST_LIB
    bf_blenlib
  )
  blender_add_test_suite_executable(BLI "${TEST_SRC}" "${INC};${TEST_INC}" "${INC_SYS}" "${LIB};${TEST_LIB}")

  add_subdirectory(tests/performance)
endif()


--- source/blender/blenloader/CMakeLists.txt ---
# SPDX-FileCopyrightText: 2006 Blender Authors
#
# SPDX-License-Identifier: GPL-2.0-or-later

set(INC
  PUBLIC .
  ../editors/include
  ../makesrna

  # RNA_prototypes.hh
  ${CMAKE_BINARY_DIR}/source/blender/makesrna
)

set(INC_SYS
)

set(SRC
  ${CMAKE_SOURCE_DIR}/release/datafiles/userdef/userdef_default_theme.c
  intern/blend_validate.cc
  intern/readblenentry.cc
  intern/readfile.cc
  intern/readfile_tempload.cc
  intern/undofile.cc
  intern/versioning_250.cc
  intern/versioning_260.cc
  intern/versioning_270.cc
  intern/versioning_280.cc
  intern/versioning_290.cc
  intern/versioning_300.cc
  intern/versioning_400.cc
  intern/versioning_410.cc
  intern/versioning_420.cc
  intern/versioning_430.cc
  intern/versioning_440.cc
  intern/versioning_450.cc
  intern/versioning_500.cc
  intern/versioning_510.cc
  intern/versioning_common.cc
  intern/versioning_defaults.cc
  intern/versioning_dna.cc
  intern/versioning_legacy.cc
  intern/versioning_userdef.cc
  intern/writefile.cc

  BLO_blend_validate.hh
  BLO_read_write.hh
  BLO_readfile.hh
  BLO_undofile.hh
  BLO_userdef_default.h
  BLO_writefile.hh
  versioning_common.hh
  intern/readfile.hh
  intern/writefile.hh
)

set(LIB
  PRIVATE bf::animrig
  PRIVATE bf::asset_system
  PRIVATE bf::blenkernel
  PRIVATE bf::blenlib
  PUBLIC bf::blenloader_core
  PRIVATE bf::blentranslation
  PRIVATE bf::bmesh
  PRIVATE bf::depsgraph
  PRIVATE bf::dna
  PRIVATE bf::draw
  PRIVATE bf::gpu
  PRIVATE bf::imbuf
  PRIVATE bf::imbuf::movie
  PRIVATE bf::intern::clog
  PRIVATE bf::intern::guardedalloc
  PRIVATE bf::extern::fmtlib
  PRIVATE bf::intern::memutil
  PRIVATE bf::nodes
  PRIVATE bf::render
  PRIVATE bf::sequencer
  PRIVATE bf::windowmanager
  PRIVATE bf::extern::xxhash
  PRIVATE bf::dependencies::zstd
)

if(WITH_BUILDINFO)
  add_definitions(-DWITH_BUILDINFO)
endif()

if(WITH_CODEC_FFMPEG)
  add_definitions(-DWITH_FFMPEG)
endif()

if(WITH_ALEMBIC)
  list(APPEND INC
    ../io/alembic
  )
  add_definitions(-DWITH_ALEMBIC)
endif()

if(WIN32)
  add_definitions(-DNOMINMAX)
endif()

blender_add_lib(bf_blenloader "${SRC}" "${INC}" "${INC_SYS}" "${LIB}")
add_library(bf::blenloader ALIAS bf_blenloader)

# RNA_prototypes.hh
add_dependencies(bf_blenloader bf_rna)

if(WITH_GTESTS)
  # Utility functions for test also used by other tests.
  set(TEST_UTIL_SRC
    tests/blendfile_loading_base_test.cc
    tests/blendfile_loading_base_test.h
  )
  set(TEST_UTIL_INC
    ${INC}
    ../../../tests/gtests
    ../../../intern/ghost
  )
  set(TEST_UTIL_INC_SYS
    ${INC_SYS}
    ${CMAKE_SOURCE_DIR}/extern/gtest/include
  )
  set(TEST_UTIL_LIB
    ${LIB}
    PRIVATE bf::blenfont
    bf_blenloader
    PRIVATE bf::dependencies::gflags
    PRIVATE bf::dependencies::glog
  )
  blender_add_lib(bf_blenloader_test_util "${TEST_UTIL_SRC}" "${TEST_UTIL_INC}" "${TEST_UTIL_INC_SYS}" "${TEST_UTIL_LIB}")

  # Actual `blenloader` tests.
  set(TEST_SRC
    tests/blendfile_load_test.cc
  )
  set(TEST_LIB
    ${LIB}
    bf_blenloader
    bf_blenloader_test_util
  )
  blender_add_test_suite_lib(blenloader "${TEST_SRC}" "${INC}" "${INC_SYS}" "${TEST_LIB}")
endif()

if(WITH_EXPERIMENTAL_FEATURES)
  add_definitions(-DWITH_EXPERIMENTAL_FEATURES)
endif()


--- tests/utils/readme.rst ---

Test Utilities
==============

These tests are not intended to run as part of automated unit testing,
rather they can be used to expose issues though stress testing or other less predictable
actions that aren't practical to include in unit tests.

Examples include:

- Loading many blend files from a directory, which can expose issues in file reading.
- Running operators in various contexts which can expose crashes.
- Simulating user input for so ``git bisect`` can be performed on bugs that require user interaction.
- Fuzz testing file importers & file format support.

Note that we could make reduced versions of these tests into unit tests at some point.
