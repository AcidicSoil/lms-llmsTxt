# llms-full (private-aware)
> Built from GitHub files and website pages. Large files may be truncated.

--- docs/tutorials/installation.md ---
# Installation Guide

This guide explains how to install **Agent-Lightning**. You can install it from **PyPI** (the Python Package Index) for general use or directly from the **source code** if you plan to contribute or need fine-grained control over dependencies.

!!! info "Platform and Hardware Requirements"
    Agent-Lightning is officially supported on **Linux distributions** (Ubuntu 22.04 or later is recommended).
    At the moment **macOS and Windows** (outside of WSL2) are not supported.

    The Python runtime must be **Python 3.10 or newer**. We recommend using the latest patch release of Python 3.10, 3.11, or 3.12 to pick up performance and security updates.

    A **GPU is optional**—you only need CUDA-capable hardware if you plan to fine-tune model weights or run GPU-accelerated workloads. CPU-only environments are fully supported for evaluation and inference.

## Installing from PyPI

The easiest way to get started is by installing Agent-Lightning directly from PyPI. This ensures you get the latest **stable release** of the package, tested for compatibility and reliability.

### Install the Stable Release

Run the following command in your terminal:

```bash
pip install --upgrade agentlightning
```

This installs or upgrades Agent-Lightning to the newest stable version.

!!! tip

    If you intend to use **Agent-Lightning** with [**VERL**](../algorithm-zoo/verl.md) or run any of its **example scripts**, you’ll need to install some additional dependencies.
    See the sections on [Algorithm-specific installation](#algorithm-specific-installation) and [Example-specific installation](#example-specific-installation) for details.

### Install the Nightly Build (Latest Features)

Agent-Lightning also publishes **nightly builds**, which contain the latest experimental features and improvements from the main branch. These are available via **Test PyPI**.

```bash
pip install --upgrade --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ --pre agentlightning
```

!!! warning
    The nightly builds are cutting-edge but may include unstable or untested changes.
    Use them **at your own risk**, especially in production environments.

## Algorithm-specific Installation

Agent-Lightning supports multiple learning algorithms. Some of them like [APO](../algorithm-zoo/apo.md) or [VERL](../algorithm-zoo/verl.md) require extra dependencies. You can install them automatically using **optional extras** or manually if you prefer finer control.

### Installing APO

[APO](../algorithm-zoo/apo.md) is an algorithm module that depends on libraries such as [POML](https://github.com/microsoft/POML).
You can install Agent-Lightning with APO support by running:

```bash
pip install agentlightning[apo]
```

!!! warning
    APO also depends on the [OpenAI Python SDK](https://github.com/openai/openai-python), version **2.0 or newer**.
    Ensure your SDK version is up to date to avoid compatibility issues.

### Installing VERL

[VERL](../algorithm-zoo/verl.md) integrates with libraries like **PyTorch**, **vLLM**, and **VERL framework**.
Although you *can* install all dependencies automatically, we recommend doing it manually to avoid version conflicts.

```bash
pip install agentlightning[verl]
```

!!! tip "Recommended Manual Setup (More Stable)"
    Automated installation may cause issues if you don’t have a compatible **PyTorch** or **CUDA** version preinstalled.
    For a more stable setup, install dependencies step-by-step:

    ```bash
    pip install torch==2.8.0 torchvision==0.23.0 --index-url https://download.pytorch.org/whl/cu128
    pip install flash-attn --no-build-isolation
    pip install vllm==0.10.2
    pip install verl==0.5.0
    ```

    This approach ensures compatibility with CUDA 12.8 and minimizes dependency conflicts.

## Example-specific Installation

Each example in the `examples/` directory may have its own additional dependencies.
Please refer to the **README** file of each example for detailed setup instructions:

[See Example READMEs]({{ src("examples") }}).

## Installing from Source (for Developers and Contributors)

If you plan to contribute to Agent-Lightning or prefer to work with the latest development code, install it directly from the **source repository**.

### Why Install from Source?

* You want to **modify or contribute** to the project.
* You prefer an **isolated development environment**.
* You want to test unreleased features or fix bugs locally.

### Using `uv` for Dependency Management

Starting with version **0.2**, Agent-Lightning uses [`uv`](https://docs.astral.sh/uv/) as its **default dependency manager**.

`uv` is a fast and safe alternative to `pip` that:

* Installs packages **in seconds** (instead of minutes),
* Prevents **dependency conflicts**,
* Supports **grouped dependencies** for optional features.

Before proceeding, make sure `uv` is installed.

### Minimal Developer Installation

```bash
git clone https://github.com/microsoft/agent-lightning
cd agent-lightning
uv sync --group dev
```

This command sets up a clean development environment with only the essential dependencies.

### Installing All Extras (CPU or GPU)

`uv sync` can also handle algorithm-specific and example-specific dependencies in one step.

For a CPU-only machine:

```bash
uv sync --frozen \
    --extra apo \
    --extra verl \
    --group dev \
    --group torch-cpu \
    --group torch-stable \
    --group trl \
    --group agents \
    --no-default-groups
```

For a GPU-equipped machine that is CUDA 12.8 compatible:

```bash
uv sync --frozen \
    --extra apo \
    --extra verl \
    --group dev \
    --group torch-gpu-stable \
    --group trl \
    --group agents \
    --no-default-groups
```

Read more about Agent-lightning managed dependency groups [here]({{ src("pyproject.toml") }}).

### Building the Dashboard

The Agent-Lightning dashboard is built using [Vite](https://vite.dev/). To build the dashboard, run the following command:

```bash
cd dashboard
npm ci
npm run build
```

Some HTML and JavaScript assets will be generated in the `agentlightning/dashboard` directory.

### Activating Your Environment

After syncing dependencies, `uv` automatically creates a virtual environment inside the `.venv/` directory.

You can use it in two ways:

```bash
# Option 1: Prefix commands with uv run
uv run python your_script.py

# Option 2: Activate the virtual environment
source .venv/bin/activate
python your_script.py
```

!!! warning "Before Contributing"

    Agent-Lightning enforces code style and linting rules via **pre-commit hooks**.
    Installing them early prevents many avoidable formatting issues.

    ```bash
    uv run pre-commit install
    uv run pre-commit run --all-files --show-diff-on-failure --color=always
    ```


## Links discovered
- [**VERL**](https://github.com/microsoft/agent-lightning/blob/main/docs/algorithm-zoo/verl.md)
- [APO](https://github.com/microsoft/agent-lightning/blob/main/docs/algorithm-zoo/apo.md)
- [VERL](https://github.com/microsoft/agent-lightning/blob/main/docs/algorithm-zoo/verl.md)
- [POML](https://github.com/microsoft/POML)
- [OpenAI Python SDK](https://github.com/openai/openai-python)
- [`uv`](https://docs.astral.sh/uv/)
- [Vite](https://vite.dev/)

--- docs/tutorials/debug.md ---
# Debugging and Troubleshooting

When you train your own agent with Agent-lightning, most failures surface because the agent logic is brittle or simply incorrect. Debugging becomes easier when you peel back the stack: start by driving the rollout logic on its own, dry-run the trainer loop, and only then bring the full algorithm and runner topology online. The [`examples/apo/apo_debug.py`]({{ src("examples/apo/apo_debug.py") }}) script demonstrates these techniques; this guide expands on each approach and helps you decide when to reach for them.

## Debugging with Dashboard

When you launch an experiment with [`Trainer.fit`][agentlightning.Trainer.fit] or start an isolated store via [`agl store`](../reference/cli.md), the terminal prints a message similar to:

```text
INFO     Agent-lightning dashboard will be available at http://192.168.0.107:4747
```

Visit that URL, and you will see the Agent-lightning dashboard:

![Dashboard](../assets/dashboard-page-rollouts.png)

The dashboard surfaces everything stored inside [the store](../deep-dive/store.md). Because the store mediates interactions between algorithms and runners, inspecting it often reveals which side is causing issues such as stale rollouts, unresponsive workers, or empty traces.

For example, the VERL algorithm may receive no token IDs and emit `cannot reshape tensor of 0 elements into shape [1, 0, -1, 128] because the unspecified dimension size -1 can be any value and is ambiguous` ([Issue #50](https://github.com/microsoft/agent-lightning/issues/50), [Issue #76](https://github.com/microsoft/agent-lightning/issues/76)). Several scenarios can produce that error: the runner might not produce trace spans at all, it might produce spans without token IDs, or the IDs may be present but formatted incorrectly. Inspecting the dashboard traces helps you pinpoint which condition applies.

![Dashboard Traces Page](../assets/dashboard-page-traces.png)

By checking whether the trace span is empty and whether token IDs appear in the span attributes, you can narrow the issue to either the runner (agent) side or the algorithm side. Then apply the techniques below to debug the faulty component.

## Debug-level Logging

Starting from v0.3, detailed signals such as store server access logs, runner lifecycle logs, and span payloads only appear when the log level is `DEBUG` so the default output stays readable. Enable debug-level logging by adding the following snippet near the top of your script:

```python
import agentlightning as agl

agl.setup_logging("DEBUG")
```

Set the log level on every process if your setup involves multiple workers. For example, when [running stores in isolation][debug-with-external-store], configure the store process explicitly:

```bash
agl store --port 4747 --log-level DEBUG
```

## Using [`Runner`][agentlightning.Runner] in Isolation

[`Runner`][agentlightning.Runner] is a long-lived worker that wraps your [`LitAgent`][agentlightning.LitAgent], coordinates tracing, and talks to the [`LightningStore`][agentlightning.LightningStore]. In typical training flows the trainer manages runners for you, but being able to spin one up manually is invaluable while debugging.

If you define rollout logic with [`@rollout`][agentlightning.rollout] or implement a [`LitAgent`][agentlightning.LitAgent] directly, you will get a [`LitAgent`][agentlightning.LitAgent] instance and you should be able to execute it with [`LitAgentRunner`][agentlightning.LitAgentRunner], which is a subclass of [`Runner`][agentlightning.Runner]. The runner needs but does not instantiate a [`Tracer`][agentlightning.Tracer], so supply one yourself. See [Working with Traces](./traces.md) for a walkthrough of tracer options.

[`Runner.run_context`][agentlightning.Runner.run_context] prepares the runner to execute a particular agent. Besides the agent and tracer you must provide a store that will collect spans and rollouts. [`InMemoryLightningStore`][agentlightning.InMemoryLightningStore] keeps everything in-process, which is perfect for debugging sessions.

```python
import agentlightning as agl

tracer = agl.OtelTracer()
runner = agl.LitAgentRunner(tracer)
store = agl.InMemoryLightningStore()

with runner.run_context(agent=apo_rollout, store=store):
    ...
```

Inside the [`run_context`][agentlightning.Runner.run_context] block you can call [`runner.step(...)`][agentlightning.Runner.step] to execute a single rollout. The payload includes the task input and any [`NamedResources`][agentlightning.NamedResources] the agent expects. Read [introduction to Resources][introduction-to-resources] and [NamedResources][introduction-to-named-resources] for more details. For example, if your agent references a [`PromptTemplate`][agentlightning.PromptTemplate], pass it through the `resources` argument:

```python
with runner.run_context(agent=apo_rollout, store=store):
    resource = agl.PromptTemplate(template="You are a helpful assistant. {any_question}", engine="f-string")
    rollout = await runner.step(
        "Explain why the sky appears blue using principles of light scattering in 100 words.",
        resources={"main_prompt": resource},
    )
```

You can do as many things as you want within the [`Runner.run_context`][agentlightning.Runner.run_context] block. After the rollout finishes you can query the store to inspect what happened:

```python
print(await store.query_rollouts())
print(await store.query_spans(rollout.rollout_id))
```

Example output (with a reward span captured):

```python
[Rollout(rollout_id='ro-519769241af8', input='Explain why the sky appears blue using principles of light scattering in 100 words.', start_time=1760706315.6996238, ..., status='succeeded')]
[Span(rollout_id='ro-519769241af8', attempt_id='at-a6b62caf', sequence_id=1, ..., name='agentlightning.annotation', attributes={'agentlightning.reward.0.value': 0.95}, ...)]
```

Swap in an [`AgentOpsTracer`][agentlightning.AgentOpsTracer] instead of [`OtelTracer`][agentlightning.OtelTracer] to see the underlying LLM spans alongside reward information:

```python
[
    Span(rollout_id='ro-519769241af8', attempt_id='at-a6b62caf', sequence_id=1, ..., name='openai.chat.completion', attributes={..., 'gen_ai.prompt.0.role': 'user', 'gen_ai.prompt.0.content': 'You are a helpful assistant. Explain why the sky appears blue using principles of light scattering in 100 words.', ...}),
    Span(rollout_id='ro-519769241af8', attempt_id='at-a6b62caf', sequence_id=2, ..., name='openai.chat.completion', attributes={..., 'gen_ai.prompt.0.role': 'user', 'gen_ai.prompt.0.content': 'Evaluate how well the output fulfills the task...', ...}),
    Span(rollout_id='ro-519769241af8', attempt_id='at-a6b62caf', sequence_id=3, ..., name='agentlightning.annotation', attributes={'agentlightning.reward.0.value': 0.95}, ...)
]
```

!!! tip

    Spans too difficult to read? Try using [`Adapter`][agentlightning.Adapter] to convert them into a [more readable format](./traces.md).

[`Runner.step`][agentlightning.Runner.step] executes a full rollout even though it is named "step". The companion method [`Runner.iter`][agentlightning.Runner.iter] executes multiple "steps" by continuously pulling new rollout inputs from the store until a stop event is set. Use `iter` once you are confident the single-step path works and you have another worker [`enqueue_rollout`][agentlightning.LightningStore.enqueue_rollout] to the store.

!!! tip

    You can also call [`Runner.step`][agentlightning.Runner.step] to inject ad-hoc rollouts into a running store being used by another algorithm, so that the rollouts can be consumed by the algorithms. This is very recently known as the paradigm of ["online RL"](https://cursor.com/blog/tab-rl). At the moment, no algorithm in the [algorithm zoo](../algorithm-zoo/index.md) consumes externally generated rollouts, but the data flow is available there if you need it.

## Debug with LLM Proxy

If you are dealing with LLM optimization like Reinforcement Learning, we generally recommend using an online stable LLM service for your debugging purposes, like `openai/gpt-4.1-nano`. After the debugging is done, you can switch to a local training endpoint.

However, if you want to use a local LLM features like [getting the token IDs](../deep-dive/serving-llm.md), you can also manually start a local vLLM server by:

```bash
vllm serve Qwen/Qwen2.5-0.5B-Instruct --port 8080
```

Then start the LLM proxy via the following script:

```python
import asyncio
import aiohttp
import agentlightning as agl

async def serve_llm_proxy():
    store = agl.InMemoryLightningStore()
    store_server = agl.LightningStoreServer(store, "127.0.0.1", 8081)
    await store_server.start()

    llm_proxy = agl.LLMProxy(
        port=8082,
        model_list=[
            {
                "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
                "litellm_params": {
                    "model": "hosted_vllm/Qwen/Qwen2.5-0.5B-Instruct",
                    "api_base": "http://localhost:8080/v1",
                },
            }
        ],
        store=store_server,
    )

    await llm_proxy.start()
    await asyncio.sleep(1000000)
```

Test the served LLM proxy with a client like:

```python
async def test_llm_proxy():
    async with aiohttp.ClientSession() as session:
        async with session.post("http://localhost:8082/v1/chat/completions", json={
            "model": "Qwen/Qwen2.5-0.5B-Instruct",
            "messages": [{"role": "user", "content": "Hello, world!"}],
        }) as response:
            print(await response.json())
```

You can now use the LLM proxy by specifying environment variables:

```bash
export OPENAI_API_BASE=http://localhost:8081/v1
export OPENAI_API_KEY=dummy
```

You might see warnings about `Missing or invalid rollout_id, attempt_id, or sequence_id` in the LLM proxy logs. This is fine because you don't have a rollout and attempt yet when you are debugging. When you started the training, the algorithm will create the rollouts for you and the warnings will go away.

## Hook into Runner's Lifecycle

[`Runner.run_context`][agentlightning.Runner.run_context] accepts a `hooks` argument so you can observe or augment lifecycle events without editing your agent. Hooks subclass [`Hook`][agentlightning.Hook] and can respond to four asynchronous callbacks: [`on_trace_start`][agentlightning.Hook.on_trace_start], [`on_rollout_start`][agentlightning.Hook.on_rollout_start], [`on_rollout_end`][agentlightning.Hook.on_rollout_end], and [`on_trace_end`][agentlightning.Hook.on_trace_end]. This is useful for:

- Capturing raw OpenTelemetry spans before they hit the store and before the [`LitAgentRunner`][agentlightning.LitAgentRunner] do postprocessing on the rollout
- Inspecting the tracer instance after they are activated
- Logging rollout inputs before they are processed by the agent

The `hook` mode in [`examples/apo/apo_debug.py`]({{ src("examples/apo/apo_debug.py") }}) prints every span collected during a rollout:

```python
import agentlightning as agl

# ... Same as previous example

class DebugHook(agl.Hook):
    async def on_trace_end(self, *, agent, runner, tracer, rollout):
        trace = tracer.get_last_trace()
        print("Trace spans collected during the rollout:")
        for span in trace:
            print(f"- {span.name} (status: {span.status}):\n  {span.attributes}")

with runner.run_context(
    agent=apo_rollout,
    store=store,
    hooks=[DebugHook()],
):
    await runner.step(
        "Explain why the sky appears blue using principles of light scattering in 100 words.",
        resources={"main_prompt": resource},
    )
```

Because hooks run inside the runner process you can also attach debuggers or breakpoints directly in the callback implementations.

!!! note

    For a better understanding of where hooks are called, we show a pseudo code of Runner's working flow below:

    ```python
    resources = await store.get_latest_resources()
    rollout = ...
    try:
        # <-- on_rollout_start
        with tracer.trace_context(...):
            # <--- on_trace_start
            result = await agent.rollout(...)
            # <--- on_trace_end
        post_process_result(result)
    except Exception:
        # <-- on_rollout_end
        await store.update_attempt(status=...)
    ```

## Dry-Run the Trainer Loop

Once single rollouts behave, switch to the trainer’s dry-run mode. [`Trainer.dev`][agentlightning.Trainer.dev] spins up a lightweight fast algorithm — [`agentlightning.Baseline`][agentlightning.Baseline] by default — so you can exercise the same infrastructure as [`Trainer.fit`][agentlightning.Trainer.fit] without standing up complex stacks like RL or SFT.

!!! warning
    When you enable multiple runners via `n_runners`, the trainer may execute them in separate worker processes. Attaching a debugger such as `pdb` is only practical when `n_runners=1`, and even then the runner might not live in the main process.

```python
import agentlightning as agl

dataset: agl.Dataset[str] = [
    "Explain why the sky appears blue using principles of light scattering in 100 words.",
    "What's the capital of France?",
]
resource = agl.PromptTemplate(template="You are a helpful assistant. {any_question}", engine="f-string")

trainer = agl.Trainer(
    n_runners=1,
    initial_resources={"main_prompt": resource},
)
trainer.dev(apo_rollout, dataset)
```

Just like [`Runner.run_context`][agentlightning.Runner.run_context], [`Trainer.dev`][agentlightning.Trainer.dev] requires the [`NamedResources`][agentlightning.NamedResources] your agent expects. The key difference is that resources are attached to the trainer rather than the runner.

[`Trainer.dev`][agentlightning.Trainer.dev] uses an almost switchable interface from [`Trainer.fit`][agentlightning.Trainer.fit]. It also needs a dataset to iterate over, similar to [`fit`][agentlightning.Trainer.fit]. Under the hood [`dev`][agentlightning.Trainer.dev] uses the same implementation as [`fit`][agentlightning.Trainer.fit], which means you can spin up multiple runners, observe scheduler behavior, and validate how algorithms adapt rollouts. The default [`Baseline`][agentlightning.Baseline] logs detailed traces so you can see each rollout as the algorithm perceives it:

```text
21:20:30 Initial resources set: {'main_prompt': PromptTemplate(resource_type='prompt_template', template='You are a helpful assistant. {any_question}', engine='f-string')}
21:20:30 Proceeding epoch 1/1.
21:20:30 Enqueued rollout ro-302fb202bd85 in train mode with sample: Explain why the sky appears blue using principles of light scattering in 100 words.
21:20:30 Enqueued rollout ro-e65a3ffaa540 in train mode with sample: What's the capital of France?
21:20:30 Waiting for 2 harvest tasks to complete...
21:20:30 [Rollout ro-302fb202bd85] Status is initialized to queuing.
21:20:30 [Rollout ro-e65a3ffaa540] Status is initialized to queuing.
21:20:35 [Rollout ro-302fb202bd85] Finished with status succeeded in 3.80 seconds.
21:20:35 [Rollout ro-302fb202bd85 | Attempt 1] ID: at-f84ad21c. Status: succeeded. Worker: Worker-0
21:20:35 [Rollout ro-302fb202bd85 | Attempt at-f84ad21c | Span 3a286a856af6bea8] #1 (openai.chat.completion) ... 1.95 seconds. Attribute keys: ['gen_ai.request.type', 'gen_ai.system', ...]
21:20:35 [Rollout ro-302fb202bd85 | Attempt at-f84ad21c | Span e2f44b775e058dd6] #2 (openai.chat.completion) ... 1.24 seconds. Attribute keys: ['gen_ai.request.type', 'gen_ai.system', ...]
21:20:35 [Rollout ro-302fb202bd85 | Attempt at-f84ad21c | Span 45ee3c94fa1070ec] #3 (agentlightning.annotation) ... 0.00 seconds. Attribute keys: ['agentlightning.reward.0.value']
21:20:35 [Rollout ro-302fb202bd85] Adapted data: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': '...', 'agent_name': ''}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=0.95, metadata={'response_id': '...', 'agent_name': ''})]
21:20:35 Finished 1 rollouts.
21:20:35 [Rollout ro-e65a3ffaa540] Status changed to preparing.
21:20:40 [Rollout ro-e65a3ffaa540] Finished with status succeeded in 6.39 seconds.
21:20:40 [Rollout ro-e65a3ffaa540 | Attempt 1] ID: at-eaefa5d4. Status: succeeded. Worker: Worker-0
21:20:40 [Rollout ro-e65a3ffaa540 | Attempt at-eaefa5d4 | Span 901dd6acc0f50147] #1 (openai.chat.completion) ... 1.30 seconds. Attribute keys: ['gen_ai.request.type', 'gen_ai.system', ...]
21:20:40 [Rollout ro-e65a3ffaa540 | Attempt at-eaefa5d4 | Span 52e0aa63e02be611] #2 (openai.chat.completion) ... 1.26 seconds. Attribute keys: ['gen_ai.request.type', 'gen_ai.system', ...]
21:20:40 [Rollout ro-e65a3ffaa540 | Attempt at-eaefa5d4 | Span 6c452de193fbffd3] #3 (agentlightning.annotation) ... 0.00 seconds. Attribute keys: ['agentlightning.reward.0.value']
21:20:40 [Rollout ro-e65a3ffaa540] Adapted data: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': '...', 'agent_name': ''}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=1.0, metadata={'response_id': '...', 'agent_name': ''})]
21:20:40 Finished 2 rollouts.
```

The only limitation is that resources remain static and components like [`LLMProxy`][agentlightning.LLMProxy] are not wired in. For richer dry runs you can subclass [`FastAlgorithm`][agentlightning.FastAlgorithm] and override the pieces you care about.

## Debug the Algorithm-Runner Boundary

[](){ #debug-with-external-store }

Debugging algorithms in Agent-Lightning is often more challenging than debugging agents. Algorithms are typically **stateful** and depend on several moving parts — runners, stores, and trainers — which makes it difficult to isolate and inspect their behavior. Even mocking an agent to cooperate with an algorithm can be costly and error-prone. To simplify this, Agent-Lightning provides a way to run algorithms in isolation so you can attach a debugger and inspect internal state without interference from other components.

By default, [`Trainer.fit`][agentlightning.Trainer.fit] runs the algorithm in the main process and thread, but its logs are interleaved with those from the store and runners, making it hard to follow what’s happening inside the algorithm itself. In [*Write Your First Algorithm*](../how-to/write-first-algorithm.md), we covered how to stand up a store, algorithm, and runner in isolation for your own implementations. This section extends that approach to cover two common questions:

1. How can I run built-in or class-based algorithms (inheriting from [`Algorithm`][agentlightning.Algorithm]) in isolation?
2. How can I still use [`Trainer`][agentlightning.Trainer] features like `n_runners`, `adapter`, or `llm_proxy` while debugging?

The solution is to keep using a [`Trainer`][agentlightning.Trainer] instance but **manage the store yourself**, running the algorithm and runner roles separately. This approach mirrors the internal process orchestration of [`Trainer.fit`][agentlightning.Trainer.fit], but with more visibility and control. Below, we show a step-by-step guide to achieve this with the [`calc_agent` example]({{ src("examples/calc_x/train_calc_agent.py") }}).

**1. Launch the store manually.**
In a separate terminal, start the store:

```bash
agl store --port 4747
```

Add `--log-level DEBUG` to the command to see the detailed logs.

Then, in your training script, create a [`LightningStoreClient`][agentlightning.LightningStoreClient] and pass it to the trainer:

```python
client = agl.LightningStoreClient("http://localhost:4747")
trainer = agl.Trainer(store=client, ...)
```

Set the environment variable `AGL_MANAGED_STORE=0` so the trainer doesn't attempt to manage the store automatically.

**2. Start the runner and algorithm processes separately.**
Each process should run the same training script, but with different environment variables specifying the current role.
This setup faithfully mirrors how [`Trainer.fit`][agentlightning.Trainer.fit] orchestrates these components behind the scenes.

```bash
# Terminal 2 – Runner process
AGL_MANAGED_STORE=0 AGL_CURRENT_ROLE=runner \
    python train_calc_agent.py --external-store-address http://localhost:4747 --val-file data/test_mini.parquet

# Terminal 3 – Algorithm process
AGL_MANAGED_STORE=0 AGL_CURRENT_ROLE=algorithm \
    python train_calc_agent.py --external-store-address http://localhost:4747 --val-file data/test_mini.parquet
```

**3. Reuse your existing trainer configuration.**
You can continue using the same datasets, adapters, and proxies as usual. Because the store is now external, you can:

* Attach debuggers to either the algorithm or runner process
* Add fine-grained logging or tracing
* Simulate partial failures or latency in individual components

This setup provides a faithful reproduction of the algorithm–runner interaction while keeping the store visible for inspection. Once you’ve resolved the issue, simply set `AGL_MANAGED_STORE=1` (or omit it) to return to the standard managed training workflow.


## Links discovered
- [`agl store`](https://github.com/microsoft/agent-lightning/blob/main/docs/reference/cli.md)
- [Dashboard](https://github.com/microsoft/agent-lightning/blob/main/docs/assets/dashboard-page-rollouts.png)
- [the store](https://github.com/microsoft/agent-lightning/blob/main/docs/deep-dive/store.md)
- [Issue #50](https://github.com/microsoft/agent-lightning/issues/50)
- [Issue #76](https://github.com/microsoft/agent-lightning/issues/76)
- [Dashboard Traces Page](https://github.com/microsoft/agent-lightning/blob/main/docs/assets/dashboard-page-traces.png)
- [Working with Traces](https://github.com/microsoft/agent-lightning/blob/main/docs/tutorials/traces.md)
- [more readable format](https://github.com/microsoft/agent-lightning/blob/main/docs/tutorials/traces.md)
- ["online RL"](https://cursor.com/blog/tab-rl)
- [algorithm zoo](https://github.com/microsoft/agent-lightning/blob/main/docs/algorithm-zoo/index.md)
- [getting the token IDs](https://github.com/microsoft/agent-lightning/blob/main/docs/deep-dive/serving-llm.md)
- [*Write Your First Algorithm*](https://github.com/microsoft/agent-lightning/blob/main/docs/how-to/write-first-algorithm.md)

--- docs/tutorials/emitter.md ---
# Using Emitters

[](){ #using-emitter }

While returning a single float for the final reward is sufficient for many algorithm-agent combinations, some advanced scenarios require richer feedback. For instance, an algorithm might learn more effectively if it receives intermediate rewards throughout a multi-step task, or if the agent needs to emit additional spans for debugging or analysis.

Agent-lightning provides an **emitter** module for recording custom spans inside your agent logic. Just as [Tracer][agentlightning.Tracer] automatically instruments common operations (for example, LLM calls), each emitter helper sends a [Span][agentlightning.Span] that captures Agent-lightning-specific work so downstream algorithms can query it later. See [Working with Traces](./traces.md) for more details.

For multi-step routines such as function calls, tools, or adapters, wrap code with [`operation`][agentlightning.operation] — either as a decorator or a context manager — to capture inputs, outputs, and metadata on a dedicated [`operation`][agentlightning.operation] span. This makes it easier to correlate downstream annotations (like rewards or messages) with the higher-level work that produced them.

You can find the emitter functions in [`agentlightning.emitter`](../reference/agent.md).

## Emitting Rewards, Messages, and More

Here are the primary emitter functions:

* [`emit_reward(value: float)`][agentlightning.emit_reward]: Records an intermediate/final reward, which is a convenient wrapper of [`emit_annotation`][agentlightning.emit_annotation].
* [`emit_annotation(attributes: Dict[str, Any])`][agentlightning.emit_annotation]: Records arbitrary metadata as a span.
* [`emit_message(message: str)`][agentlightning.emit_message]: Records a simple log message as a span.
* [`emit_exception(exception: BaseException)`][agentlightning.emit_exception]: Records a Python exception, including its type, message, and stack trace.
* [`emit_object(obj: Any)`][agentlightning.emit_object]: Records any JSON-serializable object, perfect for structured data.

Let's first see an example of an agent using these emitters to provide detailed feedback.

```python
import agentlightning as agl

@agl.rollout
def multi_step_agent(task: dict, prompt_template: PromptTemplate) -> float:
    try:
        # Step 1: Initial planning
        agl.emit_message("Starting planning phase.")
        plan = generate_plan(task, prompt_template)
        agl.emit_object({"plan_steps": len(plan), "first_step": plan[0]})

        # Award a small reward for a valid plan
        plan_reward = grade_plan(plan)
        agl.emit_reward(plan_reward)

        # Step 2: Execute the plan
        agl.emit_message(f"Executing {len(plan)}-step plan.")
        execution_result = execute_plan(plan)

        # Step 3: Final evaluation
        final_reward = custom_grade_final_result(execution_result, task["expected_output"])

        # The return value is treated as the final reward for the rollout
        return final_reward

    except ValueError as e:
        # Record the specific error and return a failure reward
        agl.emit_exception(e)
        return 0.0
```

Each helper accepts nested `attributes` (or keyword arguments for [`operation`][agentlightning.operation]) and automatically flattens/sanitizes them into dotted OpenTelemetry keys. This means you can pass ordinary dictionaries/lists without pre-processing and still get consistent attribute names such as `meta.any_attribute` across all emitter operations. Agent-lightning does not restrict the attributes you supply, but it is best to consult [OpenTelemetry's semantic conventions](https://opentelemetry.io/docs/specs/semconv/) for recommended names. Agent-lightning also defines [specific semconv](../reference/semconv.md) for its own use cases. The pattern looks like this:

```python
from opentelemetry.semconv.attributes import server_attributes
from agentlightning import emit_object

emit_object({
    "name": "John Doe",
    "age": 30,
    "email": "john.doe@example.com",
}, attributes={
    server_attributes.SERVER_ADDRESS: "127.0.0.1",
    server_attributes.SERVER_PORT: 8080,
})
```

Running the above code sends the following span to the backend if you have a tracer active:

```text
Span(
    name='agentlightning.object',
    attributes={
        'agentlightning.object.type': 'dict',
        'agentlightning.object.json': '{"name": "John Doe", "age": 30, "email": "john.doe@example.com"}',
        'server.address': '127.0.0.1',
        'server.port': 8080
    }
)
```

!!! tip

    If you don't have a tracer active, the above code will raise the following error:

    ```text
    RuntimeError: No active tracer found. Cannot emit object span.
    ```

    By default, emitter helpers delegate to the active tracer to create and export spans (specifically via [`Tracer.create_span`][agentlightning.Tracer.create_span]). If you want to emit spans without an active tracer, set `propagate=False` to keep the span local — a useful option for offline tests. The default `True` streams spans through the active tracer/exporters.

When working with [agentlightning.semconv](../reference/semconv.md), you typically use utilities such as [`make_tag_attributes`][agentlightning.utils.otel.make_tag_attributes] and [`make_link_attributes`][agentlightning.utils.otel.make_link_attributes] to build the attributes dictionary. For example:

```python
from agentlightning.utils.otel import make_tag_attributes

emit_annotation(make_tag_attributes(["tool", "calculator", "fast", "good"]))
```

The above code will send a span with the following attributes to the backend:

```json
{
    "agentlightning.tag.0": "tool",
    "agentlightning.tag.1": "calculator",
    "agentlightning.tag.2": "fast",
    "agentlightning.tag.3": "good"
}
```

A counterpart utility function [`extract_tags_from_attributes`][agentlightning.utils.otel.extract_tags_from_attributes] is also available to extract the tags from the attributes dictionary.

## Operations

The [`operation`][agentlightning.operation] helper tracks logical units of work within your agent, capturing inputs, outputs, timing, and success/failure status. Unlike point-in-time emitters, operations create a span representing a time interval. Use operations for tool calls, multi-step workflows, debugging, and performance monitoring. [`operation`][agentlightning.operation] works as either a decorator or a context manager.

The decorator automatically captures function arguments as inputs and the return value as output:

```python
import agentlightning as agl

@agl.operation
def search_documents(query: str, max_results: int = 10) -> list[dict]:
    results = perform_search(query, max_results)
    return results

@agl.operation(category="tool", priority="high")
def execute_calculation(expression: str) -> float:
    return eval_safely(expression)
```

The example above emits a span with `{"category": "tool", "priority": "high"}` attributes. It also records the function input and output via [OPERATION_INPUT][agentlightning.semconv.LightningSpanAttributes.OPERATION_INPUT] and [OPERATION_OUTPUT][agentlightning.semconv.LightningSpanAttributes.OPERATION_OUTPUT]. It works with async functions too:

```python
@agl.operation
async def async_api_call(endpoint: str, payload: dict) -> dict:
    response = await http_client.post(endpoint, json=payload)
    return response.json()
```

Override the operation name if needed:

```python
@agl.operation(name="custom-name")
def any_weird_name_i_dont_want():
    pass
```

For more control, [`operation`][agentlightning.operation] can also be used as a context manager to explicitly record inputs and outputs:

```python
with agl.operation(tool_name="web_search") as op:
    op.set_input(query="latest AI research", filters={"date": "2024"})
    results = search_web("latest AI research", {"date": "2024"})
    op.set_output({"result_count": len(results), "top_result": results[0]})
```

The `propagate=False` flag also applies to [`operation`][agentlightning.operation] when you want to keep operations local without requiring an active tracer:

```python
@agl.operation(propagate=False)
def local_test():
    return "Not sent to backend"
```

## Linking to Other Spans

Sometimes a span should explicitly point back to another span that produced the input it is working on (for example, linking a reward annotation to the [`agentlightning.operation`][agentlightning.operation] span that generated a response). Agent-lightning encodes these relationships through flattened link attributes. The helper [`make_link_attributes`][agentlightning.utils.otel.make_link_attributes] converts a dictionary of keys such as `trace_id`, `span_id`, or any custom attribute into the `"agentlightning.link.*"` ([LightningSpanAttributes.LINK][agentlightning.semconv.LightningSpanAttributes.LINK]) fields expected by the backend. Later, [`query_linked_spans`][agentlightning.utils.otel.query_linked_spans] can recover the original span(s) from those link descriptors.

```python
import opentelemetry.trace as trace_api
from agentlightning import emit_annotation, operation
from agentlightning.utils.otel import make_link_attributes, make_tag_attributes

with operation(conversation_id="chat-42") as op:
    # ... perform the work ...
    link_attrs = make_link_attributes({
        "conversation_id": "chat-42",
    })

    emit_annotation(
        {
            **link_attrs,
            **make_tag_attributes(["reward", "good"]),
        }
    )
```

When analyzing in adapters, pass the extracted link models to [`query_linked_spans`][agentlightning.utils.otel.query_linked_spans] to retrieve the matching span(s):

```python
from agentlightning.utils.otel import extract_links_from_attributes, query_linked_spans

annotation_span = ...  # Span from your trace store
operation_spans = [...]  # list of spans you want to search

link_models = extract_links_from_attributes(annotation_span.attributes)
matches = query_linked_spans(operation_spans, link_models)
assert matches  # Contains the original operation span
```

!!! tip "Correlating Rewards with LLM Requests"

    [Tracer](./traces.md) instruments each request/response as its own span. You can link to the [`gen_ai.response.id`](https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-events/) attribute, which comes from the LLM response ID.

    ```python
    from agentlightning import emit_reward
    from agentlightning.utils.otel import make_link_attributes

    result = call_llm(prompt)
    reward_links = make_link_attributes({"gen_ai.response.id": result.id})
    emit_reward(0.9, attributes=reward_links)
    ```

    Later, use the same `gen_ai.response.id` key inside `query_linked_spans` to find the reward(s) that reference that specific LLM request span.


## Links discovered
- [Working with Traces](https://github.com/microsoft/agent-lightning/blob/main/docs/tutorials/traces.md)
- [`agentlightning.emitter`](https://github.com/microsoft/agent-lightning/blob/main/docs/reference/agent.md)
- [OpenTelemetry's semantic conventions](https://opentelemetry.io/docs/specs/semconv/)
- [specific semconv](https://github.com/microsoft/agent-lightning/blob/main/docs/reference/semconv.md)
- [agentlightning.semconv](https://github.com/microsoft/agent-lightning/blob/main/docs/reference/semconv.md)
- [Tracer](https://github.com/microsoft/agent-lightning/blob/main/docs/tutorials/traces.md)
- [`gen_ai.response.id`](https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-events/)

--- docs/how-to/examples-catalog.md ---
# Examples Catalog

!!! tip "Want to Contribute?"

    We welcome contributions to the examples catalog! Please refer to the [Contributing](../community/contributing.md) guide for more details.

<div class="grid cards" markdown>

-   :material-robot:{ .lg .middle } __APO room selector__

    ---

    Prompt-optimize a room-booking agent with the built-in APO algorithm, then contrast it with the write-your-own algorithm and debugging workflows in the tutorials. Pairs well with the [Train the First Agent how-to]({{ src("docs/how-to/train-first-agent.md") }}) and the [Write the First Algorithm guide]({{ src("docs/how-to/write-first-algorithm.md") }}).

    [:octicons-repo-24: Browse source]({{ src("examples/apo") }})

-   :material-cloud-sync:{ .lg .middle } __Azure OpenAI SFT__

    ---

    Run a supervised fine-tuning loop against Azure OpenAI: roll out the capital-lookup agent, turn traces into JSONL, launch fine-tunes, and redeploy the resulting checkpoints through Azure CLI.

    [:octicons-repo-24: Browse source]({{ src("examples/azure") }})

-   :material-calculator:{ .lg .middle } __Calc-X VERL math__

    ---

    VERL-based reinforcement learning setup for a math-reasoning agent that uses AutoGen plus an MCP calculator tool to solve Calc-X problems end to end.

    [:octicons-repo-24: Browse source]({{ src("examples/calc_x") }})

-   :material-chart-box:{ .lg .middle } __ChartQA vision-language RL__

    ---

    LangGraph-powered workflow for answering chart questions end to end: rollout the multi-modality agent with GPT or vLLM, and train with VERL/GRPO plus self-refinement loops.

    [:octicons-repo-24: Browse source]({{ src("examples/chartqa") }})

-   :material-code-braces:{ .lg .middle } __Claude Code SWE-bench__

    ---

    Instrumented driver that runs Anthropic's Claude Code workflow on SWE-bench instances while streaming traces through Agent-lightning—supports hosted vLLM, official Anthropic, or any OpenAI-compatible backend and emits datasets for downstream tuning.

    [:octicons-repo-24: Browse source]({{ src("examples/claude_code") }})

-   :material-view-grid:{ .lg .middle } __Minimal building blocks__

    ---

    Bite-sized scripts that isolate Agent-lightning primitives (e.g., LightningStore usage, LLM proxying, minimal vLLM host) so you can study each part before composing larger workflows.

    [:octicons-repo-24: Browse source]({{ src("examples/minimal") }})

-   :material-book-open-page-variant:{ .lg .middle } __RAG (MuSiQue)__

    ---

    Retrieval-Augmented Generation pipeline that preps a Wikipedia retriever via MCP and trains a MuSiQue QA agent with GRPO. Documented for historical reference (verified on Agent-lightning v0.1.x).

    [:octicons-repo-24: Browse source]({{ src("examples/rag") }})

-   :material-database:{ .lg .middle } __Spider SQL agent__

    ---

    LangGraph-powered text-to-SQL workflow for the Spider benchmark, combining LangChain tooling with Agent-lightning rollouts; follow along with the [how-to for training SQL agents]({{ src("docs/how-to/train-sql-agent.md") }}).

    [:octicons-repo-24: Browse source]({{ src("examples/spider") }})

-   :material-thought-bubble:{ .lg .middle } __Tinker integration__

    ---

    Adapter package ([`agl_tinker`]({{ src("examples/tinker/agl_tinker") }})) with Tinker plus sample CrewAI/OpenAI agents that feed Agent-lightning traces into Tinker’s reinforcement-learning backend for both toy and 20-Questions-style workflows.

    [:octicons-repo-24: Browse source]({{ src("examples/tinker") }})

-   :material-fast-forward:{ .lg .middle } __Unsloth SFT__

    ---

    Supervised fine-tuning loop that ranks math-agent rollouts, fine-tunes with Unsloth’s 4-bit LoRA stack, and mirrors the [Fine-tune with Unsloth recipe]({{ src("docs/how-to/unsloth-sft.md") }}).

    [:octicons-repo-24: Browse source]({{ src("examples/unsloth") }})

</div>


## Links discovered
- [Contributing](https://github.com/microsoft/agent-lightning/blob/main/docs/community/contributing.md)

--- docs/tutorials/parallelize.md ---
# Scaling out Agent-lightning

Agent-lightning splits training into an **algorithm bundle** and a **runner bundle** that exchange work through the [`LightningStore`][agentlightning.LightningStore]. This tutorial shows how to increase rollout throughput, place bundles across processes or machines, and keep the algorithm side scalable with external frameworks.

## Parallelizing Rollouts with [`Trainer`][agentlightning.Trainer]

Before we dive into the details of the bundles and execution strategies, let's first revisit how to parallelize rollouts with [`Trainer`][agentlightning.Trainer].

[`Trainer`][agentlightning.Trainer] is the quickest way to dial up parallelism. Even when `n_runners = 1`, calling [`Trainer.fit`][agentlightning.Trainer.fit] runs the algorithm and runners in parallel. The algorithm enqueues rollouts; runners dequeue them and execute your [`LitAgent`][agentlightning.LitAgent], and the algorithm collects spans via its [`Adapter`][agentlightning.Adapter] before scheduling the next batch.

!!! note

    One of the most important features of [`Trainer`][agentlightning.Trainer] is the ability to abort things gracefully. For example, if you press `Ctrl+C` in the terminal, the algorithm will abort and the runners will stop executing. If the algorithm crashes, the runners will also stop executing.

Increase throughput by setting `n_runners` when constructing the trainer. The following example comes from [train_calc_agent.py]({{ src("examples/calc_x/train_calc_agent.py") }}). Since backend LLMs usually use techniques like [continuous batching](https://docs.vllm.ai/en/latest/) to increase throughput, you do not have to worry about overwhelming the backend with too many requests.

```python
import agentlightning as agl
from datasets import Dataset as HFDataset
from calc_agent import calc_agent

train_dataset = HFDataset.from_parquet("data/train.parquet").to_list()
val_dataset = HFDataset.from_parquet("data/test.parquet").to_list()

algorithm = agl.VERL(verl_config)

trainer = agl.Trainer(
    algorithm=algorithm,
    n_runners=8,  # launch eight rollout workers
    tracer=agl.OtelTracer(),
    adapter=agl.LlmProxyTraceToTriplet(),
)

trainer.fit(calc_agent, train_dataset=train_dataset, val_dataset=val_dataset)
```

In [`Trainer`][agentlightning.Trainer], there are multiple other initialization parameters that you can use to customize the training process. For example, you can use `max_rollouts` to keep smoke tests short. Pass a concrete [`LightningStore`][agentlightning.LightningStore] instance when you need persistence or want to share the queue across multiple scripts.

!!! tip

    Before scaling out, run [`Trainer.dev()`][agentlightning.Trainer.dev] with `n_runners=1` to verify the rollout logic and spans without burning GPU hours.

## Bundles and Execution Strategies

When [`Trainer`][agentlightning.Trainer] starts, it packages its configuration into two callable **bundles**:

![Illustration of bundles and execution strategies](../assets/execution-bundles.svg)

The **algorithm bundle** wraps your [`Algorithm`][agentlightning.Algorithm], adapter, and any LLM proxy into a single callable that can be aborted via a signal event.

```python
async def algorithm_bundle(store: LightningStore, event: ExecutionEvent) -> None:
    ...
```

The **runner bundle** wraps the [`Runner`][agentlightning.Runner], tracer, hooks, and agent into a single callable that can be aborted via a signal event. Unlike the algorithm bundle, the runner bundle is expected to be replicated.

```python
async def runner_bundle(store: LightningStore, worker_id: int, event: ExecutionEvent) -> None:
    ...
```

An **execution strategy** then decides where those bundles are placed (threads vs processes vs multiple machines), how many runner replicas to launch, and how lifecycle events such as shutdown are coordinated.

By default, the trainer builds an [`InMemoryLightningStore`][agentlightning.InMemoryLightningStore] if you do not provide one. Because that store has no locking or cross-process transport, the execution strategy is the component that wraps it in thread-safe or HTTP-safe facades ([`LightningStoreThreaded`][agentlightning.LightningStoreThreaded], [`LightningStoreServer`][agentlightning.LightningStoreServer]) before handing it to bundles. For a deeper look at these facades, see [Understanding the Store](../deep-dive/store.md) and [Birds' Eye View](../deep-dive/birds-eye-view.md).

Agent-lightning provides two built-in execution strategies: [`SharedMemoryExecutionStrategy`][agentlightning.SharedMemoryExecutionStrategy] and [`ClientServerExecutionStrategy`][agentlightning.ClientServerExecutionStrategy]. You can pass a string alias, a configuration dictionary, or a pre-built strategy instance:

```python
import agentlightning as agl

algorithm = agl.Baseline()

# Short alias for the shared-memory strategy.
# Because the runner lives on the main thread in this mode,
# n_runners must be 1 unless you move the algorithm to the main thread.
trainer = agl.Trainer(algorithm=algorithm, n_runners=1, strategy="shm")

# Dict with overrides; keep the algorithm on the main thread so multiple runner threads can spawn.
# Specifying `n_runners` inside strategy is equivalent to passing `n_runners` to the trainer.
trainer = agl.Trainer(
    algorithm=algorithm,
    strategy={
        "type": "shm",
        "n_runners": 8,
        "main_thread": "algorithm",
    },
)

# Pass an existing strategy instance – Trainer respects the strategy's own `n_runners`.
strategy = agl.SharedMemoryExecutionStrategy(main_thread="algorithm", n_runners=4)
trainer = agl.Trainer(algorithm=algorithm, strategy=strategy)
```

If you omit the strategy, the trainer defaults to `ClientServerExecutionStrategy(n_runners=trainer.n_runners)`. You can still re-specify the client-server strategy through aliases or configuration to tweak ports and other settings:

```python
trainer = agl.Trainer(
    algorithm=algorithm,
    n_runners=8,
    strategy={"type": "cs", "server_port": 9999},
)
```

Environment variables give you another layer of control. For example:

```python
import os

os.environ["AGL_SERVER_PORT"] = "10000"
os.environ["AGL_CURRENT_ROLE"] = "algorithm"
os.environ["AGL_MANAGED_STORE"] = "0"

trainer = agl.Trainer(algorithm=algorithm, n_runners=8, strategy="cs")
```

The resulting [`ClientServerExecutionStrategy`][agentlightning.ClientServerExecutionStrategy] picks up the port, role, and managed-store flag from the environment.

!!! tip

    The same configuration patterns apply to other trainer components. For example,
    ```python
    trainer = agl.Trainer(algorithm=algorithm, tracer=agl.OtelTracer())
    ```
    wires in a custom tracer, while
    ```python
    trainer = agl.Trainer(algorithm=algorithm, adapter="agentlightning.adapter.TraceToMessages")
    ```
    swaps in a different adapter. Passing a dict lets you tweak the init parameters of defaults without naming the class explicitly:

    ```python
    trainer = agl.Trainer(
        algorithm=algorithm,
        adapter={"agent_match": "plan_agent", "repair_hierarchy": False},
    )
    ```

The next sections walk through the two built-in strategies and how they affect placement and store access.

## Client-server Architecture

The default [`ClientServerExecutionStrategy`][agentlightning.ClientServerExecutionStrategy] starts a [`LightningStoreServer`][agentlightning.LightningStoreServer] alongside the algorithm and spawns runner processes that talk to it through [`LightningStoreClient`][agentlightning.LightningStoreClient]. All runners share the HTTP endpoint, so the queue and spans stay consistent across processes or machines.

If you simply instantiate [`Trainer`][agentlightning.Trainer] (as above), it will send the algorithm bundle and runner bundle to [`ClientServerExecutionStrategy`][agentlightning.ClientServerExecutionStrategy], which will then:

1. Launch \(N+1\) processes: \(N\) runner processes and 1 algorithm process (one of them could live in the main process).
2. The algorithm process will take the store received from [`Trainer`][agentlightning.Trainer], wrap it in a [`LightningStoreServer`][agentlightning.LightningStoreServer], and start serving it over HTTP.
3. The runner processes discard the store and create a new store, which is a client that connects to the algorithm process through [`LightningStoreClient`][agentlightning.LightningStoreClient], and start executing the runner bundle.
4. The strategy automatically escalates shutdown (cooperative stop → `SIGINT` → `terminate()` → `kill()`) so long-running runners do not linger.

You can override server placement or ports, and whether to automatically wrap the store, through constructor arguments or environment variables:

```python
trainer = agl.Trainer(
    algorithm=algorithm,
    n_runners=1,
    strategy={
        "type": "cs",
        "server_host": "0.0.0.0",
        "server_port": 9999,
        "main_process": "runner",
    },
)
```

Set `AGL_SERVER_HOST` and `AGL_SERVER_PORT` if you prefer environment-based configuration. You can also use `AGL_MANAGED_STORE` if you do not want the execution strategy to wrap the store for you. An example is shown in [Debugging with External Store][debug-with-external-store].

Algorithms sometimes require heterogeneous computation resources, such as GPU accelerators, while runners sometimes require a specific environment to run because many agent frameworks are fragile in their dependencies. A role-based launch pattern helps you place the algorithm on a dedicated machine with more GPU memory, while runners can live on another machine with more flexible dependencies. This is possible via `AGL_CURRENT_ROLE="algorithm"` or `AGL_CURRENT_ROLE="runner"` environment variables. When running on different machines, you also need to set `AGL_SERVER_HOST` and `AGL_SERVER_PORT` to the IP address and port of the algorithm machine. You might recognize that this convention is very similar to `MASTER_ADDR` and `MASTER_PORT` in [PyTorch distributed training](https://docs.pytorch.org/docs/stable/notes/ddp.html).

### Launching Algorithm and Runner Roles on Separate Machines

When you want to stretch the algorithm onto a GPU-rich machine and keep rollout workers close to the data source (or on machines with a more permissive dependency stack), launch the same training script in different terminals with role-specific environment variables. The client–server strategy will route each process to the right side of the queue as long as they share the same `AGL_SERVER_HOST`/`AGL_SERVER_PORT` pair.

**1. Pick an address and port for the store.** Decide which machine will host the algorithm. Choose a TCP port that can be reached by the runner machines (for example, open it in your firewall configuration). In this example we will use `10.0.0.4:4747`.

**2. Start the algorithm process.** On the machine that should run the algorithm, expose the store by binding to all network interfaces and mark the role as `algorithm`.

```bash
export AGL_SERVER_HOST=0.0.0.0
export AGL_SERVER_PORT=4747
export AGL_CURRENT_ROLE=algorithm

python train_calc_agent.py
```

Leaving `AGL_MANAGED_STORE` unset (or setting it to `1`) lets the strategy create the [`LightningStoreServer`][agentlightning.LightningStoreServer] for you. Otherwise, you can use the method in the previous section to create a store on your own.

**3. Start rollout workers on remote machines.** Every runner machine should point to the algorithm host and declare itself as the `runner` role. You can start multiple processes per machine or repeat the command on additional hosts.

```bash
export AGL_SERVER_HOST=10.0.0.4
export AGL_SERVER_PORT=4747
export AGL_CURRENT_ROLE=runner
python train_calc_agent.py --n-runners 4
```

The runner process automatically connects via [`LightningStoreClient`][agentlightning.LightningStoreClient]. Adjust `--n-runners` to spawn the desired number of worker processes on that machine.

**4. Scale out as needed.** Repeat step 3 on as many machines as you need. When you are done, stop the algorithm process. However, since the runners are on different machines, the strategy WILL NOT send a cooperative stop signal to the connected runners. So you need to kill the runners on your own.

This role-based launch mirrors what [`Trainer.fit`][agentlightning.Trainer.fit] does inside a single machine while letting you spread work across a fleet. Because every process shares the same training script, you keep a single source of truth for dataset loading, adapters, and tracers, but you can tune compute resources independently for the algorithm and rollout workers.

### Shared-memory Strategy

[`SharedMemoryExecutionStrategy`][agentlightning.SharedMemoryExecutionStrategy] keeps everything inside one process. The runner runs on the main thread (by default) while the algorithm lives on a Python thread guarded by [`LightningStoreThreaded`][agentlightning.LightningStoreThreaded].

Use it when you want easier debugging with shared breakpoints and no serialization overhead, or minimal startup time for unit tests. It's not a good choice for many algorithms that require heavy model training because [`LightningStoreThreaded`][agentlightning.LightningStoreThreaded] does not work for multiprocessing. Using it with multiprocessing algorithms will lead to undefined behavior.

Sample configuration:

```python
trainer = agl.Trainer(
    algorithm=algorithm,
    strategy="shm",
)
```

You can further customize the init parameters of [`SharedMemoryExecutionStrategy`][agentlightning.SharedMemoryExecutionStrategy]. With `main_thread="runner"`, the runner occupies the main thread and `n_runners` must be `1`. The strategy respects `AGL_MANAGED_STORE`; set it to `0` to opt out of the `LightningStoreThreaded` wrapper.

## Parallelizing Algorithms

Runner parallelism scales rollout throughput, but the algorithm loop remains a single-process loop inside the execution strategy. We understand that many algorithms have parallelization built in, but that's outside the parallelization scope of Agent-lightning.

Agent-lightning strives to make algorithms’ own parallelization work well under our execution strategies. The biggest challenge turns out to come from the store. For example, [`VERL`][agentlightning.algorithm.verl.VERL] uses [Ray](https://www.ray.io/) and launches [FSDP](https://docs.pytorch.org/tutorials/intermediate/FSDP_tutorial.html) and [vLLM](https://vllm.ai/) components internally. [`ClientServerExecutionStrategy`][agentlightning.ClientServerExecutionStrategy] has to make sure that the server is not simultaneously serving in multiple processes or Ray workers, and that there is only one single authoritative source of truth for all subprocesses to connect to. Subprocesses connect to the store via a small [`LightningStoreClient`][agentlightning.LightningStoreClient] bundled within [`LightningStoreServer`][agentlightning.LightningStoreServer].

!!! note

    The [birds' eye view][birds-eye-view-client-server-strategy] illustrates how adapters, proxies, and stores interact when the algorithm spawns additional workers. Use that diagram as a checklist when introducing new distributed components.

## Parallelizing [`LightningStore`][agentlightning.LightningStore]

By default, Agent-lightning persists rollouts and spans in an in-memory store. [`Trainer.fit`][agentlightning.Trainer.fit] spins it up automatically, or you can launch it yourself via the [`agl store` command](../reference/cli.md). [`InMemoryLightningStore`][agentlightning.InMemoryLightningStore] keeps all state inside the current process, which makes local iteration fast but introduces two production constraints:

1. Spans are evicted once the process crosses its memory cap, so long runs risk data loss unless the host has abundant RAM.
2. Although the store is well optimized via asynchronous programming, the store lives in a single process and remains bound by the GIL, preventing it from saturating multi-core machines.

!!! note "General note for all server-client stores"

    If your algorithm and runners communicate through HTTP protocol (which should be the default for 99% of the cases), you need to ensure the file limit is sufficiently large to avoid the "Too many open files" error. You can set the file limit by running the following command:

    ```bash
    ulimit -n 100000
    ```

For resilient runs, switch to a persistent backend such as [`MongoLightningStore`][agentlightning.store.mongo.MongoLightningStore], which writes data to MongoDB instead of local RAM. Agent-lightning relies on [pymongo](https://pymongo.readthedocs.io/en/stable/) to interact with MongoDB, which can be installed via:

```bash
pip install agentlightning[mongo]
```

To use the MongoDB store, you need to pass the MongoDB URI to the store constructor. The URI should be in the format of `mongodb://<host>:<port>/<database>?replicaSet=<replicaSet>`.

```python
from agentlightning.store.mongo import MongoLightningStore

trainer = agl.Trainer(
    algorithm=algorithm,
    store=MongoLightningStore(mongo_uri="mongodb://localhost:27017/?replicaSet=rs0"),
)
```

!!! tip "Setting up MongoDB"

    MongoDB is a popular document-oriented database. Before running Agent-lightning with [`MongoLightningStore`][agentlightning.store.mongo.MongoLightningStore], make sure that you've already had a MongoDB instance running. Setting up can be conveniently done via Docker Compose via [compose.mongo.yml]({{ src("docker/compose.mongo.yml") }}). Unless targeting serious production use, we recommend creating the data folders and setting them to `777` permission to avoid permission issues.

    ```bash
    mkdir -p data/mongo-host
    chmod 777 data/mongo-host
    docker compose -f compose.mongo.yml up -d
    ```

    Alternatively, you can also install MongoDB manually following the [official documentation](https://www.mongodb.com/docs/manual/installation/). If you installed MongoDB manually, an important note is that you need to ensure that the MongoDB instance has enabled replica set feature, since Agent-lightning uses the transactional operations internally. The simplest approach is to use the following script (executed in the MongoDB shell) to initialize the replica set:

    ```javascript
    rs.initiate({
      _id: "rs0",
      members: [{ _id: 0, host: "localhost:27017" }],
    });
    ```

To scale out further, launch the store server via [`agl store --backend mongo`](../reference/cli.md) (see [Debugging with External Store][debug-with-external-store]). The CLI accepts `--n-workers`, which starts the server under `gunicorn` with multiple worker processes so concurrent runners can push and pull at higher throughput. This option applies only to persistent backends; an in-memory store, on the other hand, cannot be sharded across workers because its state lives inside one process.

!!! note

    The `--n-workers` here is the number of worker processes for the store server, NOT related to the number of rollout runners.

## Increasing Throughput of LLM Proxy

Agent-lightning includes an optional [`LLMProxy`][agentlightning.LLMProxy] that wraps [LiteLLM](https://docs.litellm.ai/) to provide a unified OpenAI-compatible endpoint for your agents. When rollout throughput increases, the proxy can become a bottleneck. You can scale it out using the same pattern as the store server.

To increase proxy throughput, pass `num_workers` when constructing the proxy:

```python
import agentlightning as agl

proxy = agl.LLMProxy(
    port=4000,
    launch_mode="mp",  # multiprocessing mode
    num_workers=4,  # four gunicorn workers handle concurrent requests
)
```

You can also configure the proxy through [`Trainer`][agentlightning.Trainer]:

```python
trainer = agl.Trainer(
    algorithm=algorithm,
    n_runners=8,  # The runners here is the rollout runners, not related to LLM proxy replicas
    llm_proxy={"port": 4000, "num_workers": 4},  # launch mode is actually mp by default
)
```

When `num_workers > 1`, the launcher starts gunicorn with the specified number of worker processes. Each worker runs its own event loop, allowing the proxy to handle many concurrent LLM requests without being blocked by Python's GIL.

!!! tip

    When using `mp` launch mode, [`LLMProxy`][agentlightning.LLMProxy] will start the server in a separate process. To make sure the proxy is still accessing the same store as the main process, you need to set the store to be [zero-copy compatible][store-capabilities], which means, either the store is a native zero-copy store like [`MongoLightningStore`][agentlightning.store.mongo.MongoLightningStore] or the store is wrapped via [`LightningStoreServer`][agentlightning.LightningStoreServer] or [`LightningStoreClient`][agentlightning.LightningStoreClient].

!!! note "Shared Server Infrastructure"

    Both [`LightningStoreServer`][agentlightning.LightningStoreServer] and [`LLMProxy`][agentlightning.LLMProxy] rely on a common utility called [`PythonServerLauncherArgs`][agentlightning.utils.server_launcher.PythonServerLauncherArgs]. This dataclass captures the settings needed to launch a FastAPI application:

    ```python
    from agentlightning.utils import PythonServerLauncherArgs

    args = PythonServerLauncherArgs(
        port=8000,
        host="0.0.0.0",
        n_workers=4,          # spawn 4 gunicorn workers
        launch_mode="thread", # or "mp" for multiprocessing, "asyncio" for in-loop
    )
    ```

    Under the hood, [`PythonServerLauncher`][agentlightning.utils.server_launcher.PythonServerLauncher] reads these arguments and chooses between uvicorn (single worker) and gunicorn (multiple workers) automatically.


## Links discovered
- [continuous batching](https://docs.vllm.ai/en/latest/)
- [Illustration of bundles and execution strategies](https://github.com/microsoft/agent-lightning/blob/main/docs/assets/execution-bundles.svg)
- [Understanding the Store](https://github.com/microsoft/agent-lightning/blob/main/docs/deep-dive/store.md)
- [Birds' Eye View](https://github.com/microsoft/agent-lightning/blob/main/docs/deep-dive/birds-eye-view.md)
- [PyTorch distributed training](https://docs.pytorch.org/docs/stable/notes/ddp.html)
- [Ray](https://www.ray.io/)
- [FSDP](https://docs.pytorch.org/tutorials/intermediate/FSDP_tutorial.html)
- [vLLM](https://vllm.ai/)
- [`agl store` command](https://github.com/microsoft/agent-lightning/blob/main/docs/reference/cli.md)
- [pymongo](https://pymongo.readthedocs.io/en/stable/)
- [official documentation](https://www.mongodb.com/docs/manual/installation/)
- [`agl store --backend mongo`](https://github.com/microsoft/agent-lightning/blob/main/docs/reference/cli.md)
- [LiteLLM](https://docs.litellm.ai/)

--- docs/tutorials/traces.md ---
# Working with Traces

Tracing is the secret capability that lets Agent-lightning train almost any agent without rewriting its core logic. The idea was born in observability tooling inside LLMOps workflows and, in Agent-lightning, evolved into a first-class primitive inside the learning loop. Beyond helping you understand what happened inside a rollout, traces provide reward spans and other learning signals that power reinforcement learning and fine-tuning algorithms.

![OpenTelemetry spans](../assets/opentelemetry-trace.jpg)

Agent-lightning stores every recorded operation as a [`Span`][agentlightning.Span] inside a [`LightningStore`][agentlightning.LightningStore]. The naming comes from [OpenTelemetry spans](https://opentelemetry.io/docs/concepts/signals/traces/), shown in the screenshot above. A span can represent an LLM call, a tool invocation, a graph edge, an explicit reward emission, or an arbitrary Python code block. Spans form a tree where parent spans describe higher-level steps and children record the detailed work. The sections below walk through how spans are produced and how to interpret them once they reach the store.

## Writing Spans

Most [`Runner`][agentlightning.Runner] implementations wire a [`Tracer`][agentlightning.Tracer] into the agent’s lifecycle. The tracer is responsible for installing instrumentation, buffering OpenTelemetry spans, and committing them to the [`LightningStore`][agentlightning.LightningStore]. When a runner executes a rollout, it allocates a store-backed tracing context:

```python
async with tracer.trace_context(
    name="my-rollout",
    store=store,
    rollout_id=rollout.rollout_id,
    attempt_id=attempt.attempt_id,
):
    await run_agent_logic()
```

The context manager then requests sequence numbers from the store, converts OpenTelemetry spans into [`Span`][agentlightning.Span] objects, and persists them in the middle or at the end of the attempt, depending on the tracer implementation. Agent-lightning ships two tracers out of the box; both rely on [OpenTelemetry Traces](https://opentelemetry.io/docs/concepts/signals/traces/) and ignore metrics or logs.

!!! tip "What's instrumentation?"

    In simple terms, *instrumentation* means adding "patches" or hooks inside your code so you can observe what it’s doing while it runs. Think of it like putting flight recorders in an airplane — instrumentation records key actions, inputs, outputs, and timings without changing how the code behaves. In Agent-lightning tracers, this instrumentation automatically creates spans (small, structured records of work) that show what each part of an agent did, how long it took, and how different steps connect together.

### AgentOps Tracer

[`AgentOpsTracer`][agentlightning.AgentOpsTracer] will be the default tracer when [`Trainer`][agentlightning.Trainer] is used but no tracer is explicitly specified. It bootstraps the [AgentOps SDK](https://www.agentops.ai/) locally, installs the supplied instrumentation hooks (LangChain, LangGraph, LiteLLM, FastAPI, and others) provided by the [AgentOps Python SDK](https://github.com/AgentOps-AI/agentops), and forwards everything through a local OpenTelemetry [`TracerProvider`](https://opentelemetry.io/docs/specs/otel/trace/api/). [`AgentOpsTracer`][agentlightning.AgentOpsTracer] never calls the hosted AgentOps service; instead, it attaches a `LightningSpanProcessor` implemented by the Agent-lightning team so that spans are captured and shipped straight into the store.

Because it shares the AgentOps instrumentation surface, any framework supported by AgentOps automatically gains tracing in Agent-lightning. We layer additional hooks on top of AgentOps to capture features that the SDK misses today:

1. Certain providers emit extra metadata — for example, [token IDs returned by vLLM](../deep-dive/serving-llm.md) — that are not recorded by the stock SDK. We augment those spans with the missing payloads.
2. AgentOps constructs parent-child relationships on a best-effort basis, but mixed instrumentation (for example, OpenAI Agent SDK alongside direct OpenAI Chat Completion calls) can leave segments disconnected. Our implementation (actually implemented in the [`TracerTraceToTriplet`][agentlightning.TracerTraceToTriplet] adapter) repairs those relationships when the hierarchy can be inferred from rollout context.
3. Some versions of downstream frameworks simply do not emit spans for critical events (LangGraph node entrances are a common example). The tracer installs lightweight shims so those spans appear consistently.

If a vendor integration behaves unexpectedly, users are encouraged to combine the tracer with [Hooks](./debug.md) to inspect the raw spans or diagnostics, and/or implement a specialized tracer for the framework in question.

### OpenTelemetry Tracer

[`OtelTracer`][agentlightning.OtelTracer] is a minimal implementation that initializes a vanilla [`TracerProvider`](https://opentelemetry.io/docs/specs/otel/trace/api/) and gives you direct control over span creation through the standard `opentelemetry.trace` API. Use it when you already have explicit instrumentation in your agent, when the AgentOps SDK does not support your framework, or when you want to emit custom spans from business logic.

!!! note

    [Microsoft Agent Framework](https://github.com/microsoft/agent-framework) is a typical example with built-in OpenTelemetry support. Once you set `OBSERVABILITY_SETTINGS.enable_otel = True`, the framework will automatically emit OpenTelemetry spans, and [`OtelTracer`][agentlightning.OtelTracer] will be able to capture them. No extra instrumentation is needed.

Inside your agent you can call `opentelemetry.trace.get_trace_provider().get_tracer("my-agent")` and use that tracer to [create spans](https://opentelemetry.io/docs/languages/python/cookbook/) exactly as you would in any OpenTelemetry application. The Lightning span processor attached by [`OtelTracer`][agentlightning.OtelTracer] guarantees that every span is sequenced, converted, and written to the store. The same applies for emitted rewards ([`emit_reward`][agentlightning.emit_reward]) and other emitter signals, which are just a special case of manually-created spans.

### Weave Tracer (Experimental)

[`WeaveTracer`][agentlightning.tracer.weave.WeaveTracer] is an experimental tracer that integrates with the [Weave Python SDK](https://docs.wandb.ai/weave). Use it as a substitute for [`AgentOpsTracer`][agentlightning.AgentOpsTracer] when the AgentOps SDK does not fit your environment.

The Weave SDK instruments LLM calls and agent libraries directly. Unlike [`AgentOpsTracer`][agentlightning.AgentOpsTracer], Weave does not rely on OpenTelemetry to export spans; it routes everything through a dedicated Weave Trace Server. Agent-lightning implements a custom Weave Trace Server so every call captured by the Weave SDK can be persisted to the [`LightningStore`][agentlightning.LightningStore].

!!! warning

    [`WeaveTracer`][agentlightning.tracer.weave.WeaveTracer] remains experimental and has not been tested as thoroughly as [`AgentOpsTracer`][agentlightning.AgentOpsTracer]. It may conflict with libraries that ship OpenTelemetry instrumentation by default (for example, LiteLLM-based LLM proxies). Use the tracer with caution and report any issues to the Agent-lightning team.

### LLM Proxy

Sometimes the runner can’t observe the agent directly — because it’s in another language or running remotely. [`LLMProxy`][agentlightning.LLMProxy] bridges that gap by instrumenting the server side of LLM calls. It wraps [LiteLLM](https://docs.litellm.ai/) and adds middleware that accepts prefixed routes like `/rollout/{rid}/attempt/{aid}/v1/chat/completions`. Before forwarding, the middleware rewrites the path to `/v1/chat/completions`, fetches a monotonic `sequence_id` from the `LightningStore`, injects `x-rollout-id`, `x-attempt-id`, and `x-sequence-id` into the request headers, and then forwards the request to the backend LLM endpoint.

LiteLLM produces OpenTelemetry spans for the request/response. A custom `LightningSpanExporter` reads the rollout/attempt/sequence identifiers from the recorded request headers and persists each span to the store. Because the `sequence_id` is allocated at the start of the request, traces stay in strict order even across machines with skewed clocks or asynchronous responses.

```mermaid
sequenceDiagram
    participant Agent
    participant Proxy as LLM Proxy
    participant Backend as LLM Backend
    participant Store as LightningStore

    Agent->>Proxy: POST /rollout/{rid}/attempt/{aid}/v1/chat/completions
    Proxy->>Store: get_next_span_sequence_id(rid, aid)
    Store-->>Proxy: sequence_id
    Proxy->>Backend: Forward /v1/chat/completions<br>(headers: rid, aid, sid)
    Backend-->>Proxy: Response (tokens, usage, token_ids)
    Proxy->>Store: Export OTEL spans (rid, aid, sequence_id)
    Proxy-->>Agent: OpenAI-compatible response
```

[`LLMProxy`][agentlightning.LLMProxy] actually provides more functionalities than just the middleware for tracing. Read [Serving LLM](../deep-dive/serving-llm.md) for more details.

[](){ #distributed-tracing }

!!! note "Distributed Tracing"

    Agent-lightning enforces deterministic span ordering by assigning a monotonic [`sequence_id`][agentlightning.Span.sequence_id] to every span within an attempt. Before calling [`LightningStore.add_span`][agentlightning.LightningStore.add_span] or [`LightningStore.add_otel_span`][agentlightning.LightningStore.add_otel_span], tracers are expected to call [`LightningStore.get_next_span_sequence_id`][agentlightning.LightningStore.get_next_span_sequence_id] to get the next sequence id. This removes clock skew and merges spans produced on different machines or threads. If you implement a custom tracer or exporter, make sure you do this (or respect the one provided in headers by components such as [`LLMProxy`][agentlightning.LLMProxy]); otherwise, adapters will struggle to properly reconstruct the execution tree.

### Custom Tracer

If none of the built-in tracers fit your environment, the first option to consider is to [return the spans](./write-agents.md) directly from your agent implementation. If that's not possible, or you want to support multiple agents in a unified effort, you can implement your own tracer by subclassing [`Tracer`][agentlightning.Tracer].

Custom tracers must implement at least [`trace_context`][agentlightning.Tracer.trace_context]. The [`trace_context`][agentlightning.Tracer.trace_context] coroutine should install or activate whatever instrumentation you need, then yield a span processor that ultimately adds spans to the store. You can reuse the `LightningSpanProcessor` if you produce OpenTelemetry `ReadableSpan` objects, or call [`LightningStore.add_span`][agentlightning.LightningStore.add_span] directly if you generate [`Span`][agentlightning.Span] instances yourself.

Advanced tracers often run auxiliary services (for example, starting a telemetry daemon or attaching to a container runtime) inside `init_worker` and tear them down in `teardown_worker`. The [`ParallelWorkerBase`][agentlightning.ParallelWorkerBase] lifecycle that `Tracer` inherits from ensures those hooks are executed in every runner subprocess.

## Reading Traces

Generally, there are two approaches to reading traces. When you only need a quick look, [`Tracer.get_last_trace`][agentlightning.Tracer.get_last_trace] returns the raw OpenTelemetry spans captured most recently. For historical analysis, use the [`LightningStore.query_spans`][agentlightning.LightningStore.query_spans] API, which yields normalized [`Span`][agentlightning.Span] objects keyed by rollout ID and attempt ID. Combine those queries with [`LightningStore.query_rollouts`][agentlightning.LightningStore.query_rollouts] to align spans with rollout status, retries, and timing information.

Spans arrive asynchronously, originate from different processes, and form hierarchies rather than simple lists. The attributes of each span are tedious and unfriendly to human readers. This combination makes raw traces time-consuming to inspect, especially when you only care about specific signals such as rewards, LLM prompts, responses, or tool outputs. Understanding how the store exposes traces and how adapters reshape them will save hours when debugging or training.

!!! note "Why traces can be difficult to read?"

    The trace tree for a single rollout typically mixes multiple abstraction layers: a planner span may contain several LLM spans, each of which contains tool execution spans that can themselves trigger nested agent invocations. There are also instrumentations at different levels. For example, when a request delegates to another library (e.g., from LangChain to OpenAI), two libraries might emit spans for the same request. At the top level, there could be concurrently running agents that may flush spans slightly out of order. Sorting by `sequence_id` restores the chronological view, but interpreting the tree requires additional context about parent-child relationships and rollout metadata.

### Adapter

[Adapters][agentlightning.Adapter] transform lists of spans into higher-level data structures that training algorithms can consume directly. Agent-lightning provides several adapters out of the box:

* [`TracerTraceToTriplet`][agentlightning.TracerTraceToTriplet] converts spans into `(prompt, response, reward)` triplets, which power reinforcement-learning algorithms such as [VERL](../algorithm-zoo/verl.md) and connect trace data to gradient updates.
* [`TraceToMessages`][agentlightning.TraceToMessages] rewrites spans into OpenAI chat message JSON suitable for supervised fine-tuning or evaluation harnesses.
* [`LlmProxyTraceToTriplet`][agentlightning.LlmProxyTraceToTriplet] mirrors [`TracerTraceToTriplet`][agentlightning.TracerTraceToTriplet] but understands spans emitted by [LLMProxy][agentlightning.LLMProxy]. It is experimental and might be merged with [`TracerTraceToTriplet`][agentlightning.TracerTraceToTriplet] in the future.

Adapters are regular Python callable instances, so you can plug them into [`Trainer`][agentlightning.Trainer] via the `adapter` argument, or call them manually during exploration. When used in [`Trainer`][agentlightning.Trainer], adapters are bundled into the [`Algorithm`][agentlightning.Algorithm] before the algorithm runs, through the [`Algorithm.set_adapter`][agentlightning.Algorithm.set_adapter] method.

You can also customize an [`Adapter`][agentlightning.Adapter] by extending the implementations above or subclassing the base class. If you need a bespoke format, subclass [`TraceAdapter`][agentlightning.TraceAdapter] (for store spans) or [`OtelTraceAdapter`][agentlightning.OtelTraceAdapter] (for raw OpenTelemetry spans) and implement `adapt` (these two classes can usually share the same implementation).

### Reading Rewards

Rewards are recorded as dedicated spans named [`agentlightning.annotation`][agentlightning.semconv.AGL_ANNOTATION]. Emitting a reward through [`emit_reward`][agentlightning.emit_reward] or [`emit_annotation`][agentlightning.emit_annotation] ensures the value is stored in the span’s `attributes`. To audit rewards, fetch spans from the store and use the helper utilities in [`agentlightning.emitter`](../reference/agent.md):

```python
from agentlightning.emitter import find_final_reward

spans = await store.query_spans(rollout_id)
reward = find_final_reward(spans)
print(f"Final reward: {reward}")
```

[`find_reward_spans`][agentlightning.find_reward_spans] returns every reward span so you can visualize intermediate shaping signals, while [`find_final_reward`][agentlightning.find_final_reward] extracts the last non-null reward per attempt. While these helpers are convenient, they may not help you fully understand the chronological or hierarchical relationships between reward spans and other spans. Using an [`Adapter`][agentlightning.Adapter] — especially the same one used in the algorithm you’re working with — remains the recommended way to inspect your generated spans.


## Links discovered
- [OpenTelemetry spans](https://github.com/microsoft/agent-lightning/blob/main/docs/assets/opentelemetry-trace.jpg)
- [OpenTelemetry spans](https://opentelemetry.io/docs/concepts/signals/traces/)
- [OpenTelemetry Traces](https://opentelemetry.io/docs/concepts/signals/traces/)
- [AgentOps SDK](https://www.agentops.ai/)
- [AgentOps Python SDK](https://github.com/AgentOps-AI/agentops)
- [`TracerProvider`](https://opentelemetry.io/docs/specs/otel/trace/api/)
- [token IDs returned by vLLM](https://github.com/microsoft/agent-lightning/blob/main/docs/deep-dive/serving-llm.md)
- [Hooks](https://github.com/microsoft/agent-lightning/blob/main/docs/tutorials/debug.md)
- [Microsoft Agent Framework](https://github.com/microsoft/agent-framework)
- [create spans](https://opentelemetry.io/docs/languages/python/cookbook/)
- [Weave Python SDK](https://docs.wandb.ai/weave)
- [LiteLLM](https://docs.litellm.ai/)
- [Serving LLM](https://github.com/microsoft/agent-lightning/blob/main/docs/deep-dive/serving-llm.md)
- [return the spans](https://github.com/microsoft/agent-lightning/blob/main/docs/tutorials/write-agents.md)
- [VERL](https://github.com/microsoft/agent-lightning/blob/main/docs/algorithm-zoo/verl.md)
- [`agentlightning.emitter`](https://github.com/microsoft/agent-lightning/blob/main/docs/reference/agent.md)

--- docs/how-to/train-first-agent.md ---
# Train the First Agent with Agent-lightning

Welcome! This tutorial is your first step into making AI agents smarter using the **Agent-lightning** framework. We'll show you how to take a simple agent and automatically improve its performance through a process called [**Automatic Prompt Optimization (APO)**](../algorithm-zoo/apo.md).

The main goal of Agent-lightning is to provide a structured way to **train your agents**. Just like you train a machine learning model on data, you can train an agent on a task dataset. This could involve using Reinforcement Learning (RL) to teach it new behaviors or, as we'll do today, optimizing its prompts to make it more accurate and reliable.

!!! tip

    You can open the sample code [room_selector_apo.py]({{ src("examples/apo/room_selector_apo.py") }}) and [room_selector.py]({{ src("examples/apo/room_selector.py") }}) as you go through this tutorial.

## Our Example: The Room Selector Agent

Today, we'll work with an agent whose job is to book a meeting room. It's a common but tricky task with multiple constraints.

Here's how the agent works:

- **Input:** It receives a task with specific requirements, like "`Find a room for 4 people at 10:00 AM with a whiteboard.`"
- **Action:** The agent uses a Large Language Model (LLM) to understand the request. It can also use tools, which are pre-defined functions it can call, to get more information, such as checking room availability in an external database.
- **Output:** Its final decision is the ID of the best room it found, like "`A103`".
- **Reward:** After the agent makes its choice, a separate "grader" function scores its performance on a scale of 0 to 1. This score is called its **reward**. A perfect choice gets a 1.0, while a wrong one gets a 0.0.

The agent's logic is sound, but its performance heavily depends on its initial prompt. A poorly worded prompt will confuse the LLM, leading to bad decisions. Our goal is to use Agent-lightning to find the best possible prompt automatically.

!!! tip "A Closer Look at the Agent's Logic"

    Modern LLMs can do more than just generate text; they can decide to call functions you provide. This is often called tool use or function calling. Our agent uses this capability to make informed decisions. If you're new to this concept, you can read more about it in [OpenAI's documentation](https://platform.openai.com/docs/guides/function-calling).

    Here is a sketch of the agent's logic, adhering closely to the OpenAI API:

    ```python
    # Pseudo-code for the Room Selector agent

    import openai
    import json

    def room_selector_agent(task, prompt):
        client = openai.OpenAI()
        messages = [{"role": "user", "content": prompt.format(**task)}]
        tools = [ ... ] # Tool definition for the LLM

        # 1. First LLM call to decide if a tool is needed.
        response = client.chat.completions.create(
            model="gpt-5-mini",
            messages=messages,
            tools=tools,
            tool_choice="auto",
        )
        response_message = response.choices[0].message
        tool_calls = response_message.tool_calls

        # 2. Check if the LLM wants to use a tool.
        if tool_calls:
            messages.append(response_message) # Append assistant's reply

            # 3. Execute the tool and get the real-world data.
            for tool_call in tool_calls:
                function_name = tool_call.function.name
                if function_name == "get_rooms_and_availability":
                    function_args = json.loads(tool_call.function.arguments)
                    # Query the local room database
                    function_response = get_rooms_and_availability(
                        date=function_args.get("date"),
                        time_str=function_args.get("time"),
                        duration_min=function_args.get("duration_min"),
                    )
                    messages.append({
                        "tool_call_id": tool_call.id,
                        "role": "tool",
                        "name": function_name,
                        "content": json.dumps(function_response),
                    })

            # 4. Second LLM call with the tool's output to get a final choice.
            second_response = client.chat.completions.create(
                model="gpt-5-mini",
                messages=messages,
            )
            final_choice = second_response.choices[0].message.content
        else:
            final_choice = response_message.content

        # 5. Grade the final choice to get a reward.
        reward = grade_the_choice(final_choice, task["expected_choice"])
        return reward
    ```

In Agent-lightning, you wrap this logic in a Python function marked with the [`@rollout`][agentlightning.rollout] decorator, so that the agent can be managed and tuned by Agent-lightning's runner and trainer. The `prompt_template` that the APO algorithm tunes is passed in as an argument:

```python
import agentlightning as agl

@agl.rollout
def room_selector(task: RoomSelectionTask, prompt_template: agl.PromptTemplate) -> float:
    # ... agent logic using the prompt_template ...

    # The final reward is determined by a grader function
    reward = room_selection_grader(client, final_message, task["expected_choice"])
    return reward
```

## Core Concepts: Tasks, Rollouts, Spans, and Prompt Templates

To understand how Agent-lightning works, you need to know these key terms.

### Task

A task is a specific input or problem statement given to the agent. It defines what the agent needs to accomplish.

!!! example "Analogy: Task"

    If the agent is a chef, a task is the recipe request: "Bake a chocolate cake."

### Rollout

A rollout is a single, complete execution of an agent attempting to solve a given **task**. It's the entire story from receiving the task to producing a final result and receiving a reward. A rollout captures a full trace of the agent's execution.

!!! example "Analogy: Rollout"

    A rollout is one full attempt by the chef to bake the chocolate cake, from gathering ingredients to the final taste test.

### Span

A span represents a single unit of work or an operation within a **rollout**. Spans are the building blocks of a trace. They have a start and end time and contain details about the specific operation, like an LLM call, a tool execution, or a reward calculation. For a more precise definition, see the [OpenTelemetry documentation](https://opentelemetry.io/docs/concepts/signals/traces/).

!!! example "Analogy: Span"

    If the rollout is "baking a cake," a span could be "preheating the oven," "mixing flour and sugar," or "adding frosting." Each is a distinct step or unit of work.

The picture below from [ADK](https://google.github.io/adk-docs/observability/cloud-trace/) shows a typical rollout, where each rectangle in the waterfall visualizes a span. As can be seen in the visualization, spans can be sequential, parallel or nested among each other. In other frameworks, the terminology might be slightly different. Agent-lightning follows the terminologies used by OpenTelemetry to avoid confusion.

![AgentOps Waterfall Visualization](../assets/agentops-waterfall-visualization.jpg)

### Prompt Template

A prompt template is a reusable instruction for the agent, often containing placeholders that can be filled in with specific details from a task. It is a key **"resource"** that the algorithm learns and improves over time.

!!! example "Analogy: Resource (Prompt Template)"

    If the task is the recipe request, the prompt template is the master recipe card that the chef follows. The algorithm's job is to edit this recipe card to make the instructions clearer and the final dish better.

## The Training Loop: How the Magic Happens

Training in Agent-lightning revolves around a clear, managed loop, orchestrated by the **Trainer**. The diagram below illustrates this core interaction:

![Loop of Tasks and Spans](../assets/tasks-spans-loop.svg){ .center }

**The Loop Explained:**

- **Algorithm to Agent (via Trainer):** The **Algorithm** (the "brain") creates an improved **Prompt Template** and selects **Tasks**. The Trainer then sends both to the Agent.
- **Agent to Algorithm (via Trainer):** For each task it receives, the Agent uses the provided prompt template to perform a Rollout, executing its logic and potentially using tools. During this rollout, the runner that runs the agent captures Spans that detail every step. The agent also calculates a Reward for its performance on the task. These spans and rewards are then sent back to the Algorithm via the Trainer.
- **Algorithm Learning:** The Algorithm then analyzes these spans and rewards to learn how to improve the agent's behavior, for example, by generating a better prompt. This improved prompt is then used in the next iteration of tasks.

This cycle continues, allowing the agent to continuously learn and get better at solving tasks.

!!! note

    In the next tutorial, we will see that the "via Trainer" here is not accurate. It's actually via the runner and store.

### The Algorithm

The algorithm is the smart part of the system that drives the improvement. In this tutorial, we use [**APO**][agentlightning.algorithm.apo.APO] (Automatic Prompt Optimization). It works in a few steps:

1. **Evaluate:** The algorithm first asks for rollouts to be run using the current prompt template to see how well it performs.
2. **Critique:** It then looks at the detailed spans from those rollouts. Using a powerful LLM (`gpt-5-mini`), it generates a "textual gradient", which is a natural language critique of the prompt. For example: "The prompt is ambiguous about how to handle tie-breakers for equally good rooms."
3. **Rewrite:** Finally, it gives the critique and the original prompt to another LLM (`gpt-4.1-mini`) and asks it to apply the edits, generating a new, improved prompt template.

This cycle repeats, with each round producing a slightly better prompt. To use it, you simply initialize the APO class with your desired hyperparameters.

```python
# In the main training script: run_apo.py
from openai import AsyncOpenAI

openai = AsyncOpenAI()
algo = agl.APO(openai)
```

!!! tip

    Make sure you have `OPENAI_API_KEY` set in your environment variables.

### The Trainer

The Trainer is the central component you'll interact with. It connects everything and manages the entire workflow by running the loop described above. You configure the Trainer, providing the algorithm, the number of parallel runners, and the initial prompt. A single call to [`trainer.fit()`][agentlightning.Trainer.fit] kicks off the entire process!

```python
# 1. Configure the Trainer with the algorithm and initial prompt
trainer = agl.Trainer(
    algorithm=algo,
    n_runners=8, # Run 8 agents in parallel to try out the prompts
    initial_resources={
        # The initial prompt template to be tuned
        "prompt_template": prompt_template_baseline()
    },
    # This is used to convert the span data into a message format consumable by APO algorithm
    adapter=agl.TraceToMessages(),
)

# 2. Load datasets: They can be list of task objects consumable by `room_selector`.
dataset_train, dataset_val = ...

# 3. Start the training process!
trainer.fit(
    agent=room_selector,
    train_dataset=dataset_train,
    val_dataset=dataset_val
)
```

!!! tip

    [`TraceToMessages`][agentlightning.TraceToMessages] is a convenience adapter that converts spans into OpenAI chat messages. It requires `openai >= 1.100.0` to be installed.

## Training Results

The APO algorithm successfully improved the agent's performance. We ran the example with the following hyper-parameters:

- `val_batch_size` = 10
- `gradient_batch_size` = 4
- `beam_width` = 2
- `branch_factor` = 2
- `beam_rounds` = 2

The validation accuracy on the 29 samples of datasets steadily increase from 0.569 (baseline) to **0.721** (after round 2). The tuning takes around 10 minutes with 8 runners. We ran twice, and the results are shown in the chart below.

<div style="height:400px">
<canvas data-chart='{ "type": "line", "data": { "labels": ["Baseline", "After round 1", "After round 2"], "datasets": [ { "label": "Run #1", "data": [0.569, 0.638, 0.721], "spanGaps": true }, { "label": "Run #2", "data": [0.534, 0.628, 0.645], "spanGaps": true } ] }, "options": { "interaction": { "mode": "nearest", "intersect": false }, "plugins": { "legend": { "display": true, "position": "top" }, "title": { "display": true, "text": "Validation Accuracy Across Rounds" } }, "scales": { "x": { "title": { "display": true, "text": "Round" } }, "y": { "title": { "display": true, "text": "Accuracy" } } } } }'></canvas>
</div>

This demonstrates how Agent-lightning can efficiently and automatically enhance your agent's capabilities with just a few lines of code.


## Links discovered
- [**Automatic Prompt Optimization (APO)**](https://github.com/microsoft/agent-lightning/blob/main/docs/algorithm-zoo/apo.md)
- [OpenAI's documentation](https://platform.openai.com/docs/guides/function-calling)
- [OpenTelemetry documentation](https://opentelemetry.io/docs/concepts/signals/traces/)
- [ADK](https://google.github.io/adk-docs/observability/cloud-trace/)
- [AgentOps Waterfall Visualization](https://github.com/microsoft/agent-lightning/blob/main/docs/assets/agentops-waterfall-visualization.jpg)
- [Loop of Tasks and Spans](https://github.com/microsoft/agent-lightning/blob/main/docs/assets/tasks-spans-loop.svg)

--- docs/how-to/train-sql-agent.md ---
# Train SQL Agent with Agent-lightning and VERL

This walkthrough builds upon the **Agent-lightning SQL Agent** example and explains how the system components integrate: a **LangGraph-based SQL agent** wrapped as a [`LitAgent`][agentlightning.LitAgent], the **[`VERL`][agentlightning.algorithm.verl.VERL] reinforcement learning (RL) algorithm**, and the **[`Trainer`][agentlightning.Trainer]**, which coordinates both training and debugging.

The command-line interface in [`examples/spider/train_sql_agent.py`]({{ src("examples/spider/train_sql_agent.py") }}) provides a complete runnable example. However, this document focuses on understanding the underlying architecture so you can effectively adapt the workflow to your own agents.

## SQL Agent Architecture

Agent-lightning integrates seamlessly with various orchestration frameworks, including [Agent Framework](https://github.com/microsoft/agent-framework), [AutoGen](https://github.com/microsoft/autogen), [CrewAI](https://www.crewai.com/), [LangGraph](https://github.com/langchain-ai/langgraph), and the [OpenAI Agents SDK](https://github.com/openai/openai-agents-python). It can also interoperate with custom Python logic.

In this example, **LangGraph** defines a cyclic workflow that mirrors an analyst’s iterative SQL development process. The following graph (rendered directly from [`sql_agent.py`]({{ src("examples/spider/sql_agent.py") }})) illustrates how the agent drafts, executes, critiques, and refines queries until a satisfactory result is achieved.

```mermaid
---
config:
  flowchart:
    curve: linear
---
graph LR;
        __start__([<p>__start__</p>]):::first
        write_query(write_query)
        execute_query(execute_query)
        check_query(check_query)
        rewrite_query(rewrite_query)
        __end__([<p>__end__</p>]):::last
        __start__ --> write_query;
        check_query -.-> __end__;
        check_query -.-> rewrite_query;
        execute_query --> check_query;
        rewrite_query --> execute_query;
        write_query --> execute_query;
        classDef default fill:#f2f2f2,line-height:1.2
        classDef first fill-opacity:0
        classDef last fill:#cccccc
```

!!! note

    The workflow proceeds through the following stages:

    1. **write_query** – Generates an initial SQL query from the user’s question and the database schema.
    2. **execute_query** – Executes the generated query against the target database.
    3. **check_query** – Evaluates the query and its results (or errors) using a specialized prompt (`CHECK_QUERY_PROMPT`) to detect issues.
    4. **rewrite_query** – If issues are identified, the agent rewrites the query using feedback from the previous step and re-enters the loop.
    5. **END** – The cycle terminates when the query is validated or the maximum iteration count (`max_turns`) is reached. Each *turn* consists of one full loop through the `write_query`, `execute_query`, `check_query`, and (if applicable) `rewrite_query` stages.

In this tutorial, **reinforcement learning (RL)** is used to optimize the `write_query` and `rewrite_query` stages. While the `check_query` step shares the same underlying LLM weights, its trace data is not used for learning.

To keep the design modular and maintainable, it is recommended to define the LangGraph-based SQL Agent in a separate file and expose it via a builder function such as:

```python
def build_langgraph_sql_agent(
    database_path: str,
    openai_base_url: str,
    model: str,
    sampling_parameters: Dict[str, Any],
    max_turns: int,
    truncate_length: int
):
    builder = StateGraph(State)
    builder.add_node(write_query)
    ...

    builder.add_edge(START, "write_query")
    ...

    return builder.compile().graph()
```

This approach isolates your LangGraph logic from Agent-lightning version changes, improving both readability and debuggability.

## Bridging LangGraph and Agent-lightning

!!! tip

    Keep [`sql_agent.py`]({{ src("examples/spider/sql_agent.py") }}) open on the side while reading this section. This will help you understand how the code snippets shown here work in practice.

The **`LitSQLAgent`** class defined in [`sql_agent.py`]({{ src("examples/spider/sql_agent.py") }}) acts as the bridge. It subclasses [`agl.LitAgent`][agentlightning.LitAgent], allowing the runner to provision shared resources (e.g., [LLMs][agentlightning.LLM]) for each rollout.

Below is a simplified illustration of the key logic (note: this is conceptual pseudocode; the actual implementation includes dataset-specific details):

```python
class LitSQLAgent(agl.LitAgent[Dict[str, Any]]):

    def __init__(self, max_turns: int, truncate_length: int):
        # Every turn here refers to a full cycle of write/exe/check/rewrite
        self.max_turns = max_turns
        self.truncate_length = truncate_length

    def rollout(
        self,
        task: Dict[str, Any],
        resources: agl.NamedResources,
        rollout: agl.Rollout
    ) -> float | None:
        llm: agl.LLM = resources["main_llm"]
        agent = build_langgraph_sql_agent(
            database_path="sqlite:///" + task["db_id"],
            max_turns=self.max_turns,
            truncate_length=self.truncate_length,
            openai_base_url=llm.get_base_url(rollout.rollout_id, rollout.attempt.attempt_id),
            model=llm.model,
            sampling_parameters=llm.sampling_parameters,
        )
        result = agent.invoke({"question": question}, {
            "callbacks": [self.tracer.get_langchain_handler()],
            "recursion_limit": 100,
        })
        reward = evaluate_query(result["query"], ground_truth, db_path, raise_on_error=False)
        return reward
```

The `LitSQLAgent` serves as a lightweight wrapper around the LangGraph agent, providing the correct interface for the [`rollout`][agentlightning.LitAgent.rollout] method. It constructs the LangGraph agent, invokes it, and returns the evaluation result as a reward signal.

The `"main_llm"` resource key is a convention between the agent and [VERL][agentlightning.algorithm.verl.VERL]. It is used to inject an OpenAI-compatible endpoint from the [VERL][agentlightning.algorithm.verl.VERL] algorithm during rollout. Two approaches are supported to use this [agentlightning.LLM][] resource:

1. **Direct access** – Use [`llm.endpoint`][agentlightning.LLM.endpoint] for a simple integration (identical to the v0.1 example).
2. **Context-aware access** – Use [`get_base_url`][agentlightning.ProxyLLM.get_base_url] with [`rollout.rollout_id`][agentlightning.Rollout.rollout_id] and [`rollout.attempt.attempt_id`][agentlightning.Attempt.attempt_id].
   This approach enables per-caller trace attribution, improving trace collection per rollout or attempt when runner-side tracers are unavailable. For details, see [Working with Traces](../tutorials/traces.md).

## Reward Signal and Evaluation

The `evaluate_query` function provides the reward mechanism for RL training. In agent training, obtaining a consistent and meaningful reward signal is often challenging. Fortunately, this is simplified when using the [**Spider dataset**](https://yale-lily.github.io/spider). The dataset includes ~8k samples containing natural-language questions, database schemas, and ground-truth SQL queries.

Using the [**Spider evaluator**](https://github.com/taoyds/test-suite-sql-eval), the agent's generated query is executed and compared to the ground-truth query on the target database. The two queries are considered equivalent if they produce identical execution results.

!!! attention

    The ground-truth queries must **never** be exposed to the agent during training to prevent data leakage.

In this setup, the reward is returned directly from the [`rollout`][agentlightning.LitAgent.rollout] method, enabling the runner to forward it back to the RL algorithm.

!!! warning

    Avoid using [`emit_reward`][agentlightning.emit_reward] in conjunction with returning a reward value. Doing both will cause the algorithm to receive duplicate reward signals, leading to inconsistent training behavior.

## Configuring VERL for Reinforcement Learning

View [`examples/spider/train_sql_agent.py`]({{ src("examples/spider/train_sql_agent.py") }}) for a full reinforcement learning configuration, which is a plain Python dictionary. It mirrors (and actually *is*) the [shell arguments](https://verl.readthedocs.io/en/latest/index.html) used to launch training in the VERL framework but is easier to tweak programmatically:

```python
verl_config: Dict[str, Any] = {
    "algorithm": {"adv_estimator": "grpo", "use_kl_in_reward": False},
    "data": {
        # train_files and val_files are no longer needed here
        # because data are read in agl.Trainer
        ...,
        # Controls how many tasks are pooled per step
        # (multiplied by actor_rollout_ref.rollout.n)
        "train_batch_size": 32,
        # Prompt and responses larger than these lengths are truncated
        "max_prompt_length": 4096,
        "max_response_length": 2048,
    },
    "actor_rollout_ref": {
        "rollout": {
            # Only vLLM is supported currently
            "name": "vllm",
            # Equals to group size of GRPO
            "n": 4,
            # Used to enable tool call parser in vLLM
            "multi_turn": {"format": "hermes"},
            ...
        },
        "actor": {"ppo_mini_batch_size": 32, "optim": {"lr": 1e-6}, ...},
        "model": {
            # Config your preferred LLM here
            "path": "Qwen/Qwen2.5-Coder-1.5B-Instruct",
            ...
        },
    },
    "trainer": {
        "n_gpus_per_node": 1,
        # Validation once before training starts
        "val_before_train": True,
        # Validation every N training steps
        "test_freq": 32,
        # Save checkpoints every N training steps
        "save_freq": 64,
        # Go through the train dataset this many times
        "total_epochs": 2
    },
}
```

This is equivalent to the following CLI invocation:

```bash
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    algorithm.use_kl_in_reward=False \
    data.train_batch_size=32 \
    data.max_prompt_length=4096 \
    data.max_response_length=2048 \
    actor_rollout_ref.rollout.name=vllm \
    actor_rollout_ref.rollout.n=4 \
    actor_rollout_ref.rollout.multi_turn.format=hermes \
    actor_rollout_ref.actor.ppo_mini_batch_size=32 \
    actor_rollout_ref.actor.optim.lr=1e-6 \
    actor_rollout_ref.model.path=Qwen/Qwen2.5-Coder-1.5B-Instruct \
    trainer.n_gpus_per_node=1 \
    trainer.val_before_train=True \
    trainer.test_freq=32 \
    trainer.save_freq=64 \
    trainer.total_epochs=2
```

!!! warning
    We used to provide a CLI called `python -m agentlightning.verl` to launch training in v0.1. This is no longer the recommended approach. Instead, use [`agl.Trainer`][agentlightning.Trainer] to run VERL and agent runners together, or follow the [debugging tutorial](../tutorials/debug.md) if you want an isolated experience similar to v0.1.
## Orchestrating Training with [`Trainer`][agentlightning.Trainer]

[`Trainer`][agentlightning.Trainer] is the high-level orchestrator that integrates the agent, algorithm, dataset, and distributed runners. The key benefits of using the [`Trainer`][agentlightning.Trainer] are:

1. It allows you to launch everything with a single line of code: `trainer.fit(...)`.
2. It exposes configuration options such as `n_runners` to control parallelism and `adapter` to define how algorithms interpret the trace data produced by the agent.

An example usage is shown below:

```python
import agentlightning as agl

agent = LitSQLAgent()
algorithm = agl.VERL(verl_config)
trainer = agl.Trainer(
    n_runners=10,
    algorithm=algorithm,
    adapter={"agent_match": active_agent},
)
train_data = pd.read_parquet("data/train_spider.parquet").to_dict("records")
val_data = pd.read_parquet("data/test_dev_500.parquet").to_dict("records")
trainer.fit(agent, train_dataset=train_data, val_dataset=val_data)
```

First, `agl.VERL(verl_config)` launches the [`VERL`][agentlightning.algorithm.verl.VERL] algorithm and its OpenAI-compatible proxy. The `train_data` and `val_data` are passed into [`VERL`][agentlightning.algorithm.verl.VERL], which enqueues tasks to a centralized task queue managed by the [`LightningStore`][agentlightning.LightningStore], accessible to all runners.

When [`Trainer.fit`][agentlightning.Trainer.fit] is called, it launches 10 concurrent runners (as specified by `n_runners=10`). Each runner pulls tasks from the centralized task queue, executes the agent’s [`rollout`][agentlightning.LitAgent.rollout] method, collects traces, and returns rewards to VERL for training.

The [`Adapter`][agentlightning.Adapter], as discussed earlier, is used at the algorithm side, and receives the traces emitted by the agent and runners. The `agent_match` parameter ensures [`VERL`][agentlightning.algorithm.verl.VERL] only ingests spans from the specific agent you want to optimize.
In the example above, there are at least three agents—`write_query`, `rewrite_query`, and `check_query`. By setting `agent_match` to a regex like `"write"`, both `write_query` and `rewrite_query` agents are optimized simultaneously. You can also set it to `"write|check"` or `None` to include all agents if desired.

## Dry-Run the Pipeline with [`Trainer.dev`][agentlightning.Trainer.dev]

Before committing hours of GPU time, you can **dry-run** the agent with [`Trainer.dev()`][agentlightning.Trainer.dev]. This method swaps in the lightweight [`Baseline`][agentlightning.Baseline] algorithm, enqueues up to ten tasks, and prints every span emitted by the agent. Because it uses the same runner stack as full training, it’s ideal for verifying database connections and LangGraph control flow.

To begin, the agent needs a valid OpenAI-compatible endpoint since VERL is not active in this mode. You can use OpenAI’s official API or your own local LLM endpoint. Wrap it as follows:

```python
trainer = agl.Trainer(
    n_workers=1,
    initial_resources={
        "main_llm": agl.LLM(
            endpoint=os.environ["OPENAI_API_BASE"],
            model="gpt-4.1-nano",
            sampling_parameters={"temperature": 0.7},
        )
    },
)
```

Then, call [`trainer.dev(...)`][agentlightning.Trainer.dev] with a small number of tasks:

```python
dev_data = pd.read_parquet("data/test_dev_500.parquet").to_dict("records")[:10]
trainer.dev(agent, dev_dataset=dev_data)
```

Run this in a Python session or adapt your script to include a `--dev` flag. Once the spans appear healthy and the rewards are non-zero, switch back to [`trainer.fit(...)`][agentlightning.Trainer.fit] for full RL training. See the [debugging tutorial](../tutorials/debug.md) for more tips on how to debug the agent.

## Running the Sample Code

The following tutorial explains how to run the complete example in [`examples/spider`]({{ src("examples/spider") }}).

### Dataset

The trainer expects three Parquet files inside `examples/spider/data`:
`train_spider.parquet`, `test_dev_500.parquet`, and `test_dev.parquet`.

Download the curated dataset bundle provided with the repository:

```bash
cd examples/spider
pip install gdown  # included in the 'experiment' optional dependency
gdown --fuzzy https://drive.google.com/file/d/1oi9J1jZP9TyM35L85CL3qeGWl2jqlnL6/view
unzip -q spider-data.zip -d data
rm spider-data.zip
```

If you prefer to generate the files yourself, download [Spider 1.0](https://yale-lily.github.io/spider) and run:

```bash
python spider_eval/convert_dataset.py
```

Set `VERL_SPIDER_DATA_DIR` if you store the dataset outside the default `data` directory.

### Dependencies

Create a clean virtual environment, activate it, and install Agent-lightning with the VERL extras required by [this tutorial](../tutorials/installation.md). Install LangChain-related dependencies as needed.

For full training profiles, plan to use a GPU with at least **40 GB** of memory.

### Launch Training

From [`examples/spider`]({{ src("examples/spider") }}), run one of the helper scripts depending on your model preference:

```bash
python train_sql_agent.py qwen   # Default Qwen-2.5-Coder-1.5B run
python train_sql_agent.py llama  # LLaMA-3.2-1B with llama3_json tool parser
```

The script instantiates `LitSQLAgent` and launches [`trainer.fit`][agentlightning.Trainer.fit].
Provide `--active-agent my_agent_variant` if you only want to train one of the agents in the graph.

For the LLaMA profile, export an `HF_TOKEN` before running so VERL can download the model weights.

!!! tip "Troubleshooting"

    If you have got some Ray worker errors on either `WANDB_API_KEY` not set, or `HF_TOKEN` not set, or data not found, please try to restart the Ray cluster with the helper script: [scripts/restart_ray.sh]({{ src("scripts/restart_ray.sh") }}), which essentially stops the ray cluster if any, and starts a new one:

    ```bash
    env RAY_DEBUG=legacy HYDRA_FULL_ERROR=1 VLLM_USE_V1=1 ray start --head --dashboard-host=0.0.0.0
    ```

!!! note "Launching Training with NPUs"

    The example also supports running with **Huawei Ascend NPUs**. This feature is contributed by [Teams from Huawei](https://github.com/microsoft/agent-lightning/pull/272). To use it, resort to the function `config_train_npu` in the script.

    **Hardware Supported:** Atlas 200T A2 Box16, Atlas 900 A2 PODc, Atlas 800T A3. At least **a single 40GB NPU** is required to run the **Qwen2.5-Coder-1.5B-Instruct** model.

    **Environment Setup:** Python 3.11.13, CANN 8.2.RC1, torch 2.7.1+cpu, torch_npu 2.7.1.dev20250724. For basic environment preparation, please refer to this [document](https://gitcode.com/Ascend/pytorch).

    Before installing dependencies, configure the following pip mirrors:

    ```bash
    pip config set global.index-url http://repo.huaweicloud.com/repository/pypi/simple
    pip config set global.extra-index-url "https://download.pytorch.org/whl/cpu/ https://mirrors.huaweicloud.com/ascend/repos/pypi"
    ```

    Then install vLLM, vLLM-Ascend and VERL:

    ```bash
    pip install vllm==0.10.0 --trusted-host repo.huaweicloud.com
    pip install vllm-Ascend==0.10.0rc1 --trusted-host repo.huaweicloud.com
    pip install verl==0.5.0
    ```

    To ensure the VERL framework runs correctly on NPU, add the following lines to `verl/utils/vllm_utils.py`:

    ```python
    from vllm_ascend.patch import platform
    from vllm_ascend.patch import worker
    ```

    See the following reference for more details: [https://github.com/vllm-project/vllm-ascend/issues/1776](https://github.com/vllm-project/vllm-ascend/issues/1776).

    After the above dependencies have been installed, from [`examples/spider`]({{ src("examples/spider") }}) run the following script command:

    ```bash
    python train_sql_agent.py npu
    ```

### Debugging the Agent without VERL

[`sql_agent.py`]({{ src("examples/spider/sql_agent.py") }}) also provides a `debug_sql_agent()` helper to run the LangGraph workflow directly against a local or hosted OpenAI-compatible endpoint before using VERL.

Set the following environment variables, then execute the file:

```bash
export OPENAI_API_BASE=<your_api_base>
export OPENAI_API_KEY=<your_api_key>
cd examples/spider
python sql_agent.py
```

This allows you to verify that the workflow and prompts behave as expected before reinforcement learning is introduced.

### Evaluation

The following results were obtained by running `python train_sql_agent.py qwen` on a single 80 GB GPU.
Training completes in approximately **12 hours**.
The training curves below are smoothed by aggregating every 16 steps for better visualization.

Additional evaluation results were collected with a legacy version — Agent-lightning v0.1.1, `verl==0.5.0`, and `vllm==0.10.0`.
You can find them in this write-up:
[Training AI Agents to Write and Self-Correct SQL with Reinforcement Learning](https://medium.com/@yugez/training-ai-agents-to-write-and-self-correct-sql-with-reinforcement-learning-571ed31281ad)


<div style="height:400px">
<canvas data-chart='{"type": "line", "data": {"labels": [0.0, 16.0, 32.0, 48.0, 64.0, 80.0, 96.0, 112.0, 128.0, 144.0, 160.0, 176.0, 192.0, 208.0, 224.0, 240.0, 256.0, 272.0, 288.0, 304.0, 320.0, 336.0, 352.0, 368.0, 384.0, 400.0, 416.0, 432.0], "datasets": [{"label": "Training", "data": [0.4609375, 0.5041666666666667, 0.5790441176470589, 0.6015625, 0.6070772058823529, 0.6208333333333333, 0.6668198529411765, 0.66875, 0.6709558823529411, 0.6708333333333333, 0.6847426470588235, 0.6791666666666667, 0.6819852941176471, 0.690625, 0.7008272058823529, 0.7453125, 0.7398897058823529, 0.7119791666666667, 0.7224264705882353, 0.7114583333333333, 0.7431066176470589, 0.7427083333333333, 0.75, 0.7302083333333333, 0.7247242647058824, 0.7390625, 0.7463235294117647, 0.7376302083333334], "spanGaps": true}, {"label": "Validation", "data": [0.342, null, 0.594, null, 0.642, null, 0.66, null, 0.676, null, 0.676, null, 0.694, null, 0.712, null, 0.702, null, 0.678, null, 0.702, null, 0.702, null, 0.674, null, 0.734, 0.722], "spanGaps": true}]}, "options": {"interaction": {"mode": "nearest", "intersect": false}, "plugins": {"legend": {"display": true, "position": "top"}, "title": {"display": true, "text": "SQL Agent Training Result (agent_match = write)"}}, "scales": {"x": {"title": {"display": true, "text": "Step (aggregated)"}}, "y": {"title": {"display": true, "text": "Accuracy"}}}}}'></canvas>
</div>

<div style="height:400px">
<canvas data-chart='{"type": "line", "data": {"labels": [0.0, 16.0, 32.0, 48.0, 64.0, 80.0, 96.0, 112.0, 128.0, 144.0, 160.0, 176.0, 192.0, 208.0, 224.0, 240.0, 256.0, 272.0, 288.0, 304.0, 320.0, 336.0, 352.0, 368.0, 384.0, 400.0, 416.0, 432.0], "datasets": [{"label": "Training", "data": [0.4560546875, 0.578125, 0.6167279411764706, 0.6401041666666667, 0.6461397058823529, 0.6598958333333333, 0.6838235294117647, 0.69375, 0.6916360294117647, 0.6833333333333333, 0.6893382352941176, 0.6921875, 0.6838235294117647, 0.70625, 0.7045036764705882, 0.7442708333333333, 0.7288602941176471, 0.7317708333333334, 0.7311580882352942, 0.7286458333333333, 0.7316176470588235, 0.7359375, 0.7366727941176471, 0.7208333333333333, 0.7118566176470589, 0.7296875, 0.7389705882352942, 0.7350260416666666], "spanGaps": true}, {"label": "Validation", "data": [0.33, null, 0.62, null, 0.662, null, 0.682, null, 0.696, null, 0.7, null, 0.708, null, 0.692, null, 0.72, null, 0.7, null, 0.7, null, 0.702, null, 0.694, null, 0.702, 0.682], "spanGaps": true}]}, "options": {"interaction": {"mode": "nearest", "intersect": false}, "plugins": {"legend": {"display": true, "position": "top"}, "title": {"display": true, "text": "SQL Agent Training Result (agent_match = null)"}}, "scales": {"x": {"title": {"display": true, "text": "Step (aggregated)"}}, "y": {"title": {"display": true, "text": "Value"}}}}}'></canvas>
</div>


## Links discovered
- [Agent Framework](https://github.com/microsoft/agent-framework)
- [AutoGen](https://github.com/microsoft/autogen)
- [CrewAI](https://www.crewai.com/)
- [LangGraph](https://github.com/langchain-ai/langgraph)
- [OpenAI Agents SDK](https://github.com/openai/openai-agents-python)
- [Working with Traces](https://github.com/microsoft/agent-lightning/blob/main/docs/tutorials/traces.md)
- [**Spider dataset**](https://yale-lily.github.io/spider)
- [**Spider evaluator**](https://github.com/taoyds/test-suite-sql-eval)
- [shell arguments](https://verl.readthedocs.io/en/latest/index.html)
- [debugging tutorial](https://github.com/microsoft/agent-lightning/blob/main/docs/tutorials/debug.md)
- [Spider 1.0](https://yale-lily.github.io/spider)
- [this tutorial](https://github.com/microsoft/agent-lightning/blob/main/docs/tutorials/installation.md)
- [Teams from Huawei](https://github.com/microsoft/agent-lightning/pull/272)
- [document](https://gitcode.com/Ascend/pytorch)
- [https://github.com/vllm-project/vllm-ascend/issues/1776](https://github.com/vllm-project/vllm-ascend/issues/1776)
- [Training AI Agents to Write and Self-Correct SQL with Reinforcement Learning](https://medium.com/@yugez/training-ai-agents-to-write-and-self-correct-sql-with-reinforcement-learning-571ed31281ad)

--- docs/how-to/unsloth-sft.md ---
# Fine-tune with Unsloth SFT

!!! note "Prerequisites"

    Please make sure you have read [Write the First Algorithm](./write-first-algorithm.md). Although that recipe is based on a simple prompt tuning algorithm, it introduces the core concepts of Agent-lightning and you should be familiar with them before proceeding.

This recipe builds on [Write the First Algorithm](./write-first-algorithm.md). Instead of iterating on a prompt, we will fine-tune a large language model with [Unsloth](https://docs.unsloth.ai/)'s SFT Trainer and keep the whole loop inside Agent-lightning. The new pieces you will meet are the **LLM proxy**, the **trace-to-triplet adapter**, a [vLLM](https://github.com/vllm-project/vllm) inference endpoint, and an agent implemented with the [OpenAI Agents SDK](https://openai.github.io/openai-agents-python/). The full sample code is available in the [`examples/unsloth`]({{ src("examples/unsloth") }}) folder.

!!! warning

    You need a GPU that can host the Unsloth base model and run vLLM. The sample defaults to `unsloth/Qwen3-4B-Instruct-2507`, which requires at least 16GB of GPU memory under 4-bit quantization.

## The Data and Serving Loop

To tune a large language model in Supervised Fine-Tuning (SFT), we commonly need a dataset with input/output samples. For example, the [TRL SFT Trainer](https://huggingface.co/docs/trl/sft_trainer) expects a dataset with samples like the following:

```json
{"messages": [{"role": "user", "content": "What color is the sky?"},
              {"role": "assistant", "content": "It is blue."}]}
```

With supervised fine-tuning, the LLM learns to generate the "assistant" response as close as possible to the completion in the dataset.

Typically, the dataset used in SFT should be a curated set of samples. The samples can be either hand-written by humans, or generated by a more powerful model, which is known as [data distillation](https://docs.nvidia.com/nemo-framework/user-guide/24.12/modelalignment/knowledge-distillation.html). However, in this recipe, we use a different setup that relies on samples generated by the model itself. We use the reward emitted by the agent to select the top-performing samples.

Overall, the flow of the algorithm is an iteration of the following steps:

1. Serve the current checkpoint (with vLLM).
2. Publish the vLLM endpoint through the LLM proxy and let runners roll out some tasks with the current model.
3. Collect the traces from the rollouts and transform the highest-rewarded ones into a dataset that is acceptable for Unsloth SFT Trainer.
4. Launch Unsloth to fine-tune on the dataset and save a new checkpoint.

You will find the full source code of this iteration in `sft_one_iter` in [sft_algorithm.py]({{ src("examples/unsloth/sft_algorithm.py") }}). We will elaborate on each part below.

### Serving the Model with vLLM and Proxy

Most modern agents do not use the model directly; instead, they use an API like the OpenAI chat completions API to interact with the model. Therefore, we need a vLLM-based inference server launched before rollouts. The serving code looks like the following. See the `vllm_server` function in [sft_algorithm.py]({{ src("examples/unsloth/sft_algorithm.py") }}) if you want to see a more robust version.

```python
from openai import OpenAI

vllm_process = subprocess.Popen([
    "vllm", "serve", model_path, "--port", str(port),
    "--enable-auto-tool-choice", "--tool-call-parser", "hermes"
])

# Wait for the server to be ready
url = f"http://localhost:{port}/health"
start = time.time()
client = httpx.Client()

while True:
    if client.get(url).status_code == 200:
        break

server_address = f"http://localhost:{port}/v1"

# Try using the vLLM server
openai = OpenAI(base_url=server_address)
...
```

In this recipe, we do not expose the server address directly to the agent runners, because we want to install a "middleware" to collect the prompts and responses of all the requests. In general, it's up to you to decide whether to hide the vLLM server behind a proxy or not.

The "middleware" here is [`LLMProxy`][agentlightning.LLMProxy], which is an independent [LiteLLM](https://docs.litellm.ai/) server that forwards the requests to the vLLM server. It also exposes an OpenAI-compatible API that the runners can target without caring about where the model lives. The benefits of using the proxy are:

1. **Traces:** The proxy automatically logs the prompts and responses of all the requests into the store.
2. **Token IDs:** The proxy augments the requests so that the vLLM server can return the prompt and response token IDs (see more details in [Serving LLM](../deep-dive/serving-llm.md)).

The [`LLMProxy`][agentlightning.LLMProxy] accepts a list of model configurations, in the same syntax as LiteLLM's [`model_list`](https://docs.litellm.ai/docs/proxy/configs). Include a `hosted_vllm/` prefix to the models to activate LiteLLM's [vLLM integration](https://docs.litellm.ai/docs/providers/vllm).

```python
import agentlightning as agl

llm_proxy = agl.LLMProxy(port=port, store=store)
model_list = [
    {
        "model_name": "Qwen3-4B-Instruct",
        "litellm_params": {"model": f"hosted_vllm/{model_path}", "api_base": server_address},
    }
]
llm_proxy.update_model_list(model_list)
# If the proxy is not running, it will start automatically.
await llm_proxy.restart()
# Add the proxy as a resource to the store so that the runners can access it via URL.
resource_update = await store.add_resources({"main_llm": llm_proxy.as_resource()})
```

### Spawn Rollout and Collect Spans

Once the proxy is registered as a resource, the algorithm schedules work for the rollout runners. Each problem from a training dataset becomes a rollout with the proxy baked into its resources:

```python
rollouts: list[Rollout] = []
for sample in train_dataset:
    rollouts.append(
        await store.enqueue_rollout(
            input=sample,
            mode="train",
            resources_id=resources_update.resources_id,
        )
    )
```

`resources_id` ties every rollout to the `main_llm` proxy resource we just uploaded. The runners on the other side poll the store ([`LitAgentRunner.iter()`][agentlightning.LitAgentRunner.iter]) and execute the agent for each rollout. On the algorithm side we wait for completions with a non-blocking polling loop:

```python
completed_rollouts: list[Rollout] = []
while True:
    completed_rollouts = await store.wait_for_rollouts(
        rollout_ids=[r.rollout_id for r in rollouts],
        timeout=0.0,
    )
    if len(completed_rollouts) == len(rollouts):
        break
    await asyncio.sleep(5.0)
```

!!! note

    The `timeout=0.0` is needed here because this example uses a [`LightningStoreClient`][agentlightning.LightningStoreClient], and `wait_for_rollouts` establishes an HTTP connection to that store. Currently, only non-blocking wait requests are supported, which avoids holding the store connection open.

Once the rollouts complete, we terminate the vLLM server to free up GPU memory.

```python
vllm_process.terminate()
vllm_process.join(timeout=10.0)
```

### Adapt the Spans to HuggingFace Dataset

[`LlmProxyTraceToTriplet`][agentlightning.LlmProxyTraceToTriplet] converts the proxy’s spans (which might be dozens to hundreds per rollout) into [`Triplet`][agentlightning.Triplet] objects that contain prompt/response token IDs plus an optional reward. The adapter may return multiple triplets per rollout (one per chat-completion call). To bias training toward successful reasoning chains the algorithm walks the triplets in reverse order, keeps the most recent reward, and turns each prompt/response pair into Hugging Face dataset rows:

```python
all_triplets = []
data_adapter = agl.LlmProxyTraceToTriplet()

for rollout in completed_rollouts:
    spans = await store.query_spans(rollout.rollout_id, "latest")
    triplets = data_adapter.adapt(spans)

    recent_reward = None
    for triplet in reversed(triplets):
        if triplet.reward is not None:
            recent_reward = triplet.reward
        if recent_reward is None:
            continue

        input_ids = triplet.prompt["token_ids"] + triplet.response["token_ids"]
        # We don't train on prompt tokens, so they are masked out by setting to -100.
        labels = [-100] * len(triplet.prompt["token_ids"]) + triplet.response["token_ids"]
        # This matches the dataset format required by the Unsloth SFT trainer.
        all_triplets.append(
            {
                "input_ids": input_ids,
                "attention_mask": [1] * len(input_ids),
                "labels": labels,
                "reward": recent_reward,
            }
        )
```

!!! note

    You might notice that the dataset format used here differs from the format described in the **SFT Trainer** documentation. According to the documentation, dataset samples should be provided as plain text strings or message objects.

    As a matter of fact, this example leverages some [undocumented behavior](https://github.com/huggingface/trl/blob/e0eec055b412c48ad754149c475a87a8fca34fb4/trl/trainer/sft_trainer.py#L887) in the SFT Trainer implementation. When the dataset already includes a `"input_ids"` column, the Trainer automatically marks it as `is_processed` and skips the internal tokenization step.

    Since we already have spans with token IDs generated by the [`LLMProxy`][agentlightning.LLMProxy], providing them directly avoids unnecessary [**re-tokenization** and related complications](../deep-dive/serving-llm.md). This approach will both save processing time and increase consistency between training and inference.

After aggregating every rollout we shuffle, sort by reward, and keep the top fraction (e.g., 50%) before shuffling again. The resulting list feeds directly into `datasets.Dataset.from_list`, which is the format Unsloth’s SFT trainer expects.

```python
from datasets import Dataset as HuggingFaceDataset

random.shuffle(all_triplets)
all_triplets.sort(key=lambda x: x["reward"], reverse=True)
sliced_triplets = all_triplets[: max(1, int(len(all_triplets) * triplet_fraction))]
# Shuffle the sliced triplets again
random.shuffle(sliced_triplets)

sft_dataset = HuggingFaceDataset.from_list(sliced_triplets)
```

### Launch Unsloth Training

The heavy lifting happens in [`trl.SFTTrainer`](https://huggingface.co/docs/trl/sft_trainer) (see [unsloth_helper.py]({{ src("examples/unsloth/unsloth_helper.py") }}) on how it's used). We launch it in a fresh process created with `multiprocessing.get_context("spawn")` so CUDA memory is reliably reclaimed when training ends. Launching it in the same process will also work for the first iteration, but we found that the memory won't be freed properly for subsequent vLLM serving.

```python
context = multiprocessing.get_context("spawn")
unsloth_process = context.Process(
    target=unsloth_training,
    args=(model_path, sft_dataset, next_model_path),
    daemon=True,
)
unsloth_process.start()
unsloth_process.join(timeout=600.0)
```

Inside the `unsloth_training` subprocess, Unsloth loads the previous checkpoint in 4-bit, applies LoRA adapters, and forwards the Hugging Face dataset to [`trl.SFTTrainer`](https://huggingface.co/docs/trl/sft_trainer) with the configuration defined in [`SFTConfig`](https://huggingface.co/docs/trl/sft_trainer#trl.SFTConfig) (batch size, accumulation steps, learning rate, etc.). The merged 16-bit weights are saved under `models/version_<iteration + 1>` so the next iteration can immediately serve them with vLLM.

```python
from unsloth import FastLanguageModel
# TRL is patched by unsloth.
from trl import SFTConfig, SFTTrainer

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_path,
    load_in_4bit=True,  # 4 bit quantization to reduce memory
)

# Config the model to use LoRA
model = FastLanguageModel.get_peft_model(
    model,
    r=32,
    ...
)

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=sft_dataset,
    ...
)

# This is the heaviest step.
trainer_stats = trainer.train()

# Save in 16-bit for vLLM inference later
model.save_pretrained_merged(next_model_path, tokenizer, save_method="merged_16bit")
```

## Math Agent: OpenAI Agents SDK with MCP

We build an agent with the [OpenAI Agents SDK](https://openai.github.io/openai-agents-python/) to wire a calculator MCP tool and an OpenAI-compatible chat completion model together. The agent aims to solve a math problem and returns a reward indicating whether the answer is correct or not. The runner injects the `LLM` resource supplied by the algorithm side:

```python
import os
from typing import TypedDict

import agentlightning as agl
from agents import Agent, ModelSettings, OpenAIChatCompletionsModel, Runner as OpenAIRunner
from agents.mcp import MCPServerStdio
from openai import AsyncOpenAI

class GsmProblem(TypedDict):
    input: str
    target: float

def compute_reward(result: str, target: float) -> float:
    ...

@agl.rollout
async def math_agent(task: GsmProblem, llm: agl.LLM) -> float:
    async with MCPServerStdio(
        name="Calculator via uvx",
        params={"command": "uvx", "args": ["mcp-server-calculator"]},
    ) as server:
        agent = Agent(
            name="Assistant",
            instructions=(
                "Use the calculator tool for every question. "
                "Return only the numeric answer wrapped like ### <answer> ###."
            ),
            mcp_servers=[server],
            model=OpenAIChatCompletionsModel(
                model=llm.model,
                openai_client=AsyncOpenAI(
                    base_url=llm.endpoint,
                    api_key=llm.api_key or "dummy",
                ),
            ),
            model_settings=ModelSettings(
                temperature=llm.sampling_parameters.get("temperature", 0.0),
            ),
        )
        result = await OpenAIRunner.run(agent, task["input"])
    return compute_reward(result.final_output, task["target"])
```

!!! tip

    You can test the agent with a dry run:

    ```python
    import asyncio

    llm = agl.LLM(
        endpoint=os.environ["OPENAI_BASE_URL"],
        api_key=os.environ["OPENAI_API_KEY"],
        model="gpt-4.1-mini",
    )
    asyncio.run(math_agent({"input": "What is 1 + 1?", "target": 2.0}, llm))
    ```

## Run this Recipe

The full runnable script for this recipe resides in [`examples/unsloth`]({{ src("examples/unsloth") }}) folder.

Before running this example, install `unsloth`, `vllm`, and the other libraries used in the examples (the project uses CUDA tooling, TRL, rich, datasets, etc.). We tested with `unsloth==2025.10.1`. `unsloth==2025.10.2` and `2025.10.3` are not working because of an [issue](https://github.com/unslothai/unsloth/issues/3451) we have been investigating with the unsloth team.

It's recommended to download the base model before running the example, such that the first iteration and subsequent iterations can both load from local checkpoints.

```bash
hf download unsloth/Qwen3-4B-Instruct-2507 --local-dir models/version_0
```

The repository already contains `examples/unsloth/data_gsmhard.jsonl` (which is a very small subset of the [GSM-hard math dataset](https://huggingface.co/datasets/reasoning-machines/gsm-hard) for demonstration purposes).

### Run Manually

Similar to the [Write the First Algorithm](./write-first-algorithm.md) recipe, you can open three terminals and start each component in parallel.

```bash
agl store --port 4747
python examples/unsloth/sft_rollout_runners.py
python examples/unsloth/sft_algorithm.py
```

In this case, [`sft_rollout_runners.py`]({{ src("examples/unsloth/sft_rollout_runners.py") }}) is a simple spawner implemented in Python that spawns 4 runners in parallel. The runners all connect to the same store server executing in another terminal.

```python
import agentlightning as agl

def run_rollout(store: agl.LightningStore, worker_id: int) -> None:
    # Since the server side has already used LiteLLM proxy to collect traces,
    # a simple OtelTracer to collect the rewards is enough.
    tracer = agl.OtelTracer()

    runner = agl.LitAgentRunner(tracer=tracer)

    with runner.run_context(agent=math_agent, store=store, worker_id=worker_id):
        asyncio.run(runner.iter())


def spawn_runners(store: agl.LightningStore, n_runners: int) -> None:
    runners = [
        multiprocessing.Process(target=run_rollout, args=(store, worker_id))
        for worker_id in range(n_runners)
    ]
    for runner in runners:
        runner.start()

    for runner in runners:
        runner.join()


store = agl.LightningStoreClient("http://localhost:4747")
spawn_runners(store=store, n_runners=4)
```

!!! tip

    Try to swap [`OtelTracer`][agentlightning.OtelTracer] in the runners with other tracers like [`AgentOpsTracer`][agentlightning.AgentOpsTracer]. Try to use a different adapter at the algorithm side such as [`TracerTraceToTriplet`][agentlightning.TracerTraceToTriplet] to see what happens.

### Run Everything with Trainer

We also show how to wrap everything into a single script using [`Trainer`][agentlightning.Trainer]. [`sft_allinone.py`]({{ src("examples/unsloth/sft_allinone.py") }}) wires the same components together, replacing the manual management of runners above.

```python
class UnslothSupervisedFinetuning(agl.Algorithm):

    async def run(
        self,
        train_dataset: Optional[Dataset[GsmProblem]] = None,
        val_dataset: Optional[Dataset[GsmProblem]] = None,
    ):
        # Use the store, llm_proxy, and adapter from the trainer
        store = self.get_store()
        llm_proxy = self.get_llm_proxy()
        data_adapter = self.get_adapter()

        for iteration in range(self.max_iterations):
            ...  # Same logic as sft_algorithm.py

algo = UnslothSupervisedFinetuning(
    max_iterations=2,
    vllm_port=12316,
    train_triplet_fraction=0.5,
    initial_model_path="models/version_0",
)

# The LLM proxy can be created before Trainer
trainer = Trainer(
    n_runners=4,
    algorithm=algo,
    llm_proxy=LLMProxy(port=12358),
)

trainer.fit(math_agent, load_math_dataset())
```

You might wonder where the initialization of [`Adapter`][agentlightning.Adapter] happens in this code. It turns out that [`TracerTraceToTriplet`][agentlightning.TracerTraceToTriplet] is the default adapter in [`Trainer`][agentlightning.Trainer], so we don't need to create one manually.

Now you can run the example with:

```bash
python examples/unsloth/sft_allinone.py
```

It starts an [`InMemoryLighningStore`][agentlightning.InMemoryLightningStore] for you, launches four worker processes, iterates the SFT loop, and prints the final checkpoint path when done. Adjust `max_iterations`, `train_triplet_fraction`, `n_runners`, or the proxy port to match your hardware or training goals. If you already run an external store or proxy you can also pass those objects into [`Trainer`][agentlightning.Trainer] instead of relying on the [Trainer-managed defaults][debug-with-external-store].

!!! info

    As a future plan, we might graduate this example into a more powerful SFT algorithm bundled into [Algorithm Zoo](../algorithm-zoo/index.md). Currently, this `UnslothSupervisedFinetuning` is still for demo purposes.


## Links discovered
- [Write the First Algorithm](https://github.com/microsoft/agent-lightning/blob/main/docs/how-to/write-first-algorithm.md)
- [Unsloth](https://docs.unsloth.ai/)
- [vLLM](https://github.com/vllm-project/vllm)
- [OpenAI Agents SDK](https://openai.github.io/openai-agents-python/)
- [TRL SFT Trainer](https://huggingface.co/docs/trl/sft_trainer)
- [data distillation](https://docs.nvidia.com/nemo-framework/user-guide/24.12/modelalignment/knowledge-distillation.html)
- [LiteLLM](https://docs.litellm.ai/)
- [Serving LLM](https://github.com/microsoft/agent-lightning/blob/main/docs/deep-dive/serving-llm.md)
- [`model_list`](https://docs.litellm.ai/docs/proxy/configs)
- [vLLM integration](https://docs.litellm.ai/docs/providers/vllm)
- [undocumented behavior](https://github.com/huggingface/trl/blob/e0eec055b412c48ad754149c475a87a8fca34fb4/trl/trainer/sft_trainer.py#L887)
- [**re-tokenization** and related complications](https://github.com/microsoft/agent-lightning/blob/main/docs/deep-dive/serving-llm.md)
- [`trl.SFTTrainer`](https://huggingface.co/docs/trl/sft_trainer)
- [`SFTConfig`](https://huggingface.co/docs/trl/sft_trainer#trl.SFTConfig)
- [issue](https://github.com/unslothai/unsloth/issues/3451)
- [GSM-hard math dataset](https://huggingface.co/datasets/reasoning-machines/gsm-hard)
- [Algorithm Zoo](https://github.com/microsoft/agent-lightning/blob/main/docs/algorithm-zoo/index.md)

--- docs/tutorials/write-agents.md ---
# Writing Agents

This tutorial will focus on the heart of the system: the agent itself, guiding you through the different ways to define an agent's logic in Agent-lightning.

The basic requirements for any agent are:

1.  It must accept a single **task** as input.
2.  It must accept a set of tunable **resources** (like a [PromptTemplate][agentlightning.PromptTemplate] or [LLM][agentlightning.LLM]).
3.  It must **emit** trace span data so that algorithms can understand its behavior and learn from it. *The simplest way to do this is by returning a final reward.*

In practice, please also bear in mind that tasks, resources, and spans have extra requirements, in order to make it *trainable* within Agent-lightning:

1.  You will need a training dataset containing a set of tasks, of the same type that your agent expects as input.
2.  The tunable resources are related to the algorithm. For example, the APO algorithm we've seen tunes a [PromptTemplate][agentlightning.PromptTemplate]. Other algorithms might tune model weights or other configurations.
3.  The type of spans an algorithm can use varies. Almost all algorithms support a single, final reward span at the end of a rollout. However, not all algorithms support rewards emitted mid-rollout, let alone other kinds of spans like exceptions or log messages.

This tutorial will show you how to write an agent that can handle various tasks and resources and emit all kinds of spans. However, you should understand that agents and algorithms are often co-designed. Supporting new types of resources or spans in an algorithm is often much more complex than just adding them to an agent.

## [`@rollout`][agentlightning.rollout] Decorator

The simplest way to create an agent is by writing a standard Python function and marking it with the [@rollout][agentlightning.rollout] decorator. This approach is perfect for agents with straightforward logic that doesn't require complex state management.

Agent-lightning automatically inspects your function's signature and injects the required resources. For example, if your function has a parameter named `prompt_template`, Agent-lightning will find the [PromptTemplate][agentlightning.PromptTemplate] resource for the current rollout and pass it in.

Let's revisit the `room_selector` agent from the first tutorial:

```python
from typing import TypedDict
from agentlightning import PromptTemplate, rollout

# Define a data structure for the task input
class RoomSelectionTask(TypedDict):
    # ... fields for the task ...
    pass

@rollout
def room_selector(task: RoomSelectionTask, prompt_template: PromptTemplate) -> float:
    # 1. Use the injected prompt_template to format the input for the LLM
    prompt = prompt_template.format(**task)

    # 2. Execute the agent's logic (e.g., call an LLM, use tools)
    # ...

    # 3. Grade the final choice to get a reward
    reward = room_selection_grader(final_message, task["expected_choice"])

    # 4. Return the final reward as a float
    return reward
```

When you train this agent, the dataset is expected to be a list of `RoomSelectionTask` objects:

```python
from agentlightning import Dataset, Trainer

dataset: Dataset[RoomSelectionTask] = [
    RoomSelectionTask(date="2025-10-15", time="10:00", duration_min=60, attendees=10),
    RoomSelectionTask(date="2025-10-16", time="10:00", duration_min=60, attendees=10),
]

Trainer().fit(agent=room_selector, train_dataset=dataset)
```

Behind the scenes, the [`@rollout`][agentlightning.rollout] decorator wraps your function in a `FunctionalLitAgent` object, which is a subclass of [LitAgent][agentlightning.LitAgent] introduced below, making it compatible with the [Trainer][agentlightning.Trainer] and [Runner][agentlightning.Runner]. It supports parameters like `task`, `prompt_template`, `llm`, and `rollout`, giving you flexible access to the execution context.

Here is another example with more advanced usage with `llm` and `rollout` as parameters. The `llm` parameter gives you an OpenAI-compatible LLM endpoint to interact with, which can be tuned under the hood by algorithms. The `rollout` parameter gives you the full [Rollout][agentlightning.Rollout] object, which contains the rollout ID, rollout mode (training or validation), etc.

```python
from openai import OpenAI
from agentlightning import LLM, Rollout

class FlightBookingTask(TypedDict):
    request: str
    expected_booking: dict

@rollout
def flight_assistant(task: FlightBookingTask, llm: LLM, rollout: Rollout) -> float:
    print(f"Rollout ID: {rollout.rollout_id}")
    print(f"Rollout Mode: {rollout.mode}")

    # Use the tuned LLM resource to create an OpenAI client
    client = OpenAI(
        # This endpoint could be a proxy to a proxy to a proxy ...
        # It could be different every time `flight_assistant` is called
        # But it should be OpenAI-API compatible
        base_url=llm.endpoint,

        # Use a dummy key if not provided
        # Usually this does not matter because the training LLM is often not guarded by an API key
        # But you can use `or os.environ["OPENAI_API_KEY"]` to make the function compatible with 3rd-party LLMs
        api_key=llm.api_key or "dummy-key",
    )

    # Make an API call with the specified model
    response = client.chat.completions.create(
        model=llm.model,
        messages=[{"role": "user", "content": task["request"]}],
    )
    # Whether the API supports features like streaming, tool calls, etc. depends on
    # the endpoint that algorithms are serving to you.
    final_message = response.choices[0].message.content

    # Grade the result and return a reward
    reward = grade_flight_booking(final_message, task["expected_booking"])
    return reward
```

## Return Values from Agents

The value your agent function returns (i.e., the return value of the function decorated by [`@rollout`][agentlightning.rollout]) is crucial, as it's the primary way to report the outcome of a rollout. Agent-lightning supports several return types to accommodate different scenarios, from simple rewards to detailed, custom traces.

* **`float`**: This is the simplest and most common return type. The `float` is treated as the **final reward** for the entire rollout. Agent-lightning automatically creates a final reward span based on this value.

* **`None`**: Returning `None` tells the runner that trace collection is being handled entirely by the [Tracer][agentlightning.Tracer] through auto-instrumentation (e.g., via AgentOps). In this case, the runner will simply retrieve the spans that the tracer has already captured.

!!! important "Emitting the Final Reward"

    When returning `None`, you must still ensure a final reward is logged. You can do this by using the [`emit_reward`][agentlightning.emit_reward] function (covered in the [Use Emitters](./emitter.md) documentation). Wrapping your reward calculation function with the `@reward` decorator is NOT the recommended approach any more.

* **`list[ReadableSpan]`**, **`list[SpanCoreFields]`**, or **`list[Span]`**: For advanced use cases, you can manually construct and return a complete list of all spans for the rollout. This gives you full control over the trace data. You can return either a list of OpenTelemetry `ReadableSpan` objects or Agent-lightning's native `Span` objects.

For most users, returning a **`float`** for simple agents or returning **`None`** and using the emitter for more complex ones are the recommended approaches.

## Class-based Agents

For more complex agents that require state, helper methods, or distinct logic for training versus validation, you can create a class that inherits from [`LitAgent`][agentlightning.LitAgent]. This object-oriented approach provides more structure and control over the agent's lifecycle.

To create a class-based agent, you subclass [agentlightning.LitAgent][] and implement its [`rollout`][agentlightning.LitAgent.rollout] method.

[](){ #introduction-to-named-resources }

Here's how the `room_selector` could be implemented as a class. The rollout method has a slightly different signature than the function-based agent, mainly in how it handles the resources. Putting it simply, algorithms do not just send a [PromptTemplate][agentlightning.PromptTemplate] to the agents, they instead send [NamedResources][agentlightning.NamedResources], which is a mapping from resource key to [Resource][agentlightning.Resource]. This design is to allow for more advanced features like multi-resource tuning.

With [`@rollout`][agentlightning.rollout] decorator, the resource with correctly matched type will be automatically injected into the rollout method. However, when you use a class-based agent, you need to manually access the resource from the `resources` dictionary. Built-in algorithms listed their resource key naming conventions [here](../algorithm-zoo/index.md).

```python
import agentlightning as agl

class RoomSelectorAgent(agl.LitAgent[RoomSelectionTask]):
    def rollout(self, task: RoomSelectionTask, resources: agl.NamedResources, rollout: agl.Rollout) -> float:
        # 1. Access the prompt_template from the resources dictionary
        prompt_template = resources["prompt_template"]

        # 2. Execute the agent's logic
        prompt = prompt_template.format(**task)
        # ...

        # 3. Grade the final choice
        reward = room_selection_grader(final_message, task["expected_choice"])

        # 4. Return the final reward
        return reward

# To use it with the trainer:
# agent = RoomSelectorAgent()
# trainer.fit(agent=agent, ...)
```

The [`LitAgent`][agentlightning.LitAgent] class provides several methods you can override for more fine-grained control:

* [`rollout()`][agentlightning.LitAgent.rollout]: The primary method for the agent's logic. It's called for both training and validation by default.
* [`training_rollout()`][agentlightning.LitAgent.training_rollout] / [`validation_rollout()`][agentlightning.LitAgent.validation_rollout]: Implement these if you need different behavior during training (e.g., with exploration) and validation (e.g., with deterministic choices).
* [`rollout_async()`][agentlightning.LitAgent.rollout_async] / [`training_rollout_async()`][agentlightning.LitAgent.training_rollout_async] / [`validation_rollout_async()`][agentlightning.LitAgent.validation_rollout_async]: Implement the asynchronous versions of these methods if your agent uses `asyncio`.

!!! note

    Rollout is always executed in an asynchronous context no matter whether the agent is asynchronous or synchronous. If your synchronous agent contains some `asyncio.run()` calls, it might raise an error that there is already an event loop running. To avoid blocking the event loop, it's recommended to offload the inner async operations to a separate thread. Here is a sample code:

    ```python
    import asyncio
    import queue
    import threading

    def run_sync_ephemeral(coro) -> Any:
        """
        Run an async coroutine from sync code.
        - If no loop in this thread: use asyncio.run() directly.
        - If already in an event loop: spawn a worker thread that calls asyncio.run()
        (which creates and closes a brand-new event loop per call).
        """
        try:
            asyncio.get_running_loop()
        except RuntimeError:
            # No running loop in this thread; safe to use asyncio.run
            return asyncio.run(coro)

        # Already in a running loop -> execute in a worker thread
        q = queue.Queue[Any]()

        def worker():
            try:
                result = asyncio.run(coro)  # creates & closes its own loop
                q.put((True, result))
            except BaseException as e:
                q.put((False, e))

        t = threading.Thread(target=worker, daemon=True)
        t.start()
        ok, payload = q.get()
        t.join()
        if ok:
            return payload
        raise payload
    ```


## Links discovered
- [Use Emitters](https://github.com/microsoft/agent-lightning/blob/main/docs/tutorials/emitter.md)
- [here](https://github.com/microsoft/agent-lightning/blob/main/docs/algorithm-zoo/index.md)

--- examples/README.md ---
# ⚡ Examples Catalog

This catalog highlights the examples shipped with Agent-lightning.

Community-contributed examples and recipes are available in the [contrib](../contrib) directory.

| Example | Description | CI Maintenance |
|---------|-------------|----------------|
| [apo](./apo) | Automatic Prompt Optimization tutorials covering built-in, custom, and debugging workflows. | [![apo workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-apo.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-apo.yml) |
| [azure](./azure) | Supervised fine-tuning with Azure OpenAI. | [![azure workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-azure.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-azure.yml) |
| [calc_x](./calc_x) | VERL-powered math reasoning agent training that uses AutoGen with an MCP calculator tool. | [![calc_x workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-calc-x.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-calc-x.yml) |
| [chartqa](./chartqa) | Vision-language ChartQA agent that reasons over charts with LangGraph and VERL plus multi-step self-refinement. | [![chartqa workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-chartqa.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-chartqa.yml) |
| [claude_code](./claude_code) | Claude Code SWE-bench harness that records Agent-lightning traces across Anthropic, vLLM, and OpenAI-compatible backends. | [![claude_code workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-claude-code.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-claude-code.yml) |
| [minimal](./minimal) | Bite-sized programs that demonstrate how individual Agent-lightning building blocks behave in isolation. | [![minimal workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml) |
| [rag](./rag) | Retrieval-Augmented Generation pipeline targeting the MuSiQue dataset with Wikipedia retrieval. | [![rag workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-rag.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-rag.yml) |
| [spider](./spider) | Text-to-SQL reinforcement learning training on the Spider dataset using LangGraph. | [![spider workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-spider.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-spider.yml) |
| [tinker](./tinker) | Reinforcement learning with Tinker as the backend training service. | [![tinker workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-tinker.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-tinker.yml) |
| [unsloth](./unsloth) | Supervised fine-tuning example powered by Unsloth with 4-bit quantization and LoRA. | [![unsloth workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unsloth.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-unsloth.yml) |

## `examples-*` workflow status

CI status above avoids taking any workflow running with latest dependencies into account. That's why we reference the corresponding `badge-*` workflows instead. The following table displays the raw `examples-*` workflow status whenever the project is maintained by CI.

| Workflow | Status |
|----------|--------|
| `examples-apo.yml` | [![examples-apo workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-apo.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-apo.yml) |
| `examples-azure.yml` | [![examples-azure workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-azure.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-azure.yml) |
| `examples-calc-x.yml` | [![examples-calc-x workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-calc-x.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-calc-x.yml) |
| `examples-chartqa.yml` | [![examples-chartqa workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-chartqa.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-chartqa.yml) |
| `examples-claude-code.yml` | [![examples-claude-code workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-claude-code.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-claude-code.yml) |
| `examples-compat.yml` | [![examples-compat workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-compat.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-compat.yml) |
| `examples-rag.yml` | [![examples-rag workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-rag.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-rag.yml) |
| `examples-spider.yml` | [![examples-spider workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-spider.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-spider.yml) |
| `examples-tinker.yml` | [![examples-tinker workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-tinker.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-tinker.yml) |
| `examples-unsloth.yml` | [![examples-unsloth workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-unsloth.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-unsloth.yml) |


## Links discovered
- [contrib](https://github.com/microsoft/agent-lightning/blob/main/contrib.md)
- [apo](https://github.com/microsoft/agent-lightning/blob/main/examples/apo.md)
- [![apo workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-apo.yml/badge.svg)
- [azure](https://github.com/microsoft/agent-lightning/blob/main/examples/azure.md)
- [![azure workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-azure.yml/badge.svg)
- [calc_x](https://github.com/microsoft/agent-lightning/blob/main/examples/calc_x.md)
- [![calc_x workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-calc-x.yml/badge.svg)
- [chartqa](https://github.com/microsoft/agent-lightning/blob/main/examples/chartqa.md)
- [![chartqa workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-chartqa.yml/badge.svg)
- [claude_code](https://github.com/microsoft/agent-lightning/blob/main/examples/claude_code.md)
- [![claude_code workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-claude-code.yml/badge.svg)
- [minimal](https://github.com/microsoft/agent-lightning/blob/main/examples/minimal.md)
- [![minimal workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml/badge.svg)
- [rag](https://github.com/microsoft/agent-lightning/blob/main/examples/rag.md)
- [![rag workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-rag.yml/badge.svg)
- [spider](https://github.com/microsoft/agent-lightning/blob/main/examples/spider.md)
- [![spider workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-spider.yml/badge.svg)
- [tinker](https://github.com/microsoft/agent-lightning/blob/main/examples/tinker.md)
- [![tinker workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-tinker.yml/badge.svg)
- [unsloth](https://github.com/microsoft/agent-lightning/blob/main/examples/unsloth.md)
- [![unsloth workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unsloth.yml/badge.svg)
- [![examples-apo workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-apo.yml/badge.svg)
- [![examples-azure workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-azure.yml/badge.svg)
- [![examples-calc-x workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-calc-x.yml/badge.svg)
- [![examples-chartqa workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-chartqa.yml/badge.svg)
- [![examples-claude-code workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-claude-code.yml/badge.svg)
- [![examples-compat workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-compat.yml/badge.svg)
- [![examples-rag workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-rag.yml/badge.svg)
- [![examples-spider workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-spider.yml/badge.svg)
- [![examples-tinker workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-tinker.yml/badge.svg)
- [![examples-unsloth workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-unsloth.yml/badge.svg)

--- examples/apo/README.md ---
# APO Example

[![apo CI status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-apo.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-apo.yml)

This example folder contains three complementary tutorials that demonstrate different aspects of Agent-Lightning. It's compatible with Agent-lightning v0.2 or later.

## Overview

The folder showcases three distinct use cases: using the built-in APO algorithm to train a room selection agent, creating custom training algorithms from scratch, and debugging agents effectively. Each tutorial is self-contained and demonstrates a specific workflow.

## Requirements

Follow the [installation guide](../../docs/tutorials/installation.md) to install Agent-Lightning and APO-extra dependencies. All examples also require an OpenAI-compatible API service.

## Included Files

| File/Directory | Description |
|----------------|-------------|
| `room_selector.py` | Room booking agent implementation using function calling |
| `room_selector_apo.py` | Training script using the built-in APO algorithm to optimize prompts |
| `room_tasks.jsonl` | Dataset with room booking scenarios and expected selections |
| `apo_custom_algorithm.py` | Tutorial on creating custom algorithms (runnable as algo or runner) |
| `apo_custom_algorithm_trainer.py` | Shows how to integrate custom algorithms into the Trainer |
| `apo_debug.py` | Tutorial demonstrating various agent debugging techniques |
| `legacy_apo_client.py` | Deprecated APO client implementation compatible with Agent-lightning v0.1.x |
| `legacy_apo_server.py` | Deprecated APO server implementation compatible with Agent-lightning v0.1.x |

## Sample 1: Using Built-in APO Algorithm

The `room_selector_apo.py` script demonstrates how to use Agent-Lightning's built-in APO (Asynchronous Prompt Optimization) algorithm to train a room booking agent. The agent learns to select meeting rooms based on duration, attendee count, equipment needs, accessibility requirements, and availability.

Run the training with:

```bash
python room_selector_apo.py
```

This script initializes the APO algorithm with beam search parameters, loads the room booking dataset, and optimizes the agent's prompt template through iterative training. The algorithm automatically manages the training loop, gradient computation, and prompt updates. Read more about this example in [Train the First Agent with APO](../../docs/how-to/train-first-agent.md).

## Sample 2: Creating Custom Algorithms

The `apo_custom_algorithm.py` and `apo_custom_algorithm_trainer.py` files teach you how to implement custom training algorithms from scratch. This is useful when the built-in algorithms don't fit your specific needs. See [Custom Algorithm tutorial](../../docs/how-to/write-first-algorithm.md) for more details.

### Option A: Run algorithm and runner separately

Start the store, algorithm, and runner in three separate terminals:

```bash
# Terminal 1: Start the store
agl store

# Terminal 2: Run the algorithm
python apo_custom_algorithm.py algo

# Terminal 3: Run the rollout runner
python apo_custom_algorithm.py runner
```

### Option B: Run integrated version

Use the integrated trainer that handles all components:

```bash
python apo_custom_algorithm_trainer.py
```

## Sample 3: Debugging Agents

The `apo_debug.py` script demonstrates multiple approaches to debugging agents in Agent-Lightning:

```bash
python apo_debug.py
```

Read more about this example in [Debugging Agents](../../docs/tutorials/debug.md).

## Appendix: Dataset Format

The `room_tasks.jsonl` file contains meeting scenarios with the following structure:

```json
{
  "id": "s01",
  "task_input": {
    "date": "2025-10-13",
    "time": "16:30",
    "duration_min": 30,
    "attendees": 12,
    "needs": ["projector", "confphone"],
    "accessible_required": true
  },
  "expected_choice": "Nova"
}
```


## Links discovered
- [![apo CI status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-apo.yml/badge.svg)
- [installation guide](https://github.com/microsoft/agent-lightning/blob/main/docs/tutorials/installation.md)
- [Train the First Agent with APO](https://github.com/microsoft/agent-lightning/blob/main/docs/how-to/train-first-agent.md)
- [Custom Algorithm tutorial](https://github.com/microsoft/agent-lightning/blob/main/docs/how-to/write-first-algorithm.md)
- [Debugging Agents](https://github.com/microsoft/agent-lightning/blob/main/docs/tutorials/debug.md)

--- examples/azure/README.md ---
# Supervised Fine-tuning with Azure OpenAI

[![azure CI status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-azure.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-azure.yml)

This example walks through an end-to-end supervised fine-tuning loop on Azure OpenAI. The trainer runs a toy capital-lookup agent, collects traces with rewards, submits fine-tuning jobs using those traces, and deploys every successful checkpoint as a new Azure OpenAI deployment.

**NOTE: The example is tested and compatible with Agent-lightning v0.2.x, but it's not yet maintained on CI due to the difficulty of maintaining a logged-in status in the testing environment.**

## Prerequisites

You need an Azure subscription with an Azure OpenAI resource that supports fine-tuning in your region and a base deployment you can reuse (the defaults assume `gpt-4.1-mini` backed by `gpt-4.1-mini-2025-04-14`). Sign in with the Azure CLI (`az login`) and install the project dependencies, for example via `uv sync` from the repository root.

## Setup

Copy the sample environment file `.env.example`, fill in your Azure values, and source it before running any scripts:

```bash
cp examples/azure_finetune/.env.example examples/azure_finetune/.env
# edit examples/azure_finetune/.env with your keys and identifiers
source examples/azure_finetune/.env
```

Confirm that you have successfully logged into Azure with:

```bash
az account show
```

## Included Files

| File | Description |
| --- | --- |
| `aoai_finetune.py` | Fine-tuning algorithm that batches rollouts, filters traces, launches jobs, deploys checkpoints, and evaluates them. |
| `train_capital_agent.py` | Trainer entry point that loads `capital_samples.csv` and orchestrates three fine-tuning iterations. |
| `capital_agent.py` | Tool-enabled agent that calls `country_capital_lookup`, producing reward `1.0` when the response contains the expected capital. |
| `capital_samples.csv` | Prompt/answer pairs that the trainer splits 80/20 into training and validation sets. |
| `tests/test_deployment.py` | Smoke tests for deployment helper methods when live Azure credentials are configured. |

## Workflow Overview

- **Stage 1 – Collect traces.** `Trainer` points runners at your base deployment and gathers rollouts in batches of `finetune_every_n_rollouts`.
- **Stage 2 – Filter and package data.** Rewards and telemetries from `capital_agent` are collected by Agent-lightning, which drives filtering via `data_filter_ratio`, and the remaining traces are serialized into Azure OpenAI JSONL format.
- **Stage 3 – Fine-tune.** `AzureOpenAIFinetune.finetune` uploads the dataset, waits for the fine-tuning job to finish, and returns the new base model identifier.
- **Stage 4 – Deploy and evaluate.** A versioned deployment such as `gpt-4.1-mini-ft_v01` is created, old deployments are pruned when `max_deployments` is exceeded, and validation rollouts confirm the reward.

The process is shown in the following diagram:

<p align="center">
  <img src="./assets/aoai_finetune.svg" alt="Azure OpenAI Finetune" style="width:100%"/>
</p>

## Capital Agent

`capital_agent.py` defines a tool-enabled agent that must call `country_capital_lookup` whenever a user asks for a capital city. The deterministic lookup table keeps the task simple, and the reward function checks that the final response contains the expected capital name. Run the script directly to validate credentials or debug tool call behavior:

```bash
python capital_agent.py
```

The agent executes five sample tasks, prints each tool interaction, and records traces via the Agent Lightning tracer.

## Running the Example

Start the full fine-tuning loop from the repository root:

```bash
python train_capital_agent.py
```

`train_capital_agent.py` divides the dataset into training and validation subsets, then completes three fine-tune → deploy → evaluate iterations. Expect short rollout times paired with longer waits (up to 4 hours in our experiments) for Azure’s fine-tuning queue; deployments usually reach `Succeeded` within 2-3 minutes. The console output looks like this:

```log
10:13:02,624 Starting client-server execution with 2 runner(s) [role=both, main_process=algorithm]
10:13:02,639 Starting LightningStore server on localhost:4747
10:13:02,749 [AOAI FT 1/3] [Stage 1] Starting fine-tuning iteration with 24 tasks...
10:13:02,750 [AOAI FT 1/3] [Stage 2] Using model deployment: gpt-4.1-mini
10:13:03,428 [Worker 1] Started async rollouts (max: unlimited).
10:13:03,429 [Worker 0] Started async rollouts (max: unlimited).
10:13:05,279 [Worker 0 | Rollout ro-efab388d2f0e] Completed in 1.83s. Collected 4 span(s). Final reward: 1.0
10:13:05,454 [Worker 1 | Rollout ro-8ba08859ae85] Completed in 2.01s. Collected 4 span(s). Final reward: 1.0
[... 22 more rollouts omitted ...]
10:13:28,430 [AOAI FT 1/3] [Stage 3] Completed rollouts for 24 tasks.
10:13:28,431 [AOAI FT 1/3] Keeping 28 example(s) for fine-tuning after reward-based filtering.
10:13:28,431 [AOAI FT 1/3] [Stage 4] Prepared 28 training examples after filtering.
10:13:28,431 [AOAI FT 1/3] [Stage 5] Starting fine-tuning for model gpt-4.1-mini-2025-04-14...
10:13:29,854 [AOAI FT 1/3] Uploaded training file to Azure OpenAI (file_id=file-0fd6e72151094a0eb0306de7aae4883b).
10:13:41,216 [AOAI FT 1/3] Fine-tuning job ftjob-0ee45c42591b4f4a8bd4f49ef2301dcd created for base model gpt-4.1-mini-2025-04-14.
10:13:41,217 [AOAI FT 1/3] Waiting for fine-tuning job ftjob-0ee45c42591b4f4a8bd4f49ef2301dcd to complete.
12:29:11,444 [AOAI FT 1/3] Fine-tuning job ftjob-0ee45c42591b4f4a8bd4f49ef2301dcd succeeded with new model id gpt-4.1-mini-2025-04-14.ft-0ee45c42591b4f4a8bd4f49ef2301dcd-v01.
12:29:11,444 [AOAI FT 1/3] [Stage 6] Deploying fine-tuned model...
12:29:14,217 [AOAI FT 1/3] Waiting for deployment gpt-4.1-mini-ft_v01 to become ready.
12:29:15,458 [AOAI FT 1/3] Waiting for deployment to be ready. Current provisioning state of gpt-4.1-mini-ft_v01: Creating
[... 7 repetitive deployment status checks omitted ...]
12:32:53,773 [AOAI FT 1/3] Waiting for deployment to be ready. Current provisioning state of gpt-4.1-mini-ft_v01: Succeeded
12:32:53,773 [AOAI FT 1/3] Deployment gpt-4.1-mini-ft_v01 is ready with version 1.
12:32:53,774 [AOAI FT 1/3] [Stage 7] Evaluating on validation dataset...
[... 8 validation rollouts omitted ...]
12:33:03,979 [AOAI FT 1/3] [Stage 7] Evaluation completed. Average reward: 1.0000
12:33:03,979 [AOAI FT 2/3] [Stage 1] Starting fine-tuning iteration with 24 tasks...
12:33:03,979 [AOAI FT 2/3] [Stage 2] Using model deployment: gpt-4.1-mini-ft_v01
[... 24 rollouts omitted ...]
12:33:34,619 [AOAI FT 2/3] [Stage 3] Completed rollouts for 24 tasks.
12:33:34,620 [AOAI FT 2/3] [Stage 4] Prepared 27 training examples after filtering.
12:33:34,620 [AOAI FT 2/3] [Stage 5] Starting fine-tuning for model gpt-4.1-mini-2025-04-14.ft-0ee45c42591b4f4a8bd4f49ef2301dcd-v01...
12:35:12,694 [AOAI FT 2/3] Waiting for fine-tuning job ftjob-06366e441ee24a0ea242014fea8fbc3a to complete.
13:16:43,810 [AOAI FT 2/3] Fine-tuning job ftjob-06366e441ee24a0ea242014fea8fbc3a succeeded with new model id gpt-4.1-mini-2025-04-14.ft-06366e441ee24a0ea242014fea8fbc3a-v02.
13:16:43,810 [AOAI FT 2/3] [Stage 6] Deploying fine-tuned model...
13:16:46,263 [AOAI FT 2/3] Waiting for deployment gpt-4.1-mini-ft_v02 to become ready.
[... 5 repetitive deployment status checks omitted ...]
13:19:23,856 [AOAI FT 2/3] Waiting for deployment to be ready. Current provisioning state of gpt-4.1-mini-ft_v02: Succeeded
13:19:23,857 [AOAI FT 2/3] [Stage 7] Evaluating on validation dataset...
[... 8 validation rollouts omitted ...]
13:19:39,072 [AOAI FT 2/3] [Stage 7] Evaluation completed. Average reward: 1.0000
13:19:39,072 [AOAI FT 3/3] [Stage 1] Starting fine-tuning iteration with 24 tasks...
13:19:39,073 [AOAI FT 3/3] [Stage 2] Using model deployment: gpt-4.1-mini-ft_v02
[... 24 rollouts omitted ...]
13:20:04,721 [AOAI FT 3/3] [Stage 3] Completed rollouts for 24 tasks.
13:20:04,722 [AOAI FT 3/3] [Stage 4] Prepared 27 training examples after filtering.
13:20:04,722 [AOAI FT 3/3] [Stage 5] Starting fine-tuning for model gpt-4.1-mini-2025-04-14.ft-06366e441ee24a0ea242014fea8fbc3a-v02...
13:20:17,013 [AOAI FT 3/3] Waiting for fine-tuning job ftjob-2651d3183a4b40679d4c3fc886940c0c to complete.
14:02:47,241 [AOAI FT 3/3] Fine-tuning job ftjob-2651d3183a4b40679d4c3fc886940c0c succeeded with new model id gpt-4.1-mini-2025-04-14.ft-2651d3183a4b40679d4c3fc886940c0c-v03.
14:02:47,242 [AOAI FT 3/3] [Stage 6] Deploying fine-tuned model...
14:02:47,242 [AOAI FT 3/3] Maximum number of deployments reached (2). Cleaning up old deployments.
14:02:47,242 [AOAI FT 3/3] Deleting old deployment gpt-4.1-mini-ft_v01.
14:02:48,925 [AOAI FT 3/3] Deployment gpt-4.1-mini-ft_v01 deleted successfully.
14:02:51,168 [AOAI FT 3/3] Waiting for deployment gpt-4.1-mini-ft_v03 to become ready.
[... 7 repetitive deployment status checks omitted ...]
14:06:30,300 [AOAI FT 3/3] Waiting for deployment to be ready. Current provisioning state of gpt-4.1-mini-ft_v03: Succeeded
14:06:30,301 [AOAI FT 3/3] [Stage 7] Evaluating on validation dataset...
[... 8 validation rollouts omitted ...]
14:06:45,506 [AOAI FT 3/3] [Stage 7] Evaluation completed. Average reward: 1.0000
14:06:45,506 Stopping server...
14:06:45,657 Server stopped.
```

## Tips and Cleanup

Tweak `finetune_every_n_rollouts`, `max_deployments`, and `data_filter_ratio` in `train_capital_agent.py` to align with your quotas. While jobs run, visit the Azure OpenAI portal to confirm status. When you are done, delete unused deployments there.


## Links discovered
- [![azure CI status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-azure.yml/badge.svg)

--- examples/calc_x/README.md ---
# Calc-X Example

[![calc_x CI status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-calc-x.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-calc-x.yml)

This example demonstrates training a mathematical reasoning agent using Agent-Lightning with the VERL algorithm and AutoGen framework. The agent solves math problems using a calculator tool through the Model Context Protocol (MCP). It's compatible with Agent-lightning v0.2 or later.

## Requirements

This example requires a single node with at least one 40GB GPU. Follow the [installation guide](../../docs/tutorials/installation.md) to install Agent-Lightning and VERL-related dependencies.

Additionally, ensure `uv` and the MCP calculator server are properly installed. The agent relies on the MCP protocol to access calculator functionality during problem-solving.

```bash
pip install "autogen-agentchat" "autogen-ext[openai]" "mcp>=1.10.0"
```

## Dataset

Download the Calc-X dataset in parquet format from [here](https://drive.google.com/file/d/1FQMyKLLd6hP9dw9rfZn1EZOWNvKaDsqw/view?usp=sharing) and extract it to the `data` folder:

```bash
unzip calc-x-data.zip -d data
```

The dataset contains mathematical problems with ground truth solutions for training and evaluation.

## Included Files

| File/Directory | Description |
|----------------|-------------|
| `calc_agent.py` | Math problem-solving agent using AutoGen and MCP calculator tool |
| `train_calc_agent.py` | Training script using VERL algorithm with configurable hyperparameters |
| `eval_utils.py` | Evaluation utilities for assessing agent accuracy on math problems |
| `data/` | Directory containing training and test datasets in parquet format |
| `tests/` | Test files including MCP calculator verification script |
| `legacy_calc_agent.py` | Legacy agent implementation compatible with Agent-lightning v0.1.x (deprecated) |
| `legacy_calc_agent_debug.py` | Legacy debugging script compatible with Agent-lightning v0.1.x (deprecated) |
| `legacy_train.sh` | Legacy training script compatible with Agent-lightning v0.1.x (deprecated) |

## Running Examples

### Training

The training process uses distributed Ray workers to run agent rollouts in parallel while the training server optimizes the model. Start Ray before launching the training:

```bash
bash ../../scripts/restart_ray.sh
```

If you want to track experiments with Weights & Biases, set the `WANDB_API_KEY` environment variable **before starting Ray**.

Then run the training script:

```bash
python train_calc_agent.py --train-file data/train.parquet --val-file data/test.parquet
```

The script automatically launches agent workers and the training server. The agent workers execute math problem rollouts using the MCP calculator, while the training server applies the VERL algorithm to improve the model based on rewards.

### Debugging

To test the agent interactively without training:

```bash
python calc_agent.py
```

This runs the agent on sample problems to verify that the MCP calculator integration and AutoGen setup work correctly. This test relies on an OpenAI service available. Set `OPENAI_API_KEY` environment variable to the API key of the OpenAI service; and `OPENAI_API_BASE` environment variable to the base URL of the OpenAI service.

A very common issue is that the agent may hang indefinitely if the environment is not properly configured. Verify that `uv` and the MCP calculator server are correctly installed by running:

```bash
python tests/test_mcp_calculator.py
```


## Links discovered
- [![calc_x CI status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-calc-x.yml/badge.svg)
- [installation guide](https://github.com/microsoft/agent-lightning/blob/main/docs/tutorials/installation.md)
- [here](https://drive.google.com/file/d/1FQMyKLLd6hP9dw9rfZn1EZOWNvKaDsqw/view?usp=sharing)

--- examples/chartqa/README.md ---
# ChartQA Example

[![chartqa workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-chartqa.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-chartqa.yml)

This example demonstrates training a visual reasoning agent on the ChartQA dataset using Agent-Lightning with the VERL algorithm and LangGraph framework. The agent answers questions about charts through a multi-step workflow with self-refinement.

## Requirements

This example requires a single node with at least one 40GB GPU. Install dependencies with:

```bash
uv sync --frozen \
    --group dev \
    --group experiment \
    --group image \
    --group langchain \
    --group vllm-0-10-2 \
    --group torch-gpu-stable
```

**Currently vLLM 0.10.2 is the only tested version. You might see issues like `cu_seqlens_q must be on CUDA` or flash-attn installation failures if you use other versions.** (See https://github.com/vllm-project/vllm/issues/27340)

## Dataset

Download the ChartQA dataset and prepare it for training:

```bash
cd examples/chartqa
python prepare_data.py
```

This downloads the ChartQA dataset from HuggingFace (`HuggingFaceM4/ChartQA`), saves images locally, and creates parquet files for training/testing. No HuggingFace token is required (public dataset).

**Dataset Statistics:**

- Training: ~18,000 chart question-answer pairs
- Test: ~2,500 pairs
- Chart types: Bar, line, pie, scatter, etc.

## Included Files

| File/Directory | Description |
|----------------|-------------|
| `chartqa_agent.py` | Chart reasoning agent using LangGraph with multi-step workflow (observe → extract → calculate → check → refine) |
| `train_chartqa_agent.py` | Training script using VERL algorithm with configurable hyperparameters (debug, qwen) |
| `debug_chartqa_agent.py` | Debugging script to test the agent with cloud APIs or a local vLLM proxy |
| `prepare_data.py` | Script to download ChartQA dataset from HuggingFace and prepare parquet files |
| `prompts.py` | Prompt templates for the agent workflow |
| `multimodal_utils.py` | Utility functions for encoding images to base64 |
| `env_var.py` | Environment variables and configurations |
| `data/` | Directory containing images and parquet files after download |

## Running Examples

### Debugging with Cloud API (Default)

For quick testing with OpenAI or other cloud APIs (no local GPU required):

```bash
export OPENAI_API_KEY=<your-api-key>
python debug_chartqa_agent.py
```

For other providers (Azure, etc.), set `OPENAI_API_BASE`:

```bash
export OPENAI_API_BASE=https://your-resource.openai.azure.com/v1
export OPENAI_MODEL=gpt-4o
python debug_chartqa_agent.py
```

### Debugging with Local Model (LLMProxy)

To test the agent with a local vLLM server and LLMProxy:

```bash
# Start a vLLM server (specify image path for VLM)
export CHARTQA_DATA_DIR=<path to chartqa data>
vllm serve Qwen/Qwen2-VL-2B-Instruct \
    --gpu-memory-utilization 0.6 \
    --max-model-len 4096 \
    --allowed-local-media-path $CHARTQA_DATA_DIR \
    --enable-prefix-caching \
    --port 8088

# Run the agent with LLMProxy
USE_LLM_PROXY=1 \
    OPENAI_API_BASE=http://localhost:8088/v1 \
    OPENAI_MODEL=Qwen/Qwen2-VL-2B-Instruct \
    python debug_chartqa_agent.py
```

### Training with Local Model

```bash
python train_chartqa_agent.py debug --n-runners 2
```

You can also use an external store server (recommended for distributed setups), first start the store:

```bash
agl store --port 4747
```

Then run the training script with the external store address:

```bash
AGL_MANAGED_STORE=0 python train_chartqa_agent.py qwen --external-store-address http://localhost:4747
```

If you want to track experiments with Weights & Biases, set the `WANDB_API_KEY` environment variable before training.

The script automatically launches agent workers and the training server. The agent workers execute chart reasoning rollouts using the vision-language model, while the training server applies the VERL algorithm (GRPO) to improve the model based on answer accuracy rewards.


## Links discovered
- [![chartqa workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-chartqa.yml/badge.svg)

--- examples/claude_code/README.md ---
# Training Claude Code with Agent-lightning

[![claude-code CI status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-claude-code.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-claude-code.yml)

This example shows how to wrap Anthropic's Claude Code experience with Agent-lightning instrumentation to solve SWE-bench tasks, collect spans/logs, and optionally convert those traces into HuggingFace datasets.

**NOTE:** This example only shows how to integrate Claude Code as an agent in Agent-lightning. The training part is still under development and welcoming contributions!

## Overview

`claude_code_agent.py` spins up a Lightning Store, an LLM proxy, and the Claude Code controller. Each SWE-bench instance is executed inside the official container image so you can either prompt-tune against Anthropic's hosted models or point Claude Code at a self-hosted OpenAI-compatible backend such as vLLM. When a backend surfaces token IDs/logprobs (e.g., vLLM), the traces are turned into triplets that downstream fine-tuning pipelines can consume.

## Requirements

First, install Agent-lightning following the [installation guide](https://microsoft.github.io/agent-lightning/stable/tutorials/installation/). Then install the SWE-bench harness plus utilities used by this example:

```bash
(uv) pip install swebench transformers datasets python-dotenv
```

Docker must be available because each SWE-bench instance is executed in a container via `swebench_utils`.

Finally, set API credentials depending on backend:

- `ANTHROPIC_API_KEY` for the official Claude Code path.
- `OPENAI_API_KEY` (or another OpenAI-compatible key) for the `openai` backend.
- A running OpenAI-compatible server (e.g., vLLM) when using the `vllm` backend.

## Dataset

`swebench_samples.jsonl` contains a handful of SWE-bench issues for smoke testing. For full-scale benchmarks load `princeton-nlp/SWE-bench` via `load_swebench_dataset` or point `--dataset-path` to your own JSONL file.

## Included Files

| File/Directory | Description |
|----------------|-------------|
| `claude_code_agent.py` | CLI entry point that launches the Lightning store, LLM proxy, and Claude Code agent |
| `claude_code_controller.py` | Manages the SWE-bench Docker runtime and translates model outputs into git patches |
| `extended_adapter.py` | Adapter that converts LLM proxy spans into triplets with token IDs, logprobs, and chat history |
| `swebench_samples.jsonl` | Mini SWE-bench subset for quick validation |
| `swebench_utils/` | Utilities for running/evaluating SWE-bench instances inside containers |
| `templates/handle_hook.template.sh` | Helper script injected into containers for hook handling |
| `templates/settings.template.json` | Base configuration consumed by Claude Code CLI |

## Running the Example

All commands are issued from `examples/claude_code`. Inspect the module-level docstring in `claude_code_agent.py` for the full CLI reference.

### Hosted vLLM (open-source models)

First, launch your model behind an OpenAI-compatible endpoint, for example:

```bash
vllm serve Qwen/Qwen3-Coder-30B-A3B-Instruct \
    --max-model-len 131072 \
    --enable-auto-tool-choice \
    --tool-call-parser qwen3_coder
```

Run the Agent-lightning harness and point it at the server:

```bash
python claude_code_agent.py vllm \
    --backend-model-high Qwen/Qwen3-Coder-30B-A3B-Instruct \
    --backend-model-low Qwen/Qwen3-Coder-30B-A3B-Instruct \
    --frontend-model-high claude-sonnet-4-5-20250929 \
    --frontend-model-low claude-haiku-4-5-20251001 \
    --base-url http://localhost:8000/v1 \
    --dataset-path swebench_samples.jsonl \
    --output-dir data_debug \
    --max-turns 5 \
    --limit 2
```

The backend model names must match what the server exposes. Because this mode surfaces token IDs/logprobs, the script saves both raw span logs and HuggingFace datasets per instance.

### Official Claude Code (Anthropic API)

```bash
export ANTHROPIC_API_KEY=sk-...
python claude_code_agent.py anthropic \
    --dataset-path swebench_samples.jsonl \
    --output-dir data_anthropic \
    --frontend-model-high claude-sonnet-4-5-20250929 \
    --frontend-model-low claude-haiku-4-5-20251001
```

Backend model flags are optional here because the Anthropic API strings match the frontend names. This path is ideal for validating prompts against the hosted experience (trace outputs do not contain token IDs or logprobs).

### OpenAI-Compatible Providers

```bash
export OPENAI_API_KEY=sk-...
python claude_code_agent.py openai \
    --backend-model-high gpt-4.1 \
    --backend-model-low gpt-4o-mini \
    --dataset-path swebench_samples.jsonl \
    --output-dir data_openai
```

Use this mode whenever Claude Code should talk to Azure OpenAI, OpenAI, or another compatible provider. `--base-url` is optional—pass it if your endpoint differs from the public OpenAI URL.

Adjust `--max-turns`, `--cooldown-seconds`, and `--limit` to control runtime and rate limits regardless of backend.

## Outputs and Trace Collection

- `output_dir/stream_<instance_id>.json` contains the complete span stream captured from the Lightning Store for each rollout.
- When running with `backend_type=vllm`, `output_dir/dataset-<instance_id>/` stores a HuggingFace dataset with token IDs, logprobs, prompts, and metadata produced by `ExtendedLlmProxyTraceToTriplet`.
- `logs/<instance_id>/` is created by the SWE-bench runtime and mirrors the console output from the container.
- Return values from the agent are also evaluated via `swebench_utils.evaluation.evaluate`, so `data_debug` (or your chosen folder) will contain evaluation reports alongside traces.

Use these artifacts to fine-tune models, debug Claude Code behavior, or replay rollouts in downstream Agent-lightning workflows.


## Links discovered
- [![claude-code CI status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-claude-code.yml/badge.svg)
- [installation guide](https://microsoft.github.io/agent-lightning/stable/tutorials/installation/)

--- examples/minimal/README.md ---
# Minimal Component Showcase

[![minimal CI status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml)

`examples/minimal` provides bite-sized programs that demonstrate how individual Agent-lightning building blocks behave in isolation.

Each module have been documented with its own CLI usage in the module-level docstring. Use this directory as a reference when wiring the same pieces into a larger system.

## What’s Included?

| Component | Demonstrated In | Highlights |
| --- | --- | --- |
| LightningStore + OTLP ingestion | `write_traces.py` | Shows how `OtelTracer` and `AgentOpsTracer` open rollouts, emit spans, and optionally forward them to a remote store client. |
| MultiMetrics backend | `write_metrics.py` | Emits counters/histograms through `ConsoleMetricsBackend` and `PrometheusMetricsBackend` simultaneously, exposing `/metrics` for scraping. |
| LLM proxying | `llm_proxy.py` | Guards either OpenAI or a local vLLM deployment with `LLMProxy`, proving how requests are routed through `/rollout/<id>/attempt/<id>` namespaces and captured in the store. |
| vLLM lifecycle | `vllm_server.py` | Minimal context manager that shells out to `vllm serve`, monitors readiness, and tears down the process safely. |

All runtime instructions (CLI arguments, required environment variables, etc.) are embedded directly in each script’s top-level docstring so the source stays self-documenting.

For full-fledged training workflows or multi-component experiments, browse the other subdirectories under `examples/`. This `minimal` folder deliberately keeps each demonstration focused on a single component so you can understand and test them independently.


## Links discovered
- [![minimal CI status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml/badge.svg)

--- examples/rag/README.md ---
# RAG Agent Example

[![rag workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-rag.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-rag.yml)

This example demonstrates training a Retrieval-Augmented Generation (RAG) agent using Agent-Lightning with retrieval capabilities. The agent answers multi-hop questions from a tiny MuSiQue dataset by retrieving and reasoning over Wikipedia passages.

## Overview

This example can run on a single GPU for demonstration purposes.

**Step 1:** Set up the environment. It is recommended to setup with uv and activate the virtual environment with:

```bash
uv sync --frozen --extra apo --group agents --group torch-gpu-stable --extra verl --group rag
source .venv/bin/activate
```

**Step 2:** Prepare the tiny dataset.

```bash
pip install gdown

# tiny training dataset
cd examples/rag
gdown --fuzzy "https://drive.google.com/file/d/1Pq4Ag8zVoN8gUtLu0LcBfY35Dm5zL0hq/view?usp=drive_link" \
    -O dataset_tiny.parquet

# chunks_candidate_tiny.pkl
gdown --fuzzy "https://drive.google.com/file/d/1REXCpRLbeZu1KfWWKhIGEQe_WNHUOBkS/view?usp=drive_link" \
    -O chunks_candidate_tiny.pkl

# index_hnsw_faiss_n32e40_tiny.index
gdown --fuzzy "https://drive.google.com/file/d/1f6P-h_8KSRhe5pqDHWbRQWvUhTygfZ-c/view?usp=drive_link" \
    -O index_hnsw_faiss_n32e40_tiny.index
```

**Step 3:** Start the MCP server. Open a terminal and run:

```bash
python wiki_retriever_mcp.py
```

**Step 4:** Start training. Open another terminal and run:

```bash
python train_rag.py
```

## Included Files

| File/Directory | Description |
|----------------|-------------|
| `rag_agent.py` | RAG agent example using the OpenAI Agents SDK, with debugging utils |
| `train_rag.py` | Initiates the GRPO training process |
| `metric_utils.py` | Scoring utilities for exact match, F1 score, and response parsing |
| `wiki_retriever_mcp.py` | MCP server for Wikipedia retrieval |

## How to Prepare the Retrieval Corpus Yourself

To enable semantic retrieval with this MCP server, you need two files:

1. **FAISS index file** (`.index`)
2. **Chunk list file** (`.pkl`)

These two files work together: the FAISS index stores the vector embeddings and their mapping to integer IDs, while the pickle file stores the actual text chunks. The integer IDs in the index correspond exactly to the positions in the chunk list.

### Step 1: Collecting Text Chunks

First, you need a collection of text passages (chunks). For example, you can download a Wikipedia-based dataset such as `wiki18_100w.zip` from the [FlashRAG_dataset](https://huggingface.co/datasets/FlashRAG) or use other pre-split corpora.

### Step 2: Creating the FAISS Index (`nq_hnsw_faiss_n32e40.index`)

- Use a sentence embedding model (e.g., `BAAI/bge-large-en-v1.5`) to encode each chunk into a vector.
- Build a FAISS index from these vectors.
- In this example, we use an **HNSW index** (Hierarchical Navigable Small World graph), which supports efficient approximate nearest-neighbor search.
- The index stores only embeddings and integer IDs (no raw text).

### Step 3: Creating the Chunk List (`nq_list.pkl`)

- Store the raw text chunks in a Python list.
- Save this list with `pickle`.
- The index ID returned by FAISS corresponds to the list index in this file. For example, if FAISS search returns `I[0][i] = 12345`, then the corresponding text chunk is `chunks[12345]`.

### Example Schema

- **`nq_hnsw_faiss_n32e40.index`**
  - Type: FAISS HNSW index
  - Contains:
    - Vector embeddings
    - Graph structure for fast search
    - Integer IDs mapping to chunk positions

- **`nq_list.pkl`**
  - Type: Pickled Python list
  - Element type: string (or dict with text + metadata, depending on preprocessing)
  - Example:
    ```python
    [
        "The Eiffel Tower is located in Paris, France.",
        "Albert Einstein developed the theory of relativity.",
        ...
    ]
    ```

### Step 4: Code Example - Building Index and Chunk List

**Warning:** The following example demonstrates a small-scale workflow only. In practice, for large datasets, you should encode the text in batches and incrementally add them to the index.

```python
import faiss
import pickle
from sentence_transformers import SentenceTransformer

# 1. Prepare your text chunks (list of strings)
chunk_texts = [
    "The Eiffel Tower is located in Paris, France.",
    "Albert Einstein developed the theory of relativity.",
    "Python is a popular programming language.",
    # ... more chunks
]

# 2. Load embedding model
model = SentenceTransformer("BAAI/bge-large-en-v1.5")

# 3. Encode text chunks into embeddings
embeddings = model.encode(chunk_texts, normalize_embeddings=True)

# 4. Build FAISS HNSW index
dim = embeddings.shape[1]
index = faiss.IndexHNSWFlat(dim, 32)   # 32 neighbors by default
index.hnsw.efConstruction = 40         # efConstruction parameter
index.add(embeddings)

# 5. Save FAISS index
faiss.write_index(index, "nq_hnsw_faiss_n32e40.index")

# 6. Save chunk list
with open("nq_list.pkl", "wb") as f:
    pickle.dump(chunk_texts, f)

print("Index and chunk list saved successfully.")
```


## Links discovered
- [![rag workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-rag.yml/badge.svg)
- [FlashRAG_dataset](https://huggingface.co/datasets/FlashRAG)

--- examples/spider/README.md ---
# Spider Example

[![spider CI status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-spider.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-spider.yml)

This example demonstrates how to train a text-to-SQL agent on the Spider dataset using Agent-Lightning with reinforcement learning. It's compatible with Agent-lightning v0.2 or later.

## Requirements

This example depends on LangChain v0.x and several SQL-related libraries. Install the required dependencies with:

```bash
pip install "langgraph<1.0" "langchain[openai]<1.0" "langchain-community" "langchain-text-splitters<1.0" "sqlparse" "nltk"
```

Additionally, follow the [installation guide](../../docs/tutorials/installation.md) to install Agent-Lightning and VERL-related dependencies.

## Dataset

Detailed dataset preparation instructions are available in the [How to Train a SQL Agent](../../docs/how-to/train-sql-agent.md) guide.

## Included Files

| File/Directory | Description |
|----------------|-------------|
| `train_sql_agent.py` | Training script for SQL agents with support for multiple model configurations (Qwen, LLaMA, fast mode for CI) |
| `sql_agent.py` | SQL agent implementation using LangGraph and LangChain, with debugging capabilities |
| `data/` | Directory containing the Spider dataset files |
| `spider_eval/` | Evaluation utilities for assessing SQL agent performance |

## Running Examples

### Training

Train a SQL agent using the Qwen2.5-Coder-1.5B-Instruct model with the following command. This requires a single node with at least one 40GB GPU:

```bash
python train_sql_agent.py qwen
```

If you want to use an NPU for training, please refer to the **Launch Training with NPUS** section in [How to Train a SQL Agent](../../docs/how-to/train-sql-agent.md).

### Debugging

To test and debug the SQL agent interactively:

```bash
python sql_agent.py
```

This command requires an OpenAI-compatible API service. Configure your service endpoint and credentials using the `OPENAI_API_BASE` and `OPENAI_API_KEY` environment variables.


## Links discovered
- [![spider CI status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-spider.yml/badge.svg)
- [installation guide](https://github.com/microsoft/agent-lightning/blob/main/docs/tutorials/installation.md)
- [How to Train a SQL Agent](https://github.com/microsoft/agent-lightning/blob/main/docs/how-to/train-sql-agent.md)

--- examples/tinker/README.md ---
# Tinker + Agent-lightning Integration

[![tinker CI status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-tinker.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/examples-tinker.yml)

This example shows how to use [Tinker's reinforcement-learning infrastructure](https://tinker-docs.thinkingmachines.ai/) as a fine-tuning backend for agents written against Agent-lightning. You author the agent exactly the way you would for deployment, while the bridge code reconstructs Tinker-compatible trajectories from Agent-lightning traces.

## How this differs from the original Tinker Cookbook RL recipe

Real-world agent apps orchestrate logic in familiar frameworks (CrewAI, LangChain, AutoGen, OpenAI Agents, etc.) or by calling OpenAI-compatible REST APIs. A simple number-guessing agent might look like this:

```python
def guess_number_agent():
    client = openai.OpenAI()
    messages = [{"role": "user", "content": "Guess a number between 1 and 100."}]
    for _ in range(MAX_TURNS):
        response = client.chat.completions.create(model="gpt-4.1", messages=messages)
        response_content = response.choices[0].message.content
        messages.append({"role": "assistant", "content": response_content})
        guessed_number = extract_number(response_content)
        if guessed_number == gold_answer:
            return 1.0
        elif guessed_number < gold_answer:
            messages.append({"role": "user", "content": "Too low"})
        else:
            messages.append({"role": "user", "content": "Too high"})
    return 0.0
```

The reference [Tinker Cookbook example](https://github.com/thinking-machines-lab/tinker-cookbook/tree/51d9e8226f2dcf82ceac272c734a5f6e3b4f0203/tinker_cookbook/recipes/multiplayer_rl/guess_number), however, expects you to rewrite the same logic into a callback-style `Env`, and it creates a simple loop to iterate between a language model (`TokenCompleter`) and the `Env`.

```python
class GuessNumberEnv:
    def __init__(self, gold_answer: int):
        self.system_prompt: Message = {"role": "system", "content": SYSTEM_PROMPT}
        self.turns: list[Message] = []
        self.gold_answer: int = gold_answer

    async def initial_observation(self) -> list[int]:
        return message_to_tokens(self.system_prompt)

    async def step(self, action_tokens: list[int]) -> tuple[list[int], float, bool]:
        action_message = tokens_to_message(action_tokens)
        guessed_number = extract_number(action_message["content"])

        if guessed_number == self.gold_answer:
            text, reward = "Correct", 1.0
        elif guessed_number < self.gold_answer:
            text, reward = "Too low", 0.0
        else:
            text, reward = "Too high", 0.0

        self.turns.append(action_message)
        self.turns.append({"role": "assistant", "content": text})
        episode_done = reward == 1 or len(self.turns) // 2 >= MAX_TURNS
        return message_to_tokens(self.turns), reward, episode_done
```

As agents grow more complex, writing them in callback style becomes increasingly painful. You have to break the control flow whenever an LLM call is required, which fragments the code and makes it harder to maintain.

Agent-lightning hides that translation step: you keep the first style for development and production, while the framework queues tasks to the store, rebuilds trajectories from spans, and feeds them to the training loop. This example shows how to make Tinker's original training loop work with Agent-lightning.

## Included files

| Path | Purpose |
| ---- | ------- |
| `hello.py` | Minimal end-to-end fine-tuning example. Trains a model to repeat small identity strings. |
| `q20_agent.py` | CrewAI flow that powers the 20 Questions player, answerer, and mock search tool. Shared by training and evaluation. **Unrelated to Agent-lightning or Tinker.** |
| `q20_train.py` | Reinforcement-learning driver that adapts the Cookbook loop to Agent-lightning rollouts. Supports dry-run, distributed training, and search tool toggles. **Related to both Agent-lightning and Tinker.** |
| `q20_evaluate.py` | Offline evaluator that reuses the CrewAI flow to benchmark any OpenAI- or Qwen-backed model against the provided dataset. **Related to Tinker only.** |
| `q20_nouns.csv` | Categories and answers used for training and validation. Contains `split` and `search_enabled` metadata. |
| `agl_tinker/` | Bridge package for integrating Agent-lightning with Tinker (see breakdown below). |
| `tests/test_tinker_llm.py` | Sanity tests for the custom LiteLLM provider. Run with `pytest examples/tinker/tests`. |
| `.env.example` | Template for environment variables required by LiteLLM, CrewAI helpers, and the hosted Tinker service. |

`agl_tinker/` components:

| Path | Purpose |
| ---- | ------- |
| `agl_tinker/algo.py` | Agent-lightning `Algorithm` wrapper that plugs the training loop into `agl.Trainer`. |
| `agl_tinker/env.py` | Dummy env and dataset builders that adapt Agent-lightning tasks to Tinker expectations. |
| `agl_tinker/llm.py` | LiteLLM custom provider backed by the Tinker sampling client. |
| `agl_tinker/rollout.py` | Span-to-trajectory reconstruction and rollout batching helpers. |
| `agl_tinker/train.py` | RL training loop adapted from the Tinker Cookbook. |

## Setup

**1. Install dependencies.** From the repo root:

```bash
uv sync --frozen --extra apo --group dev --group agents --group tinker
```

If you are not using `uv`, make sure `tinker`, `tinker_cookbook`, `litellm`, `crewai`, and Agent-lightning are available in the same environment.

**2. Copy the environment template and fill in credentials:**

```bash
cp examples/tinker/.env.example examples/tinker/.env
```

- `OPENAI_API_KEY` / `OPENAI_BASE_URL`: routes helper agents (answerer, search, tool simulations) through a LiteLLM or OpenAI-compatible endpoint.
- `TINKER_API_KEY`: required to talk to the hosted Tinker training service. Skip if you are using OpenAI models only.
- `WANDB_API_KEY`: optional, enables Weights & Biases logging when configured in `q20_train.py`.
- `CREWAI_DISABLE_TELEMETRY=true`: keeps CrewAI from emitting its own telemetry so that Agent-lightning tracing stays coherent.

3. Load the environment before running commands, e.g. `dotenv run -- <command>` or export the variables manually.

## Running the Hello 1024 example

This is the quickest way to see the integration in action. It fine-tunes a Qwen model so it introduces itself with the target identity.

**One-click workflow (spawns store, algorithm, and runners in a single process)**

```bash
dotenv run python hello.py oneclick
```

The script will pick free ports for the LiteLLM proxy and Agent-lightning store, then iterate through the synthetic dataset of identities.

**Distributed workflow (useful for inspecting each component)**

```bash
agl store --port 4747
dotenv run python hello.py algo
dotenv run python hello.py runner
```

Start the commands in separate terminals. The algorithm process connects to the existing store, while the runner process launches eight worker processes by default. Logs are written to `examples/tinker/logs/hello`.

## Training the 20 Questions agent

The 20 Questions setup mirrors the official Cookbook recipe but drives rollouts through the shared CrewAI flow.

**Dry run (in-memory store and LiteLLM proxy)**

```bash
dotenv run python q20_train.py dryrun
```

Useful to verify that the CrewAI flow, reward emission, and span reconstruction succeed on a handful of samples without touching the hosted Tinker service.

**Full distributed training**

```bash
agl store --port 4747
dotenv run python q20_train.py algo --model qwen30b --search --port 4747
dotenv run python q20_train.py runner --port 4747 --n-runners 4
```

`--model` selects the Tinker-hosted checkpoint (`qwen4b` or `qwen30b`). Add `--search` to enable the mocked search tool, which relies on the helper LLM defined in the environment variables (the example uses an LLM-powered search simulation instead of a real API). Training metrics and checkpoints are recorded under `examples/tinker/logs/q20_*`. You can also use `verl` as a substitute for the `algo` command when Tinker service is not available.

You can run additional runner processes at any time; they register with the store and start dequeuing tasks immediately.

## Evaluating a model on 20 Questions

Reuse the CrewAI flow to benchmark any OpenAI-compatible model (hosted on Tinker, OpenAI, or another LiteLLM backend):

```bash
dotenv run python q20_evaluate.py \
  --model Qwen/Qwen3-30B-A3B-Instruct-2507 \
  --output-file logs/twenty_questions_results.jsonl \
  --search
```

Results append to the specified JSONL file so you can compute aggregate stats later.

## How the bridge works

The `agl_tinker` package keeps the rest of the Tinker or Tinker Cookbook's codebase untouched by emulating the interfaces it expects:

- `AGLDatasetBuilder` and `AGLDummyEnv` wrap plain Agent-lightning datasets so batches still yield Tinker `EnvGroupBuilder` objects, even though rollouts run remotely.
- `do_group_of_group_rollouts` (in [`rollout.py`](agl_tinker/rollout.py)) enqueues tasks to the Agent-lightning store, waits for runners to finish, then reconstructs `Trajectory` objects from span triplets collected by `TracerTraceToTriplet`.
- `TinkerLLM` implements LiteLLM's `CustomLLM` so the training loop can update sampling clients and expose them through an OpenAI-compatible endpoint without rewriting agent code.
- `agl_tinker.algo.Tinker` satisfies Agent-lightning's `Algorithm` contract, meaning you can launch training via `agl.Trainer` alongside other algorithms, schedulers, or resources.

Because spans and rewards are emitted by the same rollout function you would deploy, evaluation and production stay in sync—no separate simulator code paths to maintain.

## Troubleshooting tips

- If the runner logs show `Triplet has no token_ids`, ensure your LiteLLM proxy returns logprobs and token IDs, and that the token IDs are present in the store. The provided adapter requires them to rebuild trajectories. See the debugging tutorial for more details.
- CrewAI telemetry must stay disabled (see `.env.example`) so AgentOps traces remain self-contained; otherwise, you may see malformed traces.
- Tune `learning_rate`, `batch_size` and `group_size` carefully. The training is sensitive to these hyper-parameters.


## Links discovered
- [![tinker CI status](https://github.com/microsoft/agent-lightning/actions/workflows/examples-tinker.yml/badge.svg)
- [Tinker's reinforcement-learning infrastructure](https://tinker-docs.thinkingmachines.ai/)
- [Tinker Cookbook example](https://github.com/thinking-machines-lab/tinker-cookbook/tree/51d9e8226f2dcf82ceac272c734a5f6e3b4f0203/tinker_cookbook/recipes/multiplayer_rl/guess_number)
- [`rollout.py`](https://github.com/microsoft/agent-lightning/blob/main/examples/tinker/agl_tinker/rollout.py)

--- scripts/export_openapi.py ---
# Copyright (c) Microsoft. All rights reserved.

"""Generate OpenAPI specification for the LightningStore server.

Run this every time when you make changes to the LightningStore server.
"""

import asyncio
import json

from agentlightning.store.client_server import LightningStoreServer
from agentlightning.store.memory import InMemoryLightningStore


async def main():
    store = InMemoryLightningStore()
    server = LightningStoreServer(store, host="0.0.0.0", port=23333)
    await server.start()

    with open("docs/assets/store-openapi.json", "w") as f:
        json.dump(server.app.openapi(), f)  # type: ignore
    await server.stop()


if __name__ == "__main__":
    asyncio.run(main())


--- dashboard/src/features/rollouts/api.ts ---
// Copyright (c) Microsoft. All rights reserved.

import type { BaseQueryFn } from '@reduxjs/toolkit/query';
import { createApi, fetchBaseQuery, type FetchArgs, type FetchBaseQueryError } from '@reduxjs/toolkit/query/react';
import type { RootState } from '@/store';
import { camelCaseKeys } from '@/utils/format';
import type {
  Attempt,
  PaginatedResponse,
  Resources,
  Rollout,
  RolloutMode,
  RolloutStatus,
  Span,
  Timestamp,
  Worker,
  WorkerStatus,
} from '../../types';

const rawBaseQuery = fetchBaseQuery({
  baseUrl: '/',
});

const buildAbsoluteUrl = (baseUrl: string, path: string) => {
  if (path.startsWith('http://') || path.startsWith('https://')) {
    return path;
  }

  const normalizedBase = baseUrl.replace(/\/+$/, '');
  const normalizedPath = path.replace(/^\/+/, '');
  if (!normalizedBase) {
    return `/${normalizedPath}`;
  }
  return `${normalizedBase}/${normalizedPath}`;
};

const normalizeHeartbeat = (
  attempt: Partial<Attempt> & { lastHeartbeatTime?: Timestamp | null; lastHeartBeatTime?: Timestamp | null },
): Timestamp | null => {
  if (typeof attempt.lastHeartbeatTime === 'number') {
    return attempt.lastHeartbeatTime;
  }

  if (typeof attempt.lastHeartBeatTime === 'number') {
    return attempt.lastHeartBeatTime;
  }

  if (typeof attempt.startTime === 'number') {
    return attempt.startTime;
  }

  return null;
};

const normalizeAttempt = (value: unknown): Attempt | null => {
  if (value === null || typeof value === 'undefined') {
    return null;
  }

  const camelized = camelCaseKeys(value) as Attempt & {
    lastHeartbeatTime?: Timestamp | null;
    lastHeartBeatTime?: Timestamp | null;
  };
  const { lastHeartbeatTime, lastHeartBeatTime, ...rest } = camelized;

  return {
    ...rest,
    lastHeartbeatTime: normalizeHeartbeat({ ...rest, lastHeartbeatTime, lastHeartBeatTime }),
  };
};

const normalizeAttemptStrict = (value: unknown): Attempt => {
  const normalized = normalizeAttempt(value);
  if (!normalized) {
    throw new Error('Expected attempt payload');
  }
  return normalized;
};

const normalizeRollout = (value: unknown): Rollout => {
  const camelized = camelCaseKeys(value) as Rollout & { attempt?: unknown };
  const { attempt, ...rest } = camelized;

  return {
    ...rest,
    attempt: normalizeAttempt(attempt),
  };
};

const normalizeSpan = (value: unknown): Span => {
  const camelized = camelCaseKeys(value) as Span & {
    status?: {
      status_code?: Span['status']['status_code'];
      statusCode?: Span['status']['status_code'];
      description?: string | null;
    };
  };
  const rawStatus = camelized.status ?? { status_code: 'UNSET', description: null };
  const result = {
    ...camelized,
    parentId: camelized.parentId ?? null,
    // The following fields does not need to be normalized to camel case
    // For example, gen_ai.xxx should not become genAi.xxx
    attributes: (value as any).attributes ?? {},
    context: (value as any).context ?? {},
    parent: (value as any).parent ?? null,
    resource: (value as any).resource ?? {},
    status: {
      status_code: rawStatus.status_code ?? rawStatus.statusCode ?? 'UNSET',
      description: rawStatus.description ?? null,
    },
  };
  return result;
};

const normalizeResources = (value: unknown): Resources => {
  const camelized = camelCaseKeys(value) as Resources;
  return {
    resourcesId: camelized.resourcesId,
    version: camelized.version,
    createTime: camelized.createTime,
    updateTime: camelized.updateTime,
    resources: camelized.resources ?? {},
  };
};

const normalizeWorker = (value: unknown): Worker => {
  const camelized = camelCaseKeys(value) as Worker;
  return {
    workerId: camelized.workerId,
    status: camelized.status,
    heartbeatStats: camelized.heartbeatStats ?? null,
    lastHeartbeatTime: camelized.lastHeartbeatTime ?? null,
    lastDequeueTime: camelized.lastDequeueTime ?? null,
    lastBusyTime: camelized.lastBusyTime ?? null,
    lastIdleTime: camelized.lastIdleTime ?? null,
    currentRolloutId: camelized.currentRolloutId ?? null,
    currentAttemptId: camelized.currentAttemptId ?? null,
  };
};

const normalizePaginatedResponse = <T>(value: unknown, normalizer: (item: unknown) => T): PaginatedResponse<T> => {
  if (!value || typeof value !== 'object') {
    throw new Error('Expected paginated response payload');
  }

  const converted = value as {
    items?: unknown;
    limit?: number;
    offset?: number;
    total?: number;
  };

  const itemsSource = Array.isArray(converted.items) ? converted.items : [];

  return {
    items: itemsSource.map((item) => normalizer(item)),
    limit: typeof converted.limit === 'number' ? converted.limit : itemsSource.length,
    offset: typeof converted.offset === 'number' ? converted.offset : 0,
    total: typeof converted.total === 'number' ? converted.total : itemsSource.length,
  };
};

const dynamicBaseQuery: BaseQueryFn<string | FetchArgs, unknown, FetchBaseQueryError> = async (
  args,
  api,
  extraOptions,
) => {
  const state = api.getState() as RootState;
  const stateBaseUrl = state.config?.baseUrl;
  const fallbackBaseUrl = typeof window !== 'undefined' ? window.location.origin : '';
  const baseUrl = stateBaseUrl && stateBaseUrl.trim().length > 0 ? stateBaseUrl : fallbackBaseUrl;
  const preparedArgs: FetchArgs =
    typeof args === 'string'
      ? { url: args }
      : {
          ...args,
          url: args.url ?? '',
        };

  const absoluteUrl = buildAbsoluteUrl(baseUrl, preparedArgs.url ?? '');
  return rawBaseQuery({ ...preparedArgs, url: absoluteUrl }, api, extraOptions);
};

export type GetRolloutsQueryArgs = {
  limit: number;
  offset: number;
  sortBy?: string | null;
  sortOrder?: 'asc' | 'desc';
  statusIn?: RolloutStatus[];
  rolloutIdContains?: string | null;
  modeIn?: RolloutMode[];
};

export type GetResourcesQueryArgs = {
  limit: number;
  offset: number;
  sortBy?: string | null;
  sortOrder?: 'asc' | 'desc';
  resourcesIdContains?: string | null;
};

export type GetWorkersQueryArgs = {
  limit: number;
  offset: number;
  sortBy?: string | null;
  sortOrder?: 'asc' | 'desc';
  workerIdContains?: string | null;
  statusIn?: WorkerStatus[];
};

export type GetRolloutAttemptsQueryArgs = {
  rolloutId: string;
  limit?: number;
  offset?: number;
  sortBy?: string | null;
  sortOrder?: 'asc' | 'desc';
};

export type GetSpansQueryArgs = {
  rolloutId: string;
  attemptId?: string | null;
  limit?: number;
  offset?: number;
  sortBy?: string | null;
  sortOrder?: 'asc' | 'desc';
  traceIdContains?: string | null;
  spanIdContains?: string | null;
  parentIdContains?: string | null;
  nameContains?: string | null;
  filterLogic?: 'and' | 'or' | null;
};

export const rolloutsApi = createApi({
  reducerPath: 'rolloutsApi',
  baseQuery: dynamicBaseQuery,
  tagTypes: ['Rollout', 'Span', 'Resources', 'Worker'],
  endpoints: (builder) => ({
    getResources: builder.query<PaginatedResponse<Resources>, GetResourcesQueryArgs>({
      query: ({ limit, offset, sortBy, sortOrder, resourcesIdContains }) => {
        const searchParams = new URLSearchParams();
        searchParams.set('limit', String(typeof limit === 'number' ? limit : -1));
        searchParams.set('offset', String(typeof offset === 'number' ? offset : 0));
        if (sortBy) {
          searchParams.set('sort_by', sortBy);
        }
        if (sortOrder) {
          searchParams.set('sort_order', sortOrder);
        }
        if (resourcesIdContains && resourcesIdContains.trim().length > 0) {
          searchParams.set('resources_id_contains', resourcesIdContains.trim());
        }

        const queryString = searchParams.toString();
        const url = queryString.length > 0 ? `v1/agl/resources?${queryString}` : 'v1/agl/resources';
        return { url, method: 'GET' };
      },
      transformResponse: (response: unknown) => normalizePaginatedResponse(response, normalizeResources),
      providesTags: (result) =>
        result
          ? [
              { type: 'Resources' as const, id: 'LIST' },
              ...result.items.map((item) => ({ type: 'Resources' as const, id: item.resourcesId })),
            ]
          : [{ type: 'Resources' as const, id: 'LIST' }],
    }),
    getWorkers: builder.query<PaginatedResponse<Worker>, GetWorkersQueryArgs>({
      query: ({ limit, offset, sortBy, sortOrder, workerIdContains, statusIn }) => {
        const searchParams = new URLSearchParams();
        searchParams.set('limit', String(typeof limit === 'number' ? limit : -1));
        searchParams.set('offset', String(typeof offset === 'number' ? offset : 0));
        if (sortBy) {
          searchParams.set('sort_by', sortBy);
        }
        if (sortOrder) {
          searchParams.set('sort_order', sortOrder);
        }
        if (workerIdContains && workerIdContains.trim().length > 0) {
          searchParams.set('worker_id_contains', workerIdContains.trim());
        }
        if (statusIn && statusIn.length > 0) {
          statusIn.forEach((status) => searchParams.append('status_in', status));
        }

        const queryString = searchParams.toString();
        const url = queryString.length > 0 ? `v1/agl/workers?${queryString}` : 'v1/agl/workers';
        return { url, method: 'GET' };
      },
      transformResponse: (response: unknown) => normalizePaginatedResponse(response, normalizeWorker),
      providesTags: (result) =>
        result
          ? [
              { type: 'Worker' as const, id: 'LIST' },
              ...result.items.map((worker) => ({ type: 'Worker' as const, id: worker.workerId })),
            ]
          : [{ type: 'Worker' as const, id: 'LIST' }],
    }),
    getRollouts: builder.query<PaginatedResponse<Rollout>, GetRolloutsQueryArgs>({
      query: ({ limit, offset, sortBy, sortOrder, statusIn, rolloutIdContains, modeIn }) => {
        const searchParams = new URLSearchParams();
        searchParams.set('limit', String(typeof limit === 'number' ? limit : -1));
        searchParams.set('offset', String(typeof offset === 'number' ? offset : 0));
        if (sortBy) {
          searchParams.set('sort_by', sortBy);
        }
        if (sortOrder) {
          searchParams.set('sort_order', sortOrder);
        }
        if (statusIn && statusIn.length > 0) {
          statusIn.forEach((status) => searchParams.append('status_in', status));
        }
        if (modeIn && modeIn.length > 0) {
          modeIn.forEach((mode) => searchParams.append('mode_in', mode));
        }
        if (rolloutIdContains && rolloutIdContains.trim().length > 0) {
          searchParams.set('rollout_id_contains', rolloutIdContains.trim());
        }

        const queryString = searchParams.toString();
        const url = queryString.length > 0 ? `v1/agl/rollouts?${queryString}` : 'v1/agl/rollouts';
        return { url, method: 'GET' };
      },
      transformResponse: (response: unknown) => normalizePaginatedResponse(response, normalizeRollout),
      providesTags: (result) =>
        result
          ? [
              { type: 'Rollout' as const, id: 'LIST' },
              ...result.items.map((rollout) => ({ type: 'Rollout' as const, id: rollout.rolloutId })),
            ]
          : [{ type: 'Rollout' as const, id: 'LIST' }],
    }),
    getRolloutAttempts: builder.query<PaginatedResponse<Attempt>, GetRolloutAttemptsQueryArgs>({
      query: ({ rolloutId, limit = -1, offset = 0, sortBy, sortOrder }) => {
        const searchParams = new URLSearchParams();
        searchParams.set('limit', String(typeof limit === 'number' ? limit : -1));
        searchParams.set('offset', String(typeof offset === 'number' ? offset : 0));
        if (sortBy) {
          searchParams.set('sort_by', sortBy);
        }
        if (sortOrder) {
          searchParams.set('sort_order', sortOrder);
        }
        const queryString = searchParams.toString();
        const url =
          queryString.length > 0
            ? `v1/agl/rollouts/${rolloutId}/attempts?${queryString}`
            : `v1/agl/rollouts/${rolloutId}/attempts`;
        return { url, method: 'GET' };
      },
      transformResponse: (response: unknown) => normalizePaginatedResponse(response, normalizeAttemptStrict),
      providesTags: (_result, _error, queryArgs) => [{ type: 'Rollout', id: queryArgs.rolloutId }],
    }),
    getSpans: builder.query<PaginatedResponse<Span>, GetSpansQueryArgs>({
      query: (args) => {
        if (!args.rolloutId) {
          throw new Error('rolloutId is required to fetch spans');
        }
        const searchParams = new URLSearchParams({ rollout_id: args.rolloutId });
        if (args.attemptId) {
          searchParams.set('attempt_id', args.attemptId);
        }
        if (typeof args.limit === 'number') {
          searchParams.set('limit', String(args.limit));
        }
        if (typeof args.offset === 'number') {
          searchParams.set('offset', String(args.offset));
        }
        if (args.sortBy) {
          searchParams.set('sort_by', args.sortBy);
        }
        if (args.sortOrder) {
          searchParams.set('sort_order', args.sortOrder);
        }
        if (args.traceIdContains) {
          searchParams.set('trace_id_contains', args.traceIdContains);
        }
        if (args.spanIdContains) {
          searchParams.set('span_id_contains', args.spanIdContains);
        }
        if (args.parentIdContains) {
          searchParams.set('parent_id_contains', args.parentIdContains);
        }
        if (args.nameContains) {
          searchParams.set('name_contains', args.nameContains);
        }
        if (args.filterLogic) {
          searchParams.set('filter_logic', args.filterLogic);
        }
        return { url: `v1/agl/spans?${searchParams.toString()}`, method: 'GET' };
      },
      transformResponse: (response: unknown) => normalizePaginatedResponse(response, normalizeSpan),
      providesTags: (_result, _error, args) =>
        args
          ? [
              { type: 'Span' as const, id: `${args.rolloutId}:${args.attemptId ?? 'latest'}` },
              { type: 'Span' as const, id: 'LIST' },
            ]
          : [{ type: 'Span' as const, id: 'LIST' }],
    }),
  }),
});

export const {
  useGetResourcesQuery,
  useGetWorkersQuery,
  useGetRolloutsQuery,
  useGetRolloutAttemptsQuery,
  useGetSpansQuery,
} = rolloutsApi;


--- SECURITY.md ---
<!-- BEGIN MICROSOFT SECURITY.MD V1.0.0 BLOCK -->

## Security

Microsoft takes the security of our software products and services seriously, which
includes all source code repositories in our GitHub organizations.

**Please do not report security vulnerabilities through public GitHub issues.**

For security reporting information, locations, contact information, and policies,
please review the latest guidance for Microsoft repositories at
[https://aka.ms/SECURITY.md](https://aka.ms/SECURITY.md).

<!-- END MICROSOFT SECURITY.MD BLOCK -->


## Links discovered
- [https://aka.ms/SECURITY.md](https://aka.ms/SECURITY.md)

--- tests/benchmark/analysis.py ---
# Copyright (c) Microsoft. All rights reserved.

"""Lightweight benchmark report for the Prometheus + Grafana stack shipped with Agent Lightning."""

from __future__ import annotations

import argparse
import datetime as dt
import json
import math
from dataclasses import dataclass
from typing import Any, Callable, Dict, Iterable, List, Mapping, Optional, Sequence, Set, Tuple, cast
from urllib import error, parse, request


class PrometheusQueryError(RuntimeError):
    """Raised when Prometheus returns an error payload."""


class PrometheusClient:
    """Tiny helper around the Prometheus HTTP API."""

    def __init__(
        self,
        base_url: str,
        timeout: float = 10.0,
        default_time: Optional[dt.datetime] = None,
    ):
        self.base_url = base_url.rstrip("/")
        self.timeout = timeout
        self.default_time = default_time

    def query_vector(self, expr: str, eval_time: Optional[dt.datetime] = None) -> List[Mapping[str, object]]:
        params: Dict[str, str] = {"query": expr}
        query_time = eval_time or self.default_time
        if query_time is not None:
            params["time"] = query_time.isoformat()
        payload = self._get("/api/v1/query", params)
        status = payload.get("status")
        if not isinstance(status, str) or status != "success":
            error_msg = payload.get("error", "unknown error")
            raise PrometheusQueryError(str(error_msg))
        data_obj = payload.get("data", {})
        if isinstance(data_obj, dict):
            data = cast(Dict[str, Any], data_obj)
        else:
            data = {}
        result_type_obj = data.get("resultType")
        result_type = result_type_obj if isinstance(result_type_obj, str) else None
        raw_result_obj = data.get("result", [])
        raw_result: List[object]
        if isinstance(raw_result_obj, list):
            raw_result = cast(List[object], raw_result_obj)
        else:
            raw_result = []
        if result_type == "scalar":
            if len(raw_result) >= 2:
                ts = raw_result[0]
                value = raw_result[1]
                return [{"metric": {}, "value": [ts, value]}]
            return []
        vector_result: List[Mapping[str, object]] = [
            cast(Mapping[str, object], item) for item in raw_result if isinstance(item, Mapping)
        ]
        if result_type == "matrix":
            collapsed: List[Dict[str, object]] = []
            for series in vector_result:
                values_obj = series.get("values")
                if isinstance(values_obj, list) and values_obj and isinstance(values_obj[-1], Sequence):
                    last = cast(Sequence[object], values_obj[-1])
                else:
                    continue
                metric_obj = series.get("metric")
                if isinstance(metric_obj, Mapping):
                    metric: Dict[str, object] = dict(cast(Mapping[str, object], metric_obj))
                else:
                    metric = {}
                collapsed.append({"metric": metric, "value": list(last)})
            return cast(List[Mapping[str, object]], collapsed)
        if result_type == "vector":
            return vector_result
        return []

    def query_scalar(self, expr: str, eval_time: Optional[dt.datetime] = None) -> Optional[float]:
        samples = self.query_vector(expr, eval_time=eval_time)
        if not samples:
            return None
        return _sample_value(samples[0])

    def _get(self, path: str, data: Optional[Mapping[str, str]] = None) -> Dict[str, Any]:
        encoded: Optional[bytes] = None
        if data is not None:
            encoded = parse.urlencode(data).encode()
        req = request.Request(f"{self.base_url}{path}", data=encoded)
        try:
            with request.urlopen(req, timeout=self.timeout) as resp:
                loaded = json.loads(resp.read().decode())
                if isinstance(loaded, dict):
                    return cast(Dict[str, Any], loaded)
                return {}
        except error.URLError as exc:  # pragma: no cover - network/infra issues
            raise PrometheusQueryError(str(exc)) from exc


def parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Summarize benchmark metrics from Prometheus.")
    parser.add_argument("--prom-url", default="http://localhost:9090", help="Base URL for the Prometheus API.")
    parser.add_argument(
        "--store-url",
        default="http://localhost:4747/v1/agl",
        help="Base URL for the Lightning Store API (without the /statistics suffix).",
    )
    parser.add_argument("--timeout", type=float, default=10.0, help="HTTP timeout in seconds.")
    parser.add_argument("--start", type=str, help="ISO timestamp (e.g. 2024-05-01T12:00:00Z).")
    parser.add_argument("--end", type=str, help="ISO timestamp (default: now).")
    parser.add_argument(
        "--duration",
        type=str,
        default="5m",
        help="Fallback duration (e.g. 5m, 1h) used when --start is omitted.",
    )
    return parser.parse_args(argv)


def parse_timestamp(value: Optional[str], default: Optional[dt.datetime] = None) -> Optional[dt.datetime]:
    if value is None:
        return default
    try:
        if value.endswith("Z"):
            value = value[:-1] + "+00:00"
        return dt.datetime.fromisoformat(value).astimezone(dt.timezone.utc)
    except ValueError as exc:  # pragma: no cover - invalid CLI input
        raise SystemExit(f"Invalid timestamp '{value}': {exc}") from exc


def parse_duration(text: str) -> dt.timedelta:
    units = {"s": 1, "m": 60, "h": 3600}
    if text.isdigit():
        return dt.timedelta(seconds=int(text))
    suffix = text[-1]
    if suffix not in units:
        raise SystemExit(f"Unsupported duration '{text}'. Use Ns/Nm/Nh.")
    try:
        value = int(text[:-1])
    except ValueError as exc:  # pragma: no cover - invalid CLI input
        raise SystemExit(f"Invalid duration '{text}': {exc}") from exc
    return dt.timedelta(seconds=value * units[suffix])


def format_window(seconds: float) -> str:
    seconds = max(int(seconds), 1)
    return f"{seconds}s"


def clamp_window_seconds(duration_seconds: float) -> int:
    return max(int(duration_seconds), 1)


def compute_peak_window(duration_seconds: float) -> str:
    peak_seconds = max(min(int(duration_seconds), 60), 1)
    return f"{peak_seconds}s"


def compute_subquery_step(duration_seconds: float) -> str:
    step_seconds = max(int(duration_seconds / 60), 1)
    step_seconds = min(step_seconds, 15)
    return f"{step_seconds}s"


def _sample_value(sample: Mapping[str, object]) -> Optional[float]:
    value_obj = sample.get("value")
    if not isinstance(value_obj, Sequence):
        return None
    value_seq = cast(Sequence[object], value_obj)
    if len(value_seq) < 2:
        return None
    candidate = value_seq[1]
    if isinstance(candidate, (int, float)):
        return float(candidate)
    if isinstance(candidate, str):
        try:
            return float(candidate)
        except ValueError:
            return None
    return None


def vector_to_map(
    samples: Optional[Sequence[Mapping[str, object]]],
    labels: Sequence[str],
) -> Dict[Any, float]:
    mapping: Dict[Any, float] = {}
    if not samples:
        return mapping
    for sample in samples:
        metric_obj = sample.get("metric", {})
        if isinstance(metric_obj, Mapping):
            metric: Dict[str, object] = dict(cast(Mapping[str, object], metric_obj))
        else:
            metric = {}
        if len(labels) == 1:
            key: Any = str(metric.get(labels[0], ""))
        else:
            key = tuple(str(metric.get(label, "")) for label in labels)
        value = _sample_value(sample)
        if value is not None:
            mapping[key] = value
    return mapping


def _normalize_label_value(value: Any) -> str:
    if value is None:
        return "-"
    text = str(value)
    return text if text else "-"


def vector_to_labeled_map(
    samples: Optional[Sequence[Mapping[str, object]]],
    labels: Sequence[str],
) -> Dict[Tuple[str, ...], float]:
    mapping: Dict[Tuple[str, ...], float] = {}
    if not samples:
        return mapping
    for sample in samples:
        metric_obj = sample.get("metric", {})
        if isinstance(metric_obj, Mapping):
            metric = dict(cast(Mapping[str, object], metric_obj))
        else:
            metric = {}
        if labels:
            key = tuple(_normalize_label_value(metric.get(label)) for label in labels)
        else:
            key = tuple[str, ...]()
        value = _sample_value(sample)
        if value is not None:
            mapping[key] = value
    return mapping


def sum_by_clause(labels: Sequence[str]) -> str:
    if labels:
        joined = ", ".join(labels)
        return f"sum by ({joined})"
    return "sum"


def histogram_sum_by_clause(labels: Sequence[str]) -> str:
    le_prefixed = ("le", *labels)
    joined = ", ".join(le_prefixed)
    return f"sum by ({joined})"


def histogram_sum_metric_name(bucket_metric: str) -> str:
    if bucket_metric.endswith("_bucket"):
        return f"{bucket_metric[: -len('_bucket')]}_sum"
    return f"{bucket_metric}_sum"


def histogram_count_metric_name(bucket_metric: str) -> str:
    if bucket_metric.endswith("_bucket"):
        return f"{bucket_metric[: -len('_bucket')]}_count"
    return f"{bucket_metric}_count"


def divide_or_none(numerator: Optional[float], denominator: Optional[float]) -> Optional[float]:
    if numerator is None or denominator is None:
        return None
    if denominator == 0:
        return None
    return numerator / denominator


def compute_average_time_map(
    time_totals: Mapping[Tuple[str, ...], float],
    count_totals: Mapping[Tuple[str, ...], float],
) -> Dict[Tuple[str, ...], float]:
    averages: Dict[Tuple[str, ...], float] = {}
    keys = set(time_totals.keys()).union(count_totals.keys())
    for key in keys:
        avg = divide_or_none(time_totals.get(key), count_totals.get(key))
        if avg is not None:
            averages[key] = avg
    return averages


def safe_vector(client: PrometheusClient, expr: str) -> Optional[List[Mapping[str, object]]]:
    try:
        return client.query_vector(expr)
    except PrometheusQueryError as exc:
        print(f"[warn] Prometheus query failed: {exc} (expr={expr})")
        return None


def safe_scalar(client: PrometheusClient, expr: str) -> Optional[float]:
    try:
        return client.query_scalar(expr)
    except PrometheusQueryError as exc:
        print(f"[warn] Prometheus query failed: {exc} (expr={expr})")
        return None


def fetch_store_statistics(store_url: str, timeout: float) -> Optional[Dict[str, Any]]:
    store_url = store_url.rstrip("/")
    stats_url = f"{store_url}/statistics"
    req = request.Request(stats_url)
    try:
        with request.urlopen(req, timeout=timeout) as resp:
            loaded = json.loads(resp.read().decode())
            if isinstance(loaded, Mapping):
                return dict(cast(Mapping[str, Any], loaded))
            return None
    except error.URLError as exc:
        print(f"[warn] Failed to fetch store statistics: {exc} (url={stats_url})")
        return None
    except json.JSONDecodeError as exc:
        print(f"[warn] Failed to decode store statistics: {exc} (url={stats_url})")
        return None
    except TimeoutError as exc:
        print(f"[warn] Timeout fetching store statistics: {exc} (url={stats_url})")
        return None


@dataclass
class CollectionThroughput:
    name: str
    count: Optional[float]
    per_sec: Optional[float]


@dataclass
class MetricRow:
    label_values: Tuple[str, ...]
    avg_rate: Optional[float]
    max_rate: Optional[float]
    min_rate: Optional[float]
    p50: Optional[float]
    p95: Optional[float]
    p99: Optional[float]
    max_latency: Optional[float]
    time_per_sec: Optional[float]
    time_per_request: Optional[float]
    avg_rate_delta: Optional[float]
    p50_delta: Optional[float]
    p95_delta: Optional[float]
    time_delta: Optional[float]
    time_per_request_delta: Optional[float]


@dataclass(frozen=True)
class MetricGroupSpec:
    title: str
    histogram_bucket_metric: str
    label_names: Tuple[str, ...]
    label_headers: Tuple[str, ...]
    selector: str = ""
    sum_metric: Optional[str] = None
    count_metric: Optional[str] = None


def metric_row_sort_key(row: MetricRow) -> Tuple[str, ...]:
    return row.label_values


STORE_TOTAL_FIELDS = {
    "rollouts": "total_rollouts",
    "spans": "total_spans",
    "attempts": "total_attempts",
    "resources": "total_resources",
    "workers": "total_workers",
}
STORE_TOTAL_COLLECTIONS = tuple(STORE_TOTAL_FIELDS.keys())


def _coerce_int(value: Any) -> Optional[int]:
    if isinstance(value, bool):
        return int(value)
    if isinstance(value, int):
        return value
    if isinstance(value, float):
        if math.isnan(value):
            return None
        return int(value)
    if isinstance(value, str):
        try:
            return int(value)
        except ValueError:
            try:
                return int(float(value))
            except ValueError:
                return None
    return None


def extract_store_totals(stats: Optional[Mapping[str, Any]]) -> Dict[str, Optional[int]]:
    totals: Dict[str, Optional[int]] = {}
    if not stats:
        return totals
    for display_name, field_name in STORE_TOTAL_FIELDS.items():
        if field_name in stats:
            totals[display_name] = _coerce_int(stats.get(field_name))
        else:
            totals[display_name] = None
    return totals


def gather_collection_throughput(
    client: PrometheusClient, collections: Sequence[str], duration_seconds: float
) -> List[CollectionThroughput]:
    rows: List[CollectionThroughput] = []
    window = format_window(duration_seconds)
    for collection in collections:
        # Successful insert operations reflect the number of new records.
        expr = (
            "sum("
            f'increase(mongo_operation_total{{collection="{collection}", operation="insert", status="ok"}}[{window}])'
            ")"
        )
        count = safe_scalar(client, expr)
        if count is not None and count < 0:
            count = 0.0
        per_sec = (count / duration_seconds) if (count is not None and duration_seconds > 0) else None
        rows.append(CollectionThroughput(collection, count, per_sec))
    return rows


def gather_metric_group(
    client: PrometheusClient,
    spec: MetricGroupSpec,
    *,
    window: str,
    window_seconds: int,
    peak_window: str,
    subquery_step: str,
    half_window: Optional[str],
    half_window_seconds: Optional[int],
) -> List[MetricRow]:
    label_names = spec.label_names
    sum_clause = sum_by_clause(label_names)
    hist_clause = histogram_sum_by_clause(label_names)
    bucket_metric = f"{spec.histogram_bucket_metric}{spec.selector}" if spec.selector else spec.histogram_bucket_metric
    base_sum_metric = spec.sum_metric or histogram_sum_metric_name(spec.histogram_bucket_metric)
    sum_metric = f"{base_sum_metric}{spec.selector}" if spec.selector else base_sum_metric
    base_count_metric = spec.count_metric or histogram_count_metric_name(spec.histogram_bucket_metric)
    count_metric = f"{base_count_metric}{spec.selector}" if spec.selector else base_count_metric

    count_total_expr = f"{sum_clause}(increase({count_metric}[{window}]))"
    count_total_map = vector_to_labeled_map(safe_vector(client, count_total_expr), label_names)
    avg_map = {key: value / window_seconds for key, value in count_total_map.items()} if window_seconds > 0 else {}

    peak_expr = f"{sum_clause}(irate({count_metric}[{peak_window}]))"
    max_expr = f"max_over_time(({peak_expr})[{window}:{subquery_step}])"
    min_expr = f"min_over_time(({peak_expr})[{window}:{subquery_step}])"
    max_map = vector_to_labeled_map(safe_vector(client, max_expr), label_names)
    min_map = vector_to_labeled_map(safe_vector(client, min_expr), label_names)

    p50_map = vector_to_labeled_map(
        safe_vector(
            client,
            f"histogram_quantile(0.50, {hist_clause}(increase({bucket_metric}[{window}])))",
        ),
        label_names,
    )
    p95_map = vector_to_labeled_map(
        safe_vector(
            client,
            f"histogram_quantile(0.95, {hist_clause}(increase({bucket_metric}[{window}])))",
        ),
        label_names,
    )
    p99_map = vector_to_labeled_map(
        safe_vector(
            client,
            f"histogram_quantile(0.99, {hist_clause}(increase({bucket_metric}[{window}])))",
        ),
        label_names,
    )
    max_latency_map = vector_to_labeled_map(
        safe_vector(
            client,
            f"histogram_quantile(1.00, {hist_clause}(increase({bucket_metric}[{window}])))",
        ),
        label_names,
    )

    time_total_expr = f"{sum_clause}(increase({sum_metric}[{window}]))"
    time_total_map = vector_to_labeled_map(safe_vector(client, time_total_expr), label_names)
    time_rate_map = {key: value / window_seconds for key, value in time_total_map.items()} if window_seconds > 0 else {}
    avg_time_map = compute_average_time_map(time_total_map, count_total_map)

    if half_window and half_window_seconds and half_window_seconds > 0:
        count_late_expr = f"{sum_clause}(increase({count_metric}[{half_window}]))"
        count_early_expr = f"{sum_clause}(increase({count_metric}[{half_window}] offset {half_window}))"
        count_late_total_map = vector_to_labeled_map(safe_vector(client, count_late_expr), label_names)
        count_early_total_map = vector_to_labeled_map(safe_vector(client, count_early_expr), label_names)
        avg_late_map = {key: value / half_window_seconds for key, value in count_late_total_map.items()}
        avg_early_map = {key: value / half_window_seconds for key, value in count_early_total_map.items()}

        p50_late_expr = f"histogram_quantile(0.50, {hist_clause}(increase({bucket_metric}[{half_window}])))"
        p50_early_expr = (
            f"histogram_quantile(0.50, {hist_clause}(increase({bucket_metric}[{half_window}] offset {half_window})))"
        )
        p50_late_map = vector_to_labeled_map(safe_vector(client, p50_late_expr), label_names)
        p50_early_map = vector_to_labeled_map(safe_vector(client, p50_early_expr), label_names)

        p95_late_expr = f"histogram_quantile(0.95, {hist_clause}(increase({bucket_metric}[{half_window}])))"
        p95_early_expr = (
            f"histogram_quantile(0.95, {hist_clause}(increase({bucket_metric}[{half_window}] offset {half_window})))"
        )
        p95_late_map = vector_to_labeled_map(safe_vector(client, p95_late_expr), label_names)
        p95_early_map = vector_to_labeled_map(safe_vector(client, p95_early_expr), label_names)

        time_late_expr = f"{sum_clause}(increase({sum_metric}[{half_window}]))"
        time_early_expr = f"{sum_clause}(increase({sum_metric}[{half_window}] offset {half_window}))"
        time_late_total_map = vector_to_labeled_map(safe_vector(client, time_late_expr), label_names)
        time_early_total_map = vector_to_labeled_map(safe_vector(client, time_early_expr), label_names)
        time_late_map = {key: value / half_window_seconds for key, value in time_late_total_map.items()}
        time_early_map = {key: value / half_window_seconds for key, value in time_early_total_map.items()}
        avg_time_late_map = compute_average_time_map(time_late_total_map, count_late_total_map)
        avg_time_early_map = compute_average_time_map(time_early_total_map, count_early_total_map)
    else:
        count_late_total_map: Dict[Tuple[str, ...], float] = {}
        count_early_total_map: Dict[Tuple[str, ...], float] = {}
        avg_late_map: Dict[Tuple[str, ...], float] = {}
        avg_early_map: Dict[Tuple[str, ...], float] = {}
        p50_late_map: Dict[Tuple[str, ...], float] = {}
        p50_early_map: Dict[Tuple[str, ...], float] = {}
        p95_late_map: Dict[Tuple[str, ...], float] = {}
        p95_early_map: Dict[Tuple[str, ...], float] = {}
        time_late_map: Dict[Tuple[str, ...], float] = {}
        time_early_map: Dict[Tuple[str, ...], float] = {}
        time_late_total_map = {}
        time_early_total_map = {}
        avg_time_late_map = {}
        avg_time_early_map = {}

    all_keys: Set[Tuple[str, ...]] = set()
    all_keys.update(count_total_map.keys())
    all_keys.update(avg_map.keys())
    all_keys.update(max_map.keys())
    all_keys.update(min_map.keys())
    all_keys.update(p50_map.keys())
    all_keys.update(p95_map.keys())
    all_keys.update(p99_map.keys())
    all_keys.update(max_latency_map.keys())
    all_keys.update(time_rate_map.keys())
    all_keys.update(avg_time_map.keys())
    all_keys.update(count_late_total_map.keys())
    all_keys.update(count_early_total_map.keys())
    all_keys.update(avg_late_map.keys())
    all_keys.update(avg_early_map.keys())
    all_keys.update(p50_late_map.keys())
    all_keys.update(p50_early_map.keys())
    all_keys.update(p95_late_map.keys())
    all_keys.update(p95_early_map.keys())
    all_keys.update(time_late_map.keys())
    all_keys.update(time_early_map.keys())
    all_keys.update(avg_time_late_map.keys())
    all_keys.update(avg_time_early_map.keys())

    if not all_keys:
        return []

    def build_delta(
        late_map: Mapping[Tuple[str, ...], float],
        early_map: Mapping[Tuple[str, ...], float],
        key: Tuple[str, ...],
    ) -> Optional[float]:
        late = late_map.get(key)
        early = early_map.get(key)
        if late is None or early is None:
            return None
        return late - early

    rows: List[MetricRow] = []
    for key in sorted(all_keys):
        rows.append(
            MetricRow(
                label_values=key,
                avg_rate=avg_map.get(key),
                max_rate=max_map.get(key),
                min_rate=min_map.get(key),
                p50=p50_map.get(key),
                p95=p95_map.get(key),
                p99=p99_map.get(key),
                max_latency=max_latency_map.get(key),
                time_per_sec=time_rate_map.get(key),
                time_per_request=avg_time_map.get(key),
                avg_rate_delta=build_delta(avg_late_map, avg_early_map, key),
                p50_delta=build_delta(p50_late_map, p50_early_map, key),
                p95_delta=build_delta(p95_late_map, p95_early_map, key),
                time_delta=build_delta(time_late_map, time_early_map, key),
                time_per_request_delta=build_delta(avg_time_late_map, avg_time_early_map, key),
            )
        )
    return rows


def gather_diagnostics(client: PrometheusClient, window: str) -> Dict[str, Any]:
    diagnostics: Dict[str, Any] = {}
    diagnostics["mongo_ops"] = vector_to_map(
        safe_vector(
            client,
            f"sum by (operation)(rate(mongo_operation_total{{operation!='ensure_collection'}}[{window}]))",
        ),
        ("operation",),
    )
    diagnostics["mongo_latency_p50"] = vector_to_map(
        safe_vector(
            client,
            f"histogram_quantile(0.50, sum by (le, operation)(rate(mongo_operation_duration_seconds_bucket{{operation!='ensure_collection'}}[{window}])))",
        ),
        ("operation",),
    )
    diagnostics["mongo_latency_p95"] = vector_to_map(
        safe_vector(
            client,
            f"histogram_quantile(0.95, sum by (le, operation)(rate(mongo_operation_duration_seconds_bucket{{operation!='ensure_collection'}}[{window}])))",
        ),
        ("operation",),
    )
    diagnostics["mongo_latency_p99"] = vector_to_map(
        safe_vector(
            client,
            f"histogram_quantile(0.99, sum by (le, operation)(rate(mongo_operation_duration_seconds_bucket{{operation!='ensure_collection'}}[{window}])))",
        ),
        ("operation",),
    )
    opcounters_samples = safe_vector(client, f"sum by (legacy_op_type)(rate(mongodb_ss_opcounters[{window}]))")
    mongo_opcounters: Dict[str, float] = {}
    if opcounters_samples:
        for sample in opcounters_samples:
            metric_obj = sample.get("metric", {})
            if isinstance(metric_obj, Mapping):
                metric: Dict[str, object] = dict(cast(Mapping[str, object], metric_obj))
            else:
                metric = {}
            label_value = metric.get("legacy_op_type") or metric.get("type")
            label = str(label_value) if label_value is not None else ""
            value = _sample_value(sample)
            if value is not None:
                mongo_opcounters[str(label or "-")] = value
    diagnostics["mongo_opcounters"] = mongo_opcounters
    diagnostics["mongo_connections"] = safe_scalar(client, "avg(mongodb_ss_connections{conn_type='current'})")
    diagnostics["memory_lock_rate"] = vector_to_map(
        safe_vector(client, f"sum by (collection)(rate(memory_collection_lock_rate_total[{window}]))"),
        ("collection",),
    )
    diagnostics["memory_lock_p50"] = vector_to_map(
        safe_vector(
            client,
            f"histogram_quantile(0.50, sum by (le, collection)(rate(memory_collection_lock_latency_seconds_bucket[{window}])))",
        ),
        ("collection",),
    )
    diagnostics["memory_lock_p95"] = vector_to_map(
        safe_vector(
            client,
            f"histogram_quantile(0.95, sum by (le, collection)(rate(memory_collection_lock_latency_seconds_bucket[{window}])))",
        ),
        ("collection",),
    )
    diagnostics["memory_lock_p99"] = vector_to_map(
        safe_vector(
            client,
            f"histogram_quantile(0.99, sum by (le, collection)(rate(memory_collection_lock_latency_seconds_bucket[{window}])))",
        ),
        ("collection",),
    )
    diagnostics["cpu_usage"] = safe_scalar(client, f"1 - avg(rate(node_cpu_seconds_total{{mode='idle'}}[{window}]))")
    diagnostics["memory_total"] = safe_scalar(client, "avg(node_memory_MemTotal_bytes)")
    diagnostics["memory_available"] = safe_scalar(client, "avg(node_memory_MemAvailable_bytes)")
    diagnostics["network_rx"] = safe_scalar(
        client,
        f"sum(rate(node_network_receive_bytes_total{{device!~'lo|docker.*'}}[{window}]))",
    )
    diagnostics["network_tx"] = safe_scalar(
        client,
        f"sum(rate(node_network_transmit_bytes_total{{device!~'lo|docker.*'}}[{window}]))",
    )
    diagnostics["disk_read_ops"] = safe_scalar(client, f"sum(rate(node_disk_reads_completed_total[{window}]))")
    diagnostics["disk_write_ops"] = safe_scalar(client, f"sum(rate(node_disk_writes_completed_total[{window}]))")
    diagnostics["disk_read_bytes"] = safe_scalar(client, f"sum(rate(node_disk_read_bytes_total[{window}]))")
    diagnostics["disk_write_bytes"] = safe_scalar(client, f"sum(rate(node_disk_written_bytes_total[{window}]))")
    return diagnostics


def render_table(headers: Sequence[str], rows: Sequence[Sequence[str]]) -> List[str]:
    if not rows:
        return [f"(no data for {headers})"]
    widths = [len(h) for h in headers]
    rendered: List[List[str]] = []
    for row in rows:
        rendered_row = [str(cell) for cell in row]
        for idx, cell in enumerate(rendered_row):
            widths[idx] = max(widths[idx], len(cell))
        rendered.append(rendered_row)

    lines = [
        " | ".join(headers[idx].ljust(widths[idx]) for idx in range(len(headers))),
        "-+-".join("-" * widths[idx] for idx in range(len(headers))),
    ]
    for row in rendered:
        lines.append(" | ".join(row[idx].ljust(widths[idx]) for idx in range(len(headers))))
    return lines


def fmt_rate(value: Optional[float]) -> str:
    if value is None or math.isnan(value):
        return "-"
    return f"{value:.2f}/s"


def fmt_latency(value: Optional[float]) -> str:
    if value is None or math.isnan(value):
        return "-"
    if abs(value) < 10:
        return f"{value * 1e3:.2f} ms"
    return f"{value:.2f} s"


def fmt_bytes(value: Optional[float]) -> str:
    if value is None or math.isnan(value):
        return "-"
    units = ["B", "KB", "MB", "GB", "TB", "PB"]
    idx = 0
    current = value
    while current >= 1024 and idx < len(units) - 1:
        current /= 1024
        idx += 1
    return f"{current:.2f} {units[idx]}"


def fmt_percentage(value: Optional[float]) -> str:
    if value is None or math.isnan(value):
        return "-"
    return f"{value * 100:4.1f}%"


def section(title: str, body: Iterable[str]) -> List[str]:
    lines = [f"## {title}"]
    lines.extend(body)
    lines.append("")
    return lines


def render_metric_group_table(
    spec: MetricGroupSpec,
    rows: Sequence[MetricRow],
    extra_columns: Optional[Sequence[Tuple[str, Callable[[MetricRow], str]]]] = None,
) -> List[str]:
    headers = list(spec.label_headers)
    headers.extend(
        [
            "Avg Rate/s",
            "Max Rate/s",
            "Min Rate/s",
            "P50",
            "P95",
            "P99",
            "Max Latency",
            "Time/s",
            "Avg Time/req",
            "Avg Rate Δ",
            "P50 Δ",
            "P95 Δ",
            "Time Δ",
            "Avg Time/req Δ",
        ]
    )
    column_renderers: Sequence[Tuple[str, Callable[[MetricRow], str]]] = extra_columns or ()
    for header, _ in column_renderers:
        headers.append(header)
    if not rows:
        return render_table(headers, [])
    sorted_rows = sorted(rows, key=metric_row_sort_key)
    rendered_rows: List[List[str]] = []
    for row in sorted_rows:
        label_cells = list(row.label_values) if spec.label_headers else []
        metrics = [
            fmt_rate(row.avg_rate),
            fmt_rate(row.max_rate),
            fmt_rate(row.min_rate),
            fmt_latency(row.p50),
            fmt_latency(row.p95),
            fmt_latency(row.p99),
            fmt_latency(row.max_latency),
            fmt_latency(row.time_per_sec),
            fmt_latency(row.time_per_request),
            fmt_rate(row.avg_rate_delta),
            fmt_latency(row.p50_delta),
            fmt_latency(row.p95_delta),
            fmt_latency(row.time_delta),
            fmt_latency(row.time_per_request_delta),
        ]
        extra_cells = [renderer(row) for _, renderer in column_renderers]
        rendered_rows.append(label_cells + metrics + extra_cells)
    return render_table(headers, rendered_rows)


def make_time_share_column(
    *,
    label_index: int,
    column_title: str,
    time_per_sec_map: Mapping[str, Optional[float]],
) -> Tuple[str, Callable[[MetricRow], str]]:
    def render_cell(row: MetricRow) -> str:
        if not row.label_values or len(row.label_values) <= label_index:
            return "-"
        label_value = row.label_values[label_index]
        store_time = time_per_sec_map.get(label_value)
        collection_time = row.time_per_sec
        if (
            store_time is None
            or collection_time is None
            or math.isnan(store_time)
            or math.isnan(collection_time)
            or store_time <= 0
        ):
            return "-"
        return fmt_percentage(collection_time / store_time)

    return (column_title, render_cell)


def main(argv: Optional[Sequence[str]] = None) -> None:
    args = parse_args(argv)
    end = parse_timestamp(args.end, default=dt.datetime.now(dt.timezone.utc))
    if end is None:
        raise SystemExit("End timestamp could not be determined.")
    start = parse_timestamp(args.start)
    if start is None:
        duration = parse_duration(args.duration)
        start = end - duration
    assert start is not None
    duration_seconds = max((end - start).total_seconds(), 1.0)
    window_seconds = clamp_window_seconds(duration_seconds)
    window = format_window(duration_seconds)
    peak_window = compute_peak_window(duration_seconds)
    subquery_step = compute_subquery_step(duration_seconds)
    half_window_seconds = window_seconds // 2 if window_seconds // 2 >= 1 else None
    half_window = format_window(half_window_seconds) if half_window_seconds else None

    client = PrometheusClient(args.prom_url, timeout=args.timeout, default_time=end)
    store_stats = fetch_store_statistics(args.store_url, timeout=args.timeout)
    store_totals = extract_store_totals(store_stats)
    lines: List[str] = [
        f"Agent Lightning benchmark report",
        f"Range: {start.isoformat()} — {end.isoformat()} ({duration_seconds:.0f}s window)",
        f"Prometheus: {args.prom_url}",
        f"Store: {args.store_url}",
        "",
    ]

    # Throughput
    throughput_rows = gather_collection_throughput(
        client, collections=STORE_TOTAL_COLLECTIONS, duration_seconds=duration_seconds
    )
    throughput_table: List[List[str]] = []
    for item in throughput_rows:
        store_total = store_totals.get(item.name)
        if store_total is not None:
            count_value: Optional[int] = store_total
        elif item.count is not None:
            count_value = int(item.count)
        else:
            count_value = None
        if count_value is None:
            count_str = "-"
        else:
            count_str = f"{count_value:,}"
        if count_value is not None and duration_seconds > 0:
            per_sec_value = float(count_value) / duration_seconds
        else:
            per_sec_value = item.per_sec
        throughput_table.append([item.name, count_str, fmt_rate(per_sec_value)])
    lines.extend(
        section(
            "Rollout / Attempt / Span / Resource / Worker Throughput",
            render_table(["Collection", "Count", "Per Sec"], throughput_table),
        )
    )

    metric_categories: Sequence[Tuple[str, Sequence[MetricGroupSpec]]] = [
        (
            "HTTP Metrics",
            (
                MetricGroupSpec(
                    title="agl.http ungrouped",
                    histogram_bucket_metric="agl_http_latency_bucket",
                    label_names=tuple(),
                    label_headers=tuple(),
                ),
                MetricGroupSpec(
                    title="agl.http grouped by path, method",
                    histogram_bucket_metric="agl_http_latency_bucket",
                    label_names=("path", "method"),
                    label_headers=("Path", "Method"),
                ),
                MetricGroupSpec(
                    title="agl.http grouped by path, method, status",
                    histogram_bucket_metric="agl_http_latency_bucket",
                    label_names=("path", "method", "status"),
                    label_headers=("Path", "Method", "Status"),
                ),
            ),
        ),
        (
            "Store Metrics",
            (
                MetricGroupSpec(
                    title="agl.store ungrouped",
                    histogram_bucket_metric="agl_store_latency_bucket",
                    label_names=tuple(),
                    label_headers=tuple(),
                ),
                MetricGroupSpec(
                    title="agl.store grouped by method",
                    histogram_bucket_metric="agl_store_latency_bucket",
                    label_names=("method",),
                    label_headers=("Method",),
                ),
                MetricGroupSpec(
                    title="agl.store grouped by method, status",
                    histogram_bucket_metric="agl_store_latency_bucket",
                    label_names=("method", "status"),
                    label_headers=("Method", "Status"),
                ),
                MetricGroupSpec(
                    title="agl.store grouped by store_pubmeth, method, status",
                    histogram_bucket_metric="agl_store_latency_bucket",
                    label_names=("store_pubmeth", "method", "status"),
                    label_headers=("Store Method", "Private Method", "Status"),
                ),
            ),
        ),
        (
            "Rollout Outcomes",
            (
                MetricGroupSpec(
                    title="agl.rollouts ungrouped",
                    histogram_bucket_metric="agl_rollouts_duration_bucket",
                    label_names=tuple(),
                    label_headers=tuple(),
                ),
                MetricGroupSpec(
                    title="agl.rollouts grouped by status",
                    histogram_bucket_metric="agl_rollouts_duration_bucket",
                    label_names=("status",),
                    label_headers=("Status",),
                ),
            ),
        ),
        (
            "Collection Metrics",
            (
                MetricGroupSpec(
                    title="agl.collections grouped by store_pubmeth, collection, operation",
                    histogram_bucket_metric="agl_collections_latency_bucket",
                    label_names=("store_pubmeth", "collection"),
                    label_headers=("Store Method", "Collection"),
                ),
                MetricGroupSpec(
                    title="agl.collections grouped by store_pubmeth, collection, operation, status",
                    histogram_bucket_metric="agl_collections_latency_bucket",
                    label_names=("store_pubmeth", "collection", "operation", "status"),
                    label_headers=("Store Method", "Collection", "Operation", "Status"),
                ),
                MetricGroupSpec(
                    title="agl.collections grouped by store_privmeth, collection, operation, status",
                    histogram_bucket_metric="agl_collections_latency_bucket",
                    label_names=("store_privmeth", "collection", "operation", "status"),
                    label_headers=("Store Priv Meth", "Collection", "Operation", "Status"),
                ),
                MetricGroupSpec(
                    title="agl.collections grouped by collection, operation, status",
                    histogram_bucket_metric="agl_collections_latency_bucket",
                    label_names=("collection", "operation", "status"),
                    label_headers=("Collection", "Operation", "Status"),
                ),
            ),
        ),
    ]

    store_method_time_per_sec: Dict[str, Optional[float]] = {}

    for category_title, specs in metric_categories:
        category_lines: List[str] = []
        for idx, spec in enumerate(specs):
            rows = gather_metric_group(
                client,
                spec,
                window=window,
                window_seconds=window_seconds,
                peak_window=peak_window,
                subquery_step=subquery_step,
                half_window=half_window,
                half_window_seconds=half_window_seconds,
            )
            if spec.histogram_bucket_metric == "agl_store_latency_bucket" and spec.label_names == ("method",):
                store_method_time_per_sec = {
                    row.label_values[0]: row.time_per_sec
                    for row in rows
                    if row.label_values and len(row.label_values) == 1
                }
            extra_column_specs: List[Tuple[str, Callable[[MetricRow], str]]] = []
            if "store_pubmeth" in spec.label_names:
                pubmeth_index = spec.label_names.index("store_pubmeth")
                extra_column_specs.append(
                    make_time_share_column(
                        label_index=pubmeth_index,
                        column_title="Share %",
                        time_per_sec_map=store_method_time_per_sec,
                    )
                )
            if "store_privmeth" in spec.label_names:
                privmeth_index = spec.label_names.index("store_privmeth")
                extra_column_specs.append(
                    make_time_share_column(
                        label_index=privmeth_index,
                        column_title="Share (Priv) %",
                        time_per_sec_map=store_method_time_per_sec,
                    )
                )
            extra_columns = extra_column_specs or None
            category_lines.append("### " + spec.title)
            category_lines.extend(render_metric_group_table(spec, rows, extra_columns=extra_columns))
            if idx != len(specs) - 1:
                category_lines.append("")
        lines.extend(section(category_title, category_lines))

    # Diagnostics
    diag = gather_diagnostics(client, window)
    diagnostics_blocks: List[List[str]] = []

    mongo_ops = cast(Dict[str, float], diag.get("mongo_ops", {}))
    mongo_latency_p50 = cast(Dict[str, float], diag.get("mongo_latency_p50", {}))
    mongo_latency_p95 = cast(Dict[str, float], diag.get("mongo_latency_p95", {}))
    mongo_latency_p99 = cast(Dict[str, float], diag.get("mongo_latency_p99", {}))
    mongo_op_keys = sorted(
        {
            *mongo_ops.keys(),
            *mongo_latency_p50.keys(),
            *mongo_latency_p95.keys(),
            *mongo_latency_p99.keys(),
        },
        key=str,
    )
    mongo_ops_rows = [
        [
            op or "-",
            fmt_rate(mongo_ops.get(op)),
            fmt_latency(mongo_latency_p50.get(op)),
            fmt_latency(mongo_latency_p95.get(op)),
            fmt_latency(mongo_latency_p99.get(op)),
        ]
        for op in mongo_op_keys
    ]
    diagnostics_blocks.append(render_table(["Mongo Operation", "Ops/s", "P50", "P95", "P99"], mongo_ops_rows))

    mongo_opcounters = cast(Dict[str, float], diag.get("mongo_opcounters", {}))
    mongo_opcounters_rows = [
        [op_type or "-", fmt_rate(rate)]
        for op_type, rate in sorted(mongo_opcounters.items(), key=lambda item: str(item[0]))
    ]
    diagnostics_blocks.append(render_table(["MongoDB Opcounter", "Ops/s"], mongo_opcounters_rows))

    mongo_misc_rows: List[List[str]] = []
    if diag.get("mongo_connections") is not None:
        mongo_misc_rows.append(["MongoDB connections (avg)", f"{diag['mongo_connections']:.2f}"])
    if mongo_misc_rows:
        diagnostics_blocks.append(render_table(["Mongo Metric", "Value"], mongo_misc_rows))

    node_rows: List[List[str]] = []
    if diag.get("cpu_usage") is not None:
        node_rows.append(["CPU usage", fmt_percentage(diag["cpu_usage"])])
    mem_total = diag.get("memory_total")
    mem_available = diag.get("memory_available")
    if mem_total and mem_available:
        used = mem_total - mem_available
        node_rows.append(
            ["Memory usage", f"{fmt_bytes(used)} / {fmt_bytes(mem_total)} ({fmt_percentage(used / mem_total)})"]
        )
    node_rows.append(["Network rx", f"{fmt_bytes(diag.get('network_rx'))}/s"])
    node_rows.append(["Network tx", f"{fmt_bytes(diag.get('network_tx'))}/s"])
    node_rows.append(["Disk read ops", fmt_rate(diag.get("disk_read_ops"))])
    node_rows.append(["Disk read bytes", f"{fmt_bytes(diag.get('disk_read_bytes'))}/s"])
    node_rows.append(["Disk write ops", fmt_rate(diag.get("disk_write_ops"))])
    node_rows.append(["Disk write bytes", f"{fmt_bytes(diag.get('disk_write_bytes'))}/s"])
    diagnostics_blocks.append(render_table(["Node Metric", "Value"], node_rows))

    diagnostics_lines: List[str] = []
    for idx, block in enumerate(diagnostics_blocks):
        diagnostics_lines.extend(block)
        if idx != len(diagnostics_blocks) - 1:
            diagnostics_lines.append("")

    lines.extend(section("Diagnostics", diagnostics_lines))

    print("\n".join(lines))


if __name__ == "__main__":  # pragma: no cover - manual execution
    main()


--- tests/benchmark/benchmark_store.py ---
# Copyright (c) Microsoft. All rights reserved.

"""Benchmarking store performance by writing and querying spans from the store."""

import argparse
import asyncio
import os
import random
import sys
import threading
import time
from typing import Any, Dict, List, Literal, Optional, Sequence, Set, Tuple, cast

from rich.console import Console

import agentlightning as agl
from agentlightning.utils.otel import get_tracer

from .utils import flatten_dict, random_dict

console = Console(width=200)

# Minus 10 to leave time for setting up env.
MAX_RUNTIME_SECONDS = (int(os.getenv("GITHUB_ACTIONS_TIMEOUT_MINUTES", "30")) - 10) * 60
MAX_STALE_SECONDS = 300


class RolloutProgressTracker:
    """Helper for tracking rollout progress and surfacing stale worker states."""

    def __init__(self, max_stale_seconds: float = MAX_STALE_SECONDS) -> None:
        self._max_stale_seconds = max_stale_seconds
        self._last_progress = time.perf_counter()

    def record_progress(self) -> None:
        self._last_progress = time.perf_counter()

    async def handle_progress(
        self,
        *,
        progress_made: bool,
        pending_rollout_ids: Sequence[str],
        store: agl.LightningStore,
    ) -> None:
        if progress_made:
            self.record_progress()
            return
        await self._check_for_stale(pending_rollout_ids=pending_rollout_ids, store=store)

    async def _check_for_stale(self, *, pending_rollout_ids: Sequence[str], store: agl.LightningStore) -> None:
        if not pending_rollout_ids:
            return
        elapsed = time.perf_counter() - self._last_progress
        if elapsed <= self._max_stale_seconds / 2:
            return
        console.print(f"Stale rollouts: {pending_rollout_ids}")
        if elapsed > self._max_stale_seconds:
            current_workers = await store.query_workers()
            console.print("Stalled. Current worker status shown below:")
            for worker in current_workers:
                console.print(f"  Worker: {worker}", no_wrap=True, overflow="ignore", crop=False)
            raise RuntimeError("Rollout progress has stalled for too long")


def _abort_due_to_timeout() -> None:
    sys.stderr.write(f"[benchmark] Exiting after exceeding the {MAX_RUNTIME_SECONDS // 60} minute timeout.\n")
    sys.stderr.flush()
    os._exit(1)


def _start_timeout_guard(timeout_seconds: float) -> threading.Timer:
    timer = threading.Timer(timeout_seconds, _abort_due_to_timeout)
    timer.daemon = True
    timer.start()
    return timer


def generate_attributes() -> Dict[str, Any]:
    return flatten_dict(
        random_dict(
            depth=(1, 3),
            breadth=(2, 6),
            key_length=(3, 20),
            value_length=(5, 300),
        )
    )


def make_agent(max_rounds: int, sleep_seconds: float) -> agl.LitAgent[str]:
    @agl.rollout
    async def agent(task: str, llm: agl.LLM):
        tracer = get_tracer()
        rounds = random.randint(1, max_rounds)
        selected_round = random.randint(0, rounds - 1)

        for i in range(rounds):
            with tracer.start_as_current_span(f"agent{i}") as span:
                # Nested Span
                with tracer.start_as_current_span(f"round{i}_1") as span:
                    await asyncio.sleep(random.uniform(0.0, sleep_seconds))
                    span.set_attributes(generate_attributes())
                    if i == selected_round:
                        span.set_attribute("task", task)

                # Nested Span
                with tracer.start_as_current_span(f"round{i}_2") as span:
                    await asyncio.sleep(random.uniform(0.0, sleep_seconds))
                    span.set_attributes(generate_attributes())

            if random.uniform(0, 1) < 0.5:
                agl.emit_reward(random.uniform(0.0, 1.0))

        # Final Span
        with tracer.start_as_current_span("final") as span:
            await asyncio.sleep(random.uniform(0.0, sleep_seconds))
            span.set_attributes(generate_attributes())

        agl.emit_reward(random.uniform(1.0, 2.0))

    return agent


def check_spans(spans: Sequence[agl.Span], task: str) -> None:
    """Check if the spans contain the task."""
    found_task = any(span.attributes.get("task") == task for span in spans)

    final_reward = agl.find_final_reward(spans)
    if final_reward is None:
        raise ValueError("Final reward is not found")
    if not (final_reward >= 1 and final_reward <= 2):
        raise ValueError(f"Final reward {final_reward} is not in the range of 1 to 2")
    if not found_task:
        raise ValueError(f"Task {task} is not found in the spans")


class AlgorithmBatch(agl.Algorithm):
    def __init__(
        self,
        mode: Literal["batch", "batch_partial", "single"],
        total_tasks: int,
        batch_size: Optional[int] = None,
        remaining_tasks: Optional[int] = None,
        concurrency: Optional[int] = None,
    ):
        self.mode = mode
        self.total_tasks = total_tasks
        self.batch_size = batch_size
        self.remaining_tasks = remaining_tasks
        self.concurrency = concurrency

    async def run(
        self, train_dataset: Optional[agl.Dataset[Any]] = None, val_dataset: Optional[agl.Dataset[Any]] = None
    ):
        if self.mode == "batch":
            assert self.batch_size is not None
            await self.algorithm_batch(self.total_tasks, self.batch_size)
        elif self.mode == "batch_partial":
            assert self.batch_size is not None
            assert self.remaining_tasks is not None
            await self.algorithm_batch_with_completion_threshold(
                self.total_tasks, self.batch_size, self.remaining_tasks
            )
        elif self.mode == "single":
            assert self.concurrency is not None
            await self.algorithm_batch_single(self.total_tasks, self.concurrency)
        else:
            raise ValueError(f"Invalid mode: {self.mode}")

    async def algorithm_batch(self, total_tasks: int, batch_size: int):
        """
        At each time, the algorithm will enqueue a batch of rollouts of size `batch_size`.
        The algorithm will use wait_for_rollouts to wait for all rollouts to complete.
        It then checks whether all rollouts are successful and check the spans to ensure the task is found
        and the last reward is in the range of 1 to 2.
        After that, the algorithm will enqueue a new batch of new tasks, until the total number of tasks is reached.
        """
        store = self.get_store()
        tracker = RolloutProgressTracker()
        submitted = 0

        while submitted < total_tasks:
            print(f"Submitting batch {submitted} of {total_tasks}")
            batch_count = min(batch_size, total_tasks - submitted)
            batch_rollouts: List[Tuple[str, str]] = []
            await store.add_resources(
                {
                    "llm": agl.LLM(
                        endpoint=f"http://localhost:{submitted}/v1",
                        model=f"test-model-{submitted}",
                    )
                }
            )
            for _ in range(batch_count):
                task_name = f"task-{submitted}-generated"
                rollout = await store.enqueue_rollout(input=task_name, mode="train")
                batch_rollouts.append((rollout.rollout_id, task_name))
                submitted += 1

            pending = {rollout_id: task_name for rollout_id, task_name in batch_rollouts}
            completed_ids: Set[str] = set()
            tracker.record_progress()
            while len(completed_ids) < len(batch_rollouts):
                finished_rollouts = await store.wait_for_rollouts(
                    rollout_ids=[rollout_id for rollout_id, _ in batch_rollouts],
                    timeout=0.0,
                )
                complete_ids_updated: bool = False
                for rollout in finished_rollouts:
                    rollout_id = rollout.rollout_id
                    if rollout_id in completed_ids:
                        continue
                    if rollout.status != "succeeded":
                        raise RuntimeError(f"Rollout {rollout_id} finished with status {rollout.status}")
                    spans = await store.query_spans(rollout_id=rollout_id, attempt_id="latest")
                    check_spans(spans, pending[rollout_id])
                    completed_ids.add(rollout_id)
                    complete_ids_updated = True

                unfinished_ids = [rollout_id for rollout_id, _ in batch_rollouts if rollout_id not in completed_ids]
                await tracker.handle_progress(
                    progress_made=complete_ids_updated,
                    pending_rollout_ids=unfinished_ids,
                    store=store,
                )

                await asyncio.sleep(5.0)

    async def algorithm_batch_with_completion_threshold(self, total_tasks: int, batch_size: int, remaining_tasks: int):
        """Different from `algorithm_batch`, this algorithm will use query_rollouts to get rollouts' status.
        It will enqueue a new batch of new tasks when the number of running rollouts is less than the remaining tasks threshold.
        """
        store = self.get_store()
        tracker = RolloutProgressTracker()
        submitted = 0
        completed = 0
        active_rollouts: Dict[str, str] = {}

        while completed < total_tasks:
            console.print(f"Completed {completed} of {total_tasks} rollouts")
            if submitted < total_tasks and len(active_rollouts) < remaining_tasks:
                batch_count = min(batch_size, total_tasks - submitted)
                await store.add_resources(
                    {
                        "llm": agl.LLM(
                            endpoint=f"http://localhost:{submitted}/v1",
                            model=f"test-model-{submitted}",
                        )
                    }
                )
                for _ in range(batch_count):
                    task_name = f"task-{submitted}"
                    rollout = await store.enqueue_rollout(input=task_name, mode="train")
                    active_rollouts[rollout.rollout_id] = task_name
                    submitted += 1
                continue

            if not active_rollouts:
                await asyncio.sleep(0.01)
                continue

            rollouts = await store.query_rollouts(rollout_id_in=list(active_rollouts.keys()))
            newly_completed = 0
            for rollout in rollouts:
                rollout_id = rollout.rollout_id
                if rollout_id not in active_rollouts:
                    continue
                if rollout.status in ("queuing", "preparing", "running", "requeuing"):
                    continue
                if rollout.status != "succeeded":
                    raise RuntimeError(f"Rollout {rollout_id} finished with status {rollout.status}")
                spans = await store.query_spans(rollout_id=rollout_id, attempt_id="latest")
                check_spans(spans, active_rollouts.pop(rollout_id))
                completed += 1
                newly_completed += 1

            await tracker.handle_progress(
                progress_made=newly_completed > 0,
                pending_rollout_ids=list(active_rollouts.keys()),
                store=store,
            )

            if newly_completed == 0:
                await asyncio.sleep(5.0)

    async def algorithm_batch_single(self, total_tasks: int, concurrency: int):
        """Different from `algorithm_batch`, this algorithm will use one async function to enqueue one rollout at a time.
        The function only cares about the rollout it's currently processing.
        It waits for the rollouts with `get_rollout_by_id` and check the spans to ensure the rollout is successful.
        The concurrency is managed via a asyncio semaphore.
        """
        store = self.get_store()
        semaphore = asyncio.Semaphore(concurrency)
        tracker = RolloutProgressTracker()
        active_rollouts: Set[str] = set()
        active_lock = asyncio.Lock()

        async def emit_progress(progress_made: bool) -> None:
            if progress_made:
                async with active_lock:
                    pending_ids = list(active_rollouts)
                await tracker.handle_progress(progress_made=True, pending_rollout_ids=pending_ids, store=store)
                return
            async with active_lock:
                pending_ids = list(active_rollouts)
            await tracker.handle_progress(progress_made=False, pending_rollout_ids=pending_ids, store=store)

        async def handle_single(task_index: int) -> None:
            task_name = f"task-{task_index}"
            async with semaphore:
                console.print(f"Submitting task {task_index} of {total_tasks}")
                await store.add_resources(
                    {
                        "llm": agl.LLM(
                            endpoint=f"http://localhost:{task_index}/v1",
                            model=f"test-model-{task_index}",
                        )
                    }
                )
                rollout = await store.enqueue_rollout(input=task_name, mode="train")
                rollout_id = rollout.rollout_id
                async with active_lock:
                    active_rollouts.add(rollout_id)
                try:
                    while True:
                        current = await store.get_rollout_by_id(rollout_id)
                        if current is not None and current.status in ("failed", "succeeded", "cancelled"):
                            if current.status != "succeeded":
                                raise RuntimeError(f"Rollout {rollout_id} finished with status {current.status}")
                            break
                        await emit_progress(progress_made=False)
                        await asyncio.sleep(5.0)
                    spans = await store.query_spans(rollout_id=rollout_id, attempt_id="latest")
                    check_spans(spans, task_name)
                    await emit_progress(progress_made=True)
                finally:
                    async with active_lock:
                        active_rollouts.discard(rollout_id)

        all_tasks = [handle_single(i) for i in range(total_tasks)]
        await asyncio.gather(*all_tasks)


def parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Benchmark LightningStore implementations with synthetic rollouts.")
    parser.add_argument("--store-url", default="http://localhost:4747", help="Lightning Store endpoint base URL.")
    parser.add_argument(
        "--mode",
        choices=("batch", "batch_partial", "single"),
        default="batch",
        help="Algorithm mode to exercise different submission patterns.",
    )
    parser.add_argument("--total-tasks", type=int, default=128 * 128, help="Total number of rollouts to submit.")
    parser.add_argument("--batch-size", type=int, default=128, help="Batch size for batch-style modes.")
    parser.add_argument(
        "--remaining-tasks",
        type=int,
        default=512,
        help="Target number of in-flight rollouts before submitting more (batch_partial mode).",
    )
    parser.add_argument("--concurrency", type=int, default=32, help="Maximum concurrent rollouts for single mode.")
    parser.add_argument("--n-runners", type=int, default=32, help="Number of runner processes to launch.")
    parser.add_argument("--max-rounds", type=int, default=10, help="Maximum number of rounds for each rollout.")
    parser.add_argument("--sleep-seconds", type=float, default=1.0, help="Sleep seconds for each rollout.")
    parser.add_argument("--debug", action="store_true", help="Enable verbose debug logging.")
    parser.add_argument("--debug-otel", action="store_true", help="Enable verbose debug logging for OTel.")
    args = parser.parse_args(argv)

    if args.total_tasks <= 0:
        parser.error("--total-tasks must be positive")
    if args.n_runners <= 0:
        parser.error("--n-runners must be positive")
    if args.mode in {"batch", "batch_partial"} and (args.batch_size is None or args.batch_size <= 0):
        parser.error("--batch-size must be positive for batch modes")
    if args.mode == "batch_partial" and (args.remaining_tasks is None or args.remaining_tasks <= 0):
        parser.error("--remaining-tasks must be positive for batch_partial mode")
    if args.mode == "single" and (args.concurrency is None or args.concurrency <= 0):
        parser.error("--concurrency must be positive for single mode")
    if args.max_rounds <= 0:
        parser.error("--max-rounds must be positive")
    if args.sleep_seconds <= 0:
        parser.error("--sleep-seconds must be positive")

    return args


def main(argv: Optional[Sequence[str]] = None) -> None:
    args = parse_args(argv)
    agl.setup_logging(
        "DEBUG" if args.debug else "INFO",
        submodule_levels={"agentlightning.utils.otel": "DEBUG" if args.debug_otel else "INFO"},
    )
    store = agl.LightningStoreClient(args.store_url)
    timeout_guard = _start_timeout_guard(MAX_RUNTIME_SECONDS)
    try:
        trainer = agl.Trainer(
            store=store,
            algorithm=AlgorithmBatch(
                mode=cast(Literal["batch", "batch_partial", "single"], args.mode),
                total_tasks=args.total_tasks,
                batch_size=args.batch_size,
                remaining_tasks=args.remaining_tasks,
                concurrency=args.concurrency,
            ),
            n_runners=args.n_runners,
            strategy={
                "type": "cs",
                "managed_store": False,
            },
        )
        trainer.fit(make_agent(max_rounds=args.max_rounds, sleep_seconds=args.sleep_seconds))
    finally:
        timeout_guard.cancel()
        asyncio.run(store.close())


if __name__ == "__main__":
    main()


--- tests/benchmark/collection_benchmark.py ---
# Copyright (c) Microsoft. All rights reserved.

"""Collection-level contention benchmarks for Agent Lightning."""

from __future__ import annotations

import argparse
import asyncio
import json
import math
import multiprocessing as mp
import random
import threading
import time
import uuid
from contextlib import asynccontextmanager
from dataclasses import asdict, dataclass
from multiprocessing.process import BaseProcess
from pathlib import Path
from queue import Empty, Queue
from typing import Any, AsyncContextManager, Callable, Dict, List, Mapping, Sequence

from pymongo import AsyncMongoClient
from rich.console import Console
from rich.table import Table

from agentlightning.store.collection.base import LightningCollections
from agentlightning.store.collection.memory import InMemoryLightningCollections
from agentlightning.store.collection.mongo import MongoClientPool, MongoLightningCollections
from agentlightning.types import Rollout, RolloutConfig

console = Console()

DEFAULT_TOTAL_TASKS = 100_000
DEFAULT_CONCURRENCY = 1_024
DEFAULT_TASK_PREFIX = "collection-bench"
MONGO_DEFAULT_DB = "agentlightning_collection_bench"


@dataclass
class WorkerResult:
    durations: List[float]
    failures: int


@dataclass
class BenchmarkResult:
    backend: str
    name: str
    total_tasks: int
    concurrency: int
    successes: int
    failures: int
    duration: float
    throughput: float
    avg_latency: float
    p50_latency: float
    p95_latency: float
    p99_latency: float
    min_latency: float
    max_latency: float
    success_rate: float
    ops_per_worker: float

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


def parse_args(argv: Sequence[str] | None = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Benchmark LightningStore collections without the store server.")
    parser.add_argument("benchmark", choices=("insert", "dequeue"), help="Benchmarks to run.")
    parser.add_argument("--backend", choices=("memory", "mongo"), default="memory", help="Collection backend to test.")
    parser.add_argument("--total-tasks", type=int, default=DEFAULT_TOTAL_TASKS, help="Total operations to run.")
    parser.add_argument("--concurrency", type=int, default=DEFAULT_CONCURRENCY, help="Number of concurrent workers.")
    parser.add_argument("--task-prefix", default=DEFAULT_TASK_PREFIX, help="Base prefix for generated workload IDs.")
    parser.add_argument("--summary-file", help="Optional newline-delimited JSON summary output.")
    parser.add_argument(
        "--mongo-uri", default="mongodb://localhost:27017/?replicaSet=rs0", help="Mongo connection URI."
    )
    parser.add_argument("--mongo-database", default=MONGO_DEFAULT_DB, help="Mongo database for benchmark artifacts.")
    return parser.parse_args(argv)


def _percentile(values: Sequence[float], percentile: float) -> float:
    if not values:
        return 0.0
    if len(values) == 1:
        return values[0]
    rank = (len(values) - 1) * percentile
    lower = math.floor(rank)
    upper = math.ceil(rank)
    if lower == upper:
        return values[int(rank)]
    return values[lower] * (upper - rank) + values[upper] * (rank - lower)


def _aggregate_results(
    *,
    backend: str,
    name: str,
    results: Sequence[WorkerResult],
    concurrency: int,
    total_tasks: int,
    duration: float,
) -> BenchmarkResult:
    successes = sum(len(result.durations) for result in results)
    failures = sum(result.failures for result in results)
    latencies = [lat for result in results for lat in result.durations]
    throughput = successes / duration if duration > 0 else 0.0
    avg_latency = (sum(latencies) / len(latencies)) if latencies else 0.0
    sorted_latencies = sorted(latencies)
    return BenchmarkResult(
        backend=backend,
        name=name,
        total_tasks=total_tasks,
        concurrency=concurrency,
        successes=successes,
        failures=failures,
        duration=duration,
        throughput=throughput,
        avg_latency=avg_latency,
        p50_latency=_percentile(sorted_latencies, 0.50),
        p95_latency=_percentile(sorted_latencies, 0.95),
        p99_latency=_percentile(sorted_latencies, 0.99),
        min_latency=sorted_latencies[0] if sorted_latencies else 0.0,
        max_latency=sorted_latencies[-1] if sorted_latencies else 0.0,
        success_rate=(successes / (successes + failures)) if (successes + failures) else 0.0,
        ops_per_worker=(successes / concurrency) if concurrency else 0.0,
    )


def _render_results(results: Sequence[BenchmarkResult]) -> None:
    if not results:
        console.print("[yellow]No benchmark results to display.[/yellow]")
        return
    table = Table(title="Collection Benchmarks", show_lines=False)
    table.add_column("Backend")
    table.add_column("Benchmark")
    table.add_column("Successes", justify="right")
    table.add_column("Failures", justify="right")
    table.add_column("Throughput (req/s)", justify="right")
    table.add_column("Avg Latency (ms)", justify="right")
    table.add_column("P95 (ms)", justify="right")
    table.add_column("P99 (ms)", justify="right")
    table.add_column("Success Rate", justify="right")
    for result in results:
        table.add_row(
            result.backend,
            result.name,
            f"{result.successes:,}",
            f"{result.failures:,}",
            f"{result.throughput:,.2f}",
            f"{result.avg_latency * 1e3:,.2f}",
            f"{result.p95_latency * 1e3:,.2f}",
            f"{result.p99_latency * 1e3:,.2f}",
            f"{result.success_rate * 100:,.2f}%",
        )
    console.print(table)


def _write_summary(results: Sequence[BenchmarkResult], file_path: Path) -> None:
    file_path.parent.mkdir(parents=True, exist_ok=True)
    with file_path.open("a", encoding="utf-8") as handle:
        for result in results:
            handle.write(json.dumps(result.to_dict()) + "\n")


def _make_rollout(worker_index: int, sequence: int, task_prefix: str) -> Rollout:
    rollout_id = f"{task_prefix}-ro-{worker_index}-{sequence}-{uuid.uuid4().hex}"
    current_time = time.time()
    return Rollout(
        rollout_id=rollout_id,
        input={"task": rollout_id},
        start_time=current_time,
        end_time=None,
        mode="train",
        resources_id=None,
        status="queuing",
        config=RolloutConfig(),
        metadata={},
    )


async def _preload_queue(collections: LightningCollections, total_tasks: int, task_prefix: str) -> None:
    batch: List[str] = []
    for idx in range(total_tasks):
        batch.append(f"{task_prefix}-queue-{idx}")
        if len(batch) >= 512:
            async with collections.atomic(mode="rw", labels=["rollout_queue"]) as collections_atomic:
                await collections_atomic.rollout_queue.enqueue(batch)
            batch.clear()
    if batch:
        async with collections.atomic(mode="rw", labels=["rollout_queue"]) as collections_atomic:
            await collections_atomic.rollout_queue.enqueue(batch)


async def _reset_mongo_database(uri: str, database: str) -> None:
    client = AsyncMongoClient[Mapping[str, Any]](uri)
    try:
        await client.drop_database(database)
    finally:
        await client.close()


class BaseBenchmark:
    """Shared control flow for collection benchmarks across backends."""

    def __init__(
        self, *, backend: str, total_tasks: int, concurrency: int, task_prefix: str, name: str, kind: str
    ) -> None:
        self.backend = backend
        self.total_tasks = total_tasks
        self.concurrency = concurrency
        self.task_prefix = task_prefix
        self.name = name
        self.kind = kind

    def run(self) -> BenchmarkResult:
        asyncio.run(self.setup())
        start = time.perf_counter()

        results = self.spawn_workers(worker_fn=self.worker_entrypoint)
        duration = time.perf_counter() - start
        return _aggregate_results(
            backend=self.backend,
            name=self.name,
            results=results,
            concurrency=self.concurrency,
            total_tasks=self.total_tasks,
            duration=duration,
        )

    def spawn_workers(
        self,
        worker_fn: Callable[[int, Any, Any], WorkerResult],
    ) -> List[WorkerResult]:
        raise NotImplementedError()

    def worker_entrypoint(self, worker_index: int, task_queue: Any, start_barrier: Any) -> WorkerResult:
        start_barrier.wait()
        console.print(f"Worker {worker_index} starting")

        async def _runner() -> WorkerResult:
            async with self.worker_context() as collections:
                if self.kind == "insert":
                    return await insert_worker_async(
                        collections,
                        worker_index=worker_index,
                        task_queue=task_queue,
                        task_prefix=self.task_prefix,
                    )
                if self.kind == "dequeue":
                    return await dequeue_worker_async(
                        collections,
                        worker_index=worker_index,
                        task_queue=task_queue,
                    )
                raise ValueError(f"Unknown benchmark kind: {self.kind}")

        return asyncio.run(_runner())

    def worker_context(self, *args: Any, **kwargs: Any) -> AsyncContextManager[LightningCollections]:
        """Provide the execution context for the benchmark workers."""
        raise NotImplementedError()

    async def setup(self) -> None:
        """Prepare backend-specific state before running workers."""
        if self.kind == "dequeue":
            async with self.worker_context() as collections:
                await _preload_queue(collections, self.total_tasks, self.task_prefix)


class MemoryBenchmark(BaseBenchmark):

    def __init__(
        self,
        *,
        total_tasks: int,
        concurrency: int,
        task_prefix: str,
        kind: str,
    ) -> None:
        super().__init__(
            total_tasks=total_tasks,
            concurrency=concurrency,
            task_prefix=task_prefix,
            name=f"collection-{kind}",
            backend="memory",
            kind=kind,
        )
        self.collections = InMemoryLightningCollections(lock_type="thread")

    def spawn_workers(
        self,
        worker_fn: Callable[[int, Any, Any], WorkerResult],
    ) -> List[WorkerResult]:
        task_queue: Queue[int] = Queue()
        for task_id in range(self.total_tasks):
            task_queue.put(task_id)
        start_barrier = threading.Barrier(self.concurrency)
        results: List[WorkerResult | None] = [None] * self.concurrency

        def _thread_target(worker_index: int) -> None:
            results[worker_index] = worker_fn(worker_index, task_queue, start_barrier)

        threads: List[threading.Thread] = []
        for worker_index in range(self.concurrency):
            thread = threading.Thread(target=_thread_target, args=(worker_index,))
            thread.start()
            threads.append(thread)
        for thread in threads:
            thread.join()

        return [result for result in results if result is not None]

    @asynccontextmanager
    async def worker_context(self, *args: Any, **kwargs: Any):
        yield self.collections


class MongoBenchmark(BaseBenchmark):
    def __init__(
        self,
        *,
        total_tasks: int,
        concurrency: int,
        task_prefix: str,
        kind: str,
        mongo_uri: str,
        mongo_database: str,
    ) -> None:
        super().__init__(
            total_tasks=total_tasks,
            concurrency=concurrency,
            task_prefix=task_prefix,
            name=f"collection-{kind}",
            backend="mongo",
            kind=kind,
        )
        self.mongo_uri = mongo_uri
        self.mongo_database = mongo_database
        self.partition_id = f"partition-{uuid.uuid4().hex}"

    async def setup(self) -> None:
        await _reset_mongo_database(self.mongo_uri, self.mongo_database)
        return await super().setup()

    @asynccontextmanager
    async def worker_context(self):
        pool = MongoClientPool[Mapping[str, Any]](mongo_uri=self.mongo_uri)
        collections = MongoLightningCollections(
            client_pool=pool,
            database_name=self.mongo_database,
            partition_id=self.partition_id,
            tracker=None,
        )

        try:
            yield collections
        finally:
            await pool.close()

    def spawn_workers(
        self,
        worker_fn: Callable[[int, Any, Any], WorkerResult],
    ) -> List[WorkerResult]:
        ctx = mp.get_context("fork")
        task_queue = ctx.Queue()
        for task_id in range(self.total_tasks):
            task_queue.put(task_id)
        start_barrier = ctx.Barrier(self.concurrency)
        result_queue = ctx.Queue()

        processes: List[BaseProcess] = []
        for worker_index in range(self.concurrency):
            process = ctx.Process(
                target=_process_worker_target,
                args=(self, worker_index, task_queue, start_barrier, result_queue),
            )
            process.start()
            processes.append(process)

        collected: List[WorkerResult] = []
        errors: List[Exception] = []
        for _ in range(self.concurrency):
            item = result_queue.get()
            if isinstance(item, Exception):
                errors.append(item)
            else:
                collected.append(item)

        for process in processes:
            process.join()

        if errors:
            raise RuntimeError("One or more worker processes failed") from errors[0]

        return collected


def _process_worker_target(
    benchmark: BaseBenchmark,
    worker_index: int,
    task_queue: Any,
    start_barrier: Any,
    result_queue: Any,
) -> None:
    try:
        result = benchmark.worker_entrypoint(worker_index, task_queue, start_barrier)
    except Exception as exc:
        result_queue.put(exc)
        raise
    else:
        result_queue.put(result)


async def insert_worker_async(
    collections: LightningCollections,
    *,
    worker_index: int,
    task_queue: Any,
    task_prefix: str,
) -> WorkerResult:
    durations: List[float] = []
    failures = 0
    while True:
        try:
            sequence = task_queue.get_nowait()
        except Empty:
            break
        rollout = _make_rollout(worker_index, sequence, task_prefix)
        req_start = time.perf_counter()
        try:
            async with collections.atomic(mode="rw", labels=["rollouts"]) as collections_atomic:
                if random.uniform(0, 1) < 0.01:
                    console.print("Inserting rollout:", rollout.rollout_id)
                await collections_atomic.rollouts.insert([rollout])
            durations.append(time.perf_counter() - req_start)
        except Exception:
            failures += 1
    return WorkerResult(durations=durations, failures=failures)


async def dequeue_worker_async(
    collections: LightningCollections,
    *,
    worker_index: int,
    task_queue: Any,
) -> WorkerResult:
    del worker_index  # unused but kept for symmetry
    durations: List[float] = []
    failures = 0
    while True:
        try:
            task_queue.get_nowait()
        except Empty:
            break
        req_start = time.perf_counter()
        try:
            async with collections.atomic(mode="rw", labels=["rollout_queue"]) as collections_atomic:
                items = await collections_atomic.rollout_queue.dequeue(limit=1)
                if items and random.uniform(0, 1) < 0.01:
                    console.print("Dequeued items:", items[0])
        except Exception:
            failures += 1
            continue
        if not items:
            break
        durations.append(time.perf_counter() - req_start)
    return WorkerResult(durations=durations, failures=failures)


def run_benchmark(args: argparse.Namespace, benchmark_kind: str) -> BenchmarkResult:
    params = {
        "total_tasks": args.total_tasks,
        "concurrency": args.concurrency,
        "task_prefix": args.task_prefix,
    }
    if args.backend == "memory":
        return MemoryBenchmark(kind=benchmark_kind, **params).run()

    mongo_params = {
        **params,
        "mongo_uri": args.mongo_uri,
        "mongo_database": args.mongo_database,
    }
    return MongoBenchmark(kind=benchmark_kind, **mongo_params).run()


def main(argv: Sequence[str] | None = None) -> None:
    args = parse_args(argv)
    if args.total_tasks <= 0:
        raise ValueError("total-tasks must be positive")
    if args.concurrency <= 0:
        raise ValueError("concurrency must be positive")

    results: List[BenchmarkResult] = []
    results.append(run_benchmark(args, args.benchmark))

    _render_results(results)

    if args.summary_file:
        _write_summary(results, Path(args.summary_file))


if __name__ == "__main__":  # pragma: no cover - manual execution
    main()


--- tests/benchmark/micro_benchmark.py ---
# Copyright (c) Microsoft. All rights reserved.

"""Micro benchmarks for the store."""

from __future__ import annotations

import argparse
import asyncio
import multiprocessing
import time
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Sequence

from rich.console import Console

import agentlightning as agl
from agentlightning.types import EnqueueRolloutRequest, OtelResource, Span, SpanContext, TraceStatus
from agentlightning.utils.metrics import ConsoleMetricsBackend, MultiMetricsBackend
from agentlightning.utils.system_snapshot import system_snapshot

from .utils import flatten_dict, random_dict

console = Console()


async def _enqueue_rollouts_for_benchmark(store_url: str, *, total_rollouts: int, task_prefix: str) -> None:
    """Utility that enqueues a fixed number of rollouts for a benchmark."""
    store = agl.LightningStoreClient(store_url)
    console.print(f"Enqueuing {total_rollouts} rollouts for {task_prefix} benchmark")
    try:
        await store.enqueue_many_rollouts(
            [EnqueueRolloutRequest(input={"task": f"{task_prefix}-Task-{i}"}) for i in range(total_rollouts)]
        )
    finally:
        await store.close()


def _close_store_client(store: agl.LightningStoreClient) -> None:
    try:
        asyncio.run(store.close())
    except Exception:
        pass


def _make_span(rollout_id: str, attempt_id: str, sequence_id: int, name: str, attribute_size: int) -> Span:
    trace_hex = f"{sequence_id:032x}"
    span_hex = f"{sequence_id:016x}"
    return Span(
        rollout_id=rollout_id,
        attempt_id=attempt_id,
        sequence_id=sequence_id,
        trace_id=trace_hex,
        span_id=span_hex,
        parent_id=None,
        name=name,
        status=TraceStatus(status_code="OK"),
        attributes=flatten_dict(
            random_dict(
                depth=1,
                breadth=attribute_size,
                key_length=(3, 20),
                value_length=(5, 300),
            )
        ),
        events=[],
        links=[],
        start_time=None,
        end_time=None,
        context=SpanContext(trace_id=trace_hex, span_id=span_hex, is_remote=False, trace_state={}),
        parent=None,
        resource=OtelResource(attributes={}, schema_url=""),
    )


@dataclass
class BenchmarkSummary:
    mode: str
    total_tasks: int
    successes: int
    duration: float

    @property
    def success_rate(self) -> float:
        if self.total_tasks == 0:
            return 0.0
        return self.successes / self.total_tasks

    @property
    def throughput(self) -> float:
        if self.duration <= 0:
            return 0.0
        return self.successes / self.duration


def parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Micro benchmarks for the store.")
    parser.add_argument("--store-url", default="http://localhost:4747", help="Lightning Store endpoint base URL.")
    parser.add_argument("--summary-file", help="File to append final benchmark summary.")
    parser.add_argument(
        "mode",
        choices=("worker", "dequeue-empty", "dequeue-only", "rollout", "dequeue-update-attempt", "metrics"),
        help="Mode to exercise different operations (metrics targets MultiMetricsBackend fan-out).",
    )
    args = parser.parse_args(argv)
    return args


def _update_worker_task(args: tuple[str, str, str]) -> bool:
    store_url, worker_id, task_id = args
    console.print(f"Updating worker {worker_id} for task {task_id}")
    store = agl.LightningStoreClient(store_url)
    try:
        asyncio.run(store.update_worker(worker_id, system_snapshot()))
        return True
    except Exception as e:
        console.print(f"Error updating worker {worker_id} for task {task_id}: {e}")
        return False
    finally:
        _close_store_client(store)


def simulate_many_update_workers(store_url: str) -> BenchmarkSummary:
    """Simulate many update workers."""

    start_time = time.time()

    # Use a multiprocessing pool to update workers.
    worker_ids = [(f"Worker-{i % 1024}", f"Task-{j}") for i in range(1024) for j in range(10)]
    with multiprocessing.get_context("fork").Pool(processes=1024) as pool:
        successful_tasks = pool.map(_update_worker_task, [(store_url, *worker_id) for worker_id in worker_ids])

    end_time = time.time()
    successes = sum(successful_tasks)
    duration = end_time - start_time
    throughput = successes / duration if duration > 0 else 0.0
    console.print(f"Success rate: {successes / len(worker_ids):.3f}")
    console.print(f"Time taken: {duration:.3f} seconds")
    console.print(f"Throughput: {throughput:.3f} workers/second")
    return BenchmarkSummary(mode="worker", total_tasks=len(worker_ids), successes=successes, duration=duration)


def _dequeue_empty_and_update_workers_task(args: tuple[str, str, str]) -> bool:
    store_url, worker_id, task_id = args
    console.print(f"Dequeueing empty and updating worker {worker_id} for task {task_id}")
    store = agl.LightningStoreClient(store_url)

    async def _async_task() -> None:
        await store.dequeue_rollout(worker_id=worker_id)
        await store.update_worker(worker_id, system_snapshot())

    try:
        asyncio.run(_async_task())
        return True
    except Exception as e:
        console.print(f"Error dequeueing empty and updating worker {worker_id} for task {task_id}: {e}")
        return False
    finally:
        _close_store_client(store)


def simulate_dequeue_empty_and_update_workers(store_url: str) -> BenchmarkSummary:
    """Simulate dequeue empty and update workers."""
    start_time = time.time()

    worker_ids = [(f"Worker-{i % 1024}", f"Task-{j}") for i in range(1024) for j in range(10)]
    with multiprocessing.get_context("fork").Pool(processes=1024) as pool:
        successful_tasks = pool.map(
            _dequeue_empty_and_update_workers_task, [(store_url, *worker_id) for worker_id in worker_ids]
        )

    end_time = time.time()
    successes = sum(successful_tasks)
    duration = end_time - start_time
    throughput = successes / duration if duration > 0 else 0.0
    console.print(f"Success rate: {successes / len(worker_ids):.3f}")
    console.print(f"Time taken: {duration:.3f} seconds")
    console.print(f"Throughput: {throughput:.3f} workers/second")
    return BenchmarkSummary(mode="dequeue-empty", total_tasks=len(worker_ids), successes=successes, duration=duration)


def _rollout_flow_task(args: tuple[str, int, int]) -> bool:
    store_url, task_id, spans_per_attempt = args
    store = agl.LightningStoreClient(store_url)

    async def _async_task() -> None:
        console.print(f"Starting rollout for task {task_id} with {spans_per_attempt} spans")
        attempted = await store.start_rollout(input={"task": task_id})
        rollout_id = attempted.rollout_id
        attempt_id = attempted.attempt.attempt_id
        for seq in range(1, spans_per_attempt + 1):
            console.print(f"Adding span {seq} for task {task_id} with {spans_per_attempt} spans")
            span = _make_span(
                rollout_id,
                attempt_id,
                task_id * spans_per_attempt + seq,
                f"micro-span-{seq}",
                attribute_size=1,
            )
            await store.add_span(span)
        console.print(f"Updating attempt {attempt_id} for task {task_id} with {spans_per_attempt} spans")
        await store.update_attempt(rollout_id, attempt_id, status="succeeded")

    try:
        asyncio.run(_async_task())
        return True
    except Exception as e:
        console.print(f"Error running rollout task {task_id}: {e}")
        return False
    finally:
        _close_store_client(store)


def simulate_rollout_with_spans(store_url: str, spans_per_attempt: int = 4) -> BenchmarkSummary:
    """Simulate full rollout lifecycle with spans."""
    start_time = time.time()
    task_ids = list(range(1024 * 4))
    with multiprocessing.get_context("fork").Pool(processes=256) as pool:
        successful_tasks = pool.map(
            _rollout_flow_task, [(store_url, task_id, spans_per_attempt) for task_id in task_ids]
        )

    end_time = time.time()
    successes = sum(successful_tasks)
    duration = end_time - start_time
    throughput = successes / duration if duration > 0 else 0.0
    console.print(f"Rollout success rate: {successes / len(task_ids):.3f}")
    console.print(f"Time taken: {duration:.3f} seconds")
    console.print(f"Throughput: {throughput:.3f} rollouts/second")
    return BenchmarkSummary(mode="rollout", total_tasks=len(task_ids), successes=successes, duration=duration)


def _dequeue_only_task(args: tuple[str, str, str]) -> bool:
    store_url, worker_id, task_id = args
    console.print(f"[Dequeue-Only Task {task_id}] Dequeueing rollout for worker {worker_id}")
    store = agl.LightningStoreClient(store_url)

    async def _async_task() -> bool:
        attempted = await store.dequeue_rollout()  # no worker_id
        if attempted is None:
            console.print(f"[Dequeue-Only Task {task_id}] No rollout available to dequeue")
            return False
        return True

    try:
        return asyncio.run(_async_task())
    except Exception as e:
        console.print(f"Error dequeueing only worker {worker_id} for task {task_id}: {e}")
        return False
    finally:
        _close_store_client(store)


def dequeue_rollouts(store_url: str) -> BenchmarkSummary:
    """Benchmark simple dequeues without any additional mutations."""
    start_time = time.time()
    total_workers = 512
    attempts_per_worker = 16
    total_rollouts = total_workers * attempts_per_worker

    asyncio.run(_enqueue_rollouts_for_benchmark(store_url, total_rollouts=total_rollouts, task_prefix="DequeueOnly"))

    worker_jobs = [
        (f"Worker-{worker_idx}-Attempt-{attempt_idx}", f"Task-{attempt_idx * total_workers + worker_idx}")
        for worker_idx in range(total_workers)
        for attempt_idx in range(attempts_per_worker)
    ]
    with multiprocessing.get_context("fork").Pool(processes=total_workers) as pool:
        successful_tasks = pool.map(
            _dequeue_only_task, [(store_url, worker_id, task_id) for worker_id, task_id in worker_jobs]
        )

    async def _query_remaining_rollouts() -> List[str]:
        store = agl.LightningStoreClient(store_url)
        try:
            remaining_rollouts = await store.query_rollouts(status_in=["queuing"])
            return [item.rollout_id for item in remaining_rollouts]
        finally:
            await store.close()

    end_time = time.time()
    remaining_rollouts = asyncio.run(_query_remaining_rollouts())
    successes = sum(successful_tasks)
    duration = end_time - start_time
    throughput = successes / duration if duration > 0 else 0.0
    console.print(f"Remaining rollouts: {remaining_rollouts}")
    console.print(f"Remaining rollouts count: {len(remaining_rollouts)}")
    console.print(f"Dequeue-only success rate: {successes / len(worker_jobs):.3f}")
    console.print(f"Time taken: {duration:.3f} seconds")
    console.print(f"Throughput: {throughput:.3f} rollouts/second")
    return BenchmarkSummary(mode="dequeue-only", total_tasks=len(worker_jobs), successes=successes, duration=duration)


def _dequeue_and_update_attempt_task(args: tuple[str, str, str, int]) -> bool:
    store_url, worker_id, task_id, spans_per_attempt = args
    console.print(f"Dequeueing and update attempt with worker {worker_id} for task {task_id}")
    store = agl.LightningStoreClient(store_url)

    async def _async_task() -> bool:
        console.print(f"[Task {task_id}] Dequeueing rollout")
        attempted = await store.dequeue_rollout(worker_id=worker_id)
        if attempted is None:
            console.print(f"[Task {task_id}] No rollout available to dequeue")
            return False
        console.print(f"[Task {task_id}] Retrieving span sequence IDs")
        sequence_ids = await store.get_many_span_sequence_ids(
            [(attempted.rollout_id, attempted.attempt.attempt_id) for _ in range(spans_per_attempt)]
        )
        if len(sequence_ids) != spans_per_attempt:
            console.print(
                f"[Task {task_id}] Unable to retrieve enough span sequence IDs: "
                f"expected={spans_per_attempt} got={len(sequence_ids)}"
            )
            return False
        console.print(f"[Task {task_id}] Adding {spans_per_attempt} spans")
        spans = [
            _make_span(
                attempted.rollout_id,
                attempted.attempt.attempt_id,
                sequence_id,
                f"micro-span-{sequence_id}",
                attribute_size=32,
            )
            for sequence_id in sequence_ids
        ]
        stored_spans = await store.add_many_spans(spans)
        if len(stored_spans) != len(spans):
            console.print(
                f"[Task {task_id}] Only stored {len(stored_spans)}/{len(spans)} spans for "
                f"rollout_id={attempted.rollout_id} attempt_id={attempted.attempt.attempt_id}"
            )
            return False
        console.print(
            f"[Task {task_id}] Updating attempt to succeeded: rollout_id={attempted.rollout_id} "
            f"attempt_id={attempted.attempt.attempt_id}"
        )
        await store.update_attempt(attempted.rollout_id, attempted.attempt.attempt_id, status="succeeded")
        return True

    try:
        return asyncio.run(_async_task())
    except Exception as e:
        console.print(f"Error dequeueing and updating worker {worker_id} for task {task_id}: {e}")
        return False
    finally:
        _close_store_client(store)


def dequeue_and_update_attempts(store_url: str, spans_per_attempt: int = 4) -> BenchmarkSummary:
    """Simulate dequeueing rollouts and updating attempts with spans."""
    start_time = time.time()
    total_workers = 512
    attempts_per_worker = 16
    total_rollouts = total_workers * attempts_per_worker

    asyncio.run(_enqueue_rollouts_for_benchmark(store_url, total_rollouts=total_rollouts, task_prefix="Dequeue"))

    worker_jobs = [
        (f"Worker-{worker_idx}-Attempt-{attempt_idx}", f"Task-{attempt_idx * total_workers + worker_idx}")
        for worker_idx in range(total_workers)
        for attempt_idx in range(attempts_per_worker)
    ]
    with multiprocessing.get_context("fork").Pool(processes=total_workers) as pool:
        successful_tasks = pool.map(
            _dequeue_and_update_attempt_task,
            [(store_url, worker_id, task_id, spans_per_attempt) for worker_id, task_id in worker_jobs],
        )

    end_time = time.time()
    successes = sum(successful_tasks)
    duration = end_time - start_time
    throughput = successes / duration if duration > 0 else 0.0
    console.print(f"Dequeue and update attempt success rate: {successes / len(worker_jobs):.3f}")
    console.print(f"Time taken: {duration:.3f} seconds")
    console.print(f"Throughput: {throughput:.3f} rollouts/second")
    return BenchmarkSummary(
        mode="dequeue-update-attempt", total_tasks=len(worker_jobs), successes=successes, duration=duration
    )


def benchmark_multi_metrics_backend(iterations: int = 10_000_000) -> BenchmarkSummary:
    """Benchmark MultiMetricsBackend fan-out cost."""

    console.print(f"Benchmarking MultiMetricsBackend for {iterations} iterations (2 metric ops per iteration)")

    agl.setup_logging()

    console_backend = ConsoleMetricsBackend(window_seconds=0.5, log_interval_seconds=0.1, group_level=None)
    console_backend_secondary = ConsoleMetricsBackend(
        window_seconds=None, log_interval_seconds=1_000_000.0, group_level=None
    )
    backend = MultiMetricsBackend([console_backend, console_backend_secondary])

    backend.register_counter("benchmark.metrics.counter", label_names=["worker"])
    backend.register_histogram(
        "benchmark.metrics.latency",
        label_names=["worker"],
        buckets=(0.001, 0.005, 0.05, 0.5, 1.0),
    )
    labels = {"worker": "benchmark"}

    async def _exercise_metrics() -> None:
        for i in range(iterations):
            await backend.inc_counter("benchmark.metrics.counter", labels=labels)
            await backend.observe_histogram(
                "benchmark.metrics.latency",
                value=(i % 100) / 100.0,
                labels=labels,
            )

    start_time = time.time()
    asyncio.run(_exercise_metrics())
    duration = time.time() - start_time
    total_ops = iterations * 2
    throughput = total_ops / duration if duration > 0 else 0.0

    console.print(f"Executed {total_ops} metric updates in {duration:.3f}s ({throughput:.1f} ops/s)")
    return BenchmarkSummary(mode="metrics", total_tasks=total_ops, successes=total_ops, duration=duration)


def record_summary(summary: BenchmarkSummary, summary_file: Optional[str]) -> None:
    message = (
        f"[summary] mode={summary.mode} success_rate={summary.success_rate:.3f} "
        f"throughput={summary.throughput:.3f} ops/s duration={summary.duration:.3f}s "
        f"success={summary.successes}/{summary.total_tasks}"
    )
    console.print(message)
    if summary_file:
        path = Path(summary_file)
        path.parent.mkdir(parents=True, exist_ok=True)
        with path.open("a", encoding="utf-8") as fh:
            fh.write(message + "\n")


def main(argv: Optional[Sequence[str]] = None) -> None:
    args = parse_args(argv)
    if args.mode == "worker":
        summary = simulate_many_update_workers(args.store_url)
    elif args.mode == "dequeue-empty":
        summary = simulate_dequeue_empty_and_update_workers(args.store_url)
    elif args.mode == "dequeue-only":
        summary = dequeue_rollouts(args.store_url)
    elif args.mode == "rollout":
        summary = simulate_rollout_with_spans(args.store_url)
    elif args.mode == "dequeue-update-attempt":
        summary = dequeue_and_update_attempts(args.store_url)
    elif args.mode == "metrics":
        summary = benchmark_multi_metrics_backend()
    else:
        raise ValueError(f"Invalid mode: {args.mode}")
    record_summary(summary, args.summary_file)
    if summary.success_rate < 1.0:
        raise ValueError(f"Benchmark failed with success rate {summary.success_rate:.3f}")


if __name__ == "__main__":
    main()


--- tests/benchmark/utils.py ---
# Copyright (c) Microsoft. All rights reserved.

"""Generating random test data for benchmarking."""

import random
import string
from typing import Any, Callable, Dict, Optional, Tuple, Union, cast


def random_string(length: int, *, alphabet: Optional[str] = None) -> str:
    """
    Generate a random string of fixed length.

    Args:
        length: Length of the generated string.
        alphabet: Optional character set to draw from. If None, uses [A-Za-z0-9].
    """
    if length < 0:
        raise ValueError("String length cannot be negative.")

    alphabet = alphabet or (string.ascii_letters + string.digits)
    return "".join(random.choices(alphabet, k=length))


def _resolve_param(value: Union[int, Tuple[int, int]], name: str) -> int:
    """
    Convert parameter into a concrete integer.
    If value is an int, return it.
    If value is a tuple, interpret it as (low, high) and sample uniformly.
    """
    if isinstance(value, int):
        if value < 0:
            raise ValueError(f"{name} cannot be negative.")
        return value

    if (
        isinstance(value, tuple)  # type: ignore
        and len(value) == 2
        and isinstance(value[0], int)  # type: ignore
        and isinstance(value[1], int)  # type: ignore
    ):
        low, high = value
        if low < 0 or high < 0:
            raise ValueError(f"{name} range cannot contain negative values.")
        if low > high:
            raise ValueError(f"{name} tuple must be (low, high) with low <= high.")
        return random.randint(low, high)

    raise TypeError(f"{name} must be an int or a 2-tuple of ints.")


def default_value_factory(length: int) -> str:
    """Default value factory for generating string payloads."""
    return random_string(length)


def random_dict(
    *,
    depth: Union[int, Tuple[int, int]],
    breadth: Union[int, Tuple[int, int]],
    key_length: Union[int, Tuple[int, int]],
    value_length: Union[int, Tuple[int, int]],
    value_factory: Optional[Callable[[int], Any]] = None,
) -> Dict[str, Any]:
    """
    Generate a nested dictionary with configurable depth, breadth, and
    value sizes. Integer or (low, high) tuples are supported for
    all structural parameters.

    Args:
        depth: Number of nested levels or a tuple specifying a range.
        breadth: Number of keys per level or a tuple range.
        key_length: Length of each key or a tuple range.
        value_length: Length of each value or a tuple range.
        value_factory: Function mapping `value_length` → value.

    Returns:
        A nested dictionary of arbitrary size.
    """
    # Default factory
    if value_factory is None:
        value_factory = random_string

    def build(level: int) -> Dict[str, Any]:
        # For each level, breadth/key/value lengths may vary, so draw fresh each time
        current_breadth = _resolve_param(breadth, "breadth")

        if current_breadth < 0:
            raise ValueError("Breadth cannot be negative.")

        target_depth = depth if isinstance(depth, int) else _resolve_param((level, depth[1]), "depth")

        if level == target_depth:
            # leaf nodes
            return {
                random_string(_resolve_param(key_length, "key_length")): value_factory(
                    _resolve_param(value_length, "value_length")
                )
                for _ in range(current_breadth)
            }

        # nested nodes
        return {
            random_string(_resolve_param(key_length, "key_length")): build(level + 1) for _ in range(current_breadth)
        }

    return build(1)


def flatten_dict(d: Dict[str, Any], prefix: str = "") -> Dict[str, Any]:
    """Flatten a nested dictionary into a single level dictionary. Keys are joined by dots."""

    result: Dict[str, Any] = {}
    for key, value in d.items():
        if isinstance(value, dict):
            result.update(flatten_dict(cast(Dict[str, Any], value), f"{prefix}.{key}" if prefix else key))
        else:
            result[f"{prefix}.{key}" if prefix else key] = value
    return result


if __name__ == "__main__":
    # Example usage
    import json

    structured_dict = random_dict(
        depth=(1, 3),
        breadth=(2, 6),
        key_length=(3, 20),
        value_length=(5, 300),
    )

    print(json.dumps(flatten_dict(structured_dict), indent=2))


--- AGENTS.md ---
# Repository Guidelines

## Architecture Overview
Agent Lightning runs through a continuous loop: runners and tracers emit spans, `LightningStore` (`agentlightning/store/`) keeps them synchronized, and algorithms in `agentlightning/algorithm/` consume those traces to improve behavior.

## Project Structure & Module Organization
- `agentlightning/`: adapters, execution stack, training loop, tracer, reward logic, and the `agl` CLI.
- `docs/` & `examples/`: narrative and procedural docs (assets in `docs/assets/`, navigation in `mkdocs.yml`) plus runnable workflows whose READMEs point to their companion how-to guides. `docs/how-to` covers task-focused instructions, while `docs/tutorials` explains concepts and subsystems.
- `dashboard/`, `scripts/`, `tests/`: UI bundles, release/dataset/CI automation, and mirrored coverage of the runtime tree. Record download steps rather than committing binaries.

## Build, Test, and Development Commands
- `uv sync --group dev` — provision tooling once per environment.
- `uv run --no-sync pytest -v` — execute the full suite; add a path or `-k expr` to narrow the run.
- `uv run --no-sync pyright` — enforce static typing parity with CI.
- `uv run --no-sync pre-commit run --all-files --show-diff-on-failure` and `uv run --no-sync mkdocs build --strict` — keep formatting tidy and documentation valid.
Always commit the refreshed `uv.lock` when dependencies shift, and mention optional groups (VERL, APO, GPU) in PR notes.

## Common Issues & Fixes
- When `uv run` errors with `Permission denied` under `~/.cache`, override both cache locations inline: ``UV_CACHE="$(pwd)/.cache_uv" XDG_CACHE_HOME="$(pwd)/.cache_xdg" uv run --no-sync <command>``.

## Coding Style & Naming Conventions
- Target `requires-python >= 3.10`, four-space indentation, 120-character lines (though docstrings may run longer), and formatter-owned diffs (Black + isort, `black` profile). Use `snake_case` for modules, functions, and variables; `PascalCase` for classes and React components; lowercase hyphenation for CLI flags, branch names, and TypeScript filenames.
- Maintain exhaustive type hints (pyright enforces them) and prefer shared dataclasses or Pydantic models from `agentlightning.types`.
- Author Google-style docstrings for new modules or public methods—succinct descriptions, no redundant type info, no redundant `Key features/components` bullet points. Use mkdocs styles: `[][]` syntax for cross-references and single backticks for inline code blocks.
- Writing logs is encouraged, especially for long functions with multiple steps and try-except blocks that catch all exceptions. Use `logging.getLogger(__name__)` to get loggers. Distinguish between DEBUG, INFO, WARNING, and ERROR logs.

## Testing Guidelines
- Mirror runtime directories under `tests/` and match filenames for quick traceability.
- Parametrize pytest cases and apply markers (`openai`, `gpu`, `agentops`, `mongo`, `llmproxy`) so optional suites can be skipped via selectors like `-m "not mongo"` yet still exercised in CI.
- Lean on fixtures, favor real stores/spans/agents over mocks, and drive coverage across the majority of branches.
- If an imported module is missing from the environment, check whether `uv sync` has been run with the right groups. Do not make stubs for external dependencies unless necessary.

## Example Contributions
- Ship each example with a README that includes smoke-test instructions so maintainers can validate quickly. The README must contain an "Included Files" section summarizing every file and its role.
- Keep runnable example modules self-contained with a module-level docstring describing CLI usage. Document important or educational classes/functions with targeted docstrings and inline comments where clarity matters.
- Add a CI workflow per example named `examples-<name>.yml` in `.github/workflows/`. Register it in `badge-<name>.yml`, `badge-examples.yml`, and `badge-latest.yml` when applicable so badges stay accurate.

## Commit & Pull Request Guidelines
- Branch from a fresh `main` using `feature/<slug>`, `fix/<slug>`, `docs/<slug>`, or `chore/<slug>`.
- Write imperative, scoped commits, reference issues with `Fixes #123`, and rerun pre-commit plus the relevant pytest/doc builds before pushing.
- Use PR descriptions to summarize intent, list verification commands, call out dependency or docs-navigation updates, and link new docs/examples via `mkdocs.yml` or `examples/README.md`. Include logs for dashboard changes.


--- CLAUDE.md ---
AGENTS.md

--- RAI_README.md ---
# Responsible AI Transparency Documentation - Agent Lightning

## OVERVIEW

Agent Lightning is a flexible and extensible framework that enables seamless agent optimization for any existing agent framework. Agent optimization includes various data-driven techniques to customize the agent for better performance, including but not limited to model fine-tuning, prompt tuning, and model selection. And the agent frameworks refer to popular and easy-to-use agent developing frameworks such as OpenAI Agents SDK, Microsoft AutoGen, and LangChain.

### WHAT CAN AGENT LIGHTNING DO
Agent lightning was developed to bridge the gap between agent workflow development and agent optimization, empowering developers to go beyond static, pre-trained models and unlock the full potential of adaptive, learning-based agents. Agent Lightning is a training framework which can be used for any LLMs.

### INTENDED USES
Agent Lightning is best suited for agent researchers and developers. They can easily fine-tune models in existing agent frameworks with Agent Lightning. This can improve model performance on the targeted scenarios.

### OUT-OF-SCOPE USES
Agent Lightning is not well-suited for users who are not familiar with agent development and machine learning concepts.

We do not recommend using Agent Lightning in commercial or real-world applications without further testing and development. It is being released for research purposes.

Agent Lightning was not designed or evaluated for all possible downstream purposes. Developers should consider its inherent limitations as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness concerns specific to each intended downstream use.

Agent Lightning should not be used in highly regulated domains where inaccurate outputs could suggest actions that lead to injury or negatively impact an individual's legal, financial, or life opportunities.

We do not recommend using Agent Lightning in the context of high-risk decision making (e.g. in law enforcement, legal, finance, or healthcare).

## HOW TO GET STARTED
To begin using Agent Lightning, here are some instructions.
1.	Install dependencies, including Python, uv, PyTorch, FlashAttention, vLLM, verl.
2.	Clone and install Agent Lightning.
3.	Convert the dataset (provided by the user) into parquet file, which contains multiple columns. Each column contains a data id, an input and an expected output.
4.	Run agent, which is developed by the user.
5.	Run the training process via “bash train.sh”

## EVALUATION
Agent Lightning was evaluated on its ability to correctly complete 3 example tasks: (1) Math. The model needs to answer some math questions, and when answering one question, the model can use the calculator as its tool to help answer. (2) Text2SQL. The model is given a question related to the database, and it is required to generate a SQL which can query the database, find the information to answer the question. (3) Retrieval-Augmented Generation (RAG). The model is given a question which needs some information from Wikipedia to answer. The model is required to generate some queries to find the related information in Wikipedia, and answer the question according to retrieved documents.

### EVALUATION METHODS AND RESULTS
For detailed evaluation methods and results, please refer to the latest version of our  [technical report](https://arxiv.org/abs/2508.03680).


## LIMITATIONS
Agent Lightning was developed for research and experimental purposes. Further testing and validation are needed before considering its application in commercial or real-world scenarios.

Agent Lightning was designed and tested using the English language. Performance in other languages may vary and should be assessed by someone who is both an expert in the expected outputs and a native speaker of that language.

Outputs generated by AI may include factual errors, fabrication, or speculation. Users are responsible for assessing the accuracy of generated content. All decisions leveraging outputs of the system should be made with human oversight and not be based solely on system outputs.
Agent Lightning inherits any biases, errors, or omissions produced by its base model. Developers are advised to choose an appropriate base LLM/MLLM carefully, depending on the intended use case.
We use some demo cases to show the effectiveness of our training framework. See their links to understand the capabilities and limitations of this model.

## BEST PRACTICES
Better performance can be achieved by following the instructions in how to get started section.

We strongly encourage users to use LLMs/MLLMs that support robust Responsible AI mitigations, such as Azure Open AI (AOAI) services. Such services continually update their safety and RAI mitigations with the latest industry standards for responsible use. For more on AOAI’s best practices when employing foundations models for scripts and applications:
- [Blog post on responsible AI features in AOAI that were presented at Ignite 2023](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/announcing-new-ai-safety-amp-responsible-ai-features-in-azure/ba-p/3983686)
- [Overview of Responsible AI practices for Azure OpenAI models](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/overview)
- [Azure OpenAI Transparency Note](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/transparency-note)
- [OpenAI’s Usage policies](https://openai.com/policies/usage-policies)
- [Azure OpenAI’s Code of Conduct](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/code-of-conduct)

Users are responsible for sourcing their datasets legally and ethically. This could include securing appropriate rights, ensuring consent for use of audio/images, and/or the anonymization of data prior to use in research.

Users are reminded to be mindful of data privacy concerns and are encouraged to review the privacy policies associated with any models and data storage solutions interfacing with Agent Lightning.

It is the user’s responsibility to ensure that the use of Agent Lightning complies with relevant data protection regulations and organizational guidelines.

## LICENSE
We use the MIT license.

## CONTACT
We welcome feedback and collaboration from our audience. If you have suggestions, questions, or observe unexpected/offensive behavior in our technology, please contact us at agent-lightning@microsoft.com.

If the team receives reports of undesired behavior or identifies issues independently, we will update this repository with appropriate mitigations.



---

*Last updated: September 6, 2025*
*Document version: 1.0*


## Links discovered
- [technical report](https://arxiv.org/abs/2508.03680)
- [Blog post on responsible AI features in AOAI that were presented at Ignite 2023](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/announcing-new-ai-safety-amp-responsible-ai-features-in-azure/ba-p/3983686)
- [Overview of Responsible AI practices for Azure OpenAI models](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/overview)
- [Azure OpenAI Transparency Note](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/transparency-note)
- [OpenAI’s Usage policies](https://openai.com/policies/usage-policies)
- [Azure OpenAI’s Code of Conduct](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/code-of-conduct)

--- README.md ---
<p align="center">
  <img src="docs/assets/readme-banner.svg" alt="Agent-lightning-banner" style="width:600px"/>
</p>

# Agent Lightning⚡

[![Unit Tests](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml)
[![Documentation](https://img.shields.io/badge/GitHub%20Pages-Documentation-blue)](https://microsoft.github.io/agent-lightning/)
[![PyPI version](https://badge.fury.io/py/agentlightning.svg)](https://badge.fury.io/py/agentlightning)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/microsoft/agent-lightning)
[![Discord](https://img.shields.io/badge/Discord-Join-5865F2?logo=discord&logoColor=white)](https://discord.gg/RYk7CdvDR7)

**The absolute trainer to light up AI agents.**

Join our [Discord community](https://discord.gg/RYk7CdvDR7) to connect with other users and contributors.

## ⚡ Core Features

- Turn your agent into an optimizable beast with **ZERO CODE CHANGE** (almost)! 💤
- Build with **ANY** agent framework (LangChain, OpenAI Agent SDK, AutoGen, CrewAI, Microsoft Agent Framework...); or even WITHOUT agent framework (Python OpenAI). You name it! 🤖
- **Selectively** optimize one or more agents in a multi-agent system. 🎯
- Embraces **Algorithms** like Reinforcement Learning, Automatic Prompt Optimization, Supervised Fine-tuning and more. 🤗

Read more on our [documentation website](https://microsoft.github.io/agent-lightning/).

<p align="center">
  <img src="docs/assets/readme-diff.svg" alt="Agent-Lightning Core Quickstart" style="width:100%"/>
</p>

## ⚡ Installation

```bash
pip install agentlightning
```

For the latest nightly build (cutting-edge features), you can install from Test PyPI:

```bash
pip install --upgrade --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ --pre agentlightning
```

Please refer to our [installation guide](https://microsoft.github.io/agent-lightning/stable/tutorials/installation/) for more details.

To start using Agent-lightning, check out our [documentation](https://microsoft.github.io/agent-lightning/) and [examples](./examples).

## ⚡ Articles

- 12/17/2025 [Adopting the Trajectory Level Aggregation for Faster Training](https://agent-lightning.github.io/posts/trajectory_level_aggregation/) Agent-lightning blog.
- 11/4/2025 [Tuning ANY AI agent with Tinker ✕ Agent-lightning](https://medium.com/@yugez/tuning-any-ai-agent-with-tinker-agent-lightning-part-1-1d8c9a397f0e) Medium. See also [Part 2](https://medium.com/@yugez/tuning-any-ai-agent-with-tinker-agent-lightning-part-2-332c5437f0dc).
- 10/22/2025 [No More Retokenization Drift: Returning Token IDs via the OpenAI Compatible API Matters in Agent RL](https://blog.vllm.ai/2025/10/22/agent-lightning.html) vLLM blog. See also [Zhihu writeup](https://zhuanlan.zhihu.com/p/1965067274642785725).
- 8/11/2025 [Training AI Agents to Write and Self-correct SQL with Reinforcement Learning](https://medium.com/@yugez/training-ai-agents-to-write-and-self-correct-sql-with-reinforcement-learning-571ed31281ad) Medium.
- 8/5/2025 [Agent Lightning: Train ANY AI Agents with Reinforcement Learning](https://arxiv.org/abs/2508.03680) arXiv paper.
- 7/26/2025 [We discovered an approach to train any AI agent with RL, with (almost) zero code changes.](https://www.reddit.com/r/LocalLLaMA/comments/1m9m670/we_discovered_an_approach_to_train_any_ai_agent/) Reddit.
- 6/6/2025 [Agent Lightning - Microsoft Research](https://www.microsoft.com/en-us/research/project/agent-lightning/) Project page.

## ⚡ Community Projects

- [DeepWerewolf](https://github.com/af-74413592/DeepWerewolf) — A case study of agent RL training for the Chinese Werewolf game built with AgentScope and Agent Lightning.
- [AgentFlow](https://agentflow.stanford.edu/) — A modular multi-agent framework that combines planner, executor, verifier, and generator agents with the Flow-GRPO algorithm to tackle long-horizon, sparse-reward tasks.
- [Youtu-Agent](https://github.com/TencentCloudADP/Youtu-agent) — Youtu-Agent lets you build and train your agent with ease. Built with [a modified branch](https://github.com/microsoft/agent-lightning/tree/contrib/youtu-agent-lightning) of Agent Lightning, Youtu-Agent has verified up to 128 GPUs RL training on maths/code and search capabilities with steady convergence. Also check [the recipe](https://github.com/TencentCloudADP/youtu-agent/tree/rl/agl) and their blog [*Stop Wrestling with Your Agent RL: How Youtu-Agent Achieved Stable, 128-GPU Scaling Without Breaking a Sweat*](https://spotted-coconut-df8.notion.site/Stop-Wrestling-with-Your-Agent-RL-How-Youtu-Agent-Achieved-Stable-128-GPU-Scaling-Without-Breaking-2ca5e8f089ba80539a98c582b65e0233).

## ⚡ Architecture

Agent Lightning keeps the moving parts to a minimum so you can focus on your idea, not the plumbing. Your agent continues to run as usual; you can still use any agent framework you like; you drop in the lightweight `agl.emit_xxx()` helper, or let the tracer collect every prompt, tool call, and reward. Those events become structured spans that flow into the LightningStore, a central hub that keeps tasks, resources, and traces in sync.

On the other side of the store sits the algorithm you choose, or write yourself. The algorithm reads spans, learns from them, and posts updated resources such as refined prompt templates or new policy weights. The Trainer ties it all together: it streams datasets to runners, ferries resources between the store and the algorithm, and updates the inference engine when improvements land. You can either stop there, or simply let the same loop keep turning.

No rewrites, no lock-in, just a clear path from first rollout to steady improvement.

<p align="center">
  <img src="docs/assets/readme-architecture.svg" alt="Agent-lightning Architecture" style="width:100%"/>
</p>

## ⚡ CI Status

| Workflow | Status |
|----------|--------|
| CPU Tests | [![tests workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml) |
| Full Tests | [![tests summary workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml) |
| UI Tests | [![UI Tests](https://github.com/microsoft/agent-lightning/actions/workflows/dashboard.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/dashboard.yml) |
| Examples Integration | [![examples summary workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml) |
| Latest Dependency Compatibility | [![latest summary workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml) |
| Legacy Examples Compatibility | [![compat summary workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-compat.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/badge-compat.yml) |

## ⚡ Citation

If you find Agent Lightning useful in your research or projects, please cite our paper:

```bibtex
@misc{luo2025agentlightningtrainai,
      title={Agent Lightning: Train ANY AI Agents with Reinforcement Learning},
      author={Xufang Luo and Yuge Zhang and Zhiyuan He and Zilong Wang and Siyun Zhao and Dongsheng Li and Luna K. Qiu and Yuqing Yang},
      year={2025},
      eprint={2508.03680},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2508.03680},
}
```

## ⚡ Contributing

This project welcomes contributions and suggestions. Start by reading the [Contributing Guide](docs/community/contributing.md) for recommended contribution points, environment setup, branching conventions, and pull request expectations. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## ⚡ Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.

## ⚡ Responsible AI

This project has been evaluated and certified to comply with the Microsoft Responsible AI Standard. The team will continue to monitor and maintain the repository, addressing any severe issues, including potential harms, if they arise.

## ⚡ License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.


## Links discovered
- [![Unit Tests](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml/badge.svg)
- [![Documentation](https://img.shields.io/badge/GitHub%20Pages-Documentation-blue)
- [![PyPI version](https://badge.fury.io/py/agentlightning.svg)
- [![License](https://img.shields.io/badge/license-MIT-blue.svg)
- [![Ask DeepWiki](https://deepwiki.com/badge.svg)
- [![Discord](https://img.shields.io/badge/Discord-Join-5865F2?logo=discord&logoColor=white)
- [Discord community](https://discord.gg/RYk7CdvDR7)
- [documentation website](https://microsoft.github.io/agent-lightning/)
- [installation guide](https://microsoft.github.io/agent-lightning/stable/tutorials/installation/)
- [documentation](https://microsoft.github.io/agent-lightning/)
- [examples](https://github.com/microsoft/agent-lightning/blob/main/examples.md)
- [Adopting the Trajectory Level Aggregation for Faster Training](https://agent-lightning.github.io/posts/trajectory_level_aggregation/)
- [Tuning ANY AI agent with Tinker ✕ Agent-lightning](https://medium.com/@yugez/tuning-any-ai-agent-with-tinker-agent-lightning-part-1-1d8c9a397f0e)
- [Part 2](https://medium.com/@yugez/tuning-any-ai-agent-with-tinker-agent-lightning-part-2-332c5437f0dc)
- [No More Retokenization Drift: Returning Token IDs via the OpenAI Compatible API Matters in Agent RL](https://blog.vllm.ai/2025/10/22/agent-lightning.html)
- [Zhihu writeup](https://zhuanlan.zhihu.com/p/1965067274642785725)
- [Training AI Agents to Write and Self-correct SQL with Reinforcement Learning](https://medium.com/@yugez/training-ai-agents-to-write-and-self-correct-sql-with-reinforcement-learning-571ed31281ad)
- [Agent Lightning: Train ANY AI Agents with Reinforcement Learning](https://arxiv.org/abs/2508.03680)
- [We discovered an approach to train any AI agent with RL, with (almost) zero code changes.](https://www.reddit.com/r/LocalLLaMA/comments/1m9m670/we_discovered_an_approach_to_train_any_ai_agent/)
- [Agent Lightning - Microsoft Research](https://www.microsoft.com/en-us/research/project/agent-lightning/)
- [DeepWerewolf](https://github.com/af-74413592/DeepWerewolf)
- [AgentFlow](https://agentflow.stanford.edu/)
- [Youtu-Agent](https://github.com/TencentCloudADP/Youtu-agent)
- [a modified branch](https://github.com/microsoft/agent-lightning/tree/contrib/youtu-agent-lightning)
- [the recipe](https://github.com/TencentCloudADP/youtu-agent/tree/rl/agl)
- [*Stop Wrestling with Your Agent RL: How Youtu-Agent Achieved Stable, 128-GPU Scaling Without Breaking a Sweat*](https://spotted-coconut-df8.notion.site/Stop-Wrestling-with-Your-Agent-RL-How-Youtu-Agent-Achieved-Stable-128-GPU-Scaling-Without-Breaking-2ca5e8f089ba80539a98c582b65e0233)
- [![tests workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml/badge.svg)
- [![tests summary workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml/badge.svg)
- [![UI Tests](https://github.com/microsoft/agent-lightning/actions/workflows/dashboard.yml/badge.svg)
- [![examples summary workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml/badge.svg)
- [![latest summary workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml/badge.svg)
- [![compat summary workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-compat.yml/badge.svg)
- [Contributing Guide](https://github.com/microsoft/agent-lightning/blob/main/docs/community/contributing.md)
- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)
- [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)
- [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general)
- [LICENSE](https://github.com/microsoft/agent-lightning/blob/main/LICENSE.md)

--- agentlightning/client.py ---
# Copyright (c) Microsoft. All rights reserved.

"""Utilities for interacting with legacy Agent Lightning servers.

This module contains compatibility shims that speak the deprecated HTTP
interface used by older Agent Lightning deployments. Modern code should prefer
the store-based APIs exposed by `agentlightning.store`, but keeping these
clients available makes it easier to migrate existing workflows incrementally.
"""

import asyncio
import logging
import time
import urllib.parse
import warnings
from typing import Any, Dict, List, Optional, Union

import aiohttp
import requests

from .types import NamedResources, ResourcesUpdate, RolloutLegacy, Task, TaskIfAny, TaskInput

logger = logging.getLogger(__name__)


class AgentLightningClient:
    """Client wrapper for the legacy version-aware Agent Lightning server.

    The client exposes synchronous and asynchronous helpers for polling tasks,
    retrieving resource bundles, and submitting rollouts. It also maintains a
    simple in-memory cache keyed by the server-provided resource identifier to
    avoid redundant network requests.

    !!! warning "Deprecated"
        [`AgentLightningClient`][agentlightning.client.AgentLightningClient] is part of
        the legacy client/server stack. New code should rely on the store-based APIs
        implemented in `agentlightning.store`.

    Attributes:
        endpoint: Base URL of the Agent Lightning server.
        poll_interval: Delay in seconds between polling attempts when no task is
            available.
        timeout: Timeout in seconds applied to HTTP requests.
        task_count: Number of tasks claimed during the lifetime of this client.
    """

    _next_task_uri = "/task"
    _resources_uri = "/resources"
    _latest_resources_uri = "/resources/latest"
    _report_rollout_uri = "/rollout"

    def __init__(self, endpoint: str, poll_interval: float = 5.0, timeout: float = 10.0):
        """Initialize the client.

        Args:
            endpoint: Root URL of the Agent Lightning server.
            poll_interval: Seconds to wait between polling attempts.
            timeout: Seconds before a request to the server is considered timed out.
        """
        warnings.warn(
            "AgentLightningClient is deprecated. Please use LightningStoreClient instead.", DeprecationWarning
        )
        self.endpoint = endpoint
        self.task_count = 0
        self.poll_interval = poll_interval
        self.timeout = timeout
        self._resource_cache: Dict[str, ResourcesUpdate] = {}  # TODO: mechanism to evict cache
        self._default_headers = {"X-AgentLightning-Client": "true"}

    async def _request_json_async(self, url: str) -> Optional[Dict[str, Any]]:
        """Perform an asynchronous ``GET`` request and parse the JSON payload.

        Args:
            url: Fully qualified URL to query.

        Returns:
            Parsed JSON body as a dictionary if the request succeeds; otherwise ``None``.
        """
        timeout = aiohttp.ClientTimeout(total=self.timeout)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            try:
                async with session.get(url, headers=self._default_headers) as resp:
                    resp.raise_for_status()
                    return await resp.json()
            except Exception as e:
                logger.debug(f"Async GET request failed for {url}: {e}")
                return None

    async def _post_json_async(self, url: str, payload: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Perform an asynchronous ``POST`` request with a JSON body.

        Args:
            url: Fully qualified URL that accepts the payload.
            payload: Dictionary that will be serialized and sent as JSON.

        Returns:
            Parsed JSON body as a dictionary if the request succeeds; otherwise ``None``.
        """
        timeout = aiohttp.ClientTimeout(total=self.timeout)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            try:
                async with session.post(url, json=payload, headers=self._default_headers) as resp:
                    resp.raise_for_status()
                    return await resp.json()
            except Exception as e:
                logger.debug(f"Async POST request failed for {url}: {e}")
                return None

    async def poll_next_task_async(self) -> Optional[Task]:
        """Poll the server asynchronously until a task becomes available.

        Returns:
            The next [`Task`][agentlightning.Task] exposed by the server,
            or ``None`` if polling fails.
        """
        url = urllib.parse.urljoin(self.endpoint, self._next_task_uri)
        while True:
            response = await self._request_json_async(url)
            if response:
                task_if_any = TaskIfAny.model_validate(response)
                if task_if_any.is_available and task_if_any.task:
                    self.task_count += 1
                    logger.info(f"[Task {self.task_count} Received] ID: {task_if_any.task.rollout_id}")
                    return task_if_any.task
            logger.debug(f"No task available yet. Retrying in {self.poll_interval} seconds...")
            await asyncio.sleep(self.poll_interval)

    async def get_resources_by_id_async(self, resource_id: str) -> Optional[ResourcesUpdate]:
        """Fetch a specific resource bundle by identifier.

        Args:
            resource_id: Identifier sourced from the task metadata.

        Returns:
            Cached or freshly downloaded
            [`ResourcesUpdate`][agentlightning.ResourcesUpdate], or
            ``None`` when the server returns an error.
        """
        if resource_id in self._resource_cache:
            logger.debug(f"Found resources '{resource_id}' in cache.")
            return self._resource_cache[resource_id]

        url = urllib.parse.urljoin(self.endpoint, f"{self._resources_uri}/{resource_id}")
        response = await self._request_json_async(url)
        if response:
            resources_update = ResourcesUpdate.model_validate(response)
            self._resource_cache[resource_id] = resources_update
            logger.info(f"Fetched and cached resources for ID: {resource_id}")
            return resources_update
        return None

    async def get_latest_resources_async(self) -> Optional[ResourcesUpdate]:
        """Fetch the most recent resource bundle advertised by the server.

        Returns:
            [`ResourcesUpdate`][agentlightning.ResourcesUpdate] for the
            newest version, or ``None`` when unavailable.
        """
        url = urllib.parse.urljoin(self.endpoint, self._latest_resources_uri)
        response = await self._request_json_async(url)
        if response:
            resources_update = ResourcesUpdate.model_validate(response)
            # Cache this result as well
            self._resource_cache[resources_update.resources_id] = resources_update
            return resources_update
        return None

    async def post_rollout_async(self, rollout: RolloutLegacy) -> Optional[Dict[str, Any]]:
        """Submit a completed rollout back to the server.

        Args:
            rollout: Legacy rollout payload produced by the executor.

        Returns:
            Parsed JSON response returned by the server, or ``None`` when the request fails.
        """
        url = urllib.parse.urljoin(self.endpoint, self._report_rollout_uri)
        payload = rollout.model_dump(mode="json")
        return await self._post_json_async(url, payload)

    def _request_json(self, url: str) -> Optional[Dict[str, Any]]:
        """Perform a blocking ``GET`` request and parse the JSON payload.

        Args:
            url: Fully qualified URL to query.

        Returns:
            Parsed JSON body as a dictionary if the request succeeds; otherwise ``None``.
        """
        try:
            response = requests.get(url, timeout=self.timeout, headers=self._default_headers)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            logger.debug(f"Sync GET request failed for {url}: {e}")
            return None

    def _post_json(self, url: str, payload: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Perform a blocking ``POST`` request with a JSON payload.

        Args:
            url: Fully qualified URL that accepts the payload.
            payload: Dictionary that will be serialized and sent as JSON.

        Returns:
            Parsed JSON body as a dictionary if the request succeeds; otherwise ``None``.
        """
        try:
            response = requests.post(url, json=payload, timeout=self.timeout, headers=self._default_headers)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            logger.debug(f"Sync POST request failed for {url}: {e}")
            return None

    def poll_next_task(self) -> Optional[Task]:
        """Poll the server synchronously until a task becomes available.

        Returns:
            The next [`Task`][agentlightning.Task] available for execution, or
            ``None`` if polling fails.
        """
        url = urllib.parse.urljoin(self.endpoint, self._next_task_uri)
        while True:
            response = self._request_json(url)
            if response:
                task_if_any = TaskIfAny.model_validate(response)
                if task_if_any.is_available and task_if_any.task:
                    self.task_count += 1
                    logger.info(f"[Task {self.task_count} Received] ID: {task_if_any.task.rollout_id}")
                    return task_if_any.task
            logger.debug(f"No task available yet. Retrying in {self.poll_interval} seconds...")
            time.sleep(self.poll_interval)

    def get_resources_by_id(self, resource_id: str) -> Optional[ResourcesUpdate]:
        """Fetch a specific resource bundle by identifier.

        Args:
            resource_id: Identifier sourced from the task metadata.

        Returns:
            Cached or freshly downloaded
            [`ResourcesUpdate`][agentlightning.ResourcesUpdate], or
            ``None`` when the server returns an error.
        """
        if resource_id in self._resource_cache:
            logger.debug(f"Found resources '{resource_id}' in cache.")
            return self._resource_cache[resource_id]

        url = urllib.parse.urljoin(self.endpoint, f"{self._resources_uri}/{resource_id}")
        response = self._request_json(url)
        if response:
            resources_update = ResourcesUpdate.model_validate(response)
            self._resource_cache[resource_id] = resources_update
            logger.info(f"Fetched and cached resources for ID: {resource_id}")
            return resources_update
        return None

    def get_latest_resources(self) -> Optional[ResourcesUpdate]:
        """Fetch the most recent resource bundle advertised by the server.

        Returns:
            [`ResourcesUpdate`][agentlightning.ResourcesUpdate] for the
            newest version, or ``None`` when unavailable.
        """
        url = urllib.parse.urljoin(self.endpoint, self._latest_resources_uri)
        response = self._request_json(url)
        if response:
            resources_update = ResourcesUpdate.model_validate(response)
            self._resource_cache[resources_update.resources_id] = resources_update
            return resources_update
        return None

    def post_rollout(self, rollout: RolloutLegacy) -> Optional[Dict[str, Any]]:
        """Submit a completed rollout back to the server.

        Args:
            rollout: Legacy rollout payload produced by the executor.

        Returns:
            Parsed JSON response returned by the server, or ``None`` when the request fails.
        """
        url = urllib.parse.urljoin(self.endpoint, self._report_rollout_uri)
        payload = rollout.model_dump(mode="json")
        return self._post_json(url, payload)


class DevTaskLoader(AgentLightningClient):
    """In-memory task loader used for development and integration tests.

    The loader mimics the behavior of the legacy HTTP server by storing tasks and
    resources locally. Polling methods simply iterate over the provided collection,
    allowing rapid iteration without provisioning any external infrastructure.

    !!! warning "Deprecated"

        [`DevTaskLoader`][agentlightning.client.DevTaskLoader] is a compatibility shim.
        Prefer [`Trainer.dev`][agentlightning.Trainer.dev] for new code.
    """

    def __init__(
        self,
        tasks: Union[List[TaskInput], List[Task]],
        resources: Union[NamedResources, ResourcesUpdate],
        **kwargs: Any,
    ):
        """Initialize the loader with predefined tasks and resources.

        Args:
            tasks: Sequence of task inputs or preconstructed tasks that will be served in
                order.
            resources: Static resources returned for any `resources_id` query.
            **kwargs: Additional keyword arguments forwarded to the parent client.

        Raises:
            ValueError: If no tasks are provided or both [`Task`][agentlightning.Task]
                and [`TaskInput`][agentlightning.TaskInput] instances are mixed.
        """
        warnings.warn("DevTaskLoader is deprecated. Please use Trainer.dev instead.", DeprecationWarning)
        super().__init__(endpoint="local://", **kwargs)
        self._tasks = tasks.copy()
        if len(self._tasks) == 0:
            raise ValueError("DevTaskLoader requires at least one task to be provided.")

        # Check if tasks are mixture of TaskInput and Task
        if any(isinstance(task, Task) for task in self._tasks):
            if not all(isinstance(task, Task) for task in self._tasks):
                raise ValueError("All tasks must be either Task or TaskInput objects.")

        self._task_index = 0

        if isinstance(resources, ResourcesUpdate):
            self._resources_update = resources
        else:
            self._resources_update = ResourcesUpdate(
                resources_id="local", resources=resources, create_time=time.time(), update_time=time.time(), version=1
            )

        # Store rollouts posted back to the loader for easy debugging of local runs
        self._rollouts: List[RolloutLegacy] = []

    @property
    def rollouts(self) -> List[RolloutLegacy]:
        """Return the rollouts posted back to the loader during development runs."""
        return self._rollouts

    def poll_next_task(self) -> Optional[Task]:
        """Return the next task from the local queue.

        If [`TaskInput`][agentlightning.TaskInput] instances were provided,
        they are converted into [`Task`][agentlightning.Task] objects on the
        fly. Otherwise, the preconstructed tasks are returned in sequence.

        Returns:
            Next task to execute.
        """
        if self._task_index >= len(self._tasks):
            self._task_index = 0

        task_or_input = self._tasks[self._task_index]

        if isinstance(task_or_input, Task):
            task = task_or_input
        else:
            rollout_id = f"local_task_{self._task_index + 1:03d}"
            task = Task(
                rollout_id=rollout_id,
                input=task_or_input,
                resources_id=self._resources_update.resources_id,
                create_time=time.time(),
            )

        self._task_index += 1
        self.task_count += 1
        logger.info(f"[Task {self.task_count} Received] Task ID: {task.rollout_id}")
        return task

    def get_resources_by_id(self, resource_id: str) -> Optional[ResourcesUpdate]:
        logger.debug(f"DevTaskLoader checking resources for ID: {resource_id}")
        if resource_id != self._resources_update.resources_id:
            raise ValueError(
                f"Resource ID '{resource_id}' not found. Only '{self._resources_update.resources_id}' is available."
            )
        return self._resources_update

    def get_latest_resources(self) -> Optional[ResourcesUpdate]:
        logger.debug("DevTaskLoader returning latest resources.")
        return self._resources_update

    def post_rollout(self, rollout: RolloutLegacy) -> Optional[Dict[str, Any]]:
        logger.debug(f"DevTaskLoader received rollout for task: {rollout.rollout_id}")
        self._rollouts.append(rollout)
        return {"status": "received", "rollout_id": rollout.rollout_id}

    async def poll_next_task_async(self) -> Optional[Task]:
        return self.poll_next_task()

    async def get_resources_by_id_async(self, resource_id: str) -> Optional[ResourcesUpdate]:
        return self.get_resources_by_id(resource_id)

    async def get_latest_resources_async(self) -> Optional[ResourcesUpdate]:
        return self.get_latest_resources()

    async def post_rollout_async(self, rollout: RolloutLegacy) -> Optional[Dict[str, Any]]:
        return self.post_rollout(rollout)

    def __repr__(self):
        return f"DevTaskLoader(num_tasks={len(self._tasks)}, resources={self._resources_update.resources})"


--- agentlightning/config.py ---
# Copyright (c) Microsoft. All rights reserved.

"""
This file is not carefully reviewed.
It might contain unintentional bugs and issues.
Please always review the parsed construction arguments before using them.
"""

from __future__ import annotations

import argparse
import inspect
import logging
from typing import _GenericAlias  # type: ignore
from typing import (
    Any,
    Callable,
    Dict,
    List,
    Tuple,
    Type,
    TypeVar,
    Union,
    get_args,
    get_origin,
    get_type_hints,
    overload,
)

CliConfigurable = Any

logger = logging.getLogger(__name__)

__all__ = ["lightning_cli"]

# TypeVars for precise return type hinting with overloads
_C = TypeVar("_C", bound=CliConfigurable)
_C1 = TypeVar("_C1", bound=CliConfigurable)
_C2 = TypeVar("_C2", bound=CliConfigurable)
_C3 = TypeVar("_C3", bound=CliConfigurable)
_C4 = TypeVar("_C4", bound=CliConfigurable)


# Custom type for CLI arguments that can be string or None
def nullable_str(value: str) -> str | None:
    """Converts specific string values (case-insensitive) to None, otherwise returns the string."""
    if value.lower() in ["none", "null", "~", "nil"]:  # Define keywords for None
        return None
    return value


def nullable_int(value: str) -> int | None:
    """Converts specific string values (case-insensitive) to None, otherwise returns the integer."""
    if value.lower() in ["none", "null", "~", "nil"]:  # Define keywords for None
        return None
    try:
        return int(value)
    except ValueError:
        raise argparse.ArgumentTypeError(f"Invalid integer value: '{value}'")


def nullable_float(value: str) -> float | None:
    """Converts specific string values (case-insensitive) to None, otherwise returns the float."""
    if value.lower() in ["none", "null", "~", "nil"]:  # Define keywords for None
        return None
    try:
        return float(value)
    except ValueError:
        raise argparse.ArgumentTypeError(f"Invalid float value: '{value}'")


def _str_to_bool(v: str) -> bool:
    """Converts common string representations of bool to Python bool (case-insensitive)."""
    if isinstance(v, bool):  # type: ignore
        return v  # Allow passing bools directly if used programmatically
    lowered_v = v.lower()
    if lowered_v in ("yes", "true", "t", "y", "1"):
        return True
    elif lowered_v in ("no", "false", "f", "n", "0"):
        return False
    else:
        raise argparse.ArgumentTypeError(f"Boolean value expected (e.g., 'true', 'false', 'yes', 'no'), got '{v}'")


def _get_param_type_details(param_annotation: Any) -> Tuple[Any, bool, bool]:
    """Normalize an annotation into its core type, optionality, and list status.

    Args:
        param_annotation: The annotation to inspect.

    Returns:
        A tuple ``(core_type, is_optional, is_list)`` describing the normalized type.

        - For ``Optional[T]`` → ``(T, True, is_list_status_of_T)``
        - For ``List[T]`` → ``(List[T], is_optional_status_of_List, True)``
        - For ``Optional[List[T]]`` → ``(List[T], True, True)``
    """
    is_optional = False
    is_list = False
    current_type = param_annotation

    # Check for outer Optional
    origin = get_origin(current_type)
    if origin is Union:
        union_args = get_args(current_type)
        if len(union_args) == 2 and type(None) in union_args:
            is_optional = True
            current_type = next(arg for arg in union_args if arg is not type(None))  # Unwrap Optional

    # Check if the (potentially unwrapped) type is a List
    origin = get_origin(current_type)  # Re-check origin after potential unwrap
    if origin is list or (isinstance(current_type, _GenericAlias) and current_type.__origin__ is list):
        is_list = True

    return current_type, is_optional, is_list


def _determine_argparse_type(param_type: Any) -> Callable[[str], Any]:
    """Determines the type for argparse based on parameter type details."""
    core_type, is_optional, _ = _get_param_type_details(param_type)
    if core_type is str and is_optional:
        return nullable_str  # Special handling for Optional[str]
    elif core_type is int and is_optional:
        return nullable_int
    elif core_type is float and is_optional:
        return nullable_float
    elif core_type is bool:
        return _str_to_bool  # Special handling for bool
    elif core_type in (int, float, str):
        return core_type
    return str  # Default to str if no specific type is provided (including empty)


def _determine_argparse_type_and_nargs(
    core_param_type: Any, is_param_list: bool  # The type after unwrapping an outer Optional
) -> Dict[str, Any]:
    """Determines the 'type' and 'nargs' for argparse based on parameter type details."""
    kwargs: Dict[str, Any] = {}

    if is_param_list:
        kwargs["nargs"] = "*"  # Allows zero or more arguments for lists
        list_item_annotations = get_args(core_param_type)  # For List[T], core_param_type is List[T]

        if list_item_annotations and list_item_annotations[0] is not Any:
            item_ann = list_item_annotations[0]
            # Check if the list item itself is, e.g., Optional[str] or bool
            kwargs["type"] = _determine_argparse_type(item_ann)
        else:
            kwargs["type"] = str
    else:  # Not a list
        kwargs["type"] = _determine_argparse_type(core_param_type)
    return kwargs


def _build_help_string(cls_name: str, param_name: str, core_type: Any, is_optional: bool, is_list: bool) -> str:
    """Constructs a descriptive help string for a CLI argument."""
    type_display_name = "Any"
    if core_type is not inspect.Parameter.empty:
        type_display_name = getattr(core_type, "__name__", str(core_type))

    if is_list:
        list_item_args = get_args(core_type)  # core_type is List[T] here
        item_name = "Any"
        if list_item_args and list_item_args[0] is not Any:
            inner_item_core_type, inner_item_optional, _ = _get_param_type_details(list_item_args[0])
            item_name = getattr(inner_item_core_type, "__name__", str(inner_item_core_type))
            if inner_item_optional:  # e.g. List[Optional[str]]
                item_name = f"Optional[{item_name}]"
        type_display_name = f"List[{item_name}]"

    full_type_display = f"Optional[{type_display_name}]" if is_optional and not is_list else type_display_name
    if is_optional and is_list:  # e.g. Optional[List[str]]
        full_type_display = f"Optional[{type_display_name}]"

    help_str = f"For {cls_name}: '{param_name}'. Inferred type: {full_type_display}."
    return help_str


def _add_argument_for_parameter(
    parser: argparse.ArgumentParser,
    cls: Type[CliConfigurable],
    param_name: str,
    param_obj: inspect.Parameter,
    dest_name: str,
    resolved_param_annotation: Any = None,
) -> None:
    """Configures and adds a single CLI argument for an __init__ parameter."""
    if resolved_param_annotation is None:
        param_type_annotation = param_obj.annotation
    else:
        param_type_annotation = resolved_param_annotation

    # core_type is the main type (e.g., int, str, List[str]), after unwrapping the outermost Optional.
    # is_overall_optional indicates if the parameter itself can be None (e.g. param: Optional[T] = None)
    # is_list indicates if core_type is a List.
    core_type, is_overall_optional, is_list = _get_param_type_details(param_type_annotation)

    has_init_default = param_obj.default is not inspect.Parameter.empty
    init_default_value = param_obj.default if has_init_default else None

    argparse_kwargs = _determine_argparse_type_and_nargs(core_type if is_list else param_type_annotation, is_list)

    if has_init_default:
        argparse_kwargs["default"] = init_default_value
    elif is_overall_optional:  # Parameter is Optional (e.g. Optional[int]) and no explicit default in __init__
        argparse_kwargs["default"] = None  # So, if not provided on CLI, it becomes None.

    argparse_kwargs["help"] = _build_help_string(cls.__name__, param_name, core_type, is_overall_optional, is_list)

    if not has_init_default and not is_overall_optional:  # Required if no __init__ default AND not Optional
        argparse_kwargs["required"] = True
        if "default" in argparse_kwargs:  # Should not happen if logic is correct
            del argparse_kwargs["default"]

    cli_arg_name = f"--{cls.__name__.lower()}.{param_name.replace('_', '-')}"
    parser.add_argument(cli_arg_name, dest=dest_name, **argparse_kwargs)


def _add_arguments_for_class(
    parser: argparse.ArgumentParser,
    cls: Type[CliConfigurable],
    class_arg_configs_maps: Dict[Type[CliConfigurable], Dict[str, str]],  # Maps cls to {param_name: dest_name}
) -> None:
    """Adds all relevant CLI arguments for a given class by processing its __init__ parameters."""
    cls_name_lower = cls.__name__.lower()
    sig = inspect.signature(cls.__init__)

    try:
        # Resolve string annotations to actual types using get_type_hints.
        # For methods, get_type_hints automatically uses obj.__globals__ for globalns.
        resolved_hints = get_type_hints(cls.__init__)
    except Exception as e:
        logger.warning(
            f"Could not resolve type hints for {cls.__name__}.__init__ using get_type_hints: {e}. "
            f"CLI argument parsing for this class might be based on string annotations, "
            "which could be unreliable for complex types."
        )
        resolved_hints = {}  # Fallback to an empty dict if resolution fails

    if cls not in class_arg_configs_maps:  # Ensure the class entry exists
        class_arg_configs_maps[cls] = {}

    for param_name, param_obj in sig.parameters.items():
        if param_name == "self":  # Skip 'self'
            continue

        dest_name = f"{cls_name_lower}_{param_name}"  # Unique destination for argparse
        class_arg_configs_maps[cls][param_name] = dest_name  # Store mapping for later instantiation

        # Use the resolved hint if available, otherwise fallback to param_obj.annotation (which might be a string)
        actual_param_annotation = resolved_hints.get(param_name, param_obj.annotation)
        _add_argument_for_parameter(parser, cls, param_name, param_obj, dest_name, actual_param_annotation)


def _create_argument_parser() -> argparse.ArgumentParser:
    """Creates and returns the main ArgumentParser with default settings."""
    return argparse.ArgumentParser(
        description="CLI configurator for application components.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,  # Automatically shows default values in help
    )


def _instantiate_classes(
    parsed_args: argparse.Namespace,
    classes: Tuple[Type[CliConfigurable], ...],
    class_arg_configs_maps: Dict[Type[CliConfigurable], Dict[str, str]],
) -> Tuple[CliConfigurable, ...]:
    """Instantiates classes using the parsed CLI arguments and the stored mappings."""
    instances_list: List[CliConfigurable] = []
    for cls in classes:
        constructor_args: Dict[str, Any] = {}
        # Get the {__init__ param_name: argparse_dest_name} map for the current class
        param_to_dest_map = class_arg_configs_maps.get(cls, {})

        sig = inspect.signature(cls.__init__)
        for param_name_in_sig, _ in sig.parameters.items():
            if param_name_in_sig == "self":
                continue

            dest_name_for_arg = param_to_dest_map.get(param_name_in_sig)
            if dest_name_for_arg and hasattr(parsed_args, dest_name_for_arg):
                value = getattr(parsed_args, dest_name_for_arg)
                constructor_args[param_name_in_sig] = value
            # If an argument was required by argparse, parse_args() would have exited if missing.
            # If not required and not provided, its default value (set by argparse) is used.

        try:
            logger.info("Instantiating %s with args: %s", cls.__name__, constructor_args)
            instances_list.append(cls(**constructor_args))
        except Exception as e:
            parsed_args_for_cls = {
                k: getattr(parsed_args, v) for k, v in param_to_dest_map.items() if hasattr(parsed_args, v)
            }
            logger.error(
                f"Error instantiating {cls.__name__} with resolved args {constructor_args}. "
                f"Parsed args for class: "
                f"{parsed_args_for_cls}. "
                f"Error: {e}"
            )
            raise

    return tuple(instances_list)


@overload
def lightning_cli(cls1: Type[_C1]) -> _C1: ...
@overload
def lightning_cli(cls1: Type[_C1], cls2: Type[_C2]) -> Tuple[_C1, _C2]: ...
@overload
def lightning_cli(cls1: Type[_C1], cls2: Type[_C2], cls3: Type[_C3]) -> Tuple[_C1, _C2, _C3]: ...
@overload
def lightning_cli(cls1: Type[_C1], cls2: Type[_C2], cls3: Type[_C3], cls4: Type[_C4]) -> Tuple[_C1, _C2, _C3, _C4]: ...
@overload  # Fallback for more than 4 or a dynamic number of classes
def lightning_cli(*classes: Type[CliConfigurable]) -> Tuple[CliConfigurable, ...]: ...


# FIXME: lightning_cli needs to be fixed to comply with the latest trainer implementation.


def lightning_cli(*classes: Type[CliConfigurable]) -> CliConfigurable | Tuple[CliConfigurable, ...]:  # type: ignore
    """
    Parses command-line arguments to configure and instantiate provided CliConfigurable classes.

    Args:
        *classes: One or more classes that inherit from CliConfigurable. Each class's
                  __init__ parameters will be exposed as command-line arguments.

    Returns:
        A tuple of instantiated objects, corresponding to the input classes in order.
    """
    if not classes:
        return tuple()  # Return an empty tuple if no classes are provided

    parser = _create_argument_parser()

    # This map will store {cls: {init_param_name: argparse_dest_name}}
    class_arg_configs_maps: Dict[Type[CliConfigurable], Dict[str, str]] = {}

    for cls in classes:
        _add_arguments_for_class(parser, cls, class_arg_configs_maps)

    parsed_args = parser.parse_args()  # Uses sys.argv[1:] by default

    # Correctly handle single class case for return type matching overloads
    instances = _instantiate_classes(parsed_args, classes, class_arg_configs_maps)
    if len(classes) == 1:
        return instances[0]
    return instances


--- agentlightning/env_var.py ---
# Copyright (c) Microsoft. All rights reserved.

"""Environment variable managements."""

from __future__ import annotations

import os
from enum import Enum
from typing import overload

__all__ = [
    "LightningEnvVar",
    "resolve_bool_env_var",
    "resolve_int_env_var",
    "resolve_str_env_var",
]


class LightningEnvVar(Enum):
    """Environment variables for Agent Lightning."""

    AGL_EMITTER_DEBUG = "AGL_EMITTER_DEBUG"
    """Enable debug logging for the emitter."""

    AGL_MANAGED_STORE = "AGL_MANAGED_STORE"
    """If yes, the [`ExecutionStrategy`][agentlightning.ExecutionStrategy]
    constructs LightningStore wrappers automatically. When `False` the provided
    `store` is passed directly to the bundles, allowing callers to manage
    store wrappers manually."""

    AGL_CURRENT_ROLE = "AGL_CURRENT_ROLE"
    """Which side(s) to run in this process. Used in
    [`ClientServerExecutionStrategy`][agentlightning.ClientServerExecutionStrategy]."""

    AGL_SERVER_HOST = "AGL_SERVER_HOST"
    """Interface the [`LightningStoreServer`][agentlightning.LightningStoreServer]
    binds to when running the algorithm bundle locally."""

    AGL_SERVER_PORT = "AGL_SERVER_PORT"
    """Port the [`LightningStoreServer`][agentlightning.LightningStoreServer] listens to."""


_TRUTHY_VALUES = {"1", "true", "yes", "on"}
_FALSY_VALUES = {"0", "false", "no", "off"}


@overload
def resolve_bool_env_var(env_var: LightningEnvVar, override: bool, fallback: bool) -> bool: ...


@overload
def resolve_bool_env_var(env_var: LightningEnvVar, *, fallback: bool) -> bool: ...


@overload
def resolve_bool_env_var(
    env_var: LightningEnvVar, override: bool | None = None, fallback: bool | None = None
) -> bool | None: ...


def resolve_bool_env_var(
    env_var: LightningEnvVar, override: bool | None = None, fallback: bool | None = None
) -> bool | None:
    """Resolve a boolean environment variable.

    Args:
        env_var: The environment variable to resolve.
        override: Optional override supplied by the caller.
        fallback: Default value if the environment variable is not set.
    """

    if override is not None:
        return override

    env_value = os.getenv(env_var.value)
    if env_value is None:
        return fallback

    normalized = env_value.strip().lower()
    if normalized in _TRUTHY_VALUES:
        return True
    if normalized in _FALSY_VALUES:
        return False

    raise ValueError(f"{env_var.value} must be one of {_TRUTHY_VALUES} or {_FALSY_VALUES}")


@overload
def resolve_int_env_var(env_var: LightningEnvVar, override: int, fallback: int) -> int: ...


@overload
def resolve_int_env_var(env_var: LightningEnvVar, *, fallback: int) -> int: ...


@overload
def resolve_int_env_var(
    env_var: LightningEnvVar, override: int | None = None, fallback: int | None = None
) -> int | None: ...


def resolve_int_env_var(
    env_var: LightningEnvVar, override: int | None = None, fallback: int | None = None
) -> int | None:
    """Resolve an integer environment variable.

    Args:
        env_var: The environment variable to resolve.
        override: Optional override supplied by the caller.
        fallback: Default value if the environment variable is not set.
    """
    if override is not None:
        return override

    env_value = os.getenv(env_var.value)
    if env_value is None:
        return fallback

    try:
        return int(env_value)
    except ValueError:
        raise ValueError(f"{env_var.value} must be an integer")


@overload
def resolve_str_env_var(env_var: LightningEnvVar, override: str, fallback: str) -> str: ...


@overload
def resolve_str_env_var(env_var: LightningEnvVar, *, fallback: str) -> str: ...


@overload
def resolve_str_env_var(
    env_var: LightningEnvVar, override: str | None = None, fallback: str | None = None
) -> str | None: ...


def resolve_str_env_var(
    env_var: LightningEnvVar, override: str | None = None, fallback: str | None = None
) -> str | None:
    """Resolve a string environment variable.

    Args:
        env_var: The environment variable to resolve.
        override: Optional override supplied by the caller.
        fallback: Default value if the environment variable is not set.
    """
    if override is not None:
        return override

    env_value = os.getenv(env_var.value)
    if env_value is None:
        return fallback

    return env_value


--- agentlightning/__init__.py ---
# Copyright (c) Microsoft. All rights reserved.

__version__ = "0.3.1"

from .adapter import *
from .algorithm import *
from .client import AgentLightningClient, DevTaskLoader  # deprecated  # type: ignore
from .config import *
from .emitter import *
from .env_var import *
from .execution import *
from .litagent import *
from .llm_proxy import *
from .logging import configure_logger  # deprecated  # type: ignore
from .logging import setup as setup_logging  # type: ignore
from .logging import setup_module as setup_module_logging  # type: ignore
from .runner import *
from .server import AgentLightningServer  # deprecated  # type: ignore
from .store import *
from .tracer import *
from .trainer import *
from .types import *


--- agentlightning/llm_proxy.py ---
# Copyright (c) Microsoft. All rights reserved.

from __future__ import annotations

import ast
import asyncio
import json
import logging
import os
import re
import tempfile
import threading
import time
from contextlib import asynccontextmanager
from datetime import datetime
from typing import (
    Any,
    AsyncGenerator,
    Awaitable,
    Callable,
    Dict,
    Iterable,
    List,
    Literal,
    Optional,
    Sequence,
    Tuple,
    Type,
    TypedDict,
    Union,
    cast,
)

import litellm
import opentelemetry.trace as trace_api
import yaml
from fastapi import Request, Response
from fastapi.responses import StreamingResponse
from litellm.integrations.custom_logger import CustomLogger
from litellm.integrations.opentelemetry import OpenTelemetry, OpenTelemetryConfig
from litellm.proxy.proxy_server import app, save_worker_config  # pyright: ignore[reportUnknownVariableType]
from litellm.types.utils import CallTypes
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import ReadableSpan
from opentelemetry.sdk.trace.export import SpanExporter, SpanExportResult
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.types import Scope

from agentlightning.semconv import LightningResourceAttributes
from agentlightning.types import LLM, ProxyLLM
from agentlightning.utils.server_launcher import (
    LaunchMode,
    PythonServerLauncher,
    PythonServerLauncherArgs,
    noop_context,
)

from .store.base import LightningStore

logger = logging.getLogger(__name__)

__all__ = [
    "LLMProxy",
]


class ModelConfig(TypedDict):
    """LiteLLM model registration entry.

    This mirrors the items in LiteLLM's `model_list` section.

    Attributes:
        model_name: Logical model name exposed by the proxy.
        litellm_params: Parameters passed to LiteLLM for this model
            (e.g., backend model id, api_base, additional options).
    """  # Google style kept concise.

    model_name: str
    litellm_params: Dict[str, Any]


def _get_pre_call_data(args: Any, kwargs: Any) -> Dict[str, Any]:
    """Extract LiteLLM request payload from hook args.

    The LiteLLM logger hooks receive `(*args, **kwargs)` whose third positional
    argument or `data=` kwarg contains the request payload.

    Args:
        args: Positional arguments from the hook.
        kwargs: Keyword arguments from the hook.

    Returns:
        The request payload dict.

    Raises:
        ValueError: If the payload cannot be located or is not a dict.
    """
    if kwargs.get("data"):
        data = kwargs["data"]
    elif len(args) >= 3:
        data = args[2]
    else:
        raise ValueError(f"Unable to get request data from args or kwargs: {args}, {kwargs}")
    if not isinstance(data, dict):
        raise ValueError(f"Request data is not a dictionary: {data}")
    return cast(Dict[str, Any], data)


def _reset_litellm_logging_worker() -> None:
    """Reset LiteLLM's global logging worker to the current event loop.

    LiteLLM keeps a module-level ``GLOBAL_LOGGING_WORKER`` singleton that owns an
    ``asyncio.Queue``. The queue is bound to the event loop where it was created.
    When the proxy is restarted, Uvicorn spins up a brand new event loop in a new
    thread. If the existing logging worker (and its queue) are reused, LiteLLM
    raises ``RuntimeError: <Queue ...> is bound to a different event loop`` the
    next time it tries to log. Recreating the worker ensures that LiteLLM will
    lazily initialise a fresh queue on the new loop.
    """

    # ``GLOBAL_LOGGING_WORKER`` is imported in a few LiteLLM modules at runtime.
    # Update any already-imported references so future calls use the fresh worker.
    try:
        import litellm.utils as litellm_utils
        from litellm.litellm_core_utils import logging_worker as litellm_logging_worker

        litellm_logging_worker.GLOBAL_LOGGING_WORKER = litellm_logging_worker.LoggingWorker()
        litellm_utils.GLOBAL_LOGGING_WORKER = litellm_logging_worker.GLOBAL_LOGGING_WORKER  # type: ignore[reportAttributeAccessIssue]
    except Exception:  # pragma: no cover - best-effort hygiene
        logger.warning("Unable to propagate LiteLLM logging worker reset.", exc_info=True)


def _reset_litellm_logging_callback_manager() -> None:
    """Reset LiteLLM's global callback manager.

    To get rid of the warning message: "Cannot add callback - would exceed MAX_CALLBACKS limit of 30."
    when litellm is restarted multiple times in the same process.

    It does not respect existing input/output callbacks.
    """

    try:
        litellm.logging_callback_manager._reset_all_callbacks()  # pyright: ignore[reportPrivateUsage]
    except Exception:  # pragma: no cover - best-effort hygiene
        logger.warning("Unable to reset LiteLLM logging callback manager.", exc_info=True)


class AddReturnTokenIds(CustomLogger):
    """LiteLLM logger hook to request token ids from vLLM.

    This mutates the outgoing request payload to include `return_token_ids=True`
    for backends that support token id return (e.g., vLLM).

    See also:
        [vLLM PR #22587](https://github.com/vllm-project/vllm/pull/22587)
    """

    async def async_pre_call_hook(self, *args: Any, **kwargs: Any) -> Optional[Union[Exception, str, Dict[str, Any]]]:
        """Async pre-call hook to adjust request payload.

        Args:
            args: Positional args from LiteLLM.
            kwargs: Keyword args from LiteLLM.

        Returns:
            Either an updated payload dict or an Exception to short-circuit.
        """
        try:
            data = _get_pre_call_data(args, kwargs)
        except Exception as e:
            return e

        # Ensure token ids are requested from the backend when supported.
        return {**data, "return_token_ids": True}


class AddLogprobs(CustomLogger):
    """LiteLLM logger hook to request logprobs from vLLM.

    This mutates the outgoing request payload to include `logprobs=1`
    for backends that support logprobs return (e.g., vLLM).
    """

    async def async_pre_call_hook(self, *args: Any, **kwargs: Any) -> Optional[Union[Exception, str, Dict[str, Any]]]:
        """Async pre-call hook to adjust request payload."""
        try:
            data = _get_pre_call_data(args, kwargs)
        except Exception as e:
            return e

        # Ensure logprobs are requested from the backend when supported.
        return {**data, "logprobs": 1}


class LightningSpanExporter(SpanExporter):
    """Buffered OTEL span exporter with subtree flushing and training-store sink.

    Design:

    * Spans are buffered until a root span's entire subtree is available.
    * A private event loop on a daemon thread runs async flush logic.
    * Rollout/attempt/sequence metadata is reconstructed by merging headers
      from any span within a subtree.

    Thread-safety:

    * Buffer access is protected by a re-entrant lock.
    * Export is synchronous to the caller yet schedules an async flush on the
      internal loop, then waits for completion.
    """

    def __init__(self, _store: Optional[LightningStore] = None):
        self._store: Optional[LightningStore] = _store  # this is only for testing purposes
        self._buffer: List[ReadableSpan] = []
        self._lock: Optional[threading.Lock] = None
        self._loop_lock_pid: Optional[int] = None

        # Single dedicated event loop running in a daemon thread.
        # This decouples OTEL SDK threads from our async store I/O.
        # Deferred creation until first use.
        self._loop: Optional[asyncio.AbstractEventLoop] = None
        self._loop_thread: Optional[threading.Thread] = None

        self._otlp_exporter = OTLPSpanExporter()

    def _ensure_loop(self) -> asyncio.AbstractEventLoop:
        """Lazily initialize the event loop and thread on first use.

        Returns:
            asyncio.AbstractEventLoop: The initialized event loop.
        """
        self._clear_loop_and_lock()
        if self._loop is None:
            self._loop = asyncio.new_event_loop()
            self._loop_thread = threading.Thread(target=self._run_loop, name="LightningSpanExporterLoop", daemon=True)
            self._loop_thread.start()
        return self._loop

    def _ensure_lock(self) -> threading.Lock:
        """Lazily initialize the lock on first use.

        Returns:
            threading.Lock: The initialized lock.
        """
        self._clear_loop_and_lock()
        if self._lock is None:
            self._lock = threading.Lock()
        return self._lock

    def _clear_loop_and_lock(self) -> None:
        """Clear the loop and lock.
        This happens if the exporter was used in a process then used in another process.

        This should only happen in CI.
        """
        if os.getpid() != self._loop_lock_pid:
            logger.warning("Loop and lock are not owned by the current process. Clearing them.")
            self._loop = None
            self._loop_thread = None
            self._lock = None
            self._loop_lock_pid = os.getpid()
        elif self._loop_lock_pid is None:
            self._loop_lock_pid = os.getpid()

    def _run_loop(self) -> None:
        """Run the private asyncio loop forever on the exporter thread."""
        assert self._loop is not None, "Loop should be initialized before thread starts"
        asyncio.set_event_loop(self._loop)
        self._loop.run_forever()

    def shutdown(self) -> None:
        """Shut down the exporter event loop.

        Safe to call at process exit.

        """
        if self._loop is None:
            return

        try:

            def _stop():
                assert self._loop is not None
                self._loop.stop()

            self._loop.call_soon_threadsafe(_stop)
            if self._loop_thread is not None:
                self._loop_thread.join(timeout=2.0)
            self._loop.close()
        except Exception:
            logger.exception("Error during exporter shutdown")

    def export(self, spans: Sequence[ReadableSpan]) -> SpanExportResult:
        """Export spans via buffered subtree flush.

        Appends spans to the internal buffer, then triggers an async flush on the
        private event loop. Blocks until that flush completes.

        Args:
            spans: Sequence of spans to export.

        Returns:
            SpanExportResult: SUCCESS on flush success, else FAILURE.
        """
        # Buffer append under lock to protect against concurrent exporters.
        with self._ensure_lock():
            for span in spans:
                self._buffer.append(span)
            default_endpoint = self._otlp_exporter._endpoint  # pyright: ignore[reportPrivateUsage]
            try:
                self._maybe_flush()
            except Exception as e:
                logger.exception("Export flush failed: %s", e)
                return SpanExportResult.FAILURE
            finally:
                self._otlp_exporter._endpoint = default_endpoint  # pyright: ignore[reportPrivateUsage]

        return SpanExportResult.SUCCESS

    def _maybe_flush(self):
        """Flush ready subtrees from the buffer.

        Strategy:
            We consider a subtree "ready" if we can identify a root span. We
            then take that root and all its descendants out of the buffer and
            try to reconstruct rollout/attempt/sequence headers by merging any
            span's `metadata.requester_custom_headers` within the subtree.

        Required headers:
            `x-rollout-id` (str), `x-attempt-id` (str), `x-sequence-id` (str of int)

        Raises:
            None directly. Logs and skips malformed spans.

        """
        # Iterate over current roots. Each iteration pops a whole subtree.
        for root_span_id in self._get_root_span_ids():
            subtree_spans = self._pop_subtrees(root_span_id)
            if not subtree_spans:
                continue

            # Store is initialized lazily here in most cases.
            store = self._store or get_active_llm_proxy().get_store()
            if store is None:
                logger.warning("Store is not set in LLMProxy. Cannot log spans to store.")
                continue

            # If the store supports OTLP endpoint, use it.
            if store.capabilities.get("otlp_traces", False):
                otlp_traces_endpoint = store.otlp_traces_endpoint()
                self._otlp_exporter._endpoint = otlp_traces_endpoint  # pyright: ignore[reportPrivateUsage]
                otlp_enabled = True
            else:
                otlp_enabled = False

            # Merge all custom headers found in the subtree.
            headers_merged: Dict[str, Any] = {}

            for span in subtree_spans:
                if span.attributes is None:
                    continue
                headers_str = span.attributes.get("metadata.requester_custom_headers")
                if headers_str is None:
                    continue
                if not isinstance(headers_str, str):
                    logger.error(
                        f"metadata.requester_custom_headers is not stored as a string: {headers_str}. Skipping the span."
                    )
                    continue
                if not headers_str.strip():
                    logger.warning("metadata.requester_custom_headers is an empty string. Skipping the span.")
                    continue
                try:
                    # Use literal_eval to parse the stringified dict safely.
                    headers = ast.literal_eval(headers_str)
                except Exception as e:
                    logger.error(
                        f"Failed to parse metadata.requester_custom_headers: {headers_str}, error: {e}. Skipping the span."
                    )
                    continue
                if not isinstance(headers, dict):
                    logger.error(
                        f"metadata.requester_custom_headers is not parsed as a dict: {headers}. Skipping the span."
                    )
                    continue
                headers_merged.update(cast(Dict[str, Any], headers))

            if not headers_merged:
                logger.warning(
                    f"No headers found in {len(subtree_spans)} subtree spans of root {root_span_id}. Cannot log to store."
                )
                continue

            # Validate and normalize required header fields.
            rollout_id = headers_merged.get("x-rollout-id")
            attempt_id = headers_merged.get("x-attempt-id")
            sequence_id = headers_merged.get("x-sequence-id")
            if not rollout_id or not attempt_id or not sequence_id or not sequence_id.isdigit():
                logger.warning(
                    f"Missing or invalid rollout_id, attempt_id, or sequence_id in headers: {headers_merged}. Cannot log to store."
                )
                continue
            if not isinstance(rollout_id, str) or not isinstance(attempt_id, str):
                logger.warning(
                    f"rollout_id or attempt_id is not a string: {rollout_id}, {attempt_id}. Cannot log to store."
                )
                continue
            sequence_id_decimal = int(sequence_id)

            # Persist each span in the subtree with the resolved identifiers.
            if otlp_enabled:
                # If store has OTLP support, directly use OTLP exporter and export in batch
                for span in subtree_spans:
                    span._resource = span._resource.merge(  # pyright: ignore[reportPrivateUsage]
                        Resource.create(
                            {
                                LightningResourceAttributes.ROLLOUT_ID.value: rollout_id,
                                LightningResourceAttributes.ATTEMPT_ID.value: attempt_id,
                                LightningResourceAttributes.SPAN_SEQUENCE_ID.value: sequence_id_decimal,
                            }
                        )
                    )
                export_result = self._otlp_exporter.export(subtree_spans)
                if export_result != SpanExportResult.SUCCESS:
                    raise RuntimeError(f"Failed to export spans via OTLP exporter. Result: {export_result}")

            else:
                # The old way: store does not support OTLP endpoint
                for span in subtree_spans:
                    loop = self._ensure_loop()
                    add_otel_span_task = store.add_otel_span(
                        rollout_id=rollout_id,
                        attempt_id=attempt_id,
                        sequence_id=sequence_id_decimal,
                        readable_span=span,
                    )
                    fut = asyncio.run_coroutine_threadsafe(add_otel_span_task, loop)
                    fut.result()  # Bubble up any exceptions from the coroutine.

    def _get_root_span_ids(self) -> Iterable[int]:
        """Yield span_ids for root spans currently in the buffer.

        A root span is defined as one with `parent is None`.

        Yields:
            int: Span id for each root span found.
        """
        for span in self._buffer:
            if span.parent is None:
                span_context = span.get_span_context()
                if span_context is not None:
                    yield span_context.span_id

    def _get_subtrees(self, root_span_id: int) -> Iterable[int]:
        """Yield span_ids in the subtree rooted at `root_span_id`.

        Depth-first traversal over the current buffer.

        Args:
            root_span_id: The span id of the root.

        Yields:
            int: Span ids including the root and all descendants found.
        """
        # Yield the root span id first.
        yield root_span_id
        for span in self._buffer:
            # Check whether the span's parent is the root_span_id.
            if span.parent is not None and span.parent.span_id == root_span_id:
                span_context = span.get_span_context()
                if span_context is not None:
                    # Recursively get child spans.
                    yield from self._get_subtrees(span_context.span_id)

    def _pop_subtrees(self, root_span_id: int) -> List[ReadableSpan]:
        """Remove and return the subtree for a particular root from the buffer.

        Args:
            root_span_id: Root span id identifying the subtree.

        Returns:
            list[ReadableSpan]: Spans that were part of the subtree. Order follows buffer order.
        """
        subtree_span_ids = set(self._get_subtrees(root_span_id))
        subtree_spans: List[ReadableSpan] = []
        new_buffer: List[ReadableSpan] = []
        for span in self._buffer:
            span_context = span.get_span_context()
            if span_context is not None and span_context.span_id in subtree_span_ids:
                subtree_spans.append(span)
            else:
                new_buffer.append(span)
        # Replace buffer with remaining spans to avoid re-processing.
        self._buffer = new_buffer
        return subtree_spans


class LightningOpenTelemetry(OpenTelemetry):
    """OpenTelemetry integration that exports spans to the Lightning store.

    Responsibilities:

    * Ensures each request is annotated with a per-attempt sequence id so spans
      are ordered deterministically even with clock skew across nodes.
    * Uses [`LightningSpanExporter`][agentlightning.llm_proxy.LightningSpanExporter] to persist spans for analytics and training.
    """

    def __init__(self):
        config = OpenTelemetryConfig(exporter=LightningSpanExporter())

        # Check for tracer initialization
        if _check_tracer_provider():
            logger.error("Tracer is already initialized. OpenTelemetry may not work as expected.")

        super().__init__(config=config)  # pyright: ignore[reportUnknownMemberType]

    async def async_pre_call_deployment_hook(
        self, kwargs: Dict[str, Any], call_type: Optional[CallTypes] = None
    ) -> Optional[Dict[str, Any]]:
        """The root span is sometimes missing (e.g., when Anthropic endpoint is used).
        It is created in an auth module in LiteLLM. If it's missing, we create it here.
        """
        if "metadata" not in kwargs or "litellm_parent_otel_span" not in kwargs["metadata"]:
            parent_otel_span = self.create_litellm_proxy_request_started_span(  # type: ignore
                start_time=datetime.now(),
                headers=kwargs.get("headers", {}),
            )
            updated_metadata = {**kwargs.get("metadata", {}), "litellm_parent_otel_span": parent_otel_span}

            return {**kwargs, "metadata": updated_metadata}
        else:
            return kwargs


class RolloutAttemptMiddleware(BaseHTTPMiddleware):
    """
    Rewrites /rollout/{rid}/attempt/{aid}/... -> /...
    and injects x-rollout-id, x-attempt-id, x-sequence-id headers.

    LLMProxy can update store later without rebuilding middleware.
    """

    async def dispatch(self, request: Request, call_next: Callable[[Request], Awaitable[Response]]) -> Response:
        # Decode rollout and attempt from the URL prefix. Example:
        #   /rollout/r123/attempt/a456/v1/chat/completions
        # becomes
        #   /v1/chat/completions
        # while adding request-scoped headers for trace attribution.
        path = request.url.path

        match = re.match(r"^/rollout/([^/]+)/attempt/([^/]+)(/.*)?$", path)
        if match:
            rollout_id = match.group(1)
            attempt_id = match.group(2)
            new_path = match.group(3) if match.group(3) is not None else "/"

            # Rewrite the ASGI scope path so downstream sees a clean OpenAI path.
            request.scope["path"] = new_path
            request.scope["raw_path"] = new_path.encode()

            store = get_active_llm_proxy().get_store()
            if store is not None:
                # Allocate a monotonic sequence id per (rollout, attempt).
                sequence_id = await store.get_next_span_sequence_id(rollout_id, attempt_id)

                # Inject headers so downstream components and exporters can retrieve them.
                request.scope["headers"] = list(request.scope["headers"]) + [
                    (b"x-rollout-id", rollout_id.encode()),
                    (b"x-attempt-id", attempt_id.encode()),
                    (b"x-sequence-id", str(sequence_id).encode()),
                ]
            else:
                logger.warning("Store is not set. Skipping sequence id allocation and header injection.")

        response = await call_next(request)
        return response


class MessageInspectionMiddleware(BaseHTTPMiddleware):
    """Middleware to inspect the request and response bodies.

    It's for debugging purposes. Add it via "message_inspection" middleware alias.
    """

    async def dispatch(self, request: Request, call_next: Callable[[Request], Awaitable[Response]]) -> Response:
        ti = time.time()
        logger.info(f"Received request with scope: {request.scope}")
        logger.info(f"Received request with body: {await request.body()}")
        response = await call_next(request)
        elapsed = time.time() - ti
        logger.info(f"Response to request took {elapsed} seconds")
        logger.info(f"Received response with status code: {response.status_code}")
        logger.info(f"Received response with body: {response.body}")
        return response


class StreamConversionMiddleware(BaseHTTPMiddleware):
    """Middleware to convert streaming responses to non-streaming responses.

    Useful for backend that only supports non-streaming responses.

    LiteLLM's OpenTelemetry is also buggy with streaming responses.
    The conversion will hopefully bypass the bug.
    """

    async def dispatch(self, request: Request, call_next: Callable[[Request], Awaitable[Response]]) -> Response:
        # Only process POST requests to completion endpoints
        if request.method != "POST":
            return await call_next(request)

        # Check if it's a chat completions or messages endpoint
        endpoint_format: Literal["openai", "anthropic", "unknown"] = "unknown"
        if request.url.path.endswith("/chat/completions") or "/chat/completions?" in request.url.path:
            endpoint_format = "openai"
        elif request.url.path.endswith("/messages") or "/messages?" in request.url.path:
            endpoint_format = "anthropic"
        else:
            endpoint_format = "unknown"

        if endpoint_format == "unknown":
            # Directly bypass the middleware
            return await call_next(request)

        # Read the request body
        try:
            json_body = await request.json()
        except json.JSONDecodeError:
            logger.warning(f"Request body is not valid JSON: {request.body}")
            return await call_next(request)

        # Check if streaming is requested
        is_streaming = json_body.get("stream", False)

        # Simple case: no streaming requested, just return the response
        if not is_streaming:
            return await call_next(request)

        # Now the stream case
        return await self._handle_stream_case(request, json_body, endpoint_format, call_next)

    async def _handle_stream_case(
        self,
        request: Request,
        json_body: Dict[str, Any],
        endpoint_format: Literal["openai", "anthropic"],
        call_next: Callable[[Request], Awaitable[Response]],
    ) -> Response:
        # 1) Modify the request body to force stream=False
        modified_json = dict(json_body)
        modified_json["stream"] = False
        modified_body = json.dumps(modified_json).encode("utf-8")

        # 2) Build a new scope + receive that yields our modified body
        scope: Scope = dict(request.scope)
        # rewrite headers for accept/content-length
        new_headers: List[Tuple[bytes, bytes]] = []
        saw_accept = False
        for k, v in scope["headers"]:
            kl = k.lower()
            if kl == b"accept":
                saw_accept = True
                new_headers.append((k, b"application/json"))
            elif kl == b"content-length":
                # replace with new length
                continue
            else:
                new_headers.append((k, v))
        if not saw_accept:
            new_headers.append((b"accept", b"application/json"))
        new_headers.append((b"content-length", str(len(modified_body)).encode("ascii")))
        scope["headers"] = new_headers

        # Directly modify the request body
        # Creating a new request won't work because request is cached in the base class
        request._body = modified_body  # type: ignore

        response = await call_next(request)

        buffered: Optional[bytes] = None
        # 4) If OK, buffer the response body (it should be JSON because we forced stream=False)
        if 200 <= response.status_code < 300:
            try:
                if hasattr(response, "body_iterator"):
                    # Buffer body safely
                    body_chunks: List[bytes] = []
                    async for chunk in response.body_iterator:  # type: ignore
                        body_chunks.append(chunk)  # type: ignore
                    buffered = b"".join(body_chunks)
                else:
                    buffered = response.body  # type: ignore

                data = json.loads(buffered or b"{}")

                if endpoint_format == "anthropic":
                    return StreamingResponse(
                        self.anthropic_stream_generator(data),
                        media_type="text/event-stream",
                        headers={"Cache-Control": "no-cache", "Connection": "keep-alive", "X-Accel-Buffering": "no"},
                    )
                else:
                    # openai format
                    return StreamingResponse(
                        self.openai_stream_generator(data),
                        media_type="text/event-stream",
                        headers={"Cache-Control": "no-cache", "Connection": "keep-alive", "X-Accel-Buffering": "no"},
                    )
            except Exception as e:
                # If anything goes wrong, fall back to non-streaming JSON
                logger.exception(f"Error converting to stream; returning non-stream response: {e}")
                # Rebuild the consumed response
                return Response(
                    content=buffered if buffered is not None else b"",
                    status_code=response.status_code,
                    headers=dict(response.headers),
                    media_type=response.media_type,
                    background=response.background,
                )
        else:
            return response

    async def anthropic_stream_generator(self, original_response: Dict[str, Any]):
        """Generate Anthropic SSE-formatted chunks from complete content blocks

        This is a dirty hack for Anthropic-style streaming from non-streaming response.
        The sse format is subject to change based on Anthropic's implementation.
        If so, try to use `MessageInspectionMiddleware` to inspect the update and fix accordingly.
        """
        # Anthropic format - handle multiple content blocks (text + tool_use)
        content_blocks: List[Dict[str, Any]] = original_response.get("content", [])
        message_id = original_response.get("id", f"msg_{int(time.time() * 1000)}")
        model = original_response.get("model", "claude")

        # Send message_start event
        message_start: Dict[str, Any] = {
            "type": "message_start",
            "message": {
                "id": message_id,
                "type": "message",
                "role": "assistant",
                "content": [],
                "model": model,
                "stop_reason": None,
                "stop_sequence": None,
                "usage": original_response.get("usage", {"input_tokens": 0, "output_tokens": 0}),
            },
        }
        yield f"event: message_start\ndata: {json.dumps(message_start)}\n\n"

        # Send ping to keep connection alive
        ping = {"type": "ping"}
        yield f"event: ping\ndata: {json.dumps(ping)}\n\n"

        # Process each content block
        for block_index, block in enumerate(content_blocks):
            block_type = block.get("type", "text")

            if block_type == "text":
                # Handle text block
                content = block.get("text", "")

                # Send content_block_start event
                content_block_start = {
                    "type": "content_block_start",
                    "index": block_index,
                    "content_block": {"type": "text", "text": ""},
                }
                yield f"event: content_block_start\ndata: {json.dumps(content_block_start)}\n\n"

                # Stream text content in chunks
                if content:
                    words = content.split()
                    chunk_size = 5

                    for i in range(0, len(words), chunk_size):
                        chunk_words = words[i : i + chunk_size]
                        text_chunk = " ".join(chunk_words)

                        # Add space after chunk unless it's the last one
                        if i + chunk_size < len(words):
                            text_chunk += " "

                        content_block_delta = {
                            "type": "content_block_delta",
                            "index": block_index,
                            "delta": {"type": "text_delta", "text": text_chunk},
                        }
                        yield f"event: content_block_delta\ndata: {json.dumps(content_block_delta)}\n\n"
                        await asyncio.sleep(0.02)

                # Send content_block_stop event
                content_block_stop = {"type": "content_block_stop", "index": block_index}
                yield f"event: content_block_stop\ndata: {json.dumps(content_block_stop)}\n\n"

            elif block_type == "tool_use":
                # Handle tool_use block
                tool_name = block.get("name", "")
                tool_input = block.get("input", {})
                tool_id = block.get("id", f"toolu_{int(time.time() * 1000)}")

                # Send content_block_start event for tool use
                content_block_start: Dict[str, Any] = {
                    "type": "content_block_start",
                    "index": block_index,
                    "content_block": {"type": "tool_use", "id": tool_id, "name": tool_name, "input": {}},
                }
                yield f"event: content_block_start\ndata: {json.dumps(content_block_start)}\n\n"

                # Stream tool input as JSON string chunks
                input_json = json.dumps(tool_input)
                chunk_size = 20  # characters per chunk for JSON

                for i in range(0, len(input_json), chunk_size):
                    json_chunk = input_json[i : i + chunk_size]

                    content_block_delta = {
                        "type": "content_block_delta",
                        "index": block_index,
                        "delta": {"type": "input_json_delta", "partial_json": json_chunk},
                    }
                    yield f"event: content_block_delta\ndata: {json.dumps(content_block_delta)}\n\n"
                    await asyncio.sleep(0.01)

                # Send content_block_stop event
                content_block_stop = {"type": "content_block_stop", "index": block_index}
                yield f"event: content_block_stop\ndata: {json.dumps(content_block_stop)}\n\n"

        # Send message_delta event with stop reason
        message_delta = {
            "type": "message_delta",
            "delta": {"stop_reason": original_response.get("stop_reason", "end_turn"), "stop_sequence": None},
            "usage": {"output_tokens": original_response.get("usage", {}).get("output_tokens", 0)},
        }
        yield f"event: message_delta\ndata: {json.dumps(message_delta)}\n\n"

        # Send message_stop event
        message_stop = {"type": "message_stop"}
        yield f"event: message_stop\ndata: {json.dumps(message_stop)}\n\n"

    async def openai_stream_generator(self, response_json: Dict[str, Any]) -> AsyncGenerator[str, Any]:
        """
        Convert a *complete* OpenAI chat.completions choice into a stream of
        OpenAI-compatible SSE chunks.

        This emits:

          - an initial delta with the role ("assistant"),
          - a sequence of deltas for message.content (split into small chunks),
          - deltas for any tool_calls (including id/name and chunked arguments),
          - a terminal chunk with finish_reason,
          - and finally the literal '[DONE]'.

        Notes:

        - We only handle a *single* choice (index 0 typically).
        - We purposefully don't attempt to stream logprobs.
        - Chunking strategy is simple and conservative to avoid splitting
          multi-byte characters: we slice on spaces where possible, then fall
          back to fixed-size substrings.
        """
        choice = cast(Dict[str, Any], (response_json.get("choices") or [{}])[0])
        model = response_json.get("model", "unknown")
        created: int = int(time.time())
        index: int = choice.get("index", 0)

        message: Dict[str, Any] = choice.get("message", {}) or {}
        role: str = message.get("role", "assistant")
        content: str = message.get("content") or ""
        tool_calls: List[Any] = message.get("tool_calls") or []
        finish_reason: Optional[str] = choice.get(
            "finish_reason"
        )  # e.g., "stop", "length", "tool_calls", "content_filter"

        def sse_chunk(obj: Dict[str, Any]) -> str:
            return f"data: {json.dumps(obj, ensure_ascii=False)}\n\n"

        # 1) initial chunk with the role
        yield sse_chunk(
            {
                "id": f"chatcmpl-{created}",
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{"index": index, "delta": {"role": role}, "finish_reason": None}],
            }
        )

        # 2) stream textual content as small deltas
        async def stream_content(text: str):
            if not text:
                return
            # prefer splitting on spaces in ~20–40 char pieces
            approx = 28
            start = 0
            n = len(text)
            while start < n:
                end = min(start + approx, n)
                if end < n:
                    # try to break on a space going forward
                    space = text.rfind(" ", start, end)
                    if space > start:
                        end = space + 1
                delta_text = text[start:end]
                start = end
                if not delta_text:
                    break
                yield sse_chunk(
                    {
                        "id": f"chatcmpl-{created}",
                        "object": "chat.completion.chunk",
                        "created": created,
                        "model": model,
                        "choices": [{"index": index, "delta": {"content": delta_text}, "finish_reason": None}],
                    }
                )
                # tiny pause helps some UIs animate smoothly; keep very small
                await asyncio.sleep(0.0)

        async for piece in stream_content(content):  # type: ignore[misc]
            yield piece  # pass through the produced chunks

        # 3) stream tool_calls if present (id/name first, then arguments piecemeal)
        for tc_index, tc in enumerate(tool_calls):
            tc_type = tc.get("type", "function")
            tc_id = tc.get("id") or f"call_{created}_{tc_index}"
            fn: Dict[str, Any] = (tc.get("function") or {}) if tc_type == "function" else {}
            fn_name: str = fn.get("name", "")
            fn_args: str = fn.get("arguments", "") or ""

            # (a) delta that announces the tool call id/type/name
            yield sse_chunk(
                {
                    "id": f"chatcmpl-{created}",
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": model,
                    "choices": [
                        {
                            "index": index,
                            "delta": {
                                "tool_calls": [
                                    {"index": tc_index, "id": tc_id, "type": tc_type, "function": {"name": fn_name}}
                                ]
                            },
                            "finish_reason": None,
                        }
                    ],
                }
            )

            # (b) stream arguments in small substrings
            arg_chunk_size = 40
            for pos in range(0, len(fn_args), arg_chunk_size):
                partial = fn_args[pos : pos + arg_chunk_size]
                yield sse_chunk(
                    {
                        "id": f"chatcmpl-{created}",
                        "object": "chat.completion.chunk",
                        "created": created,
                        "model": model,
                        "choices": [
                            {
                                "index": index,
                                "delta": {"tool_calls": [{"index": tc_index, "function": {"arguments": partial}}]},
                                "finish_reason": None,
                            }
                        ],
                    }
                )
                await asyncio.sleep(0.0)

        # 4) terminal chunk with finish_reason (default to "stop" if missing)
        yield sse_chunk(
            {
                "id": f"chatcmpl-{created}",
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [
                    {
                        "index": index,
                        "delta": {},
                        "finish_reason": finish_reason or ("tool_calls" if tool_calls else "stop"),
                    }
                ],
            }
        )

        # 5) literal DONE sentinel
        yield "data: [DONE]\n\n"


_MIDDLEWARE_REGISTRY: Dict[str, Type[BaseHTTPMiddleware]] = {
    "rollout_attempt": RolloutAttemptMiddleware,
    "stream_conversion": StreamConversionMiddleware,
    "message_inspection": MessageInspectionMiddleware,
}


_CALLBACK_REGISTRY = {
    "return_token_ids": AddReturnTokenIds,
    "logprobs": AddLogprobs,
    "opentelemetry": LightningOpenTelemetry,
}


class LLMProxy:
    """Host a LiteLLM OpenAI-compatible proxy bound to a LightningStore.

    The proxy:

    * Serves an OpenAI-compatible API via uvicorn.
    * Adds rollout/attempt routing and headers via middleware.
    * Registers OTEL export and token-id callbacks.
    * Writes a LiteLLM worker config file with `model_list` and settings.

    Lifecycle:

    * [`start()`][agentlightning.LLMProxy.start] writes config, starts uvicorn server in a thread, and waits until ready.
    * [`stop()`][agentlightning.LLMProxy.stop] tears down the server and removes the temp config file.
    * [`restart()`][agentlightning.LLMProxy.restart] convenience wrapper to stop then start.

    !!! note

        As the LLM Proxy sets up an OpenTelemetry tracer, it's recommended to run it in a different
        process from the main runner (i.e., tracer from agents). See `launch_mode` for how to change that.

    !!! warning

        By default (or when "stream_conversion" middleware is enabled), the LLM Proxy will convert OpenAI and Anthropic requests with `stream=True`
        to a non-streaming request before going through the LiteLLM proxy. This is because the OpenTelemetry tracer provided by
        LiteLLM is buggy with streaming responses. You can disable this by removing the "stream_conversion" middleware.
        In that case, you might lose some tracing information like token IDs.

    !!! danger

        Do not run LLM proxy in the same process as the main runner. It's easy to cause conflicts in the tracer provider
        with tracers like [`AgentOpsTracer`][agentlightning.AgentOpsTracer].

    Args:
        port: TCP port to bind. Will bind to a random port if not provided.
        model_list: LiteLLM `model_list` entries.
        store: LightningStore used for span sequence and persistence.
        host: Publicly reachable host used in resource endpoints. See `host` of `launcher_args` for more details.
        litellm_config: Extra LiteLLM proxy config merged with `model_list`.
        num_retries: Default LiteLLM retry count injected into `litellm_settings`.
        num_workers: Number of workers to run in the server. Only applicable for "mp" launch mode. Ignored if launcher_args is provided.
            When `num_workers > 1`, the server will be run using [gunicorn](https://gunicorn.org/).
        launch_mode: Launch mode for the server. Defaults to "mp". Cannot be used together with launcher_args. Ignored if launcher_args is provided.
            It's recommended to use `launch_mode="mp"` to launch the proxy, which will launch the server in a separate process.
            `launch_mode="thread"` can also be used if used in caution. It will launch the server in a separate thread.
            `launch_mode="asyncio"` launches the server in the current thread as an asyncio task.
            It is NOT recommended because it often causes hanging requests. Only use it if you know what you are doing.
        launcher_args: Arguments for the server launcher. If this is provided, host, port, and launch_mode will be ignored. Cannot be used together with port, host, and launch_mode.
        middlewares: List of FastAPI middleware classes or strings to register. You can specify the class aliases or classes that have been imported.
            If not provided, the default middlewares (RolloutAttemptMiddleware and StreamConversionMiddleware) will be used.
            Available middleware aliases are: "rollout_attempt", "stream_conversion", "message_inspection".
            Middlewares are the **first layer** of request processing. They are applied to all requests before the LiteLLM proxy.
        callbacks: List of LiteLLM callback classes or strings to register. You can specify the class aliases or classes that have been imported.
            If not provided, the default callbacks (AddReturnTokenIds and LightningOpenTelemetry) will be used.
            Available callback aliases are: "return_token_ids", "opentelemetry", "logprobs".
    """

    def __init__(
        self,
        port: int | None = None,
        model_list: List[ModelConfig] | None = None,
        store: Optional[LightningStore] = None,
        host: str | None = None,
        litellm_config: Dict[str, Any] | None = None,
        num_retries: int = 0,
        num_workers: int = 1,
        launch_mode: LaunchMode = "mp",
        launcher_args: PythonServerLauncherArgs | None = None,
        middlewares: Sequence[Union[Type[BaseHTTPMiddleware], str]] | None = None,
        callbacks: Sequence[Union[Type[CustomLogger], str]] | None = None,
    ):
        self.store = store

        if launcher_args is not None and (
            port is not None or host is not None or launch_mode != "mp" or num_workers != 1
        ):
            raise ValueError("port, host, launch_mode, and num_workers cannot be set when launcher_args is provided.")

        self.server_launcher_args = launcher_args or PythonServerLauncherArgs(
            port=port,
            host=host,
            launch_mode=launch_mode,
            n_workers=num_workers,
            # NOTE: This /health endpoint can be slow sometimes because it actually probes the backend LLM service.
            healthcheck_url="/health",
            startup_timeout=60.0,
        )

        if self.server_launcher_args.healthcheck_url is None:
            logger.warning("healthcheck_url is not set. LLM Proxy will not be checked for healthiness after starting.")

        self.model_list = model_list or []
        self.litellm_config = litellm_config or {}

        # Ensure num_retries is present inside the litellm_settings block.
        self.litellm_config.setdefault("litellm_settings", {})
        self.litellm_config["litellm_settings"].setdefault("num_retries", num_retries)
        self.server_launcher = PythonServerLauncher(app, self.server_launcher_args, noop_context())

        self._config_file = None

        self.middlewares: List[Type[BaseHTTPMiddleware]] = []
        if middlewares is None:
            middlewares = ["rollout_attempt", "stream_conversion"]
        for middleware in middlewares:
            if isinstance(middleware, str):
                if middleware not in _MIDDLEWARE_REGISTRY:
                    raise ValueError(
                        f"Invalid middleware alias: {middleware}. Available aliases are: {list(_MIDDLEWARE_REGISTRY.keys())}"
                    )
                middleware = _MIDDLEWARE_REGISTRY[middleware]
                self.middlewares.append(middleware)
            else:
                self.middlewares.append(middleware)

        self.callbacks: List[Type[CustomLogger]] = []
        if callbacks is None:
            callbacks = ["return_token_ids", "opentelemetry"]
        for callback in callbacks:
            if isinstance(callback, str):
                if callback not in _CALLBACK_REGISTRY:
                    raise ValueError(
                        f"Invalid callback alias: {callback}. Available aliases are: {list(_CALLBACK_REGISTRY.keys())}"
                    )
                callback = _CALLBACK_REGISTRY[callback]
                self.callbacks.append(callback)
            else:
                self.callbacks.append(callback)

    def get_store(self) -> Optional[LightningStore]:
        """Get the store used by the proxy.

        Returns:
            The store used by the proxy.
        """
        return self.store

    def set_store(self, store: LightningStore) -> None:
        """Set the store for the proxy.

        Args:
            store: The store to use for the proxy.
        """
        self.store = store

    def update_model_list(self, model_list: List[ModelConfig]) -> None:
        """Replace the in-memory model list.

        Args:
            model_list: New list of model entries.
        """
        self.model_list = model_list
        logger.info(f"Updating LLMProxy model list to: {model_list}")
        # Do nothing if the server is not running.

    def initialize(self):
        """Initialize global middleware and LiteLLM callbacks.

        Installs:

        * A FastAPI middleware that rewrites /rollout/{rid}/attempt/{aid}/... paths,
        injects rollout/attempt/sequence headers, and forwards downstream.
        * LiteLLM callbacks for token ids and OpenTelemetry export.

        The middleware can only be installed once because once the FastAPI app has started,
        the middleware cannot be changed any more.

        This function does not start any server. It only wires global hooks.
        """
        if self.store is None:
            raise ValueError("Store is not set. Please set the store before initializing the LLMProxy.")

        if _global_llm_proxy is not None:
            logger.warning("A global LLMProxy is already set. Overwriting it with the new instance.")

        # Patch for LiteLLM v1.80.6+: https://github.com/BerriAI/litellm/issues/17243
        os.environ["USE_OTEL_LITELLM_REQUEST_SPAN"] = "true"

        # Set the global LLMProxy reference for middleware/exporter access.
        set_active_llm_proxy(self)

        # Install middleware if it's not already installed.
        installation_status: Dict[Any, bool] = {}
        for mw in app.user_middleware:
            installation_status[mw.cls] = True

        for mw in self.middlewares:
            if mw not in installation_status:
                logger.info(f"Adding middleware {mw} to the FastAPI app.")
                app.add_middleware(mw)
            else:
                logger.info(f"Middleware {mw} is already installed. Will not install a new one.")

        if not initialize_llm_callbacks(self.callbacks):
            # If it's not the first time to initialize the callbacks, also
            # reset LiteLLM's logging worker so its asyncio.Queue binds to the new loop.
            _reset_litellm_logging_worker()

    @asynccontextmanager
    async def _serve_context(self) -> AsyncGenerator[None, None]:
        """Context manager to serve the proxy server.

        See [`start`][agentlightning.LLMProxy.start] and [`stop`][agentlightning.LLMProxy.stop] for more details.
        """

        if not self.store:
            raise ValueError("Store is not set. Please set the store before starting the LLMProxy.")

        # Initialize global middleware and callbacks.
        self.initialize()

        # Persist a temp worker config for LiteLLM and point the proxy at it.
        self._config_file = tempfile.NamedTemporaryFile(suffix=".yaml", delete=False).name
        with open(self._config_file, "w") as fp:
            yaml.safe_dump(
                {
                    "model_list": self.model_list,
                    **self.litellm_config,
                },
                fp,
            )

        save_worker_config(config=self._config_file)

        # NOTE: When running the _serve_context in current process, you might encounter the following problems:
        # Problem 1: in litellm worker, <Queue at 0x70f1d028cd90 maxsize=50000> is bound to a different event loop
        # Problem 2: Proxy has conflicted opentelemetry setup with the main process.

        # Ready
        logger.info("LLMProxy preparation is done. Will start the server.")
        yield

        # Clean up

        logger.info("LLMProxy server is cleaning up.")

        # Remove worker config to avoid stale references.
        if self._config_file and os.path.exists(self._config_file):
            os.unlink(self._config_file)

        logger.info("LLMProxy server finishes.")

    async def start(self):
        """Start the proxy server thread and initialize global wiring.

        Side effects:

        * Sets the module-level global store for middleware/exporter access.
        * Calls `initialize()` once to register middleware and callbacks.
        * Writes a temporary YAML config consumed by LiteLLM worker.
        * Launches uvicorn in a daemon thread and waits for readiness.
        """
        # Refresh the serve context
        self.server_launcher.serve_context = self._serve_context()

        if self.store is None:
            raise ValueError("Store is not set. Please set the store before starting the LLMProxy.")

        store_capabilities = self.store.capabilities
        if self.server_launcher.args.launch_mode == "mp" and not store_capabilities.get("zero_copy", False):
            raise RuntimeError(
                "The store does not support zero-copy. Please use another store, or use asyncio or thread mode to launch the server."
            )
        elif self.server_launcher.args.launch_mode == "thread" and not store_capabilities.get("thread_safe", False):
            raise RuntimeError(
                "The store is not thread-safe. Please use another store, or use asyncio mode to launch the server."
            )
        elif self.server_launcher.args.launch_mode == "asyncio" and not store_capabilities.get("async_safe", False):
            raise RuntimeError("The store is not async-safe. Please use another store.")

        logger.info(
            f"Starting LLMProxy server in {self.server_launcher.args.launch_mode} mode with store capabilities: {store_capabilities}"
        )

        await self.server_launcher.start()

    async def stop(self):
        """Stop the proxy server and clean up temporary artifacts.

        This is a best-effort graceful shutdown with a bounded join timeout.
        """
        if not self.is_running():
            logger.warning("LLMProxy is not running. Nothing to stop.")
            return

        await self.server_launcher.stop()

    async def restart(self, *, _port: int | None = None) -> None:
        """Restart the proxy if running, else start it.

        Convenience wrapper calling `stop()` followed by `start()`.
        """
        logger.info("Restarting LLMProxy server...")
        if self.is_running():
            await self.stop()
        if _port is not None:
            self.server_launcher_args.port = _port
        await self.start()

    def is_running(self) -> bool:
        """Return whether the uvicorn server is active.

        Returns:
            bool: True if server was started and did not signal exit.
        """
        return self.server_launcher.is_running()

    def as_resource(
        self,
        rollout_id: str | None = None,
        attempt_id: str | None = None,
        model: str | None = None,
        sampling_parameters: Dict[str, Any] | None = None,
    ) -> LLM:
        """Create an `LLM` resource pointing at this proxy with rollout context.

        The returned endpoint is:
            `http://{host}:{port}/rollout/{rollout_id}/attempt/{attempt_id}`

        Args:
            rollout_id: Rollout identifier used for span attribution. If None, will instantiate a ProxyLLM resource.
            attempt_id: Attempt identifier used for span attribution. If None, will instantiate a ProxyLLM resource.
            model: Logical model name to use. If omitted and exactly one model
                is configured or all models have the same name, that model is used.
            sampling_parameters: Optional default sampling parameters.

        Returns:
            LLM: Configured resource ready for OpenAI-compatible calls.

        Raises:
            ValueError: If `model` is omitted and zero or multiple models are configured.
        """
        if model is None:
            if len(self.model_list) == 1:
                model = self.model_list[0]["model_name"]
            elif len(self.model_list) == 0:
                raise ValueError("No models found in model_list. Please specify the model.")
            else:
                first_model_name = self.model_list[0]["model_name"]
                if all(model_config["model_name"] == first_model_name for model_config in self.model_list):
                    model = first_model_name
                else:
                    raise ValueError(
                        f"Multiple models found in model_list: {self.model_list}. Please specify the model."
                    )

        if rollout_id is None and attempt_id is None:
            return ProxyLLM(
                endpoint=self.server_launcher.access_endpoint,
                model=model,
                sampling_parameters=dict(sampling_parameters or {}),
            )
        elif rollout_id is not None and attempt_id is not None:
            return LLM(
                endpoint=f"{self.server_launcher.access_endpoint}/rollout/{rollout_id}/attempt/{attempt_id}",
                model=model,
                sampling_parameters=dict(sampling_parameters or {}),
            )
        else:
            raise ValueError("Either rollout_id and attempt_id must be provided, or neither.")


_global_llm_proxy: Optional[LLMProxy] = None
_callbacks_before_litellm_start: Optional[List[Any]] = None


def get_active_llm_proxy() -> LLMProxy:
    """Get the current global LLMProxy instance.

    Returns:
        Optional[LLMProxy]: The current LLMProxy if set, else None.
    """
    if _global_llm_proxy is None:
        raise ValueError("Global LLMProxy is not set. Please call llm_proxy.start() first.")
    return _global_llm_proxy


def set_active_llm_proxy(proxy: LLMProxy) -> None:
    """Set the current global LLMProxy instance.

    Args:
        proxy: The LLMProxy instance to set as global.
    """
    global _global_llm_proxy
    _global_llm_proxy = proxy


def initialize_llm_callbacks(callback_classes: List[Type[CustomLogger]]) -> bool:
    """Restore `litellm.callbacks` to a state that is just initialized by agent-lightning.

    When litellm is restarted multiple times in the same process, more and more callbacks
    will be appended to `litellm.callbacks`, which may exceed the MAX_CALLBACKS limit.
    This function remembers the initial state of `litellm.callbacks` and always restore to that state.

    Args:
        callback_classes: List of callback classes to register.

    Returns:
        Whether the callbacks are initialized for the first time.
    """
    global _callbacks_before_litellm_start

    if _callbacks_before_litellm_start is None:
        litellm.callbacks.extend([cls() for cls in callback_classes])  # type: ignore
        _callbacks_before_litellm_start = [*litellm.callbacks]  # type: ignore
        return True

    else:
        # Put whatever is missing in the new callback classes to the existing callbacks.
        for cls in callback_classes:
            if not any(isinstance(cb, cls) for cb in _callbacks_before_litellm_start):
                logger.info(f"Adding missing callback {cls} to the existing callbacks.")
                _callbacks_before_litellm_start.append(cls())

    _reset_litellm_logging_callback_manager()

    if LightningOpenTelemetry in callback_classes:
        # Check if tracer provider is malformed due to global tracer clear in tests.
        if not _check_tracer_provider():
            logger.warning(
                "Global tracer provider might have been cleared outside. Re-initializing OpenTelemetry callback."
            )
            _callbacks_before_litellm_start = [
                cb for cb in _callbacks_before_litellm_start if not isinstance(cb, LightningOpenTelemetry)
            ] + [LightningOpenTelemetry()]
        else:
            logger.debug("Global tracer provider is valid. Reusing existing OpenTelemetry callback.")
    # Otherwise, we just skip the check for opentelemetry and use the existing callback.

    litellm.callbacks.clear()  # type: ignore
    litellm.callbacks.extend(_callbacks_before_litellm_start)  # type: ignore
    return False


def _check_tracer_provider() -> bool:
    """Check if the global tracer provider is properly initialized.

    We don't guarantee the tracer provider is our tracer provider.

    Returns:
        bool: True if the tracer provider is valid, else False.
    """
    if (
        hasattr(trace_api, "_TRACER_PROVIDER")
        and trace_api._TRACER_PROVIDER is not None  # pyright: ignore[reportPrivateUsage]
    ):
        return True
    return False


## Links discovered
- [vLLM PR #22587](https://github.com/vllm-project/vllm/pull/22587)
- [gunicorn](https://gunicorn.org/)

--- agentlightning/logging.py ---
# Copyright (c) Microsoft. All rights reserved.

from __future__ import annotations

import logging
import os
import platform
import sys
import warnings
from logging.config import dictConfig
from typing import Any, Dict, Optional

from rich.console import Console

__all__ = ["setup", "configure_logger", "setup_module"]


def configure_logger(level: int = logging.INFO, name: str = "agentlightning") -> logging.Logger:
    """Create or reset a namespaced logger with a consistent console format.

    This helper clears any previously attached handlers before binding a single
    `StreamHandler` that writes to standard output. The resulting logger does
    not propagate to the root logger, preventing duplicate log emission when
    applications compose multiple logging configurations.

    !!! danger

        This function is deprecated in favor of [`setup_logging`][agentlightning.setup_logging].

    Args:
        level: Logging level applied both to the logger and the installed
            handler. Defaults to `logging.INFO`.
        name: Dotted path for the logger instance. Defaults to
            `"agentlightning"`.

    Returns:
        Configured logger instance ready for immediate use.

    Examples:
        ```python
        from agentlightning import configure_logger

        logger = configure_logger(level=logging.INFO)
        logger.info("agent-lightning is ready!")
        ```
    """
    warnings.warn("This function is deprecated in favor of `setup_logging`.", DeprecationWarning, stacklevel=2)

    return setup_module(level=level, name=name, console=True, color=True, propagate=False)


DEFAULT_FORMAT = "%(asctime)s [%(levelname)s] (Process-%(process)d %(name)s)   %(message)s"
DATE_FORMAT = "%H:%M:%S"


def _to_level_value(lvl: int | str) -> int:
    if isinstance(lvl, int):
        return lvl
    val = getattr(logging, str(lvl).upper(), None)
    if val is None:
        raise ValueError(f"Invalid log level: {lvl}")
    return val


def _ensure_file_handler(
    logger: logging.Logger,
    filename: str,
    *,
    level: int,
    formatter: Optional[logging.Formatter],
) -> None:
    """Attach a FileHandler to `logger` for `filename` if it doesn't already exist."""
    abspath = os.path.abspath(filename)

    # Avoid duplicates
    for h in logger.handlers:
        if isinstance(h, logging.FileHandler) and getattr(h, "baseFilename", None) == abspath:
            return

    # Ensure directory exists
    dirname = os.path.dirname(abspath)
    if dirname:
        os.makedirs(dirname, exist_ok=True)

    fh = logging.FileHandler(abspath, encoding="utf-8")
    fh.setLevel(level)
    if formatter is not None:
        fh.setFormatter(formatter)
    else:
        fh.setFormatter(logging.Formatter(DEFAULT_FORMAT, DATE_FORMAT))

    logger.addHandler(fh)


def setup(
    level: int | str = "INFO",
    *,
    console: bool = True,
    color: bool | Dict[str, Any] = True,
    propagate: bool = False,
    disable_existing_loggers: bool = False,
    capture_warnings: bool = False,
    submodule_levels: Optional[dict[str, int | str]] = None,
    extra_handlers: Optional[list[logging.Handler]] = None,
    formatter: Optional[logging.Formatter] = None,
    apply_to: Optional[list[str]] = None,
    files: Optional[str | dict[str, str]] = None,
) -> None:
    """Configures logging for the `agentlightning` logger hierarchy.

    This function provides a one-stop setup utility for configuring the
    `agentlightning` root logger and optionally its submodules or external
    loggers. It supports console logging, colored rich output, per-submodule
    log levels, and optional handler/formatter injection.

    The setup is intentionally isolated: it does not modify the global root
    logger or loggers belonging to other libraries unless explicitly directed
    via `apply_to`.

    Args:
        level:
            Logging level for the base `agentlightning` logger. Accepts either
            an integer (e.g., `logging.DEBUG`) or a string level name
            (e.g., `"INFO"`). Defaults to `"INFO"`.
        console:
            Whether to attach a console handler to the logger. Defaults to
            `True`.
        color:
            Enables rich-formatted output using `RichHandler` when `True`
            or a configuration dict. If `False`, a plain text formatter is
            used instead. Defaults to `True`.
        propagate:
            Whether `agentlightning` logs should propagate to ancestor
            loggers. Defaults to `False`.
        disable_existing_loggers:
            Passed to `logging.config.dictConfig`. If `True`, disables all
            existing configured loggers before applying this configuration.
            Defaults to `False`.
        capture_warnings:
            If `True`, redirects Python `warnings` emitted via the `warnings`
            module into the logging system. Defaults to `False`.
        submodule_levels:
            Mapping of submodule logger names to logging levels. If a specified
            submodule level is more verbose than the base level, a warning is emitted.
        extra_handlers:
            A list of user-provided handlers to attach to the `agentlightning` logger.
            Handlers are added idempotently; duplicates are not reattached.
        formatter:
            A formatter to apply to any handler under `agentlightning` that does not
            already have one assigned. Useful for customizing output without overwriting
            formatters on custom handlers.
        apply_to:
            A list of additional logger names to configure identically to
            `agentlightning` base logger. Their handlers are replaced with copies of the base
            handlers, and propagation is disabled to avoid duplicate log emission.
        files:
            If a string, attach a FileHandler to the base `agentlightning` logger.
            If a dict, for each `(logger_name, filename)` pair, attach a FileHandler
            directly to that logger.
            Each file handler should use the logger's effective level at creation.

    Notes:
        * On Windows, this function forces UTF-8 mode in the console to prevent
          issues with rich output or special characters.
        * Submodule loggers can generate records below the handler's emission
          threshold. Whether such records appear depends on both the logger's
          level and the handler's level.
        * `apply_to` loggers inherit the same handlers but do not propagate
          upward, yielding isolated, consistent behavior.

    Examples:
        Basic setup:

        >>> setup()

        Enabling debug mode with no color:

        >>> setup(level="DEBUG", color=False)

        Overriding specific submodule levels:

        >>> setup(submodule_levels={"agentlightning.io": "DEBUG"})

        Attaching an additional file handler:

        >>> fh = logging.FileHandler("app.log")
        >>> setup(extra_handlers=[fh])
    """
    # Ensure UTF-8 encoding on Windows consoles
    # Note: This change does not fully represent support for execution under the windows system.
    # It only fixes console printing issues caused by special characters.
    # TODO: More comprehensive Windows support may be needed in the future.
    if platform.system() == "Windows":
        os.environ["PYTHONUTF8"] = "1"

    base_logger = setup_module(
        level,
        name="agentlightning",
        console=console,
        color=color,
        propagate=propagate,
        disable_existing_loggers=disable_existing_loggers,
    )

    base_level_value = base_logger.level

    # Apply user-provided formatter (only to handlers without one,
    # so we don't clobber custom extra_handlers)
    if formatter is not None:
        for h in base_logger.handlers:
            if h.formatter is None:
                h.setFormatter(formatter)

    # Attach user-provided handler(s) if any, idempotently
    if extra_handlers:
        for h in extra_handlers:
            if h not in base_logger.handlers:
                base_logger.addHandler(h)

    # Per-submodule levels
    if submodule_levels:
        for name, lvl in submodule_levels.items():
            sub_level = _to_level_value(lvl)

            # Emit a warning if submodule level is lower (more verbose) than the global/base level
            if sub_level < base_level_value:
                base_logger.warning(
                    "Submodule logger '%s' level %s (%s) is more verbose than base "
                    "logger level %s (%s). Records below the base level may still be "
                    "filtered out by handlers depending on their own levels.",
                    name,
                    lvl,
                    sub_level,
                    logging.getLevelName(base_level_value),
                    base_level_value,
                )

            # The logger will *create* records down to the logger's level, but a handler
            # with a higher level will still drop anything below its own threshold.
            # Effective emission is gated by both: record.level >= logger.level AND handler.level.
            logging.getLogger(name).setLevel(lvl)

    # Attach file handlers if requested
    if files is not None:
        if isinstance(files, str):
            # Single file for the entire `agentlightning` hierarchy.
            _ensure_file_handler(
                logger=base_logger,
                filename=files,
                level=base_level_value,
                formatter=formatter,
            )
        else:
            # Per-logger files
            for logger_name, filename in files.items():
                lg = logging.getLogger(logger_name)
                # Use the logger's *effective* level at creation time
                effective_level = lg.getEffectiveLevel()
                _ensure_file_handler(
                    logger=lg,
                    filename=filename,
                    level=effective_level,
                    formatter=formatter,
                )

    # Optionally apply the same handler setup to other loggers outside this module
    if apply_to:
        for name in apply_to:
            lg = logging.getLogger(name)
            # This removes any existing handlers so we don't duplicate output
            # and ensures these loggers share exactly the same handlers as base_logger.
            lg.handlers.clear()
            for h in base_logger.handlers:
                lg.addHandler(h)
            lg.setLevel(base_logger.level)
            # We've attached handlers directly to these loggers; if propagate
            # stayed True, records would bubble up to ancestor loggers and could be
            # emitted twice (here and on the parent/root). Setting False isolates them.
            lg.propagate = False

    # Optionally capture warnings
    if capture_warnings:
        logging.captureWarnings(True)


def setup_module(
    level: int | str = "INFO",
    *,
    name: str = "agentlightning",
    console: bool = True,
    color: bool | Dict[str, Any] = True,
    propagate: bool = False,
    disable_existing_loggers: bool = False,
) -> logging.Logger:
    """Initializes and returns the base logger for `agentlightning`.

    This function constructs and applies a `dictConfig` configuration for the
    logger hierarchy rooted at `name`. It supports either rich console
    formatting (via `RichHandler`) or plain text formatting, based on the
    `color` argument.

    Unlike [`setup_logging`][agentlightning.setup_logging], this function configures only a single logger namespace
    and does not attach extra handlers or submodule levels. It is primarily used
    internally by [`setup_logging`][agentlightning.setup_logging] but is also suitable for direct integration in
    custom logging workflows.
    """
    root_cfg: Dict[str, Any] = {
        "version": 1,
        "disable_existing_loggers": disable_existing_loggers,
        "loggers": {
            name: {
                "handlers": [],
                "level": level,
                "propagate": propagate,
            }
        },
        "handlers": {},
        "formatters": {},
    }

    # Choose formatter / handler definition
    if color is not False and console:
        # Console must be true to display colored outputs
        if isinstance(color, dict):
            rich_handler_config = color
        else:
            rich_handler_config: Dict[str, Any] = {
                "rich_tracebacks": False,
                "markup": False,
                "show_time": True,
                "show_path": True,
            }

            if not _has_width():
                # e.g., in a CI environment.
                rich_handler_config["console"] = Console(width=200)

        root_cfg["handlers"]["console"] = {
            "class": "rich.logging.RichHandler",
            "level": level,
            **rich_handler_config,
        }
        # RichHandler manages its own style; keep formatter None
    else:
        fmt_name = "plain"
        root_cfg["formatters"][fmt_name] = {
            "format": DEFAULT_FORMAT,
            "datefmt": DATE_FORMAT,
        }

        if console:
            root_cfg["handlers"]["console"] = {
                "class": "logging.StreamHandler",
                "level": level,
                "formatter": fmt_name,
            }

    # Attach selected handlers to agentlightning
    handler_names = list(root_cfg["handlers"].keys())
    root_cfg["loggers"][name]["handlers"] = handler_names

    # Apply dictConfig (this resets the logger handlers)
    dictConfig(root_cfg)

    return logging.getLogger(name)


def _has_width() -> bool:
    """Automatically determine whether the terminal has a width."""
    return sys.stdout.isatty()


--- agentlightning/reward.py ---
# Copyright (c) Microsoft. All rights reserved.

import warnings

from .emitter.reward import *  # noqa: F401,F403

warnings.warn("agentlightning.reward is deprecated. Please use agentlightning.emitter instead.")


--- agentlightning/semconv.py ---
# Copyright (c) Microsoft. All rights reserved.

"""Semantic conventions for Agent-lightning spans.

Conventions in this file are added on demand. We generally DO NOT add
new semantic conventions unless it's absolutely needed for certain algorithms or scenarios.
"""

from enum import Enum

from pydantic import BaseModel

AGL_ANNOTATION = "agentlightning.annotation"
"""Agent-lightning's standard span name for annotations.

Annotations are minimal span units for rewards, tags, and metadatas.
They are used to "annotate" a specific event or a part of rollout.
"""

AGL_MESSAGE = "agentlightning.message"
"""Agent-lightning's standard span name for messages and logs."""

AGL_OBJECT = "agentlightning.object"
"""Agent-lightning's standard span name for customized objects."""

AGL_EXCEPTION = "agentlightning.exception"
"""Agent-lightning's standard span name for exceptions.

Used by the exception emitter to record exception details.
"""

AGL_OPERATION = "agentlightning.operation"
"""Agent-lightning's standard span name for functions.
Wrap function or code-blocks as operations.
"""

AGL_REWARD = "agentlightning.reward"
"""Agent-lightning's standard span name for reward operations."""

AGL_VIRTUAL = "agentlightning.virtual"
"""Agent-lightning's standard span name for virtual operations.

Mostly used in adapter when needing to represent the root or intermediate operations.
"""


class LightningResourceAttributes(Enum):
    """Resource attribute names used in Agent-lightning spans."""

    ROLLOUT_ID = "agentlightning.rollout_id"
    """Resource name for rollout ID in Agent-lightning spans."""

    ATTEMPT_ID = "agentlightning.attempt_id"
    """Resource name for attempt ID in Agent-lightning spans."""

    SPAN_SEQUENCE_ID = "agentlightning.span_sequence_id"
    """Resource name for span sequence ID in Agent-lightning spans."""

    TRACER_NAME = "agentlightning.tracer.name"
    """Which tracer is used to create this span."""


class LightningSpanAttributes(Enum):
    """Attribute names that commonly appear in Agent-lightning spans.

    Exception types can't be found here because they are defined in OpenTelemetry's official semantic conventions.
    """

    REWARD = "agentlightning.reward"
    """Attribute prefix for rewards-related data in reward spans.

    It should be used as a prefix. For example, "agentlightning.reward.0.value" can
    be used to track a specific metric. See [RewardAttributes][agentlightning.semconv.RewardAttributes].
    """

    LINK = "agentlightning.link"
    """Attribute name for linking the current span to another span or other objects like requests/responses."""

    TAG = "agentlightning.tag"
    """Attribute name for tagging spans with customized strings."""

    MESSAGE_BODY = "agentlightning.message.body"
    """Attribute name for message text in message spans."""

    OBJECT_TYPE = "agentlightning.object.type"
    """Attribute name for object type (full qualified name) in object spans.

    I think builtin types like str, int, bool, list, dict are self-explanatory and
    should also be qualified to use here.
    """

    OBJECT_LITERAL = "agentlightning.object.literal"
    """Attribute name for object literal value in object spans (for str, int, bool, ...)."""

    OBJECT_JSON = "agentlightning.object.json"
    """Attribute name for object serialized value (JSON) in object spans."""

    OPERATION_NAME = "agentlightning.operation.name"
    """Attribute name for operation name in operation spans, normally the function name."""

    OPERATION_INPUT = "agentlightning.operation.input"
    """Attribute name for operation input in operation spans."""

    OPERATION_OUTPUT = "agentlightning.operation.output"
    """Attribute name for operation output in operation spans."""


class RewardAttributes(Enum):
    """Multi-dimensional reward attributes will look like:

    ```json
    {"agentlightning.reward.0.name": "efficiency", "agentlightning.reward.0.value": 0.75}
    ```

    The first reward in the reward list will automatically be the primary reward.
    If the reward list has greater than 1, it shall be a multi-dimensional case.
    """

    REWARD_NAME = "name"
    """Key for each dimension in multi-dimensional reward spans."""

    REWARD_VALUE = "value"
    """Value for each dimension in multi-dimensional reward spans."""


class RewardPydanticModel(BaseModel):
    """A stricter implementation of RewardAttributes used in otel helpers."""

    name: str
    """Name of the reward dimension."""

    value: float
    """Value of the reward dimension."""


class LinkAttributes(Enum):
    """Standard link types used in Agent-lightning spans.

    The link is more powerful than [OpenTelemetry link](https://opentelemetry.io/docs/specs/otel/trace/api/#link)
    in that it supports linking to a queryset of spans.
    It can even link to span object that hasn't been emitted yet.
    """

    KEY_MATCH = "key_match"
    """Linking to spans with matching attribute keys.

    `trace_id` and `span_id` are reserved and will be used to link to specific spans directly.

    For example, it can be `gen_ai.response.id` if intended to be link to a chat completion response span.
    Or it can be `span_id` to link to a specific span by its ID.
    """

    VALUE_MATCH = "value_match"
    """Linking to spans with corresponding attribute values on those keys."""


class LinkPydanticModel(BaseModel):
    """A stricter implementation of LinkAttributes used in otel helpers."""

    key_match: str
    """The attribute key to match on the target spans."""

    value_match: str
    """The attribute value to match on the target spans."""


## Links discovered
- [OpenTelemetry link](https://opentelemetry.io/docs/specs/otel/trace/api/#link)

--- agentlightning/server.py ---
# Copyright (c) Microsoft. All rights reserved.

"""Legacy HTTP server compatible with the original Agent Lightning protocol.

The implementation in this module predates the modern store-powered runtime and
is kept for backwards compatibility with older deployments. New applications
should migrate to the store architecture where possible.
"""

from __future__ import annotations

import asyncio
import logging
import threading
import time
import uuid
import warnings
from contextlib import asynccontextmanager
from typing import Any, Dict, List, Literal, Optional

import uvicorn
from fastapi import FastAPI, HTTPException, Path

from .types import (
    GenericResponse,
    NamedResources,
    ResourcesUpdate,
    RolloutLegacy,
    Task,
    TaskIfAny,
)

logger = logging.getLogger(__name__)


class ServerDataStore:
    """Async-safe container for in-memory server state.

    The store tracks queued tasks, claimed tasks, uploaded rollouts, and the
    currently published resources. All interactions are guarded by asyncio locks
    so that the FastAPI handlers can safely run in parallel.

    !!! warning "Deprecated"
        [`ServerDataStore`][agentlightning.server.ServerDataStore] is part of
        the legacy client/server stack. Use [`LightningStore`][agentlightning.LightningStore] instead.
    """

    def __init__(self):
        self._task_queue: asyncio.Queue[Task] = asyncio.Queue()
        self._processing_tasks: Dict[str, Task] = {}  # Currently processing tasks
        self._completed_rollouts: Dict[str, RolloutLegacy] = {}

        # Store for versioned resources
        self._resource_versions: Dict[str, NamedResources] = {}
        self._latest_resources_id: Optional[str] = None

        # Locks for thread-safe access
        self._results_lock = asyncio.Lock()
        self._resources_lock = asyncio.Lock()

    async def add_task(
        self,
        sample: Any,
        mode: Literal["train", "val", "test"] | None = None,
        resources_id: str | None = None,
        metadata: Dict[str, Any] | None = None,
    ) -> str:
        """Enqueue a new task and return the generated rollout identifier.

        Args:
            sample: Payload that describes the task input.
            mode: Phase in which the sample should be executed (`"train"`, `"val"`, or
                `"test"`).
            resources_id: Identifier of a resource bundle that the executor should
                load before running the task.
            metadata: Optional metadata forwarded to the executor.

        Returns:
            Unique rollout identifier assigned to the task.
        """
        rollout_id = f"rollout-{uuid.uuid4()}"
        task = Task(
            rollout_id=rollout_id,
            input=sample,
            mode=mode,
            resources_id=resources_id,
            create_time=time.time(),
            num_claims=0,
            metadata=metadata or {},
        )
        await self._task_queue.put(task)
        logger.info(f"Task queued: {rollout_id} (mode: {mode}, resources_id: {resources_id})")
        return rollout_id

    async def get_next_task(self) -> Optional[Task]:
        """Retrieve the next task from the queue without blocking.

        Returns:
            Next [`Task`][agentlightning.Task] ready to execute, or ``None``
            when the queue is empty.
        """
        try:
            async with self._results_lock:
                task = self._task_queue.get_nowait()
                task = task.model_copy(
                    update={
                        "last_claim_time": time.time(),
                        "num_claims": (task.num_claims or 0) + 1,
                    }
                )
                self._processing_tasks[task.rollout_id] = task
                if task.num_claims == 1:
                    logger.debug(f"Next task retrieved: {task.rollout_id}")
                else:
                    logger.info(f"Task {task.rollout_id} re-claimed (attempt {task.num_claims})")
                return task
        except asyncio.QueueEmpty:
            return None

    async def update_resources(self, update: ResourcesUpdate):
        """Persist a new resource bundle and mark it as the latest version.

        Args:
            update: Resource payload received from a client.
        """
        # TODO: evict old resources if necessary.
        async with self._resources_lock:
            self._resource_versions[update.resources_id] = update.resources
            self._latest_resources_id = update.resources_id
            logger.info(f"Resources updated. New version '{update.resources_id}' is now latest.")

    async def get_resources_by_id(self, resources_id: str) -> Optional[ResourcesUpdate]:
        """Retrieve a specific resource bundle by identifier.

        Args:
            resources_id: Identifier that was previously published to the store.

        Returns:
            Matching [`ResourcesUpdate`][agentlightning.ResourcesUpdate]
            instance, or ``None`` when the identifier is unknown.
        """
        async with self._resources_lock:
            resources = self._resource_versions.get(resources_id)
            if resources:
                return ResourcesUpdate(
                    resources_id=resources_id,
                    resources=resources,
                    create_time=time.time(),
                    update_time=time.time(),
                    version=1,
                )
            return None

    async def get_latest_resources(self) -> Optional[ResourcesUpdate]:
        """Return the most recent resource bundle, if one exists."""
        if self._latest_resources_id:
            return await self.get_resources_by_id(self._latest_resources_id)
        return None

    async def store_rollout(self, rollout: RolloutLegacy):
        """Persist a completed rollout for later inspection.

        Args:
            rollout: Rollout returned by a client.
        """
        async with self._results_lock:
            self._processing_tasks.pop(rollout.rollout_id, None)
            self._completed_rollouts[rollout.rollout_id] = rollout
            logger.info(f"Rollout received and stored: {rollout.rollout_id}")

    async def retrieve_rollout(self, rollout_id: str) -> Optional[RolloutLegacy]:
        """Retrieve and remove a stored rollout by identifier.

        Args:
            rollout_id: Identifier of the rollout to fetch.

        Returns:
            Stored [`RolloutLegacy`][agentlightning.RolloutLegacy], or ``None``
            when the identifier is unknown.
        """
        async with self._results_lock:
            return self._completed_rollouts.pop(rollout_id, None)

    async def retrieve_completed_rollouts(self) -> List[RolloutLegacy]:
        """Return all completed rollouts and clear the internal buffer."""
        async with self._results_lock:
            rollouts = list(self._completed_rollouts.values())
            self._completed_rollouts.clear()
            return rollouts

    def get_processing_tasks(self) -> Dict[str, Task]:
        """Return a copy of currently processing tasks for timeout checking."""
        return self._processing_tasks.copy()

    async def requeue_task(self, task: Task):
        """Requeue a task that timed out while being processed."""
        logger.warning(f"Requeuing task {task.rollout_id} after timeout (attempt {task.num_claims})")
        async with self._results_lock:
            # Remove from processing tasks
            self._processing_tasks.pop(task.rollout_id, None)
            self._task_queue.put_nowait(task)


class AgentLightningServer:
    """High-level controller for the legacy Agent Lightning FastAPI server.

    The controller orchestrates server start-up, task queueing, resource updates,
    and retrieval of client rollouts. It is primarily used by existing systems that
    still rely on the HTTP-based workflow.

    !!! warning "Deprecated"
        [`AgentLightningServer`][agentlightning.server.AgentLightningServer] is part of
        the legacy client/server stack. Prefer the store-based runtime for new
        integrations.
    """

    def __init__(self, host: str = "127.0.0.1", port: int = 8000, task_timeout_seconds: float = 300.0):
        """Initialize the controller.

        Args:
            host: Hostname or IP address to bind the HTTP server to.
            port: TCP port exposed by the server.
            task_timeout_seconds: Seconds before a claimed task is considered stale and
                re-queued.
        """
        warnings.warn(
            "AgentLightningServer is deprecated. Please use LightningStoreServer instead.", DeprecationWarning
        )
        self.host = host
        self.port = port
        self.endpoint = f"http://{host}:{port}"
        self._task_timeout_seconds = task_timeout_seconds

        # Defer initialization and use event for cross-thread communication
        self._store: Optional[ServerDataStore] = None
        self.loop: Optional[asyncio.AbstractEventLoop] = None
        self.startup_event = threading.Event()

        # Create FastAPI app instance with a lifespan manager
        self._app = FastAPI(lifespan=self._lifespan)
        self._setup_routes()

        self._uvicorn_config = uvicorn.Config(self._app, host=self.host, port=self.port, log_level="info")
        self._uvicorn_server = uvicorn.Server(self._uvicorn_config)

    # --- ADDED: Lifespan context manager ---
    @asynccontextmanager
    async def _lifespan(self, app: FastAPI):
        """Manage server start-up and shutdown within the event loop."""
        logger.info("Server is starting up...")
        self.loop = asyncio.get_running_loop()
        self._store = ServerDataStore()  # Initialize data store here
        self.startup_event.set()  # Signal that the server is ready

        yield

        logger.info("Server is shutting down.")
        self._store = None
        self.startup_event.clear()  # Clear the startup event
        self.loop = None

    async def _check_and_requeue_stale_tasks(self):
        """Check for stale tasks and requeue them when they exceed the timeout."""
        current_time = time.time()
        # Ensure store is initialized before checking
        if not self._store:
            return
        processing_tasks = self._store.get_processing_tasks()

        for _, task in processing_tasks.items():
            if task.last_claim_time and current_time - task.last_claim_time > self._task_timeout_seconds:
                await self._store.requeue_task(task)
                logger.warning(
                    f"Task {task.rollout_id} timed out after {self._task_timeout_seconds}s, requeued (attempt {task.num_claims})"
                )

    def _setup_routes(self):
        """Configure the FastAPI routes that make up the legacy HTTP API."""

        @self._app.get("/task", response_model=TaskIfAny)
        async def next_task() -> TaskIfAny:  # type: ignore
            """Provide the next available task to a client."""
            await self._check_and_requeue_stale_tasks()

            if not self._store:
                return TaskIfAny(is_available=False)

            task = await self._store.get_next_task()
            if task:
                logger.debug(f"Serving task {task.rollout_id} to a client.")
                return TaskIfAny(is_available=True, task=task)
            else:
                logger.debug("No task available for client.")
                return TaskIfAny(is_available=False)

        @self._app.get("/resources/latest", response_model=ResourcesUpdate)
        async def fetch_latest_resources() -> ResourcesUpdate:  # type: ignore
            """Return the most recent resource bundle published to the server."""
            if not self._store:
                raise HTTPException(status_code=503, detail="Server not fully initialized.")
            resources_update = await self._store.get_latest_resources()
            if not resources_update:
                raise HTTPException(status_code=404, detail="No resources have been set on the server.")
            logger.debug(f"Serving latest resources '{resources_update.resources_id}' to a client.")
            return resources_update

        @self._app.get("/resources/{resource_id}", response_model=ResourcesUpdate)
        async def fetch_resources_by_id(  # type: ignore
            resource_id: str = Path(..., description="The unique identifier for the resource version.")
        ) -> ResourcesUpdate:
            """Return a specific version of resources by identifier."""
            if not self._store:
                raise HTTPException(status_code=503, detail="Server not fully initialized.")
            resources_update = await self._store.get_resources_by_id(resource_id)
            if not resources_update:
                raise HTTPException(status_code=404, detail=f"Resource ID '{resource_id}' not found.")
            logger.debug(f"Serving resources for ID '{resource_id}' to a client.")
            return resources_update

        @self._app.post("/rollout", response_model=GenericResponse)
        async def post_rollout(payload: RolloutLegacy) -> GenericResponse:  # type: ignore
            """Persist the rollout reported by a client."""
            if not self._store:
                raise HTTPException(status_code=503, detail="Server not fully initialized.")
            await self._store.store_rollout(payload)
            return GenericResponse(
                status="ok",
                message=f"Rollout {payload.rollout_id} received and stored.",
            )

    async def start(self):
        """Start the FastAPI server in the background."""
        logger.info(f"Starting server at {self.endpoint}")
        asyncio.create_task(self._uvicorn_server.serve())
        await asyncio.sleep(1)  # Allow time for server to start up.

    async def stop(self):
        """Stop the FastAPI server and wait for a graceful shutdown."""
        if self._uvicorn_server.started:
            logger.info("Stopping server...")
            self._uvicorn_server.should_exit = True
            await asyncio.sleep(1)  # Allow time for graceful shutdown.
            logger.info("Server stopped.")

    async def run_forever(self):
        """Run the server indefinitely until `stop()` is invoked."""
        await self._uvicorn_server.serve()

    async def queue_task(
        self,
        sample: Any,
        mode: Literal["train", "val", "test"] | None = None,
        resources_id: str | None = None,
        metadata: Dict[str, Any] | None = None,
    ) -> str:
        """Add a task to the queue for a client to process."""
        if not self._store:
            raise RuntimeError("Store not initialized. The server may not be running.")
        return await self._store.add_task(sample, mode=mode, resources_id=resources_id, metadata=metadata)

    async def update_resources(self, resources: NamedResources) -> str:
        """Publish a new resource bundle and return its generated identifier."""
        if not self._store:
            raise RuntimeError("Store not initialized. The server may not be running.")
        resources_id = f"res-{uuid.uuid4()}"
        update = ResourcesUpdate(
            resources_id=resources_id, resources=resources, create_time=time.time(), update_time=time.time(), version=1
        )
        await self._store.update_resources(update)
        return resources_id

    async def get_completed_rollout(self, rollout_id: str) -> Optional[RolloutLegacy]:
        """Retrieve a specific completed rollout by identifier."""
        if not self._store:
            raise RuntimeError("Store not initialized. The server may not be running.")
        return await self._store.retrieve_rollout(rollout_id)

    async def poll_completed_rollout(self, rollout_id: str, timeout: Optional[float] = None) -> Optional[RolloutLegacy]:
        """Poll for a completed rollout until it becomes available or a timeout expires.

        Args:
            rollout_id: Identifier of the rollout to wait for.
            timeout: Maximum number of seconds to wait. ``None`` waits indefinitely.

        Returns:
            Retrieved rollout, or ``None`` when the timeout is reached without success.
        """
        start_time = time.time()
        while True:
            rollout = await self.get_completed_rollout(rollout_id)
            if rollout:
                return rollout
            if timeout and (time.time() - start_time) >= timeout:
                return None
            await asyncio.sleep(1)

    async def retrieve_completed_rollouts(self) -> List[RolloutLegacy]:
        """Return every completed rollout and clear the internal buffer."""
        if not self._store:
            raise RuntimeError("Store not initialized. The server may not be running.")
        return await self._store.retrieve_completed_rollouts()


--- agentlightning/runner/agent.py ---
# Copyright (c) Microsoft. All rights reserved.

"""Agent runner implementation for executing agent rollouts.

This module provides the concrete implementation of the runner interface,
handling the execution of agent rollouts with support for tracing, hooks,
and distributed worker coordination.
"""

from __future__ import annotations

import asyncio
import logging
import random
import threading
import time
from contextlib import suppress
from typing import (
    TYPE_CHECKING,
    Any,
    Awaitable,
    Callable,
    List,
    Literal,
    Optional,
    Sequence,
    TypeVar,
    cast,
)

from opentelemetry.sdk.trace import ReadableSpan

from agentlightning.litagent import LitAgent
from agentlightning.reward import emit_reward, find_final_reward
from agentlightning.store.base import LightningStore
from agentlightning.tracer.base import Tracer
from agentlightning.tracer.otel import OtelTracer
from agentlightning.types import (
    AttemptedRollout,
    Hook,
    NamedResources,
    Rollout,
    RolloutMode,
    RolloutRawResult,
    Span,
    SpanCoreFields,
)
from agentlightning.utils.system_snapshot import system_snapshot

if TYPE_CHECKING:
    from agentlightning.execution.events import ExecutionEvent

from .base import Runner

T_task = TypeVar("T_task")

logger = logging.getLogger(__name__)


class LitAgentRunner(Runner[T_task]):
    """Execute [`LitAgent`][agentlightning.LitAgent] tasks with tracing support.

    This runner manages the complete lifecycle of agent rollout execution,
    including task polling, resource management, tracing, and hooks. It supports
    both continuous iteration over tasks from the store and single-step execution.

    Attributes:
        worker_id: Identifier for the active worker process, if any.
    """

    def __init__(
        self,
        tracer: Tracer,
        max_rollouts: Optional[int] = None,
        poll_interval: float = 5.0,
        heartbeat_interval: float = 10.0,
        interval_jitter: float = 0.5,
        heartbeat_launch_mode: Literal["asyncio", "thread"] = "thread",
        heartbeat_include_gpu: bool = False,
    ) -> None:
        """Initialize the agent runner.

        Args:
            tracer: [`Tracer`][agentlightning.Tracer] used for rollout spans.
            max_rollouts: Optional cap on iterations processed by
                [`iter`][agentlightning.LitAgentRunner.iter].
            poll_interval: Seconds to wait between store polls when no work is available.
            heartbeat_interval: Seconds to wait between sending heartbeats to the store.
            interval_jitter: Jitter factor for the poll interval. The actual interval will be between
                poll_interval - interval_jitter and poll_interval + interval_jitter.
                This is to avoid the overload caused by the synchronization of the runners.
            heartbeat_launch_mode: Launch mode for the heartbeat loop. Can be "asyncio" or "thread".
                "thread" is the default and recommended mode as it prevents blocking the event loop
                under load. Use "asyncio" for simpler deployments with low worker counts.
            heartbeat_include_gpu: Whether to include GPU stats in heartbeat snapshots.
                Querying GPU stats can be slow under load, so this is disabled by default.
        """
        super().__init__()
        self._tracer = tracer
        self._max_rollouts = max_rollouts
        self._poll_interval = poll_interval
        self._heartbeat_interval = heartbeat_interval
        self._interval_jitter = interval_jitter
        self._heartbeat_launch_mode = heartbeat_launch_mode
        self._heartbeat_include_gpu = heartbeat_include_gpu
        self._random_state = random.Random()

        # Set later
        self._agent: Optional[LitAgent[T_task]] = None
        self._hooks: Sequence[Hook] = []
        self._store: Optional[LightningStore] = None
        self.worker_id: Optional[int] = None

    def init(self, agent: LitAgent[T_task], *, hooks: Optional[Sequence[Hook]] = None, **kwargs: Any) -> None:
        """Initialize the runner with the agent.

        This sets up the agent-runner relationship, registers hooks, and
        initializes the tracer.

        Args:
            agent: [`LitAgent`][agentlightning.LitAgent] instance executed by the runner.
            hooks: Optional sequence of [`Hook`][agentlightning.Hook]
                callbacks invoked around tracing and rollout boundaries.
            **kwargs: Additional initialization arguments (currently unused).
        """
        self._agent = agent
        self._agent.set_runner(self)
        self._hooks = [*hooks] if hooks is not None else []

        self._tracer.init()

    def init_worker(self, worker_id: int, store: LightningStore, **kwargs: Any) -> None:
        """Initialize the runner for each worker with worker_id and store.

        This method is called once per worker in a distributed setup to provide
        the worker with its ID and store connection.

        Args:
            worker_id: Unique identifier for this worker process.
            store: [`LightningStore`][agentlightning.LightningStore]
                used for task coordination and persistence.
            **kwargs: Additional worker-specific initialization arguments (currently unused).
        """
        self._store = store
        self.worker_id = worker_id

        self._tracer.init_worker(worker_id, store)

    def teardown(self, *args: Any, **kwargs: Any) -> None:
        """Teardown the runner and clean up all resources.

        This method resets all internal state including the agent, store,
        hooks, and worker ID, and calls the tracer's teardown method.

        Args:
            *args: Additional teardown arguments (currently unused).
            **kwargs: Additional teardown keyword arguments (currently unused).
        """
        self._agent = None
        self._store = None
        self.worker_id = None
        self._hooks = []

        self._tracer.teardown()

    def teardown_worker(self, worker_id: int, *args: Any, **kwargs: Any) -> None:
        """Teardown the runner for a specific worker.

        This method cleans up worker-specific resources and resets the worker ID.

        Args:
            worker_id: Unique identifier of the worker being torn down.
            *args: Additional teardown arguments (currently unused).
            **kwargs: Additional teardown keyword arguments (currently unused).
        """
        self.worker_id = None

        self._tracer.teardown_worker(worker_id)

    @property
    def tracer(self) -> Tracer:
        """Get the tracer instance.

        Returns:
            The Tracer instance used by this runner.
        """
        return self._tracer

    def get_agent(self) -> LitAgent[T_task]:
        """Get the agent instance.

        Returns:
            The LitAgent instance managed by this runner.

        Raises:
            ValueError: If the agent has not been initialized via [`init`][agentlightning.LitAgentRunner.init].
        """
        if self._agent is None:
            raise ValueError("Agent not initialized. Call init() first.")
        return self._agent

    def get_store(self) -> LightningStore:
        """Get the store instance.

        Returns:
            The LightningStore instance for this worker.

        Raises:
            ValueError: If the store has not been initialized via [`init_worker`][agentlightning.LitAgentRunner.init_worker].
        """
        if self._store is None:
            raise ValueError("Store not initialized. Call init_worker() first.")
        return self._store

    def get_worker_id(self) -> str:
        """Get the formatted worker ID string.

        Returns:
            A formatted string like "Worker-0" if initialized, or "Worker-Unknown"
            if the worker ID has not been set.
        """
        return f"Worker-{self.worker_id}" if self.worker_id is not None else "Worker-Unknown"

    def _log_prefix(self, rollout_id: Optional[str] = None) -> str:
        """Generate a standardized log prefix for the current worker.

        This creates a consistent prefix format for log messages to identify
        which worker and rollout the message is associated with.

        Args:
            rollout_id: Optional rollout ID to include in the prefix.

        Returns:
            A formatted log prefix string like "[Worker 0 | Rollout xyz]",
            "[Worker 0]", "[Rollout xyz]", or "[Default Worker]".
        """
        if self.worker_id is not None:
            if rollout_id:
                return f"[Worker {self.worker_id} | Rollout {rollout_id}]"
            else:
                return f"[Worker {self.worker_id}]"
        if rollout_id:
            return f"[Rollout {rollout_id}]"
        return "[Default Worker]"

    async def _trigger_hooks(
        self,
        hook_type: Literal["on_trace_start", "on_trace_end", "on_rollout_start", "on_rollout_end"],
        *args: Any,
        **kwargs: Any,
    ) -> None:
        """Trigger all registered hooks of a specific type.

        This method calls the specified hook method on all registered hooks,
        catching and logging any exceptions that occur during hook execution
        to prevent them from disrupting the main execution flow.

        Args:
            hook_type: The type of hook to trigger. Valid values are:
                "on_trace_start", "on_trace_end", "on_rollout_start", "on_rollout_end".
            *args: Positional arguments to pass to the hook methods.
            **kwargs: Keyword arguments to pass to the hook methods.
        """
        for hook in self._hooks:
            try:
                await getattr(hook, hook_type)(*args, **kwargs)
            except Exception:
                logger.exception(f"{self._log_prefix()} Exception during {hook_type} hook {hook}.")

    async def _post_process_rollout_result(
        self, rollout: AttemptedRollout, raw_result: RolloutRawResult
    ) -> List[ReadableSpan] | List[Span]:
        """Standardizes the agent's return value and report what's needed to report to the store.

        Args:
            rollout: The rollout object for the current task.
            raw_result: The output from the agent's rollout method.

        Returns:
            The spans that are assumed to be added to the store.
            This only serves as an estimation for logging purposes. For precise tracking, use the store directly.
        """
        store = self.get_store()

        trace_spans: list[Span] = []
        result_recognized: bool = False

        # Case 0: result is None
        if raw_result is None:
            trace_spans = self._tracer.get_last_trace()
            result_recognized = True

        # Case 1: result is a float (final reward)
        if isinstance(raw_result, (bool, int, float)):
            if isinstance(raw_result, (bool, int)):
                logger.warning(
                    f"{self._log_prefix(rollout.rollout_id)} Reward is not a number, got: {type(raw_result)}. "
                    "Auto converting to float."
                )
                raw_result = float(raw_result)
            # Preserve the existing spans before another span is emitted
            trace_spans = list(self._tracer.get_last_trace())
            # This will NOT emit another span to the tracer
            reward_span_core_fields = emit_reward(raw_result, propagate=False)
            # We add it to the store manually
            sequence_id = await store.get_next_span_sequence_id(rollout.rollout_id, rollout.attempt.attempt_id)
            reward_span = Span.from_core_fields(
                reward_span_core_fields,
                rollout_id=rollout.rollout_id,
                attempt_id=rollout.attempt.attempt_id,
                sequence_id=sequence_id,
            )
            await store.add_span(reward_span)
            result_recognized = True

        # Case 2-4: result is a list
        if isinstance(raw_result, list):
            # Case 2: result is a list of ReadableSpan (OpenTelemetry spans)
            if len(raw_result) > 0 and all(isinstance(t, ReadableSpan) for t in raw_result):
                if isinstance(self._tracer, OtelTracer):
                    logger.warning(
                        f"{self._log_prefix(rollout.rollout_id)} Tracer is already an OpenTelemetry tracer. "
                        "The traces should have already been added to the store. "
                        "Returning the traces from the rollout will result in duplicate spans."
                    )
                for span in raw_result:
                    added_span = await store.add_otel_span(
                        rollout.rollout_id, rollout.attempt.attempt_id, cast(ReadableSpan, span)
                    )
                    if added_span is not None:
                        trace_spans.append(added_span)
                    else:
                        logger.error(
                            f"{self._log_prefix(rollout.rollout_id)} Failed to add OpenTelemetry span to the store: {span}"
                        )
                result_recognized = True

            # Case 3: result is a list of Span (agentlightning spans)
            elif len(raw_result) > 0 and all(isinstance(t, Span) for t in raw_result):
                # Add the spans directly to the store
                for span in raw_result:
                    await store.add_span(cast(Span, span))
                trace_spans = [cast(Span, span) for span in raw_result]
                result_recognized = True

            # Case 4: result is a list of SpanCoreFields (agentlightning spans)
            elif len(raw_result) > 0 and all(isinstance(t, SpanCoreFields) for t in raw_result):
                # Add the spans directly to the store too, but needs to get sequence id first
                sequence_ids = await store.get_many_span_sequence_ids(
                    [(rollout.rollout_id, rollout.attempt.attempt_id) for _ in range(len(raw_result))]
                )
                trace_spans = [
                    Span.from_core_fields(
                        cast(SpanCoreFields, span_core_fields),
                        rollout_id=rollout.rollout_id,
                        attempt_id=rollout.attempt.attempt_id,
                        sequence_id=sequence_id,
                    )
                    for span_core_fields, sequence_id in zip(raw_result, sequence_ids, strict=True)
                ]
                await store.add_many_spans(trace_spans)
                result_recognized = True

            # Left over cases for list
            elif len(raw_result) == 0:
                logger.warning(
                    f"{self._log_prefix(rollout.rollout_id)} The rollout returns an empty list. "
                    "Please check your rollout implementation."
                )
                trace_spans = []
                result_recognized = True

            else:
                types = [type(t).__name__ for t in raw_result][:10]
                raise ValueError(
                    f"Invalid raw result type. It's expected to be a list of ReadableSpan or Span, "
                    f"but got: {', '.join(types)}..."
                )

        if not result_recognized:
            raise TypeError(
                f"Invalid raw result type. It's expected to be none, float, or a list of ReadableSpan or Span, "
                f"but got: {type(raw_result).__name__}..."
            )

        return trace_spans

    async def _emit_heartbeat(self, store: LightningStore) -> None:
        """Send a heartbeat tick to the store.

        Args:
            store: The lightning store to update.
        """
        logger.debug(f"{self._log_prefix()} Preparing to emit heartbeat.")
        worker_id = self.get_worker_id()

        try:
            snapshot = await asyncio.wait_for(
                asyncio.to_thread(system_snapshot, self._heartbeat_include_gpu),
                timeout=self._heartbeat_interval,
            )
            logger.debug(f"{self._log_prefix()} Heartbeat snapshot acquired.")
        except asyncio.TimeoutError:
            logger.warning(
                "%s Heartbeat snapshot acquisition timed out after %.1fs, skipping.",
                self._log_prefix(),
                self._heartbeat_interval,
            )
            return
        except asyncio.CancelledError:
            # bypass the exception
            raise
        except Exception:
            logger.exception("%s Unable to acquire heartbeat snapshot.", self._log_prefix())
            return

        try:
            await asyncio.wait_for(store.update_worker(worker_id, snapshot), timeout=self._heartbeat_interval)
            logger.debug(f"{self._log_prefix()} Heartbeat updated successfully.")
        except asyncio.CancelledError:
            # bypass the exception
            raise
        except asyncio.TimeoutError:
            logger.warning(
                "%s update worker heartbeat timed out after %.1fs, skipping.",
                self._log_prefix(),
                self._heartbeat_interval,
            )
        except Exception:
            logger.exception("%s Unable to update worker heartbeat.", self._log_prefix())

    def _start_heartbeat_loop(self, store: LightningStore) -> Optional[Callable[[], Awaitable[None]]]:
        """Start a background heartbeat loop and return an async stopper."""

        if self._heartbeat_interval <= 0:
            return None

        if self.worker_id is None:
            logger.warning("%s Cannot start heartbeat loop without worker_id.", self._log_prefix())
            return None

        if self._heartbeat_launch_mode == "asyncio":
            return self._start_heartbeat_asyncio_loop(store)
        if self._heartbeat_launch_mode == "thread":
            return self._start_heartbeat_thread_loop(store)
        raise ValueError(f"Unsupported heartbeat launch mode: {self._heartbeat_launch_mode}")

    def _start_heartbeat_asyncio_loop(self, store: LightningStore) -> Optional[Callable[[], Awaitable[None]]]:
        """Start a background heartbeat loop using asyncio.

        Args:
            store: The lightning store to update.

        Returns:
            An async stopper function that can be used to stop the heartbeat loop.
        """

        stop_event = asyncio.Event()

        async def heartbeat_loop() -> None:
            while not stop_event.is_set():
                try:
                    # Run _emit_heartbeat in thread pool to avoid blocking the event loop.
                    # Timeout at the interval - if it takes longer, the data is stale anyway.
                    await self._emit_heartbeat(store)
                except Exception:
                    logger.exception("%s Heartbeat failed.", self._log_prefix())
                with suppress(asyncio.TimeoutError):
                    interval = self._heartbeat_interval + self._random_state.uniform(
                        -self._interval_jitter, self._interval_jitter
                    )
                    interval = max(interval, 0.01)
                    await asyncio.wait_for(stop_event.wait(), timeout=interval)

        task = asyncio.create_task(heartbeat_loop(), name=f"{self.get_worker_id()}-heartbeat")

        async def stop() -> None:
            stop_event.set()
            with suppress(asyncio.CancelledError):
                await task

        return stop

    def _start_heartbeat_thread_loop(self, store: LightningStore) -> Optional[Callable[[], Awaitable[None]]]:
        """Start a background heartbeat loop using threading.

        It uses two threads: one to produce the snapshot and one to consume it,
        to avoid either of them blocking the event loop.

        Args:
            store: The lightning store to update.

        Returns:
            An async stopper function that can be used to stop the heartbeat loop.
        """
        stop_evt = threading.Event()
        lock = threading.Lock()

        latest_snapshot = None
        latest_ts = 0.0  # time.monotonic() when snapshot was captured

        # Consider snapshot stale after ~1 interval plus jitter slack.
        stale_after = self._heartbeat_interval + self._interval_jitter + 1.0

        worker_id = self.get_worker_id()

        def producer() -> None:
            nonlocal latest_snapshot, latest_ts
            while not stop_evt.is_set():
                try:
                    logger.debug(f"{self._log_prefix()} Heartbeat producer: acquiring snapshot.")
                    snap = system_snapshot(self._heartbeat_include_gpu)  # sync
                    logger.debug(f"{self._log_prefix()} Heartbeat producer: snapshot acquired.")
                    ts = time.monotonic()
                    with lock:
                        latest_snapshot = snap
                        latest_ts = ts
                except Exception:
                    logger.warning("%s Heartbeat producer: system_snapshot failed.", self._log_prefix(), exc_info=True)

                interval = self._heartbeat_interval + self._random_state.uniform(
                    -self._interval_jitter, self._interval_jitter
                )
                stop_evt.wait(max(interval, 0.01))

        def consumer() -> None:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            last_warned_ts = None  # Track which snapshot we've already warned about
            try:
                while not stop_evt.is_set():
                    with lock:
                        snap = latest_snapshot
                        ts = latest_ts

                    wait_interval = max(
                        self._heartbeat_interval
                        + self._random_state.uniform(-self._interval_jitter, self._interval_jitter),
                        0.01,
                    )

                    if snap is None:
                        # probably just started
                        logger.debug("%s Heartbeat consumer: no snapshot yet; skipping update.", self._log_prefix())
                        stop_evt.wait(wait_interval)
                        continue

                    age = time.monotonic() - ts
                    if age > stale_after:
                        # Only warn once per stale snapshot (check if we haven't warned about this timestamp yet)
                        if last_warned_ts != ts:
                            logger.warning(
                                "%s Heartbeat consumer: snapshot stale (age=%.2fs > %.2fs); skipping update.",
                                self._log_prefix(),
                                age,
                                stale_after,
                            )
                            last_warned_ts = ts
                        stop_evt.wait(wait_interval)
                        continue

                    try:
                        logger.debug(f"{self._log_prefix()} Heartbeat consumer: updating worker.")
                        loop.run_until_complete(
                            asyncio.wait_for(
                                store.update_worker(worker_id, snap),
                                timeout=self._heartbeat_interval,
                            )
                        )
                        logger.debug(f"{self._log_prefix()} Heartbeat consumer: worker updated.")
                    except asyncio.TimeoutError:
                        logger.warning(
                            "%s Heartbeat consumer: update timed out after %.1fs.",
                            self._log_prefix(),
                            self._heartbeat_interval,
                        )
                    except Exception:
                        logger.warning("%s Heartbeat consumer: update failed.", self._log_prefix(), exc_info=True)

                    stop_evt.wait(wait_interval)
            finally:
                with suppress(Exception):
                    loop.stop()
                with suppress(Exception):
                    loop.close()

        t_prod = threading.Thread(target=producer, name=f"{worker_id}-heartbeat-producer", daemon=True)
        t_cons = threading.Thread(target=consumer, name=f"{worker_id}-heartbeat-consumer", daemon=True)
        t_prod.start()
        t_cons.start()

        async def stop() -> None:
            stop_evt.set()
            await asyncio.to_thread(t_prod.join)
            await asyncio.to_thread(t_cons.join)

        return stop

    async def _sleep_until_next_poll(self, event: Optional[ExecutionEvent] = None) -> None:
        """Sleep until the next poll interval, with optional event-based interruption.

        If an event is provided, the method will check it periodically (every 0.1s)
        and return early if the event is set.

        Args:
            event: Optional [`ExecutionEvent`][agentlightning.ExecutionEvent] object that can be used to interrupt the sleep.
                If set during the sleep period, the method returns immediately.
        """
        interval = self._poll_interval + self._random_state.uniform(-self._interval_jitter, self._interval_jitter)
        interval = max(interval, 0.01)
        if event is None:
            await asyncio.sleep(interval)
            return
        current_time = time.time()
        next_time = current_time + interval
        while time.time() < next_time:
            await asyncio.sleep(0.1)
            if event.is_set():
                return

    async def _step_impl(self, next_rollout: AttemptedRollout, raise_on_exception: bool = False) -> str:
        """Execute a single rollout implementation.

        This is the core method that handles the execution of a single rollout,
        including resource fetching, hook triggering, agent invocation, tracing,
        and result processing.

        Args:
            next_rollout: The rollout to execute, containing input data, mode,
                and resources information.
            raise_on_exception: If True, exceptions during rollout execution will
                be re-raised. If False, exceptions are logged but not propagated.
        """
        store = self.get_store()
        agent = self.get_agent()

        rollout_id = next_rollout.rollout_id

        resources_id = next_rollout.resources_id
        resources_update = None
        if resources_id:
            resources_update = await store.get_resources_by_id(resources_id)
        else:
            logger.debug(f"{self._log_prefix(rollout_id)} No 'resources_id'. Fetching latest resources.")
            resources_update = await store.get_latest_resources()
        if not resources_update:
            if raise_on_exception:
                raise RuntimeError(f"{self._log_prefix(rollout_id)} Failed to fetch resources")
            else:
                logger.error(f"{self._log_prefix(rollout_id)} Failed to fetch resources. Skipping.")
                return rollout_id

        logger.debug(f"{self._log_prefix(rollout_id)} Resources fetched (id={resources_update.resources_id}).")

        trace_spans: List[ReadableSpan] | List[Span] = []
        has_exception: bool = False

        try:
            await self._trigger_hooks(hook_type="on_rollout_start", agent=agent, runner=self, rollout=next_rollout)

            start_time = time.time()
            logger.debug(f"{self._log_prefix(rollout_id)} Prepared for trace context.")
            async with self._tracer.trace_context(
                name=rollout_id, rollout_id=rollout_id, attempt_id=next_rollout.attempt.attempt_id
            ):
                logger.debug(f"{self._log_prefix(rollout_id)} Entered trace context.")
                await self._trigger_hooks(
                    hook_type="on_trace_start", agent=agent, runner=self, tracer=self._tracer, rollout=next_rollout
                )

                # NOTE: This is the most costly step in the whole function
                # If the rollout method becomes unresponsive or timeouts, there is nothing we can do within the runner.
                # We might need some mechanisms in execution strategy to restart the runner. But that's a future work.
                if agent.is_async():
                    rollout_method = (
                        agent.training_rollout_async if next_rollout.mode == "train" else agent.validation_rollout_async
                    )
                    logger.debug(f"{self._log_prefix(rollout_id)} Starting async rollout method.")
                    result = await rollout_method(
                        next_rollout.input, resources=resources_update.resources, rollout=next_rollout
                    )
                    logger.debug(f"{self._log_prefix(rollout_id)} Async rollout method completed.")
                else:
                    rollout_method = (
                        agent.training_rollout if next_rollout.mode == "train" else agent.validation_rollout
                    )
                    logger.debug(f"{self._log_prefix(rollout_id)} Starting sync rollout method.")
                    result = rollout_method(
                        next_rollout.input, resources=resources_update.resources, rollout=next_rollout
                    )
                    logger.debug(f"{self._log_prefix(rollout_id)} Sync rollout method completed.")

                await self._trigger_hooks(
                    hook_type="on_trace_end", agent=agent, runner=self, tracer=self._tracer, rollout=next_rollout
                )

            logger.debug(f"{self._log_prefix(rollout_id)} Trace context exited.")

            # Possible exceptions in post_process will be caught in the overall exception handler
            trace_spans = await self._post_process_rollout_result(next_rollout, result)
            last_reward = find_final_reward(trace_spans)

            end_time = time.time()
            logger.info(
                f"{self._log_prefix(rollout_id)} Completed in "
                f"{end_time - start_time:.2f}s. Collected {len(trace_spans)} span(s). "
                f"Final reward: {last_reward}"
            )

        except Exception:
            logger.exception(f"{self._log_prefix(rollout_id)} Exception during rollout.")
            has_exception = True

            if raise_on_exception:
                raise
        finally:
            try:
                await self._trigger_hooks(
                    hook_type="on_rollout_end", agent=agent, runner=self, rollout=next_rollout, spans=trace_spans
                )
            except Exception:
                logger.exception(f"{self._log_prefix(rollout_id)} Exception during on_rollout_end hook.")

            try:
                if has_exception:
                    # possibly timed out and cancelled?
                    await store.update_attempt(rollout_id, next_rollout.attempt.attempt_id, status="failed")
                else:
                    await store.update_attempt(rollout_id, next_rollout.attempt.attempt_id, status="succeeded")
            except Exception:
                logger.exception(
                    f"{self._log_prefix(rollout_id)} Exception during update_attempt. Giving up the update."
                )

        return rollout_id

    async def iter(self, *, event: Optional[ExecutionEvent] = None) -> None:
        """Run the runner, continuously iterating over tasks in the store.

        This method polls the store for new rollouts and executes them until:

        - The event is set (if provided)
        - The max_rollouts limit is reached (if configured)
        - No more tasks are available

        All exceptions during rollout execution are caught and logged but not
        propagated, allowing the runner to continue processing subsequent tasks.

        Args:
            event: Optional ExecutionEvent object to signal the runner to stop. The runner
                will check this event periodically and stop gracefully when set.
        """
        num_tasks_processed = 0
        logger.info(f"{self._log_prefix()} Started async rollouts (max: {self._max_rollouts or 'unlimited'}).")
        store = self.get_store()

        stop_heartbeat = self._start_heartbeat_loop(store)

        try:
            while not (event is not None and event.is_set()) and (
                self._max_rollouts is None or num_tasks_processed < self._max_rollouts
            ):
                # Retrieve the next rollout
                next_rollout: Optional[Rollout] = None
                while not (event is not None and event.is_set()):
                    logger.debug(f"{self._log_prefix()} Try to poll for next rollout.")
                    next_rollout = await store.dequeue_rollout(worker_id=self.get_worker_id())
                    logger.debug(f"{self._log_prefix()} Next rollout retrieved: {next_rollout}")
                    if next_rollout is None:
                        logger.debug(
                            f"{self._log_prefix()} No rollout to poll. Waiting for {self._poll_interval} seconds."
                        )
                        await self._sleep_until_next_poll(event)
                    else:
                        break

                if next_rollout is None:
                    return

                # Execute the step
                await self._step_impl(next_rollout)

                num_tasks_processed += 1
                if num_tasks_processed % 10 == 0 or num_tasks_processed == 1:
                    logger.info(
                        f"{self._log_prefix()} Progress: {num_tasks_processed}/{self._max_rollouts or 'unlimited'}"
                    )
        finally:
            if stop_heartbeat is not None:
                await stop_heartbeat()

        logger.info(f"{self._log_prefix()} Finished async rollouts. Processed {num_tasks_processed} tasks.")

    async def step(
        self,
        input: T_task,
        *,
        resources: Optional[NamedResources] = None,
        mode: Optional[RolloutMode] = None,
        event: Optional[ExecutionEvent] = None,
    ) -> Rollout:
        """Execute a single task directly, bypassing the task queue.

        This method creates a new rollout for the given input and executes it
        immediately. Unlike [`iter()`][agentlightning.LitAgentRunner.iter],
        exceptions are propagated to the caller.

        Args:
            input: The task input to be processed by the agent.
            resources: Optional named resources to be used for this specific task.
                If provided, a new resources entry will be created in the store.
                If not provided, the latest resources from the store will be used.
            mode: Optional rollout mode ("train" or "validation"). If not provided,
                the agent's default mode will be used.
            event: Optional ExecutionEvent object to signal interruption (currently unused
                but included for interface consistency).

        Returns:
            The completed rollout.

        Raises:
            Exception: Any exception that occurs during rollout execution will be
                re-raised to the caller.
        """
        store = self.get_store()

        if resources is not None:
            resources_update = await store.add_resources(resources)
            resources_id = resources_update.resources_id
        else:
            resources_id = None

        attempted_rollout = await self.get_store().start_rollout(
            input=input, mode=mode, resources_id=resources_id, worker_id=self.get_worker_id()
        )
        rollout_id = await self._step_impl(attempted_rollout, raise_on_exception=True)

        completed_rollout = await store.get_rollout_by_id(rollout_id)
        if completed_rollout is None:
            raise RuntimeError(f"{self._log_prefix()} Failed to fetch completed rollout by id after step: {rollout_id}")
        return completed_rollout


--- contrib/README.md ---
# Contrib Area

This tree hosts experimental integrations, third-party recipes, and curated recipes that are not ready for the main `agentlightning/`, `examples/`, or `docs/` trees. Treat it as an incubator: keep contributions self-contained, clearly owned, and reproducible so downstream users can vendor them without guesswork.

## When to add something here

- You are iterating on a runtime extension that would bloat the primary `agentlightning/` namespace.
- You want to share a recipe that assembles existing components for a focused agent training or optimization workflow and needs more context than the main examples directory allows.
- You need automation scripts or download helpers that will help the community but should not live under `scripts/` at the repo root.

If a contribution starts depending on core release cadence, tight CI guarantees, or repo-wide infrastructure, talk to maintainers about graduating it out of `contrib/`.

## Directory map

- `agentlightning/` — Namespace packages, utilities, and adapters that extend the published wheel. Place new code under `agentlightning/contrib/<feature>/` so `import agentlightning.contrib.<feature>` works for downstream users.
- `recipes/` — Task-focused example bundles that solve a specific problem and derive certain results. Each recipe belongs in its own directory with a README that documents usage, result reports, and ownership.
- `scripts/` — Shared automation, dataset download steps, or reproducibility helpers that support the contrib modules above.

When adding folders, document the intent in a local README, link to companion docs or examples, and update `CODEOWNERS` so future fixes reach the right reviewers quickly.

Questions or proposals for new subtrees can be discussed in Discord, GitHub issues, or GitHub Discussions before opening a PR. For the canonical requirements and review checklist, see the “Agent-lightning Contrib” section of [`docs/community/contributing.md`](../docs/community/contributing.md).


## Links discovered
- [`docs/community/contributing.md`](https://github.com/microsoft/agent-lightning/blob/main/docs/community/contributing.md)

--- contrib/recipes/search_r1/README.md ---
# Search-R1 Example

## Overview

This example implements **Search R1** within Agent Lightning. It also serves as a demonstration of a **framework-free agent training pipeline**, showing how to run end-to-end RL training without relying on specialized frameworks. **It's tested and compatible with Agent-lightning v0.2.x**.

The example is designed to run on a single node with 8 GPUs, each having at least 40 GB of memory.

## Included Files

| File/Directory | Description |
|----------------|-------------|
| `data_process.sh` | Prepares the Wikipedia corpus, datasets, and `retriever` conda environment |
| `retrieval_launch.sh` | Launches the retrieval service backed by the processed corpus |
| `retrieval_server.py` | FastAPI server that powers document retrieval during training |
| `search_r1_agent.py` | Agent-Lightning rollout script implementing the Search-R1 workflow |
| `train_search_r1_agent.py` | RL training script that coordinates GRPO optimization |
| `qa_em.py` | Exact-match evaluation utilities for validating model predictions |

---

## Prepare Data and Environment

Run the following script once to prepare data and the retriever environment:

```bash
bash data_process.sh
```

This script performs the following steps:

* Creates a new conda environment named **`retriever`**.
* Downloads the **Wikipedia data** used to build the retrieval database.
* Downloads the **training and testing datasets**.
* Stores all data under the newly created **`data/`** directory.

The environment setup and data-processing logic are adapted from [PeterGriffinJin/Search-R1](https://github.com/PeterGriffinJin/Search-R1).

---

## Prepare Retrieval Server

To start the retrieval server, run:

```bash
bash retrieval_launch.sh
```

This script activates the previously created **`retriever`** environment and starts a **retrieval server** at `http://127.0.0.1:8000` using the downloaded Wikipedia data. The server receives user queries and returns a ranked list of retrieved text passages.

The retrieval server implementation is based on `search_r1/search/retrieval_server.py`](https://github.com/PeterGriffinJin/Search-R1/blob/main/search_r1/search/retrieval_server.py).

> ⚠️ **Note:** Keep the retrieval server running during training (for example, in a separate `tmux` session or terminal window).

---

## Run RL Training (GRPO) with Llama-3.2-3B-Instruct

1. **Start Ray**

   ```bash
   bash ../../scripts/restart_ray.sh
   ```

   > If you plan to use WandB for experiment tracking, set the environment variable
   > `WANDB_API_KEY` before starting Ray.

2. **Start the Training Server**
   In another terminal, run:

   ```bash
   python train_search_r1_agent.py llama
   ```

   This script starts the RL training. Each agent follows the Search-R1 workflow, retrieving information from the database and generating answers accordingly.

---

## Benchmark Results

We evaluated Search-R1 across seven diverse question-answering benchmarks, covering both General QA (NQ, TriviaQA, PopQA) and complex multi-hop reasoning tasks (HotpotQA, 2WikiMultiHopQA, Musique, and Bamboogle).

The following tables compare the performance of the original Search-R1 implementation and the Agent-Lightning version across various base models.

| Model | Source | NQ | TriviaQA | PopQA | HotpotQA | 2Wiki | Musique | Bamboogle |
| :--- | :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| **Qwen2.5-3B-Instruct** | **Search-R1 (Original)** | 34.1 | 54.5 | 37.8 | 32.4 | 31.9 | 10.3 | 26.4 |
| | **Agent-Lightning** | **45.3** | **61.7** | **43.8** | **42.6** | **36.4** | **17.1** | **37.6** |
| **Qwen2.5-7B-Instruct** | **Search-R1 (Original)** | 39.3 | 61.0 | 39.7 | 37.0 | 41.4 | 14.6 | 36.8 |
| | **Agent-Lightning** | **46.5** | **65.9** | **46.8** | **43.7** | **46.2** | **20.3** | **47.2** |
| **Llama-3.2-3B** | **Search-R1 (Reproduced)** | 26.3 | 49.0 | 23.0 | 21.6 | 27.3 | 4.5 | 9.7 |
| | **Agent-Lightning** | **29.6** | **51.9** | **25.7** | **23.2** | **28.3** | **5.8** | 9.6 |


## Links discovered
- [PeterGriffinJin/Search-R1](https://github.com/PeterGriffinJin/Search-R1)

--- contrib/agentlightning/contrib/__init__.py ---
# Copyright (c) Microsoft. All rights reserved.

# Namespace package for agentlightning.contrib.


--- contrib/recipes/search_r1/qa_em.py ---
# Copyright (c) Microsoft. All rights reserved.

# Copyright 2024 Bytedance Ltd. and/or its affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import random
import re
import string
from typing import Mapping, Optional, Sequence, Union


def normalize_answer(s: str) -> str:
    """Lowercase, remove punctuation/articles, and normalize whitespace."""

    def remove_articles(text: str) -> str:
        return re.sub(r"\b(a|an|the)\b", " ", text)

    def white_space_fix(text: str) -> str:
        return " ".join(text.split())

    def remove_punc(text: str) -> str:
        exclude = set(string.punctuation)
        return "".join(ch for ch in text if ch not in exclude)

    def lower(text: str) -> str:
        return text.lower()

    return white_space_fix(remove_articles(remove_punc(lower(s))))


def em_check(prediction: str, golden_answers: Union[str, Sequence[str]]) -> int:
    if isinstance(golden_answers, str):
        golden_answers = [golden_answers]
    normalized_prediction = normalize_answer(prediction)
    score = 0
    for golden_answer in golden_answers:
        golden_answer = normalize_answer(golden_answer)
        if golden_answer == normalized_prediction:
            score = 1
            break
    return score


def subem_check(prediction: str, golden_answers: Union[str, Sequence[str]]) -> int:
    if isinstance(golden_answers, str):
        golden_answers = [golden_answers]
    normalized_prediction = normalize_answer(prediction)
    score = 0
    for golden_answer in golden_answers:
        golden_answer = normalize_answer(golden_answer)
        if golden_answer in normalized_prediction:
            score = 1
            break
    return score


def extract_solution(solution_str: str) -> Optional[str]:
    """Extract the last <answer>...</answer> span from a solution string.

    Returns None if fewer than two such spans are present, to match original behavior.
    """
    answer_pattern = r"<answer>(.*?)</answer>"
    match_iter = re.finditer(answer_pattern, solution_str, re.DOTALL)
    matches = list(match_iter)

    # If there are 0 or exactly 1 matches, return None
    if len(matches) == 0:
        return None

    # If there are 2 or more matches, return the last one
    return matches[-1].group(1).strip()


def compute_score_em(
    solution_str: str,
    ground_truth: Union[str, Sequence[str]],
    method: str = "strict",
    format_score: float = 0.0,
    score: float = 1.0,
) -> float:
    """Scoring function for exact match (EM)."""
    answer = extract_solution(solution_str=solution_str)
    do_print = random.randint(1, 64) == 1

    if do_print:
        print(f"--------------------------------")
        print(f"Golden answers: {ground_truth}")
        print(f"Extracted answer: {answer}")
        print(f"Solution string: {solution_str}")

    if answer is None:
        return 0.0
    else:
        if em_check(answer, ground_truth):
            return score
        else:
            return format_score


def compute_score_subem(
    solution_str: str,
    ground_truth: Mapping[str, Union[str, Sequence[str]]],
    method: str = "strict",
    format_score: float = 0.0,
    score: float = 1.0,
) -> float:
    """Scoring function for substring exact match (EM)."""
    answer = extract_solution(solution_str=solution_str)
    do_print = random.randint(1, 64) == 1

    if do_print:
        print(f"--------------------------------")
        print(f"Golden answers: {ground_truth['target']}")
        print(f"Extracted answer: {answer}")
        print(f"Solution string: {solution_str}")

    if answer is None:
        return 0.0
    else:
        if subem_check(answer, ground_truth["target"]):
            return score
        else:
            return format_score


--- contrib/recipes/search_r1/retrieval_server.py ---
# Copyright (c) Microsoft. All rights reserved.

# Copied and adapted from https://github.com/PeterGriffinJin/Search-R1/blob/main/search_r1/search/retrieval_server.py

import argparse
import json
import warnings
from typing import (
    Any,
    Dict,
    List,
    Optional,
    Sequence,
    Tuple,
    Union,
)

import datasets
import faiss  # type: ignore[reportMissingTypeStubs]
import numpy as np
import torch
import uvicorn
from fastapi import FastAPI
from numpy.typing import NDArray
from pydantic import BaseModel
from tqdm import tqdm
from transformers import AutoConfig, AutoModel, AutoTokenizer

# ---- Small helpers / aliases
Doc = Dict[str, Any]
Docs = List[Doc]
BatchDocs = List[Docs]
Scores = List[float]
BatchScores = List[Scores]


def load_corpus(corpus_path: str) -> Any:
    corpus: Any = datasets.load_dataset("json", data_files=corpus_path, split="train", num_proc=4)  # type: ignore
    return corpus


def read_jsonl(file_path: str) -> List[Dict[str, Any]]:
    data: List[Dict[str, Any]] = []
    with open(file_path, "r") as f:
        for line in f:
            data.append(json.loads(line))
    return data


def load_docs(corpus: Any, doc_idxs: Sequence[int]) -> Docs:
    results: Docs = [corpus[int(idx)] for idx in doc_idxs]
    return results


def load_model(model_path: str, use_fp16: bool = False) -> Tuple[torch.nn.Module, Any]:
    # we call AutoConfig to ensure trust_remote_code init side-effects
    _model_config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)  # type: ignore
    model: torch.nn.Module = AutoModel.from_pretrained(model_path, trust_remote_code=True)  # type: ignore
    model.eval()  # type: ignore
    model.cuda()  # type: ignore
    if use_fp16:
        model = model.half()  # type: ignore
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True, trust_remote_code=True)  # type: ignore
    return model, tokenizer  # type: ignore


def pooling(
    pooler_output: torch.Tensor,
    last_hidden_state: torch.Tensor,
    attention_mask: Optional[torch.Tensor] = None,
    pooling_method: str = "mean",
) -> torch.Tensor:
    if pooling_method == "mean":
        assert attention_mask is not None, "attention_mask is required for mean pooling"
        last_hidden = last_hidden_state.masked_fill(~attention_mask[..., None].bool(), 0.0)
        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]
    elif pooling_method == "cls":
        return last_hidden_state[:, 0]
    elif pooling_method == "pooler":
        return pooler_output
    else:
        raise NotImplementedError("Pooling method not implemented!")


class Encoder:
    def __init__(
        self,
        model_name: str,
        model_path: str,
        pooling_method: str,
        max_length: int,
        use_fp16: bool,
    ) -> None:
        self.model_name = model_name
        self.model_path = model_path
        self.pooling_method = pooling_method
        self.max_length = max_length
        self.use_fp16 = use_fp16

        self.model, self.tokenizer = load_model(model_path=model_path, use_fp16=use_fp16)
        self.model.eval()

    @torch.no_grad()  # type: ignore
    def encode(self, query_list: Union[List[str], str], is_query: bool = True) -> NDArray[np.float32]:
        # processing query for different encoders
        if isinstance(query_list, str):
            query_list = [query_list]

        if "e5" in self.model_name.lower():
            if is_query:
                query_list = [f"query: {query}" for query in query_list]
            else:
                query_list = [f"passage: {query}" for query in query_list]

        if "bge" in self.model_name.lower():
            if is_query:
                query_list = [
                    f"Represent this sentence for searching relevant passages: {query}" for query in query_list
                ]

        inputs: Dict[str, torch.Tensor] = self.tokenizer(
            query_list, max_length=self.max_length, padding=True, truncation=True, return_tensors="pt"
        )  # type: ignore[call-arg]
        inputs = {k: v.cuda() for k, v in inputs.items()}

        if "T5" in type(self.model).__name__:
            # T5-based retrieval model
            decoder_input_ids = torch.zeros((inputs["input_ids"].shape[0], 1), dtype=torch.long).to(
                inputs["input_ids"].device
            )
            output = self.model(**inputs, decoder_input_ids=decoder_input_ids, return_dict=True)
            query_emb = output.last_hidden_state[:, 0, :]
        else:
            output = self.model(**inputs, return_dict=True)
            query_emb = pooling(
                output.pooler_output,
                output.last_hidden_state,
                inputs["attention_mask"],
                self.pooling_method,
            )
            if "dpr" not in self.model_name.lower():
                query_emb = torch.nn.functional.normalize(query_emb, dim=-1)

        query_np: NDArray[np.float32] = query_emb.detach().cpu().numpy().astype(np.float32, order="C")  # type: ignore

        # cleanup
        del inputs, output
        torch.cuda.empty_cache()

        return query_np


class Config:
    """
    Minimal config class (simulating your argparse)
    Replace this with your real arguments or load them dynamically.
    """

    def __init__(
        self,
        retrieval_method: str = "bm25",
        retrieval_topk: int = 10,
        index_path: str = "./index/bm25",
        corpus_path: str = "./data/corpus.jsonl",
        dataset_path: str = "./data",
        data_split: str = "train",
        faiss_gpu: bool = True,
        retrieval_model_path: str = "./model",
        retrieval_pooling_method: str = "mean",
        retrieval_query_max_length: int = 256,
        retrieval_use_fp16: bool = False,
        retrieval_batch_size: int = 128,
    ) -> None:
        self.retrieval_method = retrieval_method
        self.retrieval_topk = retrieval_topk
        self.index_path = index_path
        self.corpus_path = corpus_path
        self.dataset_path = dataset_path
        self.data_split = data_split
        self.faiss_gpu = faiss_gpu
        self.retrieval_model_path = retrieval_model_path
        self.retrieval_pooling_method = retrieval_pooling_method
        self.retrieval_query_max_length = retrieval_query_max_length
        self.retrieval_use_fp16 = retrieval_use_fp16
        self.retrieval_batch_size = retrieval_batch_size


class BaseRetriever:
    def __init__(self, config: Config) -> None:
        self.config = config
        self.retrieval_method: str = config.retrieval_method
        self.topk: int = config.retrieval_topk

        self.index_path: str = config.index_path
        self.corpus_path: str = config.corpus_path

    def _search(self, query: str, num: Optional[int], return_score: bool) -> Union[Docs, Tuple[Docs, Scores]]:
        raise NotImplementedError

    def _batch_search(
        self, query_list: List[str], num: Optional[int], return_score: bool
    ) -> Union[BatchDocs, Tuple[BatchDocs, BatchScores]]:
        raise NotImplementedError

    def search(
        self, query: str, num: Optional[int] = None, return_score: bool = False
    ) -> Union[Docs, Tuple[Docs, Scores]]:
        return self._search(query, num, return_score)

    def batch_search(
        self, query_list: List[str], num: Optional[int] = None, return_score: bool = False
    ) -> Union[BatchDocs, Tuple[BatchDocs, BatchScores]]:
        return self._batch_search(query_list, num, return_score)


class BM25Retriever(BaseRetriever):
    def __init__(self, config: Config) -> None:
        super().__init__(config)
        # import locally to avoid hard dependency at import/type time
        try:
            from pyserini.search.lucene import LuceneSearcher  # type: ignore
        except Exception:  # pragma: no cover - typing convenience
            LuceneSearcher = Any  # type: ignore[assignment]

        self.searcher: Any = LuceneSearcher(self.index_path)  # type: ignore
        self.contain_doc: bool = self._check_contain_doc()
        if not self.contain_doc:
            self.corpus: Any = load_corpus(self.corpus_path)
        self.max_process_num: int = 8

    def _check_contain_doc(self) -> bool:
        doc = self.searcher.doc(0)
        try:
            _ = doc.raw()
            return True
        except Exception:
            return False

    def _search(
        self, query: str, num: Optional[int] = None, return_score: bool = False
    ) -> Union[Docs, Tuple[Docs, Scores]]:
        k = self.topk if num is None else num
        hits: List[Any] = self.searcher.search(query, k)
        if len(hits) < 1:
            if return_score:
                return [], []
            else:
                return []
        scores: Scores = [float(hit.score) for hit in hits]
        if len(hits) < k:
            warnings.warn("Not enough documents retrieved!")
        else:
            hits = hits[:k]

        if self.contain_doc:
            all_contents: List[str] = [json.loads(self.searcher.doc(hit.docid).raw())["contents"] for hit in hits]
            results: Docs = [
                {
                    "title": content.split("\n")[0].strip('"'),
                    "text": "\n".join(content.split("\n")[1:]),
                    "contents": content,
                }
                for content in all_contents
            ]
        else:
            results = load_docs(self.corpus, [int(hit.docid) for hit in hits])

        if return_score:
            return results, scores
        else:
            return results

    def _batch_search(
        self, query_list: List[str], num: Optional[int] = None, return_score: bool = False
    ) -> Union[BatchDocs, Tuple[BatchDocs, BatchScores]]:
        results: BatchDocs = []
        scores: BatchScores = []
        for query in query_list:
            item_result, item_score = self._search(query, num, True)  # type: ignore[misc]
            results.append(item_result)  # type: ignore[arg-type]
            scores.append(item_score)  # type: ignore[arg-type]
        if return_score:
            return results, scores
        else:
            return results


class DenseRetriever(BaseRetriever):
    def __init__(self, config: Config) -> None:
        super().__init__(config)
        index: Any = faiss.read_index(self.index_path)  # type: ignore[no-untyped-call]
        if config.faiss_gpu:
            # Some faiss GPU helpers may be missing type stubs; treat as Any.
            co: Any = faiss.GpuMultipleClonerOptions()  # type: ignore[attr-defined]
            co.useFloat16 = True
            co.shard = True
            index = faiss.index_cpu_to_all_gpus(index, co=co)  # type: ignore[no-untyped-call]

        self.index: Any = index
        self.corpus: Any = load_corpus(self.corpus_path)
        self.encoder = Encoder(
            model_name=self.retrieval_method,
            model_path=config.retrieval_model_path,
            pooling_method=config.retrieval_pooling_method,
            max_length=config.retrieval_query_max_length,
            use_fp16=config.retrieval_use_fp16,
        )
        self.topk = config.retrieval_topk
        self.batch_size = config.retrieval_batch_size

    def _search(
        self, query: str, num: Optional[int] = None, return_score: bool = False
    ) -> Union[Docs, Tuple[Docs, Scores]]:
        k = self.topk if num is None else num
        query_emb = self.encoder.encode(query)
        scores_np, idxs_np = self.index.search(query_emb, k=k)  # type: ignore[no-untyped-call]
        idxs: Sequence[int] = list(map(int, idxs_np[0]))
        scores: Scores = [float(s) for s in scores_np[0]]
        results = load_docs(self.corpus, idxs)
        if return_score:
            return results, scores
        else:
            return results

    def _batch_search(
        self, query_list: List[str], num: Optional[int] = None, return_score: bool = False
    ) -> Union[BatchDocs, Tuple[BatchDocs, BatchScores]]:
        if isinstance(query_list, str):
            query_list = [query_list]
        k = self.topk if num is None else num

        results: BatchDocs = []
        scores: BatchScores = []
        for start_idx in tqdm(range(0, len(query_list), self.batch_size), desc="Retrieval process: "):
            query_batch = query_list[start_idx : start_idx + self.batch_size]
            batch_emb = self.encoder.encode(query_batch)
            batch_scores_np, batch_idxs_np = self.index.search(batch_emb, k=k)  # type: ignore[no-untyped-call]
            batch_scores = batch_scores_np.tolist()
            batch_idxs = batch_idxs_np.tolist()

            # load_docs is not vectorized, but is a python list approach
            flat_idxs: List[int] = sum(batch_idxs, [])  # type: ignore
            batch_results_flat = load_docs(self.corpus, flat_idxs)
            # chunk them back
            chunked: List[Docs] = [batch_results_flat[i * k : (i + 1) * k] for i in range(len(batch_idxs))]

            results.extend(chunked)
            scores.extend(batch_scores)

            del batch_emb, batch_scores, batch_idxs, query_batch, flat_idxs, batch_results_flat
            torch.cuda.empty_cache()

        if return_score:
            return results, scores
        else:
            return results


def get_retriever(config: Config) -> BaseRetriever:
    if config.retrieval_method == "bm25":
        return BM25Retriever(config)
    else:
        return DenseRetriever(config)


#####################################
# FastAPI server below
#####################################


class QueryRequest(BaseModel):
    queries: List[str]
    topk: Optional[int] = None
    return_scores: bool = False


app: FastAPI = FastAPI()

# Globals created under __main__; keep typed placeholders for pyright
config: Config  # will be set in __main__
retriever: BaseRetriever  # will be set in __main__


@app.post("/retrieve")
def retrieve_endpoint(request: QueryRequest) -> Dict[str, Any]:
    """
    Endpoint that accepts queries and performs retrieval.
    Input format:
    {
      "queries": ["What is Python?", "Tell me about neural networks."],
      "topk": 3,
      "return_scores": true
    }
    """
    if not request.topk:
        request.topk = config.retrieval_topk  # fallback to default

    # Perform batch retrieval
    search_out = retriever.batch_search(
        query_list=request.queries, num=int(request.topk), return_score=request.return_scores
    )

    # Unpack depending on return_scores
    if request.return_scores:
        results, scores = search_out  # type: ignore[misc]
    else:
        results = search_out  # type: ignore[assignment]
        scores = None

    # Format response
    resp: List[Any] = []
    for i, single_result in enumerate(results):  # type: ignore[arg-type]
        if request.return_scores and scores is not None:
            combined: List[Dict[str, Any]] = []
            for doc, score in zip(single_result, scores[i]):
                combined.append({"document": doc, "score": float(score)})
            resp.append(combined)
        else:
            resp.append(single_result)
    return {"result": resp}


if __name__ == "__main__":

    parser = argparse.ArgumentParser(description="Launch the local faiss retriever.")
    parser.add_argument(
        "--index_path", type=str, default="/home/peterjin/mnt/index/wiki-18/e5_Flat.index", help="Corpus indexing file."
    )
    parser.add_argument(
        "--corpus_path",
        type=str,
        default="/home/peterjin/mnt/data/retrieval-corpus/wiki-18.jsonl",
        help="Local corpus file.",
    )
    parser.add_argument("--topk", type=int, default=3, help="Number of retrieved passages for one query.")
    parser.add_argument("--retriever_name", type=str, default="e5", help="Name of the retriever model.")
    parser.add_argument(
        "--retriever_model", type=str, default="intfloat/e5-base-v2", help="Path of the retriever model."
    )
    parser.add_argument("--faiss_gpu", action="store_true", help="Use GPU for computation")

    args = parser.parse_args()

    # 1) Build a config (could also parse from arguments).
    #    In real usage, you'd parse your CLI arguments or environment variables.
    config = Config(
        retrieval_method=args.retriever_name,  # or "dense"
        index_path=args.index_path,
        corpus_path=args.corpus_path,
        retrieval_topk=int(args.topk),
        faiss_gpu=bool(args.faiss_gpu),
        retrieval_model_path=args.retriever_model,
        retrieval_pooling_method="mean",
        retrieval_query_max_length=256,
        retrieval_use_fp16=True,
        retrieval_batch_size=512,
    )

    # 2) Instantiate a global retriever so it is loaded once and reused.
    retriever = get_retriever(config)

    # 3) Launch the server. By default, it listens on http://127.0.0.1:8000
    uvicorn.run(app, host="0.0.0.0", port=8000)


--- contrib/recipes/search_r1/search_r1_agent.py ---
# Copyright (c) Microsoft. All rights reserved.

from __future__ import annotations

import os
import re
import time
from typing import Any, Dict, List, Optional, Tuple, TypedDict, cast

import pandas as pd
import requests
from openai import OpenAI
from qa_em import compute_score_em

from agentlightning import LLM, LitAgent, NamedResources, Rollout, Trainer, configure_logger, setup_logging

setup_logging()
logger = configure_logger(name=__name__)

# Copied and adapted from https://github.com/PeterGriffinJin/Search-R1/blob/main/scripts/data_process/nq_search.py
INSTRUCTION_FORMAT = """Answer the given question. You must conduct reasoning inside <think> and </think> first every time you get new information. After reasoning, if you find you lack some knowledge, you can call a search engine by <search> query </search> and it will return the top searched results between <information> and </information>. You can search as many times as your want. If you find no further external knowledge needed, you can directly provide the answer inside <answer> and </answer>, without detailed illustrations. For example, <answer> Beijing </answer>. Question: """


class Document(TypedDict):
    contents: str


class RetrievalItem(TypedDict):
    document: Document


def eval(prediction: str, ground_truth: List[str]) -> float:
    reward_score = float(compute_score_em(prediction, ground_truth))
    print(f"pred: {prediction} | {type(ground_truth)} gold_answer: {ground_truth} | res: {reward_score}")
    return reward_score


def postprocess_response(response: str) -> str:
    """Process responses to stop at search operation or answer operation."""
    if "</search>" in response:
        response = response.split("</search>")[0] + "</search>"
    elif "</answer>" in response:
        response = response.split("</answer>")[0] + "</answer>"
    return response


def extract_action(response: str) -> Tuple[Optional[str], str]:
    """Process (text-based) predictions from llm into actions and validity flags."""
    pattern = r"<(search|answer)>(.*?)</\1>"
    match = re.search(pattern, response, re.DOTALL)
    if match:
        content = match.group(2).strip()  # Return only the content inside the tags
        action: Optional[str] = match.group(1)
    else:
        content = ""
        action = None
    return action, content


def execute_response(response: str, do_search: bool = True) -> str:
    """
    Execute predictions across multiple environments.
    """
    action, content = extract_action(response)
    if action == "answer":
        return ""
    elif action == "search":
        search_result = retrieve_doc(content) if do_search else ""
        return f"\n\n<information>{search_result}</information>\n\n"
    else:
        return (
            "\nMy previous action is invalid. If I want to search, I should put the query between <search> and </search>. "
            "If I want to give the final answer, I should put the answer between <answer> and </answer>. Let me try again.\n"
        )


def retrieve_doc(query: str) -> str:
    payload: Dict[str, Any] = {"queries": [query], "topk": 3, "return_scores": True}
    response = requests.post("http://127.0.0.1:8000/retrieve", json=payload)
    response.raise_for_status()
    json_resp: Dict[str, Any] = cast(Dict[str, Any], response.json())
    retrieval_result: List[RetrievalItem] = cast(List[RetrievalItem], json_resp["result"][0])
    retrieval_result_str = passages2string(retrieval_result)
    return retrieval_result_str


def passages2string(retrieval_result: List[RetrievalItem]) -> str:
    format_reference = ""
    for idx, doc_item in enumerate(list(retrieval_result)):
        content = doc_item["document"]["contents"]
        title = content.split("\n")[0]
        text = "\n".join(content.split("\n")[1:])
        format_reference += f"Doc {idx+1}(Title: {title}) {text}\n"
    return format_reference


def call_llm(
    llm_client: OpenAI,
    model_name: str,
    content: str,
    temperature: float = 1.0,
    max_tokens: int = 500,
) -> str:
    response = llm_client.chat.completions.create(
        model=model_name,
        messages=[{"role": "user", "content": content}],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content or ""


class SearchR1Agent(LitAgent[Dict[str, Any]]):

    def __init__(
        self,
        val_temperature: Optional[float] = 0.0,
        max_turns: int = 4,
    ) -> None:
        super().__init__()
        self.val_temperature = val_temperature
        self.data_dir = os.environ.get("VERL_SEARCHR1_DATA_DIR", "data")
        self.max_turns = max_turns

    def rollout(
        self,
        task: Dict[str, Any],
        resources: NamedResources,
        rollout: Rollout,
    ) -> float | None:
        prompt = INSTRUCTION_FORMAT + task["question"]
        answer_list: List[str] = cast(List[str], task["golden_answers"])
        rollout_id = rollout.rollout_id
        logger.info(f"[Rollout {rollout_id}] Question: {task['question']}")
        logger.info(f"[Rollout {rollout_id}] Ground Truth: {answer_list}")

        start_time = time.time()
        llm: LLM = cast(LLM, resources["main_llm"])
        client = OpenAI(
            base_url=llm.get_base_url(rollout_id, rollout.attempt.attempt_id),  # type: ignore
            api_key=os.environ.get("OPENAI_API_KEY", "token-abc123"),
        )

        if rollout.mode == "train":
            temperature = llm.sampling_parameters.get("temperature", 1.0)
        else:
            temperature = self.val_temperature if self.val_temperature is not None else 0.0

        turn_id = 0
        finished_flag = False
        rollout_content: str = ""

        try:
            while turn_id < self.max_turns and not finished_flag:
                turn_id += 1
                turn_response = call_llm(
                    client, llm.model, prompt + rollout_content, temperature=temperature, max_tokens=500
                )
                valid_turn_response = postprocess_response(turn_response)
                rollout_content += valid_turn_response
                turn_env_feedback = execute_response(valid_turn_response)
                if len(turn_env_feedback) == 0:
                    finished_flag = True
                else:
                    rollout_content += turn_env_feedback
                logger.info(f"TURN ID {turn_id} | RESP: {turn_response} | ENV FEEDBACK: {turn_env_feedback}")

            if not finished_flag:
                turn_response = call_llm(
                    client, llm.model, prompt + rollout_content, temperature=temperature, max_tokens=500
                )
                rollout_content += turn_response
                logger.info(f"LAST TURN GENERATE | RESP: {turn_response}")

        except Exception as e:
            logger.exception(f"[Rollout {rollout_id}] Error during rollout: {e}")
            return None

        end_time_rollout = time.time()
        reward_score = eval(rollout_content, answer_list)
        logger.info("[Rollout %s] Reward: %s", rollout_id, reward_score)
        end_time_eval = time.time()

        logger.info("[Rollout %s] Time taken for rollout: %.2f seconds", rollout_id, end_time_rollout - start_time)
        logger.info(
            "[Rollout %s] Time taken for evaluation: %.2f seconds", rollout_id, end_time_eval - end_time_rollout
        )
        logger.info(
            "question: {} answer: {} ground_truth: {} reward: {}".format(
                task["question"], rollout_content, answer_list, reward_score
            )
        )
        return reward_score


def debug_search_r1_agent():
    searchr1_dev_data_path = os.path.join(os.environ.get("VERL_SEARCHR1_DATA_DIR", "data"), "test.parquet")
    if not os.path.exists(searchr1_dev_data_path):
        raise FileNotFoundError(f"Search_R1 dev data file {searchr1_dev_data_path} does not exist.")
    df = pd.read_parquet(searchr1_dev_data_path).head(10)  # type: ignore
    df = cast(List[Dict[str, Any]], df.to_dict(orient="records"))  # type: ignore
    print("Debug data:", df)

    trainer = Trainer(
        n_workers=1,
        initial_resources={
            "main_llm": LLM(
                endpoint=os.environ["OPENAI_API_BASE"],
                model="gpt-4.1-nano",
                sampling_parameters={"temperature": 0.0},
            )
        },
    )
    trainer.dev(SearchR1Agent(), df)


if __name__ == "__main__":
    debug_search_r1_agent()


--- contrib/recipes/search_r1/train_search_r1_agent.py ---
# Copyright (c) Microsoft. All rights reserved.


from __future__ import annotations

import argparse
import os
from copy import deepcopy
from datetime import datetime
from typing import Any, Dict

import pandas as pd
from search_r1_agent import SearchR1Agent

import agentlightning as agl

RL_TRAINING_CONFIG: Dict[str, Any] = {
    "algorithm": {
        "adv_estimator": "grpo",
        "use_kl_in_reward": False,
    },
    "data": {
        "train_files": "data/train.parquet",
        "val_files": "data/test.parquet",
        "train_batch_size": 512,
        "max_prompt_length": 6000,
        "max_response_length": 4096,
        "truncation": "error",
    },
    "actor_rollout_ref": {
        "rollout": {
            "tensor_model_parallel_size": 1,
            "n": 5,
            "log_prob_micro_batch_size_per_gpu": 4,
            "multi_turn": {"format": "hermes"},
            "name": "vllm",
            "gpu_memory_utilization": 0.5,
            "engine_kwargs": {
                "vllm": {
                    "enable_auto_tool_choice": True,
                    "tool_call_parser": "hermes",
                }
            },
        },
        "actor": {
            "ppo_mini_batch_size": 256,
            "ppo_micro_batch_size_per_gpu": 4,
            "optim": {"lr": 1e-6, "lr_warmup_steps_ratio": 0.95},
            "use_kl_loss": True,
            "kl_loss_type": "low_var_kl",
            "kl_loss_coef": 0.001,
            "entropy_coeff": 0,
            "clip_ratio_low": 0.2,
            "clip_ratio_high": 0.3,
            "fsdp_config": {
                "param_offload": True,
                "optimizer_offload": True,
            },
        },
        "ref": {
            "log_prob_micro_batch_size_per_gpu": 4,
            "fsdp_config": {"param_offload": True},
        },
        "model": {
            "path": "Qwen/Qwen2.5-Coder-1.5B-Instruct",
            "use_remove_padding": True,
            "enable_gradient_checkpointing": True,
        },
    },
    "trainer": {
        "n_gpus_per_node": 8,
        "val_before_train": True,
        "critic_warmup": 0,
        "logger": ["console", "wandb"],
        "project_name": "AgentLightning",
        "experiment_name": "searchr1",
        "nnodes": 1,
        "test_freq": 10,
        "save_freq": 10,
        "total_epochs": 15,
        "total_training_steps": 300,
        "default_local_dir": "checkpoints/searchr1_checkpoints/",
    },
}


def config_train_fast() -> Dict[str, Any]:
    """A fast training run for CI testing purposes."""

    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    EXPERIMENT_NAME = f"searchr1_{timestamp}"
    PROJECT_NAME = "AgentLightningCI"

    # Simulate writing to $GITHUB_OUTPUT if it’s set
    github_output = os.getenv("GITHUB_OUTPUT")
    if github_output:
        with open(github_output, "a") as f:
            f.write(f"project_name={PROJECT_NAME}\n")
            f.write(f"run_name={EXPERIMENT_NAME}\n")

    print("Set environment variables:")
    print(f"PROJECT_NAME={PROJECT_NAME}")
    print(f"EXPERIMENT_NAME={EXPERIMENT_NAME}")

    config = deepcopy(RL_TRAINING_CONFIG)
    config["actor_rollout_ref"]["rollout"]["gpu_memory_utilization"] = 0.6
    config["actor_rollout_ref"]["model"]["path"] = "Qwen/Qwen2.5-Coder-0.5B-Instruct"
    config["data"]["val_files"] = "data/test_dev.parquet"
    config["trainer"]["total_epochs"] = 1
    config["trainer"]["total_training_steps"] = 1
    config["trainer"]["experiment_name"] = EXPERIMENT_NAME
    config["trainer"]["project_name"] = PROJECT_NAME
    config["trainer"]["test_freq"] = 1
    return config


def config_train_qwen() -> Dict[str, Any]:
    """A configuration for training with Qwen-2.5."""

    config = deepcopy(RL_TRAINING_CONFIG)
    return config


def config_train_llama() -> Dict[str, Any]:
    """A configuration for training with LLaMA-3.2-3B-Instruct.

    You will need a `HF_TOKEN` set to run with this config.
    """

    config = deepcopy(RL_TRAINING_CONFIG)
    config["actor_rollout_ref"]["rollout"]["multi_turn"]["format"] = "llama3_json"
    config["actor_rollout_ref"]["rollout"]["engine_kwargs"]["vllm"]["tool_call_parser"] = "llama3_json"
    config["actor_rollout_ref"]["model"]["path"] = "meta-llama/Llama-3.2-3B-Instruct"
    return config


def train(config: Dict[str, Any]) -> None:

    agent = SearchR1Agent()
    algorithm = agl.VERL(config)
    trainer = agl.Trainer(n_runners=32, algorithm=algorithm)

    train_data = pd.read_parquet(config["data"]["train_files"]).to_dict(orient="records")  # type: ignore
    val_data = pd.read_parquet(config["data"]["val_files"]).to_dict(orient="records")  # type: ignore
    trainer.fit(agent, train_dataset=train_data, val_dataset=val_data)  # type: ignore


def main() -> None:
    """Main function to parse arguments and run training."""
    parser = argparse.ArgumentParser(description="Train a Search-R1 agent using different model configurations")

    parser.add_argument(
        "config",
        choices=["fast", "qwen", "llama"],
        help="Training configuration: 'fast' (CI testing), 'qwen' (Qwen-2.5-Coder-1.5B), 'llama' (LLaMA-3.2-3B-Instruct)",
    )

    args = parser.parse_args()

    # Get the appropriate configuration
    config_functions = {"fast": config_train_fast, "qwen": config_train_qwen, "llama": config_train_llama}

    config = config_functions[args.config]()

    print(f"Starting training with '{args.config}' configuration...")

    train(config)


if __name__ == "__main__":
    main()


--- dashboard/README.md ---
# Agent-lightning Dashboard

This is the dashboard for Agent-lightning. It is a web application that allows you to inspect your Agent-lightning store and debug running experiments.

The dashboard is built with React, Mantine UI, and Storybook.

## npm scripts

## Build and dev scripts

- `dev` – start development server
- `build` – build production version of the app
- `preview` – locally preview production build

### Testing scripts

- `eslint` - runs ESLint
- `stylelint` - runs Stylelint
- `prettier` - runs Prettier
- `typecheck` - runs TypeScript typecheck
- `vitest` – runs vitest tests
- `chromatic` – runs chromatic tests

### Other scripts

- `storybook` – starts storybook dev server
- `build-storybook` – build production storybook bundle to `storybook-static`


--- dashboard/public/index.html ---
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="../src/favicon.svg" />
    <meta name="viewport" content="minimum-scale=1, initial-scale=1, width=device-width, user-scalable=no" />
    <title>Agent-lightning Dashboard</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/main.tsx"></script>
  </body>
</html>


--- dashboard/eslint.config.js ---
// Copyright (c) Microsoft. All rights reserved.

// @ts-check
import stylistic from '@stylistic/eslint-plugin';
import mantine from 'eslint-config-mantine';
import { defineConfig } from 'eslint/config';
import tseslint from 'typescript-eslint';

export default defineConfig([
  // These are arrays → safe to spread
  ...tseslint.configs.recommended,
  stylistic.configs.customize({ semi: true }),

  // mantine is often a single object → include as-is (or spread only if it's actually an array)
  ...(Array.isArray(mantine) ? mantine : [mantine]),

  // ignores go as their own entry
  { ignores: ['**/*.{mjs,cjs,js,d.ts,d.mts}'] },

  // file-specific rules
  {
    files: ['**/*.story.tsx'],
    rules: { 'no-console': 'off' },
  },

  // project/TS settings + your custom rules
  {
    languageOptions: {
      parserOptions: {
        tsconfigRootDir: process.cwd(),
        project: ['./tsconfig.json'],
      },
    },
    rules: {
      // Disabling conflict rules with prettier
      '@stylistic/brace-style': ['error', '1tbs', { allowSingleLine: false }],
      '@stylistic/no-trailing-spaces': 'error',
      '@stylistic/no-multiple-empty-lines': ['error', { max: 2, maxEOF: 1 }],
      '@stylistic/jsx-quotes': ['error', 'prefer-single'],
      '@stylistic/multiline-ternary': 'off',
      '@stylistic/arrow-parens': ['error', 'always'],
      '@stylistic/jsx-closing-bracket-location': 'off',
      '@stylistic/operator-linebreak': 'off',
      '@stylistic/jsx-newline': 'off',
      '@stylistic/jsx-one-expression-per-line': 'off',
      '@stylistic/indent': 'off',
      '@stylistic/indent-binary-ops': 'off',
    },
  },
]);


--- dashboard/vitest.shims.d.ts ---
// Copyright (c) Microsoft. All rights reserved.

/// <reference types="@vitest/browser-playwright" />


--- dashboard/.storybook/constants.ts ---
// Copyright (c) Microsoft. All rights reserved.

// Centralized constants that keep Storybook fixtures deterministic so Chromatic
// snapshots do not drift when the build environment changes.
export const STORY_DATE_NOW_MS = 1762775145209;
export const STORY_DATE_NOW_SECONDS = Math.floor(STORY_DATE_NOW_MS / 1000);

// Use a fixed origin so any code that would normally read window.location.*
// in the app can rely on the same value from Storybook fixtures. Prefer HTTPS
// so Chromatic (which is served over HTTPS) avoids mixed-content fetch errors.
export const STORY_BASE_URL = 'https://storybook.agentlightning.invalid';
export const STORY_LOCATION_HREF = `${STORY_BASE_URL}/storybook`;


--- dashboard/src/cssVariableResolver.ts ---
// Copyright (c) Microsoft. All rights reserved.

import { alpha, CSSVariablesResolver } from '@mantine/core';

export const shadcnCssVariableResolver: CSSVariablesResolver = () => ({
  variables: {
    // variables that do not depend on color scheme
    '--mantine-heading-font-weight': '600',
    '--mantine-primary-color-filled-hover': alpha('var(--mantine-primary-color-filled)', 0.9),
    '--mantine-primary-color-light': 'var(--mantine-color-zinc-light)',
    '--mantine-primary-color-light-hover': 'var(--mantine-color-zinc-light-hover)',
    '--mantine-primary-color-light-color': 'var(--mantine-color-zinc-light-color)',
  },
  light: {
    // all variables that depend on light color scheme
    '--mantine-primary-color-contrast': 'var(--mantine-color-zinc-0)', // used as primary color contrast
    '--mantine-color-text': 'var(--mantine-color-secondary-9)', // used as text color
    '--mantine-color-body': 'var(--mantine-color-white)', // used as body color
    '--mantine-color-error': 'var(--mantine-color-error-10)', // used as error color
    '--mantine-color-placeholder': 'var(--mantine-color-secondary-10)', // used as placeholder color
    '--mantine-color-anchor': 'var(--mantine-color-secondary-10)', // used as anchor color

    '--mantine-color-default': 'var(--mantine-color-secondary-0)', // used as default surface color
    '--mantine-color-default-hover': 'var(--mantine-color-secondary-1)', // used as default hover color
    '--mantine-color-default-color': 'var(--mantine-color-secondary-9)', // used as default text color
    '--mantine-color-default-border': 'var(--mantine-color-secondary-2)', // used as default border color
    '--mantine-color-dimmed': 'var(--mantine-color-secondary-10)', // used as dimmed text color

    '--mantine-color-secondary-filled': 'var(--mantine-color-white)', // used as secondary surface color
    '--mantine-color-secondary-filled-hover': 'var(--mantine-color-secondary-1)', // used as secondary hover color

    '--mantine-color-secondary-light': 'var(--mantine-color-secondary-1)', // used as primary light color
    '--mantine-color-secondary-light-hover': alpha('var(--mantine-color-secondary-light)', 0.8), // used as primary light hover color

    '--mantine-color-secondary-text': 'var(--mantine-primary-color-contrast)', // can be used as secondary text color
    '--mantine-color-secondary-light-color': 'var(--mantine-color-secondary-8)', // used as primary light variant's text color

    '--mantine-color-secondary-outline': 'var(--mantine-color-secondary-2)',
    '--mantine-color-secondary-outline-hover': 'var(--mantine-color-secondary-1)',

    // all filled colors
    '--mantine-color-zinc-filled': 'var(--mantine-color-zinc-8)',
    '--mantine-color-zinc-filled-hover': alpha('var(--mantine-color-zinc-8)', 0.9),
    '--mantine-color-slate-filled': 'var(--mantine-color-slate-8)',
    '--mantine-color-slate-filled-hover': alpha('var(--mantine-color-slate-8)', 0.9),
    '--mantine-color-gray-filled': 'var(--mantine-color-gray-8)',
    '--mantine-color-gray-filled-hover': alpha('var(--mantine-color-gray-8)', 0.9),
    '--mantine-color-neutral-filled': 'var(--mantine-color-neutral-8)',
    '--mantine-color-neutral-filled-hover': alpha('var(--mantine-color-neutral-8)', 0.9),
    '--mantine-color-stone-filled': 'var(--mantine-color-stone-8)',
    '--mantine-color-stone-filled-hover': alpha('var(--mantine-color-stone-8)', 0.9),
    '--mantine-color-red-filled': 'var(--mantine-color-red-5)',
    '--mantine-color-red-filled-hover': alpha('var(--mantine-color-red-5)', 0.9),
    '--mantine-color-rose-filled': 'var(--mantine-color-rose-5)',
    '--mantine-color-rose-filled-hover': alpha('var(--mantine-color-rose-5)', 0.9),
    '--mantine-color-orange-filled': 'var(--mantine-color-orange-5)',
    '--mantine-color-orange-filled-hover': alpha('var(--mantine-color-orange-5)', 0.9),
    '--mantine-color-amber-filled': 'var(--mantine-color-amber-5)',
    '--mantine-color-amber-filled-hover': alpha('var(--mantine-color-amber-5)', 0.9),
    '--mantine-color-yellow-filled': 'var(--mantine-color-yellow-4)',
    '--mantine-color-yellow-filled-hover': alpha('var(--mantine-color-yellow-4)', 0.9),
    '--mantine-color-lime-filled': 'var(--mantine-color-lime-5)',
    '--mantine-color-lime-filled-hover': alpha('var(--mantine-color-lime-5)', 0.9),
    '--mantine-color-green-filled': 'var(--mantine-color-green-6)',
    '--mantine-color-green-filled-hover': alpha('var(--mantine-color-green-6)', 0.9),
    '--mantine-color-emerald-filled': 'var(--mantine-color-emerald-5)',
    '--mantine-color-emerald-filled-hover': alpha('var(--mantine-color-emerald-5)', 0.9),
    '--mantine-color-teal-filled': 'var(--mantine-color-teal-5)',
    '--mantine-color-teal-filled-hover': alpha('var(--mantine-color-teal-5)', 0.9),
    '--mantine-color-cyan-filled': 'var(--mantine-color-cyan-5)',
    '--mantine-color-cyan-filled-hover': alpha('var(--mantine-color-cyan-5)', 0.9),
    '--mantine-color-sky-filled': 'var(--mantine-color-sky-5)',
    '--mantine-color-sky-filled-hover': alpha('var(--mantine-color-sky-5)', 0.9),
    '--mantine-color-blue-filled': 'var(--mantine-color-blue-6)',
    '--mantine-color-blue-filled-hover': alpha('var(--mantine-color-blue-6)', 0.9),
    '--mantine-color-indigo-filled': 'var(--mantine-color-indigo-5)',
    '--mantine-color-indigo-filled-hover': alpha('var(--mantine-color-indigo-5)', 0.9),
    '--mantine-color-violet-filled': 'var(--mantine-color-violet-5)',
    '--mantine-color-violet-filled-hover': alpha('var(--mantine-color-violet-5)', 0.9),
    '--mantine-color-purple-filled': 'var(--mantine-color-purple-5)',
    '--mantine-color-purple-filled-hover': alpha('var(--mantine-color-purple-5)', 0.9),
    '--mantine-color-fuchsia-filled': 'var(--mantine-color-fuchsia-5)',
    '--mantine-color-fuchsia-filled-hover': alpha('var(--mantine-color-fuchsia-5)', 0.9),
    '--mantine-color-pink-filled': 'var(--mantine-color-pink-5)',
    '--mantine-color-pink-filled-hover': alpha('var(--mantine-color-pink-5)', 0.9),

    // all light colors
    '--mantine-color-zinc-light': alpha('var(--mantine-color-zinc-4)', 0.1),
    '--mantine-color-zinc-light-hover': alpha('var(--mantine-color-zinc-light)', 0.8),
    '--mantine-color-zinc-light-color': 'var(--mantine-color-zinc-6)',
    '--mantine-color-slate-light': alpha('var(--mantine-color-slate-4)', 0.1),
    '--mantine-color-slate-light-hover': alpha('var(--mantine-color-slate-light)', 0.8),
    '--mantine-color-slate-light-color': 'var(--mantine-color-slate-6)',
    '--mantine-color-gray-light': alpha('var(--mantine-color-gray-4)', 0.1),
    '--mantine-color-gray-light-hover': alpha('var(--mantine-color-gray-light)', 0.8),
    '--mantine-color-gray-light-color': 'var(--mantine-color-gray-6)',
    '--mantine-color-neutral-light': alpha('var(--mantine-color-neutral-4)', 0.1),
    '--mantine-color-neutral-light-hover': alpha('var(--mantine-color-neutral-light)', 0.8),
    '--mantine-color-neutral-light-color': 'var(--mantine-color-neutral-6)',
    '--mantine-color-stone-light': alpha('var(--mantine-color-stone-4)', 0.1),
    '--mantine-color-stone-light-hover': alpha('var(--mantine-color-stone-light)', 0.8),
    '--mantine-color-stone-light-color': 'var(--mantine-color-stone-6)',
    '--mantine-color-red-light': alpha('var(--mantine-color-red-4)', 0.1),
    '--mantine-color-red-light-hover': alpha('var(--mantine-color-red-light)', 0.8),
    '--mantine-color-red-light-color': 'var(--mantine-color-red-6)',
    '--mantine-color-rose-light': alpha('var(--mantine-color-rose-4)', 0.1),
    '--mantine-color-rose-light-hover': alpha('var(--mantine-color-rose-light)', 0.8),
    '--mantine-color-rose-light-color': 'var(--mantine-color-rose-6)',
    '--mantine-color-orange-light': alpha('var(--mantine-color-orange-4)', 0.1),
    '--mantine-color-orange-light-hover': alpha('var(--mantine-color-orange-light)', 0.8),
    '--mantine-color-orange-light-color': 'var(--mantine-color-orange-6)',
    '--mantine-color-amber-light': alpha('var(--mantine-color-amber-4)', 0.1),
    '--mantine-color-amber-light-hover': alpha('var(--mantine-color-amber-light)', 0.8),
    '--mantine-color-amber-light-color': 'var(--mantine-color-amber-6)',
    '--mantine-color-yellow-light': alpha('var(--mantine-color-yellow-4)', 0.1),
    '--mantine-color-yellow-light-hover': alpha('var(--mantine-color-yellow-light)', 0.8),
    '--mantine-color-yellow-light-color': 'var(--mantine-color-yellow-6)',
    '--mantine-color-lime-light': alpha('var(--mantine-color-lime-4)', 0.1),
    '--mantine-color-lime-light-hover': alpha('var(--mantine-color-lime-light)', 0.8),
    '--mantine-color-lime-light-color': 'var(--mantine-color-lime-6)',
    '--mantine-color-green-light': alpha('var(--mantine-color-green-4)', 0.1),
    '--mantine-color-green-light-hover': alpha('var(--mantine-color-green-light)', 0.8),
    '--mantine-color-green-light-color': 'var(--mantine-color-green-6)',
    '--mantine-color-emerald-light': alpha('var(--mantine-color-emerald-4)', 0.1),
    '--mantine-color-emerald-light-hover': alpha('var(--mantine-color-emerald-light)', 0.8),
    '--mantine-color-emerald-light-color': 'var(--mantine-color-emerald-6)',
    '--mantine-color-teal-light': alpha('var(--mantine-color-teal-4)', 0.1),
    '--mantine-color-teal-light-hover': alpha('var(--mantine-color-teal-light)', 0.8),
    '--mantine-color-teal-light-color': 'var(--mantine-color-teal-6)',
    '--mantine-color-cyan-light': alpha('var(--mantine-color-cyan-4)', 0.1),
    '--mantine-color-cyan-light-hover': alpha('var(--mantine-color-cyan-light)', 0.8),
    '--mantine-color-cyan-light-color': 'var(--mantine-color-cyan-6)',
    '--mantine-color-sky-light': alpha('var(--mantine-color-sky-4)', 0.1),
    '--mantine-color-sky-light-hover': alpha('var(--mantine-color-sky-light)', 0.8),
    '--mantine-color-sky-light-color': 'var(--mantine-color-sky-6)',
    '--mantine-color-blue-light': alpha('var(--mantine-color-blue-4)', 0.1),
    '--mantine-color-blue-light-hover': alpha('var(--mantine-color-blue-light)', 0.8),
    '--mantine-color-blue-light-color': 'var(--mantine-color-blue-6)',
    '--mantine-color-indigo-light': alpha('var(--mantine-color-indigo-4)', 0.1),
    '--mantine-color-indigo-light-hover': alpha('var(--mantine-color-indigo-light)', 0.8),
    '--mantine-color-indigo-light-color': 'var(--mantine-color-indigo-6)',
    '--mantine-color-violet-light': alpha('var(--mantine-color-violet-4)', 0.1),
    '--mantine-color-violet-light-hover': alpha('var(--mantine-color-violet-light)', 0.8),
    '--mantine-color-violet-light-color': 'var(--mantine-color-violet-6)',
    '--mantine-color-purple-light': alpha('var(--mantine-color-purple-4)', 0.1),
    '--mantine-color-purple-light-hover': alpha('var(--mantine-color-purple-light)', 0.8),
    '--mantine-color-purple-light-color': 'var(--mantine-color-purple-6)',
    '--mantine-color-fuchsia-light': alpha('var(--mantine-color-fuchsia-4)', 0.1),
    '--mantine-color-fuchsia-light-hover': alpha('var(--mantine-color-fuchsia-light)', 0.8),
    '--mantine-color-fuchsia-light-color': 'var(--mantine-color-fuchsia-6)',
    '--mantine-color-pink-light': alpha('var(--mantine-color-pink-4)', 0.1),
    '--mantine-color-pink-light-hover': alpha('var(--mantine-color-pink-light)', 0.8),
    '--mantine-color-pink-light-color': 'var(--mantine-color-pink-6)',

    // all outline colors
    '--mantine-color-zinc-outline': 'var(--mantine-color-zinc-8)',
    '--mantine-color-zinc-outline-hover': alpha('var(--mantine-color-zinc-4)', 0.1),
    '--mantine-color-slate-outline': 'var(--mantine-color-slate-8)',
    '--mantine-color-slate-outline-hover': alpha('var(--mantine-color-slate-4)', 0.1),
    '--mantine-color-gray-outline': 'var(--mantine-color-gray-8)',
    '--mantine-color-gray-outline-hover': alpha('var(--mantine-color-gray-4)', 0.1),
    '--mantine-color-neutral-outline': 'var(--mantine-color-neutral-8)',
    '--mantine-color-neutral-outline-hover': alpha('var(--mantine-color-neutral-4)', 0.1),
    '--mantine-color-stone-outline': 'var(--mantine-color-stone-8)',
    '--mantine-color-stone-outline-hover': alpha('var(--mantine-color-stone-4)', 0.1),
    '--mantine-color-red-outline': 'var(--mantine-color-red-5)',
    '--mantine-color-red-outline-hover': alpha('var(--mantine-color-red-4)', 0.1),
    '--mantine-color-rose-outline': 'var(--mantine-color-rose-5)',
    '--mantine-color-rose-outline-hover': alpha('var(--mantine-color-rose-4)', 0.1),
    '--mantine-color-orange-outline': 'var(--mantine-color-orange-5)',
    '--mantine-color-orange-outline-hover': alpha('var(--mantine-color-orange-4)', 0.1),
    '--mantine-color-amber-outline': 'var(--mantine-color-amber-5)',
    '--mantine-color-amber-outline-hover': alpha('var(--mantine-color-amber-4)', 0.1),
    '--mantine-color-yellow-outline': 'var(--mantine-color-yellow-4)',
    '--mantine-color-yellow-outline-hover': alpha('var(--mantine-color-yellow-4)', 0.1),
    '--mantine-color-lime-outline': 'var(--mantine-color-lime-5)',
    '--mantine-color-lime-outline-hover': alpha('var(--mantine-color-lime-4)', 0.1),
    '--mantine-color-green-outline': 'var(--mantine-color-green-6)',
    '--mantine-color-green-outline-hover': alpha('var(--mantine-color-green-4)', 0.1),
    '--mantine-color-emerald-outline': 'var(--mantine-color-emerald-5)',
    '--mantine-color-emerald-outline-hover': alpha('var(--mantine-color-emerald-4)', 0.1),
    '--mantine-color-teal-outline': 'var(--mantine-color-teal-5)',
    '--mantine-color-teal-outline-hover': alpha('var(--mantine-color-teal-4)', 0.1),
    '--mantine-color-cyan-outline': 'var(--mantine-color-cyan-5)',
    '--mantine-color-cyan-outline-hover': alpha('var(--mantine-color-cyan-4)', 0.1),
    '--mantine-color-sky-outline': 'var(--mantine-color-sky-5)',
    '--mantine-color-sky-outline-hover': alpha('var(--mantine-color-sky-4)', 0.1),
    '--mantine-color-blue-outline': 'var(--mantine-color-blue-6)',
    '--mantine-color-blue-outline-hover': alpha('var(--mantine-color-blue-4)', 0.1),
    '--mantine-color-indigo-outline': 'var(--mantine-color-indigo-5)',
    '--mantine-color-indigo-outline-hover': alpha('var(--mantine-color-indigo-4)', 0.1),
    '--mantine-color-violet-outline': 'var(--mantine-color-violet-5)',
    '--mantine-color-violet-outline-hover': alpha('var(--mantine-color-violet-4)', 0.1),
    '--mantine-color-purple-outline': 'var(--mantine-color-purple-5)',
    '--mantine-color-purple-outline-hover': alpha('var(--mantine-color-purple-4)', 0.1),
    '--mantine-color-fuchsia-outline': 'var(--mantine-color-fuchsia-5)',
    '--mantine-color-fuchsia-outline-hover': alpha('var(--mantine-color-fuchsia-4)', 0.1),
    '--mantine-color-pink-outline': 'var(--mantine-color-pink-5)',
    '--mantine-color-pink-outline-hover': alpha('var(--mantine-color-pink-4)', 0.1),

    // all contrast colors
    '--mantine-color-zinc-contrast': 'var(--mantine-color-zinc-0)',
    '--mantine-color-slate-contrast': 'var(--mantine-color-slate-0)',
    '--mantine-color-gray-contrast': 'var(--mantine-color-gray-0)',
    '--mantine-color-neutral-contrast': 'var(--mantine-color-neutral-0)',
    '--mantine-color-stone-contrast': 'var(--mantine-color-stone-0)',
    '--mantine-color-red-contrast': 'var(--mantine-color-red-0)',
    '--mantine-color-rose-contrast': 'var(--mantine-color-rose-0)',
    '--mantine-color-orange-contrast': 'var(--mantine-color-stone-0)',
    '--mantine-color-amber-contrast': 'var(--mantine-color-amber-0)',
    '--mantine-color-yellow-contrast': '#422006',
    '--mantine-color-lime-contrast': 'var(--mantine-color-lime-0)',
    '--mantine-color-green-contrast': 'var(--mantine-color-rose-0)',
    '--mantine-color-emerald-contrast': 'var(--mantine-color-emerald-0)',
    '--mantine-color-teal-contrast': 'var(--mantine-color-teal-0)',
    '--mantine-color-cyan-contrast': 'var(--mantine-color-cyan-0)',
    '--mantine-color-sky-contrast': 'var(--mantine-color-sky-0)',
    '--mantine-color-blue-contrast': 'var(--mantine-color-slate-0)',
    '--mantine-color-indigo-contrast': 'var(--mantine-color-indigo-0)',
    '--mantine-color-violet-contrast': 'var(--mantine-color-gray-0)',
    '--mantine-color-purple-contrast': 'var(--mantine-color-purple-0)',
    '--mantine-color-fuchsia-contrast': 'var(--mantine-color-fuchsia-0)',
    '--mantine-color-pink-contrast': 'var(--mantine-color-pink-0)',
  },
  dark: {
    // all variables that depend on dark color scheme
    '--mantine-primary-color-contrast': 'var(--mantine-color-zinc-8)', // used as primary color contrast
    '--mantine-color-text': 'var(--mantine-color-secondary-0)', // used as text color
    '--mantine-color-body': 'var(--mantine-color-secondary-9)', // used as body color
    '--mantine-color-error': 'var(--mantine-color-error-10)', // used as error color
    '--mantine-color-placeholder': 'var(--mantine-color-secondary-4)', // used as placeholder color
    '--mantine-color-anchor': 'var(--mantine-color-secondary-4)', // used as anchor color

    '--mantine-color-default': 'var(--mantine-color-secondary-9)', // used as default surface color
    '--mantine-color-default-hover': 'var(--mantine-color-secondary-7)', // used as default hover color
    '--mantine-color-default-color': 'var(--mantine-color-secondary-1)', // used as default text color
    '--mantine-color-default-border': 'var(--mantine-color-secondary-7)', // used as default border color
    '--mantine-color-dimmed': 'var(--mantine-color-secondary-4)', // used as dimmed text color

    '--mantine-color-secondary-filled': 'var(--mantine-color-secondary-8)', // used as secondary surface color
    '--mantine-color-secondary-filled-hover': alpha('var(--mantine-color-secondary-filled)', 0.9), // used as secondary hover color

    '--mantine-color-secondary-light': 'var(--mantine-color-secondary-7)', // used as primary light color
    '--mantine-color-secondary-light-hover': alpha('var(--mantine-color-secondary-light)', 0.8), // used as primary light hover color

    '--mantine-color-secondary-text': 'var(--mantine-primary-color-contrast)', // can be used as secondary text color
    '--mantine-color-secondary-light-color': 'var(--mantine-color-secondary-0)', // used as primary light text color

    '--mantine-color-secondary-outline': 'var(--mantine-color-secondary-7)',
    '--mantine-color-secondary-outline-hover': 'var(--mantine-color-secondary-7)',

    // all filled colors
    '--mantine-color-zinc-filled': 'var(--mantine-color-zinc-0)',
    '--mantine-color-zinc-filled-hover': alpha('var(--mantine-color-zinc-0)', 0.9),
    '--mantine-color-slate-filled': 'var(--mantine-color-slate-0)',
    '--mantine-color-slate-filled-hover': alpha('var(--mantine-color-slate-0)', 0.9),
    '--mantine-color-gray-filled': 'var(--mantine-color-gray-0)',
    '--mantine-color-gray-filled-hover': alpha('var(--mantine-color-gray-0)', 0.9),
    '--mantine-color-neutral-filled': 'var(--mantine-color-neutral-0)',
    '--mantine-color-neutral-filled-hover': alpha('var(--mantine-color-neutral-0)', 0.9),
    '--mantine-color-stone-filled': 'var(--mantine-color-stone-0)',
    '--mantine-color-stone-filled-hover': alpha('var(--mantine-color-stone-0)', 0.9),
    '--mantine-color-red-filled': 'var(--mantine-color-red-5)',
    '--mantine-color-red-filled-hover': alpha('var(--mantine-color-red-5)', 0.9),
    '--mantine-color-rose-filled': 'var(--mantine-color-rose-5)',
    '--mantine-color-rose-filled-hover': alpha('var(--mantine-color-rose-5)', 0.9),
    '--mantine-color-orange-filled': 'var(--mantine-color-orange-6)',
    '--mantine-color-orange-filled-hover': alpha('var(--mantine-color-orange-6)', 0.9),
    '--mantine-color-amber-filled': 'var(--mantine-color-amber-5)',
    '--mantine-color-amber-filled-hover': alpha('var(--mantine-color-amber-5)', 0.9),
    '--mantine-color-yellow-filled': 'var(--mantine-color-yellow-4)',
    '--mantine-color-yellow-filled-hover': alpha('var(--mantine-color-yellow-4)', 0.9),
    '--mantine-color-lime-filled': 'var(--mantine-color-lime-4)',
    '--mantine-color-lime-filled-hover': alpha('var(--mantine-color-lime-4)', 0.9),
    '--mantine-color-green-filled': 'var(--mantine-color-green-5)',
    '--mantine-color-green-filled-hover': alpha('var(--mantine-color-green-5)', 0.9),
    '--mantine-color-emerald-filled': 'var(--mantine-color-emerald-5)',
    '--mantine-color-emerald-filled-hover': alpha('var(--mantine-color-emerald-5)', 0.9),
    '--mantine-color-teal-filled': 'var(--mantine-color-teal-4)',
    '--mantine-color-teal-filled-hover': alpha('var(--mantine-color-teal-4)', 0.9),
    '--mantine-color-cyan-filled': 'var(--mantine-color-cyan-4)',
    '--mantine-color-cyan-filled-hover': alpha('var(--mantine-color-cyan-4)', 0.9),
    '--mantine-color-sky-filled': 'var(--mantine-color-sky-4)',
    '--mantine-color-sky-filled-hover': alpha('var(--mantine-color-sky-4)', 0.9),
    '--mantine-color-blue-filled': 'var(--mantine-color-blue-5)',
    '--mantine-color-blue-filled-hover': alpha('var(--mantine-color-blue-5)', 0.9),
    '--mantine-color-indigo-filled': 'var(--mantine-color-indigo-6)',
    '--mantine-color-indigo-filled-hover': alpha('var(--mantine-color-indigo-6)', 0.9),
    '--mantine-color-violet-filled': 'var(--mantine-color-violet-6)',
    '--mantine-color-violet-filled-hover': alpha('var(--mantine-color-violet-6)', 0.9),
    '--mantine-color-purple-filled': 'var(--mantine-color-purple-6)',
    '--mantine-color-purple-filled-hover': alpha('var(--mantine-color-purple-6)', 0.9),
    '--mantine-color-fuchsia-filled': 'var(--mantine-color-fuchsia-7)',
    '--mantine-color-fuchsia-filled-hover': alpha('var(--mantine-color-fuchsia-7)', 0.9),
    '--mantine-color-pink-filled': 'var(--mantine-color-pink-6)',
    '--mantine-color-pink-filled-hover': alpha('var(--mantine-color-pink-6)', 0.9),

    // all light colors
    '--mantine-color-zinc-light': alpha('var(--mantine-color-zinc-4)', 0.15),
    '--mantine-color-zinc-light-hover': alpha('var(--mantine-color-zinc-light)', 0.8),
    '--mantine-color-zinc-light-color': 'var(--mantine-color-zinc-3)',
    '--mantine-color-slate-light': alpha('var(--mantine-color-slate-4)', 0.15),
    '--mantine-color-slate-light-hover': alpha('var(--mantine-color-slate-light)', 0.8),
    '--mantine-color-slate-light-color': 'var(--mantine-color-slate-3)',
    '--mantine-color-gray-light': alpha('var(--mantine-color-gray-4)', 0.15),
    '--mantine-color-gray-light-hover': alpha('var(--mantine-color-gray-light)', 0.8),
    '--mantine-color-gray-light-color': 'var(--mantine-color-gray-3)',
    '--mantine-color-neutral-light': alpha('var(--mantine-color-neutral-4)', 0.15),
    '--mantine-color-neutral-light-hover': alpha('var(--mantine-color-neutral-light)', 0.8),
    '--mantine-color-neutral-light-color': 'var(--mantine-color-neutral-3)',
    '--mantine-color-stone-light': alpha('var(--mantine-color-stone-4)', 0.15),
    '--mantine-color-stone-light-hover': alpha('var(--mantine-color-stone-light)', 0.8),
    '--mantine-color-stone-light-color': 'var(--mantine-color-stone-3)',
    '--mantine-color-red-light': alpha('var(--mantine-color-red-4)', 0.15),
    '--mantine-color-red-light-hover': alpha('var(--mantine-color-red-light)', 0.8),
    '--mantine-color-red-light-color': 'var(--mantine-color-red-3)',
    '--mantine-color-rose-light': alpha('var(--mantine-color-rose-4)', 0.15),
    '--mantine-color-rose-light-hover': alpha('var(--mantine-color-rose-light)', 0.8),
    '--mantine-color-rose-light-color': 'var(--mantine-color-rose-3)',
    '--mantine-color-orange-light': alpha('var(--mantine-color-orange-4)', 0.15),
    '--mantine-color-orange-light-hover': alpha('var(--mantine-color-orange-light)', 0.8),
    '--mantine-color-orange-light-color': 'var(--mantine-color-orange-3)',
    '--mantine-color-amber-light': alpha('var(--mantine-color-amber-4)', 0.15),
    '--mantine-color-amber-light-hover': alpha('var(--mantine-color-amber-light)', 0.8),
    '--mantine-color-amber-light-color': 'var(--mantine-color-amber-3)',
    '--mantine-color-yellow-light': alpha('var(--mantine-color-yellow-4)', 0.15),
    '--mantine-color-yellow-light-hover': alpha('var(--mantine-color-yellow-light)', 0.8),
    '--mantine-color-yellow-light-color': 'var(--mantine-color-yellow-3)',
    '--mantine-color-lime-light': alpha('var(--mantine-color-lime-4)', 0.15),
    '--mantine-color-lime-light-hover': alpha('var(--mantine-color-lime-light)', 0.8),
    '--mantine-color-lime-light-color': 'var(--mantine-color-lime-3)',
    '--mantine-color-green-light': alpha('var(--mantine-color-green-4)', 0.15),
    '--mantine-color-green-light-hover': alpha('var(--mantine-color-green-light)', 0.8),
    '--mantine-color-green-light-color': 'var(--mantine-color-green-3)',
    '--mantine-color-emerald-light': alpha('var(--mantine-color-emerald-4)', 0.15),
    '--mantine-color-emerald-light-hover': alpha('var(--mantine-color-emerald-light)', 0.8),
    '--mantine-color-emerald-light-color': 'var(--mantine-color-emerald-3)',
    '--mantine-color-teal-light': alpha('var(--mantine-color-teal-4)', 0.15),
    '--mantine-color-teal-light-hover': alpha('var(--mantine-color-teal-light)', 0.8),
    '--mantine-color-teal-light-color': 'var(--mantine-color-teal-3)',
    '--mantine-color-cyan-light': alpha('var(--mantine-color-cyan-4)', 0.15),
    '--mantine-color-cyan-light-hover': alpha('var(--mantine-color-cyan-light)', 0.8),
    '--mantine-color-cyan-light-color': 'var(--mantine-color-cyan-3)',
    '--mantine-color-sky-light': alpha('var(--mantine-color-sky-4)', 0.15),
    '--mantine-color-sky-light-hover': alpha('var(--mantine-color-sky-light)', 0.8),
    '--mantine-color-sky-light-color': 'var(--mantine-color-sky-3)',
    '--mantine-color-blue-light': alpha('var(--mantine-color-blue-4)', 0.15),
    '--mantine-color-blue-light-hover': alpha('var(--mantine-color-blue-light)', 0.8),
    '--mantine-color-blue-light-color': 'var(--mantine-color-blue-3)',
    '--mantine-color-indigo-light': alpha('var(--mantine-color-indigo-4)', 0.15),
    '--mantine-color-indigo-light-hover': alpha('var(--mantine-color-indigo-light)', 0.8),
    '--mantine-color-indigo-light-color': 'var(--mantine-color-indigo-3)',
    '--mantine-color-violet-light': alpha('var(--mantine-color-violet-4)', 0.15),
    '--mantine-color-violet-light-hover': alpha('var(--mantine-color-violet-light)', 0.8),
    '--mantine-color-violet-light-color': 'var(--mantine-color-violet-3)',
    '--mantine-color-purple-light': alpha('var(--mantine-color-purple-4)', 0.15),
    '--mantine-color-purple-light-hover': alpha('var(--mantine-color-purple-light)', 0.8),
    '--mantine-color-purple-light-color': 'var(--mantine-color-purple-3)',
    '--mantine-color-fuchsia-light': alpha('var(--mantine-color-fuchsia-4)', 0.15),
    '--mantine-color-fuchsia-light-hover': alpha('var(--mantine-color-fuchsia-light)', 0.8),
    '--mantine-color-fuchsia-light-color': 'var(--mantine-color-fuchsia-3)',
    '--mantine-color-pink-light': alpha('var(--mantine-color-pink-4)', 0.15),
    '--mantine-color-pink-light-hover': alpha('var(--mantine-color-pink-light)', 0.8),
    '--mantine-color-pink-light-color': 'var(--mantine-color-pink-3)',

    // all outline colors
    '--mantine-color-zinc-outline': 'var(--mantine-color-zinc-0)',
    '--mantine-color-zinc-outline-hover': alpha('var(--mantine-color-zinc-4)', 0.15),
    '--mantine-color-slate-outline': 'var(--mantine-color-slate-0)',
    '--mantine-color-slate-outline-hover': alpha('var(--mantine-color-slate-4)', 0.15),
    '--mantine-color-gray-outline': 'var(--mantine-color-gray-0)',
    '--mantine-color-gray-outline-hover': alpha('var(--mantine-color-gray-4)', 0.15),
    '--mantine-color-neutral-outline': 'var(--mantine-color-neutral-0)',
    '--mantine-color-neutral-outline-hover': alpha('var(--mantine-color-neutral-4)', 0.15),
    '--mantine-color-stone-outline': 'var(--mantine-color-stone-0)',
    '--mantine-color-stone-outline-hover': alpha('var(--mantine-color-stone-4)', 0.15),
    '--mantine-color-red-outline': 'var(--mantine-color-red-5)',
    '--mantine-color-red-outline-hover': alpha('var(--mantine-color-red-4)', 0.15),
    '--mantine-color-rose-outline': 'var(--mantine-color-rose-5)',
    '--mantine-color-rose-outline-hover': alpha('var(--mantine-color-rose-4)', 0.15),
    '--mantine-color-orange-outline': 'var(--mantine-color-orange-6)',
    '--mantine-color-orange-outline-hover': alpha('var(--mantine-color-orange-4)', 0.15),
    '--mantine-color-amber-outline': 'var(--mantine-color-amber-5)',
    '--mantine-color-amber-outline-hover': alpha('var(--mantine-color-amber-4)', 0.15),
    '--mantine-color-yellow-outline': 'var(--mantine-color-yellow-4)',
    '--mantine-color-yellow-outline-hover': alpha('var(--mantine-color-yellow-4)', 0.15),
    '--mantine-color-lime-outline': 'var(--mantine-color-lime-4)',
    '--mantine-color-lime-outline-hover': alpha('var(--mantine-color-lime-4)', 0.15),
    '--mantine-color-green-outline': 'var(--mantine-color-green-5)',
    '--mantine-color-green-outline-hover': alpha('var(--mantine-color-green-4)', 0.15),
    '--mantine-color-emerald-outline': 'var(--mantine-color-emerald-5)',
    '--mantine-color-emerald-outline-hover': alpha('var(--mantine-color-emerald-4)', 0.15),
    '--mantine-color-teal-outline': 'var(--mantine-color-teal-4)',
    '--mantine-color-teal-outline-hover': alpha('var(--mantine-color-teal-4)', 0.15),
    '--mantine-color-cyan-outline': 'var(--mantine-color-cyan-4)',
    '--mantine-color-cyan-outline-hover': alpha('var(--mantine-color-cyan-4)', 0.15),
    '--mantine-color-sky-outline': 'var(--mantine-color-sky-4)',
    '--mantine-color-sky-outline-hover': alpha('var(--mantine-color-sky-4)', 0.15),
    '--mantine-color-blue-outline': 'var(--mantine-color-blue-5)',
    '--mantine-color-blue-outline-hover': alpha('var(--mantine-color-blue-4)', 0.15),
    '--mantine-color-indigo-outline': 'var(--mantine-color-indigo-6)',
    '--mantine-color-indigo-outline-hover': alpha('var(--mantine-color-indigo-4)', 0.15),
    '--mantine-color-violet-outline': 'var(--mantine-color-violet-6)',
    '--mantine-color-violet-outline-hover': alpha('var(--mantine-color-violet-4)', 0.15),
    '--mantine-color-purple-outline': 'var(--mantine-color-purple-6)',
    '--mantine-color-purple-outline-hover': alpha('var(--mantine-color-purple-4)', 0.15),
    '--mantine-color-fuchsia-outline': 'var(--mantine-color-fuchsia-7)',
    '--mantine-color-fuchsia-outline-hover': alpha('var(--mantine-color-fuchsia-4)', 0.15),
    '--mantine-color-pink-outline': 'var(--mantine-color-pink-6)',
    '--mantine-color-pink-outline-hover': alpha('var(--mantine-color-pink-4)', 0.15),

    // all contrast colors
    '--mantine-color-zinc-contrast': 'var(--mantine-color-zinc-8)',
    '--mantine-color-slate-contrast': 'var(--mantine-color-slate-8)',
    '--mantine-color-gray-contrast': 'var(--mantine-color-gray-8)',
    '--mantine-color-neutral-contrast': 'var(--mantine-color-neutral-8)',
    '--mantine-color-stone-contrast': 'var(--mantine-color-stone-8)',
    '--mantine-color-red-contrast': 'var(--mantine-color-red-0)',
    '--mantine-color-rose-contrast': 'var(--mantine-color-rose-0)',
    '--mantine-color-orange-contrast': 'var(--mantine-color-stone-0)',
    '--mantine-color-amber-contrast': 'var(--mantine-color-stone-8)',
    '--mantine-color-yellow-contrast': '#422006',
    '--mantine-color-lime-contrast': 'var(--mantine-color-stone-8)',
    '--mantine-color-green-contrast': 'var(--mantine-color-green-9)',
    '--mantine-color-emerald-contrast': 'var(--mantine-color-stone-0)',
    '--mantine-color-teal-contrast': 'var(--mantine-color-slate-8)',
    '--mantine-color-cyan-contrast': 'var(--mantine-color-slate-8)',
    '--mantine-color-sky-contrast': 'var(--mantine-color-slate-8)',
    '--mantine-color-blue-contrast': 'var(--mantine-color-slate-0)',
    '--mantine-color-indigo-contrast': 'var(--mantine-color-gray-0)',
    '--mantine-color-violet-contrast': 'var(--mantine-color-gray-0)',
    '--mantine-color-purple-contrast': 'var(--mantine-color-gray-0)',
    '--mantine-color-fuchsia-contrast': 'var(--mantine-color-gray-0)',
    '--mantine-color-pink-contrast': 'var(--mantine-color-gray-0)',
  },
});


--- dashboard/test-utils/index.ts ---
// Copyright (c) Microsoft. All rights reserved.

import userEvent from '@testing-library/user-event';

export * from '@testing-library/react';
export { render } from './render';
export { userEvent };
export { createServerBackedStore, getPythonServerBaseUrl } from './server';


--- dashboard/.storybook/main.ts ---
// Copyright (c) Microsoft. All rights reserved.

import type { StorybookConfig } from '@storybook/react-vite';

const config: StorybookConfig = {
  core: {
    disableWhatsNewNotifications: true,
    disableTelemetry: true,
    enableCrashReports: false,
  },
  stories: ['../src/**/*.mdx', '../src/**/*.story.@(js|jsx|ts|tsx)'],
  staticDirs: ['../static'],
  addons: ['@storybook/addon-themes', '@storybook/addon-vitest'],
  framework: {
    name: '@storybook/react-vite',
    options: {},
  },
};

export default config;


--- dashboard/static/mockServiceWorker.js ---
// Copyright (c) Microsoft. All rights reserved.

/* eslint-disable */
/* tslint:disable */

/**
 * Mock Service Worker.
 * @see https://github.com/mswjs/msw
 * - Please do NOT modify this file.
 */

const PACKAGE_VERSION = '2.11.6';
const INTEGRITY_CHECKSUM = '4db4a41e972cec1b64cc569c66952d82';
const IS_MOCKED_RESPONSE = Symbol('isMockedResponse');
const activeClientIds = new Set();

addEventListener('install', function () {
  self.skipWaiting();
});

addEventListener('activate', function (event) {
  event.waitUntil(self.clients.claim());
});

addEventListener('message', async function (event) {
  const clientId = Reflect.get(event.source || {}, 'id');

  if (!clientId || !self.clients) {
    return;
  }

  const client = await self.clients.get(clientId);

  if (!client) {
    return;
  }

  const allClients = await self.clients.matchAll({
    type: 'window',
  });

  switch (event.data) {
    case 'KEEPALIVE_REQUEST': {
      sendToClient(client, {
        type: 'KEEPALIVE_RESPONSE',
      });
      break;
    }

    case 'INTEGRITY_CHECK_REQUEST': {
      sendToClient(client, {
        type: 'INTEGRITY_CHECK_RESPONSE',
        payload: {
          packageVersion: PACKAGE_VERSION,
          checksum: INTEGRITY_CHECKSUM,
        },
      });
      break;
    }

    case 'MOCK_ACTIVATE': {
      activeClientIds.add(clientId);

      sendToClient(client, {
        type: 'MOCKING_ENABLED',
        payload: {
          client: {
            id: client.id,
            frameType: client.frameType,
          },
        },
      });
      break;
    }

    case 'CLIENT_CLOSED': {
      activeClientIds.delete(clientId);

      const remainingClients = allClients.filter((client) => {
        return client.id !== clientId;
      });

      // Unregister itself when there are no more clients
      if (remainingClients.length === 0) {
        self.registration.unregister();
      }

      break;
    }
  }
});

addEventListener('fetch', function (event) {
  const requestInterceptedAt = Date.now();

  // Bypass navigation requests.
  if (event.request.mode === 'navigate') {
    return;
  }

  // Opening the DevTools triggers the "only-if-cached" request
  // that cannot be handled by the worker. Bypass such requests.
  if (event.request.cache === 'only-if-cached' && event.request.mode !== 'same-origin') {
    return;
  }

  // Bypass all requests when there are no active clients.
  // Prevents the self-unregistered worked from handling requests
  // after it's been terminated (still remains active until the next reload).
  if (activeClientIds.size === 0) {
    return;
  }

  const requestId = crypto.randomUUID();
  event.respondWith(handleRequest(event, requestId, requestInterceptedAt));
});

/**
 * @param {FetchEvent} event
 * @param {string} requestId
 * @param {number} requestInterceptedAt
 */
async function handleRequest(event, requestId, requestInterceptedAt) {
  const client = await resolveMainClient(event);
  const requestCloneForEvents = event.request.clone();
  const response = await getResponse(event, client, requestId, requestInterceptedAt);

  // Send back the response clone for the "response:*" life-cycle events.
  // Ensure MSW is active and ready to handle the message, otherwise
  // this message will pend indefinitely.
  if (client && activeClientIds.has(client.id)) {
    const serializedRequest = await serializeRequest(requestCloneForEvents);

    // Clone the response so both the client and the library could consume it.
    const responseClone = response.clone();

    sendToClient(
      client,
      {
        type: 'RESPONSE',
        payload: {
          isMockedResponse: IS_MOCKED_RESPONSE in response,
          request: {
            id: requestId,
            ...serializedRequest,
          },
          response: {
            type: responseClone.type,
            status: responseClone.status,
            statusText: responseClone.statusText,
            headers: Object.fromEntries(responseClone.headers.entries()),
            body: responseClone.body,
          },
        },
      },
      responseClone.body ? [serializedRequest.body, responseClone.body] : [],
    );
  }

  return response;
}

/**
 * Resolve the main client for the given event.
 * Client that issues a request doesn't necessarily equal the client
 * that registered the worker. It's with the latter the worker should
 * communicate with during the response resolving phase.
 * @param {FetchEvent} event
 * @returns {Promise<Client | undefined>}
 */
async function resolveMainClient(event) {
  const client = await self.clients.get(event.clientId);

  if (activeClientIds.has(event.clientId)) {
    return client;
  }

  if (client?.frameType === 'top-level') {
    return client;
  }

  const allClients = await self.clients.matchAll({
    type: 'window',
  });

  return allClients
    .filter((client) => {
      // Get only those clients that are currently visible.
      return client.visibilityState === 'visible';
    })
    .find((client) => {
      // Find the client ID that's recorded in the
      // set of clients that have registered the worker.
      return activeClientIds.has(client.id);
    });
}

/**
 * @param {FetchEvent} event
 * @param {Client | undefined} client
 * @param {string} requestId
 * @param {number} requestInterceptedAt
 * @returns {Promise<Response>}
 */
async function getResponse(event, client, requestId, requestInterceptedAt) {
  // Clone the request because it might've been already used
  // (i.e. its body has been read and sent to the client).
  const requestClone = event.request.clone();

  function passthrough() {
    // Cast the request headers to a new Headers instance
    // so the headers can be manipulated with.
    const headers = new Headers(requestClone.headers);

    // Remove the "accept" header value that marked this request as passthrough.
    // This prevents request alteration and also keeps it compliant with the
    // user-defined CORS policies.
    const acceptHeader = headers.get('accept');
    if (acceptHeader) {
      const values = acceptHeader.split(',').map((value) => value.trim());
      const filteredValues = values.filter((value) => value !== 'msw/passthrough');

      if (filteredValues.length > 0) {
        headers.set('accept', filteredValues.join(', '));
      } else {
        headers.delete('accept');
      }
    }

    return fetch(requestClone, { headers });
  }

  // Bypass mocking when the client is not active.
  if (!client) {
    return passthrough();
  }

  // Bypass initial page load requests (i.e. static assets).
  // The absence of the immediate/parent client in the map of the active clients
  // means that MSW hasn't dispatched the "MOCK_ACTIVATE" event yet
  // and is not ready to handle requests.
  if (!activeClientIds.has(client.id)) {
    return passthrough();
  }

  // Notify the client that a request has been intercepted.
  const serializedRequest = await serializeRequest(event.request);
  const clientMessage = await sendToClient(
    client,
    {
      type: 'REQUEST',
      payload: {
        id: requestId,
        interceptedAt: requestInterceptedAt,
        ...serializedRequest,
      },
    },
    [serializedRequest.body],
  );

  switch (clientMessage.type) {
    case 'MOCK_RESPONSE': {
      return respondWithMock(clientMessage.data);
    }

    case 'PASSTHROUGH': {
      return passthrough();
    }
  }

  return passthrough();
}

/**
 * @param {Client} client
 * @param {any} message
 * @param {Array<Transferable>} transferrables
 * @returns {Promise<any>}
 */
function sendToClient(client, message, transferrables = []) {
  return new Promise((resolve, reject) => {
    const channel = new MessageChannel();

    channel.port1.onmessage = (event) => {
      if (event.data && event.data.error) {
        return reject(event.data.error);
      }

      resolve(event.data);
    };

    client.postMessage(message, [channel.port2, ...transferrables.filter(Boolean)]);
  });
}

/**
 * @param {Response} response
 * @returns {Response}
 */
function respondWithMock(response) {
  // Setting response status code to 0 is a no-op.
  // However, when responding with a "Response.error()", the produced Response
  // instance will have status code set to 0. Since it's not possible to create
  // a Response instance with status code 0, handle that use-case separately.
  if (response.status === 0) {
    return Response.error();
  }

  const mockedResponse = new Response(response.body, response);

  Reflect.defineProperty(mockedResponse, IS_MOCKED_RESPONSE, {
    value: true,
    enumerable: true,
  });

  return mockedResponse;
}

/**
 * @param {Request} request
 */
async function serializeRequest(request) {
  return {
    url: request.url,
    mode: request.mode,
    method: request.method,
    headers: Object.fromEntries(request.headers.entries()),
    cache: request.cache,
    credentials: request.credentials,
    destination: request.destination,
    integrity: request.integrity,
    redirect: request.redirect,
    referrer: request.referrer,
    referrerPolicy: request.referrerPolicy,
    body: await request.arrayBuffer(),
    keepalive: request.keepalive,
  };
}


--- dashboard/.storybook/modes.ts ---
// Copyright (c) Microsoft. All rights reserved.

export const allModes = {
  MD: {
    viewport: 'md',
  },
  LG: {
    viewport: 'lg',
  },
  XL: {
    viewport: 'xl',
  },
} as const;


--- scripts/badge_aggregation.js ---
// Copyright (c) Microsoft. All rights reserved.

/**
 * Aggregates the “latest completed” results of several dependent workflows and fails
 * this step if any required job/variant is missing or not successful.
 *
 * Usage (from actions/github-script@v8):
 *
 *   const badgeAggregation = require('./scripts/badge_aggregation.js');
 *   const dependencies = [
 *     { workflow: 'examples-calc-x.yml', label: 'calc-x.latest', variants: ['latest'] },
 *     { workflow: 'examples-spider.yml', label: 'spider.latest', variants: ['latest'] },
 *     { workflow: 'examples-apo.yml', label: 'apo.latest', variants: ['latest'] },
 *     { workflow: 'examples-unsloth.yml', label: 'unsloth.latest', variants: ['latest'] },
 *     { workflow: 'tests-full.yml', label: 'tests-full.latest', variants: ['latest'] },
 *   ];
 *   await badgeAggregation({ github, context, core, dependencies });
 *
 * Notes:
 * - Requires the workflow files above to exist in .github/workflows/.
 * - Looks at the default branch "main" unless you override per dependency with dep.branch.
 * - Assumes matrix job names contain the variant in parentheses, e.g. "tests (latest)".
 */
module.exports = async function badgeAggregation({ github, context, core, dependencies }) {
  const failures = [];

  // Defensive: validate inputs early for nicer error messages.
  if (!github?.rest?.actions || !context?.repo || !core) {
    throw new Error('badgeAggregation: expected { github, context, core } from actions/github-script.');
  }
  if (!Array.isArray(dependencies) || dependencies.length === 0) {
    core.info('No dependencies provided; nothing to check.');
    return;
  }

  // Helper: paginate jobs for a run attempt (handles >100 jobs edge case).
  async function listAllJobsForAttempt(run_id, attempt_number) {
    const all = [];
    let page = 1;
    while (true) {
      const { data } = await github.rest.actions.listJobsForWorkflowRunAttempt({
        owner: context.repo.owner,
        repo: context.repo.repo,
        run_id,
        attempt_number,
        per_page: 100,
        page,
      });
      const jobs = data.jobs ?? [];
      all.push(...jobs);
      if (!data.total_count || all.length >= data.total_count || jobs.length === 0) break;
      page += 1;
    }
    return all;
  }

  // For each dependency: find the latest completed run on the target branch; then inspect its jobs.
  for (const dep of dependencies) {
    const branch = dep.branch || 'main';

    // You can pass the workflow file name as workflow_id (e.g. "examples-apo.yml").
    const { data: runsData } = await github.rest.actions.listWorkflowRuns({
      owner: context.repo.owner,
      repo: context.repo.repo,
      workflow_id: dep.workflow,
      branch: 'main', // Always check the main branch status no matter what
      status: 'completed', // only completed runs
      per_page: 50, // retrieve latest 50 so we can filter
      sort: 'created',
      direction: 'desc',
    });

    const filteredRuns = runsData?.workflow_runs?.filter(run => ['schedule', 'workflow_dispatch'].includes(run.event));

    const run = filteredRuns?.[0];
    if (!run) {
      failures.push(`No completed run found for ${dep.label} on branch "${branch}"`);
      continue;
    }

    core.info(`[${dep.label}] Found run ${run.id} with attempt ${run.run_attempt}`);
    // Get the specific attempt we want to inspect (latest attempt for that run).
    const attempt = run.run_attempt ?? 1;

    // Robust: paginate jobs in case the workflow has many.
    const jobs = await listAllJobsForAttempt(run.id, attempt);
    core.info(`[${dep.label}] Found ${jobs.length} jobs: ${jobs.map(j => j.name).join(', ')}`);

    // Match each required variant to a job. We look for the variant in parentheses, e.g. "(latest)".
    for (const variant of dep.variants || []) {
      const matchingJobs = jobs.filter(
        j => typeof j.name === 'string' && j.name.includes(variant)
      );

      if (matchingJobs.length === 0) {
        failures.push(`Missing job for ${dep.label} (variant: ${variant})`);
        continue;
      }

      for (const job of matchingJobs) {
        core.info(`[${dep.label}] ${job.name} => ${job.conclusion}`);

        // Accept only a strict "success".
        if (job.conclusion !== 'success') {
          failures.push(`${dep.label} (${job.name}) concluded ${job.conclusion}`);
        }
      }
    }
  }

  // Surface aggregated result to the workflow.
  if (failures.length) {
    core.setFailed(failures.join(' | '));
  } else {
    core.info('All latest variants succeeded.');
  }
};


--- scripts/check_headers.py ---
# Copyright (c) Microsoft. All rights reserved.

"""Ensure tracked source files include the required copyright header."""

from __future__ import annotations

import subprocess
import sys
from pathlib import Path

HEADER_TEXT = "Copyright (c) Microsoft. All rights reserved."
REPO_ROOT = Path(__file__).resolve().parent.parent

COMMENT_PREFIX_BY_SUFFIX: dict[str, str] = {
    ".py": "#",
    ".pyi": "#",
    ".pyw": "#",
    ".js": "//",
    ".jsx": "//",
    ".ts": "//",
    ".tsx": "//",
    ".mjs": "//",
    ".mts": "//",
    ".cjs": "//",
    ".cts": "//",
}
REQUIRED_HEADER_BY_SUFFIX = {
    suffix: f"{prefix} {HEADER_TEXT}" if not prefix.endswith(" ") else f"{prefix}{HEADER_TEXT}"
    for suffix, prefix in COMMENT_PREFIX_BY_SUFFIX.items()
}


def iter_source_files() -> list[Path]:
    """Return tracked source files matching supported extensions."""
    if not REQUIRED_HEADER_BY_SUFFIX:
        return []

    pathspecs = [f"*{suffix}" for suffix in sorted(REQUIRED_HEADER_BY_SUFFIX)]
    result = subprocess.run(
        [
            "git",
            "ls-files",
            "--cached",
            "--others",
            "--exclude-standard",
            "--",
            *pathspecs,
        ],
        capture_output=True,
        text=True,
        check=True,
        cwd=REPO_ROOT,
    )
    return [REPO_ROOT / line.strip() for line in result.stdout.splitlines() if line.strip()]


def main() -> int:
    missing_header: list[str] = []
    missing_blank_line: list[str] = []

    for file_path in iter_source_files():
        expected_header = REQUIRED_HEADER_BY_SUFFIX.get(file_path.suffix.lower())
        if expected_header is None:
            continue

        if not file_path.exists():
            continue

        try:
            with file_path.open("r", encoding="utf-8") as file:
                first_line = file.readline().rstrip("\r\n")
                second_line = file.readline()
        except OSError as exc:
            print(f"Failed to read {file_path}: {exc}", file=sys.stderr)
            return 1

        if first_line != expected_header:
            missing_header.append(str(file_path.relative_to(REPO_ROOT)))
            continue

        # Second line should be either an EOF or a blank line
        if second_line and second_line.strip():
            missing_blank_line.append(str(file_path.relative_to(REPO_ROOT)))

    if missing_header:
        print("The following files are missing the required copyright header:")
        for path in missing_header:
            print(f" - {path}")
        header_examples = "\n".join(sorted(set(REQUIRED_HEADER_BY_SUFFIX.values())))
        print(f"Run the appropriate script or add the header manually:\n{header_examples}")

    if missing_blank_line:
        print("The following files are missing a blank line after the copyright header:")
        for path in missing_blank_line:
            print(f" - {path}")
        print("Ensure there is an empty line separating the header from the rest of the file.")

    if missing_header or missing_blank_line:
        return 1

    return 0


if __name__ == "__main__":
    sys.exit(main())


--- scripts/cleanup_aoai.py ---
# Copyright (c) Microsoft. All rights reserved.

import os

import requests
from openai import OpenAI

# Most common Azure OpenAI setup:
#   AZURE_OPENAI_ENDPOINT="https://<resource>.openai.azure.com"
#   AZURE_OPENAI_API_KEY="..."
# Optional (only if your endpoint requires it):
#   AZURE_OPENAI_API_VERSION="2025-xx-xx"
#
# This script treats "delete finetune job" as "cancel finetune job"
# because fine-tune jobs are typically cancellable, not deletable.


def _client() -> OpenAI:
    # This script assumes AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY are set in the environment.
    endpoint = os.environ["AZURE_OPENAI_ENDPOINT"]
    api_key = os.environ["AZURE_OPENAI_API_KEY"]
    return OpenAI(api_key=api_key, base_url=endpoint)


def list_data_files():
    c = _client()
    return c.files.list(limit=100)


def list_finetune_jobs():
    c = _client()
    return c.fine_tuning.jobs.list(limit=100)


def delete_data_file(file_id: str):
    c = _client()
    return c.files.delete(file_id)


def cancel_finetune_job(job_id: str):
    c = _client()
    return c.fine_tuning.jobs.cancel(job_id)


def delete_finetune_job(job_id: str):
    # This script assumes AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY are set in the environment.
    endpoint = os.environ["AZURE_OPENAI_ENDPOINT"].rstrip("/")
    api_key = os.environ["AZURE_OPENAI_API_KEY"]
    root = endpoint.split("/openai")[0]

    url = f"{root}/openai/fine_tuning/jobs/{job_id}"
    params = {"api-version": os.environ["AZURE_OPENAI_API_VERSION"]}

    resp = requests.delete(url, headers={"api-key": api_key}, params=params, timeout=60)
    resp.raise_for_status()
    return resp.content


if __name__ == "__main__":
    # Quick demo: print IDs you could delete
    jobs = list_finetune_jobs().data
    files = list_data_files().data

    print("JOBS:")
    for j in jobs:
        print(f"  {j.id}  {getattr(j, 'status', '')}  {getattr(j, 'model', '')}")

    print("\nFILES:")
    for f in files:
        print(f"  {f.id}  {getattr(f, 'filename', '')}  {getattr(f, 'status', '')}")

    # Delete them all WITHOUT CONFIRMATION!
    for j in jobs:
        print(f"Deleting job {j.id}")
        try:
            if j.status == "running":
                cancel_finetune_job(j.id)
            delete_finetune_job(j.id)
        except Exception as exc:
            print(f"  Error deleting job {j.id}: {exc}")

    for f in files:
        print(f"Deleting file {f.id}")
        try:
            delete_data_file(f.id)
        except Exception as exc:
            print(f"  Error deleting file {f.id}: {exc}")


--- scripts/litellm_sanity_check.py ---
# Copyright (c) Microsoft. All rights reserved.

"""Utility script to perform a sanity check on LiteLLM proxy server."""

import sys

import openai


def main() -> None:
    client = openai.OpenAI(timeout=30.0)
    models = client.models.list()
    print("Available models:", models)

    total_requests = 0
    success_count = 0

    for model in models.data:
        try:
            total_requests += 1
            response = client.chat.completions.create(
                model=model.id,
                messages=[{"role": "user", "content": "Hello!"}],
            )
            print(f"Chat completion from model {model.id}:", response)
            success_count += 1
        except Exception as e:
            print(f"Chat completion failed for model {model.id}: {e}")

        try:
            total_requests += 1
            response = client.responses.create(
                model=model.id,
                input="Hello, world!",
            )
            print(f"Response from model {model.id}:", response)
            success_count += 1
        except Exception as e:
            print(f"Response failed for model {model.id}: {e}")

    if total_requests == 0:
        print("No requests made.")
        sys.exit(1)

    success_rate = success_count / total_requests
    print(f"Success rate: {success_rate * 100:.2f}% ({success_count}/{total_requests})")

    if success_rate >= 0.8:
        sys.exit(0)
    else:
        sys.exit(1)


if __name__ == "__main__":
    main()


--- scripts/mongodb_init_rs_host.js ---
// Copyright (c) Microsoft. All rights reserved.

// MongoDB replica set initialization script.
// Use this if you are accessing MongoDB from the **host**.

rs.initiate({
  _id: "rs0",
  members: [{ _id: 0, host: "localhost:27017" }],
});


--- scripts/mongodb_init_rs_profiling.js ---
// Copyright (c) Microsoft. All rights reserved.

// MongoDB replica set initialization script.
// Use this if you are accessing MongoDB from another **container**.
// `mongodb_init_rs_host.js` is the counterpart if accessing from the host.

rs.initiate({
  _id: "rs0",
  members: [{ _id: 0, host: "mongo:27017" }],
});

db.setProfilingLevel(2);


--- scripts/wandb_download_result.py ---
# Copyright (c) Microsoft. All rights reserved.

"""Usage example:

python scripts/wandb_download_result.py AgentLightning \
    --runs spider_agl_v0_2 \
    --metrics training/reward val/reward \
    --out docs/assets/sql-agent-training-result.json \
    --step 16
"""

import argparse
import json
import sys
from typing import Any, Dict, List, Tuple

import numpy as np
import pandas as pd
import wandb


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(
        description=(
            "Fetch metrics from Weights & Biases runs and output Chart.js-ready JSON. "
            "Aggregates by step bins to tame long x-axes."
        )
    )
    p.add_argument(
        "project",
        help="W&B project name (e.g., 'my-project'). Uses your default entity unless --entity is set.",
    )
    p.add_argument(
        "--entity",
        default=None,
        help="W&B entity (team/user). If omitted, uses wandb.Api().default_entity.",
    )
    p.add_argument(
        "--runs",
        nargs="+",
        required=True,
        help="Run names (display names) to include. Example: --runs a b c",
    )
    p.add_argument(
        "--metrics",
        nargs="+",
        required=True,
        help="Metric keys to fetch. Example: --metrics train/loss val/acc",
    )
    p.add_argument(
        "--step",
        type=int,
        default=1,
        help="Aggregate step size in _step units (e.g., 16 groups steps into bins of 16). Default: 1 (no binning).",
    )
    p.add_argument(
        "--out",
        default="wandb_result.json",
        help="Output file name. Default: 'wandb_result.json'",
    )
    p.add_argument(
        "--label-format",
        default="{run}:{metric}",
        help="Dataset label format. You can use {run} and {metric}. Default: '{run}:{metric}'",
    )
    p.add_argument(
        "--strict",
        action="store_true",
        help="If set, exit with nonzero code when a run or metric is missing.",
    )
    return p.parse_args()


def fetch_runs(api: wandb.Api, entity: str, project: str, run_names: List[str]) -> Dict[str, wandb.Run]:
    """
    Fetch runs by displayName matching any in run_names.
    """
    name_set = set(run_names)
    found: Dict[str, wandb.Run] = {}

    # W&B filtering supports 'displayName'
    # We fetch all runs in the project once, then pick matching ones to be robust across filters/backends.
    # If the project is huge, you can optimize to paginate/stop early—here we walk until we’ve found all.
    for run in api.runs(f"{entity}/{project}"):
        dn = getattr(run, "name", None) or getattr(run, "displayName", None)
        # run.name is usually the short name; W&B Python public API exposes it as .name
        if dn in name_set and dn not in found:
            found[dn] = run
            if len(found) == len(name_set):
                break

    return found


def aggregate_history(df: pd.DataFrame, metrics: List[str], step: int) -> pd.DataFrame:
    """
    Given a history dataframe with '_step' and metric columns,
    aggregate by floor(_step/step)*step and average metric values per bin.
    """
    if "_step" not in df.columns:
        raise ValueError("History dataframe missing required '_step' column.")

    if step < 1:
        step = 1

    # Drop rows where all requested metrics are NaN to avoid empty bins
    keep_mask = df[metrics].notna().any(axis=1)
    df = df.loc[keep_mask].copy()

    # Compute bin: bin is rounded to the nearest multiples of step
    df["_bin"] = np.round(df["_step"] / step) * step

    # Group by bin and average each metric
    grouped = df.groupby("_bin", as_index=False)[metrics].mean()

    # Ensure bins are sorted
    grouped = grouped.sort_values("_bin").reset_index(drop=True)
    return grouped


def build_chartjs(
    per_run_metric_df: Dict[Tuple[str, str], pd.DataFrame],
    label_format: str,
) -> Dict[str, Any]:
    """
    Build a Chart.js line chart dataset:
      labels: union of all bins across runs (sorted)
      datasets: one per (run, metric) pair, aligned to labels, with None for missing points
    """
    # Union of all bins
    all_bins = set()
    for df in per_run_metric_df.values():
        all_bins.update(df["_bin"].tolist())
    labels = sorted(all_bins)

    # Chart.js wants arrays of primitive x labels (we'll use the bin starts)
    # If you want to render actual x=_step values, labels are these bin starts.
    datasets = []
    for (run_name, metric), df in per_run_metric_df.items():
        series_map = dict(zip(df["_bin"].tolist(), df[metric].tolist()))
        data = [series_map.get(b, None) for b in labels]
        datasets.append(
            {
                "label": label_format.format(run=run_name, metric=metric),
                "data": data,
                # Chart.js can infer styles; consumers can style further on the frontend
                "spanGaps": True,  # nicer lines across missing bins
            }
        )

    return {
        "type": "line",
        "data": {
            "labels": labels,
            "datasets": datasets,
        },
        "options": {
            "interaction": {"mode": "nearest", "intersect": False},
            "plugins": {
                "legend": {"display": True, "position": "top"},
                "title": {"display": True, "text": "W&B Metrics (binned by step)"},
            },
            "scales": {
                "x": {"title": {"display": True, "text": "Step (bin start)"}},
                "y": {"title": {"display": True, "text": "Value"}},
            },
        },
    }


def main():
    args = parse_args()

    api = wandb.Api()
    entity = args.entity or api.default_entity
    if not entity:
        print("::error::Unable to determine W&B entity. Pass --entity.", file=sys.stderr)
        sys.exit(1)

    runs = fetch_runs(api, entity, args.project, args.runs)
    missing = [r for r in args.runs if r not in runs]
    if missing:
        msg = f"Runs not found: {', '.join(missing)}"
        if args.strict:
            print(f"::error::{msg}", file=sys.stderr)
            sys.exit(1)
        else:
            print(f"::warning::{msg}", file=sys.stderr)

    if not runs:
        print("::error::No matching runs found.", file=sys.stderr)
        sys.exit(1)

    per_run_metric_df: Dict[Tuple[str, str], pd.DataFrame] = {}

    for run_name, run in runs.items():
        # Fetch each metric separately to avoid losing sparse metrics due to row intersection.
        for metric in args.metrics:
            hist = run.history(keys=["_step", metric], pandas=True)
            if hist is None or hist.empty:
                msg = f"No history for run '{run_name}' (metric '{metric}')."
                if args.strict:
                    print(f"::error::{msg}", file=sys.stderr)
                    sys.exit(1)
                else:
                    print(f"::warning::{msg}", file=sys.stderr)
                    continue
            # Ensure numeric _step
            if "_step" not in hist.columns:
                print(
                    f"::warning::Run '{run_name}' has no '_step' column; skipping metric '{metric}'.",
                    file=sys.stderr,
                )
                continue

            # Clean to numeric where possible
            hist["_step"] = pd.to_numeric(hist["_step"], errors="coerce")
            hist = hist.dropna(subset=["_step"])
            hist["_step"] = hist["_step"].astype(int)
            # Aggregate per metric; dense metrics can be tamed with --step (e.g., 16)
            grouped = aggregate_history(hist, [metric], args.step)
            if metric not in grouped.columns:
                msg = f"Metric '{metric}' not found in run '{run_name}'."
                if args.strict:
                    print(f"::error::{msg}", file=sys.stderr)
                    sys.exit(1)
                else:
                    print(f"::warning::{msg}", file=sys.stderr)
                    continue
            # Keep only _bin and the single metric for simpler merging later
            per_run_metric_df[(run_name, metric)] = grouped[["_bin", metric]].copy()

    if not per_run_metric_df:
        print("::error::No data collected for any run/metric.", file=sys.stderr)
        sys.exit(1)

    chart = build_chartjs(per_run_metric_df, args.label_format)

    payload = json.dumps(chart, ensure_ascii=False)
    if args.out:
        with open(args.out, "w", encoding="utf-8") as f:
            f.write(payload)
        print(f"Wrote Chart.js JSON to: {args.out}")
    else:
        print(payload)


if __name__ == "__main__":
    main()


--- tests/__init__.py ---
# Copyright (c) Microsoft. All rights reserved.


--- tests/test_client.py ---
# Copyright (c) Microsoft. All rights reserved.

# type: ignore

import asyncio
import time
from typing import Any, AsyncGenerator, Dict

import pytest
import pytest_asyncio
from httpx import AsyncClient

from agentlightning import (
    LLM,
    AgentLightningClient,
    AgentLightningServer,
    NamedResources,
    PromptTemplate,
    ResourcesUpdate,
    RolloutLegacy,
    Task,
    Triplet,
)
from agentlightning.client import DevTaskLoader


@pytest.fixture
def sample_resources() -> NamedResources:
    """Provides a sample NamedResources object for testing."""
    return {
        "main_llm": LLM(
            endpoint="http://localhost:8080/v1/chat/completions",
            model="gpt-4o",
            sampling_parameters={"temperature": 0.8, "max_tokens": 500},
        ),
        "system_prompt": PromptTemplate(template="You are a master of {domain}.", engine="f-string"),
    }


@pytest.fixture
def sample_task_input() -> Dict[str, Any]:
    """Provides a sample input for a task."""
    return {"prompt": "Tell me about the Roman Empire."}


@pytest_asyncio.fixture
async def server_setup() -> AsyncGenerator[Dict[str, Any], None]:
    """
    A pytest fixture to manage the lifecycle of the AgentLightningServer.
    It starts the server before a test runs and stops it afterward,
    providing the server instance and a test client to the test function.
    """
    server = AgentLightningServer(host="127.0.0.1", port=8008, task_timeout_seconds=2.0)
    await server.start()

    # httpx.AsyncClient is used for direct HTTP requests to the server endpoints
    # within the tests, which is more reliable than spinning up a full AgentLightningClient
    # for every single check.
    async with AsyncClient(base_url=server.endpoint) as http_client:
        yield {
            "server": server,
            "http_client": http_client,
            "endpoint": server.endpoint,
        }

    await server.stop()


@pytest.mark.asyncio
async def test_uri_and_semantic_correctness(server_setup: Dict[str, Any], sample_resources: NamedResources):
    """
    Ensures that client and server URIs match and that data models
    are serialized and deserialized correctly without semantic loss.
    """
    server: AgentLightningServer = server_setup["server"]
    endpoint: str = server_setup["endpoint"]

    # 1. Update resources on the server
    resources_id = await server.update_resources(sample_resources)
    assert isinstance(resources_id, str)

    # 2. Initialize a client pointing to the server
    client = AgentLightningClient(endpoint=endpoint)

    # 3. Fetch latest resources and verify integrity
    # This checks the /resources/latest URI and the ResourcesUpdate model
    latest_res_update = await client.get_latest_resources_async()
    assert latest_res_update is not None
    assert latest_res_update.resources_id == resources_id
    assert latest_res_update.resources["main_llm"].model == "gpt-4o"
    assert isinstance(latest_res_update.resources["system_prompt"], PromptTemplate)
    assert latest_res_update.resources["system_prompt"].engine == "f-string"

    # 4. Fetch resources by ID and verify integrity
    # This checks the /resources/{resource_id} URI
    specific_res_update = await client.get_resources_by_id_async(resources_id)
    assert specific_res_update is not None
    assert specific_res_update.model_dump() == latest_res_update.model_dump()


@pytest.mark.asyncio
async def test_full_lifecycle_async(
    server_setup: Dict[str, Any], sample_resources: NamedResources, sample_task_input: Dict[str, Any]
):
    """
    Tests the complete asynchronous workflow:
    1. Server queues a task.
    2. Async client polls for the task.
    3. Async client fetches resources for the task.
    4. Async client posts a completed rollout.
    5. Server retrieves the rollout.
    """
    server: AgentLightningServer = server_setup["server"]
    endpoint: str = server_setup["endpoint"]
    client = AgentLightningClient(endpoint=endpoint, poll_interval=0.1)

    # 1. Server updates resources and queues a task
    resources_id = await server.update_resources(sample_resources)
    rollout_id = await server.queue_task(sample=sample_task_input, mode="train", resources_id=resources_id)

    # 2. Client polls for the task
    task = await client.poll_next_task_async()
    assert task is not None
    assert task.rollout_id == rollout_id
    assert task.input == sample_task_input
    assert task.resources_id == resources_id  # Task is correctly associated with latest resources

    # 3. Client fetches resources
    res_update = await client.get_resources_by_id_async(task.resources_id)
    assert res_update is not None
    assert res_update.resources_id == resources_id

    # 4. Client posts a completed rollout
    rollout_payload = RolloutLegacy(
        rollout_id=rollout_id,
        final_reward=0.95,
        triplets=[Triplet(prompt="q", response="a", reward=1.0)],
        metadata={"client_version": "1.0"},
    )
    response = await client.post_rollout_async(rollout_payload)
    assert response is not None
    assert response.get("status") == "ok"

    # 5. Server retrieves the rollout and verifies its content
    completed_rollout = await server.get_completed_rollout(rollout_id)
    assert completed_rollout is not None
    assert completed_rollout.rollout_id == rollout_id
    assert completed_rollout.final_reward == 0.95
    assert completed_rollout.triplets[0].response == "a"
    assert completed_rollout.metadata == {"client_version": "1.0"}


@pytest.mark.asyncio
async def test_full_lifecycle_sync(
    server_setup: Dict[str, Any], sample_resources: NamedResources, sample_task_input: Dict[str, Any]
):
    """
    Tests the complete synchronous workflow in a separate thread to avoid
    blocking the asyncio event loop.
    """
    server: AgentLightningServer = server_setup["server"]
    endpoint: str = server_setup["endpoint"]
    client = AgentLightningClient(endpoint=endpoint, poll_interval=0.1)

    # 1. Server updates resources and queues a task
    resources_id = await server.update_resources(sample_resources)
    rollout_id = await server.queue_task(sample=sample_task_input, mode="val")

    # Define the synchronous client workflow
    def sync_client_workflow():
        # 2. Client polls for the task
        task = client.poll_next_task()
        assert task is not None
        assert task.rollout_id == rollout_id
        assert task.input == sample_task_input
        assert task.resources_id is None

        # 3. Client fetches resources
        res_update = client.get_latest_resources()
        assert res_update is not None
        assert res_update.resources_id == resources_id

        # 4. Client posts a completed rollout
        rollout_payload = RolloutLegacy(rollout_id=rollout_id, final_reward=0.88)
        response = client.post_rollout(rollout_payload)
        assert response is not None
        assert response.get("status") == "ok"

    # Run the sync workflow in a thread
    await asyncio.to_thread(sync_client_workflow)

    # 5. Server retrieves the rollout and verifies
    completed_rollout = await server.get_completed_rollout(rollout_id)
    assert completed_rollout is not None
    assert completed_rollout.final_reward == 0.88
    assert completed_rollout.rollout_id == rollout_id


@pytest.mark.asyncio
async def test_task_timeout_and_requeue(server_setup: Dict[str, Any]):
    """
    Tests that a task is correctly re-queued if a client claims it but does not
    complete it within the timeout period.
    """
    server: AgentLightningServer = server_setup["server"]
    http_client: AsyncClient = server_setup["http_client"]

    # 1. Queue a task
    rollout_id = await server.queue_task(sample={"data": "stale_test"})

    # 2. Client 1 gets the task
    response1 = await http_client.get("/task")
    assert response1.status_code == 200
    task1_data = response1.json()
    assert task1_data["is_available"] is True
    assert task1_data["task"]["rollout_id"] == rollout_id
    assert task1_data["task"]["num_claims"] == 1

    # 3. No more tasks are available immediately
    response2 = await http_client.get("/task")
    assert response2.json()["is_available"] is False

    # 4. Wait for the task to time out (server timeout is 2.0s)
    await asyncio.sleep(2.5)

    # 5. The timeout check is triggered by the next call to /task.
    # Client 2 should now receive the same task, but re-claimed.
    response3 = await http_client.get("/task")
    assert response3.status_code == 200
    task2_data = response3.json()
    assert task2_data["is_available"] is True
    assert task2_data["task"]["rollout_id"] == rollout_id
    assert task2_data["task"]["num_claims"] == 2  # The key assertion
    assert task2_data["task"]["last_claim_time"] > task1_data["task"]["last_claim_time"]


@pytest.mark.asyncio
async def test_error_handling_no_resources(server_setup: Dict[str, Any]):
    """
    Tests that the server correctly returns a 404 error when a client
    requests resources before any have been set.
    """
    http_client: AsyncClient = server_setup["http_client"]

    # Request latest resources when none exist
    response_latest = await http_client.get("/resources/latest")
    assert response_latest.status_code == 404
    assert "No resources have been set" in response_latest.text

    # Request a specific resource that doesn't exist
    response_specific = await http_client.get("/resources/non-existent-id")
    assert response_specific.status_code == 404
    assert "not found" in response_specific.text


@pytest.mark.asyncio
async def test_client_with_bad_endpoint():
    """
    Ensures the client handles connection errors gracefully when the
    server endpoint is incorrect.
    """
    # Point client to a non-existent server
    client = AgentLightningClient(endpoint="http://127.0.0.1:9999", timeout=0.5)

    # Async methods should return None after failing to connect
    task = await client.get_latest_resources_async()
    assert task is None

    # Sync methods should also return None
    sync_resources = client.get_latest_resources()
    assert sync_resources is None


def test_local_client_core_functionality(sample_resources: NamedResources):
    """Test core DevTaskLoader functionality: initialization, polling, resources, and rollouts."""
    # Test initialization with TaskInput objects
    task_inputs = ["input1", {"prompt": "input2"}]
    client = DevTaskLoader(tasks=task_inputs, resources=sample_resources)

    assert len(client._tasks) == 2
    assert client._resources_update.resources_id == "local"
    assert client._resources_update.resources == sample_resources
    assert client.task_count == 0

    # Test polling TaskInput objects
    task1 = client.poll_next_task()
    assert isinstance(task1, Task)
    assert task1.rollout_id == "local_task_001"
    assert task1.input == "input1"
    assert task1.resources_id == "local"
    assert client.task_count == 1

    task2 = client.poll_next_task()
    assert task2.rollout_id == "local_task_002"
    assert task2.input == {"prompt": "input2"}
    assert client.task_count == 2

    # Test initialization with Task objects and ResourcesUpdate
    resources_update = ResourcesUpdate(
        resources_id="version123",
        resources=sample_resources,
        create_time=time.time(),
        update_time=time.time(),
        version=1,
    )
    tasks = [Task(rollout_id="existing_task", input="existing_input", resources_id="version123")]
    client2 = DevTaskLoader(tasks=tasks, resources=resources_update)

    task3 = client2.poll_next_task()
    assert task3.rollout_id == "existing_task"
    assert task3.input == "existing_input"

    # Test resource retrieval
    resources = client2.get_resources_by_id("version123")
    assert resources is not None
    assert resources.resources_id == "version123"
    assert resources.resources == sample_resources

    latest = client2.get_latest_resources()
    assert latest is not None
    assert latest.resources_id == "version123"

    # Test rollout posting
    rollout = RolloutLegacy(rollout_id="test_rollout", final_reward=0.9)
    result = client2.post_rollout(rollout)
    assert result is not None
    assert result["status"] == "received"
    assert result["rollout_id"] == "test_rollout"
    assert len(client2.rollouts) == 1
    assert client2.rollouts[0].rollout_id == "test_rollout"

    # Test repr
    repr_str = repr(client2)
    assert "DevTaskLoader" in repr_str
    assert "num_tasks=1" in repr_str


def test_local_client_error_handling(sample_resources: NamedResources):
    """Test DevTaskLoader error handling and edge cases."""
    # Empty tasks should raise error
    with pytest.raises(ValueError, match="DevTaskLoader requires at least one task"):
        DevTaskLoader(tasks=[], resources=sample_resources)

    # Mixed task types should raise error
    mixed_tasks = [Task(rollout_id="task1", input="input1"), "task_input2"]
    with pytest.raises(ValueError, match="All tasks must be either Task or TaskInput objects"):
        DevTaskLoader(tasks=mixed_tasks, resources=sample_resources)

    # Wrong resource ID should raise error
    resources_update = ResourcesUpdate(
        resources_id="version123",
        resources=sample_resources,
        create_time=time.time(),
        update_time=time.time(),
        version=1,
    )
    client = DevTaskLoader(tasks=["input1"], resources=resources_update)
    with pytest.raises(ValueError, match="Resource ID 'wrong_id' not found"):
        client.get_resources_by_id("wrong_id")

    # Polling beyond available tasks should cycle back
    single_task_client = DevTaskLoader(tasks=["only_task"], resources=sample_resources)
    task1 = single_task_client.poll_next_task()
    task2 = single_task_client.poll_next_task()
    assert task2.input == task1.input


@pytest.mark.asyncio
async def test_local_client_async_methods(sample_resources: NamedResources):
    """Test that DevTaskLoader async methods work correctly."""
    client = DevTaskLoader(tasks=["async_input"], resources=sample_resources)

    # Test all async methods delegate to sync versions
    task = await client.poll_next_task_async()
    assert task.input == "async_input"

    resources = await client.get_resources_by_id_async("local")
    assert resources is not None
    assert resources.resources_id == "local"

    latest = await client.get_latest_resources_async()
    assert latest is not None
    assert latest.resources_id == "local"

    rollout = RolloutLegacy(rollout_id="async_rollout", final_reward=0.8)
    result = await client.post_rollout_async(rollout)
    assert result is not None
    assert result["rollout_id"] == "async_rollout"
    assert len(client.rollouts) == 1
    assert client.rollouts[0].final_reward == 0.8


--- tests/test_config.py ---
# Copyright (c) Microsoft. All rights reserved.

# type: ignore

"""
This file is not carefully reviewed.
It is to ensure the *somewhat* correctness of the code in agentlightning/config.py.
It can contain logically erroroneous expected values.
Please do not use the file as a reference for the expected behavior of config.
"""

import argparse
import inspect
import sys
from typing import TypeVar  # Added for completeness if testing TypeVars directly
from typing import _GenericAlias  # type: ignore
from typing import (
    Any,
    Callable,
    Dict,
    List,
    Optional,
    Tuple,
    Type,
    Union,
    get_args,
    get_origin,
)
from unittest import mock  # For mock.patch.object, mock.call, MagicMock etc.

import pytest

from agentlightning import config

# TypeVar as used in the original code
CliConfigurable = Any
_C = TypeVar("_C", bound=CliConfigurable)


# --- Helper classes for testing ---
class SimpleConfig:
    def __init__(self, name: str, value: int = 10):
        self.name = name
        self.value = value


class ComplexConfig:
    def __init__(
        self,
        text: str,
        is_active: bool,
        count: Optional[int] = None,
        items: List[str] = [],  # Default mutable is usually discouraged, but for testing CLI it's okay.
        maybe_text: Optional[str] = "default_text",
        optional_list: Optional[List[int]] = None,
        list_of_optionals: List[Optional[str]] = [],
        any_param: Any = None,
        untyped_param="untyped_default",  # Parameter without type hint but with a default
    ):
        self.text = text
        self.is_active = is_active
        self.count = count
        self.items = items
        self.maybe_text = maybe_text
        self.optional_list = optional_list
        self.list_of_optionals = list_of_optionals
        self.any_param = any_param
        self.untyped_param = untyped_param


class NoInitParamsConfig:
    def __init__(self):
        pass


class OnlySelfConfig:  # For completeness, though __init__ without params is similar
    def __init__(self):
        pass


class RequiredOnlyConfig:
    def __init__(self, req_str: str, req_int: int):
        self.req_str = req_str
        self.req_int = req_int


class OptionalNoDefaultConfig:
    def __init__(self, opt_val: Optional[str]):  # No __init__ default
        self.opt_val = opt_val


# --- Tests for nullable_str ---
@pytest.mark.parametrize(
    "input_val, expected_output",
    [
        ("none", None),
        ("None", None),
        ("NONE", None),
        ("null", None),
        ("Null", None),
        ("NULL", None),
        ("~", None),
        ("nil", None),
        ("Nil", None),
        ("NIL", None),
        ("actual_string", "actual_string"),
        ("", ""),
        (" ", " "),
        ("NoneValue", "NoneValue"),  # Should not convert if not exact keyword
    ],
)
def test_nullable_str(input_val: str, expected_output: Optional[str]) -> None:
    """Tests the nullable_str function for various inputs."""
    assert config.nullable_str(input_val) == expected_output


# --- Tests for _str_to_bool ---
@pytest.mark.parametrize(
    "input_val, expected_output",
    [
        ("yes", True),
        ("Yes", True),
        ("YES", True),
        ("true", True),
        ("True", True),
        ("TRUE", True),
        ("t", True),
        ("T", True),
        ("y", True),
        ("Y", True),
        ("1", True),
        (True, True),  # Direct bool pass-through
        ("no", False),
        ("No", False),
        ("NO", False),
        ("false", False),
        ("False", False),
        ("FALSE", False),
        ("f", False),
        ("F", False),
        ("n", False),
        ("N", False),
        ("0", False),
        (False, False),  # Direct bool pass-through
    ],
)
def test_str_to_bool_valid(input_val: Union[str, bool], expected_output: bool) -> None:
    """Tests _str_to_bool with valid boolean string representations."""
    assert config._str_to_bool(input_val) == expected_output


@pytest.mark.parametrize("invalid_input", ["maybe", "2", "", " ", "trueish", "falsey"])
def test_str_to_bool_invalid(invalid_input: str) -> None:
    """Tests _str_to_bool with invalid inputs, expecting ArgumentTypeError."""
    with pytest.raises(argparse.ArgumentTypeError):
        config._str_to_bool(invalid_input)


# --- Tests for _get_param_type_details ---
@pytest.mark.parametrize(
    "annotation, expected_core_type, expected_is_optional, expected_is_list",
    [
        (str, str, False, False),
        (int, int, False, False),
        (bool, bool, False, False),
        (Any, Any, False, False),
        (Optional[str], str, True, False),
        (Union[str, None], str, True, False),  # Equivalent to Optional[str]
        (Optional[int], int, True, False),
        (List[str], List[str], False, True),  # core_type is List[str]
        (List[int], List[int], False, True),
        (Optional[List[str]], List[str], True, True),  # core_type is List[str], outer is Optional
        (Union[List[str], None], List[str], True, True),
        (List[Optional[str]], List[Optional[str]], False, True),  # core_type is List[Optional[str]]
        (Optional[List[Optional[int]]], List[Optional[int]], True, True),
        (inspect.Parameter.empty, inspect.Parameter.empty, False, False),  # No annotation
        (Union[int, str], Union[int, str], False, False),  # Non-optional Union
        (Tuple[str, int], Tuple[str, int], False, False),  # Tuple is not a List
        (Callable[[int], str], Callable[[int], str], False, False),
        (Dict[str, int], Dict[str, int], False, False),
        (List[Any], List[Any], False, True),
        (Optional[List[Any]], List[Any], True, True),
    ],
)
def test_get_param_type_details(
    annotation: Any, expected_core_type: Any, expected_is_optional: bool, expected_is_list: bool
) -> None:
    """Tests _get_param_type_details for various type annotations."""
    core_type, is_optional, is_list = config._get_param_type_details(annotation)

    # Handle comparison for complex types like Union by comparing origin and args
    if get_origin(expected_core_type) is Union and get_origin(core_type) is Union:
        assert get_origin(core_type) == get_origin(expected_core_type)
        # Sort args for comparison as order doesn't matter in Union and get_args might not preserve input order
        assert sorted(map(str, get_args(core_type))) == sorted(map(str, get_args(expected_core_type)))
    else:
        assert core_type == expected_core_type
    assert is_optional == expected_is_optional
    assert is_list == expected_is_list


# --- Tests for _determine_argparse_type_and_nargs ---
@pytest.mark.parametrize(
    "core_param_type, is_param_list, expected_kwargs",
    [
        # List cases
        (List[str], True, {"nargs": "*", "type": str}),
        (List[int], True, {"nargs": "*", "type": int}),
        (List[float], True, {"nargs": "*", "type": float}),
        (List[bool], True, {"nargs": "*", "type": config._str_to_bool}),
        (List[Optional[str]], True, {"nargs": "*", "type": config.nullable_str}),
        (List[Any], True, {"nargs": "*", "type": str}),  # Defaults to str for List[Any] items
        # Non-list cases
        (str, False, {"type": str}),
        (int, False, {"type": int}),
        (float, False, {"type": float}),
        (bool, False, {"type": config._str_to_bool}),
        (Optional[str], False, {"type": config.nullable_str}),  # Handled by re-checking details
        (Union[str, None], False, {"type": config.nullable_str}),  # Equivalent to Optional[str]
        (Any, False, {"type": str}),  # No specific argparse type for Any
        (inspect.Parameter.empty, False, {"type": str}),  # No specific type for unannotated
        # List of complex types (e.g. List[Dict]), type of item is treated as string
        (List[Dict[str, int]], True, {"nargs": "*", "type": str}),
    ],
)
def test_determine_argparse_type_and_nargs(
    core_param_type: Any, is_param_list: bool, expected_kwargs: Dict[str, Any]
) -> None:
    """Tests _determine_argparse_type_and_nargs for type and nargs mapping."""
    assert config._determine_argparse_type_and_nargs(core_param_type, is_param_list) == expected_kwargs


# --- Tests for _build_help_string ---
@pytest.mark.parametrize(
    "cls_name, param_name, core_type, is_optional, is_list, expected_help",
    [
        ("C", "p", str, False, False, "For C: 'p'. Inferred type: str."),
        ("C", "p", int, True, False, "For C: 'p'. Inferred type: Optional[int]."),
        ("C", "p_list", List[str], False, True, "For C: 'p_list'. Inferred type: List[str]."),
        ("C", "p_opt_list", List[int], True, True, "For C: 'p_opt_list'. Inferred type: Optional[List[int]]."),
        (
            "C",
            "p_list_opt",
            List[Optional[str]],
            False,
            True,
            "For C: 'p_list_opt'. Inferred type: List[Optional[str]].",
        ),
        ("C", "p_any", Any, False, False, "For C: 'p_any'. Inferred type: Any."),
        ("C", "p_opt_any", Any, True, False, "For C: 'p_opt_any'. Inferred type: Optional[Any]."),
        ("C", "p_list_any", List[Any], False, True, "For C: 'p_list_any'. Inferred type: List[Any]."),
        ("C", "p_empty", inspect.Parameter.empty, False, False, "For C: 'p_empty'. Inferred type: Any."),
    ],
)
def test_build_help_string(
    cls_name: str, param_name: str, core_type: Any, is_optional: bool, is_list: bool, expected_help: str
) -> None:
    """Tests _build_help_string for generating correct help messages."""
    assert config._build_help_string(cls_name, param_name, core_type, is_optional, is_list) == expected_help


# --- Tests for _add_argument_for_parameter ---
@pytest.fixture
def mock_parser():
    """Fixture to create a mock ArgumentParser."""
    parser = mock.MagicMock(spec=argparse.ArgumentParser)
    parser.add_argument = mock.MagicMock()
    return parser


def get_param_obj(cls, param_name):
    """Helper to get an inspect.Parameter object from a class's __init__."""
    sig = inspect.signature(cls.__init__)
    return sig.parameters[param_name]


@pytest.mark.parametrize(
    "param_name, cls, expected_cli_name_part, expected_argparse_kwargs_subset",
    [
        ("name", SimpleConfig, "simpleconfig.name", {"type": str, "required": True}),
        ("value", SimpleConfig, "simpleconfig.value", {"type": int, "default": 10}),
        ("text", ComplexConfig, "complexconfig.text", {"type": str, "required": True}),
        ("is_active", ComplexConfig, "complexconfig.is-active", {"type": config._str_to_bool, "required": True}),
        (
            "count",
            ComplexConfig,
            "complexconfig.count",
            {"type": config.nullable_int, "default": None},
        ),  # Optional[int]=None
        ("items", ComplexConfig, "complexconfig.items", {"type": str, "nargs": "*", "default": []}),
        (
            "maybe_text",
            ComplexConfig,
            "complexconfig.maybe-text",
            {"type": config.nullable_str, "default": "default_text"},
        ),
        ("optional_list", ComplexConfig, "complexconfig.optional-list", {"type": int, "nargs": "*", "default": None}),
        (
            "list_of_optionals",
            ComplexConfig,
            "complexconfig.list-of-optionals",
            {"type": config.nullable_str, "nargs": "*", "default": []},
        ),
        (
            "any_param",
            ComplexConfig,
            "complexconfig.any-param",
            {"default": None},
        ),  # Type Any implies no specific argparse type
        (
            "untyped_param",
            ComplexConfig,
            "complexconfig.untyped-param",
            {"type": str, "default": "untyped_default"},
        ),  # No type hint
        (
            "opt_val",
            OptionalNoDefaultConfig,
            "optionalnodefaultconfig.opt-val",
            {"type": config.nullable_str, "default": None},
        ),
    ],
)
def test_add_argument_for_parameter(
    mock_parser, param_name, cls, expected_cli_name_part, expected_argparse_kwargs_subset
):
    """Tests _add_argument_for_parameter for correct argument configuration."""
    param_obj = get_param_obj(cls, param_name)
    dest_name = f"{cls.__name__.lower()}_{param_name}"
    config._add_argument_for_parameter(mock_parser, cls, param_name, param_obj, dest_name)

    # Build expected CLI argument name (e.g., --simpleconfig.name)
    expected_cli_arg_name = f"--{expected_cli_name_part.replace('_', '-')}"

    # Check that add_argument was called
    assert mock_parser.add_argument.called

    # Get the actual call arguments
    actual_call_args, actual_call_kwargs = mock_parser.add_argument.call_args
    assert actual_call_args[0] == expected_cli_arg_name
    assert actual_call_kwargs["dest"] == dest_name

    # Check for the presence and correctness of specified kwargs
    for key, expected_value in expected_argparse_kwargs_subset.items():
        assert key in actual_call_kwargs, f"Key '{key}' not in argparse kwargs for {param_name}"
        assert actual_call_kwargs[key] == expected_value, f"Value for key '{key}' incorrect for {param_name}"

    # Check 'required' status carefully
    if expected_argparse_kwargs_subset.get("required", False):
        assert actual_call_kwargs.get("required") is True
    else:  # Not required or default makes it not required
        assert not actual_call_kwargs.get("required", False)  # Either 'required' is False or not present

    # Check help string was generated
    assert "help" in actual_call_kwargs
    assert isinstance(actual_call_kwargs["help"], str)


# --- Tests for _add_arguments_for_class ---
def test_add_arguments_for_class(mock_parser: Any) -> None:
    """Tests _add_arguments_for_class by checking calls to _add_argument_for_parameter."""
    class_arg_configs_maps = {}
    with mock.patch("agentlightning.config._add_argument_for_parameter") as mock_add_param_func:
        config._add_arguments_for_class(mock_parser, ComplexConfig, class_arg_configs_maps)

        assert ComplexConfig in class_arg_configs_maps
        init_params = inspect.signature(ComplexConfig.__init__).parameters
        expected_calls = 0
        for p_name, p_obj in init_params.items():
            if p_name == "self":
                continue
            expected_calls += 1
            dest = f"complexconfig_{p_name}"
            assert class_arg_configs_maps[ComplexConfig][p_name] == dest
            # Check if mock_add_param_func was called with these specific arguments
            found_call = any(
                call_args[0] == mock_parser
                and call_args[1] == ComplexConfig
                and call_args[2] == p_name
                and call_args[3] == p_obj
                and call_args[4] == dest
                for call_args, _ in mock_add_param_func.call_args_list
            )
            assert found_call, f"Expected call for {p_name} not found or with wrong arguments."
        assert mock_add_param_func.call_count == expected_calls


def test_add_arguments_for_class_no_init_params(mock_parser: Any) -> None:
    """Tests _add_arguments_for_class with a class having no __init__ parameters."""
    class_arg_configs_maps = {}
    with mock.patch("agentlightning.config._add_argument_for_parameter") as mock_add_param_func:
        config._add_arguments_for_class(mock_parser, NoInitParamsConfig, class_arg_configs_maps)
        mock_add_param_func.assert_not_called()
        assert class_arg_configs_maps[NoInitParamsConfig] == {}


def test_create_argument_parser() -> None:
    """Tests _create_argument_parser for basic parser properties."""
    parser = config._create_argument_parser()
    assert isinstance(parser, argparse.ArgumentParser)
    assert parser.description == "CLI configurator for application components."
    # The formatter_class is a type, so compare types
    assert type(parser.formatter_class) == type(argparse.ArgumentDefaultsHelpFormatter)


def test_instantiate_classes() -> None:
    """Tests _instantiate_classes with various argument types and defaults."""
    parsed_args = argparse.Namespace(
        simpleconfig_name="TestName",
        simpleconfig_value=99,
        complexconfig_text="Hello",
        complexconfig_is_active=True,
        complexconfig_count=None,  # Explicitly None
        complexconfig_items=["item1", "item2"],
        # maybe_text not provided, should use its default "default_text" from __init__
        # Argparse default for complexconfig_maybe_text would be "default_text"
        complexconfig_maybe_text="default_text",  # Simulate argparse providing the default
        complexconfig_optional_list=[1, 2, 3],
        complexconfig_list_of_optionals=["a", None, "b"],
        complexconfig_any_param=object(),
        complexconfig_untyped_param="custom_untyped",
    )
    # This simulates that 'complexconfig_maybe_text' was given a default by add_argument
    # and thus present in parsed_args.
    # If not provided on CLI and having __init__ default, argparse gives it default.

    class_arg_configs_maps = {
        SimpleConfig: {"name": "simpleconfig_name", "value": "simpleconfig_value"},
        ComplexConfig: {
            p: f"complexconfig_{p}" for p in inspect.signature(ComplexConfig.__init__).parameters if p != "self"
        },
    }

    instances = config._instantiate_classes(parsed_args, (SimpleConfig, ComplexConfig), class_arg_configs_maps)

    assert len(instances) == 2
    sc, cc = instances

    assert isinstance(sc, SimpleConfig)
    assert sc.name == "TestName"
    assert sc.value == 99

    assert isinstance(cc, ComplexConfig)
    assert cc.text == "Hello"
    assert cc.is_active is True
    assert cc.count is None
    assert cc.items == ["item1", "item2"]
    assert cc.maybe_text == "default_text"  # From __init__ via argparse default
    assert cc.optional_list == [1, 2, 3]
    assert cc.list_of_optionals == ["a", None, "b"]
    assert cc.any_param is parsed_args.complexconfig_any_param
    assert cc.untyped_param == "custom_untyped"


def test_instantiate_classes_error_handling(caplog):
    """Tests error logging during class instantiation failure."""

    class FailingConfig:
        def __init__(self, param):
            raise ValueError("Instantiaion failed")

    parsed_args = argparse.Namespace(failingconfig_param="value")
    class_arg_configs_maps = {FailingConfig: {"param": "failingconfig_param"}}

    with pytest.raises(ValueError, match="Instantiaion failed"):
        config._instantiate_classes(parsed_args, (FailingConfig,), class_arg_configs_maps)

    assert "Error instantiating FailingConfig" in caplog.text
    assert "{'param': 'value'}" in caplog.text  # constructor_args
    assert "Error: Instantiaion failed" in caplog.text


# --- Integration Tests for lightning_cli ---
def run_lightning_cli(classes_to_configure, cli_args_list):
    """Helper to run lightning_cli with mocked sys.argv."""
    # Prepend a dummy program name to cli_args_list for sys.argv
    with mock.patch.object(sys, "argv", ["test_program.py"] + cli_args_list):
        result = config.lightning_cli(*classes_to_configure)
        if not isinstance(result, tuple):
            return (result,)
        return result


def test_lightning_cli_no_classes():
    """Tests lightning_cli with no classes provided."""
    assert run_lightning_cli([], []) == tuple()


def test_lightning_cli_simple_config():
    """Tests lightning_cli with a simple class and various argument scenarios."""
    # Only required arg
    (sc1,) = run_lightning_cli([SimpleConfig], ["--simpleconfig.name", "MyName"])
    assert sc1.name == "MyName"
    assert sc1.value == 10  # Default from __init__

    # All args
    (sc2,) = run_lightning_cli([SimpleConfig], ["--simpleconfig.name", "Another", "--simpleconfig.value", "77"])
    assert sc2.name == "Another"
    assert sc2.value == 77


def test_lightning_cli_complex_config_types():
    """Tests lightning_cli with ComplexConfig, checking various type conversions."""
    cli_args = [
        "--complexconfig.text",
        "CLI Text",
        "--complexconfig.is-active",
        "yes",
        "--complexconfig.count",
        "123",
        "--complexconfig.items",
        "apple",
        "banana",
        "--complexconfig.maybe-text",
        "None",  # Test nullable_str
        "--complexconfig.optional-list",
        "10",
        "20",
        "30",
        "--complexconfig.list-of-optionals",
        "first",
        "nil",
        "third",
        "null",
        "--complexconfig.any-param",
        "AnythingGoes",
        "--complexconfig.untyped-param",
        "FromCLI",
    ]
    (cc,) = run_lightning_cli([ComplexConfig], cli_args)

    assert cc.text == "CLI Text"
    assert cc.is_active is True
    assert cc.count == 123
    assert cc.items == ["apple", "banana"]
    assert cc.maybe_text is None
    assert cc.optional_list == [10, 20, 30]
    assert cc.list_of_optionals == ["first", None, "third", None]
    assert cc.any_param == "AnythingGoes"
    assert cc.untyped_param == "FromCLI"


def test_lightning_cli_complex_config_defaults():
    """Tests that __init__ defaults are used if CLI args are not provided."""
    # Only provide required arguments for ComplexConfig
    cli_args = ["--complexconfig.text", "Minimal", "--complexconfig.is-active", "f"]
    (cc,) = run_lightning_cli([ComplexConfig], cli_args)

    assert cc.text == "Minimal"
    assert cc.is_active is False
    assert cc.count is None  # Default from __init__
    assert cc.items == []  # Default from __init__
    assert cc.maybe_text == "default_text"  # Default from __init__
    assert cc.optional_list is None  # Default from __init__
    assert cc.list_of_optionals == []  # Default from __init__
    assert cc.any_param is None  # Default from __init__
    assert cc.untyped_param == "untyped_default"  # Default from __init__


def test_lightning_cli_multiple_classes():
    """Tests configuring multiple classes simultaneously."""
    cli_args = [
        "--simpleconfig.name",
        "SC_Multi",
        "--simpleconfig.value",
        "5",
        "--complexconfig.text",
        "CC_Multi",
        "--complexconfig.is-active",
        "true",
        "--complexconfig.maybe-text",
        "NotNone",
    ]
    sc, cc = run_lightning_cli([SimpleConfig, ComplexConfig], cli_args)

    assert isinstance(sc, SimpleConfig)
    assert sc.name == "SC_Multi"
    assert sc.value == 5

    assert isinstance(cc, ComplexConfig)
    assert cc.text == "CC_Multi"
    assert cc.is_active is True
    assert cc.maybe_text == "NotNone"
    assert cc.items == []  # Default


def test_lightning_cli_missing_required_arg_exits(capsys):
    """Tests that argparse exits if a required argument is missing."""
    with pytest.raises(SystemExit) as e:
        run_lightning_cli([RequiredOnlyConfig], ["--requiredonlyconfig.req-int", "123"])
    assert e.value.code != 0  # Argparse exits with non-zero for error
    captured = capsys.readouterr()  # Capture stderr
    assert "the following arguments are required: --requiredonlyconfig.req-str" in captured.err


def test_lightning_cli_optional_no_default_behavior():
    """Tests Optional parameter without __init__ default."""
    # Not provided: should become None
    (cfg1,) = run_lightning_cli([OptionalNoDefaultConfig], [])
    assert cfg1.opt_val is None

    # Provided as "None" via nullable_str
    (cfg2,) = run_lightning_cli([OptionalNoDefaultConfig], ["--optionalnodefaultconfig.opt-val", "None"])
    assert cfg2.opt_val is None

    # Provided with a value
    (cfg3,) = run_lightning_cli([OptionalNoDefaultConfig], ["--optionalnodefaultconfig.opt-val", "ActualValue"])
    assert cfg3.opt_val == "ActualValue"


--- tests/test_env_var.py ---
# Copyright (c) Microsoft. All rights reserved.

from __future__ import annotations

import pytest

from agentlightning.env_var import (
    LightningEnvVar,
    resolve_bool_env_var,
    resolve_int_env_var,
    resolve_str_env_var,
)


def test_resolve_bool_env_var_override_takes_precedence(monkeypatch: pytest.MonkeyPatch) -> None:
    env_name = LightningEnvVar.AGL_MANAGED_STORE.value
    monkeypatch.setenv(env_name, "0")

    assert resolve_bool_env_var(LightningEnvVar.AGL_MANAGED_STORE, override=True, fallback=False) is True


@pytest.mark.parametrize(
    ("raw_value", "expected"),
    [
        ("1", True),
        (" YES ", True),
        ("on", True),
        ("0", False),
        ("no", False),
        ("Off", False),
    ],
)
def test_resolve_bool_env_var_parses_truthy_and_falsy_values(
    monkeypatch: pytest.MonkeyPatch, raw_value: str, expected: bool
) -> None:
    env_name = LightningEnvVar.AGL_MANAGED_STORE.value
    monkeypatch.setenv(env_name, raw_value)

    assert resolve_bool_env_var(LightningEnvVar.AGL_MANAGED_STORE, fallback=not expected) is expected


def test_resolve_bool_env_var_returns_fallback_when_unset(monkeypatch: pytest.MonkeyPatch) -> None:
    env_name = LightningEnvVar.AGL_MANAGED_STORE.value
    monkeypatch.delenv(env_name, raising=False)

    assert resolve_bool_env_var(LightningEnvVar.AGL_MANAGED_STORE, fallback=False) is False


def test_resolve_bool_env_var_rejects_invalid_value(monkeypatch: pytest.MonkeyPatch) -> None:
    env_name = LightningEnvVar.AGL_MANAGED_STORE.value
    monkeypatch.setenv(env_name, "maybe")

    with pytest.raises(ValueError):
        resolve_bool_env_var(LightningEnvVar.AGL_MANAGED_STORE, fallback=False)


def test_resolve_int_env_var_reads_from_environment(monkeypatch: pytest.MonkeyPatch) -> None:
    env_name = LightningEnvVar.AGL_SERVER_PORT.value
    monkeypatch.setenv(env_name, "1234")

    assert resolve_int_env_var(LightningEnvVar.AGL_SERVER_PORT, fallback=4747) == 1234


def test_resolve_int_env_var_invalid_input(monkeypatch: pytest.MonkeyPatch) -> None:
    env_name = LightningEnvVar.AGL_SERVER_PORT.value
    monkeypatch.setenv(env_name, "not-a-number")

    with pytest.raises(ValueError):
        resolve_int_env_var(LightningEnvVar.AGL_SERVER_PORT, fallback=4747)


def test_resolve_str_env_var_override_and_fallback(monkeypatch: pytest.MonkeyPatch) -> None:
    env_name = LightningEnvVar.AGL_CURRENT_ROLE.value
    monkeypatch.setenv(env_name, "client")

    assert resolve_str_env_var(LightningEnvVar.AGL_CURRENT_ROLE, override="server", fallback="both") == "server"

    monkeypatch.delenv(env_name, raising=False)
    assert resolve_str_env_var(LightningEnvVar.AGL_CURRENT_ROLE, fallback="both") == "both"


--- tests/test_logging.py ---
# Copyright (c) Microsoft. All rights reserved.

from __future__ import annotations

import io
import logging
import multiprocessing as mp
from multiprocessing.queues import Queue
from pathlib import Path
from typing import Any, Dict, List

import pytest

from agentlightning.logging import _to_level_value  # pyright: ignore[reportPrivateUsage]
from agentlightning.logging import (
    DATE_FORMAT,
    DEFAULT_FORMAT,
)

pytestmark = pytest.mark.utils


def _logging_worker(case: str, queue: Queue[Dict[str, Any]]) -> None:
    """
    Runs in a separate process using spawn. It performs a specific logging
    configuration scenario and returns a summary dict via the queue.
    """
    import logging
    import warnings

    # Re-import inside the subprocess so everything is picklable & isolated
    from agentlightning.logging import (
        setup,
        setup_module,
    )

    if case == "setup_module_plain_console":
        logger = setup_module(
            level="DEBUG",
            name="agentlightning.test",
            console=True,
            color=False,
            propagate=False,
        )

        handlers = logger.handlers
        handler = handlers[0] if handlers else None
        fmt = handler.formatter._fmt if handler and handler.formatter else None
        datefmt = handler.formatter.datefmt if handler and handler.formatter else None

        queue.put(
            {
                "logger_name": logger.name,
                "logger_level": logger.level,
                "num_handlers": len(handlers),
                "handler_class": handler.__class__.__name__ if handler else None,
                "handler_level": handler.level if handler else None,
                "fmt": fmt,
                "datefmt": datefmt,
            }
        )

    elif case == "setup_module_color_rich":
        # Rich variant: color=True uses RichHandler
        logger = setup_module(
            level="INFO",
            name="agentlightning.rich",
            console=True,
            color=True,
            propagate=False,
        )
        handlers = logger.handlers
        handler = handlers[0] if handlers else None

        queue.put(
            {
                "logger_name": logger.name,
                "logger_level": logger.level,
                "num_handlers": len(handlers),
                "handler_class": handler.__class__.__name__ if handler else None,
                "handler_has_formatter": handler.formatter is not None if handler else None,
            }
        )

    elif case == "setup_with_submodules_apply_to_capture_warnings":
        # Extra handler to attach via extra_handlers
        stream = io.StringIO()
        stream_handler = logging.StreamHandler(stream)

        setup(
            level="INFO",
            console=False,
            color=False,
            propagate=False,
            disable_existing_loggers=False,
            capture_warnings=True,
            submodule_levels={"agentlightning.io": "DEBUG"},
            extra_handlers=[stream_handler],
            apply_to=["external"],
        )

        base = logging.getLogger("agentlightning")
        sub = logging.getLogger("agentlightning.io")
        ext = logging.getLogger("external")

        # Capture warnings via logging after capture_warnings=True
        class ListHandler(logging.Handler):
            def __init__(self) -> None:
                super().__init__()
                self.records: List[logging.LogRecord] = []

            def emit(self, record: logging.LogRecord) -> None:
                self.records.append(record)

        lh = ListHandler()
        wlog = logging.getLogger("py.warnings")
        wlog.handlers.clear()
        wlog.addHandler(lh)
        wlog.setLevel(logging.WARNING)
        wlog.propagate = False

        warnings.warn("from warnings", UserWarning)

        queue.put(
            {
                "base_level": base.level,
                "base_num_handlers": len(base.handlers),
                "extra_in_base": stream_handler in base.handlers,
                "sub_level": sub.level,
                "ext_level": ext.level,
                "ext_handlers_same": base.handlers == ext.handlers,
                "ext_propagate": ext.propagate,
                "warnings_logged": len(lh.records),
            }
        )

    elif case == "setup_with_console_and_extra_handler":
        # Console + extra handler combination to test handler attachment
        stream = io.StringIO()
        extra_handler = logging.StreamHandler(stream)

        setup(
            level="WARNING",
            console=True,
            color=False,
            propagate=False,
            extra_handlers=[extra_handler],
        )

        base = logging.getLogger("agentlightning")
        handler_classes = [h.__class__.__name__ for h in base.handlers]
        has_extra = extra_handler in base.handlers

        queue.put(
            {
                "base_level": base.level,
                "num_handlers": len(base.handlers),
                "handler_classes": handler_classes,
                "has_extra": has_extra,
            }
        )

    else:
        queue.put({})


def _logging_worker_files_string(queue: Queue[Dict[str, Any]], base_dir: str) -> None:
    """
    Runs in a separate spawned process and configures logging with a single
    files=str path. Returns information about the attached FileHandler.
    """
    import logging
    import os

    from agentlightning.logging import setup

    log_path = os.path.join(base_dir, "logs", "agent.log")

    setup(
        level="INFO",
        console=False,
        color=False,
        propagate=False,
        files=log_path,
    )

    base = logging.getLogger("agentlightning")
    file_handlers = [h for h in base.handlers if isinstance(h, logging.FileHandler)]
    fh = file_handlers[0] if file_handlers else None

    fmt = fh.formatter._fmt if fh and fh.formatter else None
    datefmt = fh.formatter.datefmt if fh and fh.formatter else None

    queue.put(
        {
            "logger_level": base.level,
            "num_handlers": len(base.handlers),
            "num_file_handlers": len(file_handlers),
            "file_base": fh.baseFilename if fh else None,
            "file_level": fh.level if fh else None,
            "fmt": fmt,
            "datefmt": datefmt,
        }
    )


def _logging_worker_files_mapping(queue: Queue[Dict[str, Any]], base_dir: str) -> None:
    """
    Runs in a separate spawned process and configures logging with a files=dict
    mapping, then calls setup twice to verify idempotent FileHandler attachment.
    """
    import logging
    import os

    from agentlightning.logging import setup

    base_log = os.path.join(base_dir, "agent.log")
    external_log = os.path.join(base_dir, "external.log")

    files_mapping: Dict[str, str] = {
        "agentlightning": base_log,
        "external": external_log,
    }

    def file_handlers(logger: logging.Logger) -> list[logging.FileHandler]:
        return [h for h in logger.handlers if isinstance(h, logging.FileHandler)]

    # First setup call
    setup(
        level="DEBUG",
        console=False,
        color=False,
        propagate=False,
        files=files_mapping,
    )

    base_logger = logging.getLogger("agentlightning")
    ext_logger = logging.getLogger("external")

    base_fh_first = file_handlers(base_logger)
    ext_fh_first = file_handlers(ext_logger)

    # Second setup call with the same mapping should not add duplicate FileHandlers
    setup(
        level="DEBUG",
        console=False,
        color=False,
        propagate=False,
        files=files_mapping,
    )

    base_fh_second = file_handlers(base_logger)
    ext_fh_second = file_handlers(ext_logger)

    queue.put(
        {
            "base_level": base_logger.level,
            "ext_level": ext_logger.getEffectiveLevel(),
            "base_first_count": len(base_fh_first),
            "ext_first_count": len(ext_fh_first),
            "base_second_count": len(base_fh_second),
            "ext_second_count": len(ext_fh_second),
            "base_file_first": base_fh_first[0].baseFilename if base_fh_first else None,
            "ext_file_first": ext_fh_first[0].baseFilename if ext_fh_first else None,
            "base_file_second": base_fh_second[0].baseFilename if base_fh_second else None,
            "ext_file_second": ext_fh_second[0].baseFilename if ext_fh_second else None,
            # For sanity: capture handler levels as well
            "base_handler_level": base_fh_first[0].level if base_fh_first else None,
            "ext_handler_level": ext_fh_first[0].level if ext_fh_first else None,
        }
    )


def _run_case(case: str) -> Dict[str, Any]:
    """Helper to run a scenario in a spawn’ed process and fetch the result."""
    ctx = mp.get_context("spawn")
    q: Queue[Dict[str, Any]] = ctx.Queue()
    p = ctx.Process(target=_logging_worker, args=(case, q))
    p.start()
    result = q.get(timeout=10)
    p.join(timeout=10)
    assert p.exitcode == 0
    return result


def test_to_level_value_int_and_str() -> None:
    # direct, no multiprocessing needed
    assert _to_level_value(logging.DEBUG) == logging.DEBUG
    assert _to_level_value("info") == logging.INFO
    assert _to_level_value("WARNING") == logging.WARNING

    with pytest.raises(ValueError):
        _to_level_value("not-a-level")


def test_setup_module_plain_console_spawn() -> None:
    result = _run_case("setup_module_plain_console")

    assert result["logger_name"] == "agentlightning.test"
    assert result["logger_level"] == logging.DEBUG

    # Console handler with plain formatter configured
    assert result["num_handlers"] == 1
    assert result["handler_class"].endswith("StreamHandler")
    assert result["handler_level"] == logging.DEBUG
    assert result["fmt"] == DEFAULT_FORMAT
    assert result["datefmt"] == DATE_FORMAT


def test_setup_module_color_rich_spawn() -> None:
    # Only run this test if rich is installed
    pytest.importorskip("rich")

    result = _run_case("setup_module_color_rich")

    assert result["logger_name"] == "agentlightning.rich"
    assert result["logger_level"] == logging.INFO
    assert result["num_handlers"] == 1
    # We can’t rely on full module path, just the class name
    assert result["handler_class"].endswith("RichHandler")


def test_setup_with_submodules_apply_to_and_capture_warnings_spawn() -> None:
    result = _run_case("setup_with_submodules_apply_to_capture_warnings")

    # Base logger level and handler attachment
    assert result["base_level"] == logging.INFO
    assert result["base_num_handlers"] >= 1
    assert result["extra_in_base"] is True

    # Submodule level overridden
    assert result["sub_level"] == logging.DEBUG

    # apply_to logger mirrors base handlers & level, propagation disabled
    assert result["ext_level"] == logging.INFO
    assert result["ext_handlers_same"] is True
    assert result["ext_propagate"] is False

    # capture_warnings=True causes warnings.warn to go through logging
    assert result["warnings_logged"] >= 1


def test_setup_with_console_and_extra_handler_spawn() -> None:
    result = _run_case("setup_with_console_and_extra_handler")

    # Level propagated to base logger
    assert result["base_level"] == logging.WARNING

    # Both console handler and extra handler should be attached
    assert result["num_handlers"] >= 2
    assert any(cls.endswith("StreamHandler") for cls in result["handler_classes"])
    assert result["has_extra"] is True


def test_setup_files_string_spawn(tmp_path: Path) -> None:
    """
    Verifies that passing files as a string attaches a single FileHandler with
    the expected level and default formatter in a spawned process.
    """
    ctx = mp.get_context("spawn")
    q: Queue[Dict[str, Any]] = ctx.Queue()
    p = ctx.Process(target=_logging_worker_files_string, args=(q, str(tmp_path)))
    p.start()
    result = q.get(timeout=10)
    p.join(timeout=10)
    assert p.exitcode == 0

    assert result["logger_level"] == logging.INFO
    # We expect at least one handler and exactly one FileHandler
    assert result["num_handlers"] >= 1
    assert result["num_file_handlers"] == 1

    # Filename should be inside the tmp_path tree
    assert str(tmp_path) in result["file_base"]
    # FileHandler uses the base logger level
    assert result["file_level"] == logging.INFO

    # Default formatter applied by _ensure_file_handler
    assert result["fmt"] == DEFAULT_FORMAT
    assert result["datefmt"] == DATE_FORMAT


def test_setup_files_mapping_spawn(tmp_path: Path) -> None:
    """
    Verifies that passing files as a mapping attaches FileHandlers to each
    logger and that calling setup twice does not create duplicate handlers.
    """
    ctx = mp.get_context("spawn")
    q: Queue[Dict[str, Any]] = ctx.Queue()
    p = ctx.Process(target=_logging_worker_files_mapping, args=(q, str(tmp_path)))
    p.start()
    result = q.get(timeout=10)
    p.join(timeout=10)
    assert p.exitcode == 0

    # Base logger level is DEBUG
    assert result["base_level"] == logging.DEBUG

    # External's effective level is WARNING (inherited from root)
    assert result["ext_level"] == logging.WARNING

    # First setup: one FileHandler per logger
    assert result["base_first_count"] == 1
    assert result["ext_first_count"] == 1

    # Second setup: still one FileHandler per logger (idempotence)
    assert result["base_second_count"] == 1
    assert result["ext_second_count"] == 1

    # File paths are stable across calls
    assert result["base_file_first"] == result["base_file_second"]
    assert result["ext_file_first"] == result["ext_file_second"]

    # Paths should live under tmp_path
    assert str(tmp_path) in result["base_file_first"]
    assert str(tmp_path) in result["ext_file_first"]

    # Handler levels:
    # - base handler uses the base logger level (DEBUG)
    # - external handler uses external's effective level at creation (WARNING)
    assert result["base_handler_level"] == logging.DEBUG
    assert result["ext_handler_level"] == logging.WARNING


--- tests/store/conftest.py ---
# Copyright (c) Microsoft. All rights reserved.

from __future__ import annotations

import os
import time
from itertools import count
from typing import TYPE_CHECKING, Any, AsyncGenerator, Dict, List, Mapping, Sequence
from unittest.mock import Mock
from uuid import uuid4

import pytest
import pytest_asyncio
from opentelemetry.sdk.trace import ReadableSpan
from pydantic import BaseModel, Field
from pytest import FixtureRequest

from agentlightning.store import collection_based
from agentlightning.store.base import LightningStore
from agentlightning.store.collection import DequeBasedQueue, DictBasedKeyValue, KeyValue, ListBasedCollection, Queue
from agentlightning.store.collection.base import Collection
from agentlightning.store.memory import InMemoryLightningStore

if TYPE_CHECKING:
    from pymongo import AsyncMongoClient
    from pymongo.asynchronous.database import AsyncDatabase

__all__ = [
    "inmemory_store",
    "inmemory_debounced_store",
    "mongo_debounced_store",
    "debounced_store",
    "fake_time",
    "mock_readable_span",
    "sample_items",
    "sample_collection",
    "SampleItem",
    "QueueItem",
    "deque_queue",
    "dict_key_value",
    "dict_key_value_data",
    "temporary_mongo_database",
    "mongo_uri",
    "mongo_client_kwargs",
]


mongo_uri = os.getenv("AGL_TEST_MONGO_URI", "mongodb://localhost:27017/?replicaSet=rs0")
mongo_client_kwargs: Dict[str, Any] = {"serverSelectionTimeoutMS": 5000}


@pytest.fixture
def inmemory_store() -> InMemoryLightningStore:
    """Create a fresh InMemoryLightningStore instance."""
    return InMemoryLightningStore(scan_debounce_seconds=0)


@pytest.fixture
def inmemory_debounced_store(fake_time: _FakeTime) -> InMemoryLightningStore:
    """Create an InMemoryLightningStore configured with scan debouncing."""
    return InMemoryLightningStore(scan_debounce_seconds=5.0)


@pytest_asyncio.fixture
async def mongo_store(temporary_mongo_database: AsyncDatabase[Any]):
    """Fixture for MongoDB store implementation."""
    from agentlightning.store.mongo import MongoLightningStore

    db = MongoLightningStore(
        mongo_uri=mongo_uri,
        mongo_client_kwargs=mongo_client_kwargs,
        database_name=temporary_mongo_database.name,
        scan_debounce_seconds=0,
    )
    try:
        yield db
    finally:
        await db.close()


@pytest_asyncio.fixture
async def mongo_debounced_store(fake_time: _FakeTime, temporary_mongo_database: AsyncDatabase[Any]):
    """Fixture for MongoDB store implementation with scan debouncing."""
    from agentlightning.store.mongo import MongoLightningStore

    db = MongoLightningStore(
        mongo_uri=mongo_uri,
        mongo_client_kwargs=mongo_client_kwargs,
        database_name=temporary_mongo_database.name,
        scan_debounce_seconds=5.0,
    )
    try:
        yield db
    finally:
        await db.close()


@pytest.fixture(
    params=[
        "inmemory_store",
        pytest.param("mongo_store", marks=pytest.mark.mongo),
    ]
)
def store_fixture(request: FixtureRequest) -> AsyncGenerator[LightningStore, None]:
    """Parameterized fixture that provides different store implementations for testing."""
    return request.getfixturevalue(request.param)


@pytest.fixture(
    params=[
        "inmemory_debounced_store",
        pytest.param("mongo_debounced_store", marks=pytest.mark.mongo),
    ]
)
def debounced_store(request: FixtureRequest) -> LightningStore:
    """Parameterized fixture for debounced store implementations."""
    return request.getfixturevalue(request.param)


@pytest.fixture
def mock_readable_span() -> ReadableSpan:
    """Create a mock ReadableSpan for testing."""
    span = Mock()
    span.name = "test_span"
    context_counter = count(1)

    def _make_context() -> Mock:
        """Generate a distinct span context each time it is requested."""
        index = next(context_counter)
        context = Mock()
        context.trace_id = 111111
        context.span_id = 222222 + index
        context.is_remote = False
        context.trace_state = {}
        return context

    # Mock context
    span.get_span_context = Mock(side_effect=_make_context)

    # Mock other attributes
    span.parent = None
    # Fix mock status to return proper string values
    status_code_mock = Mock()
    status_code_mock.name = "OK"
    span.status = Mock(status_code=status_code_mock, description=None)
    span.attributes = {"test": "value"}
    span.events = []
    span.links = []
    span.start_time = time.time_ns()
    span.end_time = time.time_ns() + 1000000
    span.resource = Mock(attributes={}, schema_url="")

    return span


class SampleItem(BaseModel):
    partition: str
    index: int
    name: str
    status: str
    tags: List[str] = Field(default_factory=list)
    score: float | None = None
    rank: int | None = None
    updated_time: float | None = None
    payload: Dict[str, int] = Field(default_factory=dict)
    metadata: str | None = None


class QueueItem(BaseModel):
    idx: int


class _FakeTime:
    """Simple controllable clock for scan debouncing tests."""

    def __init__(self, start: float = 0.0) -> None:
        self._value = start

    def time(self) -> float:
        return self._value

    def set(self, value: float) -> None:
        self._value = value

    def advance(self, delta: float) -> None:
        self._value += delta


@pytest.fixture
def fake_time(monkeypatch: pytest.MonkeyPatch) -> _FakeTime:
    """Patch collection_based.time.time with a controllable clock."""
    controller = _FakeTime()
    monkeypatch.setattr(collection_based.time, "time", controller.time)
    return controller


@pytest_asyncio.fixture
async def mongo_client():
    from pymongo import AsyncMongoClient

    client = AsyncMongoClient[Any](mongo_uri, **mongo_client_kwargs)
    try:
        await client.admin.command("ping")
    except Exception as exc:  # depends on external service
        await client.close()
        raise RuntimeError(f"MongoDB not available: {exc}")

    try:
        yield client
    finally:
        await client.close()


@pytest_asyncio.fixture
async def temporary_mongo_database(mongo_client: AsyncMongoClient[Any]):
    """Yield a temporary MongoDB database for integration tests."""
    db_name = f"agentlightning-test-{uuid4().hex}"
    db = mongo_client[db_name]  # type: ignore
    try:
        yield db
    finally:
        await mongo_client.drop_database(db_name)


### Collection fixtures ###


@pytest.fixture()
def sample_items() -> List[SampleItem]:
    return [
        SampleItem(
            partition="alpha",
            index=1,
            name="urgent-phase-one",
            status="new",
            tags=["core", "urgent"],
            score=10.5,
            rank=3,
            updated_time=12.0,
            payload={"priority": 10},
            metadata="alpha-start",
        ),
        SampleItem(
            partition="alpha",
            index=2,
            name="phase-two",
            status="running",
            tags=["core"],
            score=5.0,
            rank=2,
            updated_time=None,
            payload={"priority": 5},
            metadata=None,
        ),
        SampleItem(
            partition="alpha",
            index=3,
            name="delayed-phase",
            status="blocked",
            tags=["delayed"],
            score=None,
            rank=5,
            updated_time=15.1,
            payload={"priority": 8},
            metadata="delayed-phase",
        ),
        SampleItem(
            partition="beta",
            index=1,
            name="beta-critical",
            status="new",
            tags=["beta", "urgent"],
            score=8.0,
            rank=1,
            updated_time=7.0,
            payload={"priority": 7},
            metadata="beta critical",
        ),
        SampleItem(
            partition="beta",
            index=2,
            name="beta optional",
            status="done",
            tags=["beta"],
            score=3.0,
            rank=None,
            updated_time=2.0,
            payload={"priority": 1},
            metadata="optional path",
        ),
        SampleItem(
            partition="gamma",
            index=1,
            name="gamma-phase",
            status="running",
            tags=[],
            score=9.5,
            rank=4,
            updated_time=None,
            payload={"priority": 9},
            metadata="gamma-phase data",
        ),
        SampleItem(
            partition="gamma",
            index=2,
            name="gamma-late",
            status="done",
            tags=["late", "core"],
            score=1.0,
            rank=6,
            updated_time=20.0,
            payload={"priority": 2},
            metadata="gamma late entry",
        ),
        SampleItem(
            partition="delta",
            index=1,
            name="delta misc",
            status="archived",
            tags=["misc"],
            score=4.2,
            rank=7,
            updated_time=11.0,
            payload={"priority": 3},
            metadata="delta misc block",
        ),
    ]


### Generic collection fixtures ###


@pytest.fixture()
def sample_collection_memory(sample_items: Sequence[SampleItem]) -> ListBasedCollection[SampleItem]:
    collection: Collection[SampleItem] = ListBasedCollection(list(sample_items), SampleItem, ("partition", "index"))
    setattr(collection, "_test_backend", "memory")
    return collection


@pytest_asyncio.fixture
async def sample_collection_mongo(temporary_mongo_database: AsyncDatabase[Any], sample_items: Sequence[SampleItem]):
    from agentlightning.store.collection.mongo import MongoBasedCollection, MongoClientPool

    async with MongoClientPool[Mapping[str, Any]](
        mongo_uri=mongo_uri, mongo_client_kwargs=mongo_client_kwargs
    ) as client_pool:
        collection = MongoBasedCollection(
            client_pool,
            temporary_mongo_database.name,
            "sample-items",
            "partition-123",
            ["partition", "index"],
            SampleItem,
        )
        await collection.insert(sample_items)
        setattr(collection, "_test_backend", "mongo")
        yield collection


@pytest.fixture(
    params=[
        "memory",
        pytest.param("mongo", marks=pytest.mark.mongo),
    ]
)
def sample_collection(request: pytest.FixtureRequest):
    backend = request.param
    return request.getfixturevalue("sample_collection_" + backend)


### Generic queue fixtures ###


@pytest.fixture
def deque_queue_memory() -> DequeBasedQueue[QueueItem]:
    return DequeBasedQueue(QueueItem, [QueueItem(idx=i) for i in range(3)])


@pytest_asyncio.fixture
async def deque_queue_mongo(temporary_mongo_database: AsyncDatabase[Any]):
    from agentlightning.store.collection.mongo import MongoBasedQueue, MongoClientPool

    async with MongoClientPool[Mapping[str, Any]](
        mongo_uri=mongo_uri, mongo_client_kwargs=mongo_client_kwargs
    ) as client_pool:
        queue = MongoBasedQueue[QueueItem](
            client_pool,
            temporary_mongo_database.name,
            "queue-items",
            "partition-1",
            QueueItem,
        )
        await queue.enqueue([QueueItem(idx=i) for i in range(3)])
        yield queue


@pytest.fixture(
    params=[
        "memory",
        pytest.param("mongo", marks=pytest.mark.mongo),
    ]
)
def deque_queue(request: pytest.FixtureRequest) -> AsyncGenerator[Queue[QueueItem], None]:
    backend = request.param
    return request.getfixturevalue("deque_queue_" + backend)


### Generic key-value fixtures ###


@pytest.fixture()
def dict_key_value_data() -> Dict[str, int]:
    return {"alpha": 1, "beta": 2}


@pytest.fixture()
def dict_key_value_memory(dict_key_value_data: Dict[str, int]) -> DictBasedKeyValue[str, int]:
    return DictBasedKeyValue(dict_key_value_data)


@pytest_asyncio.fixture
async def dict_key_value_mongo(temporary_mongo_database: AsyncDatabase[Any], dict_key_value_data: Dict[str, int]):
    from agentlightning.store.collection.mongo import MongoBasedKeyValue, MongoClientPool

    async with MongoClientPool[Mapping[str, Any]](
        mongo_uri=mongo_uri, mongo_client_kwargs=mongo_client_kwargs
    ) as client_pool:
        key_value = MongoBasedKeyValue[str, int](
            client_pool,
            temporary_mongo_database.name,
            "key-value-items",
            "partition-1",
            str,
            int,
        )
        for key, value in dict_key_value_data.items():
            await key_value.set(key, value)
        yield key_value


@pytest.fixture(
    params=[
        "memory",
        pytest.param("mongo", marks=pytest.mark.mongo),
    ]
)
def dict_key_value(request: pytest.FixtureRequest) -> AsyncGenerator[KeyValue[str, int], None]:
    backend = request.param
    return request.getfixturevalue("dict_key_value_" + backend)


--- tests/store/dummy_store.py ---
# Copyright (c) Microsoft. All rights reserved.

from typing import Any, Dict, List, Literal, Optional, Sequence, Tuple

from opentelemetry.sdk.trace import ReadableSpan

from agentlightning.store import LightningStoreCapabilities
from agentlightning.store.base import UNSET, LightningStore
from agentlightning.types import (
    Attempt,
    AttemptedRollout,
    AttemptStatus,
    EnqueueRolloutRequest,
    NamedResources,
    ResourcesUpdate,
    Rollout,
    RolloutConfig,
    RolloutStatus,
    Span,
    TaskInput,
    Worker,
)


class DummyLightningStore(LightningStore):
    def __init__(self, return_values: Dict[str, Any]) -> None:
        super().__init__()
        self.calls: List[tuple[str, tuple[Any, ...], Dict[str, Any]]] = []
        self.return_values = return_values

    @property
    def capabilities(self) -> LightningStoreCapabilities:
        return LightningStoreCapabilities(
            async_safe=True,
            thread_safe=False,
            zero_copy=False,
        )

    async def start_rollout(
        self,
        input: TaskInput,
        mode: Optional[str] = None,
        resources_id: Optional[str] = None,
        config: Optional[RolloutConfig] = None,
        metadata: Optional[Dict[str, Any]] = None,
        worker_id: Optional[str] = None,
    ) -> AttemptedRollout:
        self.calls.append(("start_rollout", (input, mode, resources_id, config, metadata, worker_id), {}))
        return self.return_values["start_rollout"]

    async def enqueue_rollout(
        self,
        input: TaskInput,
        mode: Optional[str] = None,
        resources_id: Optional[str] = None,
        config: Optional[RolloutConfig] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> Rollout:
        self.calls.append(("enqueue_rollout", (input, mode, resources_id, config, metadata), {}))
        return self.return_values["enqueue_rollout"]

    async def enqueue_many_rollouts(self, rollouts: Sequence[EnqueueRolloutRequest]) -> Sequence[Rollout]:
        self.calls.append(("enqueue_many_rollouts", (rollouts,), {}))
        return self.return_values["enqueue_many_rollouts"]

    async def dequeue_rollout(self, worker_id: Optional[str] = None) -> Optional[AttemptedRollout]:
        self.calls.append(("dequeue_rollout", (worker_id,), {}))
        return self.return_values["dequeue_rollout"]

    async def dequeue_many_rollouts(
        self,
        *,
        limit: int = 1,
        worker_id: Optional[str] = None,
    ) -> Sequence[AttemptedRollout]:
        self.calls.append(("dequeue_many_rollouts", (), {"limit": limit, "worker_id": worker_id}))
        return self.return_values["dequeue_many_rollouts"]

    async def start_attempt(self, rollout_id: str, worker_id: Optional[str] = None) -> AttemptedRollout:
        self.calls.append(("start_attempt", (rollout_id, worker_id), {}))
        return self.return_values["start_attempt"]

    async def query_rollouts(self, *args: Any, **kwargs: Any) -> List[Rollout]:
        self.calls.append(("query_rollouts", args, kwargs))
        return self.return_values["query_rollouts"]

    async def query_attempts(self, *args: Any, **kwargs: Any) -> List[Attempt]:
        self.calls.append(("query_attempts", args, kwargs))
        return self.return_values["query_attempts"]

    async def get_rollout_by_id(self, rollout_id: str) -> Optional[Rollout]:
        self.calls.append(("get_rollout_by_id", (rollout_id,), {}))
        return self.return_values["get_rollout_by_id"]

    async def get_latest_attempt(self, rollout_id: str) -> Optional[Attempt]:
        self.calls.append(("get_latest_attempt", (rollout_id,), {}))
        return self.return_values["get_latest_attempt"]

    async def add_resources(self, resources: NamedResources) -> ResourcesUpdate:
        self.calls.append(("add_resources", (resources,), {}))
        return self.return_values["add_resources"]

    async def update_resources(self, resources_id: str, resources: NamedResources) -> ResourcesUpdate:
        self.calls.append(("update_resources", (resources_id, resources), {}))
        return self.return_values["update_resources"]

    async def get_resources_by_id(self, resources_id: str) -> Optional[ResourcesUpdate]:
        self.calls.append(("get_resources_by_id", (resources_id,), {}))
        return self.return_values["get_resources_by_id"]

    async def get_latest_resources(self) -> Optional[ResourcesUpdate]:
        self.calls.append(("get_latest_resources", (), {}))
        return self.return_values["get_latest_resources"]

    async def query_resources(self, *args: Any, **kwargs: Any) -> List[ResourcesUpdate]:
        self.calls.append(("query_resources", args, kwargs))
        return self.return_values["query_resources"]

    async def add_span(self, span: Span) -> Optional[Span]:
        self.calls.append(("add_span", (span,), {}))
        return self.return_values["add_span"]

    async def add_many_spans(self, spans: Sequence[Span]) -> List[Span]:
        self.calls.append(("add_many_spans", (spans,), {}))
        return self.return_values["add_many_spans"]

    async def add_otel_span(
        self,
        rollout_id: str,
        attempt_id: str,
        readable_span: ReadableSpan,
        sequence_id: Optional[int] = None,
    ) -> Optional[Span]:
        self.calls.append(("add_otel_span", (rollout_id, attempt_id, readable_span, sequence_id), {}))
        return self.return_values["add_otel_span"]

    async def wait_for_rollouts(self, *, rollout_ids: List[str], timeout: Optional[float] = None) -> List[Rollout]:
        self.calls.append(("wait_for_rollouts", (), {"rollout_ids": rollout_ids, "timeout": timeout}))
        return self.return_values["wait_for_rollouts"]

    async def get_next_span_sequence_id(self, rollout_id: str, attempt_id: str) -> int:
        self.calls.append(("get_next_span_sequence_id", (rollout_id, attempt_id), {}))
        return self.return_values["get_next_span_sequence_id"]

    async def get_many_span_sequence_ids(self, rollout_attempt_ids: Sequence[Tuple[str, str]]) -> List[int]:
        self.calls.append(("get_many_span_sequence_ids", (rollout_attempt_ids,), {}))
        return self.return_values["get_many_span_sequence_ids"]

    async def query_spans(self, *args: Any, **kwargs: Any) -> List[Span]:
        self.calls.append(("query_spans", args, kwargs))
        return self.return_values["query_spans"]

    async def update_rollout(
        self,
        rollout_id: str,
        input: TaskInput | Any = UNSET,
        mode: Optional[str] | Any = UNSET,
        resources_id: Optional[str] | Any = UNSET,
        status: RolloutStatus | Any = UNSET,
        config: Any = UNSET,
        metadata: Optional[Dict[str, Any]] | Any = UNSET,
    ) -> Rollout:
        self.calls.append(
            (
                "update_rollout",
                (rollout_id, input, mode, resources_id, status, config, metadata),
                {},
            )
        )
        return self.return_values["update_rollout"]

    async def update_attempt(
        self,
        rollout_id: str,
        attempt_id: str | Literal["latest"],
        status: AttemptStatus | Any = UNSET,
        worker_id: str | Any = UNSET,
        last_heartbeat_time: float | Any = UNSET,
        metadata: Optional[Dict[str, Any]] | Any = UNSET,
    ) -> Attempt:
        self.calls.append(
            (
                "update_attempt",
                (rollout_id, attempt_id, status, worker_id, last_heartbeat_time, metadata),
                {},
            )
        )
        return self.return_values["update_attempt"]

    async def query_workers(self, *args: Any, **kwargs: Any) -> List[Worker]:
        self.calls.append(("query_workers", args, kwargs))
        return self.return_values["query_workers"]

    async def get_worker_by_id(self, worker_id: str) -> Optional[Worker]:
        self.calls.append(("get_worker_by_id", (worker_id,), {}))
        return self.return_values["get_worker_by_id"]

    async def update_worker(
        self,
        worker_id: str,
        heartbeat_stats: Dict[str, Any] | Any = UNSET,
    ) -> Worker:
        self.calls.append(
            (
                "update_worker",
                (
                    worker_id,
                    heartbeat_stats,
                ),
                {},
            )
        )
        return self.return_values["update_worker"]


def minimal_dummy_store() -> DummyLightningStore:
    # Provide minimal return values
    return DummyLightningStore(
        return_values={
            "start_rollout": None,
            "enqueue_rollout": None,
            "dequeue_rollout": None,
            "start_attempt": None,
            "query_rollouts": [],
            "query_attempts": [],
            "get_rollout_by_id": None,
            "get_latest_attempt": None,
            "add_resources": None,
            "update_resources": None,
            "get_resources_by_id": None,
            "get_latest_resources": None,
            "query_resources": [],
            "add_span": None,
            "add_many_spans": [],
            "add_otel_span": None,
            "wait_for_rollouts": [],
            "get_next_span_sequence_id": 0,
            "get_many_span_sequence_ids": [],
            "query_spans": [],
            "update_rollout": None,
            "update_attempt": None,
            "query_workers": [],
            "get_worker_by_id": None,
            "update_worker": Worker(worker_id="worker-0"),
        }
    )


--- tests/adapter/__init__.py ---
# Copyright (c) Microsoft. All rights reserved.


--- tests/algorithm/__init__.py ---
# Copyright (c) Microsoft. All rights reserved.


--- tests/common/__init__.py ---
# Copyright (c) Microsoft. All rights reserved.
