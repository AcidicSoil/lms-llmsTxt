# llms-full (private-aware)
> Built by GitHub fetches (raw or authenticated). Large files may be truncated.

--- examples/quickstart/mlflow_tracking.py ---
import os
from random import randint, random

from mlflow import log_artifacts, log_metric, log_param

if __name__ == "__main__":
    print("Running mlflow_tracking.py")

    log_param("param1", randint(0, 100))

    log_metric("foo", random())
    log_metric("foo", random() + 1)
    log_metric("foo", random() + 2)

    if not os.path.exists("outputs"):
        os.makedirs("outputs")
    with open("outputs/test.txt", "w") as f:
        f.write("hello world!")

    log_artifacts("outputs")


--- examples/demos/pythonmodel_type_hints_quickstart.ipynb ---
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66132127-faa2-43d7-a23d-94035f0dc6a4",
   "metadata": {},
   "source": [
    "# PythonModel with type hints Quickstart\n",
    "This notebook will demonstrates how to use type hints for data validation in MLflow PythonModel.\n",
    "\n",
    "## Prerequisite\n",
    "\n",
    "Install required packages: `pip install mlflow==2.20.0 openai==1.65.4`\n",
    "\n",
    "Set your OpenAI API key with `os.environ[\"OPENAI_API_KEY\"]=\"<YOUR_KEY>\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce58023-e1ab-4bf9-8325-0e706e327c54",
   "metadata": {},
   "source": [
    "## Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30503174-848c-4017-85f6-87f1b90fce70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-B83LveI6Wc2RHEdbMwNGkFh8ZoehY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='MLflow is an open-source platform designed to manage the machine learning (ML) lifecycle, which includes components such as experimentation, reproducibility, and deployment. It provides a suite of tools to help data scientists and machine learning practitioners track experiments, package and share their code, and deploy models. Here are the key components of MLflow:\\n\\n1. **MLflow Tracking**: This component allows users to log and query experiments. You can track metrics, parameters, and artifacts (such as model files) associated with different runs of your machine learning models.\\n\\n2. **MLflow Projects**: This functionality enables users to package their data science code in a reusable and reproducible way. Projects are defined with a standard format that specifies their dependencies and how to run them.\\n\\n3. **MLflow Models**: This part of MLflow provides a way to manage and deploy machine learning models in various formats. It supports multiple model types, enabling users to deploy models in different environments, such as REST APIs, cloud services, or on-premises servers.\\n\\n4. **MLflow Registry**: This is a centralized model store that manages the lifecycle of machine learning models, including versioning, stage transitions (like staging, production, archived), and annotations. It allows teams to track model changes and collaborate more effectively.\\n\\nMLflow is designed to be flexible and integrates well with existing machine learning frameworks like TensorFlow, PyTorch, Scikit-Learn, and many others. This flexibility makes it widely adopted in production ML workflows and among research communities. \\n\\nOverall, MLflow aims to streamline the process of developing, tracking, and deploying machine learning models, reducing friction and enhancing collaboration among data science teams.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1741259211, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_06737a9306', usage=CompletionUsage(completion_tokens=339, prompt_tokens=13, total_tokens=352, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import pydantic\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# Use OpenAI model\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is the MLflow?\"}]\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1522dd-089e-4908-be5d-08de29deb683",
   "metadata": {},
   "source": [
    "## Use MLflow PythonModel with type hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a960b6b-3dc7-47e3-ac4e-a50ef296cae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DSPy is a Python library designed for creating and managing decision systems, particularly in the context of data-driven applications. It provides tools for building models that can make decisions based on inputs from various data sources. The library aims to simplify the process of developing and deploying decision logic, which is often a complex task in machine learning and artificial intelligence projects.\\n\\nKey features of DSPy may include:\\n\\n1. **Declarative Syntax**: Allowing users to express decision logic in a clear and concise manner.\\n2. **Integration with Data Sources**: Facilitating easy integration with various data workflows, making it simpler to utilize datasets for decision-making.\\n3. **Evaluation and Testing**: Providing tools for evaluating and testing decision-making models, ensuring their accuracy and reliability.\\n\\nBy leveraging DSPy, data scientists and developers can focus on building effective decision systems without getting bogged down by the complexities usually associated with programming these systems from scratch.\\n\\nFor the latest updates and more specific functionalities, it's a good idea to refer to the official DSPy documentation or repository, as libraries are frequently updated and improved.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define your input schema\n",
    "class Message(pydantic.BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "# inherit mlflow PythonModel\n",
    "class MyModel(mlflow.pyfunc.PythonModel):\n",
    "    # add type hint to model_input\n",
    "    def predict(self, model_input: list[Message]) -> str:\n",
    "        response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=model_input)\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "model = MyModel()\n",
    "model.predict([{\"role\": \"user\", \"content\": \"What is DSPy?\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15d8e83c-3afa-4b75-80bf-1a8efa51a415",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MlflowException",
     "evalue": "Failed to validate data against type hint `list[Message]`, invalid elements: [('What is DSPy?', \"Expecting example to be a dictionary or pydantic model instance for Pydantic type hint, got <class 'str'>\")]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# An incorrect input will trigger validation error\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is DSPy?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/repos/mlflow/mlflow/pyfunc/utils/data_validation.py:68\u001b[0m, in \u001b[0;36m_wrap_predict_with_pyfunc.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, MlflowException):\n\u001b[0;32m---> 68\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to validate the input data against the type hint \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc_info\u001b[38;5;241m.\u001b[39minput_type_hint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m     )\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/repos/mlflow/mlflow/pyfunc/utils/data_validation.py:59\u001b[0m, in \u001b[0;36m_wrap_predict_with_pyfunc.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m         args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_model_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_input_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_type_hint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_param_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, MlflowException):\n",
      "File \u001b[0;32m~/Documents/repos/mlflow/mlflow/pyfunc/utils/data_validation.py:200\u001b[0m, in \u001b[0;36m_validate_model_input\u001b[0;34m(args, kwargs, model_input_index_in_sig, type_hint, model_input_param_name)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_pos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     data \u001b[38;5;241m=\u001b[39m _convert_data_to_type_hint(model_input, type_hint)\n\u001b[0;32m--> 200\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_data_against_type_hint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_hint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_pos \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    202\u001b[0m         kwargs[model_input_param_name] \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m~/Documents/repos/mlflow/mlflow/types/type_hints.py:452\u001b[0m, in \u001b[0;36m_validate_data_against_type_hint\u001b[0;34m(data, type_hint)\u001b[0m\n\u001b[1;32m    450\u001b[0m args \u001b[38;5;241m=\u001b[39m get_args(type_hint)\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m origin_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m--> 452\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_validate_list_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m origin_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _validate_dict_elements(element_type\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;241m1\u001b[39m], data\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[0;32m~/Documents/repos/mlflow/mlflow/types/type_hints.py:528\u001b[0m, in \u001b[0;36m_validate_list_elements\u001b[0;34m(element_type, data)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m invalid_elems:\n\u001b[1;32m    525\u001b[0m     invalid_elems_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_elems[:\u001b[38;5;241m5\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ... (truncated)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(invalid_elems) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m invalid_elems\n\u001b[1;32m    527\u001b[0m     )\n\u001b[0;32m--> 528\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException\u001b[38;5;241m.\u001b[39minvalid_parameter_value(\n\u001b[1;32m    529\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to validate data against type hint `list[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_type_hint_repr(element_type)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]`, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    530\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid elements: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_elems_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    531\u001b[0m     )\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mMlflowException\u001b[0m: Failed to validate data against type hint `list[Message]`, invalid elements: [('What is DSPy?', \"Expecting example to be a dictionary or pydantic model instance for Pydantic type hint, got <class 'str'>\")]"
     ]
    }
   ],
   "source": [
    "# An incorrect input will trigger validation error\n",
    "model.predict([\"What is DSPy?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5b9c51-d840-435b-9923-c4088087020b",
   "metadata": {},
   "source": [
    "## Model logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4bde4ec-992c-432d-aefa-3087e4a59861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/06 19:07:07 INFO mlflow.models.signature: Inferring model signature from type hints\n",
      "2025/03/06 19:07:07 INFO mlflow.models.signature: Running the predict function to generate output based on input example\n"
     ]
    }
   ],
   "source": [
    "# log the model\n",
    "with mlflow.start_run():\n",
    "    model_info = mlflow.pyfunc.log_model(name=\"model\", python_model=model, input_example=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "805d1dd8-b63a-4236-9f7e-1ab845c7a39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputs: \n",
       "  [{content: string (required), role: string (required)} (required)]\n",
       "outputs: \n",
       "  [string (required)]\n",
       "params: \n",
       "  None"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info.signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca4e0a8a-924d-4b84-88d3-4a1faa2c01a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MLflow is an open-source platform designed to manage the machine learning lifecycle. It provides tools for various stages of the machine learning process, including:\\n\\n1. **Experiment Tracking**: MLflow allows you to log and track experiments, enabling you to compare different runs and their performance metrics easily. You can log parameters, metrics, tags, and artifacts related to your models.\\n\\n2. **Projects**: MLflow Projects facilitate packaging and sharing code in a reusable format. This makes it easier to reproduce experiments and share your work with others.\\n\\n3. **Models**: MLflow Models provides a standard format for packaging machine learning models. It supports various flavors of models (e.g., TensorFlow, PyTorch, Scikit-learn) and allows you to deploy them to various environments (like Docker, cloud-based services, or local servers).\\n\\n4. **Registry**: The MLflow Model Registry provides a centralized repository to manage models, including versioning, annotation, and lifecycle management (staging, production, and archived statuses).\\n\\n5. **Integration**: MLflow integrates well with popular machine learning frameworks and libraries, making it a versatile choice for data scientists and machine learning engineers.\\n\\nBy using MLflow, teams can streamline their machine learning workflows, enhance collaboration, and ensure reproducibility in their experiments, leading to more efficient model development and deployment.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the pyfunc model\n",
    "pyfunc_model = mlflow.pyfunc.load_model(model_info.model_uri)\n",
    "# the same validation works for pyfunc model predict\n",
    "pyfunc_model.predict(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b920f0d1-4c89-43fb-bdc9-e69702b059b3",
   "metadata": {},
   "source": [
    "## Verify model before deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c702c097-19aa-4530-a075-fd5c676a0d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41f48f443de4b5c810b46704358b815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/06 19:10:16 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\n",
      "2025/03/06 19:10:16 INFO mlflow.utils.virtualenv: Creating a new environment in /var/folders/9g/psrbbvm92t712cy09d7_00d00000gp/T/tmpp18g94f9/envs/virtualenv_envs/mlflow-9d81fff15053e6e06e2edaefcc9e075d6c04a094 with python version 3.9.18 using uv\n",
      "Using CPython 3.9.18 interpreter at: \u001b[36m/Users/serena.ruan/miniconda3/envs/mlflow/bin/python3.9\u001b[39m\n",
      "Creating virtual environment at: \u001b[36m/var/folders/9g/psrbbvm92t712cy09d7_00d00000gp/T/tmpp18g94f9/envs/virtualenv_envs/mlflow-9d81fff15053e6e06e2edaefcc9e075d6c04a094\u001b[39m\n",
      "Activate with: \u001b[32msource /var/folders/9g/psrbbvm92t712cy09d7_00d00000gp/T/tmpp18g94f9/envs/virtualenv_envs/mlflow-9d81fff15053e6e06e2edaefcc9e075d6c04a094/bin/activate\u001b[39m\n",
      "2025/03/06 19:10:16 INFO mlflow.utils.virtualenv: Installing dependencies\n",
      "\u001b[2mUsing Python 3.9.18 environment at: /var/folders/9g/psrbbvm92t712cy09d7_00d00000gp/T/tmpp18g94f9/envs/virtualenv_envs/mlflow-9d81fff15053e6e06e2edaefcc9e075d6c04a094\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m3 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 12ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpip\u001b[0m\u001b[2m==23.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==68.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwheel\u001b[0m\u001b[2m==0.41.2\u001b[0m\n",
      "\u001b[2mUsing Python 3.9.18 environment at: /var/folders/9g/psrbbvm92t712cy09d7_00d00000gp/T/tmpp18g94f9/envs/virtualenv_envs/mlflow-9d81fff15053e6e06e2edaefcc9e075d6c04a094\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m84 packages\u001b[0m \u001b[2min 4.91s\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 423ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m83 packages\u001b[0m \u001b[2min 849ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1malembic\u001b[0m\u001b[2m==1.15.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mannotated-types\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mattrs\u001b[0m\u001b[2m==23.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mblinker\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbrotli\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcachetools\u001b[0m\u001b[2m==5.5.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2025.1.31\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.1.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcloudpickle\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcontourpy\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcycler\u001b[0m\u001b[2m==0.12.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcython\u001b[0m\u001b[2m==3.0.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdatabricks-sdk\u001b[0m\u001b[2m==0.44.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdeprecated\u001b[0m\u001b[2m==1.2.18\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdistro\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdocker\u001b[0m\u001b[2m==7.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mexceptiongroup\u001b[0m\u001b[2m==1.2.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mflask\u001b[0m\u001b[2m==3.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfonttools\u001b[0m\u001b[2m==4.56.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgitdb\u001b[0m\u001b[2m==4.0.12\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgitpython\u001b[0m\u001b[2m==3.1.44\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogle-auth\u001b[0m\u001b[2m==2.38.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgraphene\u001b[0m\u001b[2m==3.4.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgraphql-core\u001b[0m\u001b[2m==3.2.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgraphql-relay\u001b[0m\u001b[2m==3.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgunicorn\u001b[0m\u001b[2m==23.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mh11\u001b[0m\u001b[2m==0.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mh2\u001b[0m\u001b[2m==4.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhpack\u001b[0m\u001b[2m==4.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpcore\u001b[0m\u001b[2m==1.0.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx\u001b[0m\u001b[2m==0.28.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhyperframe\u001b[0m\u001b[2m==6.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.10\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mimportlib-metadata\u001b[0m\u001b[2m==8.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mimportlib-resources\u001b[0m\u001b[2m==6.5.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mitsdangerous\u001b[0m\u001b[2m==2.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjiter\u001b[0m\u001b[2m==0.8.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjoblib\u001b[0m\u001b[2m==1.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mkiwisolver\u001b[0m\u001b[2m==1.4.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmako\u001b[0m\u001b[2m==1.3.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkdown\u001b[0m\u001b[2m==3.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmatplotlib\u001b[0m\u001b[2m==3.9.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmlflow\u001b[0m\u001b[2m==2.20.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmlflow-skinny\u001b[0m\u001b[2m==2.20.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==1.63.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-api\u001b[0m\u001b[2m==1.16.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-sdk\u001b[0m\u001b[2m==1.16.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-semantic-conventions\u001b[0m\u001b[2m==0.37b0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==24.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==5.29.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyarrow\u001b[0m\u001b[2m==19.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyasn1\u001b[0m\u001b[2m==0.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyasn1-modules\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.10.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.27.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyparsing\u001b[0m\u001b[2m==3.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dateutil\u001b[0m\u001b[2m==2.9.0.post0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpytz\u001b[0m\u001b[2m==2025.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrsa\u001b[0m\u001b[2m==4.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mscikit-learn\u001b[0m\u001b[2m==1.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mscipy\u001b[0m\u001b[2m==1.13.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msix\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msmmap\u001b[0m\u001b[2m==5.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msniffio\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msqlalchemy\u001b[0m\u001b[2m==2.0.38\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msqlparse\u001b[0m\u001b[2m==0.5.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mthreadpoolctl\u001b[0m\u001b[2m==3.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.12.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtzdata\u001b[0m\u001b[2m==2025.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwerkzeug\u001b[0m\u001b[2m==3.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwrapt\u001b[0m\u001b[2m==1.17.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mzipp\u001b[0m\u001b[2m==3.21.0\u001b[0m\n",
      "2025/03/06 19:10:22 INFO mlflow.utils.environment: === Running command '['bash', '-c', 'source /var/folders/9g/psrbbvm92t712cy09d7_00d00000gp/T/tmpp18g94f9/envs/virtualenv_envs/mlflow-9d81fff15053e6e06e2edaefcc9e075d6c04a094/bin/activate && python -c \"\"']'\n",
      "2025/03/06 19:10:22 INFO mlflow.utils.environment: === Running command '['bash', '-c', 'source /var/folders/9g/psrbbvm92t712cy09d7_00d00000gp/T/tmpp18g94f9/envs/virtualenv_envs/mlflow-9d81fff15053e6e06e2edaefcc9e075d6c04a094/bin/activate && python /Users/serena.ruan/Documents/repos/mlflow/mlflow/pyfunc/_mlflow_pyfunc_backend_predict.py --model-uri file:///Users/serena.ruan/Documents/test/mlruns/0/33b7da4d1693490b97934a5781964766/artifacts/model --content-type json --input-path /var/folders/9g/psrbbvm92t712cy09d7_00d00000gp/T/tmpz3dlhg3n/input.json']'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\": \"New York is a state located in the northeastern region of the United States. It is bordered by Vermont to the northeast, Massachusetts to the east, Connecticut to the southeast, and New Jersey and Pennsylvania to the south. The state also has access to the Atlantic Ocean to the southeast. The city of New York, often referred to simply as NYC, is the largest city in the state and is known for its significant cultural, financial, and historical influence.\"}"
     ]
    }
   ],
   "source": [
    "# verify model before serving\n",
    "mlflow.models.predict(\n",
    "    model_uri=model_info.model_uri,\n",
    "    input_data=[{\"role\": \"user\", \"content\": \"Where is New York?\"}],\n",
    "    env_manager=\"uv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386566db-841a-41cf-a46f-d75bd060b1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs:/33b7da4d1693490b97934a5781964766/model\n"
     ]
    }
   ],
   "source": [
    "model_info.model_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f72697-57c9-4a02-9c11-da35c277bab8",
   "metadata": {},
   "source": [
    "## Serve the model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e0e4e4-372b-4176-85e5-6f624b728c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run below command to serve the model locally\n",
    "# mlflow models serve -m runs:/33b7da4d1693490b97934a5781964766/model -p 6666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85013e9c-fb42-4d8a-a8f1-5fb69f695b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\"predictions\": \"British Shorthairs are generally considered to be intelligent cats, though their intelligence may manifest differently compared to some other breeds. They are known for their calm and laid-back demeanor, which can sometimes be mistaken for a lack of intelligence. In reality, they are capable of problem-solving and can be trained to perform basic commands or tricks, though they may not be as eager to please as some more active breeds.\\\\n\\\\nTheir intelligence is often reflected in their ability to adapt to their environment and their understanding of routines. British Shorthairs tend to be independent and may not seek out interaction as much as other, more playful breeds, but they can still form strong bonds with their owners. Essentially, while they might not be the most overtly intelligent cats, they possess a subtle understanding of their surroundings that reflects their adaptability and awareness.\"}'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "from mlflow.models.utils import convert_input_example_to_serving_input\n",
    "\n",
    "payload = convert_input_example_to_serving_input(\n",
    "    [{\"role\": \"user\", \"content\": \"Is British shorthair smart?\"}]\n",
    ")\n",
    "resp = requests.post(\n",
    "    \"http://127.0.0.1:6666/invocations\", data=payload, headers={\"Content-Type\": \"application/json\"}\n",
    ")\n",
    "resp.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}


--- docs/docs/classic-ml/deep-learning/sentence-transformers/tutorials/quickstart/sentence-transformers-quickstart.ipynb ---
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Sentence Transformers and MLflow\n",
    "\n",
    "Welcome to our tutorial on leveraging **Sentence Transformers** with **MLflow** for advanced natural language processing and model management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "- Set up a pipeline for sentence embeddings with `sentence-transformers`.\n",
    "- Log models and configurations using MLflow.\n",
    "- Understand and apply model signatures in MLflow to `sentence-transformers`.\n",
    "- Deploy and use models for inference with MLflow's features.\n",
    "\n",
    "#### What are Sentence Transformers?\n",
    "Sentence Transformers, an extension of the Hugging Face Transformers library, are designed for generating semantically rich sentence embeddings. They utilize models like BERT and RoBERTa, fine-tuned for tasks such as semantic search and text clustering, producing high-quality sentence-level embeddings.\n",
    "\n",
    "#### Benefits of Integrating MLflow with Sentence Transformers\n",
    "Combining MLflow with Sentence Transformers enhances NLP projects by:\n",
    "\n",
    "- Streamlining experiment management and logging.\n",
    "- Offering better control over model versions and configurations.\n",
    "- Ensuring reproducibility of results and model predictions.\n",
    "- Simplifying the deployment process in production environments.\n",
    "\n",
    "This integration empowers efficient tracking, management, and deployment of NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "# Disable tokenizers warnings when constructing pipelines\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Disable a few less-than-useful UserWarnings from setuptools and pydantic\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Environment for Sentence Embedding\n",
    "\n",
    "Begin your journey with Sentence Transformers and MLflow by establishing the core working environment.\n",
    "\n",
    "#### Key Steps for Initialization\n",
    "\n",
    "- Import necessary libraries: `SentenceTransformer` and `mlflow`.\n",
    "- Initialize the `\"all-MiniLM-L6-v2\"` Sentence Transformer model.\n",
    "    \n",
    "#### Model Initialization\n",
    "The compact and efficient `\"all-MiniLM-L6-v2\"` model is chosen for its effectiveness in generating meaningful sentence embeddings. Explore more models at the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=trending).\n",
    "\n",
    "#### Purpose of the Model\n",
    "This model excels in transforming sentences into semantically rich embeddings, applicable in various NLP tasks like semantic search and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import mlflow\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model Signature with MLflow\n",
    "Defining the model signature is a crucial step in setting up our Sentence Transformer model for consistent and expected behavior during inference.\n",
    "\n",
    "#### Steps for Signature Definition\n",
    "\n",
    "- **Prepare Example Sentences**: Define example sentences to demonstrate the model's input and output formats.\n",
    "- **Generate Model Signature**: Use the `mlflow.models.infer_signature` function with the model's input and output to automatically define the signature.\n",
    "\n",
    "#### Importance of the Model Signature\n",
    "\n",
    "- **Clarity in Data Formats**: Ensures clear documentation of the data types and structures the model expects and produces.\n",
    "- **Model Deployment and Usage**: Crucial for deploying models to production, ensuring the model receives inputs in the correct format and produces expected outputs.\n",
    "- **Error Prevention**: Helps in preventing errors during model inference by enforcing consistent data formats.\n",
    "\n",
    "**NOTE**: The `List[str]` input type is equivalent at inference time to `str`. The MLflow flavor uses a `ColSpec[str]` definition for the input type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputs: \n",
       "  [string]\n",
       "outputs: \n",
       "  [Tensor('float32', (-1, 384))]\n",
       "params: \n",
       "  None"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentences = [\"A sentence to encode.\", \"Another sentence to encode.\"]\n",
    "\n",
    "# Infer the signature of the custom model by providing an input example and the resultant prediction output.\n",
    "# We're not including any custom inference parameters in this example, but you can include them as a third argument\n",
    "# to infer_signature(), as you will see in the advanced tutorials for Sentence Transformers.\n",
    "signature = mlflow.models.infer_signature(\n",
    "    model_input=example_sentences,\n",
    "    model_output=model.encode(example_sentences),\n",
    ")\n",
    "\n",
    "# Visualize the signature\n",
    "signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an experiment\n",
    "\n",
    "We create a new MLflow Experiment so that the run we're going to log our model to does not log to the default experiment and instead has its own contextually relevant entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/sentence-transformers/tutorials/quickstart/mlruns/469990615226680434', creation_time=1701280211449, experiment_id='469990615226680434', last_update_time=1701280211449, lifecycle_stage='active', name='Introduction to Sentence Transformers', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you are running this tutorial in local mode, leave the next line commented out.\n",
    "# Otherwise, uncomment the following line and set your tracking uri to your local or remote tracking server.\n",
    "\n",
    "# mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "\n",
    "mlflow.set_experiment(\"Introduction to Sentence Transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging the Sentence Transformer Model with MLflow\n",
    "\n",
    "Logging the model in MLflow is essential for tracking, version control, and deployment, following the initialization and signature definition of our Sentence Transformer model.\n",
    "\n",
    "#### Steps for Logging the Model\n",
    "\n",
    "- **Start an MLflow Run**: Initiate a new run with `mlflow.start_run()`, grouping all logging operations.\n",
    "- **Log the Model**: Use `mlflow.sentence_transformers.log_model` to log the model, providing the model object, artifact path, signature, and an input example.\n",
    "\n",
    "#### Importance of Model Logging\n",
    "\n",
    "- **Model Management**: Facilitates the model's lifecycle management from training to deployment.\n",
    "- **Reproducibility and Tracking**: Enables tracking of model versions and ensures reproducibility.\n",
    "- **Ease of Deployment**: Simplifies deployment by allowing models to be easily deployed for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    logged_model = mlflow.sentence_transformers.log_model(\n",
    "        model=model,\n",
    "        name=\"sbert_model\",\n",
    "        signature=signature,\n",
    "        input_example=example_sentences,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model and Testing Inference\n",
    "\n",
    "After logging the Sentence Transformer model in MLflow, we demonstrate how to load and test it for real-time inference.\n",
    "    \n",
    "#### Loading the Model as a PyFunc\n",
    "\n",
    "- **Why PyFunc**: Load the logged model using `mlflow.pyfunc.load_model` for seamless integration into Python-based services or applications.\n",
    "- **Model URI**: Use the `logged_model.model_uri` to accurately locate and load the model from MLflow.\n",
    "\n",
    "#### Conducting Inference Tests\n",
    "\n",
    "- **Test Sentences**: Define sentences to test the model's embedding generation capabilities.\n",
    "- **Performing Predictions**: Use the model's `predict` method with test sentences to obtain embeddings.\n",
    "- **Printing Embedding Lengths**: Verify embedding generation by checking the length of embedding arrays, corresponding to the dimensionality of each sentence representation.\n",
    "\n",
    "#### Importance of Inference Testing\n",
    "\n",
    "- **Model Validation**: Confirm the model's expected behavior and data processing capability upon loading.\n",
    "- **Deployment Readiness**: Validate the model's readiness for real-time integration into application services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The return structure length is: 2\n",
      "The size of embedding 1 is: 384\n",
      "The size of embedding 2 is: 384\n"
     ]
    }
   ],
   "source": [
    "inference_test = [\"I enjoy pies of both apple and cherry.\", \"I prefer cookies.\"]\n",
    "\n",
    "# Load our custom model by providing the uri for where the model was logged.\n",
    "loaded_model_pyfunc = mlflow.pyfunc.load_model(logged_model.model_uri)\n",
    "\n",
    "# Perform a quick test to ensure that our loaded model generates the correct output\n",
    "embeddings_test = loaded_model_pyfunc.predict(inference_test)\n",
    "\n",
    "# Verify that the output is a list of lists of floats (our expected output format)\n",
    "print(f\"The return structure length is: {len(embeddings_test)}\")\n",
    "\n",
    "for i, embedding in enumerate(embeddings_test):\n",
    "    print(f\"The size of embedding {i + 1} is: {len(embeddings_test[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Samples of Generated Embeddings\n",
    "Examine the content of embeddings to verify their quality and understand the model's output.\n",
    "    \n",
    "#### Inspecting the Embedding Samples\n",
    "\n",
    "- **Purpose of Sampling**: Inspect a sample of the entries in each embedding to understand the vector representations generated by the model.\n",
    "- **Printing Embedding Samples**: Print the first 10 entries of each embedding vector using `embedding[:10]` to get a glimpse into the model's output.\n",
    "\n",
    "#### Why Sampling is Important\n",
    "\n",
    "- **Quality Check**: Sampling provides a quick way to verify the embeddings' quality and ensures they are meaningful and non-degenerate.\n",
    "- **Understanding Model Output**: Seeing parts of the embedding vectors offers an intuitive understanding of the model's output, beneficial for debugging and development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample of the first 10 entries in embedding 1 is: [ 0.04866192 -0.03687946  0.02408808  0.03534171 -0.12739632  0.00999414\n",
      "  0.07135344 -0.01433522  0.04296691 -0.00654414]\n",
      "The sample of the first 10 entries in embedding 2 is: [-0.03879027 -0.02373698  0.01314073  0.03589077 -0.01641303 -0.0857707\n",
      "  0.08282158 -0.03173266  0.04507608  0.02777079]\n"
     ]
    }
   ],
   "source": [
    "for i, embedding in enumerate(embeddings_test):\n",
    "    print(f\"The sample of the first 10 entries in embedding {i + 1} is: {embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native Model Loading in MLflow for Extended Functionality\n",
    "Explore the full range of Sentence Transformer functionalities with MLflow's support for native model loading.\n",
    "    \n",
    "#### Why Support Native Loading?\n",
    "\n",
    "- **Access to Native Functionalities**: Native loading unlocks all the features of the Sentence Transformer model, essential for advanced NLP tasks.\n",
    "- **Loading the Model Natively**: Use `mlflow.sentence_transformers.load_model` to load the model with its full capabilities, enhancing flexibility and efficiency.\n",
    "\n",
    "#### Generating Embeddings Using Native Model\n",
    "\n",
    "- **Model Encoding**: Employ the model's native `encode` method to generate embeddings, taking advantage of optimized functionality.\n",
    "- **Importance of Native Encoding**: Native encoding ensures the utilization of the model's full embedding generation capabilities, suitable for large-scale or complex NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/30 15:50:24 INFO mlflow.sentence_transformers: 'runs:/eeab3c1b13594fdea13e07585b1c0596/sbert_model' resolved as 'file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/sentence-transformers/tutorials/quickstart/mlruns/469990615226680434/eeab3c1b13594fdea13e07585b1c0596/artifacts/sbert_model'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample of the native library encoding call for embedding 1 is: [ 0.04866192 -0.03687946  0.02408808  0.03534171 -0.12739632  0.00999414\n",
      "  0.07135344 -0.01433522  0.04296691 -0.00654414]\n",
      "The sample of the native library encoding call for embedding 2 is: [-0.03879027 -0.02373698  0.01314073  0.03589077 -0.01641303 -0.0857707\n",
      "  0.08282158 -0.03173266  0.04507608  0.02777079]\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model as a native Sentence Transformers model (unlike above, where we loaded as a generic python function)\n",
    "loaded_model_native = mlflow.sentence_transformers.load_model(logged_model.model_uri)\n",
    "\n",
    "# Use the native model to generate embeddings by calling encode() (unlike for the generic python function which uses the single entrypoint of `predict`)\n",
    "native_embeddings = loaded_model_native.encode(inference_test)\n",
    "\n",
    "for i, embedding in enumerate(native_embeddings):\n",
    "    print(\n",
    "        f\"The sample of the native library encoding call for embedding {i + 1} is: {embedding[:10]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: Embracing the Power of Sentence Transformers with MLflow\n",
    "\n",
    "As we reach the end of our Introduction to Sentence Transformers tutorial, we have successfully navigated the basics of integrating the Sentence Transformers library with MLflow. This foundational knowledge sets the stage for more advanced and specialized applications in the field of Natural Language Processing (NLP).\n",
    "\n",
    "#### Recap of Key Learnings\n",
    "\n",
    "1. **Integration Basics**: We covered the essential steps of loading and logging a Sentence Transformer model using MLflow. This process demonstrated the simplicity and effectiveness of integrating cutting-edge NLP tools within MLflow's ecosystem.\n",
    "\n",
    "2. **Signature and Inference**: Through the creation of a model signature and the execution of inference tasks, we showcased how to operationalize the Sentence Transformer model, ensuring that it's ready for real-world applications.\n",
    "\n",
    "3. **Model Loading and Prediction**: We explored two ways of loading the model - as a PyFunc model and using the native Sentence Transformers loading mechanism. This dual approach highlighted the versatility of MLflow in accommodating different model interaction methods.\n",
    "\n",
    "4. **Embeddings Exploration**: By generating and examining sentence embeddings, we glimpsed the transformative potential of transformer models in capturing semantic information from text.\n",
    "\n",
    "#### Looking Ahead\n",
    "\n",
    "- **Expanding Horizons**: While this tutorial focused on the foundational aspects of Sentence Transformers and MLflow, there's a whole world of advanced applications waiting to be explored. From semantic similarity analysis to paraphrase mining, the potential use cases are vast and varied.\n",
    "\n",
    "- **Continued Learning**: We strongly encourage you to delve into the other tutorials in this series, which dive deeper into more intriguing use cases like similarity analysis, semantic search, and paraphrase mining. These tutorials will provide you with a broader understanding and more practical applications of Sentence Transformers in various NLP tasks.\n",
    "\n",
    "#### Final Thoughts\n",
    "\n",
    "The journey into NLP with Sentence Transformers and MLflow is just beginning. With the skills and insights gained from this tutorial, you are well-equipped to explore more complex and exciting applications. The integration of advanced NLP models with MLflow's robust management and deployment capabilities opens up new avenues for innovation and exploration in the field of language understanding and beyond.\n",
    "\n",
    "Thank you for joining us on this introductory journey, and we look forward to seeing how you apply these tools and concepts in your NLP endeavors!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow-dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

--- docs/docs/quickstart_drilldown/index.mdx ---
---
sidebar_custom_props:
  hide: true
displayed_sidebar: docsSidebar
---

import Link from "@docusaurus/Link";
import { Table } from "@site/src/components/Table";

# Quickstart options and troubleshooting

{/** Eventually, these H2s will probably all be separate articles. For now, I'm
avoiding that so as not to create a bunch of super-skinny pages. **/}

## Customize and troubleshoot MLflow installation \{#quickstart_drilldown_install}

### Python library options

Rather than the default MLflow library, you can install the following variations:

<Table>
  <thead>
    <tr>
      <th>**Name**</th>
      <th>**`pip install` command**</th>
      <th>**Description**</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>mlflow-skinny</td>
      <td>`pip install mlflow-skinny`</td>
      <td>Lightweight MLflow package without SQL storage, server, UI, or data science dependencies.</td>
    </tr>
    <tr>
      <td>mlflow[extras]</td>
      <td>`pip install mlflow[extras]`</td>
      <td>MLflow package with all dependencies needed to run various MLflow flavors. These dependencies are listed in [this document](https://github.com/mlflow/mlflow/blob/master/requirements/extra-ml-requirements.txt).</td>
    </tr>
    <tr>
      <td>In-development version</td>
      <td>`pip install git+https://github.com/mlflow/mlflow.git@master`</td>
      <td>This is the latest version of MLflow, which may be useful for getting hot-fixes or new features.</td>
    </tr>
  </tbody>
</Table>

### Python and Mac OS X

We strongly recommend using a virtual environment manager on Macs. We always recommend
using virtual environments, but they are especially important on Mac OS X because the system
`python` version varies depending on the installation and whether you've installed the Xcode
command line tools. The default environment manager for MLflow is `virtualenv`.
Other popular options are `conda` and `venv`.

### Python

We release MLflow on:

- PyPI (`pip install mlflow`)
- conda-forge (`conda install -c conda-forge mlflow`)

### R and Java

We release MLflow on:

- CRAN (`install.packages("mlflow")`)
- Maven Central (`mlflow-client`, `mlflow-parent`, `mlflow-spark`)

For R, see <APILink fn="mlflow.r" hash="">installing MLflow for R</APILink>.
For Java, see <APILink fn="mlflow.java" hash="">Java API</APILink>.

## Save and serve models \{#quickstart_drilldown_log_and_load_model}

MLflow includes a generic `MLmodel` format for saving **models** from a variety of tools in diverse
**flavors**. For example, many models can be served as Python functions, so an `MLmodel` file can
declare how each model should be interpreted as a Python function in order to let various tools
serve it. MLflow also includes tools for running such models locally and exporting them to Docker
containers or commercial serving platforms.

To illustrate this functionality, the `mlflow.sklearn` flavor can log scikit-learn models as
MLflow artifacts and then load them again for serving. There is an example training application in
[sklearn_logistic_regression/train.py](https://github.com/mlflow/mlflow/tree/master/examples/sklearn_logistic_regression).
To run it, switch to the MLflow repository root and run:

```bash
python examples/sklearn_logistic_regression/train.py
```

When you run the example, it outputs an MLflow run ID for that experiment. If you look at the
`mlflow ui`, you will also see that the run saved a **model** folder containing an `MLmodel`
description file and a pickled scikit-learn model. You can pass the run ID and the path of the model
within the artifacts directory (here **model/**) to various tools. For example, MLflow includes a
simple REST server for python-based models:

```bash
mlflow models serve -m --env-manager local runs:/<RUN_ID>/model
```

:::note
By default the server runs on port 5000. If that port is already in use, use the `--port` option to
specify a different port. For example: `mlflow models serve -m runs:/<RUN_ID>/model --port 1234`
:::

Once you have started the server, you can pass it some sample data and see the
predictions.

The following example uses `curl` to send a JSON-serialized pandas DataFrame with the `split`
orientation to the model server. For more information about the input data formats accepted by
the pyfunc model server, see the [MLflow deployment tools documentation](/deployment/deploy-model-locally).

```bash
curl -d '{"dataframe_split": {"columns": ["x"], "data": [[1], [-1]]}}' -H 'Content-Type: application/json' -X POST localhost:5000/invocations
```

which returns:

```bash
[1, 0]
```

For more information, see [MLflow Models](/model).


--- docs/docs/classic-ml/getting-started/index.mdx ---
---
sidebar_position: 3
---

import { CardGroup, PageCard } from "@site/src/components/Card";
import Link from "@docusaurus/Link";

# Getting Started with MLflow

If you're new to MLflow or seeking a refresher on its core functionalities, the
quickstart tutorials here are the perfect starting point. They will guide you
step-by-step through fundamental concepts, focusing purely on a task that will maximize your understanding of
how to use MLflow to solve a particular task.

:::tip
If you'd like to try a free trial of a fully-managed MLflow experience on Databricks, you can quickly sign up and start using MLflow for your GenAI and ML project needs without having to configure, setup, and run your own tracking server. You can learn more about the

<Link to="/ml/getting-started/databricks-trial" target="_blank">Databricks Free Trial</Link> here. This trial offers full access to a personal Databricks account that includes MLflow and other tightly integrated AI services and features.
:::

## Guidance on Running Tutorials

If you have never interfaced with the [MLflow Tracking Server](/self-hosting/architecture/tracking-server), we highly encourage you to head on over to quickly **read the guide below**. It
will help you get started as quickly as possible with tutorial content throughout the documentation.

<CardGroup>
  <PageCard
    link="/ml/getting-started/running-notebooks/"
    headerText="How to Run Tutorials"
    text="Learn about your options for running a MLflow Tracking Server for executing any of the guides and tutorials in the MLflow documentation"
  />
</CardGroup>
<br />

## Getting Started Guides

### MLflow Tracking

[MLflow Tracking](/ml/tracking) is one of the primary service components of MLflow. In these guides, you will gain an understanding of what MLflow Tracking can do to
enhance your MLOps related activities while building ML models.

![The basics of MLflow tracking](/images/tutorials/introductory/tracking-basics.png 'The basics of MLflow tracking')

In these introductory guides to MLflow Tracking, you will learn how to leverage MLflow to:

- **Log** training statistics (loss, accuracy, etc.) and hyperparameters for a model
- **Log** (save) a model for later retrieval
- **Register** a model using the [MLflow Model Registry](/ml/model-registry) to enable deployment
- **Load** the model and use it for inference

In the process of learning these key concepts, you will be exposed to the [MLflow Tracking APIs](/ml/tracking/tracking-api), the MLflow Tracking UI, and learn how to add metadata associated with
a model training event to an MLflow run.

<CardGroup>
  <PageCard link="/ml/tracking/quickstart" headerText="MLFlow Tracking Quickstart Guide" text="Learn the basics of MLflow Tracking in a fast-paced guide with a focus on seeing your first model in the MLflow UI" />
  <PageCard link="/ml/getting-started/logging-first-model" headerText="In-depth Tutorial for MLflow Tracking" text="Learn the nuances of interfacing with the MLflow Tracking Server in an in-depth tutorial" />
</CardGroup>

### Autologging Basics

A great way to get started with MLflow is to use the [autologging](/ml/tracking/autolog) feature. Autologging automatically logs your model, metrics, examples, signature, and parameters
with only a single line of code for many of the most popular ML libraries in the Python ecosystem.

<div class="center-div" style={{ width: "80%" }}>
  ![The basics of MLflow tracking](/images/tutorials/introductory/autologging-intro.png "The basics of MLflow tracking")
</div>

In this brief tutorial, you'll learn how to leverage MLflow's autologging feature to simplify your model logging activities.

<CardGroup>
  <PageCard link="/ml/tracking/autolog" headerText="MLflow Autologging Quickstart" text="Get started with logging to MLflow with the high-level autologging API in a fast-paced guide" />
</CardGroup>
<br />

### Run Comparison Basics

This quickstart tutorial focuses on the MLflow UI's run comparison feature and provides a step-by-step walkthrough of registering the best model found from a
hyperparameter tuning execution sweep. After locally serving the registered model, a brief example of preparing a model for remote [deployment](/ml/deployment)
by containerizing the model using Docker is covered.

![The basics of MLflow run comparison](/images/tutorials/introductory/intro-run-comparison.png 'The basics of MLflow run comparison')

<CardGroup>
  <PageCard link="/ml/getting-started/hyperparameter-tuning" headerText="MLflow Run Comparison Quickstart" text="Get started with using the MLflow UI to compare runs and register a model for deployment" />
</CardGroup>
<br />

### Tracking Server Quickstart

This quickstart tutorial walks through different types of [MLflow Tracking Servers](/self-hosting/architecture/tracking-server) and how to use them to log
your MLflow experiments.

<CardGroup>
  <PageCard link="/ml/getting-started/tracking-server-overview" headerText="5 Minute Tracking Server Overview" text="Learn how to log MLflow experiments with different tracking servers" />
</CardGroup>
<br />

### Model Registry Quickstart

This quickstart tutorial walks through registering a model in the MLflow model registry and how to
retrieve registered models.

<CardGroup>
  <PageCard link="/ml/getting-started/registering-first-model/" headerText="5 Minute Model Registry Overview" text="Learn how to log MLflow models to the model registry" />
</CardGroup>

## Further Learning - What's Next?

Now that you have the essentials under your belt, below are some recommended collections of tutorial and guide content that will help to broaden your
understanding of MLflow and its APIs.

- **Tracking** - Learn more about the MLflow tracking APIs by [reading the tracking guide](/ml/tracking).
- **MLflow Deployment** - Follow the comprehensive [guide on model deployment](/ml/deployment) to learn how to deploy your MLflow models to a variety of deployment targets.
- **Model Registry** - Learn about the [MLflow Model Registry](/ml/model-registry) and how it can help you manage the lifecycle of your ML models.
- **Deep Learning Library Integrations** - From PyTorch to TensorFlow and more, learn about the integrated deep learning capabilities in MLflow by [reading the deep learning guide](/ml/deep-learning).
- **Traditional ML** - Learn about the [traditional ML capabilities](/ml/traditional-ml) in MLflow and how they can help you manage your traditional ML workflows.


--- docs/docs/genai/getting-started/index.mdx ---
---
description: "Build, evaluate, and deploy production-ready GenAI applications with MLflow's comprehensive LLMOps platform"
sidebar_position: 1
---

import FeatureHighlights from "@site/src/components/FeatureHighlights";
import ConceptOverview from "@site/src/components/ConceptOverview";
import TilesGrid from "@site/src/components/TilesGrid";
import TileCard from "@site/src/components/TileCard";
import useBaseUrl from '@docusaurus/useBaseUrl';
import { Code2, TestTube, Rocket, Eye, Database, Shield, Zap, Users, TrendingUp, BookOpen, PlayCircle, Target, Settings } from "lucide-react";

# Getting Started with MLflow for GenAI

## The Complete Open Source LLMOps Platform for Production GenAI

MLflow transforms how software engineers build, evaluate, and deploy GenAI applications. Get complete observability, systematic evaluation, and deployment confidence—all while maintaining the flexibility to use any framework or model provider.

<div style={{margin: '2rem 0', textAlign: 'center'}}>
  <img
    src={useBaseUrl('/images/llms/tracing/tracing-top.gif')}
    alt="MLflow Tracing UI showing detailed GenAI observability"
    style={{maxWidth: '100%', borderRadius: '8px', boxShadow: '0 4px 12px rgba(0, 0, 0, 0.15)'}}
  />
</div>

## The GenAI Development Lifecycle

MLflow provides a complete platform that supports every stage of GenAI application development. From initial prototyping to production monitoring, these integrated capabilities ensure you can build, test, and deploy with confidence.

<ConceptOverview concepts={[
  {
    icon: Code2,
    title: "Develop & Debug",
    description: "Trace every LLM call, prompt interaction, and tool invocation. Debug complex AI workflows with complete visibility into execution paths, token usage, and decision points."
  },
  {
    icon: TestTube,
    title: "Evaluate & Improve",
    description: "Systematically test with LLM judges, human feedback, and custom metrics. Compare versions objectively and catch regressions before they reach production."
  },
  {
    icon: Rocket,
    title: "Deploy & Monitor",
    description: "Serve models with confidence using built-in deployment targets. Monitor production performance and iterate based on real-world usage patterns."
  }
]} />

## Why Open Source MLflow for GenAI?

As the original open source ML platform, MLflow brings battle-tested reliability and community-driven innovation to GenAI development. No vendor lock-in, no proprietary formats—just powerful tools that work with your stack.

<FeatureHighlights features={[
  {
    icon: Eye,
    title: "Production-Grade Observability",
    description: "Automatically instrument 15+ frameworks including OpenAI, LangChain, and LlamaIndex. Get detailed traces showing token usage, latency, and execution paths for every request—no black boxes."
  },
  {
    icon: Database,
    title: "Intelligent Prompt Management",
    description: "Version, compare, and deploy prompts with MLflow's prompt registry. Track performance across prompt variations and maintain audit trails for production systems."
  },
  {
    icon: Shield,
    title: "Automated Quality Assurance",
    description: "Build confidence with LLM judges and automated evaluation. Run systematic tests on every change and track quality metrics over time to prevent regressions."
  },
  {
    icon: Zap,
    title: "Framework-Agnostic Integration",
    description: "Use any LLM framework or provider without vendor lock-in. MLflow works with your existing tools while providing unified tracking, evaluation, and deployment."
  }
]} />

## Start Building Production GenAI Applications

MLflow transforms GenAI development from complex instrumentation to simple, one-line integrations. See how easy it is to add comprehensive observability, evaluation, and deployment to your AI applications. Visit the [Tracing guide](/genai/tracing) for more information.

### Add Complete Observability in One Line

Transform any GenAI application into a fully observable system:

```python
import mlflow

# Enable automatic tracing for your framework
mlflow.openai.autolog()  # For OpenAI
mlflow.langchain.autolog()  # For LangChain
mlflow.llama_index.autolog()  # For LlamaIndex
mlflow.dspy.autolog()  # For DSPy

# Your existing code now generates detailed traces
from openai import OpenAI

client = OpenAI()
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Explain quantum computing"}],
)
# ✅ Automatically traced: tokens, latency, cost, full request/response
```

No code changes required. Every LLM call, tool interaction, and prompt execution is automatically captured with detailed metrics.

### Manage and Optimize Prompts Systematically

Register prompts and automatically optimize them with data-driven techniques. See the [Prompt Registry](/genai/prompt-registry/create-and-edit-prompts) guide for comprehensive prompt management:

```python
import mlflow
import openai
from mlflow.genai.optimize import GepaPromptOptimizer
from mlflow.genai.scorers import Correctness

# Register an initial prompt
prompt = mlflow.genai.register_prompt(
    name="math_tutor",
    template="Answer this math question: {{question}}. Provide a clear explanation.",
)


# Define prediction function that includes prompt.format() call for your target prompt(s)
def predict_fn(question: str) -> str:
    prompt = mlflow.genai.load_prompt("prompts:/math_tutor/latest")
    completion = openai.OpenAI().chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt.format(question=question)}],
    )
    return completion.choices[0].message.content


# Prepare training data with inputs and expectations
train_data = [
    {
        "inputs": {"question": "What is 15 + 27?"},
        "expectations": {"expected_response": "42"},
    },
    {
        "inputs": {"question": "Calculate 8 × 9"},
        "expectations": {"expected_response": "72"},
    },
    {
        "inputs": {"question": "What is 100 - 37?"},
        "expectations": {"expected_response": "63"},
    },
    # ... more examples
]

# Automatically optimize the prompt using MLflow + GEPA
result = mlflow.genai.optimize_prompts(
    predict_fn=predict_fn,
    train_data=train_data,
    prompt_uris=[prompt.uri],
    optimizer=GepaPromptOptimizer(reflection_model="openai:/gpt-4o-mini"),
    scorers=[Correctness(model="openai:/gpt-4o-mini")],
)

# The optimized prompt is automatically registered as a new version
optimized_prompt = result.optimized_prompts[0]
print(f"Optimized prompt registered as version {optimized_prompt.version}")
print(f"Template: {optimized_prompt.template}")
print(f"Score: {result.final_eval_score}")
```

Transform manual prompt engineering into systematic, data-driven optimization with automatic performance tracking. Learn more in the [Optimize Prompts](/genai/prompt-registry/optimize-prompts) guide.

### Prerequisites

Ready to get started? You'll need:

- Python 3.10+ installed
- MLflow 3.5+ (`pip install --upgrade mlflow`)
- API access to an LLM provider (OpenAI, Anthropic, etc.)

---

## Essential Learning Path

Master these core capabilities to build robust GenAI applications with MLflow. Start with observability, then add systematic evaluation and deployment.

<TilesGrid>
  <TileCard
    icon={PlayCircle}
    iconSize={48}
    title="Environment Setup"
    description="Configure MLflow tracking, connect to registries, and set up your development environment for GenAI workflows"
    href="/genai/getting-started/connect-environment"
    linkText="Start setup →"
    containerHeight={64}
  />
  <TileCard
    icon={Eye}
    iconSize={48}
    title="Observability with Tracing"
    description="Auto-instrument your GenAI application to capture every LLM call, prompt, and tool interaction for complete visibility"
    href="/genai/tracing/quickstart"
    linkText="Learn tracing →"
    containerHeight={64}
  />
  <TileCard
    icon={TestTube}
    iconSize={48}
    title="Systematic Evaluation"
    description="Build confidence with LLM judges and automated testing to catch quality issues before production"
    href="/genai/eval-monitor"
    linkText="Start evaluating →"
    containerHeight={64}
  />
</TilesGrid>

These three foundations will give you the observability and quality confidence needed for production GenAI development. Each tutorial includes real code examples and best practices from production deployments.

---

## Advanced GenAI Capabilities

Once you've mastered the essentials, explore these advanced features to build sophisticated GenAI applications with enterprise-grade reliability.

<TilesGrid>
  <TileCard
    icon={Database}
    iconSize={48}
    title="Prompt Registry & Management"
    description="Version prompts, A/B test variations, and maintain audit trails for production prompt management"
    href="/genai/prompt-registry/prompt-engineering"
    linkText="Manage prompts →"
    containerHeight={64}
  />
  <TileCard
    icon={Target}
    iconSize={48}
    title="Automated Prompt Optimization"
    description="Automatically improve prompts using DSPy's MIPROv2 algorithm with data-driven optimization and performance tracking"
    href="/genai/prompt-registry/optimize-prompts"
    linkText="Optimize prompts →"
    containerHeight={64}
  />
  <TileCard
    icon={Rocket}
    iconSize={48}
    title="Model Deployment"
    description="Deploy GenAI models to production with built-in serving, scaling, and monitoring capabilities"
    href="/genai/serving"
    linkText="Deploy models →"
    containerHeight={64}
  />
</TilesGrid>

These capabilities enable you to build production-ready GenAI applications with systematic quality management and robust deployment infrastructure.

---

## Framework-Specific Integration Guides

MLflow provides deep integrations with popular GenAI frameworks. Choose your framework to get started with optimized instrumentation and best practices.

<TilesGrid>
  <TileCard
    image="/images/logos/langchain-logo.png"
    iconSize={48}
    title="LangChain Integration"
    description="Auto-trace chains, agents, and tools with comprehensive LangChain instrumentation"
    href="/genai/flavors/langchain"
    linkText="Use LangChain →"
    containerHeight={64}
  />
  <TileCard
    image="/images/logos/llamaindex-logo.svg"
    iconSize={48}
    title="LlamaIndex Integration"
    description="Instrument RAG pipelines and document processing workflows with LlamaIndex support"
    href="/genai/flavors/llama-index"
    linkText="Use LlamaIndex →"
    containerHeight={64}
  />
  <TileCard
    image="/images/logos/openai-logo.svg"
    iconSize={48}
    title="OpenAI Integration"
    description="Track completions, embeddings, and function calls with native OpenAI instrumentation"
    href="/genai/flavors/openai"
    linkText="Use OpenAI →"
    containerHeight={64}
  />
  <TileCard
    icon={Code2}
    iconSize={48}
    title="DSPy Integration"
    description="Build systematic prompt optimization workflows with DSPy modules and MLflow prompt registry"
    href="/genai/flavors/dspy"
    linkText="Use DSPy →"
    containerHeight={64}
  />
  <TileCard
    icon={Code2}
    iconSize={48}
    title="Custom Framework Support"
    description="Instrument any LLM framework or build custom integrations with MLflow's flexible APIs"
    href="/genai/flavors/chat-model-intro"
    linkText="Build custom →"
    containerHeight={64}
  />
</TilesGrid>

Each integration guide includes framework-specific examples, best practices, and optimization techniques for production deployments.

---

## Start Your GenAI Journey with MLflow

Ready to build production-ready GenAI applications? Start with the Environment Setup guide above, then explore tracing for complete observability into your AI systems. Join thousands of engineers who trust MLflow's open source platform for their GenAI development.


--- docs/docs/genai/getting-started/databricks-trial/index.mdx ---
---
sidebar_position: 2
---

import { APILink } from "@site/src/components/APILink";

# Try Managed MLflow

The [Databricks Free Trial](https://docs.databricks.com/en/getting-started/free-trial.html) offers an opportunity to experience the Databricks platform without prior cloud provider access.
Most Databricks features, including full MLflow functionality are available during the trial period, allowing you to explore the platform with trial credits.
You create account with your email only and won't get charged unless you decide to upgrade to a paid plan and register your payment information.

## Start your trial

To get started with Databricks Free Trial, visit the [Databricks Trial Signup Page](https://signup.databricks.com/?destination_url=/ml/experiments-signup?source=OSS_DOCS&dbx_source=TRY_MLFLOW&signup_experience_step=EXPRESS&provider=MLFLOW&utm_source=OSS_DOCS)
and follow the instructions outlined there. It takes about 5 minutes to set up, after which you'll have access to a nearly fully-functional Databricks Workspace for logging your tutorial experiments, traces, models, and artifacts.

:::tip
Do you already have a Databricks trial account? [Click here](https://login.databricks.com/?destination_url=/ml/experiments&dbx_source=MLFLOW_DOCS&source=MLFLOW_DOCS) if you'd like to login and get back to the MLflow UI.
:::

## First Steps

When you login for the first time, you will be directed to the MLflow Tracing tutorial, giving you an opportunity to try out one of the most powerful GenAI features that MLflow has to offer.

Simply click on either of the two tutorials and you will be able to test out MLflow's instrumentation capabilities within minutes.

<figure>
  ![MLflow Tracing Tutorial](/images/tutorials/introductory/lighthouse/tracing-tutorial.png)
  <figcaption style={{ textAlign: "center" }}>Learn Tracing within Databricks MLflow UI</figcaption>
</figure>

## Navigating the Databricks UI

Otherwise, once you log in to the Databricks Workspace on subsequent visits, you will see a landing page like this:

<figure>
  ![Databricks Trial Landing Page](/images/tutorials/introductory/lighthouse/landing-page.png)
  <figcaption style={{ textAlign: "center" }}>Databricks Landing Page</figcaption>
</figure>

In order to get to the MLflow UI, you can navigate to it by clicking on the "Experiments" link on the left-hand side (denoted by the laboratory beaker icon).
When you get to the MLflow UI on Databricks for the first time, you'll see this:

<figure>
  ![Databricks Trial MLflow UI](/images/tutorials/introductory/lighthouse/experiments-page.png)
    <figcaption style={{ textAlign: "center" }}>Databricks MLflow UI</figcaption>
</figure>

## Decisions about where to run your Notebook

With a Databricks managed instance of MLflow, you have two options for running the tutorial notebooks: importing notebooks directly into Databricks Workspace or running notebooks locally and using Databricks Workspace as a remote tracking server.

### Importing Notebooks directly into Databricks Workspace

Once you're at the main page of the Databricks Workspace, you can import any of the notebooks within this tutorial.
Firstly, click "Download this Notebook" button in a tutorial page to download the tutorial notebook.
Then navigate to the "Workspace" tab on the left and click that link to open the workspace page.
From there, navigate to `Home` and you can right click to bring up the "Import" option.
The below image shows what the import dialog should look like if you're going to directly import a notebook from the MLflow documentation website:

![Databricks Workspace import Notebook from MLflow docs website](/images/tutorials/introductory/lighthouse/import-notebook.png)

At this point, you can simply just run the tutorial.
Any calls to MLflow for creating experiments, initiating runs, logging metadata, and saving artifacts will be fully managed for you and your logging history will appear within the MLflow UI.

:::note
On the Databricks platform, an MLflow experiment is automatically created for each notebook and you can skip `mlflow.set_tracking_uri()` and `mlflow.set_experiment()` calls in tutorials.
:::

### Running Notebooks locally and using Databricks Workspace as a remote tracking server

In order to stay within the comfortable confines of your local machine and still have the use of the managed MLflow Tracking Server, you need to:

- Generate a Personal Access Token (PAT)
- Set up Databricks workspace authentication in your dev environment.
- Connect to your Databricks Workspace in your MLflow experiment session.

#### Generate a PAT

If you are following along in the Tracing Tutorial, these steps are handled for you in both tutorials within the product. You can generate a remote access token directly within the tutorial.

Otherwise, follow the steps in [this guide](https://docs.databricks.com/aws/en/dev-tools/auth/pat) to create a PAT for remotely accessing your Databricks Workspace.

#### Install Dependencies

Run the following command in your dev environment to install dependencies.

```bash
%pip install -q mlflow
```

#### Set Up Authentication to a Databricks Workspace

To set up Databricks Workspace authentication, we can use the API <APILink fn="mlflow.login" />, which will prompt you for required information:

- **Databricks Host**: Use "https://\<your workspace host\>.cloud.databricks.com/
- **Token**: Your personal access token for your Databricks Workspace.

If the authentication succeeds, you should see a message "Successfully signed in to Databricks!".

```python
import mlflow

mlflow.login()
```

```
2025/02/19 12:25:04 INFO mlflow.utils.credentials: No valid Databricks credentials found, please enter your credentials...
Databricks Host (should begin with https://):  https://<your workspace host>.cloud.databricks.com/
Token:  ········
2025/02/19 12:26:24 INFO mlflow.utils.credentials: Successfully connected to MLflow hosted tracking server! Host: https://<your workspace host>.cloud.databricks.com.
```

#### Connect MLflow Session to Databricks Workspace

We have set up the credentials, now we need to tell MLflow to send the data into Databricks Workspace.
To do so, we will use `mlflow.set_tracking_uri("databricks")` to port MLflow to Databricks Workspace. Basically
it is the command below. Please note that you need to always use _"databricks"_ as the keyword.

```python
mlflow.set_tracking_uri("databricks")
```

Now you are ready to go! Let's try starting an MLflow experiment and log some dummy metrics and view it in the UI.

#### Log Artifacts to Unity Catalog (Optional)

In order to keep all of your artifacts within a single place, you can opt to use Unity Catalog's Volumes feature.
Firstly, you need to create a Unity Catalog Volume `test.mlflow.check-databricks-connection` by following [this guide](https://docs.databricks.com/aws/en/volumes/utility-commands#create-a-volume).
Then, you can run the following code to start an experiment with the Unity Catalog Volume and log metrics to it.
Note that your experiment name must follow the `/Users/<your email>/<experiment_name>` format when using a Databricks Workspace.

```python
mlflow.create_experiment(
    "/Users/<your email>/check-databricks-connection",
    artifact_location="dbfs:/Volumes/test/mlflow/check-databricks-connection",
)
mlflow.set_experiment("/Users/<your email>/check-databricks-connection")

with mlflow.start_run():
    mlflow.log_metric("foo", 1)
    mlflow.log_metric("bar", 2)
```

```
2025/02/19 12:26:33 INFO mlflow.tracking.fluent: Experiment with name '/Users/<your email>/check-databricks-connection' does not exist. Creating a new experiment.
```

#### View Your Experiment on your Databricks Workspace

Now let's navigate to your Databricks Workspace to view the experiment result. Log in to your
Databricks Workspace, and click on top left to select machine learning
in the drop down list. Then click on the experiment icon. See the screenshot below:

<div className="center-div" style={{ width: 800, maxWidth: "100%" }}>
  ![Landing page of Databricks MLflow server](/images/quickstart/tracking-server-overview/databricks-lighthouse-landing-page.png)
</div>

In the "Experiments" view, you should be able to find the experiment "check-databricks-connection", similar to

<div className="center-div" style={{ width: 800, maxWidth: "100%" }}>
  ![Experiment view of Databricks MLflow server](/images/quickstart/tracking-server-overview/databricks-lighthouse-experiment-view.png)
</div>

Clicking on the run name, in our example it is "skillful-jay-111" (it's a randomly generated name, you will see
a different name in your Databricks console), will bring you to the run view, similar to

<div className="center-div" style={{ width: 800, maxWidth: "100%" }}>
  ![Run view of Databricks MLflow server](/images/quickstart/tracking-server-overview/databricks-lighthouse-run-view.png)
</div>

In the run view, you will see your dummy metrics _"foo"_ and _"bar"_ are logged successfully.

At this point, you're ready to go! You can run any of the tutorials locally and they will log to the managed MLflow Tracking Server.


--- docs/docs/classic-ml/getting-started/hyperparameter-tuning/index.mdx ---
# Hyperparameter Tuning & Deployment Quickstart

Master the complete MLOps workflow with MLflow's hyperparameter optimization capabilities. In this hands-on quickstart, you'll learn how to systematically find the best model parameters, track experiments, and deploy production-ready models.

## What You'll Learn

By the end of this tutorial, you'll know how to:

- 🔍 **Run intelligent hyperparameter optimization** with Hyperopt and MLflow tracking
- 📊 **Compare experiment results** using MLflow's powerful visualization tools
- 🏆 **Identify and register your best model** for production use
- 🚀 **Deploy models to REST APIs** for real-time inference
- 📦 **Build production containers** ready for cloud deployment

<div className="center-div" style={{ width: 800, maxWidth: "100%" }}>
  ![Diagram showing Data Science and MLOps workflow with MLflow](/images/quickstart/quickstart_tracking_overview.png)
</div>

## Prerequisites & Setup

### Quick Setup

For this quickstart, we'll use a local MLflow tracking server. Start it with:

```bash
mlflow ui --port 5000
```

Keep this running in a separate terminal. Your MLflow UI will be available at [http://localhost:5000](http://localhost:5000).

### Install Dependencies

```bash
pip install mlflow[extras] hyperopt tensorflow scikit-learn pandas numpy
```

### Set Environment Variable

```bash
export MLFLOW_TRACKING_URI=http://localhost:5000
```

:::tip Team Collaboration and Managed Setup
For production environments or team collaboration, consider using [MLflow Tracking Server configurations](/ml/getting-started/running-notebooks/). For a fully-managed solution, get started with Databricks Free Trial by visiting the [Databricks Trial Signup Page](https://signup.databricks.com/?destination_url=/ml/experiments-signup?source=OSS_DOCS&dbx_source=TRY_MLFLOW&signup_experience_step=EXPRESS&provider=MLFLOW&utm_source=OSS_DOCS) and follow the instructions outlined there. It takes about 5 minutes to set up, after which you'll have access to a nearly fully-functional Databricks Workspace for logging your tutorial experiments, traces, models, and artifacts.
:::

## The Challenge: Wine Quality Prediction

We'll optimize a neural network that predicts wine quality from chemical properties. Our goal is to minimize **Root Mean Square Error (RMSE)** by finding the optimal combination of:

- **Learning Rate**: How aggressively the model learns
- **Momentum**: How much the optimizer considers previous updates

## Step 1: Prepare Your Data

First, let's load and explore our dataset:

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from tensorflow import keras
import mlflow
from mlflow.models import infer_signature
from hyperopt import fmin, tpe, hp, STATUS_OK, Trials

# Load the wine quality dataset
data = pd.read_csv(
    "https://raw.githubusercontent.com/mlflow/mlflow/master/tests/datasets/winequality-white.csv",
    sep=";",
)

# Create train/validation/test splits
train, test = train_test_split(data, test_size=0.25, random_state=42)
train_x = train.drop(["quality"], axis=1).values
train_y = train[["quality"]].values.ravel()
test_x = test.drop(["quality"], axis=1).values
test_y = test[["quality"]].values.ravel()

# Further split training data for validation
train_x, valid_x, train_y, valid_y = train_test_split(
    train_x, train_y, test_size=0.2, random_state=42
)

# Create model signature for deployment
signature = infer_signature(train_x, train_y)
```

## Step 2: Define Your Model Architecture

Create a reusable function to build and train models:

```python
def create_and_train_model(learning_rate, momentum, epochs=10):
    """
    Create and train a neural network with specified hyperparameters.

    Returns:
        dict: Training results including model and metrics
    """
    # Normalize input features for better training stability
    mean = np.mean(train_x, axis=0)
    var = np.var(train_x, axis=0)

    # Define model architecture
    model = keras.Sequential(
        [
            keras.Input([train_x.shape[1]]),
            keras.layers.Normalization(mean=mean, variance=var),
            keras.layers.Dense(64, activation="relu"),
            keras.layers.Dropout(0.2),  # Add regularization
            keras.layers.Dense(32, activation="relu"),
            keras.layers.Dense(1),
        ]
    )

    # Compile with specified hyperparameters
    model.compile(
        optimizer=keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum),
        loss="mean_squared_error",
        metrics=[keras.metrics.RootMeanSquaredError()],
    )

    # Train with early stopping for efficiency
    early_stopping = keras.callbacks.EarlyStopping(
        patience=3, restore_best_weights=True
    )

    # Train the model
    history = model.fit(
        train_x,
        train_y,
        validation_data=(valid_x, valid_y),
        epochs=epochs,
        batch_size=64,
        callbacks=[early_stopping],
        verbose=0,  # Reduce output for cleaner logs
    )

    # Evaluate on validation set
    val_loss, val_rmse = model.evaluate(valid_x, valid_y, verbose=0)

    return {
        "model": model,
        "val_rmse": val_rmse,
        "val_loss": val_loss,
        "history": history,
        "epochs_trained": len(history.history["loss"]),
    }
```

## Step 3: Set Up Hyperparameter Optimization

Now let's create the optimization framework:

```python
def objective(params):
    """
    Objective function for hyperparameter optimization.
    This function will be called by Hyperopt for each trial.
    """
    with mlflow.start_run(nested=True):
        # Log hyperparameters being tested
        mlflow.log_params(
            {
                "learning_rate": params["learning_rate"],
                "momentum": params["momentum"],
                "optimizer": "SGD",
                "architecture": "64-32-1",
            }
        )

        # Train model with current hyperparameters
        result = create_and_train_model(
            learning_rate=params["learning_rate"],
            momentum=params["momentum"],
            epochs=15,
        )

        # Log training results
        mlflow.log_metrics(
            {
                "val_rmse": result["val_rmse"],
                "val_loss": result["val_loss"],
                "epochs_trained": result["epochs_trained"],
            }
        )

        # Log the trained model
        mlflow.tensorflow.log_model(result["model"], name="model", signature=signature)

        # Log training curves as artifacts
        import matplotlib.pyplot as plt

        plt.figure(figsize=(12, 4))

        plt.subplot(1, 2, 1)
        plt.plot(result["history"].history["loss"], label="Training Loss")
        plt.plot(result["history"].history["val_loss"], label="Validation Loss")
        plt.title("Model Loss")
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.legend()

        plt.subplot(1, 2, 2)
        plt.plot(
            result["history"].history["root_mean_squared_error"], label="Training RMSE"
        )
        plt.plot(
            result["history"].history["val_root_mean_squared_error"],
            label="Validation RMSE",
        )
        plt.title("Model RMSE")
        plt.xlabel("Epoch")
        plt.ylabel("RMSE")
        plt.legend()

        plt.tight_layout()
        plt.savefig("training_curves.png")
        mlflow.log_artifact("training_curves.png")
        plt.close()

        # Return loss for Hyperopt (it minimizes)
        return {"loss": result["val_rmse"], "status": STATUS_OK}


# Define search space for hyperparameters
search_space = {
    "learning_rate": hp.loguniform("learning_rate", np.log(1e-5), np.log(1e-1)),
    "momentum": hp.uniform("momentum", 0.0, 0.9),
}

print("Search space defined:")
print("- Learning rate: 1e-5 to 1e-1 (log-uniform)")
print("- Momentum: 0.0 to 0.9 (uniform)")
```

## Step 4: Run the Hyperparameter Optimization

Execute the optimization experiment:

```python
# Create or set experiment
experiment_name = "wine-quality-optimization"
mlflow.set_experiment(experiment_name)

print(f"Starting hyperparameter optimization experiment: {experiment_name}")
print("This will run 15 trials to find optimal hyperparameters...")

with mlflow.start_run(run_name="hyperparameter-sweep"):
    # Log experiment metadata
    mlflow.log_params(
        {
            "optimization_method": "Tree-structured Parzen Estimator (TPE)",
            "max_evaluations": 15,
            "objective_metric": "validation_rmse",
            "dataset": "wine-quality",
            "model_type": "neural_network",
        }
    )

    # Run optimization
    trials = Trials()
    best_params = fmin(
        fn=objective,
        space=search_space,
        algo=tpe.suggest,
        max_evals=15,
        trials=trials,
        verbose=True,
    )

    # Find and log best results
    best_trial = min(trials.results, key=lambda x: x["loss"])
    best_rmse = best_trial["loss"]

    # Log optimization results
    mlflow.log_params(
        {
            "best_learning_rate": best_params["learning_rate"],
            "best_momentum": best_params["momentum"],
        }
    )
    mlflow.log_metrics(
        {
            "best_val_rmse": best_rmse,
            "total_trials": len(trials.trials),
            "optimization_completed": 1,
        }
    )
```

## Step 5: Analyze Results in MLflow UI

Open [http://localhost:5000](http://localhost:5000) in your browser to explore your results:

### Table View Analysis

1. **Navigate to your experiment**: Click on "wine-quality-optimization"
2. **Add key columns**: Click "Columns" and add:
   - `Metrics | val_rmse`
   - `Parameters | learning_rate`
   - `Parameters | momentum`
3. **Sort by performance**: Click the `val_rmse` column header to sort by best performance

### Visual Analysis

1. **Switch to Chart view**: Click the "Chart" tab
2. **Create parallel coordinates plot**:
   - Select "Parallel coordinates"
   - Add `learning_rate` and `momentum` as coordinates
   - Set `val_rmse` as the metric
3. **Interpret the visualization**:
   - Blue lines = better performing runs
   - Red lines = worse performing runs
   - Look for patterns in successful parameter combinations

### Key Insights to Look For

- **Learning rate patterns**: Too high causes instability, too low causes slow convergence
- **Momentum effects**: Moderate momentum (0.3-0.7) often works best
- **Training curves**: Check artifacts to see if models converged properly

## Step 6: Register Your Best Model

Time to promote your best model to production:

1. **Find the best run**: In the Table view, click on the run with the lowest `val_rmse`
2. **Navigate to model artifacts**: Scroll to the "Artifacts" section
3. **Register the model**:
   - Click "Register Model" next to the model folder
   - Enter model name: `wine-quality-predictor`
   - Add description: "Optimized neural network for wine quality prediction"
   - Click "Register"

4. **Manage model lifecycle**:
   - Go to "Models" tab in MLflow UI
   - Click on your registered model
   - Transition to "Staging" stage for testing
   - Add tags and descriptions as needed

## Step 7: Deploy Your Model Locally

Test your model with a REST API deployment:

```bash
# Serve the model (choose the version number you registered)
mlflow models serve -m "models:/wine-quality-predictor/1" --port 5002
```

:::note Port Configuration
We use port 5002 to avoid conflicts with the MLflow UI running on port 5000. In production, you'd typically use port 80 or 443.
:::

### Test Your Deployment

```bash
# Test with a sample wine
curl -X POST http://localhost:5002/invocations \
  -H "Content-Type: application/json" \
  -d '{
    "dataframe_split": {
      "columns": [
        "fixed acidity", "volatile acidity", "citric acid", "residual sugar",
        "chlorides", "free sulfur dioxide", "total sulfur dioxide", "density",
        "pH", "sulphates", "alcohol"
      ],
      "data": [[7.0, 0.27, 0.36, 20.7, 0.045, 45, 170, 1.001, 3.0, 0.45, 8.8]]
    }
  }'
```

**Expected Response:**

```json
{
  "predictions": [5.31]
}
```

This predicts a wine quality score of approximately 5.31 on the 3-8 scale.

### Test with Python

```python
import requests
import json

# Prepare test data
test_wine = {
    "dataframe_split": {
        "columns": [
            "fixed acidity",
            "volatile acidity",
            "citric acid",
            "residual sugar",
            "chlorides",
            "free sulfur dioxide",
            "total sulfur dioxide",
            "density",
            "pH",
            "sulphates",
            "alcohol",
        ],
        "data": [[7.0, 0.27, 0.36, 20.7, 0.045, 45, 170, 1.001, 3.0, 0.45, 8.8]],
    }
}

# Make prediction request
response = requests.post(
    "http://localhost:5002/invocations",
    headers={"Content-Type": "application/json"},
    data=json.dumps(test_wine),
)

prediction = response.json()
print(f"Predicted wine quality: {prediction['predictions'][0]:.2f}")
```

## Step 8: Build Production Container

Create a Docker container for cloud deployment:

```bash
# Build Docker image
mlflow models build-docker \
  --model-uri "models:/wine-quality-predictor/1" \
  --name "wine-quality-api"
```

:::info Build Time
The Docker build process typically takes 3-5 minutes as it installs all dependencies and configures the runtime environment.
:::

### Test Your Container

```bash
# Run the container
docker run -p 5003:8080 wine-quality-api

# Test in another terminal
curl -X POST http://localhost:5003/invocations \
  -H "Content-Type: application/json" \
  -d '{
    "dataframe_split": {
      "columns": ["fixed acidity","volatile acidity","citric acid","residual sugar","chlorides","free sulfur dioxide","total sulfur dioxide","density","pH","sulphates","alcohol"],
      "data": [[7.0, 0.27, 0.36, 20.7, 0.045, 45, 170, 1.001, 3.0, 0.45, 8.8]]
    }
  }'
```

## Step 9: Deploy to Cloud (Optional)

Your Docker container is now ready for cloud deployment:

### Popular Cloud Options

**AWS**: Deploy to ECS, EKS, or SageMaker

```bash
# Example: Push to ECR and deploy to ECS
aws ecr create-repository --repository-name wine-quality-api
docker tag wine-quality-api:latest <your-account>.dkr.ecr.us-east-1.amazonaws.com/wine-quality-api:latest
docker push <your-account>.dkr.ecr.us-east-1.amazonaws.com/wine-quality-api:latest
```

**Azure**: Deploy to Container Instances or AKS

```bash
# Example: Deploy to Azure Container Instances
az container create \
  --resource-group myResourceGroup \
  --name wine-quality-api \
  --image wine-quality-api:latest \
  --ports 8080
```

**Google Cloud**: Deploy to Cloud Run or GKE

```bash
# Example: Deploy to Cloud Run
gcloud run deploy wine-quality-api \
  --image gcr.io/your-project/wine-quality-api \
  --platform managed \
  --port 8080
```

**Databricks**: Deploy with Mosaic AI Model Serving

```python
# First, register your model in Unity Catalog
import mlflow

mlflow.set_registry_uri("databricks-uc")

with mlflow.start_run():
    # Log your model to Unity Catalog
    mlflow.tensorflow.log_model(
        model,
        name="wine-quality-model",
        registered_model_name="main.default.wine_quality_predictor",
    )

# Then create a serving endpoint using the Databricks UI:
# 1. Navigate to "Serving" in the Databricks workspace
# 2. Click "Create serving endpoint"
# 3. Select your registered model from Unity Catalog
# 4. Configure compute and traffic settings
# 5. Deploy and test your endpoint
```

Or use the Databricks deployment client programmatically:

```python
from mlflow.deployments import get_deploy_client

# Create deployment client
client = get_deploy_client("databricks")

# Create serving endpoint
endpoint = client.create_endpoint(
    config={
        "name": "wine-quality-endpoint",
        "config": {
            "served_entities": [
                {
                    "entity_name": "main.default.wine_quality_predictor",
                    "entity_version": "1",
                    "workload_size": "Small",
                    "scale_to_zero_enabled": True,
                }
            ]
        },
    }
)

# Query the endpoint
response = client.predict(
    endpoint="wine-quality-endpoint",
    inputs={
        "dataframe_split": {
            "columns": [
                "fixed acidity",
                "volatile acidity",
                "citric acid",
                "residual sugar",
                "chlorides",
                "free sulfur dioxide",
                "total sulfur dioxide",
                "density",
                "pH",
                "sulphates",
                "alcohol",
            ],
            "data": [[7.0, 0.27, 0.36, 20.7, 0.045, 45, 170, 1.001, 3.0, 0.45, 8.8]],
        }
    },
)
```

## What You've Accomplished

🎉 **Congratulations!** You've completed a full MLOps workflow:

- ✅ **Optimized hyperparameters** using systematic search instead of guesswork
- ✅ **Tracked 15+ experiments** with complete reproducibility
- ✅ **Visualized results** to understand parameter relationships
- ✅ **Registered your best model** with proper versioning
- ✅ **Deployed to REST API** for real-time predictions
- ✅ **Containerized for production** deployment

## Next Steps

### Enhance Your MLOps Skills

- **Advanced Optimization**: Try [Optuna](https://optuna.org/) or [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) for more sophisticated hyperparameter optimization. Both work seamlessly with MLflow.
- **Model Monitoring**: Implement drift detection and performance monitoring in production
- **A/B Testing**: Compare model versions in production using MLflow's model registry
- **CI/CD Integration**: Automate model training and deployment with GitHub Actions or similar

### Scale Your Infrastructure with a [Tracking Server](/ml/getting-started/tracking-server-overview)

- **MLflow on Kubernetes**: Deploy MLflow tracking server on K8s for team collaboration
- **Database Backend**: Use PostgreSQL or MySQL instead of file-based storage
- **Artifact Storage**: Configure S3, Azure Blob, or GCS for model artifacts
- **Authentication**: Add user management and access controls with built-in [Authentication](/self-hosting/security/basic-http-auth)

The foundation you've built here scales to any machine learning problem. The key principles—systematic experimentation, comprehensive tracking, and automated deployment—remain constant across domains and complexity levels.


--- docs/docs/classic-ml/getting-started/logging-first-model/index.mdx ---
---
sidebar-position: 3
---

import { NotebookDownloadButton } from "@site/src/components/NotebookDownloadButton";

# Your First MLflow Model: Complete Tutorial

Master the fundamentals of MLflow by building your first end-to-end machine learning workflow. This hands-on tutorial takes you from setup to deployment, covering all the essential MLflow concepts you need to succeed.

## What You'll Build

By the end of this tutorial, you'll have created a complete ML pipeline that:

- 🎯 **Predicts apple quality** using a synthetic dataset you'll generate
- 📊 **Tracks experiments** with parameters, metrics, and model artifacts
- 🔍 **Compares model performance** using the MLflow UI
- 📦 **Registers your best model** for production use
- 🚀 **Deploys a working API** for real-time predictions

:::info Perfect for Beginners
🎓 No prior MLflow experience required. We'll guide you through every concept with clear explanations and practical examples.

⏱️ Complete the full tutorial at your own pace in 30-45 minutes, with each step building naturally on the previous one.
:::

## Learning Path

This tutorial is designed as a progressive learning experience:

### **Phase 1: Setup & Foundations** (10 minutes)

- [🖥️ Start Your MLflow Tracking Server](/ml/getting-started/logging-first-model/step1-tracking-server) - Get your local environment running
- [🔌 Master the MLflow Client API](/ml/getting-started/logging-first-model/step2-mlflow-client) - Learn the programmatic interface
- [📁 Understand MLflow Experiments](/ml/getting-started/logging-first-model/step3-create-experiment) - Organize your ML work

### **Phase 2: Data & Experimentation** (15 minutes)

- [🔍 Search and Filter Experiments](/ml/getting-started/logging-first-model/step4-experiment-search) - Navigate your work efficiently
- [🍎 Generate Your Apple Dataset](/ml/getting-started/logging-first-model/step5-synthetic-data) - Create realistic training data
- [📈 Log Your First ML Runs](/ml/getting-started/logging-first-model/step6-logging-a-run) - Track parameters, metrics, and models

## What Makes This Tutorial Special

### **Real-World Focused**

Instead of toy examples, you'll work with a realistic apple quality prediction problem that demonstrates practical ML workflows.

### **Hands-On Learning**

Every concept is immediately applied through code examples that you can run and modify.

### **Complete Workflow**

Experience the full ML lifecycle from data creation to model deployment, not just isolated features.

### **Visual Learning**

Extensive use of the MLflow UI helps you understand how tracking data appears in practice.

## Prerequisites

- **Python 3.8+** installed on your system
- **Basic Python knowledge** (variables, functions, loops)
- **10 minutes** for initial setup

No machine learning expertise required - we'll explain the ML concepts as we go!

## Two Ways to Follow Along

### **Interactive Web Tutorial** (Recommended)

Follow the step-by-step guide in your browser with detailed explanations and screenshots. Perfect for understanding concepts deeply.

[▶️ **Start the Interactive Tutorial**](/ml/getting-started/logging-first-model/step1-tracking-server)

### **Jupyter Notebook**

Download and run the complete tutorial locally. Great for experimentation and customization.

<NotebookDownloadButton href="https://raw.githubusercontent.com/mlflow/mlflow/master/docs/docs/classic-ml/getting-started/logging-first-model/notebooks/logging-first-model.ipynb">📓 Download the Complete Notebook</NotebookDownloadButton>

## Key Concepts You'll Master

**🖥️ MLflow Tracking Server**
Set up and connect to the central hub that stores all your ML experiments and artifacts.

**🔬 Experiments & Runs**
Organize your ML work into logical groups and track individual training sessions with complete reproducibility.

**📊 Metrics & Parameters**
Log training performance, hyperparameters, and model configuration for easy comparison and optimization.

**🤖 Model Artifacts**
Save trained models with proper versioning and metadata for consistent deployment and sharing.

**🏷️ Tags & Organization**
Use tags and descriptions to keep your experiments organized and searchable as your projects grow.

**🔍 Search & Discovery**
Find and compare experiments efficiently using MLflow's powerful search and filtering capabilities.

## What Happens Next

After completing this tutorial, you'll be ready to:

- **Apply MLflow to your own projects** with confidence in the core concepts
- **Explore advanced features** like hyperparameter tuning and A/B testing
- **Scale to team workflows** with shared tracking servers and model registries
- **Deploy production models** using MLflow's serving capabilities

## Ready to Begin?

Choose your preferred learning style and dive in! The tutorial is designed to be completed in one session, but you can also bookmark your progress and return anytime.

:::tip Get Started Now
**Interactive Tutorial**: [🚀 Start Step 1 - Tracking Server](/ml/getting-started/logging-first-model/step1-tracking-server)

**Notebook Version**: Use the download button above to get the complete Jupyter notebook
:::

---

**Questions or feedback?** This tutorial is continuously improved based on user input. Let us know how we can make your learning experience even better!


--- docs/docs/classic-ml/tracking/quickstart/index.mdx ---
---
sidebar_position: 2
---

import { CardGroup, PageCard } from "@site/src/components/Card";
import Link from "@docusaurus/Link";
import { NotebookDownloadButton } from "@site/src/components/NotebookDownloadButton";
import { Table } from "@site/src/components/Table";

# MLflow Tracking Quickstart

Welcome to MLflow!

The purpose of this quickstart is to provide a quick guide to the most essential core APIs of MLflow Tracking.
Specifically, those that enable the logging, registering, and loading of a model for inference.

:::note
For a more in-depth and tutorial-based approach (if that is your style), please see the
[Getting Started with MLflow](/ml/getting-started/logging-first-model) tutorial. We recommend that you start here first, though, as this quickstart
uses the most common and frequently-used APIs for MLflow Tracking and serves as a good foundation for the other tutorials in the documentation.
:::

## What you will learn

In just a few minutes of following along with this quickstart, you will learn:

- How to **log** parameters, metrics, and a model
- The basics of the **MLflow fluent API**
- How to **register** a model during logging
- How to navigate to a model in the **MLflow UI**
- How to **load** a logged model for inference

:::note
If you would prefer to view a Jupyter Notebook version of this tutorial, click the following link:

<Link className="button button--primary" to="notebooks/tracking_quickstart" target="_blank">
  <span>View the Notebook</span>
</Link>
:::

## Step 1 - Get MLflow

MLflow is available on PyPI.

### Installing Stable Release

If you don't already have it installed on your system, you can install it with:

```bash
pip install mlflow
```

### Installing a Release Candidate (RC)

If you are eager to test out new features and validate that an upcoming release of MLflow will work well in your infrastructure, installing the latest
release candidate may be of interest to you.

:::note
Release Candidate builds are not recommended for actual use, rather they are intended only for testing validation.
:::

To install the latest version of MLflow's release candidates for a given version, see the example below that uses MLflow 2.14.0 as an example:

```bash
# install the latest release candidate
pip install --pre mlflow

# or install a specific rc version
pip install mlflow==3.1.0rc0
```

## Step 2 - Start a Tracking Server

### Using a Managed MLflow Tracking Server

For details on options for using a managed MLflow Tracking Server, including how to create a Databricks Free Trial account with
managed MLflow, [see the guide for tracking server options](/ml/getting-started/running-notebooks/).

### Run a local Tracking Server

We're going to start a local MLflow Tracking Server, which we will connect to for logging our data for this quickstart.
From a terminal, run:

```bash
mlflow server --host 127.0.0.1 --port 8080
```

:::note
You can choose any port that you would like, provided that it's not already in use.
:::

### Set the Tracking Server URI (if not using a Databricks Managed MLflow Tracking Server)

If you're using a managed MLflow Tracking Server that is not provided by Databricks, or if you're running a local tracking server,
ensure that you set the tracking server's uri using:

```python
import mlflow

mlflow.set_tracking_uri(uri="http://<host>:<port>")
```

If this is not set within your notebook or runtime environment, the runs will be logged to your local file system.

## Step 3 - Train a model and prepare metadata for logging

In this section, we're going to log a model with MLflow. A quick overview of the steps are:

- Load and prepare the Iris dataset for modeling.
- Train a Logistic Regression model and evaluate its performance.
- Prepare the model hyperparameters and calculate metrics for logging.

```python
import mlflow
from mlflow.models import infer_signature

import pandas as pd
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


# Load the Iris dataset
X, y = datasets.load_iris(return_X_y=True)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Define the model hyperparameters
params = {
    "solver": "lbfgs",
    "max_iter": 1000,
    "random_state": 8888,
}

# Train the model
lr = LogisticRegression(**params)
lr.fit(X_train, y_train)

# Predict on the test set
y_pred = lr.predict(X_test)

# Calculate metrics
accuracy = accuracy_score(y_test, y_pred)
```

## Step 4 - Log the model and its metadata to MLflow

In this next step, we're going to use the model that we trained, the hyperparameters that we specified for the model's fit, and the
loss metrics that were calculated by evaluating the model's performance on the test data to log to MLflow.

The steps that we will take are:

- Initiate an MLflow **run** context to start a new run that we will log the model and metadata to.
- **Log** model **parameters** and performance **metrics**.
- **Tag** the run for easy retrieval.
- **Register** the model in the MLflow Model Registry while **logging** (saving) the model.

:::note
While it can be valid to wrap the entire code within the `start_run` block, this is **not recommended**. If there as in issue with the
training of the model or any other portion of code that is unrelated to MLflow-related actions, an empty or partially-logged run will be
created, which will necessitate manual cleanup of the invalid run. It is best to keep the training execution outside of the run context block
to ensure that the loggable content (parameters, metrics, artifacts, and the model) are fully materialized prior to logging.
:::

```python
# Set our tracking server uri for logging
mlflow.set_tracking_uri(uri="http://127.0.0.1:8080")

# Create a new MLflow Experiment
mlflow.set_experiment("MLflow Quickstart")

# Start an MLflow run
with mlflow.start_run():
    # Log the hyperparameters
    mlflow.log_params(params)

    # Log the loss metric
    mlflow.log_metric("accuracy", accuracy)

    # Infer the model signature
    signature = infer_signature(X_train, lr.predict(X_train))

    # Log the model, which inherits the parameters and metric
    model_info = mlflow.sklearn.log_model(
        sk_model=lr,
        name="iris_model",
        signature=signature,
        input_example=X_train,
        registered_model_name="tracking-quickstart",
    )

    # Set a tag that we can use to remind ourselves what this model was for
    mlflow.set_logged_model_tags(
        model_info.model_id, {"Training Info": "Basic LR model for iris data"}
    )
```

:::note Console Output
When you run the above code, you will see console output that looks something like this:

```text
2025/09/09 17:22:20 ERROR mlflow.webhooks.delivery: Failed to deliver webhook for event registered_model.created: FileStore does not support list_webhooks_by_event
Traceback (most recent call last):
  ...
NotImplementedError: FileStore does not support list_webhooks_by_event
```

You can ignore these errors. They will go away in [MLflow 4](https://github.com/mlflow/mlflow/issues/17562#issuecomment-3259679453) and will not be fixed until then.
:::

## Step 5 - Load the model as a Python Function (pyfunc) and use it for inference

After logging the model, we can perform inference by:

- **Loading** the model using MLflow's `pyfunc` flavor.
- Running **Predict** on new data using the loaded model.

:::note
The iris training data that we used was a numpy array structure. However, we can submit a Pandas DataFrame as well to the `predict` method, as shown
below.
:::

```python
# Load the model back for predictions as a generic Python Function model
loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)

predictions = loaded_model.predict(X_test)

iris_feature_names = datasets.load_iris().feature_names

result = pd.DataFrame(X_test, columns=iris_feature_names)
result["actual_class"] = y_test
result["predicted_class"] = predictions

result[:4]
```

The output of this code will look something like this:

<Table>
  <thead>
    <tr>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>actual_class</th>
      <th>predicted_class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>6.1</td>
      <td>2.8</td>
      <td>4.7</td>
      <td>1.2</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>5.7</td>
      <td>3.8</td>
      <td>1.7</td>
      <td>0.3</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>7.7</td>
      <td>2.6</td>
      <td>6.9</td>
      <td>2.3</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <td>6.0</td>
      <td>2.9</td>
      <td>4.5</td>
      <td>1.5</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</Table>

## Step 6 - View the Run and Model in the MLflow UI

In order to see the results of our run, we can navigate to the MLflow UI. Since we have already started the Tracking Server at
_http://localhost:8080_, we can simply navigate to that URL in our browser.

When opening the site, you will see a screen similar to the following:

<figure className="center-div" style={{ width: 1024, maxWidth: "100%", textAlign: "center" }}>
  ![MLflow UI Experiment view page](/images/tutorials/introductory/quickstart-tracking/quickstart-our-experiment.png)
  <figcaption>The main MLflow Tracking page, showing Experiments that have been created</figcaption>
</figure>

Clicking on the name of the Experiment that we created ("MLflow Quickstart") will give us a list of runs associated with the
Experiment. You should see a random name that has been generated for the run and nothing else show up in the `Table` list view to the right.

Clicking on the name of the run will take you to the Run page, where the details of what we've logged will be shown. The elements have
been highlighted below to show how and where this data is recorded within the UI.

<figure className="center-div" style={{ width: 1024, maxWidth: "100%", textAlign: "center" }}>
  ![MLflow UI Run view page](/images/tutorials/introductory/quickstart-tracking/quickstart-our-run.png)
  <figcaption>The run view page for our run</figcaption>
</figure>

Switch to the Models tab in the experiments page to view all the logged models under the Experiment, where you can see an entry for the logged model we just created ("tracking-quickstart").

<figure className="center-div" style={{ width: 1024, maxWidth: "100%", textAlign: "center" }}>
  ![MLflow UI Experiment view page models tab](/images/tutorials/introductory/quickstart-tracking/quickstart-our-experiment-models-tab.png)
  <figcaption>The models tab of the MLflow Tracking page, showing a list of all models created</figcaption>
</figure>

Clicking on the name of the model will take you to the Logged Model page, with details on the logged model and its metadata.

<figure className="center-div" style={{ width: 1024, maxWidth: "100%", textAlign: "center" }}>
  ![MLflow UI Model view page](/images/tutorials/introductory/quickstart-tracking/quickstart-our-model.png)
  <figcaption>The model view page for our logged model</figcaption>
</figure>

## Conclusion

Congratulations on working through the MLflow Tracking Quickstart! You should now have a basic understanding of how to use the MLflow Tracking API to log
models.

If you are interested in a more in-depth tutorial, please see the [Getting Started with MLflow](/ml/getting-started/logging-first-model) tutorial as a
good next step in increasing your knowledge about MLflow!


--- examples/README.md ---
## MLflow examples

### Quick Start example

- `quickstart/mlflow_tracking.py` is a basic example to introduce MLflow concepts.

## Tutorials

Various examples that depict MLflow tracking, project, and serving use cases.

- `h2o` depicts how MLflow can be use to track various random forest architectures to train models
  for predicting wine quality.
- `hyperparam` shows how to do hyperparameter tuning with MLflow and some popular optimization libraries.
- `keras` modifies
  [a Keras classification example](https://github.com/keras-team/keras/blob/ed07472bc5fc985982db355135d37059a1f887a9/examples/reuters_mlp.py)
  and uses MLflow's `mlflow.tensorflow.autolog()` API to automatically log metrics and parameters
  to MLflow during training.
- `multistep_workflow` is an end-to-end of a data ETL and ML training pipeline built as an MLflow
  project. The example shows how parts of the workflow can leverage from previously run steps.
- `pytorch` uses CNN on MNIST dataset for character recognition. The example logs TensorBoard events
  and stores (logs) them as MLflow artifacts.
- `remote_store` has a usage example of REST based backed store for tracking.
- `r_wine` demonstrates how to log parameters, metrics, and models from R.
- `sklearn_elasticnet_diabetes` uses the sklearn diabetes dataset to predict diabetes progression
  using ElasticNet.
- `sklearn_elasticnet_wine_quality` is an example for MLflow projects. This uses the Wine
  Quality dataset and Elastic Net to predict quality. The example uses `MLproject` to set up a
  Conda environment, define parameter types and defaults, entry point for training, etc.
- `sklearn_logistic_regression` is a simple MLflow example with hooks to log training data to MLflow
  tracking server.
- `supply_chain_security` shows how to strengthen the security of ML projects against supply-chain attacks by enforcing hash checks on Python packages.
- `tensorflow` contains end-to-end one run examples from train to predict for TensorFlow 2.8+ It includes usage of MLflow's
  `mlflow.tensorflow.autolog()` API, which captures TensorBoard data and logs to MLflow with no code change.
- `docker` demonstrates how to create and run an MLflow project using docker (rather than conda)
  to manage project dependencies
- `johnsnowlabs` gives you access to [20.000+ state-of-the-art enterprise NLP models in 200+ languages](https://nlp.johnsnowlabs.com/models) for medical, finance, legal and many more domains.

## Demos

- `demos` folder contains notebooks used during MLflow presentations.


--- examples/auth/README.md ---
# Basic authentication example

This example demonstrates the authentication and authorization feature of MLflow.

To run this example,

1. Start the tracking server
   ```shell
   mlflow ui --app-name=basic-auth
   ```
2. Go to `http://localhost:5000/signup` and register two users:
   - `(user_a, password_a)`
   - `(user_b, password_b)`
3. Run the script
   ```shell
   python auth.py
   ```
   Expected output:
   ```
   2023/05/02 14:03:58 INFO mlflow.tracking.fluent: Experiment with name 'experiment_a' does not exist. Creating a new experiment.
   {}
   API request to endpoint /api/2.0/mlflow/runs/create failed with error code 403 != 200. Response body: 'Permission denied'
   ```


--- examples/deployments/README.md ---
# MLflow Deployments

The examples provided within this directory show how to get started with MLflow Deployments using:

- Databricks (see the `databricks` subdirectory)


--- examples/evaluation/README.md ---
### MLflow evaluation Examples

The examples in this directory demonstrate how to use the `mlflow.evaluate()` API. Specifically,
they show how to evaluate a PyFunc model on a specified dataset using the builtin default evaluator
and specified extra metrics, where the resulting metrics & artifacts are logged to MLflow Tracking.
They also show how to specify validation thresholds for the resulting metrics to validate the quality
of your model. See full list of examples below:

- Example `evaluate_on_binary_classifier.py` evaluates an xgboost `XGBClassifier` model on dataset loaded by
  `shap.datasets.adult`.
- Example `evaluate_on_multiclass_classifier.py` evaluates a scikit-learn `LogisticRegression` model on dataset
  generated by `sklearn.datasets.make_classification`.
- Example `evaluate_on_regressor.py` evaluate as scikit-learn `LinearRegression` model on dataset loaded by
  `sklearn.datasets.fetch_california_housing`
- Example `evaluate_with_custom_metrics.py` evaluates a scikit-learn `LinearRegression`
  model with a custom metric function on dataset loaded by `sklearn.datasets.fetch_california_housing`
- Example `evaluate_with_custom_metrics_comprehensive.py` evaluates a scikit-learn `LinearRegression` model
  with a comprehensive list of custom metric functions on dataset loaded by `sklearn.datasets.fetch_california_housing`
- Example `evaluate_with_model_validation.py` trains both a candidate xgboost `XGBClassifier` model
  and a baseline `DummyClassifier` model on dataset loaded by `shap.datasets.adult`. Then, it validates
  the candidate model against specified thresholds on both builtin and extra metrics and the dummy model.

#### Prerequisites

```
pip install scikit-learn xgboost shap>=0.40 matplotlib
```

#### How to run the examples

Run in this directory with Python.

```sh
python evaluate_on_binary_classifier.py
python evaluate_on_multiclass_classifier.py
python evaluate_on_regressor.py
python evaluate_with_custom_metrics.py
python evaluate_with_custom_metrics_comprehensive.py
python evaluate_with_model_vaidation.py
```


--- examples/gateway/README.md ---
# MLflow AI Gateway

The examples provided within this directory show how to get started with individual providers and at least
one of the supported endpoint types. When configuring an instance of the MLflow AI Gateway, multiple providers,
instances of endpoint types, and model versions can be specified for each query endpoint on the server.

## Example configuration files

Within this directory are example config files for each of the supported providers. If using these as a guide
for configuring a large number of endpoints, ensure that the placeholder names (i.e., "completions", "chat", "embeddings")
are modified to prevent collisions. These names are provided for clarity only for the examples and real-world
use cases should define a relevant and meaningful endpoint name to eliminate ambiguity and minimize the chances of name collisions.

# Getting Started with MLflow AI Gateway for OpenAI

This guide will walk you through the installation and basic setup of the MLflow AI Gateway.
Within sub directories of this examples section, you can find specific executable examples
that can be used to validate a given provider's configuration through the MLflow AI Gateway.
Let's get started.

## Step 1: Installing the MLflow AI Gateway

The MLflow AI Gateway is best installed from PyPI. Open your terminal and use the following pip command:

```sh
# Installation from PyPI
pip install 'mlflow[genai]'
```

For those interested in development or in using the most recent build of the MLflow AI Gateway, you may choose to install from the fork of the repository:

```sh
# Installation from the repository
pip install -e '.[genai]'
```

## Step 2: Configuring Endpoints

Each provider has a distinct set of allowable endpoint types (i.e., chat, completions, etc) and
specific requirements for the initialization of the endpoints to interface with their services.
For full examples of configurations and supported endpoint types, see:

- [OpenAI](openai/config.yaml)
- [MosaicML](mosaicml/config.yaml)
- [Anthropic](anthropic/config.yaml)
- [Cohere](cohere/config.yaml)
- [AI21 Labs](ai21labs/config.yaml)
- [PaLM](palm/config.yaml)
- [AzureOpenAI](azure_openai/config.yaml)
- [Mistral](mistral/config.yaml)
- [TogetherAI](togetherai/config.yaml)

## Step 3: Setting Access Keys

See information on specific methods of obtaining and setting the access keys within the provider-specific documentation within this directory.

## Step 4: Starting the MLflow AI Gateway

With the MLflow configuration file in place and access key(s) set, you can now start the MLflow AI Gateway.
Replace `<provider>` with the actual path to the MLflow configuration file for the provider of your choice:

```sh
mlflow gateway start --config-path examples/gateway/<provider>/config.yaml --port 7000

# For example:
mlflow gateway start --config-path examples/gateway/openai/config.yaml --port 7000
```

## Step 5: Accessing the Interactive API Documentation

With the MLflow AI Gateway up and running, access its interactive API documentation by navigating to the following URL:

http://127.0.0.1:7000/docs

## Step 6: Sending Test Requests

After successfully setting up the MLflow AI Gateway, you can send a test request using the provided Python script.
Replace <provider> with the name of the provider example test script that you'd like to use:

```sh
python examples/gateway/<provider>/example.py
```


--- examples/lightgbm/README.md ---
# Examples for LightGBM Autologging

LightGBM autologging functionalities are demonstrated through two examples. The first example in the `lightgbm_native` folder logs a Booster model trained by `lightgbm.train()`. The second example in the `lightgbm_sklearn` folder shows how autologging works for LightGBM scikit-learn models. The autologging for all LightGBM models is enabled via `mlflow.lightgbm.autolog()`.


--- examples/llms/README.md ---
# MLflow examples for LLM use cases

This directory includes several examples for tracking, evaluating, and scoring models with LLMs.

## Summarization

The `summarization/summarization.py` script uses prompt engineering to build two summarization models for news articles with LangChain. It leverages the `mlflow.langchain` flavor to package and log the models to MLflow, `mlflow.evaluate()` to evaluate each model's performance on a small example dataset, and `mlflow.pyfunc.load_model()` to load and score the best packaged model on a new example article.

To run the example as an MLflow Project, simply execute the following command from this directory:

```
$ cd summarization && mlflow run .
```

To run the example as a Python script, simply execute the following command from this directory:

```
$ cd summarization && python summarization.py
```

Note that this example requires MLflow 2.4.0 or greater to run. Additionally, you must have [LangChain](https://python.langchain.com/en/latest/index.html) and the [OpenAI Python client](https://pypi.org/project/openai/) installed in order to run the example. We also recommend installing the [Hugging Face Evaluate library](https://huggingface.co/docs/evaluate/index) to compute [ROUGE metrics](<https://en.wikipedia.org/wiki/ROUGE_(metric)>) for summary quality. Finally, you must specify a valid OpenAI API key in the `OPENAI_API_KEY` environment variable.

## Question answering

The `question_answering/question_answering.py` script uses prompt engineering to build two models that answer questions about MLflow.

It leverages the `mlflow.openai` flavor to package and log the models to MLflow, `mlflow.evaluate()` to evaluate each model's performance on some example questions, and `mlflow.pyfunc.load_model()` to load and score the best packaged model on a new example question.

To run the example as an MLflow Project, simply execute the following command from this directory:

```
$ cd question_answering && mlflow run .
```

To run the example as a Python script, simply execute the following command from this directory:

```
$ cd question_answering && python question_answering.py
```

Note that this example requires MLflow 2.4.0 or greater to run. Additionally, you must have the [OpenAI Python client](https://pypi.org/project/openai/), [tiktoken](https://pypi.org/project/tiktoken/), and [tenacity](https://pypi.org/project/tenacity/) installed in order to run the example. Finally, you must specify a valid OpenAI API key in the `OPENAI_API_KEY` environment variable.


--- examples/mlflow_artifacts/README.md ---
# MLflow Artifacts Example

This directory contains a set of files for demonstrating the MLflow Artifacts Service.

## What does the MLflow Artifacts Service do?

The MLflow Artifacts Service serves as a proxy between the client and artifact storage (e.g. S3)
and allows the client to upload, download, and list artifacts via REST API without configuring
a set of credentials required to access resources in the artifact storage (e.g. `AWS_ACCESS_KEY_ID`
and `AWS_SECRET_ACCESS_KEY` for S3).

## Quick start

First, launch the tracking server with the artifacts service via `mlflow server`:

```sh
# Launch a tracking server with the artifacts service
$ mlflow server \
    --backend-store-uri=mlruns \
    --artifacts-destination ./mlartifacts \
    --default-artifact-root http://localhost:5000/api/2.0/mlflow-artifacts/artifacts/experiments \
    --gunicorn-opts "--log-level debug"
```

Notes:

- `--artifacts-destination` specifies the base artifact location from which to resolve artifact upload/download/list requests. In this examples, we're using a local directory `./mlartifacts`, but it can be changed to a s3 bucket or
- `--default-artifact-root` points to the `experiments` directory of the artifacts service. Therefore, the default artifact location of a newly-created experiment is set to `./mlartifacts/experiments/<experiment_id>`.
- `--gunicorn-opts "--log-level debug"` is specified to print out request logs but can be omitted if unnecessary.
- `--artifacts-only` disables all other endpoints for the tracking server apart from those involved in listing, uploading, and downloading artifacts. This makes the MLflow server a single-purpose proxy for artifact handling only.

Then, run `example.py` that performs upload, download, and list operations for artifacts:

```
$ MLFLOW_TRACKING_URI=http://localhost:5000 python example.py
```

After running the command above, the server should print out request logs for artifact operations:

```diff
...
[2021-11-05 19:13:34 +0900] [92800] [DEBUG] POST /api/2.0/mlflow/runs/create
[2021-11-05 19:13:34 +0900] [92800] [DEBUG] GET /api/2.0/mlflow/runs/get
[2021-11-05 19:13:34 +0900] [92802] [DEBUG] PUT /api/2.0/mlflow-artifacts/artifacts/0/a1b2c3d4/artifacts/a.txt
[2021-11-05 19:13:34 +0900] [92802] [DEBUG] PUT /api/2.0/mlflow-artifacts/artifacts/0/a1b2c3d4/artifacts/dir/b.txt
[2021-11-05 19:13:34 +0900] [92802] [DEBUG] POST /api/2.0/mlflow/runs/update
[2021-11-05 19:13:34 +0900] [92802] [DEBUG] GET /api/2.0/mlflow-artifacts/artifacts
...
```

The contents of the `mlartifacts` directory should look like this:

```sh
$ tree mlartifacts
mlartifacts
└── experiments
    └── 0  # experiment ID
        └── a1b2c3d4  # run ID
            └── artifacts
                ├── a.txt
                └── dir
                    └── b.txt

5 directories, 2 files
```

To delete the logged artifacts, run the following command:

```bash
mlflow gc --backend-store-uri=mlruns --run-ids <run_id>
```

### Clean up

```sh
# Remove experiment and run data
$ rm -rf mlruns

# Remove artifacts
$ rm -rf mlartifacts
```

## Advanced example using `docker-compose`

[`docker-compose.yml`](./docker-compose.yml) provides a more advanced setup than the quick-start example above:

- Tracking service uses PostgreSQL as a backend store.
- Artifact service uses MinIO as a artifact store.
- Tracking and artifacts services are running on different servers.

```sh
# Build services
$ docker-compose build

# Launch tracking and artifacts servers in the background
$ docker-compose up -d

# Run `example.py` in the client container
$ docker-compose run -v ${PWD}/example.py:/app/example.py client python example.py
```

You can view the logged artifacts on MinIO Console served at http://localhost:9001. The login username and password are `user` and `password`.

### Clean up

```sh
# Remove containers, networks, volumes, and images
$ docker-compose down --rmi all --volumes --remove-orphans
```

### Development

```sh
# Build services using the dev version of mlflow
$ ./build.sh
$ docker-compose run -v ${PWD}/example.py:/app/example.py client python example.py
```


--- examples/pyfunc/README.md ---
# Pyfunc model example

This example demonstrates the use of a pyfunc model with custom inference logic.
More specifically:

- train a simple classification model
- create a _pyfunc_ model that encapsulates the classification model with an attached module for custom inference logic

## Structure of this example

This examples contains a `train.py` file that trains a scikit-learn model with iris dataset and uses MLflow Tracking APIs to log the model. The nested **mlflow run** delivers the packaging of `pyfunc` model and `custom_code` module is attached
to act as a custom inference logic layer in inference time.

```
├── train.py
├── infer_model_code_path.py
└── custom_code.py
```

## Running this example

1. Train and log the model

```
$ python train.py
```

or train and log the model using inferred code paths

```
$ python infer_model_code_paths.py
```

2. Serve the pyfunc model

```bash
# Replace <pyfunc_run_id> with the run ID obtained in the previous step
$ mlflow models serve -m "runs:/<pyfunc_run_id>/model" -p 5001
```

3. Send a request

```
$ curl http://127.0.0.1:5001/invocations -H 'Content-Type: application/json' -d '{
  "dataframe_records": [[1, 1, 1, 1]]
}'
```

The response should look like this:

```
[0]
```


--- examples/pyspark_ml_autologging/README.md ---
# PySpark ML Autologging Examples

This directory contains examples for demonstrating how PySpark ML autologging works.

| File                     | Description                        |
| :----------------------- | :--------------------------------- |
| `logistic_regression.py` | Train a `LogisticRegression` model |
| `one_vs_rest.py`         | Train a `OneVsRest` model          |


--- mlflow/langchain/api_request_parallel_processor.py ---
# Based ons: https://github.com/openai/openai-cookbook/blob/6df6ceff470eeba26a56de131254e775292eac22/examples/api_request_parallel_processor.py
# Several changes were made to make it work with MLflow.
# Currently, only chat completion is supported.

"""
API REQUEST PARALLEL PROCESSOR

Using the LangChain API to process lots of text quickly takes some care.
If you trickle in a million API requests one by one, they'll take days to complete.
This script parallelizes requests using LangChain API.

Features:
- Streams requests from file, to avoid running out of memory for giant jobs
- Makes requests concurrently, to maximize throughput
- Logs errors, to diagnose problems with requests
"""

from __future__ import annotations

import logging
import queue
import threading
import time
import traceback
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from typing import Any

import langchain.chains
from langchain.callbacks.base import BaseCallbackHandler

import mlflow
from mlflow.exceptions import MlflowException
from mlflow.langchain.utils.chat import (
    transform_request_json_for_chat_if_necessary,
    try_transform_response_iter_to_chat_format,
    try_transform_response_to_chat_format,
)
from mlflow.langchain.utils.serialization import convert_to_serializable
from mlflow.pyfunc.context import Context, get_prediction_context
from mlflow.tracing.utils import maybe_set_prediction_context

_logger = logging.getLogger(__name__)


@dataclass
class StatusTracker:
    """
    Stores metadata about the script's progress. Only one instance is created.
    """

    num_tasks_started: int = 0
    num_tasks_in_progress: int = 0  # script ends when this reaches 0
    num_tasks_succeeded: int = 0
    num_tasks_failed: int = 0
    num_api_errors: int = 0  # excluding rate limit errors, counted above
    lock: threading.Lock = threading.Lock()

    def start_task(self):
        with self.lock:
            self.num_tasks_started += 1
            self.num_tasks_in_progress += 1

    def complete_task(self, *, success: bool):
        with self.lock:
            self.num_tasks_in_progress -= 1
            if success:
                self.num_tasks_succeeded += 1
            else:
                self.num_tasks_failed += 1

    def increment_num_api_errors(self):
        with self.lock:
            self.num_api_errors += 1


@dataclass
class APIRequest:
    """
    Stores an API request's inputs, outputs, and other metadata. Contains a method to make an API
    call.

    Args:
        index: The request's index in the tasks list
        lc_model: The LangChain model to call
        request_json: The request's input data
        results: The list to append the request's output data to, it's a list of tuples
            (index, response)
        errors: A dictionary to store any errors that occur
        convert_chat_responses: Whether to convert the model's responses to chat format
        did_perform_chat_conversion: Whether the input data was converted to chat format
            based on the model's type and input data.
        stream: Whether the request is a stream request
        prediction_context: The prediction context to use for the request
    """

    index: int
    lc_model: langchain.chains.base.Chain
    request_json: dict[str, Any]
    results: list[tuple[int, str]]
    errors: dict[int, str]
    convert_chat_responses: bool
    did_perform_chat_conversion: bool
    stream: bool
    params: dict[str, Any]
    prediction_context: Context | None = None

    def _predict_single_input(self, single_input, callback_handlers, **kwargs):
        config = kwargs.pop("config", {})
        config["callbacks"] = config.get("callbacks", []) + (callback_handlers or [])
        if self.stream:
            return self.lc_model.stream(single_input, config=config, **kwargs)
        if hasattr(self.lc_model, "invoke"):
            return self.lc_model.invoke(single_input, config=config, **kwargs)
        else:
            # for backwards compatibility, __call__ is deprecated and will be removed in 0.3.0
            # kwargs shouldn't have config field if invoking with __call__
            return self.lc_model(single_input, callbacks=callback_handlers, **kwargs)

    def _try_convert_response(self, response):
        if self.stream:
            return try_transform_response_iter_to_chat_format(response)
        else:
            return try_transform_response_to_chat_format(response)

    def single_call_api(self, callback_handlers: list[BaseCallbackHandler] | None):
        from langchain.schema import BaseRetriever

        from mlflow.langchain.utils.logging import langgraph_types, lc_runnables_types

        if isinstance(self.lc_model, BaseRetriever):
            # Retrievers are invoked differently than Chains
            response = self.lc_model.get_relevant_documents(
                **self.request_json, callbacks=callback_handlers, **self.params
            )
        elif isinstance(self.lc_model, lc_runnables_types() + langgraph_types()):
            if isinstance(self.request_json, dict):
                # This is a temporary fix for the case when spark_udf converts
                # input into pandas dataframe with column name, while the model
                # does not accept dictionaries as input, it leads to errors like
                # Expected Scalar value for String field 'query_text'
                try:
                    response = self._predict_single_input(
                        self.request_json, callback_handlers, **self.params
                    )
                except TypeError as e:
                    _logger.debug(
                        f"Failed to invoke {self.lc_model.__class__.__name__} "
                        f"with {self.request_json}. Error: {e!r}. Trying to "
                        "invoke with the first value of the dictionary."
                    )
                    self.request_json = next(iter(self.request_json.values()))
                    (
                        prepared_request_json,
                        did_perform_chat_conversion,
                    ) = transform_request_json_for_chat_if_necessary(
                        self.request_json, self.lc_model
                    )
                    self.did_perform_chat_conversion = did_perform_chat_conversion

                    response = self._predict_single_input(
                        prepared_request_json, callback_handlers, **self.params
                    )
            else:
                response = self._predict_single_input(
                    self.request_json, callback_handlers, **self.params
                )

            if self.did_perform_chat_conversion or self.convert_chat_responses:
                response = self._try_convert_response(response)
        else:
            # return_only_outputs is invalid for stream call
            if isinstance(self.lc_model, langchain.chains.base.Chain) and not self.stream:
                kwargs = {"return_only_outputs": True}
            else:
                kwargs = {}
            kwargs.update(**self.params)
            response = self._predict_single_input(self.request_json, callback_handlers, **kwargs)

            if self.did_perform_chat_conversion or self.convert_chat_responses:
                response = self._try_convert_response(response)
            elif isinstance(response, dict) and len(response) == 1:
                # to maintain existing code, single output chains will still return
                # only the result
                response = response.popitem()[1]

        return convert_to_serializable(response)

    def call_api(
        self, status_tracker: StatusTracker, callback_handlers: list[BaseCallbackHandler] | None
    ):
        """
        Calls the LangChain API and stores results.
        """
        _logger.debug(f"Request #{self.index} started with payload: {self.request_json}")

        try:
            with maybe_set_prediction_context(self.prediction_context):
                response = self.single_call_api(callback_handlers)
            _logger.debug(f"Request #{self.index} succeeded with response: {response}")
            self.results.append((self.index, response))
            status_tracker.complete_task(success=True)
        except Exception as e:
            self.errors[self.index] = (
                f"error: {e!r} {traceback.format_exc()}\n request payload: {self.request_json}"
            )
            status_tracker.increment_num_api_errors()
            status_tracker.complete_task(success=False)


def process_api_requests(
    lc_model,
    requests: list[Any | dict[str, Any]] | None = None,
    max_workers: int = 10,
    callback_handlers: list[BaseCallbackHandler] | None = None,
    convert_chat_responses: bool = False,
    params: dict[str, Any] | None = None,
    context: Context | None = None,
):
    """
    Processes API requests in parallel.
    """

    # initialize trackers
    retry_queue = queue.Queue()
    status_tracker = StatusTracker()  # single instance to track a collection of variables
    next_request = None  # variable to hold the next request to call
    context = context or get_prediction_context()

    results = []
    errors = {}

    # Note: we should call `transform_request_json_for_chat_if_necessary`
    # for the whole batch data, because the conversion should obey the rule
    # that if any record in the batch can't be converted, then all the record
    # in this batch can't be converted.
    (
        converted_chat_requests,
        did_perform_chat_conversion,
    ) = transform_request_json_for_chat_if_necessary(requests, lc_model)

    requests_iter = enumerate(converted_chat_requests)
    with ThreadPoolExecutor(
        max_workers=max_workers, thread_name_prefix="MlflowLangChainApi"
    ) as executor:
        while True:
            # get next request (if one is not already waiting for capacity)
            if not retry_queue.empty():
                next_request = retry_queue.get_nowait()
                _logger.warning(f"Retrying request {next_request.index}: {next_request}")
            elif req := next(requests_iter, None):
                # get new request
                index, converted_chat_request_json = req
                next_request = APIRequest(
                    index=index,
                    lc_model=lc_model,
                    request_json=converted_chat_request_json,
                    results=results,
                    errors=errors,
                    convert_chat_responses=convert_chat_responses,
                    did_perform_chat_conversion=did_perform_chat_conversion,
                    stream=False,
                    prediction_context=context,
                    params=params,
                )
                status_tracker.start_task()
            else:
                next_request = None

            # if enough capacity available, call API
            if next_request:
                # call API
                executor.submit(
                    next_request.call_api,
                    status_tracker=status_tracker,
                    callback_handlers=callback_handlers,
                )

            # if all tasks are finished, break
            # check next_request to avoid terminating the process
            # before extra requests need to be processed
            if status_tracker.num_tasks_in_progress == 0 and next_request is None:
                break

            time.sleep(0.001)  # avoid busy waiting

        # after finishing, log final status
        if status_tracker.num_tasks_failed > 0:
            raise mlflow.MlflowException(
                f"{status_tracker.num_tasks_failed} tasks failed. Errors: {errors}"
            )

        return [res for _, res in sorted(results)]


def process_stream_request(
    lc_model,
    request_json: Any | dict[str, Any],
    callback_handlers: list[BaseCallbackHandler] | None = None,
    convert_chat_responses: bool = False,
    params: dict[str, Any] | None = None,
):
    """
    Process single stream request.
    """
    if not hasattr(lc_model, "stream"):
        raise MlflowException(
            f"Model {lc_model.__class__.__name__} does not support streaming prediction output. "
            "No `stream` method found."
        )

    (
        converted_chat_requests,
        did_perform_chat_conversion,
    ) = transform_request_json_for_chat_if_necessary(request_json, lc_model)

    api_request = APIRequest(
        index=0,
        lc_model=lc_model,
        request_json=converted_chat_requests,
        results=None,
        errors=None,
        convert_chat_responses=convert_chat_responses,
        did_perform_chat_conversion=did_perform_chat_conversion,
        stream=True,
        prediction_context=get_prediction_context(),
        params=params,
    )
    with maybe_set_prediction_context(api_request.prediction_context):
        return api_request.single_call_api(callback_handlers)


--- mlflow/openai/api_request_parallel_processor.py ---
# Based ons: https://github.com/openai/openai-cookbook/blob/6df6ceff470eeba26a56de131254e775292eac22/examples/api_request_parallel_processor.py
# Several changes were made to make it work with MLflow.

"""
API REQUEST PARALLEL PROCESSOR

Using the OpenAI API to process lots of text quickly takes some care.
If you trickle in a million API requests one by one, they'll take days to complete.
If you flood a million API requests in parallel, they'll exceed the rate limits and fail with
errors. To maximize throughput, parallel requests need to be throttled to stay under rate limits.

This script parallelizes requests to the OpenAI API

Features:
- Makes requests concurrently, to maximize throughput
- Retries failed requests up to {max_attempts} times, to avoid missing data
- Logs errors, to diagnose problems with requests
"""

from __future__ import annotations

import logging
import threading
from concurrent.futures import FIRST_EXCEPTION, ThreadPoolExecutor, wait
from dataclasses import dataclass
from typing import Any, Callable

import mlflow

_logger = logging.getLogger(__name__)


@dataclass
class StatusTracker:
    """Stores metadata about the script's progress. Only one instance is created."""

    num_tasks_started: int = 0
    num_tasks_in_progress: int = 0  # script ends when this reaches 0
    num_tasks_succeeded: int = 0
    num_tasks_failed: int = 0
    num_rate_limit_errors: int = 0
    lock: threading.Lock = threading.Lock()
    error = None

    def start_task(self):
        with self.lock:
            self.num_tasks_started += 1
            self.num_tasks_in_progress += 1

    def complete_task(self, *, success: bool):
        with self.lock:
            self.num_tasks_in_progress -= 1
            if success:
                self.num_tasks_succeeded += 1
            else:
                self.num_tasks_failed += 1

    def increment_num_rate_limit_errors(self):
        with self.lock:
            self.num_rate_limit_errors += 1


def call_api(
    index: int,
    results: list[tuple[int, Any]],
    task: Callable[[], Any],
    status_tracker: StatusTracker,
):
    import openai

    status_tracker.start_task()
    try:
        result = task()
        _logger.debug(f"Request #{index} succeeded")
        status_tracker.complete_task(success=True)
        results.append((index, result))
    except openai.RateLimitError as e:
        status_tracker.complete_task(success=False)
        _logger.debug(f"Request #{index} failed with: {e}")
        status_tracker.increment_num_rate_limit_errors()
        status_tracker.error = mlflow.MlflowException(
            f"Request #{index} failed with rate limit: {e}."
        )
    except Exception as e:
        status_tracker.complete_task(success=False)
        _logger.debug(f"Request #{index} failed with: {e}")
        status_tracker.error = mlflow.MlflowException(
            f"Request #{index} failed with: {e.__cause__}"
        )


def process_api_requests(
    request_tasks: list[Callable[[], Any]],
    max_workers: int = 10,
):
    """Processes API requests in parallel"""
    # initialize trackers
    status_tracker = StatusTracker()  # single instance to track a collection of variables

    results: list[tuple[int, Any]] = []
    request_tasks_iter = enumerate(request_tasks)
    _logger.debug(f"Request pool executor will run {len(request_tasks)} requests")
    with ThreadPoolExecutor(
        max_workers=max_workers, thread_name_prefix="MlflowOpenAiApi"
    ) as executor:
        futures = [
            executor.submit(
                call_api,
                index=index,
                task=task,
                results=results,
                status_tracker=status_tracker,
            )
            for index, task in request_tasks_iter
        ]
        wait(futures, return_when=FIRST_EXCEPTION)

    # after finishing, log final status
    if status_tracker.num_tasks_failed > 0:
        if status_tracker.num_tasks_failed == 1:
            raise status_tracker.error
        raise mlflow.MlflowException(
            f"{status_tracker.num_tasks_failed} tasks failed. See logs for details."
        )
    if status_tracker.num_rate_limit_errors > 0:
        _logger.debug(
            f"{status_tracker.num_rate_limit_errors} rate limit errors received. "
            "Consider running at a lower rate."
        )

    return [res for _, res in sorted(results)]


--- mlflow/utils/_capture_modules.py ---
"""
This script should be executed in a fresh python interpreter process using `subprocess`.
"""

import argparse
import builtins
import functools
import importlib
import json
import os
import sys

import mlflow
from mlflow.models.model import MLMODEL_FILE_NAME, Model
from mlflow.pyfunc import MAIN
from mlflow.utils._spark_utils import _prepare_subprocess_environ_for_creating_local_spark_session
from mlflow.utils.exception_utils import get_stacktrace
from mlflow.utils.file_utils import write_to
from mlflow.utils.requirements_utils import (
    DATABRICKS_MODULES_TO_PACKAGES,
    MLFLOW_MODULES_TO_PACKAGES,
)


def _get_top_level_module(full_module_name):
    return full_module_name.split(".")[0]


def _get_second_level_module(full_module_name):
    return ".".join(full_module_name.split(".")[:2])


class _CaptureImportedModules:
    """
    A context manager to capture imported modules by temporarily applying a patch to
    `builtins.__import__` and `importlib.import_module`.

    If `record_full_module` is set to `False`, it only captures top level modules
    for inferring python package purpose.
    If `record_full_module` is set to `True`, it captures full module name for all
    imported modules and sub-modules. This is used in automatic model code path inference.
    """

    def __init__(self, record_full_module=False):
        self.imported_modules = set()
        self.original_import = None
        self.original_import_module = None
        self.record_full_module = record_full_module

    def _wrap_import(self, original):
        @functools.wraps(original)
        def wrapper(name, globals=None, locals=None, fromlist=(), level=0):
            is_absolute_import = level == 0
            if not self.record_full_module and is_absolute_import:
                self._record_imported_module(name)

            result = original(name, globals, locals, fromlist, level)

            if self.record_full_module:
                if is_absolute_import:
                    parent_modules = name.split(".")
                else:
                    parent_modules = globals["__name__"].split(".")
                    if level > 1:
                        parent_modules = parent_modules[: -(level - 1)]

                if fromlist:
                    for from_name in fromlist:
                        full_modules = parent_modules + [from_name]
                        full_module_name = ".".join(full_modules)
                        if full_module_name in sys.modules:
                            self._record_imported_module(full_module_name)
                else:
                    full_module_name = ".".join(parent_modules)
                    self._record_imported_module(full_module_name)

            return result

        return wrapper

    def _wrap_import_module(self, original):
        @functools.wraps(original)
        def wrapper(name, *args, **kwargs):
            self._record_imported_module(name)
            return original(name, *args, **kwargs)

        return wrapper

    def _record_imported_module(self, full_module_name):
        if self.record_full_module:
            self.imported_modules.add(full_module_name)
            return

        # If the module is an internal module (prefixed by "_") or is the "databricks"
        # module, which is populated by many different packages, don't record it (specific
        # module imports within the databricks namespace are still recorded and mapped to
        # their corresponding packages)
        if full_module_name.startswith("_") or full_module_name == "databricks":
            return

        top_level_module = _get_top_level_module(full_module_name)
        second_level_module = _get_second_level_module(full_module_name)

        if top_level_module == "databricks":
            # Multiple packages populate the `databricks` module namespace on Databricks;
            # to avoid bundling extraneous Databricks packages into model dependencies, we
            # scope each module to its relevant package
            if second_level_module in DATABRICKS_MODULES_TO_PACKAGES:
                self.imported_modules.add(second_level_module)
                return

            for databricks_module in DATABRICKS_MODULES_TO_PACKAGES:
                if full_module_name.startswith(databricks_module):
                    self.imported_modules.add(databricks_module)
                    return

        # special casing for mlflow extras since they may not be required by default
        if top_level_module == "mlflow":
            if second_level_module in MLFLOW_MODULES_TO_PACKAGES:
                self.imported_modules.add(second_level_module)
                return

        self.imported_modules.add(top_level_module)

    def __enter__(self):
        # Patch `builtins.__import__` and `importlib.import_module`
        self.original_import = builtins.__import__
        self.original_import_module = importlib.import_module
        builtins.__import__ = self._wrap_import(self.original_import)
        importlib.import_module = self._wrap_import_module(self.original_import_module)
        return self

    def __exit__(self, *_, **__):
        # Revert the patches
        builtins.__import__ = self.original_import
        importlib.import_module = self.original_import_module


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model-path", required=True)
    parser.add_argument("--flavor", required=True)
    parser.add_argument("--output-file", required=True)
    parser.add_argument("--sys-path", required=True)
    parser.add_argument("--module-to-throw", required=False)
    parser.add_argument("--error-file", required=False)
    parser.add_argument("--record-full-module", default=False, action="store_true")
    return parser.parse_args()


def store_imported_modules(
    cap_cm, model_path, flavor, output_file, error_file=None, record_full_module=False
):
    # If `model_path` refers to an MLflow model directory, load the model using
    # `mlflow.pyfunc.load_model`
    if os.path.isdir(model_path) and MLMODEL_FILE_NAME in os.listdir(model_path):
        mlflow_model = Model.load(model_path)
        pyfunc_conf = mlflow_model.flavors.get(mlflow.pyfunc.FLAVOR_NAME)
        input_example = mlflow_model.load_input_example(model_path)
        params = mlflow_model.load_input_example_params(model_path)

        def load_model_and_predict(original_load_fn, *args, **kwargs):
            model = original_load_fn(*args, **kwargs)
            if input_example is not None:
                try:
                    model.predict(input_example, params=params)
                except Exception as e:
                    if error_file:
                        stack_trace = get_stacktrace(e)
                        write_to(
                            error_file,
                            "Failed to run predict on input_example, dependencies "
                            "introduced in predict are not captured.\n" + stack_trace,
                        )
                    else:
                        raise e
            return model

        if record_full_module:
            # Note: if we want to record all imported modules
            # (for inferring code_paths purpose),
            # The `importlib.import_module(pyfunc_conf[MAIN])` invocation
            # must be wrapped with `cap_cm` context manager,
            # because `pyfunc_conf[MAIN]` might also be a module loaded from
            # code_paths.
            with cap_cm:
                # `mlflow.pyfunc.load_model` internally invokes
                # `importlib.import_module(pyfunc_conf[MAIN])`
                mlflow.pyfunc.load_model(model_path)
        else:
            loader_module = importlib.import_module(pyfunc_conf[MAIN])
            original = loader_module._load_pyfunc

            @functools.wraps(original)
            def _load_pyfunc_patch(*args, **kwargs):
                with cap_cm:
                    return load_model_and_predict(original, *args, **kwargs)

            loader_module._load_pyfunc = _load_pyfunc_patch
            try:
                mlflow.pyfunc.load_model(model_path)
            finally:
                loader_module._load_pyfunc = original
    # Otherwise, load the model using `mlflow.<flavor>._load_pyfunc`.
    # For models that don't contain pyfunc flavor (e.g. scikit-learn estimator
    # that doesn't implement a `predict` method),
    # we need to directly pass a model data path to this script.
    else:
        with cap_cm:
            importlib.import_module(f"mlflow.{flavor}")._load_pyfunc(model_path)

    # Store the imported modules in `output_file`
    write_to(output_file, "\n".join(cap_cm.imported_modules))


def main():
    args = parse_args()
    model_path = args.model_path
    flavor = args.flavor
    output_file = args.output_file
    error_file = args.error_file
    # Mirror `sys.path` of the parent process
    sys.path = json.loads(args.sys_path)

    if flavor == mlflow.spark.FLAVOR_NAME:
        # Create a local spark environment within the subprocess
        from mlflow.utils._spark_utils import _create_local_spark_session_for_loading_spark_model

        _prepare_subprocess_environ_for_creating_local_spark_session()
        _create_local_spark_session_for_loading_spark_model()

    cap_cm = _CaptureImportedModules(record_full_module=args.record_full_module)
    store_imported_modules(
        cap_cm,
        model_path,
        flavor,
        output_file,
        error_file,
        record_full_module=args.record_full_module,
    )

    # Clean up a spark session created by `mlflow.spark._load_pyfunc`
    if flavor == mlflow.spark.FLAVOR_NAME:
        from mlflow.utils._spark_utils import _get_active_spark_session

        spark = _get_active_spark_session()
        if spark:
            try:
                spark.stop()
            except Exception:
                # Swallow unexpected exceptions
                pass


if __name__ == "__main__":
    main()


--- mlflow/utils/_capture_transformers_modules.py ---
"""
This script should be executed in a fresh python interpreter process using `subprocess`.
"""

import json
import os
import sys

import mlflow
from mlflow.exceptions import MlflowException
from mlflow.protos.databricks_pb2 import INVALID_PARAMETER_VALUE
from mlflow.utils._capture_modules import (
    _CaptureImportedModules,
    parse_args,
    store_imported_modules,
)


class _CaptureImportedModulesForHF(_CaptureImportedModules):
    """
    A context manager to capture imported modules by temporarily applying a patch to
    `builtins.__import__` and `importlib.import_module`.
    Used for 'transformers' flavor only.
    """

    def __init__(self, module_to_throw, record_full_module=False):
        super().__init__(record_full_module=record_full_module)
        self.module_to_throw = module_to_throw

    def _record_imported_module(self, full_module_name):
        if full_module_name == self.module_to_throw or full_module_name.startswith(
            f"{self.module_to_throw}."
        ):
            raise ImportError(f"Disabled package {full_module_name}")
        return super()._record_imported_module(full_module_name)


def main():
    args = parse_args()
    model_path = args.model_path
    flavor = args.flavor
    output_file = args.output_file
    module_to_throw = args.module_to_throw
    # Mirror `sys.path` of the parent process
    sys.path = json.loads(args.sys_path)

    if flavor != mlflow.transformers.FLAVOR_NAME:
        raise MlflowException(
            f"This script is only applicable to '{mlflow.transformers.FLAVOR_NAME}' flavor, "
            "if you're applying other flavors, please use _capture_modules script.",
        )

    if module_to_throw == "":
        raise MlflowException("Please specify the module to throw.")
    elif module_to_throw == "tensorflow":
        if os.environ.get("USE_TORCH", None) != "TRUE":
            raise MlflowException(
                "The environment variable USE_TORCH has to be set to TRUE to disable Tensorflow.",
                error_code=INVALID_PARAMETER_VALUE,
            )
    elif module_to_throw == "torch":
        if os.environ.get("USE_TF", None) != "TRUE":
            raise MlflowException(
                "The environment variable USE_TF has to be set to TRUE to disable Pytorch.",
                error_code=INVALID_PARAMETER_VALUE,
            )

    cap_cm = _CaptureImportedModulesForHF(
        module_to_throw, record_full_module=args.record_full_module
    )
    store_imported_modules(cap_cm, model_path, flavor, output_file)


if __name__ == "__main__":
    main()


--- mlflow/utils/class_utils.py ---
import importlib


def _get_class_from_string(fully_qualified_class_name):
    module, class_name = fully_qualified_class_name.rsplit(".", maxsplit=1)
    return getattr(importlib.import_module(module), class_name)


--- mlflow/server/fastapi_app.py ---
"""
FastAPI application wrapper for MLflow server.

This module provides a FastAPI application that wraps the existing Flask application
using WSGIMiddleware to maintain 100% API compatibility while enabling future migration
to FastAPI endpoints.
"""

from fastapi import FastAPI
from fastapi.middleware.wsgi import WSGIMiddleware
from flask import Flask

from mlflow.server import app as flask_app
from mlflow.server.fastapi_security import init_fastapi_security
from mlflow.server.job_api import job_api_router
from mlflow.server.otel_api import otel_router
from mlflow.version import VERSION


def create_fastapi_app(flask_app: Flask = flask_app):
    """
    Create a FastAPI application that wraps the existing Flask app.

    Returns:
        FastAPI application instance with the Flask app mounted via WSGIMiddleware.
    """
    # Create FastAPI app with metadata
    fastapi_app = FastAPI(
        title="MLflow Tracking Server",
        description="MLflow Tracking Server API",
        version=VERSION,
        # TODO: Enable API documentation when we have native FastAPI endpoints
        # For now, disable docs since we only have Flask routes via WSGI
        docs_url=None,
        redoc_url=None,
        openapi_url=None,
    )

    # Initialize security middleware BEFORE adding routes
    init_fastapi_security(fastapi_app)

    # Include OpenTelemetry API router BEFORE mounting Flask app
    # This ensures FastAPI routes take precedence over the catch-all Flask mount
    fastapi_app.include_router(otel_router)

    fastapi_app.include_router(job_api_router)

    # Mount the entire Flask application at the root path
    # This ensures compatibility with existing APIs
    # NOTE: This must come AFTER include_router to avoid Flask catching all requests
    fastapi_app.mount("/", WSGIMiddleware(flask_app))

    return fastapi_app


# Create the app instance that can be used by ASGI servers
app = create_fastapi_app()


--- mlflow/server/fastapi_security.py ---
import logging
from http import HTTPStatus

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from starlette.types import ASGIApp

from mlflow.environment_variables import (
    MLFLOW_SERVER_DISABLE_SECURITY_MIDDLEWARE,
    MLFLOW_SERVER_X_FRAME_OPTIONS,
)
from mlflow.server.security_utils import (
    CORS_BLOCKED_MSG,
    HEALTH_ENDPOINTS,
    INVALID_HOST_MSG,
    get_allowed_hosts_from_env,
    get_allowed_origins_from_env,
    get_default_allowed_hosts,
    is_allowed_host_header,
    is_api_endpoint,
    should_block_cors_request,
)

_logger = logging.getLogger(__name__)


class HostValidationMiddleware:
    """Middleware to validate Host headers using fnmatch patterns."""

    def __init__(self, app: ASGIApp, allowed_hosts: list[str]):
        self.app = app
        self.allowed_hosts = allowed_hosts

    async def __call__(self, scope, receive, send):
        if scope["type"] != "http":
            return await self.app(scope, receive, send)

        if scope["path"] in HEALTH_ENDPOINTS:
            return await self.app(scope, receive, send)

        headers = dict(scope.get("headers", []))
        host = headers.get(b"host", b"").decode("utf-8")

        if not is_allowed_host_header(self.allowed_hosts, host):
            _logger.warning(f"Rejected request with invalid Host header: {host}")

            async def send_403(message):
                if message["type"] == "http.response.start":
                    message["status"] = 403
                    message["headers"] = [(b"content-type", b"text/plain")]
                await send(message)

            await send_403({"type": "http.response.start", "status": 403, "headers": []})
            await send({"type": "http.response.body", "body": INVALID_HOST_MSG.encode()})
            return

        return await self.app(scope, receive, send)


class SecurityHeadersMiddleware:
    """Middleware to add security headers to all responses."""

    def __init__(self, app: ASGIApp):
        self.app = app
        self.x_frame_options = MLFLOW_SERVER_X_FRAME_OPTIONS.get()

    async def __call__(self, scope, receive, send):
        if scope["type"] != "http":
            return await self.app(scope, receive, send)

        async def send_wrapper(message):
            if message["type"] == "http.response.start":
                headers = dict(message.get("headers", []))
                headers[b"x-content-type-options"] = b"nosniff"

                if self.x_frame_options and self.x_frame_options.upper() != "NONE":
                    headers[b"x-frame-options"] = self.x_frame_options.upper().encode()

                if (
                    scope["method"] == "OPTIONS"
                    and message.get("status") == 200
                    and is_api_endpoint(scope["path"])
                ):
                    message["status"] = HTTPStatus.NO_CONTENT

                message["headers"] = list(headers.items())
            await send(message)

        await self.app(scope, receive, send_wrapper)


class CORSBlockingMiddleware:
    """Middleware to actively block cross-origin state-changing requests."""

    def __init__(self, app: ASGIApp, allowed_origins: list[str]):
        self.app = app
        self.allowed_origins = allowed_origins

    async def __call__(self, scope, receive, send):
        if scope["type"] != "http":
            return await self.app(scope, receive, send)

        if not is_api_endpoint(scope["path"]):
            return await self.app(scope, receive, send)

        method = scope["method"]
        headers = dict(scope["headers"])
        origin = headers.get(b"origin", b"").decode("utf-8")

        if should_block_cors_request(origin, method, self.allowed_origins):
            _logger.warning(f"Blocked cross-origin request from {origin}")
            await send(
                {
                    "type": "http.response.start",
                    "status": HTTPStatus.FORBIDDEN,
                    "headers": [[b"content-type", b"text/plain"]],
                }
            )
            await send(
                {
                    "type": "http.response.body",
                    "body": CORS_BLOCKED_MSG.encode(),
                }
            )
            return

        await self.app(scope, receive, send)


def get_allowed_hosts() -> list[str]:
    """Get list of allowed hosts from environment or defaults."""
    return get_allowed_hosts_from_env() or get_default_allowed_hosts()


def get_allowed_origins() -> list[str]:
    """Get list of allowed CORS origins from environment or defaults."""
    return get_allowed_origins_from_env() or []


def init_fastapi_security(app: FastAPI) -> None:
    """
    Initialize security middleware for FastAPI application.

    This configures:
    - Host header validation (DNS rebinding protection) via TrustedHostMiddleware
    - CORS protection via CORSMiddleware
    - Security headers via custom middleware

    Args:
        app: FastAPI application instance.
    """
    if MLFLOW_SERVER_DISABLE_SECURITY_MIDDLEWARE.get() == "true":
        return

    app.add_middleware(SecurityHeadersMiddleware)

    allowed_origins = get_allowed_origins()

    if allowed_origins and "*" in allowed_origins:
        app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
            expose_headers=["*"],
        )
    else:
        app.add_middleware(CORSBlockingMiddleware, allowed_origins=allowed_origins)
        app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "PATCH"],
            allow_headers=["*"],
            expose_headers=["*"],
        )

    allowed_hosts = get_allowed_hosts()

    if allowed_hosts and "*" not in allowed_hosts:
        app.add_middleware(HostValidationMiddleware, allowed_hosts=allowed_hosts)


--- tests/pytorch/iris_data_module.py ---
import pytorch_lightning as pl
import torch
from sklearn.datasets import load_iris
from torch.utils.data import DataLoader, TensorDataset, random_split


class IrisDataModuleBase(pl.LightningDataModule):
    def __init__(self):
        super().__init__()
        self.columns = None

    def _get_iris_as_tensor_dataset(self):
        iris = load_iris()
        df = iris.data
        self.columns = iris.feature_names
        target = iris["target"]
        data = torch.Tensor(df).float()
        labels = torch.Tensor(target).long()
        return TensorDataset(data, labels)

    def setup(self, stage=None):
        # Assign train/val datasets for use in dataloaders
        if stage == "fit" or stage is None:
            iris_full = self._get_iris_as_tensor_dataset()
            self.train_set, self.val_set = random_split(iris_full, [130, 20])

        # Assign test dataset for use in dataloader(s)
        if stage == "test" or stage is None:
            self.train_set, self.test_set = random_split(self.train_set, [110, 20])


class IrisDataModule(IrisDataModuleBase):
    def train_dataloader(self):
        return DataLoader(self.train_set, batch_size=4)

    def val_dataloader(self):
        return DataLoader(self.val_set, batch_size=4)

    def test_dataloader(self):
        return DataLoader(self.test_set, batch_size=4)


class IrisDataModuleWithoutValidation(IrisDataModuleBase):
    def train_dataloader(self):
        return DataLoader(self.train_set, batch_size=4)

    def test_dataloader(self):
        return DataLoader(self.test_set, batch_size=4)


if __name__ == "__main__":
    pass


--- mlflow/server/job_api.py ---
"""
Internal job APIs for UI invocation
"""

import json
from typing import Any

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

from mlflow.entities._job import Job as JobEntity
from mlflow.entities._job_status import JobStatus
from mlflow.exceptions import MlflowException

job_api_router = APIRouter(prefix="/ajax-api/3.0/jobs", tags=["Job"])


class Job(BaseModel):
    """
    Pydantic model for job query response.
    """

    job_id: str
    creation_time: int
    function_fullname: str
    params: dict[str, Any]
    timeout: float | None
    status: JobStatus
    result: Any
    retry_count: int
    last_update_time: int

    @classmethod
    def from_job_entity(cls, job: JobEntity) -> "Job":
        return cls(
            job_id=job.job_id,
            creation_time=job.creation_time,
            function_fullname=job.function_fullname,
            params=json.loads(job.params),
            timeout=job.timeout,
            status=job.status,
            result=job.parsed_result,
            retry_count=job.retry_count,
            last_update_time=job.last_update_time,
        )


@job_api_router.get("/{job_id}", response_model=Job)
def get_job(job_id: str) -> Job:
    from mlflow.server.jobs import get_job

    try:
        job = get_job(job_id)
        return Job.from_job_entity(job)
    except MlflowException as e:
        raise HTTPException(
            status_code=e.get_http_status_code(),
            detail=e.message,
        )


class SubmitJobPayload(BaseModel):
    function_fullname: str
    params: dict[str, Any]
    timeout: float | None = None


@job_api_router.post("/", response_model=Job)
def submit_job(payload: SubmitJobPayload) -> Job:
    from mlflow.server.jobs import submit_job
    from mlflow.server.jobs.utils import _load_function

    function_fullname = payload.function_fullname
    try:
        function = _load_function(function_fullname)
        job = submit_job(function, payload.params, payload.timeout)
        return Job.from_job_entity(job)
    except MlflowException as e:
        raise HTTPException(
            status_code=e.get_http_status_code(),
            detail=e.message,
        )


class SearchJobPayload(BaseModel):
    function_fullname: str | None = None
    params: dict[str, Any] | None = None
    statuses: list[JobStatus] | None = None


class SearchJobsResponse(BaseModel):
    """
    Pydantic model for job searching response.
    """

    jobs: list[Job]


@job_api_router.post("/search", response_model=SearchJobsResponse)
def search_jobs(payload: SearchJobPayload) -> SearchJobsResponse:
    from mlflow.server.handlers import _get_job_store

    try:
        store = _get_job_store()
        job_results = [
            Job.from_job_entity(job)
            for job in store.list_jobs(
                function_fullname=payload.function_fullname,
                statuses=payload.statuses,
                params=payload.params,
            )
        ]
        return SearchJobsResponse(jobs=job_results)
    except MlflowException as e:
        raise HTTPException(
            status_code=e.get_http_status_code(),
            detail=e.message,
        )


--- mlflow/server/otel_api.py ---
"""
OpenTelemetry REST API endpoints for MLflow FastAPI server.

This module implements the OpenTelemetry Protocol (OTLP) REST API for ingesting spans
according to the OTel specification:
https://opentelemetry.io/docs/specs/otlp/#otlphttp

Note: This is a minimal implementation that serves as a placeholder for the OTel endpoint.
The actual span ingestion logic would need to properly convert incoming OTel format spans
to MLflow spans, which requires more complex conversion logic.
"""

from typing import Any

from fastapi import APIRouter, Header, HTTPException, Request, Response, status
from google.protobuf.message import DecodeError
from opentelemetry.proto.collector.trace.v1.trace_service_pb2 import ExportTraceServiceRequest
from pydantic import BaseModel, Field

from mlflow.entities.span import Span
from mlflow.server.handlers import _get_tracking_store
from mlflow.tracing.utils.otlp import MLFLOW_EXPERIMENT_ID_HEADER, OTLP_TRACES_PATH

# Create FastAPI router for OTel endpoints
otel_router = APIRouter(prefix=OTLP_TRACES_PATH, tags=["OpenTelemetry"])


class OTelExportTraceServiceResponse(BaseModel):
    """
    Pydantic model for the OTLP/HTTP ExportTraceServiceResponse.

    This matches the OpenTelemetry protocol specification for trace export responses.
    Reference: https://opentelemetry.io/docs/specs/otlp/
    """

    partialSuccess: dict[str, Any] | None = Field(
        None, description="Details about partial success of the export operation"
    )


@otel_router.post("", response_model=OTelExportTraceServiceResponse, status_code=200)
async def export_traces(
    request: Request,
    response: Response,
    x_mlflow_experiment_id: str = Header(..., alias=MLFLOW_EXPERIMENT_ID_HEADER),
    content_type: str = Header(None),
) -> OTelExportTraceServiceResponse:
    """
    Export trace spans to MLflow via the OpenTelemetry protocol.

    This endpoint accepts OTLP/HTTP protobuf trace export requests.
    Protobuf format reference: https://opentelemetry.io/docs/specs/otlp/#binary-protobuf-encoding

    Args:
        request: OTel ExportTraceServiceRequest in protobuf format
        response: FastAPI Response object for setting headers
        x_mlflow_experiment_id: Required header containing the experiment ID
        content_type: Content-Type header from the request

    Returns:
        OTel ExportTraceServiceResponse indicating success

    Raises:
        HTTPException: If the request is invalid or span logging fails
    """
    # Validate Content-Type header
    if content_type != "application/x-protobuf":
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Invalid Content-Type: {content_type}. Expected: application/x-protobuf",
        )

    # Set response Content-Type header
    response.headers["Content-Type"] = "application/x-protobuf"

    body = await request.body()
    parsed_request = ExportTraceServiceRequest()

    try:
        # In Python protobuf library 5.x, ParseFromString may not raise DecodeError on invalid data
        parsed_request.ParseFromString(body)

        # Check if we actually parsed any data
        # If no resource_spans were parsed, the data was likely invalid
        if not parsed_request.resource_spans:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Invalid OpenTelemetry protobuf format - no spans found",
            )

    except DecodeError:
        # This will catch errors in Python protobuf library 3.x
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid OpenTelemetry protobuf format",
        )

    mlflow_spans = []
    for resource_span in parsed_request.resource_spans:
        for scope_span in resource_span.scope_spans:
            for otel_proto_span in scope_span.spans:
                try:
                    mlflow_span = Span.from_otel_proto(otel_proto_span)
                    mlflow_spans.append(mlflow_span)
                except Exception:
                    raise HTTPException(
                        status_code=422,
                        detail="Cannot convert OpenTelemetry span to MLflow span",
                    )

    if mlflow_spans:
        store = _get_tracking_store()

        try:
            store.log_spans(x_mlflow_experiment_id, mlflow_spans)
        except NotImplementedError:
            raise HTTPException(
                status_code=status.HTTP_501_NOT_IMPLEMENTED,
                detail=f"REST OTLP span logging is not supported by {store.__class__.__name__}",
            )
        except Exception as e:
            raise HTTPException(
                status_code=422,
                detail=f"Cannot store OpenTelemetry spans: {e}",
            )

    return OTelExportTraceServiceResponse()


--- mlflow/server/js/.storybook/decorators/design-system.js ---
import React from 'react';
import { Global } from '@emotion/react';
import { useRef } from 'react';
import { DesignSystemContainer } from '../../src/common/components/DesignSystemContainer';

export const designSystemDecorator = (Story) => {
  const modalContainerRef = useRef(null);

  return (
    <DesignSystemContainer isCompact getPopupContainer={() => modalContainerRef.current}>
      <>
        <Global
          styles={{
            'html, body': {
              fontSize: 13,
              fontFamily:
                '-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji',
              height: '100%',
            },
            '#root': { height: '100%' },
            '*': {
              boxSizing: 'border-box',
            },
          }}
        />
        <Story />
        <div ref={modalContainerRef} />
      </>
    </DesignSystemContainer>
  );
};


--- CHANGELOG.md ---
# CHANGELOG

## 3.5.0 (2025-10-16)

MLflow 3.5.0 includes several major features and improvements!

### Major Features

- ⚙️ **Job Execution Backend**: Introduced a new job execution backend infrastructure for running asynchronous tasks with individual execution pools, job search capabilities, and transient error handling. (#17676, #18012, #18070, #18071, #18112, #18049, @WeichenXu123)
- 🎯 **Flexible Prompt Optimization API**: Introduced a new flexible API for prompt optimization with support for model switching and the GEPA algorithm, enabling more efficient prompt tuning with fewer rollouts. See the [documentation](https://mlflow.org/docs/latest/genai/prompt-registry/optimize-prompts/) to get started. (#18183, #18031, @TomeHirata)
- 🎨 **Enhanced UI Onboarding**: Improved in-product onboarding experience with trace quickstart drawer and updated homepage guidance to help users discover MLflow's latest features. (#18098, #18187, @B-Step62)
- 🔐 **Security Middleware for Tracking Server**: Added a security middleware layer to protect against DNS rebinding, CORS attacks, and other security threats. Read the [documentation](https://mlflow.org/docs/latest/ml/tracking/server/security/) for configuration details. (#17910, @BenWilson2)

### Features

- [Tracing / Tracking] Add `unlink_traces_from_run` batch operation (#18316, @harupy)
- [Tracing] Add batch trace link/unlink operations to DatabricksTracingRestStore (#18295, @harupy)
- [Tracking] Claude Code SDK autologging support (#18022, @smoorjani)
- [Tracing] Add support for reading trace configuration from environment variables (#17792, @joelrobin18)
- [Tracking] Mistral tracing improvements (#16370, @joelrobin18)
- [Tracking] Gemini token count tracking (#16248, @joelrobin18)
- [Tracking] Gemini streaming support (#16249, @joelrobin18)
- [Tracking] CrewAI token count tracking with documentation updates (#16373, @joelrobin18)
- [Evaluation] Allow passing empty scorer list for manual result comparison (#18265, @B-Step62)
- [Evaluation] Log assessments to DSPy evaluation traces (#18136, @B-Step62)
- [Evaluation] Add support for trace inputs to built-in scorers (#17943, @BenWilson2)
- [Evaluation] Add synonym handling for built-in scorers (#17980, @BenWilson2)
- [Evaluation] Add span timing tool for Agent Judges (#17948, @BenWilson2)
- [Evaluation] Allow disabling evaluation sample check (#18032, @B-Step62)
- [Evaluation] Reduce verbosity of SIMBA optimizer logs when aligning judges (#17795, @BenWilson2)
- [Evaluation] Add `__repr__` method for Judges (#17794, @BenWilson2)
- [Prompts] Add prompt registry support to MLflow webhooks (#17640, @harupy)
- [Prompts] Prompt Registry Chat UI (#17334, @joelrobin18)
- [UI] Delete parent and child runs together (#18052, @joelrobin18)
- [UI] Added move to top, move to bottom for charts (#17742, @joelrobin18)
- [Tracking] Use sampling data for run comparison to improve performance (#17645, @lkuo)
- [Tracking] Add optional 'outputs' column for evaluation dataset records (#17735, @WeichenXu123)

### Bug Fixes

- [Tracing] Fix parent run resolution mechanism for LangChain (#17273, @B-Step62)
- [Tracing] Add client-side retry for `get_trace` to improve reliability (#18224, @B-Step62)
- [Tracing] Fix OpenTelemetry dual export (#18163, @B-Step62)
- [Tracing] Suppress false warnings from span logging (#18092, #18276, @B-Step62)
- [Tracing] Fix OpenTelemetry resource attributes not propagating correctly (#18019, @xiaosha007)
- [Tracing] Fix DSPy prompt display (#17988, @B-Step62)
- [Tracing] Fix usage aggregation to avoid ancestor duplication (#17921, @TomeHirata)
- [Tracing] Fix double counting in Strands tracing (#17855, @joelrobin18)
- [Tracing] Fix `to_predict_fn` to handle traces without tags field (#17784, @harupy)
- [Tracing] URL-encode trace tag keys in `delete_trace_tag` to prevent 404 errors (#18232, @copilot-swe-agent)
- [Tracking] Fix Claude Code autologging inputs not displaying (#17858, @smoorjani)
- [Tracking] Fix runs with 0-valued metrics not appearing in experiment list contour plots (#17916, @WeichenXu123)
- [Tracking] Fix DSPy run display (#18137, @B-Step62)
- [Tracking] Allow list of types in tools JSON Schema for OpenAI autolog (#17908, @fedem96)
- [Tracking] Set tracking URI environment variable for job runner (#18073, @WeichenXu123)
- [Evaluation] Add atomicity to `job_start` API (#18226, @BenWilson2)
- [Evaluation] Fix trace ingest for outputs in `merge_records()` API (#18047, @BenWilson2)
- [Evaluation] Fix judge regression (#18039, @B-Step62)
- [Evaluation] Fix judges to use non-empty user messages for Anthropic model compatibility (#17935, @dbczumar)
- [Evaluation] Fix endpoints error in judge (#18048, @joelrobin18)
- [Model Registry] Fix creating model versions from non-Databricks tracking to Databricks Unity Catalog registry (#18244, @austinwarner-8451)
- [Model Registry] Fix registry URI instantiation for artifact download (#17982, @arpitjasa-db)
- [Model Registry] Include original error details in Unity Catalog model copy failure messages (#17997, @harupy)
- [Model Registry] Fix webhook delivery to exit early for FileStore instances (#18015, @copilot-swe-agent)
- [Prompts] Fix error suppression during prompt alias resolution when `allow_missing` is set (#17541, @mr-brobot)
- [UI] General UI improvements (#18281, @joelrobin18)
- [Models] Fix dataset issue (#18081, @joelrobin18)
- [Models] Forward dataset name and digest to PolarsDataset's `to_evaluation_dataset` method (#17886, @sadelcarpio)
- [Build] Fix `mlflow server` exiting immediately when optional `huey` package is missing (#18016, @harupy)
- [Scoring] Fix chat completion arguments (#18248, @aravind-segu)

### Documentation Updates

- [Docs] Add self-hosted documentation support (#17986, @B-Step62)
- [Docs] Add GitHub feature requests section to GenAI documentation (#18342, @TomeHirata)
- [Docs] Update Claude Code SDK tracing documentation (#18026, @smoorjani)
- [Docs] Add documentation for Analyze Experiment MCP/CLI command (#17978, @nsthorat)
- [Docs] Add deprecation notice for custom prompt judge (#18287, @smoorjani)
- [Docs] Overhaul scorer documentation (#17930, @B-Step62)
- [Docs] Add default optimizer documentation (#17814, @BenWilson2)
- [Docs] Update TypeScript SDK contribution documentation (#17995, @joelrobin18)
- [Docs] Fix Postgres 18+ mount path in documentation (#18192, @soyun11)
- [Docs] Fix typo: correct variable name from `max_few_show_examples` to `max_few_shot_examples` (#18246, @srinathmkce)
- [Docs] Replace single quotes with double quotes for Windows compatibility (#18266, @PavithraNelluri)
- [Docs] Fix typo in model registry documentation (#18038, @EddieMG)

Small bug fixes and documentation updates:

#18349, #18338, #18241, #18319, #18309, #18292, #18280, #18239, #18236, #17786, #18003, #17970, #17898, #17765, #17667, @serena-ruan; #18346, #17882, @dbrx-euirim; #18306, #18208, #18165, #18110, #18109, #18108, #18107, #18105, #18104, #18100, #18099, #18155, #18079, #18082, #18078, #18077, #18083, #18030, #18001, #17999, #17712, #17785, #17756, #17729, #17731, #17733, @daniellok-db; #18339, #18291, #18222, #18210, #18124, #18101, #18054, #18053, #18007, #17922, #17823, #17822, #17805, #17789, #17750, #17752, #17760, #17758, #17688, #17689, #17693, #17675, #17673, #17656, #17674, @harupy; #18331, #18308, #18303, #18146, @smoorjani; #18315, #18279, #18310, #18187, #18225, #18277, #18193, #18223, #18209, #18200, #18178, #17574, #18021, #18006, #17944, @B-Step62; #18290, #17946, #17627, @bbqiu; #18274, @Ninja3047; #18204, #17868, #17866, #17833, #17826, #17835, @TomeHirata; #18273, #18043, #17928, #17931, #17936, #17937, @dbczumar; #18185, #18180, #18174, #18170, #18167, #18164, #18168, #18166, #18162, #18160, #18159, #18157, #18156, #18154, #18148, #18145, #18135, #18143, #18142, #18139, #18132, #18130, #18119, #18117, #18115, #18102, #18075, #18046, #18062, #18042, #18051, #18036, #18027, #18014, #18011, #18009, #18004, #17903, #18000, #18002, #17973, #17993, #17989, #17984, #17968, #17966, #17967, #17962, #17977, #17976, #17972, #17965, #17964, #17963, #17969, #17971, #17939, #17926, #17924, #17915, #17911, #17912, #17904, #17902, #17900, #17897, #17892, #17889, #17888, #17885, #17884, #17878, #17874, #17873, #17871, #17870, #17865, #17860, #17861, #17859, #17857, #17856, #17854, #17853, #17851, #17849, #17850, #17847, #17845, #17846, #17844, #17843, #17842, #17838, #17836, #17834, #17831, #17824, #17828, #17819, #17825, #17817, #17821, #17809, #17807, #17808, #17803, #17800, #17799, #17797, #17793, #17790, #17772, #17771, #17769, #17770, #17753, #17762, #17747, #17749, #17745, #17740, #17734, #17732, #17726, #17723, #17722, #17721, #17719, #17720, #17718, #17716, #17713, #17715, #17710, #17709, #17708, #17707, #17705, #17697, #17701, #17698, #17696, #17695, @copilot-swe-agent; #18151, #18153, #17983, #18040, #17981, #17841, #17818, #17776, #17781, @BenWilson2; #18068, @alkispoly-db; #18133, @kevin-lyn; #17105, #17717, @joelrobin18; #17879, @lkuo; #17996, #17945, #17913, @WeichenXu123

## 3.5.0rc0 (2025-10-08)

MLflow 3.5.0rc0 includes several major features and improvements

Major new features:

- 🤖 **Tracing support for Claude Code SDK**: MLflow now provides a tracing integration for both the Claude Code CLI and SDK! Configure the autologging integration to track your prompts, Claude's responses, tool calls, and more. Check out this [doc page](https://mlflow.org/docs/latest/genai/tracing/integrations/listing/claude_code/) to get started. (#18022, @smoorjani)
- ✨ **Improved UI homepage**: The MLflow UI's homepage has been updated to help you get started with more of our latest features. This page will be updated regularly moving forward, allowing you to get more in-product guidance.
- 🗂️ **Evaluation datasets UI integration**: In MLflow 3.4.0, we released backend support for creating evaluation datasets for GenAI applications. In this release, we've added a new tab to the MLflow Experiment UI, allowing you to create, manage, and export traces to your datasets without having to write a line of code.
- 🧮 **GEPA support for prompt optimization**: MLflow's prompt optimization feature now supports the [GEPA algorithm](https://dspy.ai/api/optimizers/GEPA/overview/), allowing you to achieve higher performing prompts with less rollouts. For instructions on how to get started with prompt optimization, visit this [doc page](https://mlflow.org/docs/latest/genai/prompt-registry/optimize-prompts/)!
- 🔐 **Security middleware layer for tracking server**: MLflow now ships with a security middleware layer by default, allowing you to protect against DNS rebinding, CORS attacks, and more. Read the documentation [here](https://mlflow.org/docs/latest/ml/tracking/server/security/) to learn how to configure these options.

Stay tuned for the full release, which will be packed with more features and bugfixes.

To try out this release candidate, please run:

`pip install mlflow==3.5.0rc0`

## 3.4.0rc0 (2025-09-11)

MLflow 3.4.0rc0 includes several major features and improvements

### Major New Features

- 📊 **OpenTelemetry Metrics Export**: MLflow now exports span-level statistics as OpenTelemetry metrics, providing enhanced observability and monitoring capabilities for traced applications. (#17325, @dbczumar)
- 🤖 **MCP Server Integration**: Introducing the Model Context Protocol (MCP) server for MLflow, enabling AI assistants and LLMs to interact with MLflow programmatically. (#17122, @harupy)
- 🧑‍⚖️ **Custom Judges API**: New `make_judge` API enables creation of custom evaluation judges for assessing LLM outputs with domain-specific criteria. (#17647, @BenWilson2, @dbczumar, @alkispoly-db, @smoorjani)
- 📈 **Correlations Backend**: Implemented backend infrastructure for storing and computing correlations between experiment metrics using NPMI (Normalized Pointwise Mutual Information). (#17309, #17368, @BenWilson2)
- 🗂️ **Evaluation Datasets**: MLflow now supports storing and versioning evaluation datasets directly within experiments for reproducible model assessment. (#17447, @BenWilson2)
- 🔗 **Databricks Backend for MLflow Server**: MLflow server can now use Databricks as a backend, enabling seamless integration with Databricks workspaces. (#17411, @nsthorat)
- 🤖 **Claude Autologging**: Automatic tracing support for Claude AI interactions, capturing conversations and model responses. (#17305, @smoorjani)
- 🌊 **Strands Agent Tracing**: Added comprehensive tracing support for Strands agents, including automatic instrumentation for agent workflows and interactions. (#17151, @joelrobin18)
- 🧪 **Experiment Types in UI**: MLflow now introduces experiment types, helping reduce clutter between classic ML/DL and GenAI features. MLflow auto-detects the type, but you can easily adjust it via a selector next to the experiment name. (#17605, @daniellok-db)

Features:

- [Evaluation] Add ability to pass tags via dataframe in mlflow.genai.evaluate (#17549, @smoorjani)
- [Evaluation] Add custom judge model support for Safety and RetrievalRelevance builtin scorers (#17526, @dbrx-euirim)
- [Tracing] Add AI commands as MCP prompts for LLM interaction (#17608, @nsthorat)
- [Tracing] Add MLFLOW_ENABLE_OTLP_EXPORTER environment variable (#17505, @dbczumar)
- [Tracing] Support OTel and MLflow dual export (#17187, @dbczumar)
- [Tracing] Make set_destination use ContextVar for thread safety (#17219, @B-Step62)
- [CLI] Add MLflow commands CLI for exposing prompt commands to LLMs (#17530, @nsthorat)
- [CLI] Add 'mlflow runs link-traces' command (#17444, @nsthorat)
- [CLI] Add 'mlflow runs create' command for programmatic run creation (#17417, @nsthorat)
- [CLI] Add MLflow traces CLI command with comprehensive search and management capabilities (#17302, @nsthorat)
- [CLI] Add --env-file flag to all MLflow CLI commands (#17509, @nsthorat)
- [Tracking] Backend for storing scorers in MLflow experiments (#17090, @WeichenXu123)
- [Model Registry] Allow cross-workspace copying of model versions between WMR and UC (#17458, @arpitjasa-db)
- [Models] Add automatic Git-based model versioning for GenAI applications (#17076, @harupy)
- [Models] Improve WheeledModel.\_download_wheels safety (#17004, @serena-ruan)
- [Projects] Support resume run for Optuna hyperparameter optimization (#17191, @lu-wang-dl)
- [Scoring] Add MLFLOW_DEPLOYMENT_CLIENT_HTTP_REQUEST_TIMEOUT environment variable (#17252, @dbczumar)
- [UI] Add ability to hide/unhide all finished runs in Chart view (#17143, @joelrobin18)
- [Telemetry] Add MLflow OSS telemetry for invoke_custom_judge_model (#17585, @dbrx-euirim)

Bug fixes:

- [Evaluation] Implement DSPy LM interface for default Databricks model serving (#17672, @smoorjani)
- [Evaluation] Fix aggregations incorrectly applied to legacy scorer interface (#17596, @BenWilson2)
- [Evaluation] Add Unity Catalog table source support for mlflow.evaluate (#17546, @BenWilson2)
- [Evaluation] Fix custom prompt judge encoding issues with custom judge models (#17584, @dbrx-euirim)
- [Tracking] Fix OpenAI autolog to properly reconstruct Response objects from streaming events (#17535, @WeichenXu123)
- [Tracking] Add basic authentication support in TypeScript SDK (#17436, @kevin-lyn)
- [Tracking] Update scorer endpoints to v3.0 API specification (#17409, @WeichenXu123)
- [Tracking] Fix scorer status handling in MLflow tracking backend (#17379, @WeichenXu123)
- [Tracking] Fix missing source-run information in UI (#16682, @WeichenXu123)
- [Scoring] Fix spark_udf to always use stdin_serve for model serving (#17580, @WeichenXu123)
- [Scoring] Fix a bug with Spark UDF usage of uv as an environment manager (#17489, @WeichenXu123)
- [Model Registry] Extract source workspace ID from run_link during model version migration (#17600, @arpitjasa-db)
- [Models] Improve security by reducing write permissions in temporary directory creation (#17544, @BenWilson2)
- [Server-infra] Fix --env-file flag compatibility with --dev mode (#17615, @nsthorat)
- [Server-infra] Fix basic authentication with Uvicorn server (#17523, @kevin-lyn)
- [UI] Fix experiment comparison functionality in UI (#17550, @Flametaa)
- [UI] Fix compareExperimentsSearch route definitions (#17459, @WeichenXu123)

Documentation updates:

- [Docs] Add clarification for trace requirements in scorers documentation (#17542, @BenWilson2)
- [Docs] Add documentation for Claude code autotracing (#17521, @smoorjani)
- [Docs] Remove experimental status message for MPU/MPD features (#17486, @BenWilson2)
- [Docs] Remove problematic pages from documentation (#17453, @BenWilson2)
- [Docs] Add documentation for updating signatures on Databricks registered models (#17450, @arpitjasa-db)
- [Docs] Update Scorers API documentation (#17298, @WeichenXu123)
- [Docs] Add comprehensive documentation for scorers (#17258, @B-Step62)

Small bug fixes and documentation updates:

#17655, #17657, #17597, #17545, #17547, @BenWilson2; #17671, @smoorjani; #17668, #17665, #17662, #17661, #17659, #17658, #17653, #17643, #17642, #17636, #17634, #17631, #17628, #17611, #17607, #17588, #17570, #17575, #17564, #17557, #17556, #17555, #17536, #17531, #17524, #17510, #17511, #17499, #17500, #17494, #17493, #17490, #17488, #17478, #17479, #17425, #17471, #17457, #17440, #17403, #17405, #17404, #17402, #17366, #17346, #17344, #17337, #17316, #17313, #17284, #17276, #17235, #17226, #17229, @copilot-swe-agent; #17664, #17654, #17613, #17637, #17633, #17612, #17630, #17616, #17626, #17617, #17610, #17614, #17602, #17538, #17522, #17512, #17508, #17492, #17462, #17475, #17468, #17455, #17338, #17257, #17231, #17214, #17223, #17218, #17216, @harupy; #17635, #17663, #17426, #16870, #17428, #17427, #17441, #17377, @serena-ruan; #17605, #17306, @daniellok-db; #17624, #17578, #17369, #17391, #17072, #17326, #17115, @dbczumar; #17598, #17408, #17353, @nsthorat; #17601, #17553, @dbrx-euirim; #17586, #17587, #17310, #17180, @TomeHirata; #17516, @bbqiu; #17477, #17474, @WeichenXu123; #17449, @raymondzhou-db; #17470, @jacob-danner; #17378, @arpitjasa-db; #17121, @ctaymor; #17351, #17322, @ispoljari; #17292, @dsuhinin; #17287, #17281, #17230, #17245, #17237, @B-Step62

## 3.3.2 (2025-08-27)

MLflow 3.3.2 is a patch release that includes several minor improvements and bugfixes

Features:

- [Evaluation] Add support for dataset name persistence (#17250, @BenWilson2)

Bug fixes:

- [Tracing] Add retry policy support to `_invoke_litellm` for improved reliability (#17394, @dbczumar)
- [UI] fix ui sorting in experiments (#17340, @Flametaa)
- [Serving] Add Databricks Lakebase Resource (#17277, @jennsun)
- [Tracing] Fix set trace tags endpoint (#17362, @daniellok-db)

Documentation updates:

- [Docs] Add docs for package lock (#17395, @BenWilson2)
- [Docs] Fix span processor docs (#17386, @mr-brobot)

Small bug fixes and documentation updates:

#17301, #17299, @B-Step62; #17420, #17421, #17398, #17397, #17349, #17361, #17377, #17359, #17358, #17356, #17261, #17263, #17262, @serena-ruan; #17422, #17310, #17357, @TomeHirata; #17406, @sotagg; #17418, @annzhang-db; #17384, #17376, @daniellok-db

## 3.3.1 (2025-08-20)

MLflow 3.3.1 includes several major features and improvements

Bug fixes:

- [Tracking] Fix `mlflow.genai.datasets` attribute (#17307, @WeichenXu123)
- [UI] Fix tag display as column in experiment overview (#17296, @joelrobin18)
- [Tracing] Fix the slowness of dspy tracing (#17290, @TomeHirata)

Small bug fixes and documentation updates:

#17295, @gunsodo; #17272, @bbqiu

## 3.3.0 (2025-08-19)

MLflow 3.3.0 includes several major features and improvements

### Major new features:

- 🪝 **Model Registry Webhooks**: MLflow now supports [webhooks](https://mlflow.org/docs/latest/ml/webhooks/) for model registry events, enabling automated notifications and integrations with external systems. (#16583, @harupy)
- 🧭 **Agno Tracing Integration**: Added [Agno tracing integration](https://mlflow.org/docs/latest/genai/tracing/integrations/listing/agno/) for enhanced observability of AI agent workflows. (#16995, @joelrobin18)
- 🧪 **GenAI Evaluation in OSS**: MLflow open-sources [the new evaluation capability for LLM applications](https://mlflow.org/docs/latest/genai/eval-monitor/). This suite enables systematic measurement and improvement of LLM application quality, with tight integration into MLflow's observability, feedback collection, and experiment tracking capabilities. (#17161, #17159, @B-Step62)
- 🖥️ **Revamped Trace Table View**: The new trace view in MLflow UI provides a streamlined interface for exploring, filtering, and monitoring traces, with enhanced search capabilities including full-text search across requests.(#17092, @daniellok-db)
- ⚡️ **FastAPI + Uvicorn Server**: MLflow Tracking Server now defaults to FastAPI + Uvicorn for improved performance, while maintaining Flask compatibility. (#17038, @dbczumar)

New features:

- [Tracking] Add a Docker compose file to quickly start a local MLflow server with recommended minimum setup (#17065, @joelrobin18)
- [Tracing] Add `memory` span type for agentic workflows (#17034, @B-Step62)
- [Prompts] Enable custom prompt optimizers in `optimize_prompt` including DSPy support (#17052, @TomeHirata)
- [Model Registry / Prompts] Proper support for the @latest alias (#17146, @B-Step62)
- [Metrics] Allow custom tokenizer encoding in `token_count` function (#16253, @joelrobin18)

Bug fixes:

- [Tracking] Fix Databricks secret scope check to reduce audit log errors (#17166, @harupy)
- [Tracking] Fix Databricks SDK error code mapping in retry logic (#17095, @harupy)
- [Tracking] Fix Databricks secret scope check to reduce error rates (#17166, @harupy)
- [Tracing] Remove API keys from CrewAI traces to prevent credential leakage (#17082, @diy2learn)
- [Tracing] Fix LiteLLM span association issue by making callbacks synchronous (#16982, @B-Step62)
- [Tracing] Fix OpenAI Agents tracing (#17227, @B-Step62)
- [Evaluation] Fix issue with get_label_schema has no attribute (#17163, @smoorjani)
- [Docs] Fix version selector on API Reference page by adding missing CSS class and versions.json generation (#17247, @copilot-swe-agent)

Documentation updates:

- [Docs] Document custom optimizer usage with `optimize_prompt` (#17084, @TomeHirata)
- [Docs] Fix built-in scorer documentation for expectation parameter (#17075, @smoorjani)
- [Docs] Add comprehensive documentation for scorers (#17258, @B-Step62)

Small bug fixes and documentation updates:

#17230, #17264, #17289, #17287, #17265, #17238, #17215, #17224, #17185, #17148, #17193, #17157, #17067, #17033, #17087, #16973, #16875, #16956, #16959, @B-Step62; #17269, @BenWilson2; #17285, #17259, #17260, #17236, #17196, #17169, #17062, #16943, @serena-ruan; #17253, @sotagg; #17212, #17206, #17211, #17207, #17205, #17118, #17177, #17182, #17170, #17153, #17168, #17123, #17136, #17119, #17125, #17088, #17101, #17056, #17077, #17057, #17036, #17018, #17024, #17019, #16883, #16972, #16961, #16968, #16962, #16958, @harupy; #17209, #17202, #17184, #17179, #17174, #17141, #17155, #17145, #17130, #17113, #17110, #17098, #17104, #17100, #17060, #17044, #17032, #17008, #17001, #16994, #16991, #16984, #16976, @copilot-swe-agent; #17069, @hayescode; #17199, #17081, #16928, #16931, @TomeHirata; #17198, @WeichenXu123; #17195, #17192, #17131, #17128, #17124, #17120, #17102, #17093, #16941, @daniellok-db; #17070, #17074, #17073, @dbczumar

## 3.2.0 (2025-08-05)

MLflow 3.2.0 includes several major features and improvements

### Major New Features

- 🧭 **Tracing TypeScript SDK**: MLflow Tracing now supports the [TypeScript SDK](https://github.com/mlflow/mlflow/tree/master/libs/typescript), allowing developers to trace GenAI applications in TypeScript environments. (#16871, @B-Step62)
- 🔗 **Semantic Kernel Tracing**: MLflow now provides [automatic tracing support for Semantic Kernel](https://mlflow.org/docs/latest/genai/tracing/integrations/listing/semantic_kernel/), simplifying trace capture for SK-based workflows. (#16469, @michael-berk)
- 🧪 **Feedback Tracking**: MLflow OSS now natively supports tracking [human feedbacks](https://mlflow.org/docs/latest/genai/assessments/feedback/), [ground truths](https://mlflow.org/docs/latest/genai/assessments/expectations/), LLM judges on traces, providing integrated quality monitoring and feedback management capabilities. (#16743, @BenWilson2)
- 🖥️ **MLflow UI Improvements**: The MLflow UI now features **a redesigned experiment home view** and includes enhancements like pagination on the model page for better usability. (#16464, @frontsideair, #15801, @Flametaa)
- 🔍 **Updated Trace UI**: The Trace UI now has image support when rendering chat messages for OpenAI, Langchain, and Anthropic! Additionally, we're introducing a "summary view" which is a simplified, flat representation of the important spans in a trace. The full detail view is still available in a separate tab.
- 🛡️ **PII Masking in Tracing**: Added support for [masking personally identifiable information (PII) via a custom span post-processor](https://mlflow.org/docs/latest/genai/tracing/observe-with-traces/masking). (#16344, @B-Step62)
- 🐻‍❄️ **Polars Dataset Support**: MLflow now supports [Polars datasets](https://mlflow.org/docs/latest/ml/dataset/#dataset), expanding compatibility with performant DataFrame libraries. (#13006, @AlpAribal)

### 📊 Usage Tracking (New in 3.2.0)

- Starting with version 3.2.0, MLflow will begin collecting anonymized usage data about how core features of the platform are used. This data contains **no sensitive or personally identifiable information**, and users can opt out of data collection at any time. Check [MLflow documentation](https://mlflow.org/docs/latest/community/usage-tracking/) for more details. (#16439, @serena-ruan)

Features:

- [Tracing] Include mlflow-tracing as a dependency of mlflow (#16589, @B-Step62)
- [Tracing] Convert DatabricksRM output to MLflow document format (#16866, @WeichenXu123)
- [Tracing] Add unified token usage tracking for Bedrock LLMs (#16351, @mohammadsubhani)
- [Tracing] Token usage tracking for agent frameworks including Anthropic, Autogen, LlamaIndex etc. (#16251, #16362, #16246, #16258, #16313, #16312, #16340, #16357, #16358, @joelrobin18, #16387, @sanatb187)
- [Tracing] Render multi-modal trace for LangChain (#16799, @B-Step62)
- [Tracing] Support async tracing for Gemini (#16632, @B-Step62)
- [Tracing] Support global sampling for tracing (#16700, @B-Step62)
- [Tracing] ResponsesAgent tracing aggregation (#16787, @bbqiu)
- [Tracing] Add Agent and LLM complete name (#16613, @joelrobin18)
- [Tracking] Allow setting thread-local tracing destination via mlflow.tracing.set_destination (#16859, @WeichenXu123)
- [Tracking] Introduce MLFLOW_DISABLE_SCHEMA_DETAILS environment variable to toggle detailed schema errors (#16631, @NJAHNAVI2907)
- [Tracking] Add support for chat-style prompts with structured output with prompt object (#16341, @harshilprajapati96)
- [Tracking] Add support for responses.parse calls in oai autologger (#16245, @dipakkrishnan)
- [Tracking] Add support for uv as an environment manager in mlflow run (#16274, @isuyyy)
- [Evaluation] Replace guideline_adherence to guidelines (#16856, @smoorjani)
- [Evaluation] Replace Scheduled Scorers API to a Scorer Registration System (#16977, @dbrx-euirim)
- [UI] Add tag filter to the experiments page (#16648, @frontsideair)
- [UI] Add ability to the UI to edit experiment tags (#16614, @frontsideair)
- [UI] Create runs table using selected columns in the experiment view (#16804, @wangh118)
- [Scoring] Make spark_udf support 'uv' env manager (#16292, @WeichenXu123)

Bug fixes:

- [Tracking / UI] Add missing default headers and replace absolute URLs in new browser client requests (GraphQL & logged models) (#16840, @danilopeixoto)
- [Tracking] Fix tracking_uri positional argument bug in artifact repositories (#16878, @copilot-swe-agent)
- [Models] Fix UnionType support for Python 3.10 style union syntax (#16882, @harupy)
- [Tracing / Tracking] Fix OpenAI autolog Pydantic validation for enum values (#16862, @mohammadsubhani)
- [Tracking] Fix tracing for Anthropic and Langchain combination (#15151, @maver1ck)
- [Models] Fix OpenAI multimodal message logging support (#16795, @mohammadsubhani)
- [Tracing] Avoid using nested threading for Azure Databricks trace export (#16733, @TomeHirata)
- [Evaluation] Bug fix: Databricks GenAI evaluation dataset source returns string, instead of DatasetSource instance (#16712, @dbczumar)
- [Models] Fix `get_model_info` to provide logged model info (#16713, @harupy)
- [Evaluation] Fix serialization and deserialization for python scorers (#16688, @connorchenn)
- [UI] Fix GraphQL handler erroring on NaN metric values (#16628, @daniellok-db)
- [UI] Add back video artifact preview (#16620, @daniellok-db)
- [Tracing] Proper chat message reconstruction from OAI streaming response (#16519, @B-Step62)
- [Tracing] Convert trace column in search_traces() response to JSON string (#16523, @B-Step62)
- [Evaluation] Fix mlflow.evaluate crashes in \_get_binary_classifier_metrics due to … (#16485, @mohammadsubhani)
- [Evaluation] Fix trace detection logic for `mlflow.genai.evaluate` (#16932, @B-Step62)
- [Evaluation] Enable to use make_genai_metric_from_prompt for mlflow.evaluate (#16960, @TomeHirata)
- [Models] Add explicit encoding for decoding streaming Responses (#16855, @aravind-segu)
- [Tracking] Prevent from tracing DSPy model API keys (#17021, @czyzby)
- [Tracking] Fix pytorch datetime issue (#17030, @serena-ruan)
- [Tracking] Fix predict with pre-releases (#16998, @serena-ruan)

Documentation updates:

- [Docs] Overhaul of top level version management GenAI docs (#16728, @BenWilson2)
- [Docs] Fix Additional GenAI Docs pages (#16691, @BenWilson2)
- [Docs] Update the docs selector dropdown (#16280, @BenWilson2)
- [Docs] Update docs font sizing and link coloring (#16281, @BenWilson2)
- [Docs] Fix typo in model deployment page (#16999, @premkiran-o7)

Small bug fixes and documentation updates:

#17003, #17049, #17035, #17026, #16981, #16971, #16953, #16930, #16917, #16738, #16717, #16693, #16694, #16684, #16678, #16656, #16513, #16459, #16277, #16276, #16275, #16170, #16217, @serena-ruan; #16927, #16915, #16913, #16911, #16909, #16889, #16727, #16600, #16543, #16551, #16526, #16533, #16535, #16531, #16472, #16392, #16389, #16385, #16376, #16369, #16367, #16321, #16311, #16307, #16273, #16268, #16265, #16112, #16243, #16231, #16226, #16221, #16196, @copilot-swe-agent; #17050, #17048, #16955, #16894, #16885, #16860, #16841, #16835, #16801, #16701, @daniellok-db; #16898, #16881, #16858, #16735, #16823, #16814, #16647, #16750, #16809, #16794, #16793, #16789, #16780, #16770, #16773, #16771, #16772, #16768, #16752, #16754, #16751, #16748, #16730, #16729, #16346, #16709, #16704, #16703, #16702, #16658, #16662, #16645, #16639, #16640, #16626, #16572, #16566, #16565, #16563, #16561, #16559, #16544, #16539, #16520, #16508, #16505, #16494, #16495, #16491, #16487, #16482, #16473, #16465, #16456, #16458, #16394, #16445, #16433, #16434, #16413, #16417, #16416, #16414, #16415, #16378, #16350, #16323, #15788, #16263, #16256, #16237, #16234, #16219, #16216, #16207, #16199, #16192, #16705, @harupy; #17047, #17017, #17005, #16989, #16952, #16951, #16903, #16900, #16755, #16762, #16757, #15860, #16661, #16630, #16657, #16605, #16602, #16568, #16569, #16553, #16345, #16454, #16489, #16486, #16438, #16266, #16382, #16381, #16303, @B-Step62; #17028, #17027, #17020, @he7d3r; #16969, #16957, #16852, #16829, #16816, #16808, #16775, #16807, #16806, #16624, #16524, #16410, #16403, @TomeHirata; #16987, @wangh118; #16760, #16761, #16736, #16737, #16699, #16718, #16663, #16676, #16574, #16477, #16552, #16527, #16515, #16452, #16210, #16204, #16610, @frontsideair; #16723, #16124, @AveshCSingh; #16744, @BenWilson2; #16683, @dsuhinin; #16877, #16502, @bbqiu; #16619, @AchimGaedkeLynker; #16595, @Aiden-Jeon; #16480, #16479, @shushantrishav; #16398, #16331, #16328, #16329, #16293, @WeichenXu123

## 3.1.4 (2025-07-23)

MLflow 3.1.4 includes several major features and improvements

Small bug fixes and documentation updates:

#16835, #16820, @daniellok-db

## 3.1.3 (2025-07-22)

MLflow 3.1.3 includes several major features and improvements

Features:

- [Artifacts / Tracking] Do not copy file permissions when logging artifacts to local artifact repo (#16642, @connortann)
- [Tracking] Add support for OpenAI ChatCompletions parse method (#16493, @harupy)

Bug fixes:

- [Deployments] Propagate `MLFLOW_DEPLOYMENT_PREDICT_TIMEOUT` to databricks-sdk (#16783, @bbqiu)
- [Model Registry] Fix issue with search_registered_models with Databricks UC backend not supporting filter_string (#16766, @BenWilson2)
- [Evaluation] Bug fix: Databricks GenAI evaluation dataset source returns string, instead of DatasetSource instance (#16712, @dbczumar)
- [Tracking] Fix the position of added tracking_uri param to artifact store implementations (#16653, @BenWilson2)

Small bug fixes and documentation updates:

#16786, #16692, @daniellok-db; #16594, @ngoduykhanh; #16475, @harupy

## 3.1.2 (2025-07-08)

MLflow 3.1.2 is a patch release that includes several bug fixes.

Bug fixes:

- [Tracking] Fix `download_artifacts` ignoring `tracking_uri` parameter (#16461, @harupy)
- [Models] Fix event type for ResponsesAgent error (#16427, @bbqiu)
- [Models] Remove falsey chat conversion for LangGraph models (#16601, @B-Step62)
- [Tracing] Use empty Resource when instantiating OTel provider to fix LiteLLM tracing issue (#16590, @B-Step62)

Small fixes and documentation updates:

#16568, #16454, #16617, #16605, #16569, #16553, #16625, @B-Step62; #16571, #16552, #16452, #16395, #16446, #16420, #16447, #16554, #16515, @frontsideair; #16558, #16443, #16457, @16442, #16449, @harupy; #16509, #16512, #16524, #16514, #16607, @TomeHirata; #16541, @copilot-swe-agent; #16427, @bbqiu; #16573, @daniellok-db; #16470, #16281, @BenWilson2

## 3.1.1 (2025-06-25)

MLflow 3.1.1 includes several major features and improvements

Features:

- [Model Registry / Sqlalchemy] Increase prompt text limit from 5K to 100K (#16377, @harupy)
- [Tracking] Support pagination in get-history of FileStore and SqlAlchemyStore (#16325, @TomeHirata)

Bug fixes:

- [Artifacts] Support downloading logged model artifacts (#16356, @TomeHirata)
- [Models] Fix bedrock provider, configured inference profile compatibility (#15604, @lloydhamilton)
- [Tracking] Specify attribute.run_id when search_traces filters by run_id (#16295, @artjen)
- [Tracking] Fix graphql batching attacks (#16227, @serena-ruan)
- [Model Registry] Make the chunk size configurable in DatabricksSDKModelsArtifactRepository (#16247, @TomeHirata)

Documentation updates:

- [Docs] Move the Lighthouse main signup page to GenAI (#16404, @BenWilson2)
- [Docs] [DOC-FIX] Dspy doc fix (#16397, @joelrobin18)
- [Docs] Fix(docs): Resolve self-referencing 'Next' link on GenAI Tracing overview page (#16334, @mohammadsubhani)
- [Docs] Update the docs selector dropdown (#16280, @BenWilson2)
- [Docs] Update utm_source for source tracking to signup URL (#16316, @BenWilson2)
- [Docs] Fix footer rendering in docs for light mode display (#16214, @BenWilson2)

Small bug fixes and documentation updates:

#16261, @rohitarun-db; #16411, #16352, #16327, #16324, #16279, #16193, #16197, @harupy; #16409, #16348, #16347, #16290, #16286, #16283, #16271, #16223, @TomeHirata; #16326, @mohammadsubhani; #16364, @BenWilson2; #16308, #16218, @serena-ruan; #16262, @raymondzhou-db; #16191, @copilot-swe-agent; #16212, @B-Step62; #16208, @frontsideair; #16205, #16200, #16198, @daniellok-db

## 3.0.1 (2025-06-25)

MLflow 3.0.1 includes several major features and improvements

Features:

- [Model Registry / Sqlalchemy] Increase prompt text limit from 5K to 100K (#16377, @harupy)

Bug fixes:

- [Models] Fix bedrock provider, configured inference profile compatibility (#15604, @lloydhamilton)

Small bug fixes and documentation updates:

#16364, @BenWilson2; #16347, @TomeHirata; #16279, #15835, @harupy; #16182, @B-Step62

## 3.1 (2025-06-11)

MLflow 3 includes several major features and improvements

Features:

- [Tracking] MLflow 3.0 (#13211, @harupy)
- [Prompts] Add Custom Prompt Judges to `mlflow[databricks]` (#16097, @dbrx-euirim)
- [Artifacts / Model Registry / Tracking] Package model environment when registering model (#15783, @qyc)
- [Tracking] Add `MlflowSparkStudy` (#15418, @lu-wang-dl)
- [Scoring] Make `spark_udf` support DBConnect + DBR 15.4 / DBR dedicated cluster (#15968, @WeichenXu123)
- [Tracking] Lock model dependencies when logging a model using `uv` (#15875, @harupy)
- [Model Registry] Introduce `mlflow.genai.optimize_prompt` to optimize prompts (#15861, @TomeHirata)
- [Tracing] Support custom request/response preview (#15919, @B-Step62)
- [Tracking] Add integration for AutoGen > 0.4 (#14729, @TomeHirata)
- [Tracking] Support token tracking for OpenAI (#15870, @B-Step62)
- [Tracking] Support tracing `ResponsesAgent.predict_stream` (#15762, @bbqiu)
- [Tracking] Introduce client and fluent APIs for `LogLoggedModelParams` (#15717, @artjen)
- [Models] Support `predict_stream` in DSPy flavor (#15678, @TomeHirata)
- [Tracking] Record notebook and git metadata in trace metadata (#15650, @B-Step62)
- [Model Registry] Added `search_prompts` function to list all the prompts registered (#15445, @joelrobin18)
- [Models] Support compression for pyfunc log model (#14700, @antbbn)
- [Gateway] Add support for Gemini in AI Gateway (#15069, @joelrobin18)
- [Tracing] PydanticAI Autologging (#15553, @joelrobin18)
- [Tracking] Support setting databricks auth profile by `DATABRICKS_CONFIG_PROFILE` environment variable. (#15587, @WeichenXu123)
- [Tracking] create mlflow tracing for `smolagents` (#15574, @y-okt)
- [Artifacts / UI] Support for video artifacts (#15518, @joelrobin18)
- [Model Registry] Add `allow_missing` parameter in `load_prompt` (#15371, @joelrobin18)
- [Tracking] Emit a warning for `mlflow.get_artifact_uri()` usage outside active run (#12902, @Shashank1202)

Bug fixes:

- [GenAI] Add Databricks App resource (#15867, @aravind-segu)
- [Tracking] Support json-string for inputs/expectations column in Spark Dataframe (#16011, @B-Step62)
- [Tracking] Avoid generating traces from scorers during evaluation (#16004, @B-Step62)
- [GenAI] Allow multi inputs module in DSPy (#15859, @TomeHirata)
- [Tracking] Improve error handling if tracking URI is not set when running `mlflow gc` (#11773, @oleg-z)
- [Tracking] Trace search: Avoid spawning threads for span fetching if `include_spans=False` (#15634, @dbczumar)
- [Tracking] Fix `global_guideline_adherence` (#15572, @artjen)
- [Model Registry] Log `Resources` from `SystemAuthPolicy` in `CreateModelVersion` (#15485, @aravind-segu)
- [Models] `ResponsesAgent` interface update (#15601, #15741, @bbqiu)

Breaking changes:

- [Tracking] Move prompt registry APIs under `mlflow.genai.prompts` namespace (#16174, @B-Step62)
- [Model Registry] Default URI to databricks-uc when tracking URI is databricks & registry URI is unspecified (#16135, @dbczumar)
- [Tracking] Do not log SHAP explainer in `mlflow.evaluate` (#15827, @harupy)
- [Tracking] Update DataFrame schema returned from `mlflow.search_trace()` to be V3 format (#15643, @B-Step62)

Documentation updates:

- [Docs] Documentation revamp for MLflow 3.0 (#15954, @harupy)
- [Docs] Add Prompt Optimization Document Page (#15958, @TomeHirata)
- [Docs] Redesign API reference page (#15811, @besirovic)
- [Docs] MLflow 3 breaking changes list (#15716, @WeichenXu123)
- [Docs] Update Lighthouse signup and signin links (#15740, @BenWilson2)
- [Docs] Document models:/ URIs explicitly in OSS MLflow docs (#15727, @WeichenXu123)
- [Docs] Spark UDF Doc update (#15586, @WeichenXu123)

Small bug fixes and documentation updates:

#16193, #16192, #16171, #16119, #16036, #16130, #16081, #16101, #16047, #16086, #16077, #16045, #16065, #16067, #16063, #16061, #16058, #16050, #16043, #16034, #16033, #15966, #16025, #16015, #16002, #15970, #16001, #15999, #15942, #15960, #15955, #15951, #15939, #15885, #15883, #15890, #15887, #15874, #15869, #15846, #15845, #15826, #15834, #15822, #15830, #15796, #15821, #15818, #15817, #15805, #15804, #15798, #15793, #15797, #15782, #15775, #15772, #15790, #15773, #15776, #15756, #15767, #15766, #15765, #15746, #15747, #15748, #15751, #15743, #15731, #15720, #15722, #15670, #15614, #15715, #15677, #15708, #15673, #15680, #15686, #15671, #15657, #15669, #15664, #15675, #15667, #15666, #15668, #15651, #15649, #15647, #15640, #15638, #15630, #15627, #15624, #15622, #15558, #15610, #15577, #15575, #15545, #15576, #15559, #15563, #15555, #15557, #15548, #15551, #15547, #15542, #15536, #15524, #15531, #15525, #15520, #15521, #15502, #15499, #15442, #15426, #15315, #15392, #15397, #15399, #15394, #15358, #15352, #15349, #15328, #15336, #15335, @harupy; #16196, #16191, #16093, #16114, #16080, #16088, #16053, #15856, #16039, #15987, #16009, #16014, #16007, #15996, #15993, #15991, #15989, #15978, #15839, #15953, #15934, #15929, #15926, #15909, #15900, #15893, #15889, #15881, #15879, #15877, #15865, #15863, #15854, #15852, #15848, @copilot-swe-agent; #16178, #16153, #16155, #15823, #15754, #15794, #15800, #15799, #15615, #15777, #15726, #15752, #15745, #15753, #15738, #15681, #15684, #15682, #15702, #15679, #15623, #15645, #15612, #15533, #15607, #15522, @serena-ruan; #16177, #16167, #16168, #16166, #16152, #16144, #15920, #16134, #16128, #16098, #16059, #16024, #15974, #15917, #15676, #15750, @dbczumar; #16162, #16161, #16137, #16126, #16127, #16099, #16074, #16041, #16040, #16010, #15945, #15697, #15588, #15602, #15581, @rohitarun-db; #16150, #15984, #16125, #16102, #16062, #16060, #15986, #15985, #15983, #15982, #15980, #15763, @smoorjani; #16160, #16149, #16103, #15538, #16055, #16054, #16048, #16012, #16029, #16003, #15940, #15956, #15950, #15906, #15922, #15932, #15930, #15905, #15910, #15902, #15901, #15840, #15896, #15898, #15895, #15850, #15833, #15824, #15819, #15816, #15806, #15803, #15795, #15759, #15791, #15792, #15774, #15769, #15768, #15770, #15755, #15771, #15737, #15690, #15733, #15730, #15687, #15660, #15735, #15688, #15705, #15590, #15663, #15665, #15658, #15594, #15620, #15644, #15648, #15605, #15639, #15642, #15619, #15618, #15611, #15597, #15589, #15580, #15593, #15437, #15584, #15582, #15448, #15351, #15317, #15353, #15320, #15319, @B-Step62; #16151, #16142, #16111, #16106, #16051, #16046, #16044, #15971, #15957, #15810, #15749, #15706, #15683, #15728, #15732, #15707, #15621, #15567, #15566, #15523, #15479, #15404, #15400, #15378, @TomeHirata; #16026, #16072, @AveshCSingh; #15967, @euirim; #15884, #15924, #15395, #15393, #15390, @daniellok-db; #15786, @rahuja23; #15734, @lhrotk; #15809, #15739, #15695, #15654, #15694, #15655, #15653, #15608, #15543, #15573, @dhruyads; #15596, @mrharishkumar; #15742, #15723, #15633, #15606, @ShaylanDias; #15703, #15637, #15613, #15473, @joelrobin18; #15636, #15659, #15616, #15617, @raymondzhou-db; #15674, #15598, #15357, #15586, @WeichenXu123; #15691, @artjen; #15698, @prithvikannan; #15631, @hubertzub-db; #15569, @Anand1923; #15578, @y-okt; #14790, @singh-kristian; #14129, @jamblejoe; #15552, @BenWilson2; #14197, @clarachristiansen; #15505, @Conor0Callaghan; #15509, @tr33k; #15507, @vzamboulingame; #15459, @UnMelow; #13991, @abhishekpawar1060; #12161, @zhouyou9505; #15293, @tornikeo

## 2.22.1 (2025-06-06)

MLflow 2.22.1 includes several major features and improvements

Features:

- [Scoring] For DBConnect client, make spark_udf support DBR 15.4 and DBR dedicated cluster (#15938, @WeichenXu123)

Bug fixes:

- [Model Registry] Log Resources from SystemAuthPolicy in CreateModelVersion (#15485, @aravind-segu)
- [Tracking] Trace search: Avoid spawning threads for span fetching if include_spans=False (#15635, @dbczumar)

Documentation updates:

- [Docs] Spark UDF Doc update (#15586, @WeichenXu123)

Small bug fixes and documentation updates:

#15523, #15728, @TomeHirata; #13997, #16025, #15647, #16030, @harupy; #15786, @rahuja23; #15703, @joelrobin18; #15612, @serena-ruan; #16031, @daniellok-db; #15841, @frontsideair; #15807, @B-Step62

## 2.22.0 (2025-04-24)

MLflow 2.22.0 brings important bug fixes and improves the UI and tracking capabilities.

Features:

- [Tracking] Supported tracing for OpenAI Responses API.  
  (#15240, @B-Step62)
- [Tracking] Introduced `get_last_active_trace`, which affects model serving/monitoring logic.  
  (#15233, @B-Step62)
- [Tracking] Introduced async export for Databricks traces (default behavior).  
  (#15163, @B-Step62)
- [AI Gateway] Added Gemini embeddings support with corresponding unit tests.  
  (#15017, @joelrobin18)
- [Tracking / SQLAlchemy] MySQL SSL connections are now supported with client certs.  
  (#14839, @aksylumoed)
- [Models] Added Optuna storage utility for enabling parallel hyperparameter tuning.  
  (#15243, @XiaohanZhangCMU)
- [Artifacts] Added support for Azure Data Lake Storage (ADLS) artifact repositories.  
  (#14723, @serena-ruan)
- [UI] Artifact views for text now auto-refresh in the UI.  
  (#14939, @joelrobin18)

Bug Fixes:

- [Tracking / UI] Fixed serialization for structured output in `langchain_tracer` + added unit tests.  
  (#14971, @joelrobin18)
- [Server-infra] Enforced password validation for authentication (min. 8 characters).  
  (#15287, @WeichenXu123)
- [Deployments] Resolved an issue with the OpenAI Gateway adapter.  
  (#15286, @WeichenXu123)
- [Artifacts / Tracking / Server-infra] Normalized paths by stripping trailing slashes.  
  (#15016, @tarek7669)
- [Tags] Fixed bug where tag values containing `": "` were being truncated.  
  (#14896, @harupy)

Small bug fixes and documentation updates:

#15396, #15379, #15292, #15305, #15078, #15251, #15267, #15208, #15104, #15045, #15084, #15055, #15056, #15048, #14946, #14956, #14903, #14854, #14830, @serena-ruan; #15417, #15256, #15186, #15007, @TomeHirata; #15119, @bbqiu; #15413, #15314, #15311, #15303, #15301, #15288, #15275, #15269, #15272, #15268, #15262, #15266, #15264, #15261, #15252, #15249, #15244, #15236, #15235, #15237, #15140, #14982, #14898, #14893, #14861, #14870, #14853, #14849, #14813, #14822, @harupy; #15333, #15298, #15300, #15156, #15019, #14957, @B-Step62; #15313, #15297, #14880, @daniellok-db; #15066, #15074, #14913, @joelrobin18; #15232, @kbolashev; #15242, @dbczumar; #15210, #15178, @WeichenXu123; #15187, #15177, @hubertzub-db; #15059, #15070, #15050, #15012, #14959, #14918, #15005, #14965, #14858, #14930, #14927, #14786, #14883, #14863, #14852, #14788, @Gumichocopengin8; #14920, #14919, @jaceklaskowski

## 2.21.3 (2025-04-03)

MLflow 2.21.3 includes a few bugi

Bug fixes:

- [Tracking] Fix spark ML save model error in Databricks shared or serverless cluster (#15198, @WeichenXu123)
- [Tracking] Fix Spark model logging / loading in Databricks shared cluster and serverless (#15075, @WeichenXu123)

Documentation updates:

- [Docs] Add document page for DSPy optimizer tracking (#15143, @TomeHirata)

Small bug fixes and documentation updates:

#15205, @mlflow-app[bot]; #15184, #15157, #15137, @TomeHirata; #15085, @B-Step62; #15118, @bbqiu; #15172, @harupy

## 2.21.2 (2025-03-26)

MLflow 2.21.2 is a patch release that introduces minor features and bug fixes.

- Fix connection exhausting when exporting traces to Databricks (#15124, @B-Step62)
- Add logging of result table for DSPy optimizer tracking (#15061, @TomeHirata)

## 2.21.1 (2025-03-25)

MLflow 2.21.1 is a patch release that introduces minor features and addresses some minor bugs.

Features:

- [Tracking] Introduce support for logging evaluations within DSPy (#14962, @TomeHirata)
- [Tracking] Add support for run creation when DSPy compile is executed (#14949, @TomeHirata)
- [Docker / Sagemaker] Add support for building a SageMaker serving container that does not contain Java via the `--install-java option` (#14868, @rgangopadhya)

Bug fixes:

- [Tracing] Fix an issue with trace ordering due to a timestamp conversion timezone bug (#15094, @orm011)
- [Tracking] Fix a typo in the environment variable `OTEL_EXPORTER_OTLP_PROTOCOL` definition (#15008, @gabrielfu)
- [Tracking] Fix an issue in shared and serverless clusters on Databricks when logging Spark Datasources when using the evaluate API (#15077, @WeichenXu123)
- [UI] Fix a rendering issue with displaying images from within the metric tab in the UI (#15034, @TomeHirata)

Documentation updates:

- [Docs] Add additional contextual information within the set_retriever_schema API docs (#15099, @smurching)

Small bug fixes and documentation updates:

#15009, #14995, #15039, #15040, @TomeHirata; #15010, #15053, @B-Step62; #15014, #15025, #15030, #15050, #15070, @Gumichocopengin8; #15035, #15064, @joelrobin18; #15058, @serena-ruan; #14945, @turbotimon

## 2.21.0 (2025-03-14)

We are excited to announce the release of MLflow 2.21.0! This release includes a number of significant features, enhancements, and bug fixes.

### Major New Features

- 📚 **Documentation Redesign**: [MLflow documentation](https://mlflow.org/docs/latest/) is fully revamped with a new MDX-based website that provides better navigation and makes it easier to find the information you need! (#13645, @daniellok-db)
- 🤖 **Prompt Registry**: [MLflow Prompt Registry](https://mlflow.org/docs/latest/prompts/) is a powerful tool that streamlines prompt engineering and management in your GenAI applications. It enables you to version, track, and reuse prompts across your organization. (#14795, #14834, #14936, @B-Step62, #14960, #14984, @daniellok-db, #14909, @hubertzub-db)
- ⚡️ **FastAPI Scoring Server**: The [MLflow inference server](https://mlflow.org/docs/latest/deployment/deploy-model-locally/#serving-frameworks) has been migrated from Flask to FastAPI, enabling ASGI-based scalable inference for improved performance and throughput. (#14307, @TomeHirata)
- 🔍 **Enhanced Tracing Capabilities**: [MLflow Tracing](https://mlflow.org/docs/latest/tracing/) now supports synchronous/asynchronous generators and auto-tracing for Async OpenAI, providing more flexible and comprehensive tracing options. (#14459, #14400, #14793, @14792, @B-Step62)

Features:

- [Tracking] Support OpenAI Agent SDK Auto Tracing (#14987, @B-Step62)
- [Sqlalchemy / Tracking] Support mysql ssl connections with client certs (#14839, @aksylumoed)
- [Artifacts] Supports ADLS artifact repo (#14723, @serena-ruan)
- [Tracking] Add import and docs for txtai integration (#14712, @B-Step62)
- [Models] Introduce User Auth Policy for Pyfunc Models (#14538, @aravind-segu)
- [Tracking] Support new Google GenAI SDK (#14576, @TomeHirata)
- [Tracking] Support generating traces from DSPy built-in compilation and evaluation (#14400, @B-Step62)
- [Tracking] Add mlflow.log_trace API (#14418, @TomeHirata)
- [Models] ChatAgent LangGraph and LangChain Connectors (#14215, @bbqiu)

Bug fixes:

- [Models] Fix infinite recursion error with warning handler module (#14954, @BenWilson2)
- [Model Registry] Fix invalid type issue for ModelRegistry RestStore (#14980, @B-Step62)
- [Tracking] Fix: `ExperimentViewRunsControlsActionsSelectTags` doesn't set loading state to `false` when `set-tag` request fails. (#14907, @harupy)
- [Tracking] Fix a bug in tag creation where tag values containing `": "` get truncated (#14896, @harupy)
- [Tracking] Fix false alert from AMD GPU monitor (#14884, @B-Step62)
- [Tracking] Fix `mlflow.doctor` to fall back to `mlflow-skinny` when `mlflow` is not found (#14782, @harupy)
- [Models] Handle LangGraph breaking change (#14794, @B-Step62)
- [Tracking] Fix DSPy tracing in serving (#14743, @B-Step62)
- [Tracking] Add limit to the length of experiment artifact locations (#14416, @daniellok-db)
- [Build] Fix build.py to restore specific files #14444 (#14448, @arunkothari84)
- [Models] Fix false alert for ChatModel type hint (#14343, @B-Step62)
- [Model Registry] use aes256 to talk to s3 (#14354, @artjen)
- [Tracking] Fix LiteLLM autologging (#14340, @B-Step62)
- [Models] Fix ChatCompletionResponse for model serving Pydantic 1.x (#14332, @BenWilson2)

Documentation updates:

- [Tracking] Add guide about using MLflow tracing across thread (#14881, @B-Step62)
- [Docs] Add guide for tracing deepseek (#14826, @B-Step62)
- [Docs] Update llama Jupyter notebook source (#14754, @emmanuel-ferdman)
- [Docs] Replace Databricks Community Edition with Lighthouse [1] (#14642, @TomeHirata)
- [Docs] Update models from code guide and chat model guide to always recommend models from code (#14370, @smurching)
- [Artifacts] [DOC-FIX #14183] Improve documentation for 'artifact_uri' in 'download_artifacts' (#14225, @vinayakkgarg)

Small bug fixes and documentation updates:

#14994, #14992, #14990, #14979, #14964, #14969, #14944, #14948, #14957, #14958, #14942, #14940, #14935, #14929, #14805, #14876, #14833, #14748, #14744, #14666, #14668, #14664, #14667, #14580, #14475, #14439, #14397, #14363, #14361, #14377, #14378, #14337, #14324, #14339, #14259, @B-Step62; #14981, #14943, #14914, #14930, #14924, #14927, #14786, #14910, #14859, #14891, #14883, #14863, #14852, #14788, @Gumichocopengin8; #14946, #14978, #14956, #14906, #14903, #14854, #14860, #14857, #14824, #14830, #14767, #14772, #14770, #14766, #14651, #14629, #14636, #14572, #14498, #14328, #14265, @serena-ruan; #14989, #14895, #14880, #14878, #14866, #14821, #14817, #14815, #14765, #14803, #14773, #14783, #14784, #14776, #14759, #14541, #14553, #14540, #14499, #14495, #14481, #14479, #14456, #14022, #14411, #14407, #14408, #14315, #14346, #14325, #14322, #14326, #14310, #14309, #14320, #14308, @daniellok-db; #14986, #14904, #14898, #14893, #14861, #14870, #14853, #14849, #14813, #14822, #14818, #14802, #14804, #14814, #14779, #14796, #14735, #14731, #14728, #14734, #14727, #14726, #14721, #14719, #14716, #14692, #14683, #14687, #14684, #14674, #14673, #14662, #14652, #14650, #14648, #14647, #14646, #14639, #14637, #14635, #14634, #14633, #14630, #14628, #14624, #14623, #14621, #14619, #14615, #14613, #14603, #14601, #14600, #14597, #14570, #14564, #14554, #14551, #14550, #14515, #14529, #14528, #14525, #14516, #14514, #14486, #14476, #14472, #14477, #14364, #14431, #14414, #14398, #14412, #14399, #14359, #14369, #14381, #14349, #14350, #14347, #14348, #14342, #14329, #14250, #14318, #14323, #14306, #14280, #14279, #14272, #14270, #14263, #14222, @harupy; #14985, #14850, #14800, #14799, #14671, #14665, #14594, #14506, #14457, #14395, #14371, #14360, #14327, @TomeHirata; #14755, #14567, #14367, @bbqiu; #14892, @brilee; #14941, #14932, @hubertzub-db; #14913, @joelrobin18; #14756, @jiewpeng; #14701, @jaceklaskowski; #14568, #14450, @BenWilson2; #14535, @njbrake; #14507, @arunprd; #14489, @RuchitAgrawal; #14467, @seal07; #14460, @ManzoorAhmedShaikh; #14374, @wasup-yash; #14333, @singh-kristian; #14362, #14353, #14296, #13789, @dsuhinin; #14358, @apoxnen; #14335, @Fresnel-Fabian; #14178, @emmanuel-ferdman

## 2.20.4 (2025-03-13)

MLflow 2.20.4 is a tiny patch release to include a bug fix:

- [Tracking] fix: remove `log_trace` at top level module (#14873, @yxtay)

## 2.20.3 (2025-02-26)

MLflow 2.20.3 is a patch release includes several major features and improvements

Features:

- Implemented GPU metrics for AMD/HIP GPUs (#12694, @evenmn)
- Add txtai tracing integration (#14712, @B-Step62)
- Support new Google GenAI SDK (#14576, @TomeHirata)
- Support the new thinking content block in Anthropic Claude 3.7 models (#14733, @B-Step62)

Bug fixes:

- Resolve LangGraph tracing bug with `astream_event` API (#14598, @B-Step62)

Small bug fixes and documentation updates:

#14640, #14574, #14593, @serena-ruan; #14338, #14693, #14664, #14663, #14377, @B-Step62; #14680, @JulesLandrySimard; #14388, #14685, @harupy; #14704, @brilee; #14698, #14658, @bbqiu; #14660, #14659, #14632, #14616, #14594, @TomeHirata; #14535, @njbrake

## 2.20.2 (2025-02-13)

MLflow 2.20.2 is a patch release includes several bug fixes and features

Features:

- [Tracing] Support tracing sync/async generator function with @mlflow.trace (#14459, @B-Step62)
- [Tracing] Support generating traces from DSPy built-in compilation and evaluation (#14400, @B-Step62)
- [Models] ChatAgent interface enhancements and Langgraph connectors updates (#14368, #14567, @bbqiu)
- [Models] VariantType support in spark_udf (#14317, @serena-ruan)

Bug fixes:

- [Models] DSPy thread issue fix (#14471, @chenmoneygithub)

Documentation updates:

- [Docs] ChatAgent documentation updates (#14367, @bbqiu)

Small bug fixes and documentation updates:

#14410, #14569, #14440, @harupy; #14510, #14544, #14491, #14488, @bbqiu; #14518, @serena-ruan; #14517, #14500, #14461, #14478, @TomeHirata; #14512, @shaikmoeed; #14496, #14473, #14475, @B-Step62; #14467, @seal07; #14022, #14453, #14539, @daniellok-db; #14450, @BenWilson2; #14449, @SaiMadhavanG

## 2.20.1 (2025-01-30)

MLflow 2.20.1 is a patch release includes several bug fixes and features:

Features:

- Spark_udf support for the model signatures based on type hints (#14265, @serena-ruan)
- Helper connectors to use ChatAgent with LangChain and LangGraph (#14215, @bbqiu)
- Update classifier evaluator to draw RUC/Lift curves for CatBoost models by default (#14333, @singh-kristian)

Bug fixes:

- Fix Pydantic 1.x incompatibility issue (#14332, @BenWilson2)
- Apply temporary fix for LiteLLM tracing to workaround https://github.com/BerriAI/litellm/issues/8013 (#14340, @B-Step62)
- Fix false alert from type hint based model signature for ChatModel (#14343, @B-Step62)

Other small updates:

#14337, #14382, @B-Step62; #14356, @daniellok-db, #14354, @artjen, #14360, @TomuHirata,

## 2.20.0 (2025-01-23)

We are excited to announce the release of MLflow 2.20.0! This release includes a number of significant features, enhancements, and bug fixes.

### Major New Features

- **💡Type Hint-Based Model Signature**: Define your model's [signature](https://www.mlflow.org/docs/latest/model/signatures.html) in the most **Pythonic** way. MLflow now supports defining a model signature based on the type hints in your `PythonModel`'s `predict` function, and validating input data payloads against it. (#14182, #14168, #14130, #14100, #14099, @serena-ruan)

- **🧠 Bedrock / Groq Tracing Support**: [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html) now offers a one-line auto-tracing experience for **Amazon Bedrock** and **Groq** LLMs. Track LLM invocation within your model by simply adding `mlflow.bedrock.tracing` or `mlflow.groq.tracing` call to the code. (#14018, @B-Step62, #14006, @anumita0203)

- **🗒️ Inline Trace Rendering in Jupyter Notebook**: MLflow now supports rendering a trace UI **within** the notebook where you are running models. This eliminates the need to frequently switch between the notebook and browser, creating a seamless local model debugging experience. Check out [this blog post](https://mlflow.org/blog/mlflow-tracing-in-jupyter) for a quick demo! (#13955, @daniellok-db)

- **⚡️Faster Model Validation with `uv` Package Manager**: MLflow has adopted [uv](https://github.com/astral-sh/uv), a new Rust-based, super-fast Python package manager. This release adds support for the new package manager in the [mlflow.models.predict](https://www.mlflow.org/docs/latest/model/dependencies.html#validating-environment-for-prediction) API, enabling faster model environment validation. Stay tuned for more updates! (#13824, @serena-ruan)

- **🖥️ New Chat Panel in Trace UI**: THe MLflow Trace UI now shows a unified `chat` panel for LLM invocations. The update allows you to view chat messages and function calls in a rich and consistent UI across LLM providers, as well as inspect the raw input and output payloads. (#14211, @TomuHirata)

Other Features:

- Introduced `ChatAgent` base class for defining custom python agent (#13797, @bbqiu)
- Supported Tool Calling in DSPy Tracing (#14196, @B-Step62)
- Applied timeout override to within-request local scoring server for Spark UDF inference (#14202, @BenWilson2)
- Supported dictionary type for inference params (#14091, @serena-ruan)
- Make `context` parameter optional for calling `PythonModel` instance (#14059, @serena-ruan)
- Set default task for `ChatModel` (#14068, @stevenchen-db)

Bug fixes:

- [Tracking] Fix filename encoding issue in `log_image` (#14281, @TomeHirata)
- [Models] Fix the faithfulness metric for custom override parameters supplied to the callable metric implementation (#14220, @BenWilson2)
- [Artifacts] Update presigned URL list_artifacts to return an empty list instead of an exception (#14203, @arpitjasa-db)
- [Tracking] Fix rename permission model registry (#14139, @MohamedKHALILRouissi)
- [Tracking] Fix hard-dependency to langchain package in autologging (#14125, @B-Step62)
- [Tracking] Fix constraint name for MSSQL in migration 0584bdc529eb (#14146, @daniellok-db)
- [Scoring] Fix uninitialized `loaded_model` variable (#14109, @yang-chengg)
- [Model Registry] Return empty array when `DatabricksSDKModelsArtifactRepository.list_artifacts` is called on a file (#14027, @shichengzhou-db)

Documentation updates:

- [Docs] Add a quick guide for how to host MLflow on various platforms (#14289, @B-Step62)
- [Docs] Improve documentation for 'artifact_uri' in 'download_artifacts' (#14225, @vinayakkgarg)
- [Docs] Add a page for search_traces (#14033, @TomeHirata)

Small bug fixes and documentation updates:

#14294, #14252, #14233, #14205, #14217, #14172, #14188, #14167, #14166, #14163, #14162, #14161, #13971, @TomeHirata; #14299, #14280, #14279, #14278, #14272, #14270, #14268, #14269, #14263, #14258, #14222, #14248, #14128, #14112, #14111, #14093, #14096, #14095, #14090, #14089, #14085, #14078, #14074, #14070, #14053, #14060, #14035, #14014, #14002, #14000, #13997, #13996, #13995, @harupy; #14298, #14286, #14249, #14276, #14259, #14242, #14254, #14232, #14207, #14206, #14185, #14196, #14193, #14173, #14164, #14159, #14165, #14152, #14151, #14126, #14069, #13987, @B-Step62; #14295, #14265, #14271, #14262, #14235, #14239, #14234, #14228, #14227, #14229, #14218, #14216, #14213, #14208, #14204, #14198, #14187, #14181, #14177, #14176, #14156, #14169, #14099, #14086, #13983, @serena-ruan; #14155, #14067, #14140, #14132, #14072, @daniellok-db; #14178, @emmanuel-ferdman; #14247, @dbczumar; #13789, #14108, @dsuhinin; #14212, @aravind-segu; #14223, #14191, #14084, @dsmilkov; #13804, @kriscon-db; #14158, @Lodewic; #14148, #14147, #14115, #14079, #14116, @WeichenXu123; #14135, @brilee; #14133, @manos02; #14121, @LeahKorol; #14025, @nojaf; #13948, @benglewis; #13942, @justsomerandomdude264; #14003, @Ajay-Satish-01; #13982, @prithvikannan; #13638, @MaxwellSalmon

## 2.19.0 (2024-12-11)

We are excited to announce the release of MLflow 2.19.0! This release includes a number of significant features, enhancements, and bug fixes.

### Major New Features

- **ChatModel enhancements** - ChatModel now adopts `ChatCompletionRequest` and `ChatCompletionResponse` as its new schema. The `predict_stream` interface uses `ChatCompletionChunk` to deliver true streaming responses. Additionally, the `custom_inputs` and `custom_outputs` fields in ChatModel now utilize `AnyType`, enabling support for a wider variety of data types. **Note:** In a future version of MLflow, `ChatParams` (and by extension, `ChatCompletionRequest`) will have the default values for `n`, `temperature`, and `stream` removed. (#13782, #13857, @stevenchen-db)

- **Tracing improvements** - [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html) now supports both automatic and manual tracing for DSPy, LlamaIndex and Langchain flavors. Tracing feature is also auto-enabled for mlflow evaluation for all supported flavors. (#13790, #13793, #13795, #13897, @B-Step62)

- **New Tracing Integrations** - [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html) now supports **CrewAI** and **Anthropic**, enabling a one-line, fully automated tracing experience. (#13903, @TomeHirata, #13851, @gabrielfu)

- **Any Type in model signature** - MLflow now supports AnyType in model signature. It can be used to host any data types that were not supported before. (#13766, @serena-ruan)

Other Features:

- [Tracking] Add `update_current_trace` API for adding tags to an active trace. (#13828, @B-Step62)
- [Deployments] Update databricks deployments to support AI gateway & additional update endpoints (#13513, @djliden)
- [Models] Support uv in mlflow.models.predict (#13824, @serena-ruan)
- [Models] Add type hints support including pydantic models (#13924, @serena-ruan)
- [Tracking] Add the `trace.search_spans()` method for searching spans within traces (#13984, @B-Step62)

Bug fixes:

- [Tracking] Allow passing in spark connect dataframes in mlflow evaluate API (#13889, @WeichenXu123)
- [Tracking] Fix `mlflow.end_run` inside a MLflow run context manager (#13888, @WeichenXu123)
- [Scoring] Fix spark_udf conditional check on remote spark-connect client or Databricks Serverless (#13827, @WeichenXu123)
- [Models] Allow changing max_workers for built-in LLM-as-a-Judge metrics (#13858, @B-Step62)
- [Models] Support saving all langchain runnables using code-based logging (#13821, @serena-ruan)
- [Model Registry] return empty array when DatabricksSDKModelsArtifactRepository.list_artifacts is called on a file (#14027, @shichengzhou-db)
- [Tracking] Stringify param values in client.log_batch() (#14015, @B-Step62)
- [Tracking] Remove deprecated squared parameter (#14028, @B-Step62)
- [Tracking] Fix request/response field in the search_traces output (#13985, @B-Step62)

Documentation updates:

- [Docs] Add Ollama and Instructor examples in tracing doc (#13937, @B-Step62)

Small bug fixes and documentation updates:

#13972, #13968, #13917, #13912, #13906, #13846, @serena-ruan; #13969, #13959, #13957, #13958, #13925, #13882, #13879, #13881, #13869, #13870, #13868, #13854, #13849, #13847, #13836, #13823, #13811, #13820, #13775, #13768, #13764, @harupy; #13960, #13914, #13862, #13892, #13916, #13918, #13915, #13878, #13891, #13863, #13859, #13850, #13844, #13835, #13818, #13762, @B-Step62; #13913, #13848, #13774, @TomeHirata; #13936, #13954, #13883, @daniellok-db; #13947, @AHB102; #13929, #13922, @Ajay-Satish-01; #13857, @stevenchen-db; #13773, @BenWilson2; #13705, @williamjamir; #13745, #13743, @WeichenXu123; #13895, @chenmoneygithub; #14023, @theBeginner86

## 2.18.0 (2024-11-18)

We are excited to announce the release of MLflow 2.18.0! This release includes a number of significant features, enhancements, and bug fixes.

### Python Version Update

Python 3.8 is now at an end-of-life point. With official support being dropped for this legacy version, **MLflow now requires Python 3.9**
as a minimum supported version.

> Note: If you are currently using MLflow's `ChatModel` interface for authoring custom GenAI applications, please ensure that you
> have read the future breaking changes section below.

### Major New Features

- **🦺 Fluent API Thread/Process Safety** - MLflow's fluent APIs for tracking and the model registry have been overhauled to add support for both thread and multi-process safety. You are now no longer forced to use the Client APIs for managing experiments, runs, and logging from within multiprocessing and threaded applications. (#13456, #13419, @WeichenXu123)

- **🧩 DSPy flavor** - MLflow now supports logging, loading, and tracing of `DSPy` models, broadening the support for advanced GenAI authoring within MLflow. Check out the [MLflow DSPy Flavor](https://mlflow.org/docs/latest/llms/dspy/index.html) documentation to get started! (#13131, #13279, #13369, #13345, @chenmoneygithub, #13543, #13800, #13807, @B-Step62, #13289, @michael-berk)

- **🖥️ Enhanced Trace UI** - [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html)'s UI has undergone
  a significant overhaul to bring usability and quality of life updates to the experience of auditing and investigating the contents of GenAI traces, from enhanced span content rendering using markdown to a standardized span component structure, (#13685, #13357, #13242, @daniellok-db)

- **🚄 New Tracing Integrations** - [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html) now supports **DSPy**, **LiteLLM**, and **Google Gemini**, enabling a one-line, fully automated tracing experience. These integrations unlock enhanced observability across a broader range of industry tools. Stay tuned for upcoming integrations and updates! (#13801, @TomeHirata, #13585, @B-Step62)

- **📊 Expanded LLM-as-a-Judge Support** - MLflow now enhances its evaluation capabilities with support for additional providers, including `Anthropic`, `Bedrock`, `Mistral`, and `TogetherAI`, alongside existing providers like `OpenAI`. Users can now also configure proxy endpoints or self-hosted LLMs that follow the provider API specs by using the new `proxy_url` and `extra_headers` options. Visit the [LLM-as-a-Judge](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#llm-as-a-judge-metrics) documentation for more details! (#13715, #13717, @B-Step62)

- **⏰ Environment Variable Detection** - As a helpful reminder for when you are deploying models, MLflow now detects and reminds users of environment variables set during model logging, ensuring they are configured for deployment. In addition to this, the `mlflow.models.predict` utility has also been updated to include these variables in serving simulations, improving pre-deployment validation. (#13584, @serena-ruan)

### Breaking Changes to ChatModel Interface

- **ChatModel Interface Updates** - As part of a broader unification effort within MLflow and services that rely on or deeply integrate
  with MLflow's GenAI features, we are working on a phased approach to making a consistent and standard interface for custom GenAI
  application development and usage. In the first phase (planned for release in the next few releases of MLflow), we are marking
  several interfaces as deprecated, as they will be changing. These changes will be:

  - **Renaming of Interfaces**:
    - `ChatRequest` → `ChatCompletionRequest` to provide disambiguation for future planned request interfaces.
    - `ChatResponse` → `ChatCompletionResponse` for the same reason as the input interface.
    - `metadata` fields within `ChatRequest` and `ChatResponse` → `custom_inputs` and `custom_outputs`, respectively.
  - **Streaming Updates**:
    - `predict_stream` will be updated to enable true streaming for custom GenAI applications. Currently, it returns a generator with synchronous outputs from predict. In a future release, it will return a generator of `ChatCompletionChunks`, enabling asynchronous streaming. While the API call structure will remain the same, the returned data payload will change significantly, aligning with LangChain's implementation.
  - **Legacy Dataclass Deprecation**:
    - Dataclasses in `mlflow.models.rag_signatures` will be deprecated, merging into unified `ChatCompletionRequest`, `ChatCompletionResponse`, and `ChatCompletionChunks`.

Other Features:

- [Evaluate] Add Huggingface BLEU metrics to MLflow Evaluate (#12799, @nebrass)
- [Models / Databricks] Add support for `spark_udf` when running on Databricks Serverless runtime, Databricks connect, and prebuilt python environments (#13276, #13496, @WeichenXu123)
- [Scoring] Add a `model_config` parameter for `pyfunc.spark_udf` for customization of batch inference payload submission (#13517, @WeichenXu123)
- [Tracing] Standardize retriever span outputs to a list of MLflow `Document`s (#13242, @daniellok-db)
- [UI] Add support for visualizing and comparing nested parameters within the MLflow UI (#13012, @jescalada)
- [UI] Add support for comparing logged artifacts within the Compare Run page in the MLflow UI (#13145, @jescalada)
- [Databricks] Add support for `resources` definitions for `Langchain` model logging (#13315, @sunishsheth2009)
- [Databricks] Add support for defining multiple retrievers within `dependencies` for Agent definitions (#13246, @sunishsheth2009)

Bug fixes:

- [Database] Cascade deletes to datasets when deleting experiments to fix a bug in MLflow's `gc` command when deleting experiments with logged datasets (#13741, @daniellok-db)
- [Models] Fix a bug with `Langchain`'s `pyfunc` predict input conversion (#13652, @serena-ruan)
- [Models] Fix signature inference for subclasses and `Optional` dataclasses that define a model's signature (#13440, @bbqiu)
- [Tracking] Fix an issue with async logging batch splitting validation rules (#13722, @WeichenXu123)
- [Tracking] Fix an issue with `LangChain`'s autologging thread-safety behavior (#13672, @B-Step62)
- [Tracking] Disable support for running spark autologging in a threadpool due to limitations in Spark (#13599, @WeichenXu123)
- [Tracking] Mark `role` and `index` as required for chat schema (#13279, @chenmoneygithub)
- [Tracing] Handle raw response in openai autolog (#13802, @harupy)
- [Tracing] Fix a bug with tracing source run behavior when running inference with multithreading on `Langchain` models (#13610, @WeichenXu123)

Documentation updates:

- [Docs] Add docstring warnings for upcoming changes to ChatModel (#13730, @stevenchen-db)
- [Docs] Add a contributor's guide for implementing tracing integrations (#13333, @B-Step62)
- [Docs] Add guidance in the use of `model_config` when logging models as code (#13631, @sunishsheth2009)
- [Docs] Add documentation for the use of custom library artifacts with the `code_paths` model logging feature (#13702, @TomeHirata)
- [Docs] Improve `SparkML` `log_model` documentation with guidance on how return probabilities from classification models (#13684, @WeichenXu123)

Small bug fixes and documentation updates:

#13775, #13768, #13764, #13744, #13699, #13742, #13703, #13669, #13682, #13569, #13563, #13562, #13539, #13537, #13533, #13408, #13295, @serena-ruan; #13768, #13764, #13761, #13738, #13737, #13735, #13734, #13723, #13726, #13662, #13692, #13689, #13688, #13680, #13674, #13666, #13661, #13625, #13460, #13626, #13546, #13621, #13623, #13603, #13617, #13614, #13606, #13600, #13583, #13601, #13602, #13604, #13598, #13596, #13597, #13531, #13594, #13589, #13581, #13112, #13587, #13582, #13579, #13578, #13545, #13572, #13571, #13564, #13559, #13565, #13558, #13541, #13560, #13556, #13534, #13386, #13532, #13385, #13384, #13383, #13507, #13523, #13518, #13492, #13493, #13487, #13490, #13488, #13449, #13471, #13417, #13445, #13430, #13448, #13443, #13429, #13418, #13412, #13382, #13402, #13381, #13364, #13356, #13309, #13313, #13334, #13331, #13273, #13322, #13319, #13308, #13302, #13268, #13298, #13296, @harupy; #13705, @williamjamir; #13632, @shichengzhou-db; #13755, #13712, #13260, @BenWilson2; #13745, #13743, #13697, #13548, #13549, #13577, #13349, #13351, #13350, #13342, #13341, @WeichenXu123; #13807, #13798, #13787, #13786, #13762, #13749, #13733, #13678, #13721, #13611, #13528, #13444, #13450, #13360, #13416, #13415, #13336, #13305, #13271, @B-Step62; #13808, #13708, @smurching; #13739, @fedorkobak; #13728, #13719, #13695, #13677, @TomeHirata; #13776, #13736, #13649, #13285, #13292, #13282, #13283, #13267, @daniellok-db; #13711, @bhavya2109sharma; #13693, #13658, @aravind-segu; #13553, @dsuhinin; #13663, @gitlijian; #13657, #13629, @parag-shendye; #13630, @JohannesJungbluth; #13613, @itepifanio; #13480, @agjendem; #13627, @ilyaresh; #13592, #13410, #13358, #13233, @nojaf; #13660, #13505, @sunishsheth2009; #13414, @lmoros-DB; #13399, @Abubakar17; #13390, @KekmaTime; #13291, @michael-berk; #12511, @jgiannuzzi; #13265, @Ahar28; #13785, @Rick-McCoy; #13676, @hyolim-e; #13718, @annzhang-db; #13705, @williamjamir

## 2.17.2 (2024-10-31)

MLflow 2.17.2 includes several major features and improvements

Features:

- [Model Registry] DatabricksSDKModelsArtifactRepository support (#13203, @shichengzhou-db)
- [Tracking] Support extracting new UCFunctionToolkit as model resources (#13567, @serena-ruan)

Bug fixes:

- [Models] Fix RunnableBinding saving (#13566, @B-Step62)
- [Models] Pin numpy when pandas < 2.1.2 in pip requirements (#13580, @serena-ruan)

Documentation updates:

- [Docs] ChatModel tool calling tutorial (#13542, @daniellok-db)

Small bug fixes and documentation updates:

#13569, @serena-ruan; #13595, @BenWilson2; #13593, @mnijhuis-dnb;

## 2.17.1 (2024-10-25)

MLflow 2.17.1 includes several major features and improvements

Features:

- [Tracking] Support custom chat endpoint without endpoint type set as llm judge (#13538, @B-Step62)
- [Tracking] Support tracing for OpenAI Swarm (#13497, @B-Step62)
- [Tracking] Support UC Connections as model dependency and resources (#13481, #13491 @sunishsheth2009)
- [Tracking] Support Genie Spaces as model resources (#13441, @aravind-segu)
- [Models] Support new Transformers task for llm/v1/embedding (#13468, @B-Step62)

Bug fixes:

- [Tracking] Fix tool span inputs/outputs format in LangChain autolog (#13527, @B-Step62)
- [Models] Fix code_path handling for LlamaIndex flavor (#13486, @B-Step62)
- [Models] Fix signature inference for subclass and optional dataclasses (#13440, @bbqiu)
- [Tracking] Fix error thrown in set_retriever_schema's behavior when it's called twice (#13422, @sunishsheth2009)
- [Tracking] Fix dependency extraction from RunnableCallables (#13423, @aravind-segu)

Documentation updates:

- [Docs] Fixed typo in docs (#13478, @JAMNESIA)
- [Docs] Improve CLI docs - attention about setting MLFLOW_TRACKING_URI (#13465, @BartoszLitwiniuk)
- [Docs] Add documentation for infer_signature usage with GenAI flavors (#13407, @serena-ruan)

Small bug fixes and documentation updates:

#13293, #13510, #13501, #13506, #13446, @harupy; #13341, #13342, @WeichenXu123; #13396, @dvorst; #13535, @chenmoneygithub; #13503, #13469, #13416, @B-Step62; #13519, #13516, @serena-ruan; #13504, @sunishsheth2009; #13508, @KamilStachera; #13397, @kriscon-db

## 2.17.0 (2024-09-26)

We are excited to announce the release of MLflow 2.17.0! This release includes several enhancements to extend the
functionality of MLflow's ChatModel interface to further extend its versatility for handling custom GenAI application use cases.
Additionally, we've improved the interface within the tracing UI to provide a structured output for retrieved documents,
enhancing the ability to read the contents of those documents within the UI.
We're also starting the work on improving both the utility and the versatility of MLflow's evaluate functionality for GenAI,
initially with support for callable GenAI evaluation metrics.

### Major Features and notifications:

- **ChatModel enhancements** - As the GenAI-focused 'cousin' of `PythonModel`, `ChatModel` is getting some sizable functionality
  extensions. From native support for tool calling (a requirement for creating a custom agent), simpler conversions to the
  internal dataclass constructs needed to interface with `ChatModel` via the introduction of `from_dict` methods to all data structures,
  the addition of a `metadata` field to allow for full input payload customization, handling of the new `refusal` response type, to the
  inclusion of the interface type to the response structure to allow for greater integration compatibility.
  (#13191, #13180, #13143, @daniellok-db, #13102, #13071, @BenWilson2)

- **Callable GenAI Evaluation Metrics** - As the initial step in a much broader expansion of the functionalities of `mlflow.evaluate` for
  GenAI use cases, we've converted the GenAI evaluation metrics to be callable. This allows you to use them directly in packages that support
  callable GenAI evaluation metrics, as well as making it simpler to debug individual responses when prototyping solutions. (#13144, @serena-ruan)

- **Audio file support in the MLflow UI** - You can now directly 'view' audio files that have been logged and listen to them from within the MLflow UI's
  artifact viewer pane.

- **MLflow AI Gateway is no longer deprecated** - We've decided to revert our deprecation for the AI Gateway feature. We had renamed it to the
  MLflow Deployments Server, but have reconsidered and reverted the naming and namespace back to the original configuration.

Features:

- [Tracing] Add Standardization to retriever span outputs within MLflow tracing (#13242, @daniellok-db)
- [Models] Add support for LlamaIndex `Workflows` objects to be serialized when calling `log_model()` (#13277, #13305, #13336, @B-Step62)
- [Models] Add tool calling support for ChatModel (#13191, @daniellok-db)
- [Models] Add `from_dict()` function to ChatModel dataclasses (#13180, @daniellok-db)
- [Models] Add metadata field for ChatModel (#13143, @daniellok-db)
- [Models] Update ChatCompletionResponse to populate object type (#13102, @BenWilson2)
- [Models] Add support for LLM response refusal (#13071, @BenWilson2)
- [Models] Add support for resources to be passed in via `langchain.log_model()` (#13315, @sunishsheth2009)
- [Tracking] Add support for setting multiple retrievers' schema via `set_retriever_schema` (#13246, @sunishsheth2009)
- [Eval] Make Evaluation metrics callable (#13144, @serena-ruan)
- [UI] Add audio support to artifact viewer UI (#13017, @sydneyw-spotify)
- [Databricks] Add support for route_optimized parameter in databricks deployment client (#13222, @prabhatkgupta)

Bug fixes:

- [Tracking] Fix tracing for LangGraph (#13215, @B-Step62)
- [Tracking] Fix an issue with `presigned_url_artifact` requests being in the wrong format (#13366, @WeichenXu123)
- [Models] Update Databricks dependency extraction functionality to work with the `langchain-databricks` partner package. (#13266, @B-Step62)
- [Model Registry] Fix retry and credential refresh issues with artifact downloads from the model registry (#12935, @rohitarun-db)
- [Tracking] Fix LangChain autologging so that langchain-community is not required for partner packages (#13172, @B-Step62)
- [Artifacts] Fix issues with file removal for the local artifact repository (#13005, @rzalawad)

Documentation updates:

- [Docs] Add guide for building custom GenAI apps with ChatModel (#13207, @BenWilson2)
- [Docs] Add updates to the MLflow AI Gateway documentation (#13217, @daniellok-db)
- [Docs] Remove MLflow AI Gateway deprecation status (#13153, @BenWilson2)
- [Docs] Add contribution guide for MLflow tracing integrations (#13333, @B-Step62)
- [Docs] Add documentation regarding the `run_id` parameter within the `search_trace` API (#13251, @B-Step62)

Small bug fixes and documentation updates:

#13372, #13271, #13243, #13226, #13190, #13230, #13208, #13130, #13045, #13094, @B-Step62; #13302, #13238, #13234, #13205, #13200, #13196, #13198, #13193, #13192, #13194, #13189, #13184, #13182, #13161, #13179, #13178, #13110, #13162, #13173, #13171, #13169, #13168, #13167, #13156, #13127, #13133, #13089, #13073, #13057, #13058, #13067, #13062, #13061, #13052, @harupy; #13295, #13219, #13038, @serena-ruan; #13176, #13164, @WeichenXu123; #13163, @gabrielfu; #13186, @varshinimuthukumar1; #13128, #13115, @nojaf; #13120, @levscaut; #13152, #13075, @BenWilson2; #13138, @tanguylefloch-veesion; #13087, @SeanAverS; #13285, #13051, #13043, @daniellok-db; #13224, @levscaut;

## 2.16.2 (2024-09-17)

MLflow 2.16.2 includes several major features and improvements

Bug fixes:

- [Models] Revert "Update Dependency Extraction for Agents (#13105)" (#13155, @aravind-segu)

## 2.16.1 (2024-09-13)

MLflow 2.16.1 is a patch release that includes some minor feature improvements and addresses several bug fixes.

Features:

- [Tracing] Add Support for an Open Telemetry compatible exporter to configure external sinks for MLflow traces (#13118, @B-Step62)
- [Model Registry, AWS] Add support for utilizing AWS KMS-based encryption for the MLflow Model Registry (#12495, @artjen)
- [Model Registry] Add support for using the OSS Unity Catalog server as a Model Registry (#13034, #13065, #13066, @rohitarun-db)
- [Models] Introduce path-based transformers logging to reduce memory requirements for saving large transformers models (#13070, @B-Step62)

Bug fixes:

- [Tracking] Fix a data payload size issue with `Model.get_tags_dict` by eliminating the return of the internally-used `config` field (#13086, @harshilprajapati96)
- [Models] Fix an issue with LangChain Agents where sub-dependencies were not being properly extracted (#13105, @aravind-segu)
- [Tracking] Fix an issue where the wrong checkpoint for the current best model in auto checkpointing was being selected (#12981, @hareeen)
- [Tracking] Fix an issue where local timezones for trace initialization were not being taken into account in AutoGen tracing (#13047, @B-Step62)

Documentation updates:

- [Docs] Added RunLLM chat widget to MLflow's documentation site (#13123, @likawind)

Small bug fixes and documentation updates:

#13140, #13141, #13098, #13091, #13101, #13100, #13095, #13044, #13048, @B-Step62; #13142, #13092, #13132, #13055, #13049, @harupy; #13135, #13036, #13029, @serena-ruan; #13134, #13081, #13078, @daniellok-db; #13107, #13103, @kriscon-db; #13104, @arpitjasa-db; #13022, @nojaf; #13069, @minihat; #12879, @faizankshaikh

## 2.16.0 (2024-08-30)

We are excited to announce the release of MLflow 2.16.0. This release includes many major features and improvements!

### Major features:

- **LlamaIndex Enhancements**🦙 - to provide additional flexibility to the [LlamaIndex integration](https://mlflow.org/docs/latest/llms/llama-index/index.html), we now have support for the [models-from-code](https://mlflow.org/docs/latest/models.html#models-from-code) functionality for logging, extended engine-based logging, and broadened support for external vector stores.

- **LangGraph Support** - We've expanded the LangChain integration to support the agent framework [LangGraph](https://langchain-ai.github.io/langgraph/). With tracing and support for logging using the models-from-code feature, creating and storing agent applications has never been easier!

- **AutoGen Tracing** - Full automatic support for tracing multi-turn agent applications built with [Microsoft's AutoGen](https://microsoft.github.io/autogen/) framework is now available in MLflow. Enabling autologging via `mlflow.autogen.autolog()` will instrument your agents built with AutoGen.

- **Plugin support for AI Gateway** - You can now define your own provider interfaces that will work with MLflow's AI Gateway (also known as the MLflow Deployments Server). Creating an installable provider definition will allow you to connect the Gateway server to any GenAI service of your choosing.

Features:

- [UI] Add updated deployment usage examples to the MLflow artifact viewer (#13024, @serena-ruan, @daniellok-db)
- [Models] Support logging LangGraph applications via the models-from-code feature (#12996, @B-Step62)
- [Models] Extend automatic authorization pass-through support for Langgraph agents (#13001, @aravind-segu)
- [Models] Expand the support for LangChain application logging to include UCFunctionToolkit dependencies (#12966, @aravind-segu)
- [Models] Support saving LlamaIndex engine directly via the models-from-code feature (#12978, @B-Step62)
- [Models] Support models-from-code within the LlamaIndex flavor (#12944, @B-Step62)
- [Models] Remove the data structure conversion of input examples to ensure enhanced compatibility with inference signatures (#12782, @serena-ruan)
- [Models] Add the ability to retrieve the underlying model object from within `pyfunc` model wrappers (#12814, @serena-ruan)
- [Models] Add spark vector UDT type support for model signatures (#12758, @WeichenXu123)
- [Tracing] Add tracing support for AutoGen (#12913, @B-Step62)
- [Tracing] Reduce the latency overhead for tracing (#12885, @B-Step62)
- [Tracing] Add Async support for the trace decorator (#12877, @MPKonst)
- [Deployments] Introduce a plugin provider system to the AI Gateway (Deployments Server) (#12611, @gabrielfu)
- [Projects] Add support for parameter submission to MLflow Projects run in Databricks (#12854, @WeichenXu123)
- [Model Registry] Introduce support for Open Source Unity Catalog as a model registry service (#12888, @artjen)

Bug fixes:

- [Tracking] Reduce the contents of the `model-history` tag to only essential fields (#12983, @harshilprajapati96)
- [Models] Fix the behavior of defining the device to utilize when loading transformers models (#12977, @serena-ruan)
- [Models] Fix evaluate behavior for LlamaIndex (#12976, @B-Step62)
- [Models] Replace `pkg_resources` with `importlib.metadata` due to package deprecation (#12853, @harupy)
- [Tracking] Fix error handling for OpenAI autolog tracing (#12841, @B-Step62)
- [Tracking] Fix a condition where a deadlock can occur when connecting to an SFTP artifact store (#12938, @WeichenXu123)
- [Tracking] Fix an issue where code_paths dependencies were not properly initialized within the system path for LangChain models (#12923, @harshilprajapati96)
- [Tracking] Fix a type error for metrics value logging (#12876, @beomsun0829)
- [Tracking] Properly catch NVML errors when collecting GPU metrics (#12903, @chenmoneygithub)
- [Deployments] Improve Gateway schema support for the OpenAI provider (#12781, @danilopeixoto)
- [Model Registry] Fix deletion of artifacts when downloading from a non-standard DBFS location during UC model registration (#12821, @smurching)

Documentation updates:

- [Docs] Add documentation guides for LangGraph support (#13025, @BenWilson2)
- [Docs] Add additional documentation for models from code feature (#12936, @BenWilson2)
- [Docs] Add documentation for model serving input payloads (#12848, @serena-ruan)

Small bug fixes and documentation updates:

#12987, #12991, #12974, #12975, #12932, #12893, #12851, #12793, @serena-ruan; #13019, #13013, @aravind-segu; #12943, @piyushdaftary; #12906, #12898, #12757, #12750, #12727, @daniellok-db; #12995, #12985, #12964, #12962, #12960, #12953, #12951, #12937, #12914, #12929, #12907, #12897, #12880, #12865, #12864, #12862, #12850, #12847, #12833, #12835, #12826, #12824, #12795, #12796, @harupy; #12592, @antbbn; #12993, #12984, #12899, #12745, @BenWilson2; #12965, @nojaf; #12968, @bbqiu; #12956, @mickvangelderen; #12939, #12950, #12915, #12931, #12919, #12889, #12849, #12794, #12779, #12836, #12823, #12737, @B-Step62; #12903, @chenmoneygithub; #12905, @Atry; #12884, #12858, #12807, #12800, #10874, @WeichenXu123; #12342, @kriscon-db; #12742, @edwardfeng-db

## 2.15.1 (2024-08-06)

MLflow 2.15.1 is a patch release that addresses several bug fixes.

Bug fixes:

- [Tracking] Fix silent disabling of LangChain autologging for LangChain >= 0.2.10. (#12779, @B-Step62)
- [Tracking] Fix `mlflow.evaluate` crash on binary classification with data subset only contains single class (#12825, @serena-ruan)
- [Tracking] Fix incompatibility of MLflow Tracing with LlamaIndex >= 0.10.61 (#12890, @B-Step62)
- [Tracking] Record exceptions in OpenAI autolog tracing (#12841, @B-Step62)
- [Tracking] Fix url with e2 proxy (#12873, @chenmoneygithub)
- [Tracking] Fix regression of connecting to MLflow tracking server on other Databricks workspace (#12861, @WeichenXu123)
- [UI] Fix refresh button for model metrics on Experiment and Run pages (#12869, @beomsun0829)

Documentation updates:

- [Docs] Update doc for Spark ML vector type (#12827, @WeichenXu123)

Small bug fixes and documentation updates:

#12823, #12860, #12844, #12843, @B-Step62; #12863, #12828, @harupy; #12845, @djliden; #12820, @annzhang-db; #12831, @chenmoneygithub

## 2.15.0 (2024-07-29)

We are excited to announce the release candidate for MLflow 2.15.0. This release includes many major features and improvements!

### Major features:

- **LlamaIndex Flavor**🦙 - MLflow now offers a native integration with [LlamaIndex](https://www.llamaindex.ai/), one of the most popular libraries for building GenAI apps centered around custom data. This integration allows you to log LlamaIndex indices within MLflow, allowing for the loading and deployment of your indexed data for inference tasks with different engine types. MLflow also provides comprehensive tracing support for LlamaIndex operations, offering unprecedented transparency into complex queries. Check out the [MLflow LlamaIndex documentation](https://mlflow.org/docs/latest/llms/llama-index/index.html) to get started! (#12633, @michael-berk, @B-Step62)

- **OpenAI Tracing**🔍 - We've enhanced our OpenAI integration with a new tracing feature that works seamlessly with MLflow OpenAI autologging. You can now enable tracing of their OpenAI API usage with a single `mlflow.openai.autolog()` call, thereby MLflow will automatically log valuable metadata such as token usage and a history of your interactions, providing deeper insights into your OpenAI-powered applications. To start exploring this new capability, please check out [the tracing documentation](https://mlflow.org/docs/latest/llms/tracing/index.html#automatic-tracing)! (#12267, @gabrielfu)

- **Enhanced Model Deployment with New Validation Feature**✅ - To improve the reliability of model deployments, MLflow has added a new method to validate your model before deploying it to an inference endpoint. This feature helps to eliminate typical errors in input and output handling, streamlining the process of model deployment and increasing confidence in your deployed models. By catching potential issues early, you can ensure a smoother transition from development to production. (#12710, @serena-ruan)

- **Custom Metrics Definition Recording for Evaluations**📊 - We've strengthened the flexibility of defining custom metrics for model evaluation by automatically logging and versioning metrics definitions, including models used as judges and prompt templates. With this new capability, you can ensure reproducibility of evaluations across different runs and easily reuse evaluation setups for consistency, facilitating more meaningful comparisons between different models or versions. (#12487, #12509, @xq-yin)

- **Databricks SDK Integration**🔐 - MLflow's interaction with Databricks endpoints has been fully migrated to use the [Databricks SDK](https://docs.databricks.com/en/dev-tools/sdk-python.html). This change brings more robust and reliable connections between MLflow and Databricks, and access to the latest Databricks features and capabilities. We mark the legacy databricks-cli support as deprecated and will remove in the future release. (#12313, @WeichenXu123)

- **Spark VectorUDT Support**💥 - MLflow's [Model Signature](https://mlflow.org/docs/latest/model/signatures.html) framework now supports Spark Vector UDT (User Defined Type), enabling logging and deployment of models using Spark VectorUDT with robust type validation. (#12758, @WeichenXu123)

### Other Notable Changes

Features:

- [Tracking] Add `parent_id` as a parameter to the `start_run` fluent API for alternative control flows (#12721, @Flametaa)
- [Tracking] Add U2M authentication support for connecting to Databricks from MLflow (#12713, @WeichenXu123)
- [Tracking] Support deleting remote artifacts with `mlflow gc` (#12451, @M4nouel)
- [Tracing] Traces can now be deleted conveniently via UI from the Traces tab in the experiments page (#12641, @daniellok-db)
- [Models] Introduce additional parameters for the `ChatModel` interface for GenAI flavors (#12612, @WeichenXu123)
- [Models] [Transformers] Support input images encoded with b64.encodebytes (#12087, @MadhuM02)
- [Models Registry] Add support for AWS KMS encryption for the Unity Catalog model registry integration (#12495, @artjen)
- [Models] Fix MLflow Dataset hashing logic for Pandas dataframe to use `iloc` for accessing rows (#12410, @julcsii)
- [Models Registry] Support presigned urls without headers for artifact location (#12349, @artjen)
- [UI] The experiments page in the MLflow UI has an updated look, and comes with some performance optimizations for line charts (#12641, @hubertzub-db)
- [UI] Line charts can now be configured to ignore outliers in the data (#12641, @daniellok-db)
- [UI] Creating compatibility with Kubeflow Dashboard UI (#12663, @cgilviadee)
- [UI] Add a new section to the artifact page in the Tracking UI, which shows code snippet to validate model input format before deployment (#12729, @serena-ruan)

Bug fixes:

- [Tracking] Fix the model construction bug in MLflow SHAP evaluation for scikit-learn model (#12599, @serena-ruan)
- [Tracking] File store get_experiment_by_name returns all stage experiments (#12788, @serena-ruan)
- [Tracking] Fix Langchain callback injection logic for async/streaming request (#12773, @B-Step62)
- [Tracing] [OpenAI] Fix stream tracing for OpenAI to record the correct chunk structure (#12629, @BenWilson2)
- [Tracing] [LangChain] Fix LangChain tracing bug for `.batch` call due to thread unsafety (#12701, @B-Step62)
- [Tracing] [LangChain] Fix nested trace issue in LangChain tracing. (#12705, @B-Step62)
- [Tracing] Prevent intervention between MLflow Tracing and other OpenTelemetry-based libraries (#12457, @B-Step62)
- [Models] Fix `log_model` issue in MLflow >= 2.13 that causes databricks DLT py4j service crashing (#12514, @WeichenXu123)
- [Models] [Transformers] Fix batch inference issue for Transformers Whisper model (#12575, @B-Step62)
- [Models] [LangChain] Fix the empty generator issue in `predict_stream` for `AgentExecutor` and other non-Runnable chains (#12518, @B-Step62)
- [Scoring] Fix Spark UDF permission denied issue in Databricks runtime (#12774, @WeichenXu123)

Documentation updates:

- Add documentation on authentication for Databricks UC Model Registry (#12552, @WeichenXu123)
- Adding model-from-code documentation for LangChain and Pyfunc (#12325, #12336, @sunishsheth2009)
- Add FAQ entry for viewing trace exceptions (#12309, @BenWilson2)
- Add note about `fork` vs `spawn` method when using multiprocessing for parallel runs (#12337, @B-Step62)
- Add example usage of `extract_fields` for `mlflow.search_traces` (#12319, @xq-yin)
- Replace GPT-3.5-turbo with GPT-4o-mini (#12740, #12746, @Acksout)

Small bug fixes and documentation updates:

#12727, #12709, #12685, #12667, #12673, #12602, #12601, #12655, #12641, #12635, #12634, #12584, #12428, #12388, #12352, #12298, #12750, #12727, #12757, @daniellok-db; #12726, #12733, #12691, #12622, #12579, #12581, #12285, #12311, #12357, #12339, #12338, #12705, #12797, #12787, #12784, #12771, #12737, @B-Step62; #12715, @hubertzub-db; #12722, #12804, @annzhang-db; #12676, #12680, #12665, #12664, #12671, #12651, #12649, #12647, #12637, #12632, #12603, #12343, #12328, #12286, #12793, #12770, @serena-ruan; #12670, #12613, #12473, #12506, #12485, #12477, #12468, #12464, #12443, #12807, #12800, #10874, #12761, @WeichenXu123; #12690, #12678, #12686, #12545, #12621, #12598, #12583, #12582, #12510, #12580, #12570, #12571, #12559, #12538, #12537, #12519, #12515, #12507, #12508, #12502, #12499, #12497, #12447, #12467, #12426, #12448, #12430, #12420, #12385, #12371, #12359, #12284, #12345, #12316, #12287, #12303, #12291, #12795, #12786, #12796, #12792, #12791, #12778, #12777, #12755, #12751, #12753, #12749, @harupy; #12742, #12702, #12742 @edwardfeng-db; #12605, @alxhslm; #12662, @freemso; #12577, @rafyzg; #12512, @Jaishree2310; #12491, #1274, @BenWilson2; #12549, @besarthoxhaj; #12476, @jessechancy; #12541, @amanjam; #12479, #12472, #12433, #12289, @xq-yin; #12486, #12474, #11406, @jgiannuzzi; #12463, @jsuchome; #12460, @Venki1402; #12449, @yukimori; #12318, @RistoAle97; #12440, @victolee0; #12416, @Dev-98; #11771, @lababidi; #12417, @dannikay; #12663, @cgilviadee; #12410, @julcsii; #12600, @ZTZK; #12803, @hcmturner; #12747, @michael-berk; #12342, @kriscon-db; #12766, @artjen;

## 2.14.3 (2024-07-12)

MLflow 2.14.3 is a patch release that addresses bug fixes and additional documentation for released features

Features:

- [Model Registry] Add support for server-side encryption when uploading files to AWS S3 (#12495, @artjen)

Bug fixes:

- [Models] Fix stream trace logging with the OpenAI autologging implementation to record the correct chunk structure (#12629, @BenWilson2)
- [Models] Fix batch inference behavior for Whisper-based translation models to allow for multiple audio file inputs (#12575, @B-Step62)

Documentation updates:

- [Docs] Add documentation for OpenAI autologging (#12608, @BenWilson2)

Small bug fixes and documentation updates:

#12556, #12628, @B-Step62; #12582, #12560, @harupy; #12553, @nojaf

## 2.14.2 (2024-07-03)

MLflow 2.14.2 is a patch release that includes several important bug fixes and documentation enhancements.

Bug fixes:

- [Models] Fix an issue with requirements inference error handling when disabling the default warning-only behavior (#12547, @B-Step62)
- [Models] Fix dependency inference issues with Transformers models saved with the unified API `llm/v1/xxx` task definitions. (#12551, @B-Step62)
- [Models / Databricks] Fix an issue with MLlfow `log_model` introduced in MLflow 2.13.0 that causes Databricks DLT service to crash in some situations (#12514, @WeichenXu123)
- [Models] Fix an output data structure issue with the `predict_stream` implementation for LangChain AgentExecutor and other non-Runnable chains (#12518, @B-Step62)
- [Tracking] Fix an issue with the `predict_proba` inference method in the `sklearn` flavor when loading an sklearn pipeline object as `pyfunc` (#12554, @WeichenXu123)
- [Tracking] Fix an issue with the Tracing implementation where other services usage of OpenTelemetry would activate MLflow tracing and cause errors (#12457, @B-Step62)
- [Tracking / Databricks] Correct an issue when running dependency inference in Databricks that can cause duplicate dependency entries to be logged (#12493, @sunishsheth2009)

Documentation updates:

- [Docs] Add documentation and guides for the MLflow tracing schema (#12521, @BenWilson2)

Small bug fixes and documentation updates:

#12311, #12285, #12535, #12543, #12320, #12444, @B-Step62; #12310, #12340, @serena-ruan; #12409, #12432, #12471, #12497, #12499, @harupy; #12555, @nojaf; #12472, #12431, @xq-yin; #12530, #12529, #12528, #12527, #12526, #12524, #12531, #12523, #12525, #12522, @dbczumar; #12483, @jsuchome; #12465, #12441, @BenWilson2; #12450, @StarryZhang-whu

## 2.14.1 (2024-06-20)

MLflow 2.14.1 is a patch release that contains several bug fixes and documentation improvements

Bug fixes:

- [Models] Fix params and model_config handling for llm/v1/xxx Transformers model (#12401, @B-Step62)
- [UI] Fix dark mode user preference (#12386, @daniellok-db)
- [Docker] Fix docker image failing to build with `install_mlflow=False` (#12388, @daniellok-db)

Documentation updates:

- [Docs] Add link to langchain autologging page in doc (#12398, @xq-yin)
- [Docs] Add documentation for Models from Code (#12381, @BenWilson2)

Small bug fixes and documentation updates:

#12415, #12396, #12394, @harupy; #12403, #12382, @BenWilson2; #12397, @B-Step62

## 2.14.0 (2024-06-17)

MLflow 2.14.0 includes several major features and improvements that we're very excited to announce!

### Major features:

- **MLflow Tracing**: Tracing is powerful tool designed to enhance your ability to monitor, analyze, and debug GenAI applications by allowing you to inspect the intermediate outputs generated as your application handles a request. This update comes with an automatic LangChain integration to make it as easy as possible to get started, but we've also implemented high-level fluent APIs, and low-level client APIs for users who want more control over their trace instrumentation. For more information, check out the [guide in our docs](https://mlflow.org/docs/latest/llms/tracing/index.html)!
- **Unity Catalog Integration**: The MLflow Deployments server now has an integration with Unity Catalog, allowing you to leverage registered functions as tools for enhancing your chat application. For more information, check out [this guide](https://mlflow.org/docs/latest/llms/deployments/uc_integration.html)!
- **OpenAI Autologging**: Autologging support has now been added for the OpenAI model flavor. With this feature, MLflow will automatically log a model upon calling the OpenAI API. Each time a request is made, the inputs and outputs will be logged as artifacts. Check out [the guide](https://mlflow.org/docs/latest/llms/openai/guide/index.html#openai-autologging) for more information!

Other Notable Features:

- [Models] Support input images encoded with b64.encodebytes (#12087, @MadhuM02)
- [Tracking] Support async logging per X seconds (#12324, @chenmoneygithub)
- [Tracking] Provide a way to set urllib's connection number and max size (#12227, @chenmoneygithub)
- [Projects] Make MLflow project runner supporting submit spark job to databricks runtime >= 13 (#12139, @WeichenXu123)
- [UI] Add the "description" column to the runs table (#11996, @zhouyou9505)

Bug fixes:

- [Model Registry] Handle no headers presigned url (#12349, @artjen)
- [Models] Fix docstring order for ChatResponse class and make object field immutable (#12305, @xq-yin)
- [Databricks] Fix root user checking in get_databricks_nfs_temp_dir and get_databricks_local_temp_dir (#12186, @WeichenXu123)
- [Tracking] fix \_init_server process terminate hang (#12076, @zhouyou9505)
- [Scoring] Fix MLflow model container and slow test CI failure (#12042, @WeichenXu123)

Documentation updates:

- [Docs] Enhance documentation for autologging supported libraries (#12356, @xq-yin)
- [Tracking, Docs] Adding Langchain as a code example and doc string (#12325, @sunishsheth2009)
- [Tracking, Docs] Adding Pyfunc as a code example and doc string (#12336, @sunishsheth2009)
- [Docs] Add FAQ entry for viewing trace exceptions in Docs (#12309, @BenWilson2)
- [Docs] Add note about 'fork' vs 'spawn' method when using multiprocessing for parallel runs (#12337, @B-Step62)
- [Docs] Fix type error in tracing example for function wrapping (#12338, @B-Step62)
- [Docs] Add example usage of "extract_fields" for mlflow.search_traces in documentation (#12319, @xq-yin)
- [Docs] Update LangChain Autologging docs (#12306, @B-Step62)
- [Docs] Add Tracing documentation (#12191, @BenWilson2)

Small bug fixes and documentation updates:

#12359, #12308, #12350, #12284, #12345, #12316, #12287, #12303, #12291, #12288, #12265, #12170, #12248, #12263, #12249, #12251, #12239, #12241, #12240, #12235, #12242, #12172, #12215, #12228, #12216, #12164, #12225, #12203, #12181, #12198, #12195, #12192, #12146, #12171, #12163, #12166, #12124, #12106, #12113, #12112, #12074, #12077, #12058, @harupy; #12355, #12326, #12114, #12343, #12328, #12327, #12340, #12286, #12310, #12200, #12209, #12189, #12194, #12201, #12196, #12174, #12107, @serena-ruan; #12364, #12352, #12354, #12353, #12351, #12298, #12297, #12220, #12155, @daniellok-db; #12311, #12357, #12346, #12312, #12339, #12281, #12283, #12282, #12268, #12236, #12247, #12199, #12232, #12233, #12221, #12229, #12207, #12212, #12193, #12167, #12137, #12147, #12148, #12138, #12127, #12065, @B-Step62; #12289, #12253, #12330 @xq-yin; #11771, @lababidi; #12280, #12275, @BenWilson2; #12246, #12244, #12211, #12066, #12061, @WeichenXu123; #12278, @sunishsheth2009; #12136, @kriscon-db; #11911, @jessechancy; #12169, @hubertzub-db

## 2.13.2 (2024-06-06)

MLflow 2.13.2 is a patch release that includes several bug fixes and integration improvements to existing
features.

Features:

- [Tracking] Provide a way to set `urllib`'s connection number and max size (#12227, @chenmoneygithub)
- [Tracking] Support UC directory as MLflow MetaDataset (#12224, @chenmoneygithub)

Bug fixes:

- [Models] Fix inferring `mlflow[gateway]` as dependency when using `mlflow.deployment` module (#12264, @B-Step62)
- [Tracking] Flatten the model_config with `/` before logging as params (#12190, @sunishsheth2009)

Small bug fixes and documentation updates:

#12268, #12210, @B-Step62; #12214, @harupy; #12223, #12226, @annzhang-db; #12260, #12237, @prithvikannan; #12261, @BenWilson2; #12231, @serena-ruan; #12238, @sunishsheth2009

## 2.13.1 (2024-05-30)

MLflow 2.13.1 is a patch release that includes several bug fixes and integration improvements to existing features. New features that are introduced in this patch release are intended to provide a foundation to further major features that will be released in the next release.

Features:

- [MLflow] Add `mlflow[langchain]` extra that installs recommended versions of langchain with MLflow (#12182, @sunishsheth2009)
- [Tracking] Adding the ability to override the model_config in langchain flavor if loaded as pyfunc (#12085, @sunishsheth2009)
- [Model Registry] Automatically detect if Presigned URLs are required for Unity Catalog (#12177, @artjen)

Bug fixes:

- [Tracking] Use `getUserLocalTempDir` and `getUserNFSTempDir` to replace `getReplLocalTempDir` and `getReplNFSTempDir` in databricks runtime (#12105, @WeichenXu123)
- [Model] Updating chat model to take default input_example and predict to accept json during inference (#12115, @sunishsheth2009)
- [Tracking] Automatically call `load_context` when inferring signature in pyfunc (#12099, @sunishsheth2009)

Small bug fixes and documentation updates:

#12180, #12152, #12128, #12126, #12100, #12086, #12084, #12079, #12071, #12067, #12062, @serena-ruan; #12175, #12167, #12137, #12134, #12127, #12123, #12111, #12109, #12078, #12080, #12064, @B-Step62; #12142, @2maz; #12171, #12168, #12159, #12153, #12144, #12104, #12095, #12083, @harupy; #12160, @aravind-segu; #11990, @kriscon-db; #12178, #12176, #12090, #12036, @sunishsheth2009; #12162, #12110, #12088, #11937, #12075, @daniellok-db; #12133, #12131, @prithvikannan; #12132, #12035, @annzhang-db; #12121, #12120, @liangz1; #12122, #12094, @dbczumar; #12098, #12055, @mparkhe

## 2.13.0 (2024-05-20)

MLflow 2.13.0 includes several major features and improvements

With this release, we're happy to introduce several features that enhance the usability of MLflow broadly across a range of use cases.

### Major Features and Improvements:

- **Streamable Python Models**: The newly introduced `predict_stream` API for Python Models allows for custom model implementations that support the return of a generator object, permitting full customization for GenAI applications.

- **Enhanced Code Dependency Inference**: A new feature for automatically inferrring code dependencies based on detected dependencies within a model's implementation. As a supplement to the `code_paths` parameter, the introduced `infer_model_code_paths` option when logging a model will determine which additional code modules are needed in order to ensure that your models can be loaded in isolation, deployed, and reliably stored.

- **Standardization of MLflow Deployment Server**: Outputs from the Deployment Server's endpoints now conform to OpenAI's interfaces to provide a simpler integration with commonly used services.

Features:

- [Deployments] Update the MLflow Deployment Server interfaces to be OpenAI compatible (#12003, @harupy)
- [Deployments] Add `Togetherai` as a supported provider for the MLflow Deployments Server (#11557, @FotiosBistas)
- [Models] Add `predict_stream` API support for Python Models (#11791, @WeichenXu123)
- [Models] Enhance the capabilities of logging code dependencies for MLflow models (#11806, @WeichenXu123)
- [Models] Add support for RunnableBinding models in LangChain (#11980, @serena-ruan)
- [Model Registry / Databricks] Add support for renaming models registered to Unity Catalog (#11988, @artjen)
- [Model Registry / Databricks] Improve the handling of searching for invalid components from Unity Catalog registered models (#11961, @artjen)
- [Model Registry] Enhance retry logic and credential refresh to mitigate cloud provider token expiration failures when uploading or downloading artifacts (#11614, @artjen)
- [Artifacts / Databricks] Add enhanced lineage tracking for models loaded from Unity Catalog (#11305, @shichengzhou-db)
- [Tracking] Add resourcing metadata to Pyfunc models to aid in model serving environment configuration (#11832, @sunishsheth2009)
- [Tracking] Enhance LangChain signature inference for models as code (#11855, @sunishsheth2009)

Bug fixes:

- [Artifacts] Prohibit invalid configuration options for multi-part upload on AWS (#11975, @ian-ack-db)
- [Model Registry] Enforce registered model metadata equality (#12013, @artjen)
- [Models] Correct an issue with `hasattr` references in `AttrDict` usages (#11999, @BenWilson2)

Documentation updates:

- [Docs] Simplify the main documentation landing page (#12017, @BenWilson2)
- [Docs] Add documentation for the expanded code path inference feature (#11997, @BenWilson2)
- [Docs] Add documentation guidelines for the `predict_stream` API (#11976, @BenWilson2)
- [Docs] Add support for enhanced Documentation with the `JFrog` MLflow Plugin (#11426, @yonarbel)

Small bug fixes and documentation updates:

#12052, #12053, #12022, #12029, #12024, #11992, #12004, #11958, #11957, #11850, #11938, #11924, #11922, #11920, #11820, #11822, #11798, @serena-ruan; #12054, #12051, #12045, #12043, #11987, #11888, #11876, #11913, #11868, @sunishsheth2009; #12049, #12046, #12037, #11831, @dbczumar; #12047, #12038, #12020, #12021, #11970, #11968, #11967, #11965, #11963, #11941, #11956, #11953, #11934, #11921, #11454, #11836, #11826, #11793, #11790, #11776, #11765, #11763, #11746, #11748, #11740, #11735, @harupy; #12025, #12034, #12027, #11914, #11899, #11866, @BenWilson2; #12026, #11991, #11979, #11964, #11939, #11894, @daniellok-db; #11951, #11974, #11916, @annzhang-db; #12015, #11931, #11627, @jessechancy; #12014, #11917, @prithvikannan; #12012, @AveshCSingh; #12001, @yunpark93; #11984, #11983, #11977, #11977, #11949, @edwardfeng-db; #11973, @bbqiu; #11902, #11835, #11775, @B-Step62; #11845, @lababidi

## 2.12.2 (2024-05-08)

MLflow 2.12.2 is a patch release that includes several bug fixes and integration improvements to existing features. New features that are introduced in this patch release are intended to provide a foundation to further major features that will be released in the next 2 minor releases.

Features:

- [Models] Add an environment configuration flag to enable raising an exception instead of a warning for failures in model dependency inference (#11903, @BenWilson2)
- [Models] Add support for the `llm/v1/embeddings` task in the Transformers flavor to unify the input and output structures for embedding models (#11795, @B-Step62)
- [Models] Introduce model streaming return via `predict_stream()` for custom `pyfunc` models capable of returning a stream response (#11791, #11895, @WeichenXu123)
- [Evaluate] Add support for overriding the entire model evaluation judgment prompt within `mlflow.evaluate` for GenAI models (#11912, @apurva-koti)
- [Tracking] Add support for defining deployment resource metadata to configure deployment resources within `pyfunc` models (#11832, #11825, #11804, @sunishsheth2009)
- [Tracking] Add support for logging `LangChain` and custom `pyfunc` models as code (#11855, #11842, @sunishsheth2009)
- [Tracking] Modify MLflow client's behavior to read from a global asynchronous configuration state (#11778, #11780, @chenmoneygithub)
- [Tracking] Enhance system metrics data collection to include a GPU power consumption metric (#11747, @chenmoneygithub)

Bug fixes:

- [Models] Fix a validation issue when performing signature validation if `params` are specified (#11838, @WeichenXu123)
- [Databricks] Fix an issue where models cannot be loaded in the Databricks serverless runtime (#11758, @WeichenXu123)
- [Databricks] Fix an issue with the Databricks serverless runtime where scaled workers do not have authorization to read from the driver NFS mount (#11757, @WeichenXu123)
- [Databricks] Fix an issue in the Databricks serverless runtime where a model loaded via a `spark_udf` for inference fails due to a configuration issue (#11752, @WeichenXu123)
- [Server-infra] Upgrade the gunicorn dependency to version 22 to address a third-party security issue (#11742, @maitreyakv)

Documentation updates:

- [Docs] Add additional guidance on search syntax restrictions for search APIs (#11892, @BenWilson2)
- [Docs] Fix an issue with the quickstart guide where the Keras example model is defined incorrectly (#11848, @horw)
- [Docs] Provide fixes and updates to LangChain tutorials and guides (#11802, @BenWilson2)
- [Docs] Fix the model registry example within the docs for correct type formatting (#11789, @80rian)

Small bug fixes and documentation updates:

#11928, @apurva-koti; #11910, #11915, #11864, #11893, #11875, #11744, @BenWilson2; #11913, #11918, #11869, #11873, #11867, @sunishsheth2009; #11916, #11879, #11877, #11860, #11843, #11844, #11817, #11841, @annzhang-db; #11822, #11861, @serena-ruan; #11890, #11819, #11794, #11774, @B-Step62; #11880, @prithvikannan; #11833, #11818, #11954, @harupy; #11831, @dbczumar; #11812, #11816, #11800, @daniellok-db; #11788, @smurching; #11756, @IgorMilavec; #11627, @jessechancy

## 2.12.1 (2024-04-17)

MLflow 2.12.1 includes several major features and improvements

With this release, we're pleased to introduce several major new features that are focused on enhanced GenAI support, Deep Learning workflows involving images, expanded table logging functionality, and general usability enhancements within the UI and external integrations.

### Major Features and Improvements:

- **PromptFlow**: Introducing the new PromptFlow flavor, designed to enrich the GenAI landscape within MLflow. This feature simplifies the creation and management of dynamic prompts, enhancing user interaction with AI models and streamlining prompt engineering processes. (#11311, #11385 @brynn-code)

- **Enhanced Metadata Sharing for Unity Catalog**: MLflow now supports the ability to share metadata (and not model weights) within Databricks Unity Catalog. When logging a model, this functionality enables the automatic duplication of metadata into a dedicated subdirectory, distinct from the model's actual storage location, allowing for different sharing permissions and access control limits. (#11357, #11720 @WeichenXu123)

- **Code Paths Unification and Standardization**: We have unified and standardized the `code_paths` parameter across all MLflow flavors to ensure a cohesive and streamlined user experience. This change promotes consistency and reduces complexity in the model deployment lifecycle. (#11688, @BenWilson2)

- **ChatOpenAI and AzureChatOpenAI Support**: Support for the ChatOpenAI and AzureChatOpenAI interfaces has been integrated into the LangChain flavor, facilitating seamless deployment of conversational AI models. This development opens new doors for building sophisticated and responsive chat applications leveraging cutting-edge language models. (#11644, @B-Step62)

- **Custom Models in Sentence-Transformers**: The sentence-transformers flavor now supports custom models, allowing for a greater flexibility in deploying tailored NLP solutions. (#11635, @B-Step62)

- **Image Support for Log Table**: With the addition of image support in `log_table`, MLflow enhances its capabilities in handling rich media. This functionality allows for direct logging and visualization of images within the platform, improving the interpretability and analysis of visual data. (#11535, @jessechancy)

- **Streaming Support for LangChain**: The newly introduced `predict_stream` API for LangChain models supports streaming outputs, enabling real-time output for chain invocation via pyfunc. This feature is pivotal for applications requiring continuous data processing and instant feedback. (#11490, #11580 @WeichenXu123)

### Security Fixes:

- **Security Patch**: Addressed a critical Local File Read/Path Traversal vulnerability within the Model Registry, ensuring robust protection against unauthorized access and securing user data integrity. (#11376, @WeichenXu123)

Features:

- [Models] Add the PromptFlow flavor (#11311, #11385 @brynn-code)
- [Models] Add a new `predict_stream` API for streamable output for Langchain models and the `DatabricksDeploymentClient` (#11490, #11580 @WeichenXu123)
- [Models] Deprecate and add `code_paths` alias for `code_path` in `pyfunc` to be standardized to other flavor implementations (#11688, @BenWilson2)
- [Models] Add support for custom models within the `sentence-transformers` flavor (#11635, @B-Step62)
- [Models] Enable Spark `MapType` support within model signatures when used with Spark udf inference (#11265, @WeichenXu123)
- [Models] Add support for metadata-only sharing within Unity Catalog through the use of a subdirectory (#11357, #11720 @WeichenXu123)
- [Models] Add Support for the `ChatOpenAI` and `AzureChatOpenAI` LLM interfaces within the LangChain flavor (#11644, @B-Step62)
- [Artifacts] Add support for utilizing presigned URLs when uploading and downloading files when using Unity Catalog (#11534, @artjen)
- [Artifacts] Add a new `Image` object for handling the logging and optimized compression of images (#11404, @jessechancy)
- [Artifacts] Add time and step-based metadata to the logging of images (#11243, @jessechancy)
- [Artifacts] Add the ability to log a dataset to Unity Catalog by means of `UCVolumeDatasetSource` (#11301, @chenmoneygithub)
- [Tracking] Remove the restrictions for logging a table in Delta format to no longer require running within a Databricks environment (#11521, @chenmoneygithub)
- [Tracking] Add support for logging `mlflow.Image` files within tables (#11535, @jessechancy)
- [Server-infra] Introduce override configurations for controlling how http retries are handled (#11590, @BenWilson2)
- [Deployments] Implement `chat` & `chat streaming` for Anthropic within the MLflow deployments server (#11195, @gabrielfu)

Security fixes:

- [Model Registry] Fix Local File Read/Path Traversal (LFI) bypass vulnerability (#11376, @WeichenXu123)

Bug fixes:

- [Model Registry] Fix a registry configuration error that occurs within Databricks serverless clusters (#11719, @WeichenXu123)
- [Model Registry] Delete registered model permissions when deleting the underlying models (#11601, @B-Step62)
- [Model Registry] Disallow `%` in model names to prevent URL mangling within the UI (#11474, @daniellok-db)
- [Models] Fix an issue where critically important environment configurations were not being captured as langchain dependencies during model logging (#11679, @serena-ruan)
- [Models] Patch the `LangChain` loading functions to handle uncorrectable pickle-related exceptions that are thrown when loading a model in certain versions (#11582, @B-Step62)
- [Models] Fix a regression in the `sklearn` flavor to reintroduce support for custom prediction methods (#11577, @B-Step62)
- [Models] Fix an inconsistent and unreliable implementation for batch support within the `langchain` flavor (#11485, @WeichenXu123)
- [Models] Fix loading remote-code-dependent `transformers` models that contain custom code (#11412, @daniellok-db)
- [Models] Remove the legacy conversion logic within the `transformers` flavor that generates an inconsistent input example display within the MLflow UI (#11508, @B-Step62)
- [Models] Fix an issue with Keras autologging iteration input handling (#11394, @WeichenXu123)
- [Models] Fix an issue with `keras` autologging training dataset generator (#11383, @WeichenXu123)
- [Tracking] Fix an issue where a module would be imported multiple times when logging a langchain model (#11553, @sunishsheth2009)
- [Tracking] Fix the sampling logic within the `GetSampledHistoryBulkInterval` API to produce more consistent results when displayed within the UI (#11475, @daniellok-db)
- [Tracking] Fix import issues and properly resolve dependencies of `langchain` and `lanchain_community` within `langchain` models when logging (#11450, @sunishsheth2009)
- [Tracking] Improve the performance of asynchronous logging (#11346, @chenmoneygithub)
- [Deployments] Add middle-of-name truncation to excessively long deployment names for Sagemaker image deployment (#11523, @BenWilson2)

Documentation updates:

- [Docs] Add clarity and consistent documentation for `code_paths` docstrings in API documentation (#11675, @BenWilson2)
- [Docs] Add documentation guidance for `sentence-transformers` `OpenAI`-compatible API interfaces (#11373, @es94129)

Small bug fixes and documentation updates:

#11723, @freemin7; #11722, #11721, #11690, #11717, #11685, #11689, #11607, #11581, #11516, #11511, #11358, @serena-ruan; #11718, #11673, #11676, #11680, #11671, #11662, #11659, #11654, #11633, #11628, #11620, #11610, #11605, #11604, #11600, #11603, #11598, #11572, #11576, #11555, #11563, #11539, #11532, #11528, #11525, #11514, #11513, #11509, #11457, #11501, #11500, #11459, #11446, #11443, #11442, #11433, #11430, #11420, #11419, #11416, #11418, #11417, #11415, #11408, #11325, #11327, #11313, @harupy; #11707, #11527, #11663, #11529, #11517, #11510, #11489, #11455, #11427, #11389, #11378, #11326, @B-Step62; #11715, #11714, #11665, #11626, #11619, #11437, #11429, @BenWilson2; #11699, #11692, @annzhang-db; #11693, #11533, #11396, #11392, #11386, #11380, #11381, #11343, @WeichenXu123; #11696, #11687, #11683, @chilir; #11387, #11625, #11574, #11441, #11432, #11428, #11355, #11354, #11351, #11349, #11339, #11338, #11307, @daniellok-db; #11653, #11369, #11270, @chenmoneygithub; #11666, #11588, @jessechancy; #11661, @jmjeon94; #11640, @tunjan; #11639, @minkj1992; #11589, @tlm365; #11566, #11410, @brynn-code; #11570, @lababidi; #11542, #11375, #11345, @edwardfeng-db; #11463, @taranarmo; #11506, @ernestwong-db; #11502, @fzyzcjy; #11470, @clemenskol; #11452, @jkfran; #11413, @GuyAglionby; #11438, @victorsun123; #11350, @liangz1; #11370, @sunishsheth2009; #11379, #11304, @zhouyou9505; #11321, #11323, #11322, @michael-berk; #11333, @cdancette; #11228, @TomeHirata

## 2.12.0 (2024-04-16)

MLflow 2.12.0 has been yanked from PyPI due to an issue with packaging required JS components. MLflow 2.12.1 is its replacement.

## 2.11.3 (2024-03-21)

MLflow 2.11.3 is a patch release that addresses a security exploit with the Open Source MLflow tracking server and miscellaneous Databricks integration fixes

Bug fixes:

- [Security] Address an LFI exploit related to misuse of url parameters (#11473, @daniellok-db)
- [Databricks] Fix an issue with Databricks Runtime version acquisition when deploying a model using Databricks Docker Container Services (#11483, @WeichenXu123)
- [Databricks] Correct an issue with credential management within Databricks Model Serving (#11468, @victorsun123)
- [Models] Fix an issue with chat request validation for LangChain flavor (#11478, @BenWilson2)
- [Models] Fixes for LangChain models that are logged as code (#11494, #11436 @sunishsheth2009)

## 2.11.2 (2024-03-19)

MLflow 2.11.2 is a patch release that introduces corrections for the support of custom transformer models, resolves LangChain integration problems, and includes several fixes to enhance stability.

Bug fixes:

- [Security] Address LFI exploit (#11376, @WeichenXu123)
- [Models] Fix transformers models implementation to allow for custom model and component definitions to be loaded properly (#11412, #11428 @daniellok-db)
- [Models] Fix the LangChain flavor implementation to support defining an MLflow model as code (#11370, @sunishsheth2009)
- [Models] Fix LangChain VectorSearch parsing errors (#11438, @victorsun123)
- [Models] Fix LangChain import issue with the community package (#11450, @sunishsheth2009)
- [Models] Fix serialization errors with RunnableAssign in the LangChain flavor (#11358, @serena-ruan)
- [Models] Address import issues with LangChain community for Databricks models (#11350, @liangz1)
- [Registry] Fix model metadata sharing within Databricks Unity Catalog (#11357, #11392 @WeichenXu123)

Small bug fixes and documentation updates:

#11321, #11323, @michael-berk; #11326, #11455, @B-Step62; #11333, @cdancette; #11373, @es94129; #11429, @BenWilson2; #11413, @GuyAglionby; #11338, #11339, #11355, #11432, #11441, @daniellok-db; #11380, #11381, #11383, #11394, @WeichenXu123; #11446, @harupy;

## 2.11.1 (2024-03-06)

MLflow 2.11.1 is a patch release, containing fixes for some Databricks integrations and other various issues.

Bug fixes:

- [UI] Add git commit hash back to the run page UI (#11324, @daniellok-db)
- [Databricks Integration] Explicitly import vectorstores and embeddings in databricks_dependencies (#11334, @daniellok-db)
- [Databricks Integration] Modify DBR version parsing logic (#11328, @daniellok-db)

Small bug fixes and documentation updates:

#11336, #11335, @harupy; #11303, @B-Step62; #11319, @BenWilson2; #11306, @daniellok-db

## 2.11.0 (2024-03-01)

MLflow 2.11.0 includes several major features and improvements

With the MLflow 2.11.0 release, we're excited to bring a series of large and impactful features that span both GenAI and Deep Learning use cases.

- The MLflow Tracking UI got an overhaul to better support the review and comparison of training runs for Deep Learning workloads. From grouping to large-scale metric plotting throughout
  the iterations of a DL model's training cycle, there are a large number of quality of life improvements to enhance your Deep Learning MLOps workflow.

- Support for the popular [PEFT](https://www.mlflow.org/docs/latest/llms/transformers/guide/index.html#peft-models-in-mlflow-transformers-flavor) library from HuggingFace is now available
  in the `mlflow.transformers` flavor. In addition to PEFT support, we've removed the restrictions on Pipeline types
  that can be logged to MLflow, as well as the ability to, when developing and testing models, log a transformers pipeline without copying foundational model weights. These
  enhancements strive to make the transformers flavor more useful for cutting-edge GenAI models, new pipeline types, and to simplify the development process of prompt engineering, fine-tuning,
  and to make iterative development faster and cheaper. Give the updated flavor a try today! (#11240, @B-Step62)

- We've added support to both [PyTorch](https://www.mlflow.org/docs/latest/python_api/mlflow.pytorch.html#mlflow.pytorch.autolog) and
  [TensorFlow](https://www.mlflow.org/docs/latest/python_api/mlflow.tensorflow.html#mlflow.tensorflow.autolog) for automatic model weights checkpointing (including resumption from a
  previous state) for the auto logging implementations within both flavors. This highly requested feature allows you to automatically configure long-running Deep Learning training
  runs to keep a safe storage of your best epoch, eliminating the risk of a failure late in training from losing the state of the model optimization. (#11197, #10935, @WeichenXu123)

- We've added a new interface to Pyfunc for GenAI workloads. The new `ChatModel` interface allows for interacting with a deployed GenAI chat model as you would with any other provider.
  The simplified interface (no longer requiring conformance to a Pandas DataFrame input type) strives to unify the API interface experience. (#10820, @daniellok-db)

- We now support Keras 3. This large overhaul of the Keras library introduced new fundamental changes to how Keras integrates with different DL frameworks, bringing with it
  a host of new functionality and interoperability. To learn more, see the [Keras 3.0 Tutorial](https://www.mlflow.org/docs/latest/deep-learning/keras/quickstart/quickstart_keras.html)
  to start using the updated model flavor today! (#10830, @chenmoneygithub)

- [Mistral AI](https://mistral.ai/) has been added as a native [provider](https://www.mlflow.org/docs/latest/llms/deployments/index.html#providers) for the MLflow Deployments Server. You can
  now create proxied connections to the Mistral AI services for completions and embeddings with their powerful GenAI models. (#11020, @thnguyendn)

- We've added compatibility support for the OpenAI 1.x SDK. Whether you're using an OpenAI LLM for model evaluation or calling OpenAI within a LangChain model, you'll now be able to
  utilize the 1.x family of the OpenAI SDK without having to point to deprecated legacy APIs. (#11123, @harupy)

Features:

- [UI] Revamp the MLflow Tracking UI for Deep Learning workflows, offering a more intuitive and efficient user experience (#11233, @daniellok-db)
- [Data] Introduce the ability to log datasets without loading them into memory, optimizing resource usage and processing time (#11172, @chenmoneygithub)
- [Models] Introduce logging frequency controls for TensorFlow, aligning it with Keras for consistent performance monitoring (#11094, @chenmoneygithub)
- [Models] Add PySpark DataFrame support in `mlflow.pyfunc.predict`, enhancing data compatibility and analysis options for batch inference (#10939, @ernestwong-db)
- [Models] Introduce new CLI commands for updating model requirements, facilitating easier maintenance, validation and updating of models without having to re-log (#11061, @daniellok-db)
- [Models] Update Embedding API for sentence transformers to ensure compatibility with OpenAI format, broadening model application scopes (#11019, @lu-wang-dl)
- [Models] Improve input and signature support for text-generation models, optimizing for Chat and Completions tasks (#11027, @es94129)
- [Models] Enable chat and completions task outputs in the text-generation pipeline, expanding interactive capabilities (#10872, @es94129)
- [Tracking] Add node id to system metrics for enhanced logging in multi-node setups, improving diagnostics and monitoring (#11021, @chenmoneygithub)
- [Tracking] Implement `mlflow.config.enable_async_logging` for asynchronous logging, improving log handling and system performance (#11138, @chenmoneygithub)
- [Evaluate] Enhance model evaluation with endpoint URL support, streamlining performance assessments and integrations (#11262, @B-Step62)
- [Deployments] Implement chat & chat streaming support for Cohere, enhancing interactive model deployment capabilities (#10976, @gabrielfu)
- [Deployments] Enable Cohere streaming support, allowing real-time interaction functionalities for the MLflow Deployments server with the Cohere provider (#10856, @gabrielfu)
- [Docker / Scoring] Optimize Docker images for model serving, ensuring more efficient deployment and scalability (#10954, @B-Step62)
- [Scoring] Support completions (`prompt`) and embeddings (`input`) format inputs in the scoring server, increasing model interaction flexibility (#10958, @es94129)

Bug Fixes:

- [Model Registry] Correct the oversight of not utilizing the default credential file in model registry setups (#11261, @B-Step62)
- [Model Registry] Address the visibility issue of aliases in the model versions table within the registered model detail page (#11223, @smurching)
- [Models] Ensure `load_context()` is called when enforcing `ChatModel` outputs so that all required external references are included in the model object instance (#11150, @daniellok-db)
- [Models] Rectify the keras output dtype in signature mismatches, ensuring data consistency and accuracy (#11230, @chenmoneygithub)
- [Models] Resolve spark model loading failures, enhancing model reliability and accessibility (#11227, @WeichenXu123)
- [Models] Eliminate false warnings for missing signatures in Databricks, improving the user experience and model validation processes (#11181, @B-Step62)
- [Models] Implement a timeout for signature/requirement inference during Transformer model logging, optimizing the logging process and avoiding delays (#11037, @B-Step62)
- [Models] Address the missing dtype issue for transformer pipelines, ensuring data integrity and model accuracy (#10979, @B-Step62)
- [Models] Correct non-idempotent predictions due to in-place updates to model-config, stabilizing model outputs (#11014, @B-Step62)
- [Models] Fix an issue where specifying `torch.dtype` as a string was not being applied correctly to the underlying transformers model (#11297, #11295, @harupy)
- [Tracking] Fix `mlflow.evaluate` `col_mapping` bug for non-LLM/custom metrics, ensuring accurate evaluation and metric calculation (#11156, @sunishsheth2009)
- [Tracking] Resolve the `TensorInfo` TypeError exception message issue, ensuring clarity and accuracy in error reporting for users (#10953, @leecs0503)
- [Tracking] Enhance `RestException` objects to be picklable, improving their usability in distributed computing scenarios where serialization is essential (#10936, @WeichenXu123)
- [Tracking] Address the handling of unrecognized response error codes, ensuring robust error processing and improved user feedback in edge cases (#10918, @chenmoneygithub)
- [Spark] Update hardcoded `io.delta:delta-spark_2.12:3.0.0` dependency to the correct scala version, aligning dependencies with project requirements (#11149, @WeichenXu123)
- [Server-infra] Adapt to newer versions of python by avoiding `importlib.metadata.entry_points().get`, enhancing compatibility and stability (#10752, @raphaelauv)
- [Server-infra / Tracking] Introduce an environment variable to disable mlflow configuring logging on import, improving configurability and user control (#11137, @jmahlik)
- [Auth] Enhance auth validation for `mlflow.login()`, streamlining the authentication process and improving security (#11039, @chenmoneygithub)

Documentation Updates:

- [Docs] Introduce a comprehensive notebook demonstrating the use of ChatModel with Transformers and Pyfunc, providing users with practical insights and guidelines for leveraging these models (#11239, @daniellok-db)
- [Tracking / Docs] Stabilize the dataset logging APIs, removing the experimental status (#11229, @dbczumar)
- [Docs] Revise and update the documentation on authentication database configuration, offering clearer instructions and better support for setting up secure authentication mechanisms (#11176, @gabrielfu)
- [Docs] Publish a new guide and tutorial for MLflow data logging and `log_input`, enriching the documentation with actionable advice and examples for effective data handling (#10956, @BenWilson2)
- [Docs] Upgrade the documentation visuals by replacing low-resolution and poorly dithered GIFs with high-quality HTML5 videos, significantly enhancing the learning experience (#11051, @BenWilson2)
- [Docs / Examples] Correct the compatibility matrix for OpenAI in MLflow Deployments Server documentation, providing users with accurate integration details and supporting smoother deployments (#11015, @BenWilson2)

Small bug fixes and documentation updates:

#11284, #11096, #11285, #11245, #11254, #11252, #11250, #11249, #11234, #11248, #11242, #11244, #11236, #11208, #11220, #11222, #11221, #11219, #11218, #11210, #11209, #11207, #11196, #11194, #11177, #11205, #11183, #11192, #11179, #11178, #11175, #11174, #11166, #11162, #11151, #11168, #11167, #11153, #11158, #11143, #11141, #11119, #11123, #11124, #11117, #11121, #11078, #11097, #11079, #11095, #11082, #11071, #11076, #11070, #11072, #11073, #11069, #11058, #11034, #11046, #10951, #11055, #11045, #11035, #11044, #11043, #11031, #11030, #11023, #10932, #10986, #10949, #10943, #10928, #10929, #10925, #10924, #10911, @harupy; #11289, @BenWilson2; #11290, #11145, #11125, #11098, #11053, #11006, #11001, #11011, #11007, #10985, #10944, #11231, @daniellok-db; #11276, #11280, #11275, #11263, #11247, #11257, #11258, #11256, #11224, #11211, #11182, #11059, #11056, #11048, #11008, #10923, @serena-ruan; #11129, #11086, @victorsun123; #11292, #11004, #11204, #11148, #11165, #11146, #11115, #11099, #11092, #11029, #10983, @B-Step62; #11189, #11191, #11022, #11160, #11110, #11088, #11042, #10879, #10832, #10831, #10888, #10908, @michael-berk; #10627, #11217, #11200, #10969, @liangz1; #11215, #11173, #11000, #10931, @edwardfeng-db; #11188, #10711, @TomeHirata; #11186, @xhochy; #10916, @annzhang-db; #11131, #11010, #11060, @WeichenXu123; #11063, #10981, #10889, ##11269, @chenmoneygithub; #11054, #10921, @smurching; #11018, @mingyangge-db; #10989, @minkj1992; #10796, @kriscon-db; #10984, @eltociear; #10982, @holzman; #10972, @bmuskalla; #10959, @prithvikannan; #10941, @mahesh-venkatachalam; #10915, @Cokral; #10904, @dannyfriar; #11134, @WP-LKL; #11287, @serkef;

## 2.10.2 (2024-02-09)

MLflow 2.10.2 includes several major features and improvements

Small bug fixes and documentation updates:

#11065, @WeichenXu123

## 2.10.1 (2024-02-06)

MLflow 2.10.1 is a patch release, containing fixes for various bugs in the `transformers` and `langchain` flavors, the MLflow UI, and the S3 artifact store. More details can be found in the patch notes below.

Bug fixes:

- [UI] Fixed a bug that prevented datasets from showing up in the MLflow UI (#10992, @daniellok-db)
- [Artifact Store] Fixed directory bucket region name retrieval (#10967, @kriscon-db)
- Bug fixes for Transformers flavor
  - [Models] Fix an issue with transformer pipelines not inheriting the torch dtype specified on the model, causing pipeline inference to consume more resources than expected. (#10979, @B-Step62)
  - [Models] Fix non-idempotent prediction due to in-place update to model-config (#11014, @B-Step62)
  - [Models] Fixed a bug affecting prompt templating with Text2TextGeneration pipelines. Previously, calling `predict()` on a pyfunc-loaded Text2TextGeneration pipeline would fail for `string` and `List[string]` inputs. (#10960, @B-Step62)
- Bug fixes for Langchain flavor
  - Fixed errors that occur when logging inputs and outputs with different lengths (#10952, @serena-ruan)

Documentation updates:

- [Docs] Add indications of DL UI capabilities to the DL landing page (#10991, @BenWilson2)
- [Docs] Fix incorrect logo on LLMs landing page (#11017, @BenWilson2)

Small bug fixes and documentation updates:

#10930, #11005, @serena-ruan; #10927, @harupy

## 2.10.0 (2024-01-26)

MLflow 2.10.0 includes several major features and improvements

In MLflow 2.10, we're introducing a number of significant new features that are preparing the way for current and future enhanced support for Deep Learning use cases, new features to support a broadened support for GenAI applications, and some quality of life improvements for the MLflow Deployments Server (formerly the AI Gateway).

Our biggest features this release are:

- We have a new [home](https://mlflow.org). The new site landing page is fresh, modern, and contains more content than ever. We're adding new content and blogs all of the time.

- Objects and Arrays are now available as configurable input and output schema elements. These new types are particularly useful for GenAI-focused flavors that can have complex input and output types. See the new [Signature and Input Example documentation](https://mlflow.org/docs/latest/model/signatures.html) to learn more about how to use these new signature types.

- LangChain has autologging support now! When you invoke a chain, with autologging enabled, we will automatically log most chain implementations, recording and storing your configured LLM application for you. See the new [Langchain documentation](https://mlflow.org/docs/latest/llms/langchain/index.html#mlflow-langchain-autologging) to learn more about how to use this feature.

- The MLflow `transformers` flavor now supports prompt templates. You can now specify an application-specific set of instructions to submit to your GenAI pipeline in order to simplify, streamline, and integrate sets of system prompts to be supplied with each input request. Check out the updated [guide to transformers](https://www.mlflow.org/docs/latest/llms/transformers/index.html) to learn more and see examples!

- The [MLflow Deployments Server](https://mlflow.org/docs/latest/llms/deployments/index.html) now supports two new requested features: (1) OpenAI endpoints that support streaming responses. You can now configure an endpoint to return realtime responses for Chat and Completions instead of waiting for the entire text contents to be completed. (2) Rate limits can now be set per endpoint in order to help control cost overrun when using SaaS models.

- Continued the push for enhanced documentation, guides, tutorials, and examples by expanding on core MLflow functionality ([Deployments](https://mlflow.org/docs/latest/deployment/index.html), [Signatures](https://mlflow.org/docs/latest/model/signatures.html), and [Model Dependency management](https://mlflow.org/docs/latest/model/dependencies.html)), as well as entirely new pages for GenAI flavors. Check them out today!

Features:

- [Models] Introduce `Objects` and `Arrays` support for model signatures (#9936, @serena-ruan)
- [Models] Support saving prompt templates for transformers (#10791, @daniellok-db)
- [Models] Enhance the MLflow Models `predict` API to serve as a pre-logging validator of environment compatibility. (#10759, @B-Step62)
- [Models] Add support for Image Classification pipelines within the transformers flavor (#10538, @KonakanchiSwathi)
- [Models] Add support for retrieving and storing license files for transformers models (#10871, @BenWilson2)
- [Models] Add support for model serialization in the Visual NLP format for JohnSnowLabs flavor (#10603, @C-K-Loan)
- [Models] Automatically convert OpenAI input messages to LangChain chat messages for `pyfunc` predict (#10758, @dbczumar)
- [Tracking] Add support for Langchain autologging (#10801, @serena-ruan)
- [Tracking] Enhance async logging functionality by ensuring flush is called on `Futures` objects (#10715, @chenmoneygithub)
- [Tracking] Add support for a non-interactive mode for the `login()` API (#10623, @henxing)
- [Scoring] Allow MLflow model serving to support direct `dict` inputs with the `messages` key (#10742, @daniellok-db, @B-Step62)
- [Deployments] Add streaming support to the MLflow Deployments Server for OpenAI streaming return compatible routes (#10765, @gabrielfu)
- [Deployments] Add the ability to set rate limits on configured endpoints within the MLflow deployments server API (#10779, @TomeHirata)
- [Deployments] Add support for directly interfacing with OpenAI via the MLflow Deployments server (#10473, @prithvikannan)
- [UI] Introduce a number of new features for the MLflow UI (#10864, @daniellok-db)
- [Server-infra] Add an environment variable that can disallow HTTP redirects (#10655, @daniellok-db)
- [Artifacts] Add support for Multipart Upload for Azure Blob Storage (#10531, @gabrielfu)

Bug fixes:

- [Models] Add deduplication logic for pip requirements and extras handling for MLflow models (#10778, @BenWilson2)
- [Models] Add support for paddle 2.6.0 release (#10757, @WeichenXu123)
- [Tracking] Fix an issue with an incorrect retry default timeout for urllib3 1.x (#10839, @BenWilson2)
- [Recipes] Fix an issue with MLflow Recipes card display format (#10893, @WeichenXu123)
- [Java] Fix an issue with metadata collection when using Streaming Sources on certain versions of Spark where Delta is the source (#10729, @daniellok-db)
- [Scoring] Fix an issue where SageMaker tags were not propagating correctly (#9310, @clarkh-ncino)
- [Windows / Databricks] Fix an issue with executing Databricks run commands from within a Window environment (#10811, @wolpl)
- [Models / Databricks] Disable `mlflowdbfs` mounts for JohnSnowLabs flavor due to flakiness (#9872, @C-K-Loan)

Documentation updates:

- [Docs] Fixed the `KeyError: 'loss'` bug for the Quickstart guideline (#10886, @yanmxa)
- [Docs] Relocate and supplement Model Signature and Input Example docs (#10838, @BenWilson2)
- [Docs] Add the HuggingFace Model Evaluation Notebook to the website (#10789, @BenWilson2)
- [Docs] Rewrite the search run documentation (#10863, @chenmoneygithub)
- [Docs] Create documentation for transformers prompt templates (#10836, @daniellok-db)
- [Docs] Refactoring of the Getting Started page (#10798, @BenWilson2)
- [Docs] Add a guide for model dependency management (#10807, @B-Step62)
- [Docs] Add tutorials and guides for LangChain (#10770, @BenWilson2)
- [Docs] Refactor portions of the Deep Learning documentation landing page (#10736, @chenmoneygithub)
- [Docs] Refactor and overhaul the Deployment documentation and add new tutorials (#10726, @B-Step62)
- [Docs] Add a PyTorch landing page, quick start, and guide (#10687, #10737 @chenmoneygithub)
- [Docs] Add additional tutorials to OpenAI flavor docs (#10700, @BenWilson2)
- [Docs] Enhance the guides on quickly getting started with MLflow by demonstrating how to use Databricks Community Edition (#10663, @BenWilson2)
- [Docs] Create the OpenAI Flavor landing page and intro notebooks (#10622, @BenWilson2)
- [Docs] Refactor the Tensorflow flavor API docs (#10662, @chenmoneygithub)

Small bug fixes and documentation updates:

#10538, #10901, #10903, #10876, #10833, #10859, #10867, #10843, #10857, #10834, #10814, #10805, #10764, #10771, #10733, #10724, #10703, #10710, #10696, #10691, #10692, @B-Step62; #10882, #10854, #10395, #10725, #10695, #10712, #10707, #10667, #10665, #10654, #10638, #10628, @harupy; #10881, #10875, #10835, #10845, #10844, #10651, #10806, #10786, #10785, #10781, #10741, #10772, #10727, @serena-ruan; #10873, #10755, #10750, #10749, #10619, @WeichenXu123; #10877, @amueller; #10852, @QuentinAmbard; #10822, #10858, @gabrielfu; #10862, @jerrylian-db; #10840, @ernestwong-db; #10841, #10795, #10792, #10774, #10776, #10672, @BenWilson2; #10827, #10826, #10825, #10732, #10481, @michael-berk; #10828, #10680, #10629, @daniellok-db; #10799, #10800, #10578, #10782, #10783, #10723, #10464, @annzhang-db; #10803, #10731, #10708, @kriscon-db; #10797, @dbczumar; #10756, #10751, @Ankit8848; #10784, @AveshCSingh; #10769, #10763, #10717, @chenmoneygithub; #10698, @rmalani-db; #10767, @liangz1; #10682, @cdreetz; #10659, @prithvikannan; #10639, #10609, @TomeHirata

## 2.9.2 (2023-12-14)

MLflow 2.9.2 is a patch release, containing several critical security fixes and configuration updates to support extremely large model artifacts.

Features:

- [Deployments] Add the `mlflow.deployments.openai` API to simplify direct access to OpenAI services through the deployments API (#10473, @prithvikannan)
- [Server-infra] Add a new environment variable that permits disabling http redirects within the Tracking Server for enhanced security in publicly accessible tracking server deployments (#10673, @daniellok-db)
- [Artifacts] Add environment variable configurations for both Multi-part upload and Multi-part download that permits modifying the per-chunk size to support extremely large model artifacts (#10648, @harupy)

Security fixes:

- [Server-infra] Disable the ability to inject malicious code via manipulated YAML files by forcing YAML rendering to be performed in a secure Sandboxed mode (#10676, @BenWilson2, #10640, @harupy)
- [Artifacts] Prevent path traversal attacks when querying artifact URI locations by disallowing `..` path traversal queries (#10653, @B-Step62)
- [Data] Prevent a mechanism for conducting a malicious file traversal attack on Windows when using tracking APIs that interface with `HTTPDatasetSource` (#10647, @BenWilson2)
- [Artifacts] Prevent a potential path traversal attack vector via encoded url traversal paths by decoding paths prior to evaluation (#10650, @B-Step62)
- [Artifacts] Prevent the ability to conduct path traversal attacks by enforcing the use of sanitized paths with the tracking server (#10666, @harupy)
- [Artifacts] Prevent path traversal attacks when using an FTP server as a backend store by enforcing base path declarations prior to accessing user-supplied paths (#10657, @harupy)

Documentation updates:

- [Docs] Add an end-to-end tutorial for RAG creation and evaluation (#10661, @AbeOmor)
- [Docs] Add Tensorflow landing page (#10646, @chenmoneygithub)
- [Deployments / Tracking] Add endpoints to LLM evaluation docs (#10660, @prithvikannan)
- [Examples] Add retriever evaluation tutorial for LangChain and improve the Question Generation tutorial notebook (#10419, @liangz1)

Small bug fixes and documentation updates:

#10677, #10636, @serena-ruan; #10652, #10649, #10641, @harupy; #10643, #10632, @BenWilson2

## 2.9.1 (2023-12-07)

MLflow 2.9.1 is a patch release, containing a critical bug fix related to loading `pyfunc` models that were saved in previous versions of MLflow.

Bug fixes:

- [Models] Revert Changes to PythonModel that introduced loading issues for models saved in earlier versions of MLflow (#10626, @BenWilson2)

Small bug fixes and documentation updates:

#10625, @BenWilson2

## 2.9.0 (2023-12-05)

MLflow 2.9.0 includes several major features and improvements.

MLflow AI Gateway deprecation (#10420, @harupy):

The feature previously known as MLflow AI Gateway has been moved to utilize [the MLflow deployments API](https://mlflow.org/docs/latest/llms/deployments/index.html).
For guidance on migrating from the AI Gateway to the new deployments API, please see the [MLflow AI Gateway Migration Guide](https://mlflow.org/docs/latest/llms/gateway/migration.html.

MLflow Tracking docs overhaul (#10471, @B-Step62):

[The MLflow tracking docs](https://mlflow.org/docs/latest/tracking.html) have been overhauled. We'd like your feedback on the new tracking docs!

Security fixes:

Three security patches have been filed with this release and CVE's have been issued with the details involved in the security patch and potential attack vectors. Please review and update your tracking server deployments if your tracking server is not securely deployed and has open access to the internet.

- Sanitize `path` in `HttpArtifactRepository.list_artifacts` (#10585, @harupy)
- Sanitize `filename` in `Content-Disposition` header for `HTTPDatasetSource` (#10584, @harupy).
- Validate `Content-Type` header to prevent POST XSS (#10526, @B-Step62)

Features:

- [Tracking] Use `backoff_jitter` when making HTTP requests (#10486, @ajinkyavbhandare)
- [Tracking] Add default `aggregate_results` if the score type is numeric in `make_metric` API (#10490, @sunishsheth2009)
- [Tracking] Add string type of score types for metric value for genai (#10307, @sunishsheth2009)
- [Artifacts] Support multipart upload for for proxy artifact access (#9521, @harupy)
- [Models] Support saving `torch_dtype` for transformers models (#10586, @serena-ruan)
- [Models] Add built-in metric `ndcg_at_k` to retriever evaluation (#10284, @liangz1)
- [Model Registry] Implement universal `copy_model_version` (#10308, @jerrylian-db)
- [Models] Support saving/loading `RunnableSequence`, `RunnableParallel`, and `RunnableBranch` (#10521, #10611, @serena-ruan)

Bug fixes:

- [Tracking] Resume system metrics logging when resuming an existing run (#10312, @chenmoneygithub)
- [UI] Fix incorrect sorting order in line chart (#10553, @B-Step62)
- [UI] Remove extra whitespace in git URLs (#10506, @mrplants)
- [Models] Make spark_udf use NFS to broadcast model to spark executor on databricks runtime and spark connect mode (#10463, @WeichenXu123)
- [Models] Fix promptlab pyfunc models not working for chat routes (#10346, @daniellok-db)

Documentation updates:

- [Docs] Add a quickstart guide for Tensorflow (#10398, @chenmoneygithub)
- [Docs] Improve the parameter tuning guide (#10344, @chenmoneygithub)
- [Docs] Add a guide for system metrics logging (#10429, @chenmoneygithub)
- [Docs] Add instructions on how to configure credentials for Azure OpenAI (#10560, @BenWilson2)
- [Docs] Add docs and tutorials for Sentence Transformers flavor (#10476, @BenWilson2)
- [Docs] Add tutorials, examples, and guides for Transformers Flavor (#10360, @BenWilson2)

Small bug fixes and documentation updates:

#10567, #10559, #10348, #10342, #10264, #10265, @B-Step62; #10595, #10401, #10418, #10394, @chenmoneygithub; #10557, @dan-licht; #10584, #10462, #10445, #10434, #10432, #10412, #10411, #10408, #10407, #10403, #10361, #10340, #10339, #10310, #10276, #10268, #10260, #10224, #10214, @harupy; #10415, @jessechancy; #10579, #10555, @annzhang-db; #10540, @wllgrnt; #10556, @smurching; #10546, @mbenoit29; #10534, @gabrielfu; #10532, #10485, #10444, #10433, #10375, #10343, #10192, @serena-ruan; #10480, #10416, #10173, @jerrylian-db; #10527, #10448, #10443, #10442, #10441, #10440, #10439, #10381, @prithvikannan; #10509, @keenranger; #10508, #10494, @WeichenXu123; #10489, #10266, #10210, #10103, @TomeHirata; #10495, #10435, #10185, @daniellok-db; #10319, @michael-berk; #10417, @bbqiu; #10379, #10372, #10282, @BenWilson2; #10297, @KonakanchiSwathi; #10226, #10223, #10221, @milinddethe15; #10222, @flooxo; #10590, @letian-w;

## 2.8.1 (2023-11-14)

MLflow 2.8.1 is a patch release, containing some critical bug fixes and an update to our continued work on reworking our docs.

Notable details:

- The API `mlflow.llm.log_predictions` is being marked as deprecated, as its functionality has been incorporated into `mlflow.log_table`. This API will be removed in the 2.9.0 release. (#10414, @dbczumar)

Bug fixes:

- [Artifacts] Fix a regression in 2.8.0 where downloading a single file from a registered model would fail (#10362, @BenWilson2)
- [Evaluate] Fix the `Azure OpenAI` integration for `mlflow.evaluate` when using LLM `judge` metrics (#10291, @prithvikannan)
- [Evaluate] Change `Examples` to optional for the `make_genai_metric` API (#10353, @prithvikannan)
- [Evaluate] Remove the `fastapi` dependency when using `mlflow.evaluate` for LLM results (#10354, @prithvikannan)
- [Evaluate] Fix syntax issues and improve the formatting for generated prompt templates (#10402, @annzhang-db)
- [Gateway] Fix the Gateway configuration validator pre-check for OpenAI to perform instance type validation (#10379, @BenWilson2)
- [Tracking] Fix an intermittent issue with hanging threads when using asynchronous logging (#10374, @chenmoneygithub)
- [Tracking] Add a timeout for the `mlflow.login()` API to catch invalid hostname configuration input errors (#10239, @chenmoneygithub)
- [Tracking] Add a `flush` operation at the conclusion of logging system metrics (#10320, @chenmoneygithub)
- [Models] Correct the prompt template generation logic within the Prompt Engineering UI so that the prompts can be used in the Python API (#10341, @daniellok-db)
- [Models] Fix an issue in the `SHAP` model explainability functionality within `mlflow.shap.log_explanation` so that duplicate or conflicting dependencies are not registered when logging (#10305, @BenWilson2)

Documentation updates:

- [Docs] Add MLflow Tracking Quickstart (#10285, @BenWilson2)
- [Docs] Add tracking server configuration guide (#10241, @chenmoneygithub)
- [Docs] Refactor and improve the model deployment quickstart guide (#10322, @prithvikannan)
- [Docs] Add documentation for system metrics logging (#10261, @chenmoneygithub)

Small bug fixes and documentation updates:

#10367, #10359, #10358, #10340, #10310, #10276, #10277, #10247, #10260, #10220, #10263, #10259, #10219, @harupy; #10313, #10303, #10213, #10272, #10282, #10283, #10231, #10256, #10242, #10237, #10238, #10233, #10229, #10211, #10231, #10256, #10242, #10238, #10237, #10229, #10233, #10211, @BenWilson2; #10375, @serena-ruan; #10330, @Haxatron; #10342, #10249, #10249, @B-Step62; #10355, #10301, #10286, #10257, #10236, #10270, #10236, @prithvikannan; #10321, #10258, @jerrylian-db; #10245, @jessechancy; #10278, @daniellok-db; #10244, @gabrielfu; #10226, @milinddethe15; #10390, @bbqiu; #10232, @sunishsheth2009

## 2.8.0 (2023-10-28)

MLflow 2.8.0 includes several notable new features and improvements

- The MLflow Evaluate API has had extensive feature development in this release to support LLM workflows and multiple new evaluation modalities. See the new documentation, guides, and tutorials for MLflow LLM Evaluate to learn more.
- The MLflow Docs modernization effort has started. You will see a very different look and feel to the docs when visiting them, along with a batch of new tutorials and guides. More changes will be coming soon to the docs!
- 4 new LLM providers have been added! Google PaLM 2, AWS Bedrock, AI21 Labs, and HuggingFace TGI can now be configured and used within the AI Gateway. Learn more in the new AI Gateway docs!

Features:

- [Gateway] Add support for AWS Bedrock as a provider in the AI Gateway (#9598, @andrew-christianson)
- [Gateway] Add support for Huggingface Text Generation Inference as a provider in the AI Gateway (#10072, @SDonkelaarGDD)
- [Gateway] Add support for Google PaLM 2 as a provider in the AI Gateway (#9797, @arpitjasa-db)
- [Gateway] Add support for AI21labs as a provider in the AI Gateway (#9828, #10168, @zhe-db)
- [Gateway] Introduce a simplified method for setting the configuration file location for the AI Gateway via environment variable (#9822, @danilopeixoto)
- [Evaluate] Introduce default provided LLM evaluation metrics for MLflow evaluate (#9913, @prithvikannan)
- [Evaluate] Add support for evaluating inference datasets in MLflow evaluate (#9830, @liangz1)
- [Evaluate] Add support for evaluating single argument functions in MLflow evaluate (#9718, @liangz1)
- [Evaluate] Add support for Retriever LLM model type evaluation within MLflow evaluate (#10079, @liangz1)
- [Models] Add configurable parameter for external model saving in the ONNX flavor to address a regression (#10152, @daniellok-db)
- [Models] Add support for saving inference parameters in a logged model's input example (#9655, @serena-ruan)
- [Models] Add support for `completions` in the OpenAI flavor (#9838, @santiagxf)
- [Models] Add support for inference parameters for the OpenAI flavor (#9909, @santiagxf)
- [Models] Introduce support for configuration arguments to be specified when loading a model (#9251, @santiagxf)
- [Models] Add support for integrated Azure AD authentication for the OpenAI flavor (#9704, @santiagxf)
- [Models / Scoring] Introduce support for model training lineage in model serving (#9402, @M4nouel)
- [Model Registry] Introduce the `copy_model_version` client API for copying model versions across registered models (#9946, #10078, #10140, @jerrylian-db)
- [Tracking] Expand the limits of parameter value length from 500 to 6000 (#9709, @serena-ruan)
- [Tracking] Introduce support for Spark 3.5's SparkConnect mode within MLflow to allow logging models created using this operation mode of Spark (#9534, @WeichenXu123)
- [Tracking] Add support for logging system metrics to the MLflow fluent API (#9557, #9712, #9714, @chenmoneygithub)
- [Tracking] Add callbacks within MLflow for Keras and Tensorflow (#9454, #9637, #9579, @chenmoneygithub)
- [Tracking] Introduce a fluent login API for Databricks within MLflow (#9665, #10180, @chenmoneygithub)
- [Tracking] Add support for customizing auth for http requests from the MLflow client via a plugin extension (#10049, @lu-ohai)
- [Tracking] Introduce experimental asynchronous logging support for metrics, params, and tags (#9705, @sagarsumant)
- [Auth] Modify the behavior of user creation in MLflow Authentication so that only admins can create new users (#9700, @gabrielfu)
- [Artifacts] Add support for using `xethub` as an artifact store via a plugin extension (#9957, @Kelton8Z)
- [UI] Add new opt-in Model Registry UI that supports model aliases and tags (#10163, @hubertzub-db, @jerrylian-db)

Bug fixes:

- [Evaluate] Fix a bug with Azure OpenAI configuration usage within MLflow evaluate (#9982, @sunishsheth2009)
- [Models] Fix a data consistency issue when saving models that have been loaded in heterogeneous memory configuration within the transformers flavor (#10087, @BenWilson2)
- [Models] Fix an issue in the transformers flavor for complex input types by adding dynamic dataframe typing (#9044, @wamartin-aml)
- [Models] Fix an issue in the langchain flavor to provide support for chains with multiple outputs (#9497, @bbqiu)
- [Docker] Fix an issue with Docker image generation by changing the default env-manager to virtualenv (#9938, @Beramos)
- [Auth] Fix an issue with complex passwords in MLflow Auth to support a richer character set range (#9760, @dotdothu)
- [R] Fix a bug with configuration access when running MLflow R in Databricks (#10117, @zacdav-db)

Documentation updates:

- [Docs] Introduce the first phase of a larger documentation overhaul (#10197, @BenWilson2)
- [Docs] Add guide for LLM eval (#10058, #10199, @chenmoneygithub)
- [Docs] Add instructions on how to force single file serialization within the onnx flavor's save and log functions (#10178, @BenWilson2)
- [Docs] Add documentation for the relevance metric for MLflow evaluate (#10170, @sunishsheth2009)
- [Docs] Add a style guide for the contributing guide for how to structure pydoc strings (#9907, @mberk06)
- [Docs] Fix issues with the pytorch lightning autolog code example (#9964, @chenmoneygithub)
- [Docs] Update the example for `mlflow.data.from_numpy()` (#9885, @chenmoneygithub)
- [Docs] Add clear instructions for installing MLflow within R (#9835, @darshan8850)
- [Docs] Update model registry documentation to add content regarding support for model aliases (#9721, @jerrylian-db)

Small bug fixes and documentation updates:

#10202, #10189, #10188, #10159, #10175, #10165, #10154, #10083, #10082, #10081, #10071, #10077, #10070, #10053, #10057, #10055, #10020, #9928, #9929, #9944, #9979, #9923, #9842, @annzhang-db; #10203, #10196, #10172, #10176, #10145, #10115, #10107, #10054, #10056, #10018, #9976, #9999, #9998, #9995, #9978, #9973, #9975, #9972, #9974, #9960, #9925, #9920, @prithvikannan; #10144, #10166, #10143, #10129, #10059, #10123, #9555, #9619, @bbqiu; #10187, #10191, #10181, #10179, #10151, #10148, #10126, #10119, #10099, #10100, #10097, #10089, #10096, #10091, #10085, #10068, #10065, #10064, #10060, #10023, #10030, #10028, #10022, #10007, #10006, #9988, #9961, #9963, #9954, #9953, #9937, #9932, #9931, #9910, #9901, #9852, #9851, #9848, #9847, #9841, #9844, #9825, #9820, #9806, #9802, #9800, #9799, #9790, #9787, #9791, #9788, #9785, #9786, #9784, #9754, #9768, #9770, #9753, #9697, #9749, #9747, #9748, #9751, #9750, #9729, #9745, #9735, #9728, #9725, #9716, #9694, #9681, #9666, #9643, #9641, #9621, #9607, @harupy; #10200, #10201, #10142, #10139, #10133, #10090, #10086, #9934, #9933, #9845, #9831, #9794, #9692, #9627, #9626, @chenmoneygithub; #10110, @wenfeiy-db; #10195, #9895, #9880, #9679, @BenWilson2; #10174, #10177, #10109, #9706, @jerrylian-db; #10113, #9765, @smurching; #10150, #10138, #10136, @dbczumar; #10153, #10032, #9986, #9874, #9727, #9707, @serena-ruan; #10155, @shaotong-db; #10160, #10131, #10048, #10024, #10017, #10016, #10002, #9966, #9924, @sunishsheth2009; #10121, #10116, #10114, #10102, #10098, @B-Step62; #10095, #10026, #9991, @daniellok-db; #10050, @Dennis40816; #10062, #9868, @Gekko0114; #10033, @Anushka-Bhowmick; #9983, #10004, #9958, #9926, #9690, @liangz1; #9997, #9940, #9922, #9919, #9890, #9888, #9889, #9810, @TomeHirata; #9994, #9970, #9950, @lightnessofbein; #9965, #9677, @ShorthillsAI; #9906, @jessechancy; #9942, #9771, @Sai-Suraj-27; #9902, @remyleone; #9892, #9865, #9866, #9853, @montanarograziano; #9875, @Raghavan-B; #9858, @Salz0; #9878, @maksboyarin; #9882, @lukasz-gawron; #9827, @Bncer; #9819, @gabrielfu; #9792, @harshk461; #9726, @Chiragasourabh; #9663, @Abhishek-TyRnT; #9670, @mberk06; #9755, @simonlsk; #9757, #9775, #9776, #9774, @AmirAflak; #9782, @garymm; #9756, @issamarabi; #9645, @shichengzhou-db; #9671, @zhe-db; #9660, @mingyu89; #9575, @akshaya-a; #9629, @pnacht; #9876, @C-K-Loan

## 2.7.1 (2023-09-17)

MLflow 2.7.1 is a patch release containing the following features, bug fixes and changes:

Features:

- [Gateway / Databricks] Add the `set_limits` and `get_limits` APIs for AI Gateway routes within Databricks (#9516, @zhe-db)
- [Artifacts / Databricks] Add support for parallelized download and upload of artifacts within Unity Catalog (#9498, @jerrylian-db)

Bug fixes:

- [Models / R] Fix a critical bug with the `R` client that prevents models from being loaded (#9624, @BenWilson2)
- [Artifacts / Databricks] Disable multi-part download functionality for UC Volumes local file destination when downloading models (#9631, @BenWilson2)

Small bug fixes and documentation updates:

#9640, @annzhang-db; #9622, @harupy

## 2.7.0 (2023-09-12)

MLflow 2.7.0 includes several major features and improvements

- [UI / Gateway] We are excited to announce the Prompt Engineering UI. This new addition offers a suite of tools tailored for efficient prompt development, testing, and evaluation for LLM use cases. Integrated directly into the MLflow AI Gateway, it provides a seamless experience for designing, tracking, and deploying prompt templates. To read about this new feature, see the documentation at https://mlflow.org/docs/latest/llms/prompt-engineering.html (#9503, @prithvikannan)

Features:

- [Gateway] Introduce `MosaicML` as a supported provider for the MLflow `AI Gateway` (#9459, @arpitjasa-db)
- [Models] Add support for using a snapshot download location when loading a `transformers` model as `pyfunc` (#9362, @serena-ruan)
- [Server-infra] Introduce plugin support for MLflow `Tracking Server` authentication (#9191, @barrywhart)
- [Artifacts / Model Registry] Add support for storing artifacts using the `R2` backend (#9490, @shichengzhou-db)
- [Artifacts] Improve upload and download performance for Azure-based artifact stores (#9444, @jerrylian-db)
- [Sagemaker] Add support for deploying models to Sagemaker Serverless inference endpoints (#9085, @dogeplusplus)

Bug fixes:

- [Gateway] Fix a credential expiration bug by re-resolving `AI Gateway` credentials before each request (#9518, @dbczumar)
- [Gateway] Fix a bug where `search_routes` would raise an exception when no routes have been defined on the `AI Gateway` server (#9387, @QuentinAmbard)
- [Gateway] Fix compatibility issues with `pydantic` 2.x for `AI gateway` (#9339, @harupy)
- [Gateway] Fix an initialization issue in the `AI Gateway` that could render MLflow nonfunctional at import if dependencies were conflicting. (#9337, @BenWilson2)
- [Artifacts] Fix a correctness issue when downloading large artifacts to `fuse mount` paths on `Databricks` (#9545, @BenWilson2)

Documentation updates:

- [Docs] Add documentation for the `Giskard` community plugin for `mlflow.evaluate` (#9183, @rabah-khalek)

Small bug fixes and documentation updates:

#9605, #9603, #9602, #9595, #9597, #9587, #9590, #9588, #9586, #9584, #9583, #9582, #9581, #9580, #9577, #9546, #9566, #9569, #9562, #9564, #9561, #9528, #9506, #9503, #9492, #9491, #9485, #9445, #9430, #9429, #9427, #9426, #9424, #9421, #9419, #9409, #9408, #9407, #9394, #9389, #9395, #9393, #9390, #9370, #9356, #9359, #9357, #9345, #9340, #9328, #9329, #9326, #9304, #9325, #9323, #9322, #9319, #9314, @harupy; #9568, #9520, @dbczumar; #9593, @jerrylian-db; #9574, #9573, #9480, #9332, #9335, @BenWilson2; #9556, @shichengzhou-db; #9570, #9540, #9533, #9517, #9354, #9453, #9338, @prithvikannan; #9565, #9560, #9536, #9504, #9476, #9481, #9450, #9466, #9418, #9397, @serena-ruan; #9489, @dnerini; #9512, #9479, #9355, #9351, #9289 @chenmoneygithub; #9488, @bbqiu; #9474, @apurva-koti; #9505, @arpitjasa-db; #9261, @donour; #9336, #9414, #9353, @mberk06; #9451, @Bncer; #9432, @barrywhart; #9347, @GraceBrigham; #9428, #9420, #9406, @WeichenXu123; #9410, @aloahPGF; #9396, #9384, #9372, @Godwin-T; #9373, @fabiansefranek; #9382, @Sai-Suraj-27; #9378, @saidattu2003; #9375, @Increshi; #9358, @smurching; #9366, #9330, @Dev-98; #9364, @Sandeep1005; #9349, #9348, @AmirAflak; #9308, @danilopeixoto; #9596, @ShorthillsAI; #9567, @Beramos; #9524, @rabah-khalek; #9312, @dependabot[bot]

## 2.6.0 (2023-08-15)

MLflow 2.6.0 includes several major features and improvements

Features:

- [Models / Scoring] Add support for passing extra params during inference for PyFunc models (#9068, @serena-ruan)
- [Gateway] Add support for MLflow serving to MLflow AI Gateway (#9199, @BenWilson2)
- [Tracking] Support `save_kwargs` for `mlflow.log_figure` to specify extra options when saving a figure (#9179, @stroblme)
- [Artifacts] Display progress bars when uploading/download artifacts (#9195, @serena-ruan)
- [Models] Add support for logging LangChain's retriever models (#8808, @liangz1)
- [Tracking] Add support to log customized tags to runs created by autologging (#9114, @thinkall)

Bug fixes:

- [Models] Fix `text_pair` functionality for transformers `TextClassification` pipelines (#9215, @BenWilson2)
- [Models] Fix LangChain compatibility with SQLDatabase (#9192, @dbczumar)
- [Tracking] Remove patching `sklearn.metrics.get_scorer_names` in `mlflow.sklearn.autolog` to avoid duplicate logging (#9095, @WeichenXu123)

Documentation updates:

- [Docs / Examples] Add examples and documentation for MLflow AI Gateway support for MLflow model serving (#9281, @BenWilson2)
- [Docs / Examples] Add `sentence-transformers` doc & example (#9047, @es94129)

Deprecation:

- [Models] The `mlflow.mleap` module has been marked as deprecated and will be removed in a future release (#9311, @BenWilson2)

Small bug fixes and documentation updates:

#9309, #9252, #9198, #9189, #9186, #9184, @BenWilson2; #9307, @AmirAflak; #9285, #9126, @dependabot[bot]; #9302, #9209, #9194, #9187, #9175, #9177, #9163, #9161, #9129, #9123, #9053, @serena-ruan; #9305, #9303, #9271, @KekmaTime; #9300, #9299, @itsajay1029; #9294, #9293, #9274, #9268, #9264, #9246, #9255, #9253, #9254, #9245, #9202, #9243, #9238, #9234, #9233, #9227, #9226, #9223, #9224, #9222, #9225, #9220, #9208, #9212, #9207, #9203, #9201, #9200, #9154, #9146, #9147, #9153, #9148, #9145, #9136, #9132, #9131, #9128, #9121, #9124, #9125, #9108, #9103, #9100, #9098, #9101, @harupy; #9292, @Aman123lug; #9290, #9164, #9157, #9086, @Bncer; #9291, @kunal642; #9284, @NavneetSinghArora; #9286, #9262, #9142, @smurching; #9267, @tungbq; #9258, #9250, @Kunj125; #9167, #9139, #9120, #9118, #9097, @viktoriussuwandi; #9244, #9240, #9239, @Sai-Suraj-27; #9221, #9168, #9130, @gabrielfu; #9218, @tjni; #9216, @Rukiyav; #9158, #9051, @EdAbati; #9211, @scarlettrobe; #9049, @annzhang-db; #9140, @kriscon-db; #9141, @xAIdrian; #9135, @liangz1; #9067, @jmmonteiro; #9112, @WeichenXu123; #9106, @shaikmoeed; #9105, @Ankit8848; #9104, @arnabrahman

## 2.5.0 (2023-07-17)

MLflow 2.5.0 includes several major features and improvements:

- [MLflow AI Gateway] We are excited to announce the release of MLflow AI Gateway, a powerful tool designed to streamline the usage and management of various large language model (LLM) providers, such as OpenAI and Anthropic, within an organization. It offers a standardized interface that simplifies the interaction with these services and delivers centralized, secure management of credentials. To get started with MLflow AI Gateway, check out the docs at https://mlflow.org/docs/latest/gateway/index.html. (#8694, @harupy, @BenWilson2, @dbczumar)
- [Auth]: We are excited to announce the release of authentication and authorization support for MLflow Tracking and the MLflow Model Registry, providing integrated access control capabilities to both services. To get started, check out the docs at https://mlflow.org/docs/latest/auth/index.html. (#9000, #8975, #8626, #8837, #8841, @gabrielfu, @harupy)

Features:

- [Models] Add Support to the LangChain flavor for chains that contain unserializable components (#8736, @liangz1)
- [Scoring] Infer spark udf return type from model output schema (#8934, @WeichenXu123)
- [Models] Add support for automated signature inference (#8860, #8782 #8795, #8725, @jerrylian-db)

Bug fixes:

- [Security] Improve robustness to LFI attacks on Windows by enhancing path validation (#8999, @serena-ruan)
  - If you are using `mlflow server` or `mlflow ui` on Windows, we recommend upgrading to MLflow 2.5.0 as soon as possible.
- [Scoring] Support nullable array type values as spark_udf return values (#9014, @WeichenXu123)
- [Models] Revert cache deletion of system modules when adding custom model code to the system path (#8722, @trungn1)
- [Models] add micro version to mlflow version pinning (#8687, @C-K-Loan)
- [Artifacts] Prevent manually deleted artifacts from causing artifact garbage collection to fail (#8498, @PenHsuanWang)

Documentation updates:

- [Docs] Update .push_model_to_sagemaker docs (#8851, @pdifranc)
- [Docs] Fix invalid link for Azure ML documentation (#8800, @dunnkers)
- [Artifacts / Docs / Models / Projects] Adds information on the OCI MLflow plugins for seamless integration with Oralce Cloud Infrastructure services. (#8707, @mrDzurb)

Deprecation:

- [Models] Deprecate the `gluon` model flavor. The `mlflow.gluon` module will be removed in a future release. (#8968, @harupy)

Small bug fixes and documentation updates:

#9069, #9056, #9055, #9054, #9048, #9043, #9035, #9034, #9037, #9038, #8993, #8966, #8985, @BenWilson2; #9039, #9036, #8902, #8924, #8866, #8861, #8810, #8761, #8544, @jerrylian-db; #8903, @smurching; #9080, #9079, #9078, #9076, #9075, #9074, #9071, #9063, #9062, #9032, #9031, #9027, #9023, #9022, #9020, #9005, #8994, #8979, #8983, #8984, #8982, #8970, #8962, #8969, #8968, #8959, #8960, #8958, #8956, #8955, #8954, #8949, #8950, #8952, #8948, #8946, #8947, #8943, #8944, #8916, #8917, #8933, #8929, #8932, #8927, #8930, #8925, #8921, #8873, #8915, #8909, #8908, #8911, #8910, #8907, #8906, #8898, #8893, #8889, #8892, #8891, #8887, #8875, #8876, #8882, #8874, #8868, #8872, #8869, #8828, #8852, #8857, #8853, #8854, #8848, #8850, #8840, #8835, #8832, #8831, #8830, #8829, #8839, #8833, #8838, #8819, #8814, #8825, #8818, #8787, #8775, #8749, #8766, #8756, #8753, #8751, #8748, #8744, #8731, #8717, #8730, #8691, #8720, #8723, #8719, #8688, #8721, #8715, #8716, #8718, #8696, #8698, #8692, #8693, #8690, @harupy; #9030, @AlimurtuzaCodes; #9029, #9025, #9021, #9013, @viktoriussuwandi; #9010, @Bncer; #9011, @Pecunia201; #9007, #9003, @EdAbati; #9002, @prithvikannan; #8991, #8867, @AveshCSingh; #8951, #8896, #8888, #8849, @gabrielfu; #8913, #8885, #8871, #8870, #8788, #8772, #8771, @serena-ruan; #8879, @maciejskorski; #7752, @arunkumarkota; #9083, #9081, #8765, #8742, #8685, #8682, #8683, @dbczumar; #8791, @mhattingpete; #8739, @yunpark93

## 2.4.2 (2023-07-10)

MLflow 2.4.2 is a patch release containing the following bug fixes and changes:

Bug fixes:

- [Models] Add compatibility for legacy transformers serialization (#8964, @BenWilson2)
- [Models] Fix downloading MLmodel files from alias-based models:/ URIs (#8764, @smurching)
- [Models] Fix reading model flavor config from URI for models in UC (#8728, @smurching)
- [Models] Support `feature_deps` in ModelVersion creation for UC (#8867, #8815, @AveshCSingh)
- [Models] Add support for listing artifacts in UC model registry artifact repo (#8803, @smurching)
- [Core] Include resources for recipes in mlflow-skinny (#8895, @harupy)
- [UI] Enable datasets tracking UI (#8886, @harupy)
- [Artifacts] Use `MLFLOW_ENABLE_MULTIPART_DOWNLOAD` in `DatabricksArtifactRepository` (#8884, @harupy)

Documentation updates:

- [Examples / Docs] Add question-answering and summarization examples and docs with LLMs (#8695, @dbczumar)
- [Examples / Docs] Add johnsnowlabs flavor example and doc (#8689, @C-K-Loan)

Small bug fixes and documentation updates:

#8966, @BenWilson2; #8881, @harupy; #8846, #8760, @smurching

## 2.4.1 (2023-06-09)

MLflow 2.4.1 is a patch release containing the following features, bug fixes and changes:

Features:

- [Tracking] Extend SearchRuns to support datasets (#8622, @prithvikannan)
- [Models] Add an `mlflow.johnsnowlabs` flavor for the `johnsnowlabs` package (#8556, @C-K-Loan)
- [Models] Add a warning for duplicate pip requirements specified in `save_model` and `log_model` for the `transformers` flavor (#8678, @BenWilson2)

Bug fixes:

- [Security] Improve robustness to LFI attacks (#8648, @serena-ruan)
  - If you are using `mlflow server` or `mlflow ui`, we recommend upgrading to MLflow 2.4.1 as soon as possible.
- [Models] Fix an issue with `transformers` serialization for ModelCards that contain invalid characters (#8652, @BenWilson2)
- [Models] Fix connection pooling deadlocks that occurred during large file downloads (#8682, @dbczumar; #8660, @harupy)

Small bug fixes and documentation updates:

#8677, #8674, #8646, #8647, @dbczumar; #8654, #8653, #8660, #8650, #8642, #8636, #8599, #8637, #8608, #8633, #8623, #8628, #8619, @harupy; #8655, #8609, @BenWilson2; #8648, @serena-ruan; #8521, @ka1mar; #8638, @smurching; #8634, @PenHsuanWang

## 2.4.0 (2023-06-06)

MLflow 2.4.0 includes several major features and improvements

Features:

- [Tracking] Introduce dataset tracking APIs: `mlflow.data` and `mlflow.log_input()` (#8186, @prithvikannan)
- [Tracking] Add `mlflow.log_table()` and `mlflow.load_table()` APIs for logging evaluation tables (#8523, #8467, @sunishsheth2009)
- [Tracking] Introduce `mlflow.get_parent_run()` fluent API (#8493, @annzhang-db)
- [Tracking / Model Registry] Re-introduce faster artifact downloads on Databricks (#8352, @dbczumar; #8561, @harupy)
- [UI] Add dataset tracking information to MLflow Tracking UI (#8602, @prithvikannan, @hubertzub-db)
- [UI] Introduce Artifact View for comparing inputs, outputs, and metadata across models (#8602, @hubertzub-db)
- [Models] Extend `mlflow.evaluate()` to support LLM tasks (#8484, @harupy)
- [Models] Support logging subclasses of `Chain` and `LLMChain` in `mlflow.langchain` flavor (#8453, @liangz1)
- [Models] Add support for LangChain Agents to the `mlflow.langchain` flavor (#8297, @sunishsheth2009)
- [Models] Add a `mlflow.sentence_transformers` flavor for SentenceTransformers (#8479, @BenWilson2; #8547, @Loquats)
- [Models] Add support for multi-GPU inference and efficient weight loading for `mlflow.transformers` flavor (#8448, @ankit-db)
- [Models] Support the `max_shard_size` parameter in the `mlflow.transformers` flavor (#8567, @wenfeiy-db)
- [Models] Add support for audio transcription pipelines in the `mlflow.transformers` flavor (#8464, @BenWilson2)
- [Models] Add support for audio classification to `mlflow.transformers` flavor (#8492, @BenWilson2)
- [Models] Add support for URI inputs in audio models logged with the `mlflow.transformers` flavor (#8495, @BenWilson2)
- [Models] Add support for returning classifier scores in `mlflow.transformers` pyfunc outputs (#8512, @BenWilson2)
- [Models] Support optional inputs in model signatures (#8438, @apurva-koti)
- [Models] Introduce an `mlflow.models.set_signature()` API to set the signature of a logged model (#8476, @jerrylian-db)
- [Models] Persist ONNX Runtime InferenceSession options when logging a model with `mlflow.onnx.log_model()` (#8433, @leqiao-1)

Bug fixes:

- [Tracking] Terminate Spark callback server when Spark Autologging is disabled or Spark Session is shut down (#8508, @WeichenXu123)
- [Tracking] Fix compatibility of `mlflow server` with `Flask<2.0` (#8463, @kevingreer)
- [Models] Convert `mlflow.transformers` pyfunc scalar string output to list of strings during batch inference (#8546, @BenWilson2)
- [Models] Fix a bug causing outdated pyenv versions to be installed by `mlflow models build-docker` (#8488, @Hellzed)
- [Model Registry] Remove aliases from storage when a Model Version is deleted (#8459, @arpitjasa-db)

Documentation updates:

- [Docs] Publish a new MLOps Quickstart for model selection and deployment (#8462, @lobrien)
- [Docs] Add MLflavors library to Community Model Flavors documentation (#8420, @benjaminbluhm)
- [Docs] Add documentation for Registered Model Aliases (#8445, @arpitjasa-db)
- [Docs] Fix errors in documented `mlflow models` CLI command examples (#8480, @vijethmoudgalya)

Small bug fixes and documentation updates:

#8611, #8587, @dbczumar; #8617, #8620, #8615, #8603, #8604, #8601, #8596, #8598, #8597, #8589, #8580, #8581, #8575, #8582, #8577, #8576, #8578, #8561, #8568, #8551, #8528, #8550, #8489, #8530, #8534, #8533, #8532, #8524, #8520, #8517, #8516, #8515, #8514, #8506, #8503, #8500, #8504, #8496, #8486, #8485, #8468, #8471, #8473, #8470, #8458, #8447, #8446, #8434, @harupy; #8607, #8538, #8513, #8452, #8466, #8465, @serena-ruan; #8586, #8595, @prithvikannan; #8593, #8541, @kriscon-db; #8592, #8566, @annzhang-db; #8588, #8565, #8559, #8537, @BenWilson2; #8545, @apurva-koti; #8564, @DavidSpek; #8436, #8490, @jerrylian-db; #8505, @eliaskoromilas; #8483, @WeichenXu123; #8472, @leqiao-1; #8429, @jinzhang21; #8581, #8548, #8499, @gabrielfu;

## 2.3.2 (2023-05-12)

MLflow 2.3.2 is a patch release containing the following features, bug fixes and changes:

Features:

- [Models] Add GPU support for `transformers` models `pyfunc` inference and serving (#8375, @ankit-db)
- [Models] Disable autologging functionality for non-relevant models when training a `transformers` model (#8405, @BenWilson2)
- [Models] Add support for preserving and overriding `torch_dtype` values in `transformers` pipelines (#8421, @BenWilson2)
- [Models] Add support for `Feature Extraction` pipelines in the `transformers` flavor (#8423, @BenWilson2)
- [Tracking] Add basic HTTP auth support for users, registered models, and experiments permissions (#8286, @gabrielfu)

Bug Fixes:

- [Models] Fix inferred schema issue with `Text2TextGeneration` pipelines in the `transformers` flavor (#8391, @BenWilson2)
- [Models] Change MLflow dependency pinning in logged models from a range value to an exact major and minor version (#8422, @harupy)

Documentation updates:

- [Examples] Add `signature` logging to all examples and documentation (#8410, #8401, #8400, #8387 @jerrylian-db)
- [Examples] Add `sentence-transformers` examples to the `transformers` examples suite (#8425, @BenWilson2)
- [Docs] Add a new MLflow Quickstart documentation page (#8171, @lobrien)
- [Docs] Add a new introduction to MLflow page (#8365, @lobrien)
- [Docs] Add a community model plugin example and documentation for `trubrics` (#8371, @jeffkayne)
- [Docs] Add `gluon` pyfunc example to Model flavor documentation (#8403, @ericvincent18)
- [Docs] Add `statsmodels` pyfunc example to `Models` flavor documentation (#8394, @ericvincent18)

Small bug fixes and documentation updates:

#8415, #8412, #8411, #8355, #8354, #8353, #8348, @harupy; #8374, #8367, #8350, @dbczumar; #8358 @mrkaye97; #8392, #8362, @smurching; #8427, #8408, #8399, #8381, @BenWilson2; #8395, #8390, @jerrylian-db; #8402, #8398, @WeichenXu123; #8377, #8363, @arpitjasa-db; #8385, @prithvikannan; #8418, @Jeukoh;

## 2.3.1 (2023-04-27)

MLflow 2.3.1 is a patch release containing the following bug fixes and changes:

Bug fixes:

- [Security] Fix critical LFI attack vulnerability by disabling the ability to provide relative paths in registered model sources (#8281, @BenWilson2)
  - **If you are using `mlflow server` or `mlflow ui`, we recommend upgrading to MLflow 2.3.1 as soon as possible.** For more details, see https://github.com/mlflow/mlflow/security/advisories/GHSA-xg73-94fp-g449.
- [Tracking] Fix an issue causing file and model uploads to hang on Databricks (#8348, @harupy)
- [Tracking / Model Registry] Fix an issue causing file and model downloads to hang on Databricks (#8350, @dbczumar)
- [Scoring] Fix regression in schema enforcement for model serving when using the `inputs` format for inference (#8326, @BenWilson2)
- [Model Registry] Fix regression in model naming parsing where special characters were not accepted in model names (#8322, @arpitjasa-db)
- [Recipes] Fix card rendering with the pandas profiler to handle columns containing all null values (#8263, @sunishsheth2009)

Documentation updates:

- [Docs] Add an H2O pyfunc usage example to the models documentation (#8292, @ericvincent18)
- [Examples] Add a TensorFlow Core 2.x API usage example (#8235, @dheerajnbhat)

Small bug fixes and documentation updates:

#8324, #8325, @smurching; #8313, @dipanjank; #8323, @liangz1; #8331, #8328, #8319, #8316, #8308, #8293, #8289, #8283, #8284, #8285, #8282, #8241, #8270, #8272, #8271, #8268, @harupy; #8312, #8294, #8295, #8279, #8267, @BenWilson2; #8290, @jinzhang21; #8257, @WeichenXu123; #8307, @arpitjasa-db

## 2.3.0 (2023-04-18)

MLflow 2.3.0 includes several major features and improvements

Features:

- [Models] Introduce a new `transformers` named flavor (#8236, #8181, #8086, @BenWilson2)
- [Models] Introduce a new `openai` named flavor (#8191, #8155, @harupy)
- [Models] Introduce a new `langchain` named flavor (#8251, #8197, @liangz1, @sunishsheth2009)
- [Models] Add support for `Pytorch` and `Lightning` 2.0 (#8072, @shrinath-suresh)
- [Tracking] Add support for logging LLM input, output, and prompt artifacts (#8234, #8204, @sunishsheth2009)
- [Tracking] Add support for HTTP Basic Auth in the MLflow tracking server (#8130, @gabrielfu)
- [Tracking] Add `search_model_versions` to the fluent API (#8223, @mariusschlegel)
- [Artifacts] Add support for parallelized artifact downloads (#8116, @apurva-koti)
- [Artifacts] Add support for parallelized artifact uploads for AWS (#8003, @harupy)
- [Artifacts] Add content type headers to artifact upload requests for the `HttpArtifactRepository` (#8048, @WillEngler)
- [Model Registry] Add alias support for logged models within Model Registry (#8164, #8094, #8055 @arpitjasa-db)
- [UI] Add support for custom domain git providers (#7933, @gusghrlrl101)
- [Scoring] Add plugin support for customization of MLflow serving endpoints (#7757, @jmahlik)
- [Scoring] Add support to MLflow serving that allows configuration of multiple inference workers (#8035, @M4nouel)
- [Sagemaker] Add support for asynchronous inference configuration on Sagemaker (#8009, @thomasbell1985)
- [Build] Remove `shap` as a core dependency of MLflow (#8199, @jmahlik)

Bug fixes:

- [Models] Fix a bug with `tensorflow` autologging for models with multiple inputs (#8097, @jaume-ferrarons)
- [Recipes] Fix a bug with `Pandas` 2.0 updates for profiler rendering of datetime types (#7925, @sunishsheth2009)
- [Tracking] Prevent exceptions from being raised if a parameter is logged with an existing key whose value is identical to the logged parameter (#8038, @AdamStelmaszczyk)
- [Tracking] Fix an issue with deleting experiments in the FileStore backend (#8178, @mariusschlegel)
- [Tracking] Fix a UI bug where the "Source Run" field in the Model Version page points to an incorrect set of artifacts (#8156, @WeichenXu123)
- [Tracking] Fix a bug wherein renaming a run reverts its current lifecycle status to `UNFINISHED` (#8154, @WeichenXu123)
- [Tracking] Fix a bug where a file URI could be used as a model version source (#8126, @harupy)
- [Projects] Fix an issue with MLflow projects that have submodules contained within a project (#8050, @kota-iizuka)
- [Examples] Fix `lightning` hyperparameter tuning examples (#8039, @BenWilson2)
- [Server-infra] Fix bug with Cache-Control headers for static server files (#8016, @jmahlik)

Documentation updates:

- [Examples] Add a new and thorough example for the creation of custom model flavors (#7867, @benjaminbluhm)

Small bug fixes and documentation updates:

#8262, #8252, #8250, #8228, #8221, #8203, #8134, #8040, #7994, #7934, @BenWilson2; #8258, #8255, #8253, #8248, #8247, #8245, #8243, #8246, #8244, #8242, #8240, #8229, #8198, #8192, #8112, #8165, #8158, #8152, #8148, #8144, #8143, #8120, #8107, #8105, #8102, #8088, #8089, #8096, #8075, #8073, #8076, #8063, #8064, #8033, #8024, #8023, #8021, #8015, #8005, #7982, #8002, #7987, #7981, #7968, #7931, #7930, #7929, #7917, #7918, #7916, #7914, #7913, @harupy; #7955, @arjundc-db; #8219, #8110, #8093, #8087, #8091, #8092, #8029, #8028, #8031, @jerrylian-db; #8187, @apurva-koti; #8210, #8001, #8000, @arpitjasa-db; #8161, #8127, #8095, #8090, #8068, #8043, #7940, #7924, #7923, @dbczumar; #8147, @morelen17; #8106, @WeichenXu123; #8117, @eltociear; #8100, @laerciop; #8080, @elado; #8070, @grofte; #8066, @yukimori; #8027, #7998, @liangz1; #7999, @martlaf; #7964, @viditjain99; #7928, @alekseyolg; #7909, #7901, #7844, @smurching; #7971, @n30111; #8012, @mingyu89; #8137, @lobrien; #7992, @robmarkcole; #8263, @sunishsheth2009

## 2.2.2 (2023-03-14)

MLflow 2.2.2 is a patch release containing the following bug fixes:

- [Model Registry] Allow `source` to be a local path within a run's artifact directory if a `run_id` is specified (#7993, @harupy)
- [Model Registry] Fix a bug where a windows UNC path is considered a local path (#7988, @WeichenXu123)
- [Model Registry] Disallow `name` to be a file path in `FileStore.get_registered_model` (#7965, @harupy)

## 2.2.1 (2023-03-02)

MLflow 2.2.1 is a patch release containing the following bug fixes:

- [Model Registry] Fix a bug that caused too many results to be requested by default when calling `MlflowClient.search_model_versions()` (#7935, @dbczumar)
- [Model Registry] Patch for GHSA-xg73-94fp-g449 (#7908, @harupy)
- [Model Registry] Patch for GHSA-wp72-7hj9-5265 (#7965, @harupy)

## 2.2.0 (2023-02-28)

MLflow 2.2.0 includes several major features and improvements

Features:

- [Recipes] Add support for score calibration to the classification recipe (#7744, @sunishsheth2009)
- [Recipes] Add automatic label encoding to the classification recipe (#7711, @sunishsheth2009)
- [Recipes] Support custom data splitting logic in the classification and regression recipes (#7815, #7588, @sunishsheth2009)
- [Recipes] Introduce customizable MLflow Run name prefixes to the classification and regression recipes (#7746, @kamalesh0406; #7763, @sunishsheth2009)
- [UI] Add a new Chart View to the MLflow Experiment Page for model performance insights (#7864, @hubertzub-db, @apurva-koti, @prithvikannan, @ridhimag11, @sunishseth2009, @dbczumar)
- [UI] Modernize and improve parallel coordinates chart for model tuning (#7864, @hubertzub-db, @apurva-koti, @prithvikannan, @ridhimag11, @sunishseth2009, @dbczumar)
- [UI] Add typeahead suggestions to the MLflow Experiment Page search bar (#7864, @hubertzub-db, @apurva-koti, @prithvikannan, @ridhimag11, @sunishseth2009, @dbczumar)
- [UI] Improve performance of Experiments Sidebar for large numbers of experiments (#7804, @jmahlik)
- [Tracking] Introduce autologging support for native PyTorch models (#7627, @temporaer)
- [Tracking] Allow specifying `model_format` when autologging XGBoost models (#7781, @guyrosin)
- [Tracking] Add `MLFLOW_ARTIFACT_UPLOAD_DOWNLOAD_TIMEOUT` environment variable to configure artifact operation timeouts (#7783, @wamartin-aml)
- [Artifacts] Include `Content-Type` response headers for artifacts downloaded from `mlflow server` (#7827, @bali0019)
- [Model Registry] Introduce the `searchModelVersions()` API to the Java client (#7880, @gabrielfu)
- [Model Registry] Introduce `max_results`, `order_by` and `page_token` arguments to `MlflowClient.search_model_versions()` (#7623, @serena-ruan)
- [Models] Support logging large ONNX models by using external data (#7808, @dogeplusplus)
- [Models] Add support for logging Diviner models fit in Spark (#7800, @BenWilson2)
- [Models] Introduce `MLFLOW_DEFAULT_PREDICTION_DEVICE` environment variable to set the device for pyfunc model inference (#7922, @ankit-db)
- [Scoring] Publish official Docker images for the MLflow Model scoring server at github.com/mlflow/mlflow/pkgs (#7759, @dbczumar)

Bug fixes:

- [Recipes] Fix dataset format validation in the ingest step for custom dataset sources (#7638, @sunishsheth2009)
- [Recipes] Fix bug in identification of worst performing examples during training (#7658, @sunishsheth2009)
- [Recipes] Ensure consistent rendering of the recipe graph when `inspect()` is called (#7852, @sunishsheth2009)
- [Recipes] Correctly respect `positive_class` configuration in the transform step (#7626, @sunishsheth2009)
- [Recipes] Make logged metric names consistent with `mlflow.evaluate()` (#7613, @sunishsheth2009)
- [Recipes] Add `run_id` and `artifact_path` keys to logged MLmodel files (#7651, @sunishsheth2009)
- [UI] Fix bugs in UI validation of experiment names, model names, and tag keys (#7818, @subramaniam02)
- [Tracking] Resolve artifact locations to absolute paths when creating experiments (#7670, @bali0019)
- [Tracking] Exclude Delta checkpoints from Spark datasource autologging (#7902, @harupy)
- [Tracking] Consistently return an empty list from GetMetricHistory when a metric does not exist (#7589, @bali0019; #7659, @harupy)
- [Artifacts] Fix support for artifact operations on Windows paths in UNC format (#7750, @bali0019)
- [Artifacts] Fix bug in HDFS artifact listing (#7581, @pwnywiz)
- [Model Registry] Disallow creation of model versions with local filesystem sources in `mlflow server` (#7908, @harupy)
- [Model Registry] Fix handling of deleted model versions in FileStore (#7716, @harupy)
- [Model Registry] Correctly initialize Model Registry SQL tables independently of MLflow Tracking (#7704, @harupy)
- [Models] Correctly move PyTorch model outputs from GPUs to CPUs during inference with pyfunc (#7885, @ankit-db)
- [Build] Fix compatiblility issues with Python installations compiled using `PYTHONOPTIMIZE=2` (#7791, @dbczumar)
- [Build] Fix compatibility issues with the upcoming pandas 2.0 release (#7899, @harupy; #7910, @dbczumar)

Documentation updates:

- [Docs] Add an example of saving and loading Spark MLlib models with MLflow (#7706, @dipanjank)
- [Docs] Add usage examples for `mlflow.lightgbm` APIs (#7565, @canerturkseven)
- [Docs] Add an example of custom model flavor creation with `sktime` (#7624, @benjaminbluhm)
- [Docs] Clarify `precision_recall_auc` metric calculation in `mlflow.evaluate()` (#7701, @BenWilson2)
- [Docs] Remove outdated example links (#7587, @asloan7)

Small bug fixes and documentation updates:

#7866, #7751, #7724, #7699, #7697, #7666, @alekseyolg; #7896, #7861, #7858, #7862, #7872, #7859, #7863, #7767, #7766, #7765, #7741, @smurching; #7895, #7877, @viditjain99; #7898, @midhun1998; #7891, #7892, #7886, #7882, #7883, #7875, #7874, #7871, #7868, #7854, #7847, #7845, #7838, #7830, #7837, #7836, #7834, #7831, #7828, #7825, #7826, #7824, #7823, #7778, #7780, #7776, #7775, #7773, #7772, #7769, #7756, #7768, #7764, #7685, #7726, #7722, #7720, #7423, #7712, #7710, #7713, #7688, #7663, #7674, #7673, #7672, #7662, #7653, #7646, #7615, #7614, #7586, #7601, #7598, #7602, #7599, #7577, #7585, #7583, #7584, @harupy; #7865, #7803, #7753, #7719, @dipanjank; #7796, @serena-ruan; #7849, @turbotimon; #7822, #7600, @WeichenXu123; #7811, @guyrosin; #7812, #7788, #7787, #7748, #7730, #7616, #7593, @dbczumar; #7793, @Joel-hanson; #7792, #7694, #7643, @BenWilson2; #7771, #7657, #7644, @nsenno-dbr; #7738, @wkrt7; #7740, @Ark-kun; #7739, #7733, @bali0019; #7723, @andrehp; #7691, #7582, @agoyot; #7721, @Eseeldur; #7709, @srowen; #7693, @ry3s; #7649, @funkypenguin; #7665, @benjaminbluhm; #7668, @eltociear; #7550, @danielhstahl; #7920, @arjundc-db

## 2.1.0 (2022-12-21)

MLflow 2.1.0 includes several major features and improvements

Features:

- [Recipes] Introduce support for multi-class classification (#7458, @mshtelma)
- [Recipes] Extend the pyfunc representation of classification models to output scores in addition to labels (#7474, @sunishsheth2009)
- [UI] Add user ID and lifecycle stage quick search links to the Runs page (#7462, @jaeday)
- [Tracking] Paginate the GetMetricHistory API (#7523, #7415, @BenWilson2)
- [Tracking] Add Runs search aliases for Run name and start time that correspond to UI column names (#7492, @apurva-koti)
- [Tracking] Add a `/version` endpoint to `mlflow server` for querying the server's MLflow version (#7273, @joncarter1)
- [Model Registry] Add FileStore support for the Model Registry (#6605, @serena-ruan)
- [Model Registry] Introduce an `mlflow.search_registered_models()` fluent API (#7428, @TSienki)
- [Model Registry / Java] Add a `getRegisteredModel()` method to the Java client (#6602) (#7511, @drod331)
- [Model Registry / R] Add an `mlflow_set_model_version_tag()` method to the R client (#7401, @leeweijie)
- [Models] Introduce a `metadata` field to the MLmodel specification and `log_model()` methods (#7237, @jdonzallaz)
- [Models] Extend `Model.load()` to support loading MLmodel specifications from remote locations (#7517, @dbczumar)
- [Models] Pin the major version of MLflow in Models' `requirements.txt` and `conda.yaml` files (#7364, @BenWilson2)
- [Scoring] Extend `mlflow.pyfunc.spark_udf()` to support StructType results (#7527, @WeichenXu123)
- [Scoring] Extend TensorFlow and Keras Models to support multi-dimensional inputs with `mlflow.pyfunc.spark_udf()`(#7531, #7291, @WeichenXu123)
- [Scoring] Support specifying deployment environment variables and tags when deploying models to SageMaker (#7433, @jhallard)

Bug fixes:

- [Recipes] Fix a bug that prevented use of custom `early_stop` functions during model tuning (#7538, @sunishsheth2009)
- [Recipes] Fix a bug in the logic used to create a Spark session during data ingestion (#7307, @WeichenXu123)
- [Tracking] Make the metric names produced by `mlflow.autolog()` consistent with `mlflow.evaluate()` (#7418, @wenfeiy-db)
- [Tracking] Fix an autologging bug that caused nested, redundant information to be logged for XGBoost and LightGBM models (#7404, @WeichenXu123)
- [Tracking] Correctly classify SQLAlchemy OperationalErrors as retryable HTTP errors (#7240, @barrywhart)
- [Artifacts] Correctly handle special characters in credentials when using FTP artifact storage (#7479, @HCTsai)
- [Models] Address an issue that prevented MLeap models from being saved on Windows (#6966, @dbczumar)
- [Scoring] Fix a permissions issue encountered when using NFS during model scoring with `mlflow.pyfunc.spark_udf()` (#7427, @WeichenXu123)

Documentation updates:

- [Docs] Add more examples to the Runs search documentation page (#7487, @apurva-koti)
- [Docs] Add documentation for Model flavors developed by the community (#7425, @mmerce)
- [Docs] Add an example for logging and scoring ONNX Models (#7398, @Rusteam)
- [Docs] Fix a typo in the model scoring REST API example for inputs with the `dataframe_split` format (#7540, @zhouyangyu)
- [Docs] Fix a typo in the model scoring REST API example for inputs with the `dataframe_records` format (#7361, @dbczumar)

Small bug fixes and documentation updates:

#7571, #7543, #7529, #7435, #7399, @WeichenXu123; #7568, @xiaoye-hua; #7549, #7557, #7509, #7498, #7499, #7485, #7486, #7484, #7391, #7388, #7390, #7381, #7366, #7348, #7346, #7334, #7340, #7323, @BenWilson2; #7561, #7562, #7560, #7553, #7546, #7539, #7544, #7542, #7541, #7533, #7507, #7470, #7469, #7467, #7466, #7464, #7453, #7449, #7450, #7440, #7430, #7436, #7429, #7426, #7410, #7406, #7409, #7407, #7405, #7396, #7393, #7395, #7384, #7376, #7379, #7375, #7354, #7353, #7351, #7352, #7350, #7345, #6493, #7343, #7344, @harupy; #7494, @dependabot[bot]; #7526, @tobycheese; #7489, @liangz1; #7534, @Jingnan-Jia; #7496, @danielhstahl; #7504, #7503, #7459, #7454, #7447, @tsugumi-sys; #7461, @wkrt7; #7451, #7414, #7372, #7289, @sunishsheth2009; #7441, @ikrizanic; #7432, @Pochingto; #7386, @jhallard; #7370, #7373, #7371, #7336, #7341, #7342, @dbczumar; #7335, @prithvikannan

## 2.0.1 (2022-11-14)

The 2.0.1 version of MLflow is a major milestone release that focuses on simplifying the management of end-to-end MLOps workflows, providing new feature-rich functionality, and expanding upon the production-ready MLOps capabilities offered by MLflow.
This release contains several important breaking changes from the 1.x API, additional major features and improvements.

Features:

- [Recipes] MLflow Pipelines is now MLflow Recipes - a framework that enables data scientists to quickly develop high-quality models and deploy them to production
- [Recipes] Add support for classification models to MLflow Recipes (#7082, @bbarnes52)
- [UI] Introduce support for pinning runs within the experiments UI (#7177, @harupy)
- [UI] Simplify the layout and provide customized displays of metrics, parameters, and tags within the experiments UI (#7177, @harupy)
- [UI] Simplify run filtering and ordering of runs within the experiments UI (#7177, @harupy)
- [Tracking] Update `mlflow.pyfunc.get_model_dependencies()` to download all referenced requirements files for specified models (#6733, @harupy)
- [Tracking] Add support for selecting the Keras model `save_format` used by `mlflow.tensorflow.autolog()` (#7123, @balvisio)
- [Models] Set `mlflow.evaluate()` status to stable as it is now a production-ready API
- [Models] Simplify APIs for specifying custom metrics and custom artifacts during model evaluation with `mlflow.evaluate()` (#7142, @harupy)
- [Models] Correctly infer the positive label for binary classification within `mlflow.evaluate()` (#7149, @dbczumar)
- [Models] Enable automated signature logging for `tensorflow` and `keras` models when `mlflow.tensorflow.autolog()` is enabled (#6678, @BenWilson2)
- [Models] Add support for native Keras and Tensorflow Core models within `mlflow.tensorflow` (#6530, @WeichenXu123)
- [Models] Add support for defining the `model_format` used by `mlflow.xgboost.save/log_model()` (#7068, @AvikantSrivastava)
- [Scoring] Overhaul the model scoring REST API to introduce format indicators for inputs and support multiple output fields (#6575, @tomasatdatabricks; #7254, @adriangonz)
- [Scoring] Add support for ragged arrays in model signatures (#7135, @trangevi)
- [Java] Add `getModelVersion` API to the java client (#6955, @wgottschalk)

Breaking Changes:

The following list of breaking changes are arranged by their order of significance within each category.

- [Core] Support for Python 3.7 has been dropped. MLflow now requires Python >=3.8
- [Recipes] `mlflow.pipelines` APIs have been replaced with `mlflow.recipes`
- [Tracking / Registry] Remove `/preview` routes for Tracking and Model Registry REST APIs (#6667, @harupy)
- [Tracking] Remove deprecated `list` APIs for experiments, models, and runs from Python, Java, R, and REST APIs (#6785, #6786, #6787, #6788, #6800, #6868, @dbczumar)
- [Tracking] Remove deprecated `runs` response field from `Get Experiment` REST API response (#6541, #6524 @dbczumar)
- [Tracking] Remove deprecated `MlflowClient.download_artifacts` API (#6537, @WeichenXu123)
- [Tracking] Change the behavior of environment variable handling for `MLFLOW_EXPERIMENT_NAME` such that the value is always used when creating an experiment (#6674, @BenWilson2)
- [Tracking] Update `mlflow server` to run in `--serve-artifacts` mode by default (#6502, @harupy)
- [Tracking] Update Experiment ID generation for the Filestore backend to enable threadsafe concurrency (#7070, @BenWilson2)
- [Tracking] Remove `dataset_name` and `on_data_{name | hash}` suffixes from `mlflow.evaluate()` metric keys (#7042, @harupy)
- [Models / Scoring / Projects] Change default environment manager to `virtualenv` instead of `conda` for model inference and project execution (#6459, #6489 @harupy)
- [Models] Move Keras model logging APIs to the `mlflow.tensorflow` flavor and drop support for TensorFlow Estimators (#6530, @WeichenXu123)
- [Models] Remove deprecated `mlflow.sklearn.eval_and_log_metrics()` API in favor of `mlflow.evaluate()` API (#6520, @dbczumar)
- [Models] Require `mlflow.evaluate()` model inputs to be specified as URIs (#6670, @harupy)
- [Models] Drop support for returning custom metrics and artifacts from the same function when using `mlflow.evaluate()`, in favor of `custom_artifacts` (#7142, @harupy)
- [Models] Extend `PyFuncModel` spec to support `conda` and `virtualenv` subfields (#6684, @harupy)
- [Scoring] Remove support for defining input formats using the `Content-Type` header (#6575, @tomasatdatabricks; #7254, @adriangonz)
- [Scoring] Replace the `--no-conda` CLI option argument for native serving with `--env-manager='local'` (#6501, @harupy)
- [Scoring] Remove public APIs for `mlflow.sagemaker.deploy()` and `mlflow.sagemaker.delete()` in favor of MLflow deployments APIs, such as `mlflow deployments -t sagemaker` (#6650, @dbczumar)
- [Scoring] Rename input argument `df` to `inputs` in `mlflow.deployments.predict()` method (#6681, @BenWilson2)
- [Projects] Replace the `use_conda` argument with the `env_manager` argument within the `run` CLI command for MLflow Projects (#6654, @harupy)
- [Projects] Modify the MLflow Projects docker image build options by renaming `--skip-image-build` to `--build-image` with a default of `False` (#7011, @harupy)
- [Integrations/Azure] Remove deprecated `mlflow.azureml` modules from MLflow in favor of the `azure-mlflow` deployment plugin (#6691, @BenWilson2)
- [R] Remove conda integration with the R client (#6638, @harupy)

Bug fixes:

- [Recipes] Fix rendering issue with profile cards polyfill (#7154, @hubertzub-db)
- [Tracking] Set the MLflow Run name correctly when specified as part of the `tags` argument to `mlflow.start_run()` (#7228, @Cokral)
- [Tracking] Fix an issue with conflicting MLflow Run name assignment if the `mlflow.runName` tag is set (#7138, @harupy)
- [Scoring] Fix incorrect payload constructor error in SageMaker deployment client `predict()` API (#7193, @dbczumar)
- [Scoring] Fix an issue where `DataCaptureConfig` information was not preserved when updating a Sagemaker deployment (#7281, @harupy)

Small bug fixes and documentation updates:

#7309, #7314, #7288, #7276, #7244, #7207, #7175, #7107, @sunishsheth2009; #7261, #7313, #7311, #7249, #7278, #7260, #7284, #7283, #7263, #7266, #7264, #7267, #7265, #7250, #7259, #7247, #7242, #7143, #7214, #7226, #7230, #7227, #7229, #7225, #7224, #7223, #7210, #7192, #7197, #7196, #7204, #7198, #7191, #7189, #7184, #7182, #7170, #7183, #7131, #7165, #7151, #7164, #7168, #7150, #7128, #7028, #7118, #7117, #7102, #7072, #7103, #7101, #7100, #7099, #7098, #7041, #7040, #6978, #6768, #6719, #6669, #6658, #6656, #6655, #6538, #6507, #6504 @harupy; #7310, #7308, #7300, #7290, #7239, #7220, #7127, #7091, #6713 @BenWilson2; #7332, #7299, #7271, #7209, #7180, #7179, #7158, #7147, #7114, @prithvikannan; #7275, #7245, #7134, #7059, @jinzhang21; #7306, #7298, #7287, #7272, #7258, #7236, @ayushthe1; #7279, @tk1012; #7219, @rddefauw; #7333, #7218, #7208, #7188, #7190, #7176, #7137, #7136, #7130, #7124, #7079, #7052, #6541 @dbczumar; #6640, @WeichenXu123; #7200, @hubertzub-db; #7121, @Gonmeso; #6988, @alonisser; #7141, @pdifranc; #7086, @jerrylian-db; #7286, @shogohida

## 1.30.0 (2022-10-19)

MLflow 1.30.0 includes several major features and improvements

Features:

- [Pipelines] Introduce hyperparameter tuning support to MLflow Pipelines (#6859, @prithvikannan)
- [Pipelines] Introduce support for prediction outlier comparison to training data set (#6991, @jinzhang21)
- [Pipelines] Introduce support for recording all training parameters for reproducibility (#7026, #7094, @prithvikannan)
- [Pipelines] Add support for `Delta` tables as a datasource in the ingest step (#7010, @sunishsheth2009)
- [Pipelines] Add expanded support for data profiling up to 10,000 columns (#7035, @prithvikanna)
- [Pipelines] Add support for AutoML in MLflow Pipelines using FLAML (#6959, @mshtelma)
- [Pipelines] Add support for simplified transform step execution by allowing for unspecified configuration (#6909, @apurva-koti)
- [Pipelines] Introduce a data preview tab to the transform step card (#7033, @prithvikannan)
- [Tracking] Introduce `run_name` attribute for `create_run`, `get_run` and `update_run` APIs (#6782, #6798 @apurva-koti)
- [Tracking] Add support for searching by `creation_time` and `last_update_time` for the `search_experiments` API (#6979, @harupy)
- [Tracking] Add support for search terms `run_id IN` and `run ID NOT IN` for the `search_runs` API (#6945, @harupy)
- [Tracking] Add support for searching by `user_id` and `end_time` for the `search_runs` API (#6881, #6880 @subramaniam02)
- [Tracking] Add support for searching by `run_name` and `run_id` for the `search_runs` API (#6899, @harupy; #6952, @alexacole)
- [Tracking] Add support for synchronizing run `name` attribute and `mlflow.runName` tag (#6971, @BenWilson2)
- [Tracking] Add support for signed tracking server requests using AWSSigv4 and AWS IAM (#7044, @pdifranc)
- [Tracking] Introduce the `update_run()` API for modifying the `status` and `name` attributes of existing runs (#7013, @gabrielfu)
- [Tracking] Add support for experiment deletion in the `mlflow gc` cli API (#6977, @shaikmoeed)
- [Models] Add support for environment restoration in the `evaluate()` API (#6728, @jerrylian-db)
- [Models] Remove restrictions on binary classification labels in the `evaluate()` API (#7077, @dbczumar)
- [Scoring] Add support for `BooleanType` to `mlflow.pyfunc.spark_udf()` (#6913, @BenWilson2)
- [SQLAlchemy] Add support for configurable `Pool` class options for `SqlAlchemyStore` (#6883, @mingyu89)

Bug fixes:

- [Pipelines] Enable Pipeline subprocess commands to create a new `SparkSession` if one does not exist (#6846, @prithvikannan)
- [Pipelines] Fix a rendering issue with `bool` column types in Step Card data profiles (#6907, @sunishsheth2009)
- [Pipelines] Add validation and an exception if required step files are missing (#7067, @mingyu89)
- [Pipelines] Change step configuration validation to only be performed during runtime execution of a step (#6967, @prithvikannan)
- [Tracking] Fix infinite recursion bug when inferring the model schema in `mlflow.pyspark.ml.autolog()` (#6831, @harupy)
- [UI] Remove the browser error notification when failing to fetch artifacts (#7001, @kevingreer)
- [Models] Allow `mlflow-skinny` package to serve as base requirement in `MLmodel` requirements (#6974, @BenWilson2)
- [Models] Fix an issue with code path resolution for loading SparkML models (#6968, @dbczumar)
- [Models] Fix an issue with dependency inference in logging SparkML models (#6912, @BenWilson2)
- [Models] Fix an issue involving potential duplicate downloads for SparkML models (#6903, @serena-ruan)
- [Models] Add missing `pos_label` to `sklearn.metrics.precision_recall_curve` in `mlflow.evaluate()` (#6854, @dbczumar)
- [SQLAlchemy] Fix a bug in `SqlAlchemyStore` where `set_tag()` updates the incorrect tags (#7027, @gabrielfu)

Documentation updates:

- [Models] Update details regarding the default `Keras` serialization format (#7022, @balvisio)

Small bug fixes and documentation updates:

#7093, #7095, #7092, #7064, #7049, #6921, #6920, #6940, #6926, #6923, #6862, @jerrylian-db; #6946, #6954, #6938, @mingyu89; #7047, #7087, #7056, #6936, #6925, #6892, #6860, #6828, @sunishsheth2009; #7061, #7058, #7098, #7071, #7073, #7057, #7038, #7029, #6918, #6993, #6944, #6976, #6960, #6933, #6943, #6941, #6900, #6901, #6898, #6890, #6888, #6886, #6887, #6885, #6884, #6849, #6835, #6834, @harupy; #7094, #7065, #7053, #7026, #7034, #7021, #7020, #6999, #6998, #6996, #6990, #6989, #6934, #6924, #6896, #6895, #6876, #6875, #6861, @prithvikannan; #7081, #7030, #7031, #6965, #6750, @bbarnes52; #7080, #7069, #7051, #7039, #7012, #7004, @dbczumar; #7054, @jinzhang21; #7055, #7037, #7036, #6949, #6951, @apurva-koti; #6815, @michaguenther; #6897, @chaturvedakash; #7025, #6981, #6950, #6948, #6937, #6829, #6830, @BenWilson2; #6982, @vadim; #6985, #6927, @kriscon-db; #6917, #6919, #6872, #6855, @WeichenXu123; #6980, @utkarsh867; #6973, #6935, @wentinghu; #6930, @mingyangge-db; #6956, @RohanBha1; #6916, @av-maslov; #6824, @shrinath-suresh; #6732, @oojo12; #6807, @ikrizanic; #7066, @subramaniam20jan; #7043, @AvikantSrivastava; #6879, @jspablo

## 1.29.0 (2022-09-16)

MLflow 1.29.0 includes several major features and improvements

Features:

- [Pipelines] Improve performance and fidelity of dataset profiling in the scikit-learn regression Pipeline (#6792, @sunishsheth2009)
- [Pipelines] Add an `mlflow pipelines get-artifact` CLI for retrieving Pipeline artifacts (#6517, @prithvikannan)
- [Pipelines] Introduce an option for skipping dataset profiling to the scikit-learn regression Pipeline (#6456, @apurva-koti)
- [Pipelines / UI] Display an `mlflow pipelines` CLI command for reproducing a Pipeline run in the MLflow UI (#6376, @hubertzub-db)
- [Tracking] Automatically generate friendly names for Runs if not supplied by the user (#6736, @BenWilson2)
- [Tracking] Add `load_text()`, `load_image()` and `load_dict()` fluent APIs for convenient artifact loading (#6475, @subramaniam02)
- [Tracking] Add `creation_time` and `last_update_time` attributes to the Experiment class (#6756, @subramaniam02)
- [Tracking] Add official MLflow Tracking Server Dockerfiles to the MLflow repository (#6731, @oojo12)
- [Tracking] Add `searchExperiments` API to Java client and deprecate `listExperiments` (#6561, @dbczumar)
- [Tracking] Add `mlflow_search_experiments` API to R client and deprecate `mlflow_list_experiments` (#6576, @dbczumar)
- [UI] Make URLs clickable in the MLflow Tracking UI (#6526, @marijncv)
- [UI] Introduce support for csv data preview within the artifact viewer pane (#6567, @nnethery)
- [Model Registry / Models] Introduce `mlflow.models.add_libraries_to_model()` API for adding libraries to an MLflow Model (#6586, @arjundc-db)
- [Models] Add model validation support to `mlflow.evaluate()` (#6582, @jerrylian-db)
- [Models] Introduce `sample_weights` support to `mlflow.evaluate()` (#6806, @dbczumar)
- [Models] Add `pos_label` support to `mlflow.evaluate()` for identifying the positive class (#6696, @harupy)
- [Models] Make the metric name prefix and dataset info configurable in `mlflow.evaluate()` (#6593, @dbczumar)
- [Models] Add utility for validating the compatibility of a dataset with a model signature (#6494, @serena-ruan)
- [Models] Add `predict_proba()` support to the pyfunc representation of scikit-learn models (#6631, @skylarbpayne)
- [Models] Add support for Decimal type inference to MLflow Model schemas (#6600, @shitaoli-db)
- [Models] Add new CLI command for generating Dockerfiles for model serving (#6591, @anuarkaliyev23)
- [Scoring] Add `/health` endpoint to scoring server (#6574, @gabriel-milan)
- [Scoring] Support specifying a `variant_name` during Sagemaker deployment (#6486, @nfarley-soaren)
- [Scoring] Support specifying a `data_capture_config` during SageMaker deployment (#6423, @jonwiggins)

Bug fixes:

- [Tracking] Make Run and Experiment deletion and restoration idempotent (#6641, @dbczumar)
- [UI] Fix an alignment bug affecting the Experiments list in the MLflow UI (#6569, @sunishsheth2009)
- [Models] Fix a regression in the directory path structure of logged Spark Models that occurred in MLflow 1.28.0 (#6683, @gwy1995)
- [Models] No longer reload the `__main__` module when loading model code (#6647, @Jooakim)
- [Artifacts] Fix an `mlflow server` compatibility issue with HDFS when running in `--serve-artifacts` mode (#6482, @shidianshifen)
- [Scoring] Fix an inference failure with 1-dimensional tensor inputs in TensorFlow and Keras (#6796, @LiamConnell)

Documentation updates:

- [Tracking] Mark the SearchExperiments API as stable (#6551, @dbczumar)
- [Tracking / Model Registry] Deprecate the ListExperiments, ListRegisteredModels, and `list_run_infos()` APIs (#6550, @dbczumar)
- [Scoring] Deprecate `mlflow.sagemaker.deploy()` in favor of `SageMakerDeploymentClient.create()` (#6651, @dbczumar)

Small bug fixes and documentation updates:

#6803, #6804, #6801, #6791, #6772, #6745, #6762, #6760, #6761, #6741, #6725, #6720, #6666, #6708, #6717, #6704, #6711, #6710, #6706, #6699, #6700, #6702, #6701, #6685, #6664, #6644, #6653, #6629, #6639, #6624, #6565, #6558, #6557, #6552, #6549, #6534, #6533, #6516, #6514, #6506, #6509, #6505, #6492, #6490, #6478, #6481, #6464, #6463, #6460, #6461, @harupy; #6810, #6809, #6727, #6648, @BenWilson2; #6808, #6766, #6729, @jerrylian-db; #6781, #6694, @marijncv; #6580, #6661, @bbarnes52; #6778, #6687, #6623, @shraddhafalane; #6662, #6737, #6612, #6595, @sunishsheth2009; #6777, @aviralsharma07; #6665, #6743, #6573, @liangz1; #6784, @apurva-koti; #6753, #6751, @mingyu89; #6690, #6455, #6484, @kriscon-db; #6465, #6689, @hubertzub-db; #6721, @WeichenXu123; #6722, #6718, #6668, #6663, #6621, #6547, #6508, #6474, #6452, @dbczumar; #6555, #6584, #6543, #6542, #6521, @dsgibbons; #6634, #6596, #6563, #6495, @prithvikannan; #6571, @smurching; #6630, #6483, @serena-ruan; #6642, @thinkall; #6614, #6597, @jinzhang21; #6457, @cnphil; #6570, #6559, @kumaryogesh17; #6560, #6540, @iamthen0ise; #6544, @Monkero; #6438, @ahlag; #3292, @dolfinus; #6637, @ninabacc-db; #6632, @arpitjasa-db

## 1.28.0 (2022-08-09)

MLflow 1.28.0 includes several major features and improvements:

Features:

- [Pipelines] Log the full Pipeline runtime configuration to MLflow Tracking during Pipeline execution (#6359, @jinzhang21)
- [Pipelines] Add `pipeline.yaml` configurations to specify the Model Registry backend used for model registration (#6284, @sunishsheth2009)
- [Pipelines] Support optionally skipping the `transform` step of the scikit-learn regression pipeline (#6362, @sunishsheth2009)
- [Pipelines] Add UI links to Runs and Models in Pipeline Step Cards on Databricks (#6294, @dbczumar)
- [Tracking] Introduce `mlflow.search_experiments()` API for searching experiments by name and by tags (#6333, @WeichenXu123; #6227, #6172, #6154, @harupy)
- [Tracking] Increase the maximum parameter value length supported by File and SQL backends to 500 characters (#6358, @johnyNJ)
- [Tracking] Introduce an `--older-than` flag to `mlflow gc` for removing runs based on deletion time (#6354, @Jason-CKY)
- [Tracking] Add `MLFLOW_SQLALCHEMYSTORE_POOL_RECYCLE` environment variable for recycling SQLAlchemy connections (#6344, @postrational)
- [UI] Display deeply nested runs in the Runs Table on the Experiment Page (#6065, @tospe)
- [UI] Add box plot visualization for metrics to the Compare Runs page (#6308, @ahlag)
- [UI] Display tags on the Compare Runs page (#6164, @CaioCavalcanti)
- [UI] Use scientific notation for axes when viewing metric plots in log scale (#6176, @RajezMariner)
- [UI] Add button to Metrics page for downloading metrics as CSV (#6048, @rafaelvp-db)
- [UI] Include NaN and +/- infinity values in plots on the Metrics page (#6422, @hubertzub-db)
- [Tracking / Model Registry] Introduce environment variables to control retry behavior and timeouts for REST API requests (#5745, @peterdhansen)
- [Tracking / Model Registry] Make `MlflowClient` importable as `mlflow.MlflowClient` (#6085, @subramaniam02)
- [Model Registry] Add support for searching registered models and model versions by tags (#6413, #6411, #6320, @WeichenXu123)
- [Model Registry] Add `stage` parameter to `set_model_version_tag()` (#6185, @subramaniam02)
- [Model Registry] Add `--registry-store-uri` flag to `mlflow server` for specifying the Model Registry backend URI (#6142, @Secbone)
- [Models] Improve performance of Spark Model logging on Databricks (#6282, @bbarnes52)
- [Models] Include Pandas Series names in inferred model schemas (#6361, @RynoXLI)
- [Scoring] Make `model_uri` optional in `mlflow models build-docker` to support building generic model serving images (#6302, @harupy)
- [R] Support logging of NA and NaN parameter values (#6263, @nathaneastwood)

Bug fixes and documentation updates:

- [Pipelines] Improve scikit-learn regression pipeline latency by limiting dataset profiling to the first 100 columns (#6297, @sunishsheth2009)
- [Pipelines] Use `xdg-open` instead of `open` for viewing Pipeline results on Linux systems (#6326, @strangiato)
- [Pipelines] Fix a bug that skipped Step Card rendering in Jupyter Notebooks (#6378, @apurva-koti)
- [Tracking] Use the 401 HTTP response code in authorization failure REST API responses, instead of 500 (#6106, @balvisio)
- [Tracking] Correctly classify artifacts as files and directories when using Azure Blob Storage (#6237, @nerdinand)
- [Tracking] Fix a bug in the File backend that caused run metadata to be lost in the event of a failed write (#6388, @dbczumar)
- [Tracking] Adjust `mlflow.pyspark.ml.autolog()` to only log model signatures for supported input / output data types (#6365, @harupy)
- [Tracking] Adjust `mlflow.tensorflow.autolog()` to log TensorFlow early stopping callback info when `log_models=False` is specified (#6170, @WeichenXu123)
- [Tracking] Fix signature and input example logging errors in `mlflow.sklearn.autolog()` for models containing transformers (#6230, @dbczumar)
- [Tracking] Fix a failure in `mlflow gc` that occurred when removing a run whose artifacts had been previously deleted (#6165, @dbczumar)
- [Tracking] Add missing `sqlparse` library to MLflow Skinny client, which is required for search support (#6174, @dbczumar)
- [Tracking / Model Registry] Fix an `mlflow server` bug that rejected parameters and tags with empty string values (#6179, @dbczumar)
- [Model Registry] Fix a failure preventing model version schemas from being downloaded with `--serve-arifacts` enabled (#6355, @abbas123456)
- [Scoring] Patch the Java Model Server to support MLflow Models logged on recent versions of the Databricks Runtime (#6337, @dbczumar)
- [Scoring] Verify that either the deployment name or endpoint is specified when invoking the `mlflow deployments predict` CLI (#6323, @dbczumar)
- [Scoring] Properly encode datetime columns when performing batch inference with `mlflow.pyfunc.spark_udf()` (#6244, @harupy)
- [Projects] Fix an issue where local directory paths were misclassified as Git URIs when running Projects (#6218, @ElefHead)
- [R] Fix metric logging behavior for +/- infinity values (#6271, @nathaneastwood)
- [Docs] Move Python API docs for `MlflowClient` from `mlflow.tracking` to `mlflow.client` (#6405, @dbczumar)
- [Docs] Document that MLflow Pipelines requires Make (#6216, @dbczumar)
- [Docs] Improve documentation for developing and testing MLflow JS changes in `CONTRIBUTING.rst` (#6330, @ahlag)

Small bug fixes and doc updates (#6322, #6321, #6213, @KarthikKothareddy; #6409, #6408, #6396, #6402, #6399, #6398, #6397, #6390, #6381, #6386, #6385, #6373, #6375, #6380, #6374, #6372, #6363, #6353, #6352, #6350, #6351, #6349, #6347, #6287, #6341, #6342, #6340, #6338, #6319, #6314, #6316, #6317, #6318, #6315, #6313, #6311, #6300, #6292, #6291, #6289, #6290, #6278, #6279, #6276, #6272, #6252, #6243, #6250, #6242, #6241, #6240, #6224, #6220, #6208, #6219, #6207, #6171, #6206, #6199, #6196, #6191, #6190, #6175, #6167, #6161, #6160, #6153, @harupy; #6193, @jwgwalton; #6304, #6239, #6234, #6229, @sunishsheth2009; #6258, @xanderwebs; #6106, @balvisio; #6303, @bbarnes52; #6117, @wenfeiy-db; #6389, #6214, @apurva-koti; #6412, #6420, #6277, #6266, #6260, #6148, @WeichenXu123; #6120, @ameya-parab; #6281, @nathaneastwood; #6426, #6415, #6417, #6418, #6257, #6182, #6157, @dbczumar; #6189, @shrinath-suresh; #6309, @SamirPS; #5897, @temporaer; #6251, @herrmann; #6198, @sniafas; #6368, #6158, @jinzhang21; #6236, @subramaniam02; #6036, @serena-ruan; #6430, @ninabacc-db)

## 1.27.0 (2022-06-27)

MLflow 1.27.0 includes several major features and improvements:

- [**Pipelines**] With MLflow 1.27.0, we are excited to announce the release of
  [**MLflow Pipelines**](https://mlflow.org/docs/latest/pipelines.html), an opinionated framework for
  structuring MLOps workflows that simplifies and standardizes machine learning application development
  and productionization. MLflow Pipelines makes it easy for data scientists to follow best practices
  for creating production-ready ML deliverables, allowing them to focus on developing excellent models.
  MLflow Pipelines also enables ML engineers and DevOps teams to seamlessly deploy models to production
  and incorporate them into applications. To get started with MLflow Pipelines, check out the docs at
  https://mlflow.org/docs/latest/pipelines.html. (#6115)

- [UI] Introduce UI support for searching and comparing runs across multiple Experiments (#5971, @r3stl355)

More features:

- [Tracking] When using batch logging APIs, automatically split large sets of metrics, tags, and params into multiple requests (#6052, @nzw0301)
- [Tracking] When an Experiment is deleted, SQL-based backends also move the associate Runs to the "deleted" lifecycle stage (#6064, @AdityaIyengar27)
- [Tracking] Add support for logging single-element `ndarray` and tensor instances as metrics via the `mlflow.log_metric()` API (#5756, @ntakouris)
- [Models] Add support for `CatBoostRanker` models to the `mlflow.catboost` flavor (#6032, @danielgafni)
- [Models] Integrate SHAP's `KernelExplainer` with `mlflow.evaluate()`, enabling model explanations on categorical data (#6044, #5920, @WeichenXu123)
- [Models] Extend `mlflow.evaluate()` to automatically log the `score()` outputs of scikit-learn models as metrics (#5935, #5903, @WeichenXu123)

Bug fixes and documentation updates:

- [UI] Fix broken model links in the Runs table on the MLflow Experiment Page (#6014, @hctpbl)
- [Tracking/Installation] Require `sqlalchemy>=1.4.0` upon MLflow installation, which is necessary for usage of SQL-based MLflow Tracking backends (#6024, @sniafas)
- [Tracking] Fix a regression that caused `mlflow server` to reject `LogParam` API requests containing empty string values (#6031, @harupy)
- [Tracking] Fix a failure in scikit-learn autologging that occurred when `matplotlib` was not installed on the host system (#5995, @fa9r)
- [Tracking] Fix a failure in TensorFlow autologging that occurred when training models on `tf.data.Dataset` inputs (#6061, @dbczumar)
- [Artifacts] Address artifact download failures from SFTP locations that occurred due to mismanaged concurrency (#5840, @rsundqvist)
- [Models] Fix a bug where MLflow Models did not restore bundled code properly if multiple models use the same code module name (#5926, @BFAnas)
- [Models] Address an issue where `mlflow.sklearn.model()` did not properly restore bundled model code (#6037, @WeichenXu123)
- [Models] Fix a bug in `mlflow.evaluate()` that caused input data objects to be mutated when evaluating certain scikit-learn models (#6141, @dbczumar)
- [Models] Fix a failure in `mlflow.pyfunc.spark_udf` that occurred when the UDF was invoked on an empty RDD partition (#6063, @WeichenXu123)
- [Models] Fix a failure in `mlflow models build-docker` that occurred when `env-manager=local` was specified (#6046, @bneijt)
- [Projects] Improve robustness of the git repository check that occurs prior to MLflow Project execution (#6000, @dkapur17)
- [Projects] Address a failure that arose when running a Project that does not have a `master` branch (#5889, @harupy)
- [Docs] Correct several typos throughout the MLflow docs (#5959, @ryanrussell)

Small bug fixes and doc updates (#6041, @drsantos89; #6138, #6137, #6132, @sunishsheth2009; #6144, #6124, #6125, #6123, #6057, #6060, #6050, #6038, #6029, #6030, #6025, #6018, #6019, #5962, #5974, #5972, #5957, #5947, #5907, #5938, #5906, #5932, #5919, #5914, #5888, #5890, #5886, #5873, #5865, #5843, @harupy; #6113, @comojin1994; #5930, @yashaswikakumanu; #5837, @shrinath-suresh; #6067, @deepyaman; #5997, @idlefella; #6021, @BenWilson2; #5984, @Sumanth077; #5929, @krunal16-c; #5879, @kugland; #5875, @ognis1205; #6006, @ryanrussell; #6140, @jinzhang21; #5983, @elk15; #6022, @apurva-koti; #5982, @EB-Joel; #5981, #5980, @punitkashyup; #6103, @ikrizanic; #5988, #5969, @SaumyaBhushan; #6020, #5991, @WeichenXu123; #5910, #5912, @Dark-Knight11; #6005, @Asinsa; #6023, @subramaniam02; #5999, @Regis-Caelum; #6007, @CaioCavalcanti; #5943, @kvaithin; #6017, #6002, @NeoKish; #6111, @T1b4lt; #5986, @seyyidibrahimgulec; #6053, @Zohair-coder; #6146, #6145, #6143, #6139, #6134, #6136, #6135, #6133, #6071, #6070, @dbczumar; #6026, @rotate2050)

## 1.26.1 (2022-05-27)

MLflow 1.26.1 is a patch release containing the following bug fixes:

- [Installation] Fix compatibility issue with `protobuf >= 4.21.0` (#5945, @harupy)
- [Models] Fix `get_model_dependencies` behavior for `models:` URIs containing artifact paths (#5921, @harupy)
- [Models] Revert a problematic change to `artifacts` persistence in `mlflow.pyfunc.log_model()` that was introduced in MLflow 1.25.0 (#5891, @kyle-jarvis)
- [Models] Close associated image files when `EvaluationArtifact` outputs from `mlflow.evaluate()` are garbage collected (#5900, @WeichenXu123)

Small bug fixes and updates (#5874, #5942, #5941, #5940, #5938, @harupy; #5893, @PrajwalBorkar; #5909, @yashaswikakumanu; #5937, @BenWilson2)

## 1.26.0 (2022-05-16)

MLflow 1.26.0 includes several major features and improvements:

Features:

- [CLI] Add endpoint naming and options configuration to the deployment CLI (#5731, @trangevi)
- [Build,Doc] Add development environment setup script for Linux and MacOS x86 Operating Systems (#5717, @BenWilson2)
- [Tracking] Update `mlflow.set_tracking_uri` to add support for paths defined as `pathlib.Path` in addition to existing `str` path declarations (#5824, @cacharle)
- [Scoring] Add custom timeout override option to the scoring server CLI to support high latency models (#5663, @sniafas)
- [UI] Add sticky header to experiment run list table to support column name visibility when scrolling beyond page fold (#5818, @hubertzub-db)
- [Artifacts] Add GCS support for MLflow garbage collection (#5811, @aditya-iyengar-rtl-de)
- [Evaluate] Add `pos_label` argument for `eval_and_log_metrics` API to support accurate binary classifier evaluation metrics (#5807, @yxiong)
- [UI] Add fields for latest, minimum and maximum metric values on metric display page (#5574, @adamreeve)
- [Models] Add support for `input_example` and `signature` logging for pyspark ml flavor when using autologging (#5719, @bali0019)
- [Models] Add `virtualenv` environment manager support for `mlflow models docker-build` CLI (#5728, @harupy)
- [Models] Add support for wildcard module matching in log_model_allowlist for PySpark models (#5723, @serena-ruan)
- [Projects] Add `virtualenv` environment manager support for MLflow projects (#5631, @harupy)
- [Models] Add `virtualenv` environment manager support for MLflow Models (#5380, @harupy)
- [Models] Add `virtualenv` environment manager support for `mlflow.pyfunc.spark_udf` (#5676, @WeichenXu123)
- [Models] Add support for `input_example` and `signature` logging for `tensorflow` flavor when using autologging (#5510, @bali0019)
- [Server-infra] Add JSON Schema Type Validation to enable raising 400 errors on malformed requests to REST API endpoints (#5458, @mrkaye97)
- [Scoring] Introduce abstract `endpoint` interface for mlflow deployments (#5378, @trangevi)
- [UI] Add `End Time` and `Duration` fields to run comparison page (#3378, @RealArpanBhattacharya)
- [Serving] Add schema validation support when parsing input csv data for model serving (#5531, @vvijay-bolt)

Bug fixes and documentation updates:

- [Models] Fix REPL ID propagation from datasource listener to publisher for Spark data sources (#5826, @dbczumar)
- [UI] Update `ag-grid` and implement `getRowId` to improve performance in the runs table visualization (#5725, @adamreeve)
- [Serving] Fix `tf-serving` parsing to support columnar-based formatting (#5825, @arjundc-db)
- [Artifacts] Update `log_artifact` to support models larger than 2GB in HDFS (#5812, @hitchhicker)
- [Models] Fix autologging to support `lightgbm` metric names with "@" symbols within their names (#5785, @mengchendd)
- [Models] Pyfunc: Fix code directory resolution of subdirectories (#5806, @dbczumar)
- [Server-Infra] Fix mlflow-R server starting failure on windows (#5767, @serena-ruan)
- [Docs] Add documentation for `virtualenv` environment manager support for MLflow projects (#5727, @harupy)
- [UI] Fix artifacts display sizing to support full width rendering in preview pane (#5606, @szczeles)
- [Models] Fix local hostname issues when loading spark model by binding driver address to localhost (#5753, @WeichenXu123)
- [Models] Fix autologging validation and batch_size calculations for `tensorflow` flavor (#5683, @MarkYHZhang)
- [Artifacts] Fix `SqlAlchemyStore.log_batch` implementation to make it log data in batches (#5460, @erensahin)

Small bug fixes and doc updates (#5858, #5859, #5853, #5854, #5845, #5829, #5842, #5834, #5795, #5777, #5794, #5766, #5778, #5765, #5763, #5768, #5769, #5760, #5727, #5748, #5726, #5721, #5711, #5710, #5708, #5703, #5702, #5696, #5695, #5669, #5670, #5668, #5661, #5638, @harupy; #5749, @arpitjasa-db; #5675, @Davidswinkels; #5803, #5797, @ahlag; #5743, @kzhang01; #5650, #5805, #5724, #5720, #5662, @BenWilson2; #5627, @cterrelljones; #5646, @kutal10; #5758, @davideli-db; #5810, @rahulporuri; #5816, #5764, @shrinath-suresh; #5869, #5715, #5737, #5752, #5677, #5636, @WeichenXu123; #5735, @subramaniam02; #5746, @akaigraham; #5734, #5685, @lucalves; #5761, @marcelatoffernet; #5707, @aashish-khub; #5808, @ketangangal; #5730, #5700, @shaikmoeed; #5775, @dbczumar; #5747, @zhixuanevelynwu)

## 1.25.1 (2022-04-13)

MLflow 1.25.1 is a patch release containing the following bug fixes:

- [Models] Fix a `pyfunc` artifact overwrite bug for when multiple artifacts are saved in sub-directories (#5657, @kyle-jarvis)
- [Scoring] Fix permissions issue for Spark workers accessing model artifacts from a temp directory created by the driver (#5684, @WeichenXu123)

## 1.25.0 (2022-04-11)

MLflow 1.25.0 includes several major features and improvements:

Features:

- [Tracking] Introduce a new fluent API `mlflow.last_active_run()` that provides the most recent fluent active run (#5584, @MarkYHZhang)
- [Tracking] Add `experiment_names` argument to the `mlflow.search_runs()` API to support searching runs by experiment names (#5564, @r3stl355)
- [Tracking] Add a `description` parameter to `mlflow.start_run()` (#5534, @dogeplusplus)
- [Tracking] Add `log_every_n_step` parameter to `mlflow.pytorch.autolog()` to control metric logging frequency (#5516, @adamreeve)
- [Tracking] Log `pyspark.ml.param.Params` values as MLflow parameters during PySpark autologging (#5481, @serena-ruan)
- [Tracking] Add support for `pyspark.ml.Transformer`s to PySpark autologging (#5466, @serena-ruan)
- [Tracking] Add input example and signature autologging for Keras models (#5461, @bali0019)
- [Models] Introduce `mlflow.diviner` flavor for large-scale [time series forecasting](https://databricks-diviner.readthedocs.io/en/latest/?badge=latest) (#5553, @BenWilson2)
- [Models] Add `pyfunc.get_model_dependencies()` API to retrieve reproducible environment specifications for MLflow Models with the pyfunc flavor (#5503, @WeichenXu123)
- [Models] Add `code_paths` argument to all model flavors to support packaging custom module code with MLflow Models (#5448, @stevenchen-db)
- [Models] Support creating custom artifacts when evaluating models with `mlflow.evaluate()` (#5405, #5476 @MarkYHZhang)
- [Models] Add `mlflow_version` field to MLModel specification (#5515, #5576, @r3stl355)
- [Models] Add support for logging models to preexisting destination directories (#5572, @akshaya-a)
- [Scoring / Projects] Introduce `--env-manager` configuration for specifying environment restoration tools (e.g. `conda`) and deprecate `--no-conda` (#5567, @harupy)
- [Scoring] Support restoring model dependencies in `mlflow.pyfunc.spark_udf()` to ensure accurate predictions (#5487, #5561, @WeichenXu123)
- [Scoring] Add support for `numpy.ndarray` type inputs to the TensorFlow pyfunc `predict()` function (#5545, @WeichenXu123)
- [Scoring] Support deployment of MLflow Models to Sagemaker Serverless (#5610, @matthewmayo)
- [UI] Add MLflow version to header beneath logo (#5504, @adamreeve)
- [Artifacts] Introduce a `mlflow.artifacts.download_artifacts()` API mirroring the functionality of the `mlflow artifacts download` CLI (#5585, @dbczumar)
- [Artifacts] Introduce environment variables for controlling GCS artifact upload/download chunk size and timeouts (#5438, #5483, @mokrueger)

Bug fixes and documentation updates:

- [Tracking/SQLAlchemy] Create an index on `run_uuid` for PostgreSQL to improve query performance (#5446, @harupy)
- [Tracking] Remove client-side validation of metric, param, tag, and experiment fields (#5593, @BenWilson2)
- [Projects] Support setting the name of the MLflow Run when executing an MLflow Project (#5187, @bramrodenburg)
- [Scoring] Use pandas `split` orientation for DataFrame inputs to SageMaker deployment `predict()` API to preserve column ordering (#5522, @dbczumar)
- [Server-Infra] Fix runs search compatibility bugs with PostgreSQL, MySQL, and MSSQL (#5540, @harupy)
- [CLI] Fix a bug in the `mlflow-skinny` client that caused `mlflow --version` to fail (#5573, @BenWilson2)
- [Docs] Update guidance and examples for model deployment to AzureML to recommend using the `mlflow-azureml` package (#5491, @santiagxf)

Small bug fixes and doc updates (#5591, #5629, #5597, #5592, #5562, #5477, @BenWilson2; #5554, @juntai-zheng; #5570, @tahesse; #5605, @guelate; #5633, #5632, #5625, #5623, #5615, #5608, #5600, #5603, #5602, #5596, #5587, #5586, #5580, #5577, #5568, #5290, #5556, #5560, #5557, #5548, #5547, #5538, #5513, #5505, #5464, #5495, #5488, #5485, #5468, #5455, #5453, #5454, #5452, #5445, #5431, @harupy; #5640, @nchittela; #5520, #5422, @Ark-kun; #5639, #5604, @nishipy; #5543, #5532, #5447, #5435, @WeichenXu123; #5502, @singankit; #5500, @Sohamkayal4103; #5449, #5442, @apurva-koti; #5552, @vinijaiswal; #5511, @adamreeve; #5428, @jinzhang21; #5309, @sunishsheth2009; #5581, #5559, @Kr4is; #5626, #5618, #5529, @sisp; #5652, #5624, #5622, #5613, #5509, #5459, #5437, @dbczumar; #5616, @liangz1)

## 1.24.0 (2022-02-27)

MLflow 1.24.0 includes several major features and improvements:

Features:

- [Tracking] Support uploading, downloading, and listing artifacts through the MLflow server via `mlflow server --serve-artifacts` (#5320, @BenWilson2, @harupy)
- [Tracking] Add the `registered_model_name` argument to `mlflow.autolog()` for automatic model registration during autologging (#5395, @WeichenXu123)
- [UI] Improve and restructure the Compare Runs page. Additions include "show diff only" toggles and scrollable tables (#5306, @WeichenXu123)
- [Models] Introduce `mlflow.pmdarima` flavor for pmdarima models (#5373, @BenWilson2)
- [Models] When loading an MLflow Model, print a warning if a mismatch is detected between the current environment and the Model's dependencies (#5368, @WeichenXu123)
- [Models] Support computing custom scalar metrics during model evaluation with `mlflow.evaluate()` (#5389, @MarkYHZhang)
- [Scoring] Add support for deploying and evaluating SageMaker models via the [`MLflow Deployments API`](https://mlflow.org/docs/latest/models.html#deployment-to-custom-targets) (#4971, #5396, @jamestran201)

Bug fixes and documentation updates:

- [Tracking / UI] Fix artifact listing and download failures that occurred when operating the MLflow server in `--serve-artifacts` mode (#5409, @dbczumar)
- [Tracking] Support environment-variable-based authentication when making artifact requests to the MLflow server in `--serve-artifacts` mode (#5370, @TimNooren)
- [Tracking] Fix bugs in hostname and path resolution when making artifacts requests to the MLflow server in `--serve-artifacts` mode (#5384, #5385, @mert-kirpici)
- [Tracking] Fix an import error that occurred when `mlflow.log_figure()` was used without `matplotlib.figure` imported (#5406, @WeichenXu123)
- [Tracking] Correctly log XGBoost metrics containing the `@` symbol during autologging (#5403, @maxfriedrich)
- [Tracking] Fix a SQL Server database error that occurred during Runs search (#5382, @dianacarvalho1)
- [Tracking] When downloading artifacts from HDFS, store them in the user-specified destination directory (#5210, @DimaClaudiu)
- [Tracking / Model Registry] Improve performance of large artifact and model downloads (#5359, @mehtayogita)
- [Models] Fix fast.ai PyFunc inference behavior for models with 2D outputs (#5411, @santiagxf)
- [Models] Record Spark model information to the active run when `mlflow.spark.log_model()` is called (#5355, @szczeles)
- [Models] Restore onnxruntime execution providers when loading ONNX models with `mlflow.pyfunc.load_model()` (#5317, @ecm200)
- [Projects] Increase Docker image push timeout when using Projects with Docker (#5363, @zanitete)
- [Python] Fix a bug that prevented users from enabling DEBUG-level Python log outputs (#5362, @dbczumar)
- [Docs] Add a developer guide explaining how to build custom plugins for `mlflow.evaluate()` (#5333, @WeichenXu123)

Small bug fixes and doc updates (#5298, @wamartin-aml; #5399, #5321, #5313, #5307, #5305, #5268, #5284, @harupy; #5329, @Ark-kun; #5375, #5346, #5304, @dbczumar; #5401, #5366, #5345, @BenWilson2; #5326, #5315, @WeichenXu123; #5236, @singankit; #5302, @timvink; #5357, @maitre-matt; #5347, #5344, @mehtayogita; #5367, @apurva-koti; #5348, #5328, #5310, @liangz1; #5267, @sunishsheth2009)

## 1.23.1 (2022-01-27)

MLflow 1.23.1 is a patch release containing the following bug fixes:

- [Models] Fix a directory creation failure when loading PySpark ML models (#5299, @arjundc-db)
- [Model Registry] Revert to using case-insensitive validation logic for stage names in `models:/` URIs (#5312, @lichenran1234)
- [Projects] Fix a race condition during Project tar file creation (#5303, @dbczumar)

## 1.23.0 (2022-01-17)

MLflow 1.23.0 includes several major features and improvements:

Features:

- [Models] Introduce an `mlflow.evaluate()` API for evaluating MLflow Models, providing performance and explainability insights. For an overview, see https://mlflow.org/docs/latest/models.html#model-evaluation (#5069, #5092, #5256, @WeichenXu123)
- [Models] `log_model()` APIs now return information about the logged MLflow Model, including artifact location, flavors, and schema (#5230, @liangz1)
- [Models] Introduce an `mlflow.models.Model.load_input_example()` Python API for loading MLflow Model input examples (#5212, @maitre-matt)
- [Models] Add a UUID field to the MLflow Model specification. MLflow Models now have a unique identifier (#5149, #5167, @WeichenXu123)
- [Models] Support passing SciPy CSC and CSR matrices as MLflow Model input examples (#5016, @WeichenXu123)
- [Model Registry] Support specifying `latest` in model URI to get the latest version of a model regardless of the stage (#5027, @lichenran1234)
- [Tracking] Add support for LightGBM scikit-learn models to `mlflow.lightgbm.autolog()` (#5130, #5200, #5271 @jwyyy)
- [Tracking] Improve S3 artifact download speed by caching boto clients (#4695, @Samreay)
- [UI] Automatically update metric plots for in-progress runs (#5017, @cedkoffeto, @harupy)

Bug fixes and documentation updates:

- [Models] Fix a bug in MLflow Model schema enforcement where strings were incorrectly cast to Pandas objects (#5134, @stevenchen-db)
- [Models] Fix a bug where keyword arguments passed to `mlflow.pytorch.load_model()` were not applied for scripted models (#5163, @schmidt-jake)
- [Model Registry/R] Fix bug in R client `mlflow_create_model_version()` API that caused model `source` to be set incorrectly (#5185, @bramrodenburg)
- [Projects] Fix parsing behavior for Project URIs containing quotes (#5117, @dinaldoap)
- [Scoring] Use the correct 400-level error code for malformed MLflow Model Server requests (#5003, @abatomunkuev)
- [Tracking] Fix a bug where `mlflow.start_run()` modified user-supplied tags dictionary (#5191, @matheusMoreno)
- [UI] Fix a bug causing redundant scroll bars to be displayed on the Experiment Page (#5159, @sunishsheth2009)

Small bug fixes and doc updates (#5275, #5264, #5244, #5249, #5255, #5248, #5243, #5240, #5239, #5232, #5234, #5235, #5082, #5220, #5219, #5226, #5217, #5194, #5188, #5132, #5182, #5183, #5180, #5177, #5165, #5164, #5162, #5015, #5136, #5065, #5125, #5106, #5127, #5120, @harupy; #5045, @BenWilson2; #5156, @pbezglasny; #5202, @jwyyy; #3863, @JoshuaAnickat; #5205, @abhiramr; #4604, @OSobky; #4256, @einsmein; #5140, @AveshCSingh; #5273, #5186, #5176, @WeichenXu123; #5260, #5229, #5206, #5174, #5160, @liangz1)

## 1.22.0 (2021-11-29)

MLflow 1.22.0 includes several major features and improvements:

Features:

- [UI] Add a share button to the Experiment page (#4936, @marijncv)
- [UI] Improve readability of column sorting dropdown on Experiment page (#5022, @WeichenXu123; #5018, @NieuweNils, @coder-freestyle)
- [Tracking] Mark all autologging integrations as stable by removing `@experimental` decorators (#5028, @liangz1)
- [Tracking] Add optional `experiment_id` parameter to `mlflow.set_experiment()` (#5012, @dbczumar)
- [Tracking] Add support for XGBoost scikit-learn models to `mlflow.xgboost.autolog()` (#5078, @jwyyy)
- [Tracking] Improve statsmodels autologging performance by removing unnecessary metrics (#4942, @WeichenXu123)
- [Tracking] Update R client to tag nested runs with parent run ID (#4197, @yitao-li)
- [Models] Support saving and loading all XGBoost model types (#4954, @jwyyy)
- [Scoring] Support specifying AWS account and role when deploying models to SageMaker (#4923, @andresionek91)
- [Scoring] Support serving MLflow models with MLServer (#4963, @adriangonz)

Bug fixes and documentation updates:

- [UI] Fix bug causing Metric Plot page to crash when metric values are too large (#4947, @ianshan0915)
- [UI] Fix bug causing parallel coordinate curves to vanish (#5087, @harupy)
- [UI] Remove `Creator` field from Model Version page if user information is absent (#5089, @jinzhang21)
- [UI] Fix model loading instructions for non-pyfunc models in Artifact Viewer (#5006, @harupy)
- [Models] Fix a bug that added `mlflow` to `conda.yaml` even if a hashed version was already present (#5058, @maitre-matt)
- [Docs] Add Python documentation for metric, parameter, and tag key / value length limits (#4991, @westford14)
- [Examples] Update Python version used in Prophet example to fix installation errors (#5101, @BenWilson2)
- [Examples] Fix Kubernetes `resources` specification in MLflow Projects + Kubernetes example (#4948, @jianyuan)

Small bug fixes and doc updates (#5119, #5107, #5105, #5103, #5085, #5088, #5051, #5081, #5039, #5073, #5072, #5066, #5064, #5063, #5060, #4718, #5053, #5052, #5041, #5043, #5047, #5036, #5037, #5029, #5031, #5032, #5030, #5007, #5019, #5014, #5008, #4998, #4985, #4984, #4970, #4966, #4980, #4967, #4978, #4979, #4968, #4976, #4975, #4934, #4956, #4938, #4950, #4946, #4939, #4913, #4940, #4935, @harupy; #5095, #5070, #5002, #4958, #4945, @BenWilson2; #5099, @chaosddp; #5005, @you-n-g; #5042, #4952, @shrinath-suresh; #4962, #4995, @WeichenXu123; #5010, @lichenran1234; #5000, @wentinghu; #5111, @alexott; #5102, #5024, #5011, #4959, @dbczumar; #5075, #5044, #5026, #4997, #4964, #4989, @liangz1; #4999, @stevenchen-db)

## 1.21.0 (2021-10-23)

MLflow 1.21.0 includes several major features and improvements:

Features:

- [UI] Add a diff-only toggle to the runs table for filtering out columns with constant values (#4862, @marijncv)
- [UI] Add a duration column to the runs table (#4840, @marijncv)
- [UI] Display the default column sorting order in the runs table (#4847, @marijncv)
- [UI] Add `start_time` and `duration` information to exported runs CSV (#4851, @marijncv)
- [UI] Add lifecycle stage information to the run page (#4848, @marijncv)
- [UI] Collapse run page sections by default for space efficiency, limit artifact previews to 50MB (#4917, @dbczumar)
- [Tracking] Introduce autologging capabilities for PaddlePaddle model training (#4751, @jinminhao)
- [Tracking] Add an optional tags field to the CreateExperiment API (#4788, @dbczumar; #4795, @apurva-koti)
- [Tracking] Add support for deleting artifacts from SFTP stores via the `mlflow gc` CLI (#4670, @afaul)
- [Tracking] Support AzureDefaultCredential for authenticating with Azure artifact storage backends (#4002, @marijncv)
- [Models] Upgrade the fastai model flavor to support fastai V2 (`>=2.4.1`) (#4715, @jinzhang21)
- [Models] Introduce an `mlflow.prophet` model flavor for Prophet time series models (#4773, @BenWilson2)
- [Models] Introduce a CLI for publishing MLflow Models to the SageMaker Model Registry (#4669, @jinnig)
- [Models] Print a warning when inferred model dependencies are not available on PyPI (#4891, @dbczumar)
- [Models, Projects] Add `MLFLOW_CONDA_CREATE_ENV_CMD` for customizing Conda environment creation (#4746, @giacomov)

Bug fixes and documentation updates:

- [UI] Fix an issue where column selections made in the runs table were persisted across experiments (#4926, @sunishsheth2009)
- [UI] Fix an issue where the text `null` was displayed in the runs table column ordering dropdown (#4924, @harupy)
- [UI] Fix a bug causing the metric plot view to display NaN values upon click (#4858, @arpitjasa-db)
- [Tracking] Fix a model load failure for paths containing spaces or special characters on UNIX systems (#4890, @BenWilson2)
- [Tracking] Correct a migration issue that impacted usage of MLflow Tracking with SQL Server (#4880, @marijncv)
- [Tracking] Spark datasource autologging tags now respect the maximum allowable size for MLflow Tracking (#4809, @dbczumar)
- [Model Registry] Add previously-missing certificate sources for Model Registry REST API requests (#4731, @ericgosno91)
- [Model Registry] Throw an exception when users supply invalid Model Registry URIs for Databricks (#4877, @yunpark93)
- [Scoring] Fix a schema enforcement error that incorrectly cast date-like strings to datetime objects (#4902, @wentinghu)
- [Docs] Expand the documentation for the MLflow Skinny Client (#4113, @eedeleon)

Small bug fixes and doc updates (#4928, #4919, #4927, #4922, #4914, #4899, #4893, #4894, #4884, #4864, #4823, #4841, #4817, #4796, #4797, #4767, #4768, #4757, @harupy; #4863, #4838, @marijncv; #4834, @ksaur; #4772, @louisguitton; #4801, @twsl; #4929, #4887, #4856, #4843, #4789, #4780, @WeichenXu123; #4769, @Ark-kun; #4898, #4756, @apurva-koti; #4784, @lakshikaparihar; #4855, @ianshan0915; #4790, @eedeleon; #4931, #4857, #4846, 4777, #4748, @dbczumar)

## 1.20.2 (2021-09-03)

MLflow 1.20.2 is a patch release containing the following features and bug fixes:

Features:

- Enabled auto dependency inference in spark flavor in autologging (#4759, @harupy)

Bug fixes and documentation updates:

- Increased MLflow client HTTP request timeout from 10s to 120s (#4764, @jinzhang21)
- Fixed autologging compatibility bugs with TensorFlow and Keras version `2.6.0` (#4766, @dbczumar)

Small bug fixes and doc updates (#4770, @WeichenXu123)

## 1.20.1 (2021-08-26)

MLflow 1.20.1 is a patch release containing the following bug fixes:

- Avoid calling `importlib_metadata.packages_distributions` upon `mlflow.utils.requirements_utils` import (#4741, @dbczumar)
- Avoid depending on `importlib_metadata==4.7.0` (#4740, @dbczumar)

## 1.20.0 (2021-08-25)

MLflow 1.20.0 includes several major features and improvements:

Features:

- Autologging for scikit-learn now records post training metrics when scikit-learn evaluation APIs, such as `sklearn.metrics.mean_squared_error`, are called (#4491, #4628 #4638, @WeichenXu123)
- Autologging for PySpark ML now records post training metrics when model evaluation APIs, such as `Evaluator.evaluate()`, are called (#4686, @WeichenXu123)
- Add `pip_requirements` and `extra_pip_requirements` to `mlflow.*.log_model` and `mlflow.*.save_model` for directly specifying the pip requirements of the model to log / save (#4519, #4577, #4602, @harupy)
- Added `stdMetrics` entries to the training metrics recorded during PySpark CrossValidator autologging (#4672, @WeichenXu123)
- MLflow UI updates:
  1. Improved scalability of the parallel coordinates plot for run performance comparison,
  2. Added support for filtering runs based on their start time on the experiment page,
  3. Added a dropdown for runs table column sorting on the experiment page,
  4. Upgraded the AG Grid plugin, which is used for runs table loading on the experiment page, to version 25.0.0,
  5. Fixed a bug on the experiment page that caused the metrics section of the runs table to collapse when selecting columns from other table sections (#4712, @dbczumar)
- Added support for distributed execution to autologging for PyTorch Lightning (#4717, @dbczumar)
- Expanded R support for Model Registry functionality (#4527, @bramrodenburg)
- Added model scoring server support for defining custom prediction response wrappers (#4611, @Ark-kun)
- `mlflow.*.log_model` and `mlflow.*.save_model` now automatically infer the pip requirements of the model to log / save based on the current software environment (#4518, @harupy)
- Introduced support for running Sagemaker Batch Transform jobs with MLflow Models (#4410, #4589, @YQ-Wang)

Bug fixes and documentation updates:

- Deprecate `requirements_file` argument for `mlflow.*.save_model` and `mlflow.*.log_model` (#4620, @harupy)
- set nextPageToken to null (#4729, @harupy)
- Fix a bug in MLflow UI where the pagination token for run search is not refreshed when switching experiments (#4709, @harupy)
- Fix a bug in the model scoring server that rejected requests specifying a valid `Content-Type` header with the charset parameter (#4609, @Ark-kun)
- Fixed a bug that caused SQLAlchemy backends to exhaust DB connections. (#4663, @arpitjasa-db)
- Improve docker build procedures to raise exceptions if docker builds fail (#4610, @Ark-kun)
- Disable autologging for scikit-learn `cross_val_*` APIs, which are incompatible with autologging (#4590, @WeichenXu123)
- Deprecate MLflow Models support for fast.ai V1 (#4728, @dbczumar)
- Deprecate the old Azure ML deployment APIs `mlflow.azureml.cli.build_image` and `mlflow.azureml.build_image` (#4646, @trangevi)
- Deprecate MLflow Models support for TensorFlow < 2.0 and Keras < 2.3 (#4716, @harupy)

Small bug fixes and doc updates (#4730, #4722, #4725, #4723, #4703, #4710, #4679, #4694, #4707, #4708, #4706, #4705, #4625, #4701, #4700, #4662, #4699, #4682, #4691, #4684, #4683, #4675, #4666, #4648, #4653, #4651, #4641, #4649, #4627, #4637, #4632, #4634, #4621, #4619, #4622, #4460, #4608, #4605, #4599, #4600, #4581, #4583, #4565, #4575, #4564, #4580, #4572, #4570, #4574, #4576, #4568, #4559, #4537, #4542, @harupy; #4698, #4573, @Ark-kun; #4674, @kvmakes; #4555, @vagoston; #4644, @zhengjxu; #4690, #4588, @apurva-koti; #4545, #4631, #4734, @WeichenXu123; #4633, #4292, @shrinath-suresh; #4711, @jinzhang21; #4688, @murilommen; #4635, @ryan-duve; #4724, #4719, #4640, #4639, #4629, #4612, #4613, #4586, @dbczumar)

## 1.19.0 (2021-07-14)

MLflow 1.19.0 includes several major features and improvements:

Features:

- Add support for plotting per-class feature importance computed on linear boosters in XGBoost autologging (#4523, @dbczumar)
- Add `mlflow_create_registered_model` and `mlflow_delete_registered_model` for R to create/delete registered models.
- Add support for setting tags while resuming a run (#4497, @dbczumar)
- MLflow UI updates (#4490, @sunishsheth2009)

  - Add framework for internationalization support.
  - Move metric columns before parameter and tag columns in the runs table.
  - Change the display format of run start time to elapsed time (e.g. 3 minutes ago) from timestamp (e.g. 2021-07-14 14:02:10) in the runs table.

Bug fixes and documentation updates:

- Fix a bug causing MLflow UI to crash when sorting a column containing both `NaN` and empty values (#3409, @harupy)

Small bug fixes and doc updates (#4541, #4534, #4533, #4517, #4508, #4513, #4512, #4509, #4503, #4486, #4493, #4469, @harupy; #4458, @KasirajanA; #4501, @jimmyxu-db; #4521, #4515, @jerrylian-db; #4359, @shrinath-suresh; #4544, @WeichenXu123; #4549, @smurching; #4554, @derkomai; #4506, @tomasatdatabricks; #4551, #4516, #4494, @dbczumar; #4511, @keypointt)

## 1.18.0 (2021-06-18)

MLflow 1.18.0 includes several major features and improvements:

Features:

- Autologging performance improvements for XGBoost, LightGBM, and scikit-learn (#4416, #4473, @dbczumar)
- Add new PaddlePaddle flavor to MLflow Models (#4406, #4439, @jinminhao)
- Introduce paginated ListExperiments API (#3881, @wamartin-aml)
- Include Runtime version for MLflow Models logged on Databricks (#4421, @stevenchen-db)
- MLflow Models now log dependencies in pip requirements.txt format, in addition to existing conda format (#4409, #4422, @stevenchen-db)
- Add support for limiting the number child runs created by autologging for scikit-learn hyperparameter search models (#4382, @mohamad-arabi)
- Improve artifact upload / download performance on Databricks (#4260, @dbczumar)
- Migrate all model dependencies from conda to "pip" section (#4393, @WeichenXu123)

Bug fixes and documentation updates:

- Fix an MLflow UI bug that caused git source URIs to be rendered improperly (#4403, @takabayashi)
- Fix a bug that prevented reloading of MLflow Models based on the TensorFlow SavedModel format (#4223) (#4319, @saschaschramm)
- Fix a bug in the behavior of `KubernetesSubmittedRun.get_status()` for Kubernetes MLflow Project runs (#3962) (#4159, @jcasse)
- Fix a bug in TLS verification for MLflow artifact operations on S3 (#4047, @PeterSulcs)
- Fix a bug causing the MLflow server to crash after deletion of the default experiment (#4352, @asaf400)
- Fix a bug causing `mlflow models serve` to crash on Windows 10 (#4377, @simonvanbernem)
- Fix a crash in runs search when ordering by metric values against the MSSQL backend store (#2551) (#4238, @naor2013)
- Fix an autologging incompatibility issue with TensorFlow 2.5 (#4371, @dbczumar)
- Fix a bug in the `disable_for_unsupported_versions` autologging argument that caused library versions to be incorrectly compared (#4303, @WeichenXu123)

Small bug fixes and doc updates (#4405, @mohamad-arabi; #4455, #4461, #4459, #4464, #4453, #4444, #4449, #4301, #4424, #4418, #4417, #3759, #4398, #4389, #4386, #4385, #4384, #4380, #4373, #4378, #4372, #4369, #4348, #4364, #4363, #4349, #4350, #4174, #4285, #4341, @harupy; #4446, @kHarshit; #4471, @AveshCSingh; #4435, #4440, #4368, #4360, @WeichenXu123; #4431, @apurva-koti; #4428, @stevenchen-db; #4467, #4402, #4261, @dbczumar)

## 1.17.0 (2021-05-07)

MLflow 1.17.0 includes several major features and improvements:

Features:

- Add support for hyperparameter-tuning models to `mlflow.pyspark.ml.autolog()` (#4270, @WeichenXu123)

Bug fixes and documentation updates:

- Fix PyTorch Lightning callback definition for compatibility with PyTorch Lightning 1.3.0 (#4333, @dbczumar)
- Fix a bug in scikit-learn autologging that omitted artifacts for unsupervised models (#4325, @dbczumar)
- Support logging `datetime.date` objects as part of model input examples (#4313, @vperiyasamy)
- Implement HTTP request retries in the MLflow Java client for 500-level responses (#4311, @dbczumar)
- Include a community code of conduct (#4310, @dennyglee)

Small bug fixes and doc updates (#4276, #4263, @WeichenXu123; #4289, #4302, #3599, #4287, #4284, #4265, #4266, #4275, #4268, @harupy; #4335, #4297, @dbczumar; #4324, #4320, @tleyden)

## 1.16.0 (2021-04-22)

MLflow 1.16.0 includes several major features and improvements:

Features:

- Add `mlflow.pyspark.ml.autolog()` API for autologging of `pyspark.ml` estimators (#4228, @WeichenXu123)
- Add `mlflow.catboost.log_model`, `mlflow.catboost.save_model`, `mlflow.catboost.load_model` APIs for CatBoost model persistence (#2417, @harupy)
- Enable `mlflow.pyfunc.spark_udf` to use column names from model signature by default (#4236, @Loquats)
- Add `datetime` data type for model signatures (#4241, @vperiyasamy)
- Add `mlflow.sklearn.eval_and_log_metrics` API that computes and logs metrics for the given scikit-learn model and labeled dataset. (#4218, @alkispoly-db)

Bug fixes and documentation updates:

- Fix a database migration error for PostgreSQL (#4211, @dolfinus)
- Fix autologging silent mode bugs (#4231, @dbczumar)

Small bug fixes and doc updates (#4255, #4252, #4254, #4253, #4242, #4247, #4243, #4237, #4233, @harupy; #4225, @dmatrix; #4207, @shrinath-suresh; #4264, @WeichenXu123; #3884, #3866, #3885, @ankan94; #4274, #4216, @dbczumar)

## 1.15.0 (2021-03-26)

MLflow 1.15.0 includes several features, bug fixes and improvements. Notably, it includes a number of improvements to MLflow autologging:

Features:

- Add `silent=False` option to all autologging APIs, to allow suppressing MLflow warnings and logging statements during autologging setup and training (#4173, @dbczumar)
- Add `disable_for_unsupported_versions=False` option to all autologging APIs, to disable autologging for versions of ML frameworks that have not been explicitly tested against the current version of the MLflow client (#4119, @WeichenXu123)

Bug fixes:

- Autologged runs are now terminated when execution is interrupted via SIGINT (#4200, @dbczumar)
- The R `mlflow_get_experiment` API now returns the same tag structure as `mlflow_list_experiments` and `mlflow_get_run` (#4017, @lorenzwalthert)
- Fix bug where `mlflow.tensorflow.autolog` would previously mutate the user-specified callbacks list when fitting `tf.keras` models (#4195, @dbczumar)
- Fix bug where SQL-backed MLflow tracking server initialization failed when using the MLflow skinny client (#4161, @eedeleon)
- Model version creation (e.g. via `mlflow.register_model`) now fails if the model version status is not READY (#4114, @ankit-db)

Small bug fixes and doc updates (#4191, #4149, #4162, #4157, #4155, #4144, #4141, #4138, #4136, #4133, #3964, #4130, #4118, @harupy; #4139, @WeichenXu123; #4193, @smurching; #4029, @architkulkarni; #4134, @xhochy; #4116, @wenleix; #4160, @wentinghu; #4203, #4184, #4167, @dbczumar)

## 1.14.1 (2021-03-01)

MLflow 1.14.1 is a patch release containing the following bug fix:

- Fix issues in handling flexible numpy datatypes in TensorSpec (#4147, @arjundc-db)

## 1.14.0 (2021-02-18)

MLflow 1.14.0 includes several major features and improvements:

- MLflow's model inference APIs (`mlflow.pyfunc.predict`), built-in model serving tools (`mlflow models serve`), and model signatures now support tensor inputs. In particular, MLflow now provides built-in support for scoring PyTorch, TensorFlow, Keras, ONNX, and Gluon models with tensor inputs. For more information, see https://mlflow.org/docs/latest/models.html#deploy-mlflow-models (#3808, #3894, #4084, #4068 @wentinghu; #4041 @tomasatdatabricks, #4099, @arjundc-db)
- Add new `mlflow.shap.log_explainer`, `mlflow.shap.load_explainer` APIs for logging and loading `shap.Explainer` instances (#3989, @vivekchettiar)
- The MLflow Python client is now available with a reduced dependency set via the `mlflow-skinny` PyPI package (#4049, @eedeleon)
- Add new `RequestHeaderProvider` plugin interface for passing custom request headers with REST API requests made by the MLflow Python client (#4042, @jimmyxu-db)
- `mlflow.keras.log_model` now saves models in the TensorFlow SavedModel format by default instead of the older Keras H5 format (#4043, @harupy)
- `mlflow_log_model` now supports logging MLeap models in R (#3819, @yitao-li)
- Add `mlflow.pytorch.log_state_dict`, `mlflow.pytorch.load_state_dict` for logging and loading PyTorch state dicts (#3705, @shrinath-suresh)
- `mlflow gc` can now garbage-collect artifacts stored in S3 (#3958, @sklingel)

Bug fixes and documentation updates:

- Enable autologging for TensorFlow estimators that extend `tensorflow.compat.v1.estimator.Estimator` (#4097, @mohamad-arabi)
- Fix for universal autolog configs overriding integration-specific configs (#4093, @dbczumar)
- Allow `mlflow.models.infer_signature` to handle dataframes containing `pandas.api.extensions.ExtensionDtype` (#4069, @caleboverman)
- Fix bug where `mlflow_restore_run` doesn't propagate the `client` parameter to `mlflow_get_run` (#4003, @yitao-li)
- Fix bug where scoring on served model fails when request data contains a string that looks like URL and pandas version is later than 1.1.0 (#3921, @Secbone)
- Fix bug causing `mlflow_list_experiments` to fail listing experiments with tags (#3942, @lorenzwalthert)
- Fix bug where metrics plots are computed from incorrect target values in scikit-learn autologging (#3993, @mtrencseni)
- Remove redundant / verbose Python event logging message in autologging (#3978, @dbczumar)
- Fix bug where `mlflow_load_model` doesn't load metadata associated to MLflow model flavor in R (#3872, @yitao-li)
- Fix `mlflow.spark.log_model`, `mlflow.spark.load_model` APIs on passthrough-enabled environments against ACL'd artifact locations (#3443, @smurching)

Small bug fixes and doc updates (#4102, #4101, #4096, #4091, #4067, #4059, #4016, #4054, #4052, #4051, #4038, #3992, #3990, #3981, #3949, #3948, #3937, #3834, #3906, #3774, #3916, #3907, #3938, #3929, #3900, #3902, #3899, #3901, #3891, #3889, @harupy; #4014, #4001, @dmatrix; #4028, #3957, @dbczumar; #3816, @lorenzwalthert; #3939, @pauldj54; #3740, @jkthompson; #4070, #3946, @jimmyxu-db; #3836, @t-henri; #3982, @neo-anderson; #3972, #3687, #3922, @eedeleon; #4044, @WeichenXu123; #4063, @yitao-li; #3976, @whiteh; #4110, @tomasatdatabricks; #4050, @apurva-koti; #4100, #4084, @wentinghu; #3947, @vperiyasamy; #4021, @trangevi; #3773, @ankan94; #4090, @jinzhang21; #3918, @danielfrg)

## 1.13.1 (2020-12-30)

MLflow 1.13.1 is a patch release containing bug fixes and small changes:

- Fix bug causing Spark autologging to ignore configuration options specified by `mlflow.autolog()` (#3917, @dbczumar)
- Fix bugs causing metrics to be dropped during TensorFlow autologging (#3913, #3914, @dbczumar)
- Fix incorrect value of optimizer name parameter in autologging PyTorch Lightning (#3901, @harupy)
- Fix model registry database `allow_null_for_run_id` migration failure affecting MySQL databases (#3836, @t-henri)
- Fix failure in `transition_model_version_stage` when uncanonical stage name is passed (#3929, @harupy)
- Fix an undefined variable error causing AzureML model deployment to fail (#3922, @eedeleon)
- Reclassify scikit-learn as a pip dependency in MLflow Model conda environments (#3896, @harupy)
- Fix experiment view crash and artifact view inconsistency caused by artifact URIs with redundant slashes (#3928, @dbczumar)

## 1.13 (2020-12-22)

MLflow 1.13 includes several major features and improvements:

Features:

New fluent APIs for logging in-memory objects as artifacts:

- Add `mlflow.log_text` which logs text as an artifact (#3678, @harupy)
- Add `mlflow.log_dict` which logs a dictionary as an artifact (#3685, @harupy)
- Add `mlflow.log_figure` which logs a figure object as an artifact (#3707, @harupy)
- Add `mlflow.log_image` which logs an image object as an artifact (#3728, @harupy)

UI updates / fixes (#3867, @smurching):

- Add model version link in compact experiment table view
- Add logged/registered model links in experiment runs page view
- Enhance artifact viewer for MLflow models
- Model registry UI settings are now persisted across browser sessions
- Add model version `description` field to model version table

Autologging enhancements:

- Improve robustness of autologging integrations to exceptions (#3682, #3815, dbczumar; #3860, @mohamad-arabi; #3854, #3855, #3861, @harupy)
- Add `disable` configuration option for autologging (#3682, #3815, dbczumar; #3838, @mohamad-arabi; #3854, #3855, #3861, @harupy)
- Add `exclusive` configuration option for autologging (#3851, @apurva-koti; #3869, @dbczumar)
- Add `log_models` configuration option for autologging (#3663, @mohamad-arabi)
- Set tags on autologged runs for easy identification (and add tags to start_run) (#3847, @dbczumar)

More features and improvements:

- Allow Keras models to be saved with `SavedModel` format (#3552, @skylarbpayne)
- Add support for `statsmodels` flavor (#3304, @olbapjose)
- Add support for nested-run in mlflow R client (#3765, @yitao-li)
- Deploying a model using `mlflow.azureml.deploy` now integrates better with the AzureML tracking/registry. (#3419, @trangevi)
- Update schema enforcement to handle integers with missing values (#3798, @tomasatdatabricks)

Bug fixes and documentation updates:

- When running an MLflow Project on Databricks, the version of MLflow installed on the Databricks cluster will now match the version used to run the Project (#3880, @FlorisHoogenboom)
- Fix bug where metrics are not logged for single-epoch `tf.keras` training sessions (#3853, @dbczumar)
- Reject boolean types when logging MLflow metrics (#3822, @HCoban)
- Fix alignment of Keras / `tf.Keras` metric history entries when `initial_epoch` is different from zero. (#3575, @garciparedes)
- Fix bugs in autologging integrations for newer versions of TensorFlow and Keras (#3735, @dbczumar)
- Drop global `filterwwarnings` module at import time (#3621, @jogo)
- Fix bug that caused preexisting Python loggers to be disabled when using MLflow with the SQLAlchemyStore (#3653, @arthury1n)
- Fix `h5py` library incompatibility for exported Keras models (#3667, @tomasatdatabricks)

Small changes, bug fixes and doc updates (#3887, #3882, #3845, #3833, #3830, #3828, #3826, #3825, #3800, #3809, #3807, #3786, #3794, #3731, #3776, #3760, #3771, #3754, #3750, #3749, #3747, #3736, #3701, #3699, #3698, #3658, #3675, @harupy; #3723, @mohamad-arabi; #3650, #3655, @shrinath-suresh; #3850, #3753, #3725, @dmatrix; ##3867, #3670, #3664, @smurching; #3681, @sueann; #3619, @andrewnitu; #3837, @javierluraschi; #3721, @szczeles; #3653, @arthury1n; #3883, #3874, #3870, #3877, #3878, #3815, #3859, #3844, #3703, @dbczumar; #3768, @wentinghu; #3784, @HCoban; #3643, #3649, @arjundc-db; #3864, @AveshCSingh, #3756, @yitao-li)

## 1.12.1 (2020-11-19)

MLflow 1.12.1 is a patch release containing bug fixes and small changes:

- Fix `run_link` for cross-workspace model versions (#3681, @sueann)
- Remove hard dependency on matplotlib for sklearn autologging (#3703, @dbczumar)
- Do not disable existing loggers when initializing alembic (#3653, @arthury1n)

## 1.12.0 (2020-11-10)

MLflow 1.12.0 includes several major features and improvements, in particular a number of improvements to autologging and MLflow's Pytorch integrations:

Features:

Autologging:

- Add universal `mlflow.autolog` which enables autologging for all supported integrations (#3561, #3590, @andrewnitu)
- Add `mlflow.pytorch.autolog` API for automatic logging of metrics, params, and models from Pytorch Lightning training (#3601, @shrinath-suresh, #3636, @karthik-77). This API is also enabled by `mlflow.autolog`.
- Scikit-learn, XGBoost, and LightGBM autologging now support logging model signatures and input examples (#3386, #3403, #3449, @andrewnitu)
- `mlflow.sklearn.autolog` now supports logging metrics (e.g. accuracy) and plots (e.g. confusion matrix heat map) (#3423, #3327, @willzhan-db, @harupy)

PyTorch:

- `mlflow.pytorch.log_model`, `mlflow.pytorch.load_model` now support logging/loading TorchScript models (#3557, @shrinath-suresh)
- `mlflow.pytorch.log_model` supports passing `requirements_file` & `extra_files` arguments to log additional artifacts along with a model (#3436, @shrinath-suresh)

More features and improvements:

- Add `mlflow.shap.log_explanation` for logging model explanations generated by SHAP (#3513, @harupy)
- `log_model` and `create_model_version` now supports an `await_creation_for` argument (#3376, @andychow-db)
- Put preview paths before non-preview paths for backwards compatibility (#3648, @sueann)
- Clean up model registry endpoint and client method definitions (#3610, @sueann)
- MLflow deployments plugin now supports 'predict' CLI command (#3597, @shrinath-suresh)
- Support H2O for R (#3416, @yitao-li)
- Add `MLFLOW_S3_IGNORE_TLS` environment variable to enable skipping TLS verification of S3 endpoint (#3345, @dolfinus)

Bug fixes and documentation updates:

- Ensure that results are synced across distributed processes if ddp enabled (no-op else) (#3651, @SeanNaren)
- Remove optimizer step override to ensure that all accelerator cases are covered by base module (#3635, @SeanNaren)
- Fix `AttributeError` in keras autologgging (#3611, @sephib)
- Scikit-learn autologging: Exclude feature extraction / selection estimator (#3600, @dbczumar)
- Scikit-learn autologging: Fix behavior when a child and its parent are both patched (#3582, @dbczumar)
- Fix a bug where `lightgbm.Dataset(None)` fails after running `mlflow.lightgbm.autolog` (#3594, @harupy)
- Fix a bug where `xgboost.DMatrix(None)` fails after running `mlflow.xgboost.autolog` (#3584, @harupy)
- Pass `docker_args` in non-synchronous mlflow project runs (#3563, @alfozan)
- Fix a bug of `FTPArtifactRepository.log_artifacts` with `artifact_path` keyword argument (issue #3388) (#3391, @kzm4269)
- Exclude preprocessing & imputation steps from scikit-learn autologging (#3491, @dbczumar)
- Fix duplicate stderr logging during artifact logging and project execution in the R client (#3145, @yitao-li)
- Don't call `atexit.register(_flush_queue)` in `__main__` scope of `mlflow/tensorflow.py` (#3410, @harupy)
- Fix for restarting terminated run not setting status correctly (#3329, @apurva-koti)
- Fix model version run_link URL for some Databricks regions (#3417, @sueann)
- Skip JSON validation when endpoint is not MLflow REST API (#3405, @harupy)
- Document `mlflow-torchserve` plugin (#3634, @karthik-77)
- Add `mlflow-elasticsearchstore` to the doc (#3462, @AxelVivien25)
- Add code snippets for fluent and MlflowClient APIs (#3385, #3437, #3489 #3573, @dmatrix)
- Document `mlflow-yarn` backend (#3373, @fhoering)
- Fix a breakage in loading Tensorflow and Keras models (#3667, @tomasatdatabricks)

Small bug fixes and doc updates (#3607, #3616, #3534, #3598, #3542, #3568, #3349, #3554, #3544, #3541, #3533, #3535, #3516, #3512, #3497, #3522, #3521, #3492, #3502, #3434, #3422, #3394, #3387, #3294, #3324, #3654, @harupy; #3451, @jgc128; #3638, #3632, #3608, #3452, #3399, @shrinath-suresh; #3495, #3459, #3662, #3668, #3670 @smurching; #3488, @edgan8; #3639, @karthik-77; #3589, #3444, #3276, @lorenzwalthert; #3538, #3506, #3509, #3507, #3510, #3508, @rahulporuri; #3504, @sbrugman; #3486, #3466, @apurva-koti; #3477, @juntai-zheng; #3617, #3609, #3605, #3603, #3560, @dbczumar; #3411, @danielvdende; #3377, @willzhan-db; #3420, #3404, @andrewnitu; #3591, @mateiz; #3465, @abawchen; #3543, @emptalk; #3302, @bramrodenburg; #3468, @ghisvail; #3496, @extrospective; #3549, #3501, #3435, @yitao-li; #3243, @OlivierBondu; #3439, @andrewnitu; #3651, #3635 @SeanNaren, #3470, @ankit-db)

## 1.11.0 (2020-08-31)

MLflow 1.11.0 includes several major features and improvements:

Features:

- New `mlflow.sklearn.autolog()` API for automatic logging of metrics, params, and models from scikit-learn model training (#3287, @harupy; #3323, #3358 @dbczumar)
- Registered model & model version creation APIs now support specifying an initial `description` (#3271, @sueann)
- The R `mlflow_log_model` and `mlflow_load_model` APIs now support XGBoost models (#3085, @lorenzwalthert)
- New `mlflow.list_run_infos` fluent API for listing run metadata (#3183, @trangevi)
- Added section for visualizing and comparing model schemas to model version and model-version-comparison UIs (#3209, @zhidongqu-db)
- Enhanced support for using the model registry across Databricks workspaces: support for registering models to a Databricks workspace from outside the workspace (#3119, @sueann), tracking run-lineage of these models (#3128, #3164, @ankitmathur-db; #3187, @harupy), and calling `mlflow.<flavor>.load_model` against remote Databricks model registries (#3330, @sueann)
- UI support for setting/deleting registered model and model version tags (#3187, @harupy)
- UI support for archiving existing staging/production versions of a model when transitioning a new model version to staging/production (#3134, @harupy)

Bug fixes and documentation updates:

- Fixed parsing of MLflow project parameter values containing'=' (#3347, @dbczumar)
- Fixed a bug preventing listing of WASBS artifacts on the latest version of Azure Blob Storage (12.4.0) (#3348, @dbczumar)
- Fixed a bug where artifact locations become malformed when using an SFTP file store in Windows (#3168, @harupy)
- Fixed bug where `list_artifacts` returned incorrect results on GCS, preventing e.g. loading SparkML models from GCS (#3242, @santosh1994)
- Writing and reading artifacts via `MlflowClient` to a DBFS location in a Databricks tracking server specified through the `tracking_uri` parameter during the initialization of `MlflowClient` now works properly (#3220, @sueann)
- Fixed bug where `FTPArtifactRepository` returned artifact locations as absolute paths, rather than paths relative to the artifact repository root (#3210, @shaneing), and bug where calling `log_artifacts` against an FTP artifact location copied the logged directory itself into the FTP location, rather than the contents of the directory.
- Fixed bug where Databricks project execution failed due to passing of GET request params as part of the request body rather than as query parameters (#2947, @cdemonchy-pro)
- Fix bug where artifact viewer did not correctly render PDFs in MLflow 1.10 (#3172, @ankitmathur-db)
- Fixed parsing of `order_by` arguments to MLflow search APIs when ordering by fields whose names contain spaces (#3118, @jdlesage)
- Fixed bug where MLflow model schema enforcement raised exceptions when validating string columns using pandas >= 1.0 (#3130, @harupy)
- Fixed bug where `mlflow.spark.log_model` did not save model signature and input examples (#3151, @harupy)
- Fixed bug in runs UI where tags table did not reflect deletion of tags. (#3135, @ParseDark)
- Added example illustrating the use of RAPIDS with MLflow (#3028, @drobison00)

Small bug fixes and doc updates (#3326, #3344, #3314, #3289, #3225, #3288, #3279, #3265, #3263, #3260, #3255, #3267, #3266, #3264, #3256, #3253, #3231, #3245, #3191, #3238, #3192, #3188, #3189, #3180, #3178, #3166, #3181, #3142, #3165, #2960, #3129, #3244, #3359 @harupy; #3236, #3141, @AveshCSingh; #3295, #3163, @arjundc-db; #3241, #3200, @zhidongqu-db; #3338, #3275, @sueann; #3020, @magnus-m; #3322, #3219, @dmatrix; #3341, #3179, #3355, #3360, #3363 @smurching; #3124, @jdlesage; #3232, #3146, @ankitmathur-db; #3140, @andreakress; #3062, @cafeal; #3193, @tomasatdatabricks; 3115, @fhoering; #3328, @apurva-koti; #3046, @OlivierBondu; #3194, #3158, @dmatrix; #3250, @shivp950; #3259, @simonhessner; #3357 @dbczumar)

## 1.10.0 (2020-07-20)

MLflow 1.10.0 includes several major features and improvements, in particular the release of several new model registry Python client APIs.

Features:

- `MlflowClient.transition_model_version_stage` now supports an
  `archive_existing_versions` argument for archiving existing staging or production model
  versions when transitioning a new model version to staging or production (#3095, @harupy)
- Added `set_registry_uri`, `get_registry_uri` APIs. Setting the model registry URI causes
  fluent APIs like `mlflow.register_model` to communicate with the model registry at the specified
  URI (#3072, @sueann)
- Added paginated `MlflowClient.search_registered_models` API (#2939, #3023, #3027 @ankitmathur-db; #2966, @mparkhe)
- Added syntax highlighting when viewing text files (YAML etc) in the MLflow runs UI (#3041, @harupy)
- Added REST API and Python client support for setting and deleting tags on model versions and registered models,
  via the `MlflowClient.create_registered_model`, `MlflowClient.create_model_version`,
  `MlflowClient.set_registered_model_tag`, `MlflowClient.set_model_version_tag`,
  `MlflowClient.delete_registered_model_tag`, and `MlflowClient.delete_model_version_tag` APIs (#3094, @zhidongqu-db)

Bug fixes and documentation updates:

- Removed usage of deprecated `aws ecr get-login` command in `mlflow.sagemaker` (#3036, @mrugeles)
- Fixed bug where artifacts could not be viewed and downloaded from the artifact UI when using
  Azure Blob Storage (#3014, @Trollgeir)
- Databricks credentials are now propagated to the project subprocess when running MLflow projects
  within a notebook (#3035, @smurching)
- Added docs explaining how to fetching an MLflow model from the model registry (#3000, @andychow-db)

Small bug fixes and doc updates (#3112, #3102, #3089, #3103, #3096, #3090, #3049, #3080, #3070, #3078, #3083, #3051, #3050, #2875, #2982, #2949, #3121 @harupy; #3082, @ankitmathur-db; #3084, #3019, @smurching)

## 1.9.1 (2020-06-25)

MLflow 1.9.1 is a patch release containing a number of bug-fixes and improvements:

Bug fixes and improvements:

- Fixes `AttributeError` when pickling an instance of the Python `MlflowClient` class (#2955, @Polyphenolx)
- Fixes bug that prevented updating model-version descriptions in the model registry UI (#2969, @AnastasiaKol)
- Fixes bug where credentials were not properly propagated to artifact CLI commands when logging artifacts from Java to the DatabricksArtifactRepository (#3001, @dbczumar)
- Removes use of new Pandas API in new MLflow model-schema functionality, so that it can be used with older Pandas versions (#2988, @aarondav)

Small bug fixes and doc updates (#2998, @dbczumar; #2999, @arjundc-db)

## 1.9.0 (2020-06-19)

MLflow 1.9.0 includes numerous major features and improvements, and a breaking change to
experimental APIs:

Breaking Changes:

- The `new_name` argument to `MlflowClient.update_registered_model`
  has been removed. Call `MlflowClient.rename_registered_model` instead. (#2946, @mparkhe)
- The `stage` argument to `MlflowClient.update_model_version`
  has been removed. Call `MlflowClient.transition_model_version_stage` instead. (#2946, @mparkhe)

Features (MLflow Models and Flavors)

- `log_model` and `save_model` APIs now support saving model signatures (the model's input and output schema)
  and example input along with the model itself (#2698, #2775, @tomasatdatabricks). Model signatures are used
  to reorder and validate input fields when scoring/serving models using the pyfunc flavor, `mlflow models`
  CLI commands, or `mlflow.pyfunc.spark_udf` (#2920, @tomasatdatabricks and @aarondav)
- Introduce fastai model persistence and autologging APIs under `mlflow.fastai` (#2619, #2689 @antoniomdk)
- Add pluggable `mlflow.deployments` API and CLI for deploying models to custom serving tools, e.g. RedisAI
  (#2327, @hhsecond)
- Enables loading and scoring models whose conda environments include dependencies in conda-forge (#2797, @dbczumar)
- Add support for scoring ONNX-persisted models that return Python lists (#2742, @andychow-db)

Features (MLflow Projects)

- Add plugin interface for executing MLflow projects against custom backends (#2566, @jdlesage)
- Add ability to specify additional cluster-wide Python and Java libraries when executing
  MLflow projects remotely on Databricks (#2845, @pogil)
- Allow running MLflow projects against remote artifacts stored in any location with a corresponding
  ArtifactRepository implementation (Azure Blob Storage, GCS, etc) (#2774, @trangevi)
- Allow MLflow projects running on Kubernetes to specify a different tracking server to log to via the
  `KUBE_MLFLOW_TRACKING_URI` for passing a different tracking server to the kubernetes job (#2874, @catapulta)

Features (UI)

- Significant performance and scalability improvements to metric comparison and scatter plots in
  the UI (#2447, @mjlbach)
- The main MLflow experiment list UI now includes a link to the model registry UI (#2805, @zhidongqu-db),
- Enable viewing PDFs logged as artifacts from the runs UI (#2859, @ankmathur96)
- UI accessibility improvements: better color contrast (#2872, @Zangr), add child roles to DOM elements (#2871, @Zangr)

Features (Tracking Client and Server)

- Adds ability to pass client certs as part of REST API requests when using the tracking or model
  registry APIs. (#2843, @PhilipMay)
- New community plugin: support for storing artifacts in Aliyun (Alibaba Cloud) (#2917, @SeaOfOcean)
- Infer and set content type and encoding of objects when logging models and artifacts to S3 (#2881, @hajapy)
- Adds support for logging artifacts to HDFS Federation ViewFs (#2782, @fhoering)
- Add healthcheck endpoint to the MLflow server at `/health` (#2725, @crflynn)
- Improves performance of default file-based tracking storage backend by using LibYAML (if installed)
  to read experiment and run metadata (#2707, @Higgcz)

Bug fixes and documentation updates:

- Several UI fixes: remove margins around icon buttons (#2827, @harupy),
  fix alignment issues in metric view (#2811, @zhidongqu-db), add handling of `NaN`
  values in metrics plot (#2773, @dbczumar), truncate run ID in the run name when
  comparing multiple runs (#2508, @harupy)
- Database engine URLs are no longer logged when running `mlflow db upgrade` (#2849, @hajapy)
- Updates `log_artifact`, `log_model` APIs to consistently use posix paths, rather than OS-dependent
  paths, when computing artifact subpaths. (#2784, @mikeoconnor0308)
- Fix `ValueError` when scoring `tf.keras` 1.X models using `mlflow.pyfunc.predict` (#2762, @juntai-zheng)
- Fixes conda environment activation bug when running MLflow projects on Windows (#2731, @MynherVanKoek)
- `mlflow.end_run` will now clear the active run even if the run cannot be marked as
  terminated (e.g. because it's been deleted), (#2693, @ahmed-shariff)
- Add missing documentation for `mlflow.spacy` APIs (#2771, @harupy)

Small bug fixes and doc updates (#2919, @willzhan-db; #2940, #2942, #2941, #2943, #2927, #2929, #2926, #2914, #2928, #2913, #2852, #2876, #2808, #2810, #2442, #2780, #2758, #2732, #2734, #2431, #2733, #2716, @harupy; #2915, #2897, @jwgwalton; #2856, @jkthompson; #2962, @hhsecond; #2873, #2829, #2582, @dmatrix; #2908, #2865, #2880, #2866, #2833, #2785, #2723, @smurching; #2906, @dependabot[bot]; #2724, @aarondav; #2896, @ezeeetm; #2864, @tallen94; #2726, @crflynn; #2710, #2951 @mparkhe; #2935, #2921, @ankitmathur-db; #2963, #2739, @dbczumar; #2853, @stat4jason; #2709, #2792, @juntai-zheng @juntai-zheng; #2749, @HiromuHota; #2957, #2911, #2718, @arjundc-db; #2885, @willzhan-db; #2803, #2761, @pogil; #2392, @jnmclarty; #2794, @Zethson; #2766, #2916 @shubham769)

## 1.8.0 (2020-04-16)

MLflow 1.8.0 includes several major features and improvements:

Features:

- Added `mlflow.azureml.deploy` API for deploying MLflow models to AzureML (#2375 @csteegz, #2711, @akshaya-a)
- Added support for case-sensitive LIKE and case-insensitive ILIKE queries (e.g. `'params.framework LIKE '%sklearn%'`) with the SearchRuns API & UI when running against a SQLite backend (#2217, @t-henri; #2708, @mparkhe)
- Improved line smoothing in MLflow metrics UI using exponential moving averages (#2620, @Valentyn1997)
- Added `mlflow.spacy` module with support for logging and loading spaCy models (#2242, @arocketman)
- Parameter values that differ across runs are highlighted in run comparison UI (#2565, @gabrielbretschner)
- Added ability to compare source runs associated with model versions from the registered model UI (#2537, @juntai-zheng)
- Added support for alphanumerical experiment IDs in the UI. (#2568, @jonas)
- Added support for passing arguments to `docker run` when running docker-based MLflow projects (#2608, @ksanjeevan)
- Added Windows support for `mlflow sagemaker build-and-push-container` CLI & API (#2500, @AndreyBulezyuk)
- Improved performance of reading experiment data from local filesystem when LibYAML is installed (#2707, @Higgcz)
- Added a healthcheck endpoint to the REST API server at `/health` that always returns a 200 response status code, to be used to verify health of the server (#2725, @crflynn)
- MLflow metrics UI plots now scale to rendering thousands of points using scattergl (#2447, @mjlbach)

Bug fixes:

- Fixed CLI summary message in `mlflow azureml build_image` CLI (#2712, @dbczumar)
- Updated `examples/flower_classifier/score_images_rest.py` with multiple bug fixes (#2647, @tfurmston)
- Fixed pip not found error while packaging models via `mlflow models build-docker` (#2699, @HiromuHota)
- Fixed bug in `mlflow.tensorflow.autolog` causing erroneous deletion of TensorBoard logging directory (#2670, @dbczumar)
- Fixed a bug that truncated the description of the `mlflow gc` subcommand in `mlflow --help` (#2679, @dbczumar)
- Fixed bug where `mlflow models build-docker` was failing due to incorrect Miniconda download URL (#2685, @michaeltinsley)
- Fixed a bug in S3 artifact logging functionality where `MLFLOW_S3_ENDPOINT_URL` was ignored (#2629, @poppash)
- Fixed a bug where Sqlite in-memory was not working as a tracking backend store by modifying DB upgrade logic (#2667, @dbczumar)
- Fixed a bug to allow numerical parameters with values >= 1000 in R `mlflow::mlflow_run()` API (#2665, @lorenzwalthert)
- Fixed a bug where AWS creds was not found in the Windows platform due path differences (#2634, @AndreyBulezyuk)
- Fixed a bug to add pip when necessary in `_mlflow_conda_env` (#2646, @tfurmston)
- Fixed error code to be more meaningful if input to model version is incorrect (#2625, @andychow-db)
- Fixed multiple bugs in model registry (#2638, @aarondav)
- Fixed support for conda env dicts with `mlflow.pyfunc.log_model` (#2618, @dbczumar)
- Fixed a bug where hiding the start time column in the UI would also hide run selection checkboxes (#2559, @harupy)

Documentation updates:

- Added links to source code to mlflow.org (#2627, @harupy)
- Documented fix for pandas-records payload (#2660, @SaiKiranBurle)
- Fixed documentation bug in TensorFlow `load_model` utility (#2666, @pogil)
- Added the missing Model Registry description and link on the first page (#2536, @dmatrix)
- Added documentation for expected datatype for step argument in `log_metric` to match REST API (#2654, @mparkhe)
- Added usage of the model registry to the `log_model` function in `sklearn_elasticnet_wine/train.py` example (#2609, @netanel246)

Small bug fixes and doc updates (#2594, @Trollgeir; #2703,#2709, @juntai-zheng; #2538, #2632, @keigohtr; #2656, #2553, @lorenzwalthert; #2622, @pingsutw; #1391, @sueann; #2613, #2598, #2534, #2723, @smurching; #2652, #2710, @mparkhe; #2706, #2653, #2639, @tomasatdatabricks; #2611, @9dogs; #2700, #2705, @aarondav; #2675, #2540, @mengxr; #2686, @RensDimmendaal; #2694, #2695, #2532, @dbczumar; #2733, #2716, @harupy; #2726, @crflynn; #2582, #2687, @dmatrix)

## 1.7.2 (2020-03-20)

MLflow 1.7.2 is a patch release containing a minor change:

- Pin alembic version to 1.4.1 or below to prevent pep517-related installation errors
  (#2612, @smurching)

## 1.7.1 (2020-03-17)

MLflow 1.7.1 is a patch release containing bug fixes and small changes:

- Remove usage of Nonnull annotations and findbugs dependency in Java package (#2583, @mparkhe)
- Add version upper bound (<=1.3.13) to sqlalchemy dependency in Python package (#2587, @smurching)

Other bugfixes and doc updates (#2595, @mparkhe; #2567, @jdlesage)

## 1.7.0 (2020-03-02)

MLflow 1.7.0 includes several major features and improvements, and some notable breaking changes:

MLflow support for Python 2 is now deprecated and will be dropped in a future release. At that
point, existing Python 2 workflows that use MLflow will continue to work without modification, but
Python 2 users will no longer get access to the latest MLflow features and bugfixes. We recommend
that you upgrade to Python 3 - see https://docs.python.org/3/howto/pyporting.html for a migration
guide.

Breaking changes to Model Registry REST APIs:

Model Registry REST APIs have been updated to be more consistent with the other MLflow APIs. With
this release Model Registry APIs are intended to be stable until the next major version.

- Python and Java client APIs for Model Registry have been updated to use the new REST APIs. When using an MLflow client with a server using updated REST endpoints, you won't need to change any code but will need to upgrade to a new client version. The client APIs contain deprecated arguments, which for this release are backward compatible, but will be dropped in future releases. (#2457, @tomasatdatabricks; #2502, @mparkhe).
- The Model Registry UI has been updated to use the new REST APIs (#2476 @aarondav; #2507, @mparkhe)

Other Features:

- Ability to click through to individual runs from metrics plot (#2295, @harupy)
- Added `mlflow gc` CLI for permanent deletion of runs (#2265, @t-henri)
- Metric plot state is now captured in page URLs for easier link sharing (#2393, #2408, #2498 @smurching; #2459, @harupy)
- Added experiment management to MLflow UI (create/rename/delete experiments) (#2348, @ggliem)
- Ability to search for experiments by name in the UI (#2324, @ggliem)
- MLflow UI page titles now reflect the content displayed on the page (#2420, @AveshCSingh)
- Added a new `LogModel` REST API endpoint for capturing model metadata, and call it from the Python and R clients (#2369, #2430, #2468 @tomasatdatabricks)
- Java Client API to download model artifacts from Model Registry (#2308, @andychow-db)

Bug fixes and documentation updates:

- Updated Model Registry documentation page with code snippets and examples (#2493, @dmatrix; #2517, @harupy)
- Better error message for Model Registry, when using incompatible backend server (#2456, @aarondav)
- matplotlib is no longer required to use XGBoost and LightGBM autologging (#2423, @harupy)
- Fixed bug where matplotlib figures were not closed in XGBoost and LightGBM autologging (#2386, @harupy)
- Fixed parameter reading logic to support param values with newlines in FileStore (#2376, @dbczumar)
- Improve readability of run table column selector nodes (#2388, @dbczumar)
- Validate experiment name supplied to `UpdateExperiment` REST API endpoint (#2357, @ggliem)
- Fixed broken MLflow DB README link in CLI docs (#2377, @dbczumar)
- Change copyright year across docs to 2020 (#2349, @ParseThis)

Small bug fixes and doc updates (#2378, #2449, #2402, #2397, #2391, #2387, #2523, #2527 @harupy; #2314, @juntai-zheng; #2404, @andychow-db; #2343, @pogil; #2366, #2370, #2364, #2356, @AveshCSingh; #2373, #2365, #2363, @smurching; #2358, @jcuquemelle; #2490, @RensDimmendaal; #2506, @dbczumar; #2234 @Zangr; #2359 @lbernickm; #2525, @mparkhe)

## 1.6.0 (2020-01-29)

MLflow 1.6.0 includes several new features, including a better runs table interface, a utility for easier parameter tuning, and automatic logging from XGBoost, LightGBM, and Spark. It also implements a long-awaited fix allowing @ symbols in database URLs. A complete list is below:

Features:

- Adds a new runs table column view based on `ag-grid` which adds functionality for nested runs, serverside sorting, column reordering, highlighting, and more. (#2251, @Zangr)
- Adds contour plot to the run comparison page to better support parameter tuning (#2225, @harupy)
- If you use EarlyStopping with Keras autologging, MLflow now automatically captures the best model trained and the associated metrics (#2301, #2219, @juntai-zheng)
- Adds autologging functionality for LightGBM and XGBoost flavors to log feature importance, metrics per iteration, the trained model, and more. (#2275, #2238, @harupy)
- Adds an experimental mlflow.spark.autolog() API for automatic logging of Spark datasource information to the current active run. (#2220, @smurching)
- Optimizes the file store to load less data from disk for each operation (#2339, @jonas)
- Upgrades from ubuntu:16.04 to ubuntu:18.04 when building a Docker image with `mlflow models build-docker` (#2256, @andychow-db)

Bug fixes and documentation updates:

- Fixes bug when running server against database URLs with @ symbols (#2289, @hershaw)
- Fixes model Docker image build on Windows (#2257, @jahas)
- Documents the SQL Server plugin (#2320, @avflor)
- Adds a help file for the R package (#2259, @lorenzwalthert)
- Adds an example of using the Search API to find the best performing model (#2313, @AveshCSingh)
- Documents how to write and use MLflow plugins (#2270, @smurching)

Small bug fixes and doc updates (#2293, #2328, #2244, @harupy; #2269, #2332, #2306, #2307, #2292, #2267, #2191, #2231, @juntai-zheng; #2325, @shubham769; #2291, @sueann; #2315, #2249, #2288, #2278, #2253, #2181, @smurching; #2342, @tomasatdatabricks; #2245, @dependabot[bot]; #2338, @jcuquemelle; #2285, @avflor; #2340, @pogil; #2237, #2226, #2243, #2272, #2286, @dbczumar; #2281, @renaudhager; #2246, @avaucher; #2258, @lorenzwalthert; #2261, @smith-kyle; 2352, @dbczumar)

## 1.5.0 (2019-12-19)

MLflow 1.5.0 includes several major features and improvements:

New Model Flavors and Flavor Updates:

- New support for a LightGBM flavor (#2136, @harupy)
- New support for a XGBoost flavor (#2124, @harupy)
- New support for a Gluon flavor and autologging (#1973, @cosmincatalin)
- Runs automatically created by `mlflow.tensorflow.autolog()` and `mlflow.keras.autolog()` (#2088) are now automatically ended after training and/or exporting your model. See the [`docs`](https://mlflow.org/docs/latest/tracking.html#automatic-logging-from-tensorflow-and-keras-experimental) for more details (#2094, @juntai-zheng)

More features and improvements:

- When using the `mlflow server` CLI command, you can now expose metrics on `/metrics` for Prometheus via the optional --activate-parameter argument (#2097, @t-henri)
- The `mlflow ui` CLI command now has a `--host`/`-h` option to specify user-input IPs to bind to (#2176, @gandroz)
- MLflow now supports pulling Git submodules while using MLflow Projects (#2103, @badc0re)
- New `mlflow models prepare-env` command to do any preparation necessary to initialize an environment. This allows distinguishing configuration and user errors during predict/serve time (#2040, @aarondav)
- TensorFlow.Keras and Keras parameters are now logged by `autolog()` (#2119, @juntai-zheng)
- MLflow `log_params()` will recognize Spark ML params as keys and will now extract only the name attribute (#2064, @tomasatdatabricks)
- Exposes `mlflow.tracking.is_tracking_uri_set()` (#2026, @fhoering)
- The artifact image viewer now displays "Loading..." when it is loading an image (#1958, @harupy)
- The artifact image view now supports animated GIFs (#2070, @harupy)
- Adds ability to mount volumes and specify environment variables when using mlflow with docker (#1994, @nlml)
- Adds run context for detecting job information when using MLflow tracking APIs within Databricks Jobs. The following job types are supported: notebook jobs, Python Task jobs (#2205, @dbczumar)
- Performance improvement when searching for runs (#2030, #2059, @jcuquemelle; #2195, @rom1504)

Bug fixes and documentation updates:

- Fixed handling of empty directories in FS based artifact repositories (#1891, @tomasatdatabricks)
- Fixed `mlflow.keras.save_model()` usage with DBFS (#2216, @andychow-db)
- Fixed several build issues for the Docker image (#2107, @jimthompson5802)
- Fixed `mlflow_list_artifacts()` (R package) (#2200, @lorenzwalthert)
- Entrypoint commands of Kubernetes jobs are now shell-escaped (#2160, @zanitete)
- Fixed project run Conda path issue (#2147, @Zangr)
- Fixed spark model load from model repository (#2175, @tomasatdatabricks)
- Stripped "dev" suffix from PySpark versions (#2137, @dbczumar)
- Fixed note editor on the experiment page (#2054, @harupy)
- Fixed `models serve`, `models predict` CLI commands against models:/ URIs (#2067, @smurching)
- Don't unconditionally format values as metrics in generic HtmlTableView component (#2068, @smurching)
- Fixed remote execution from Windows using posixpath (#1996, @aestene)
- Add XGBoost and LightGBM examples (#2186, @harupy)
- Add note about active run instantiation side effect in fluent APIs (#2197, @andychow-db)
- The tutorial page has been refactored to be be a 'Tutorials and Examples' page (#2182, @juntai-zheng)
- Doc enhancements for XGBoost and LightGBM flavors (#2170, @harupy)
- Add doc for XGBoost flavor (#2167, @harupy)
- Updated `active_run()` docs to clarify it cannot be used accessing current run data (#2138, @juntai-zheng)
- Document models:/ scheme for URI for load_model methods (#2128, @stbof)
- Added an example using Prophet via pyfunc (#2043, @dr3s)
- Added and updated some screenshots and explicit steps for the model registry (#2086, @stbof)

Small bug fixes and doc updates (#2142, #2121, #2105, #2069, #2083, #2061, #2022, #2036, #1972, #2034, #1998, #1959, @harupy; #2202, @t-henri; #2085, @stbof; #2098, @AdamBarnhard; #2180, #2109, #1977, #2039, #2062, @smurching; #2013, @aestene; #2146, @joelcthomas; #2161, #2120, #2100, #2095, #2088, #2076, #2057, @juntai-zheng; #2077, #2058, #2027, @sueann; #2149, @zanitete; #2204, #2188, @andychow-db; #2110, #2053, @jdlesage; #2003, #1953, #2004, @Djailla; #2074, @nlml; #2116, @Silas-Asamoah; #1104, @jimthompson5802; #2072, @cclauss; #2221, #2207, #2157, #2132, #2114, #2063, #2065, #2055, @dbczumar; #2033, @cthoyt; #2048, @philip-khor; #2002, @jspoorta; #2000, @christang; #2078, @dennyglee; #1986, @vguerra; #2020, @dependabot[bot])

## 1.4.0 (2019-10-30)

MLflow 1.4.0 includes several major features:

- Model Registry (Beta). Adds an experimental model registry feature, where you can manage, version, and keep lineage of your production models. (#1943, @mparkhe, @Zangr, @sueann, @dbczumar, @smurching, @gioa, @clemens-db, @pogil, @mateiz; #1988, #1989, #1995, #2021, @mparkhe; #1983, #1982, #1967, @dbczumar)
- TensorFlow updates

  - MLflow Keras model saving, loading, and logging has been updated to be compatible with TensorFlow 2.0. (#1927, @juntai-zheng)
  - Autologging for `tf.estimator` and `tf.keras` models has been updated to be compatible with TensorFlow 2.0. The same functionalities of autologging in TensorFlow 1.x are available in TensorFlow 2.0, namely when fitting `tf.keras` models and when exporting saved `tf.estimator` models. (#1910, @juntai-zheng)
  - Examples and READMEs for both TensorFlow 1.X and TensorFlow 2.0 have been added to `mlflow/examples/tensorflow`. (#1946, @juntai-zheng)

More features and improvements:

- [API] Add functions `get_run`, `get_experiment`, `get_experiment_by_name` to the fluent API (#1923, @fhoering)
- [UI] Use Plotly as artifact image viewer, which allows zooming and panning (#1934, @harupy)
- [UI] Support deleting tags from the run details page (#1933, @harupy)
- [UI] Enable scrolling to zoom in metric and run comparison plots (#1929, @harupy)
- [Artifacts] Add support of viewfs URIs for HDFS federation for artifacts (#1947, @t-henri)
- [Models] Spark UDFs can now be called with struct input if the underlying spark implementation supports it. The data is passed as a pandas DataFrame with column names matching those in the struct. (#1882, @tomasatdatabricks)
- [Models] Spark models will now load faster from DFS by skipping unnecessary copies (#2008, @tomasatdatabricks)

Bug fixes and documentation updates:

- [Projects] Make detection of `MLproject` files case-insensitive (#1981, @smurching)
- [UI] Fix a bug where viewing metrics containing forward-slashes in the name would break the MLflow UI (#1968, @smurching)
- [CLI] `models serve` command now works in Windows (#1949, @rboyes)
- [Scoring] Fix a dependency installation bug in Java MLflow model scoring server (#1913, @smurching)

Small bug fixes and doc updates (#1932, #1935, @harupy; #1907, @marnixkoops; #1911, @HackyRoot; #1931, @jmcarp; #2007, @deniskovalenko; #1966, #1955, #1952, @Djailla; #1915, @sueann; #1978, #1894, @smurching; #1940, #1900, #1904, @mparkhe; #1914, @jerrygb; #1857, @mengxr; #2009, @dbczumar)

## 1.3 (2019-09-30)

MLflow 1.3.0 includes several major features and improvements:

Features:

- The Python client now supports logging & loading models using TensorFlow 2.0 (#1872, @juntai-zheng)
- Significant performance improvements when fetching runs and experiments in MLflow servers that use SQL database-backed storage (#1767, #1878, #1805 @dbczumar)
- New `GetExperimentByName` REST API endpoint, used in the Python client to speed up `set_experiment` and `get_experiment_by_name` (#1775, @smurching)
- New `mlflow.delete_run`, `mlflow.delete_experiment` fluent APIs in the Python client(#1396, @MerelTheisenQB)
- New CLI command (`mlflow experiments csv`) to export runs of an experiment into a CSV (#1705, @jdlesage)
- Directories can now be logged as artifacts via `mlflow.log_artifact` in the Python fluent API (#1697, @apurva-koti)
- HTML and geojson artifacts are now rendered in the run UI (#1838, @sim-san; #1803, @spadarian)
- Keras autologging support for `fit_generator` Keras API (#1757, @charnger)
- MLflow models packaged as docker containers can be executed via Google Cloud Run (#1778, @ngallot)
- Artifact storage configurations are propagated to containers when executing docker-based MLflow projects locally (#1621, @nlaille)
- The Python, Java, R clients and UI now retry HTTP requests on 429 (Too Many Requests) errors (#1846, #1851, #1858, #1859 @tomasatdatabricks; #1847, @smurching)

Bug fixes and documentation updates:

- The R `mlflow_list_artifact` API no longer throws when listing artifacts for an empty run (#1862, @smurching)
- Fixed a bug preventing running the MLflow server against an MS SQL database (#1758, @sifanLV)
- MLmodel files (artifacts) now correctly display in the run UI (#1819, @ankitmathur-db)
- The Python `mlflow.start_run` API now throws when resuming a run whose experiment ID differs from the
  active experiment ID set via `mlflow.set_experiment` (#1820, @mcminnra).
- `MlflowClient.log_metric` now logs metric timestamps with millisecond (as opposed to second) resolution (#1804, @ustcscgyer)
- Fixed bugs when listing (#1800, @ahutterTA) and downloading (#1890, @jdlesage) artifacts stored in HDFS.
- Fixed a bug preventing Kubernetes Projects from pushing to private Docker repositories (#1788, @dbczumar)
- Fixed a bug preventing deploying Spark models to AzureML (#1769, @Ben-Epstein)
- Fixed experiment id resolution in projects (#1715, @drewmcdonald)
- Updated parallel coordinates plot to show all fields available in compared runs (#1753, @mateiz)
- Streamlined docs for getting started with hosted MLflow (#1834, #1785, #1860 @smurching)

Small bug fixes and doc updates (#1848, @pingsutw; #1868, @iver56; #1787, @apurvakoti; #1741, #1737, @apurva-koti; #1876, #1861, #1852, #1801, #1754, #1726, #1780, #1807 @smurching; #1859, #1858, #1851, @tomasatdatabricks; #1841, @ankitmathur-db; #1744, #1746, #1751, @mateiz; #1821, #1730, @dbczumar; #1727, cfmcgrady; #1716, @axsaucedo; #1714, @fhoering; #1405, @ancasarb; #1502, @jimthompson5802; #1720, jke-zq; #1871, @mehdi254; #1782, @stbof)

## 1.2 (2019-08-09)

MLflow 1.2 includes the following major features and improvements:

- Experiments now have editable tags and descriptions (#1630, #1632, #1678, @ankitmathur-db)
- Search latency has been significantly reduced in the SQLAlchemyStore (#1660, @t-henri)

**More features and improvements**

- Backend stores now support run tag values up to 5000 characters in length. Some store implementations may support longer tag values (#1687, @ankitmathur-db)
- Gunicorn options can now be configured for the `mlflow models serve` CLI with the `GUNICORN_CMD_ARGS` environment variable (#1557, @LarsDu)
- Jsonnet artifacts can now be previewed in the UI (#1683, @ankitmathur-db)
- Adds an optional `python_version` argument to `mlflow_install` for specifying the Python version (e.g. "3.5") to use within the conda environment created for installing the MLflow CLI. If `python_version` is unspecified, `mlflow_install` defaults to using Python 3.6. (#1722, @smurching)

**Bug fixes and documentation updates**

- [Tracking] The Autologging feature is now more resilient to tracking errors (#1690, @apurva-koti)
- [Tracking] The `runs` field in in the `GetExperiment.Response` proto has been deprecated & will be removed in MLflow 2.0. Please use the `Search Runs` API for fetching runs instead (#1647, @dbczumar)
- [Projects] Fixed a bug that prevented docker-based MLflow Projects from logging artifacts to the `LocalArtifactRepository` (#1450, @nlaille)
- [Projects] Running MLflow projects with the `--no-conda` flag in R no longer requires Anaconda to be installed (#1650, @spadarian)
- [Models/Scoring] Fixed a bug that prevented Spark UDFs from being loaded on Databricks (#1658, @smurching)
- [UI] AJAX requests made by the MLflow Server Frontend now specify correct MIME-Types (#1679, @ynotzort)
- [UI] Previews now render correctly for artifacts with uppercase file extensions (e.g., `.JSON`, `.YAML`) (#1664, @ankitmathur-db)
- [UI] Fixed a bug that caused search API errors to surface a Niagara Falls page (#1681, @dbczumar)
- [Installation] MLflow dependencies are now selected properly based on the target installation platform (#1643, @akshaya-a)
- [UI] Fixed a bug where the "load more" button in the experiment view did not appear on browsers in Windows (#1718, @Zangr)

Small bug fixes and doc updates (#1663, #1719, @dbczumar; #1693, @max-allen-db; #1695, #1659, @smurching; #1675, @jdlesage; #1699, @ankitmathur-db; #1696, @aarondav; #1710, #1700, #1656, @apurva-koti)

## 1.1 (2019-07-22)

MLflow 1.1 includes several major features and improvements:

In MLflow Tracking:

- Experimental support for autologging from Tensorflow and Keras. Using `mlflow.tensorflow.autolog()` will enable automatic logging of metrics and optimizer parameters from TensorFlow to MLflow. The feature will work with TensorFlow versions `1.12 <= v < 2.0`. (#1520, #1601, @apurva-koti)
- Parallel coordinates plot in the MLflow compare run UI. Adds out of the box support for a parallel coordinates plot. The plot allows users to observe relationships between a n-dimensional set of parameters to metrics. It visualizes all runs as lines that are color-coded based on the value of a metric (e.g. accuracy), and shows what parameter values each run took on. (#1497, @Zangr)
- Pandas based search API. Adds the ability to return the results of a search as a pandas dataframe using the new `mlflow.search_runs` API. (#1483, #1548, @max-allen-db)
- Java fluent API. Adds a new set of APIs to create and log to MLflow runs. This API contrasts with the existing low level `MlflowClient` API which simply wraps the REST APIs. The new fluent API allows you to create and log runs similar to how you would using the Python fluent API. (#1508, @andrewmchen)
- Run tags improvements. Adds the ability to add and edit tags from the run view UI, delete tags from the API, and view tags in the experiment search view. (#1400, #1426, @Zangr; #1548, #1558, @ankitmathur-db)
- Search API improvements. Adds order by and pagination to the search API. Pagination allows you to read a large set of runs in small page sized chunks. This allows clients and backend implementations to handle an unbounded set of runs in a scalable manner. (#1444, @sueann; #1437, #1455, #1482, #1485, #1542, @aarondav; #1567, @max-allen-db; #1217, @mparkhe)
- Windows support for running the MLflow tracking server and UI. (#1080, @akshaya-a)

In MLflow Projects:

- Experimental support to run Docker based MLprojects in Kubernetes. Adds the first fully open source remote execution backend for MLflow projects. With this, you can leverage elastic compute resources managed by kubernetes for their ML training purposes. For example, you can run grid search over a set of hyperparameters by running several instances of an MLproject in parallel. (#1181, @marcusrehm, @tomasatdatabricks, @andrewmchen; #1566, @stbof, @dbczumar; #1574 @dbczumar)

**More features and improvements**

In MLflow Tracking:

- Paginated "load more" and backend sorting for experiment search view UI. This change allows the UI to scalably display the sorted runs from large experiments. (#1564, @Zangr)
- Search results are encoded in the URL. This allows you to share searches through their URL and to deep link to them. (#1416, @apurva-koti)
- Ability to serve MLflow UI behind `jupyter-server-proxy` or outside of the root path `/`. Previous to MLflow 1.1, the UI could only be hosted on `/` since the Javascript makes requests directly to `/ajax-api/...`. With this patch, MLflow will make requests to `ajax-api/...` or a path relative to where the HTML is being served. (#1413, @xhochy)

In MLflow Models:

- Update `mlflow.spark.log_model()` to accept descendants of pyspark.Model (#1519, @ankitmathur-db)
- Support for saving custom Keras models with `custom_objects`. This field is semantically equivalent to custom_objects parameter of `keras.models.load_model()` function (#1525, @ankitmathur-db)
- New more performant split orient based input format for pyfunc scoring server (#1479, @lennon310)
- Ability to specify gunicorn server options for pyfunc scoring server built with `mlflow models build-docker`. #1428, @lennon310)

**Bug fixes and documentation updates**

- [Tracking] Fix database migration for MySQL. `mlflow db upgrade` should now work for MySQL backends. (#1404, @sueann)
- [Tracking] Make CLI `mlflow server` and `mlflow ui` commands to work with SQLAlchemy URIs that specify a database driver. (#1411, @sueann)
- [Tracking] Fix usability bugs related to FTP artifact repository. (#1398, @kafendt; #1421, @nlaille)
- [Tracking] Return appropriate HTTP status codes for MLflowException (#1434, @max-allen-db)
- [Tracking] Fix sorting by user ID in the experiment search view. (#1401, @andrewmchen)
- [Tracking] Allow calling log_metric with NaNs and infs. (#1573, @tomasatdatabricks)
- [Tracking] Fixes an infinite loop in downloading artifacts logged via dbfs and retrieved via S3. (#1605, @sueann)
- [Projects] Docker projects should preserve directory structure (#1436, @ahutterTA)
- [Projects] Fix conda activation for newer versions of conda. (#1576, @avinashraghuthu, @smurching)
- [Models] Allow you to log Tensorflow keras models from the `tf.keras` module. (#1546, @tomasatdatabricks)

Small bug fixes and doc updates (#1463, @mateiz; #1641, #1622, #1418, @sueann; #1607, #1568, #1536, #1478, #1406, #1408, @smurching; #1504, @LizaShak; #1490, @acroz; #1633, #1631, #1603, #1589, #1569, #1526, #1446, #1438, @apurva-koti; #1456, @Taur1ne; #1547, #1495, @aarondav; #1610, #1600, #1492, #1493, #1447, @tomasatdatabricks; #1430, @javierluraschi; #1424, @nathansuh; #1488, @henningsway; #1590, #1427, @Zangr; #1629, #1614, #1574, #1521, #1522, @dbczumar; #1577, #1514, @ankitmathur-db; #1588, #1566, @stbof; #1575, #1599, @max-allen-db; #1592, @abaveja313; #1606, @andrewmchen)

## 1.0 (2019-06-03)

MLflow 1.0 includes many significant features and improvements. From this version, MLflow is no longer beta, and all APIs except those marked as experimental are intended to be stable until the next major version. As such, this release includes a number of breaking changes.

Major features, improvements, and breaking changes

- Support for recording, querying, and visualizing metrics along a new "step" axis (x coordinate), providing increased flexibility for examining model performance relative to training progress. For example, you can now record performance metrics as a function of the number of training iterations or epochs. MLflow 1.0's enhanced metrics UI enables you to visualize the change in a metric's value as a function of its step, augmenting MLflow's existing UI for plotting a metric's value as a function of wall-clock time. (#1202, #1237, @dbczumar; #1132, #1142, #1143, @smurching; #1211, #1225, @Zangr; #1372, @stbof)
- Search improvements. MLflow 1.0 includes additional support in both the API and UI for searching runs within a single experiment or a group of experiments. The search filter API supports a simplified version of the `SQL WHERE` clause. In addition to searching using run's metrics and params, the API has been enhanced to support a subset of run attributes as well as user and [system tags](https://mlflow.org/docs/latest/tracking.html#system-tags). For details see [Search syntax](https://mlflow.org/docs/latest/search-syntax.html#syntax) and [examples for programmatically searching runs](https://mlflow.org/docs/latest/search-syntax.html#programmatically-searching-runs). (#1245, #1272, #1323, #1326, @mparkhe; #1052, @Zangr; #1363, @aarondav)
- Logging metrics in batches. MLflow 1.0 now has a `runs/log-batch` REST API endpoint for logging multiple metrics, params, and tags in a single API request. The endpoint useful for performant logging of multiple metrics at the end of a model training epoch (see [example](https://github.com/mlflow/mlflow/blob/bb8c7602dcb6a3a8786301fe6b98f01e8d3f288d/examples/hyperparam/search_hyperopt.py#L161)), or logging of many input model parameters at the start of training. You can call this batched-logging endpoint from Python (`mlflow.log_metrics`, `mlflow.log_params`, `mlflow.set_tags`), R (`mlflow_log_batch`), and Java (`MlflowClient.logBatch`). (#1214, @dbczumar; see 0.9.1 and 0.9.0 for other changes)
- Windows support for MLflow Tracking. The Tracking portion of the MLflow client is now supported on Windows. (#1171, @eedeleon, @tomasatdatabricks)
- HDFS support for artifacts. Hadoop artifact repository with Kerberos authorization support was added, so you can use HDFS to log and retrieve models and other artifacts. (#1011, @jaroslawk)
- CLI command to build Docker images for serving. Added an `mlflow models build-docker` CLI command for building a Docker image capable of serving an MLflow model. The model is served at port 8080 within the container by default. Note that this API is experimental and does not guarantee that the arguments nor format of the Docker container will remain the same. (#1329, @smurching, @tomasatdatabricks)
- New `onnx` model flavor for saving, loading, and evaluating ONNX models with MLflow. ONNX flavor APIs are available in the `mlflow.onnx` module. (#1127, @avflor, @dbczumar; #1388, #1389, @dbczumar)
- Major breaking changes:

  - Some of the breaking changes involve database schema changes in the SQLAlchemy tracking store. If your database instance's schema is not up-to-date, MLflow will issue an error at the start-up of `mlflow server` or `mlflow ui`. To migrate an existing database to the newest schema, you can use the `mlflow db upgrade` CLI command. (#1155, #1371, @smurching; #1360, @aarondav)
  - [Installation] The MLflow Python package no longer depends on `scikit-learn`, `mleap`, or `boto3`. If you want to use the `scikit-learn` support, the `MLeap` support, or `s3` artifact repository / `sagemaker` support, you will have to install these respective dependencies explicitly. (#1223, @aarondav)
  - [Artifacts] In the Models API, an artifact's location is now represented as a URI. See the [documentation](https://mlflow.org/docs/latest/tracking.html#artifact-locations) for the list of accepted URIs. (#1190, #1254, @dbczumar; #1174, @dbczumar, @sueann; #1206, @tomasatdatabricks; #1253, @stbof)

    - The affected methods are:

      - Python: `<model-type>.load_model`, `azureml.build_image`, `sagemaker.deploy`, `sagemaker.run_local`, `pyfunc._load_model_env`, `pyfunc.load_pyfunc`, and `pyfunc.spark_udf`
      - R: `mlflow_load_model`, `mlflow_rfunc_predict`, `mlflow_rfunc_serve`
      - CLI: `mlflow models serve`, `mlflow models predict`, `mlflow sagemaker`, `mlflow azureml` (with the new `--model-uri` option)

    - To allow referring to artifacts in the context of a run, MLflow introduces a new URI scheme of the form `runs:/<run_id>/relative/path/to/artifact`. (#1169, #1175, @sueann)

  - [CLI] `mlflow pyfunc` and `mlflow rfunc` commands have been unified as `mlflow models` (#1257, @tomasatdatabricks; #1321, @dbczumar)
  - [CLI] `mlflow artifacts download`, `mlflow artifacts download-from-uri` and `mlflow download` commands have been consolidated into `mlflow artifacts download` (#1233, @sueann)
  - [Runs] Expose `RunData` fields (`metrics`, `params`, `tags`) as dictionaries. Note that the `mlflow.entities.RunData` constructor still accepts lists of `metric`/`param`/`tag` entities. (#1078, @smurching)
  - [Runs] Rename `run_uuid` to `run_id` in Python, Java, and REST API. Where necessary, MLflow will continue to accept `run_uuid` until MLflow 1.1. (#1187, @aarondav)

Other breaking changes

CLI:

- The `--file-store` option is deprecated in `mlflow server` and `mlflow ui` commands. (#1196, @smurching)
- The `--host` and `--gunicorn-opts` options are removed in the `mlflow ui` command. (#1267, @aarondav)
- Arguments to `mlflow experiments` subcommands, notably `--experiment-name` and `--experiment-id` are now options (#1235, @sueann)
- `mlflow sagemaker list-flavors` has been removed (#1233, @sueann)

Tracking:

- The `user` property of `Run`s has been moved to tags (similarly, the `run_name`, `source_type`, `source_name` properties were moved to tags in 0.9.0). (#1230, @acroz; #1275, #1276, @aarondav)
- In R, the return values of experiment CRUD APIs have been updated to more closely match the REST API. In particular, `mlflow_create_experiment` now returns a string experiment ID instead of an experiment, and the other APIs return NULL. (#1246, @smurching)
- `RunInfo.status`'s type is now string. (#1264, @mparkhe)
- Remove deprecated `RunInfo` properties from `start_run`. (#1220, @aarondav)
- As deprecated in 0.9.1 and before, the `RunInfo` fields `run_name`, `source_name`, `source_version`, `source_type`, and `entry_point_name` and the `SearchRuns` field `anded_expressions` have been removed from the REST API and Python, Java, and R tracking client APIs. They are still available as tags, documented in the REST API documentation. (#1188, @aarondav)

Models and deployment:

- In Python, require arguments as keywords in `log_model`, `save_model` and `add_to_model` methods in the `tensorflow` and `mleap` modules to avoid breaking changes in the future (#1226, @sueann)
- Remove the unsupported `jars` argument from ``spark.log_model` in Python (#1222, @sueann)
- Introduce `pyfunc.load_model` to be consistent with other Models modules. `pyfunc.load_pyfunc` will be deprecated in the near future. (#1222, @sueann)
- Rename `dst_path` parameter in `pyfunc.save_model` to `path` (#1221, @aarondav)
- R flavors refactor (#1299, @kevinykuo)

  - `mlflow_predict()` has been added in favor of `mlflow_predict_model()` and `mlflow_predict_flavor()` which have been removed.
  - `mlflow_save_model()` is now a generic and `mlflow_save_flavor()` is no longer needed and has been removed.
  - `mlflow_predict()` takes `...` to pass to underlying predict methods.
  - `mlflow_load_flavor()` now has the signature `function(flavor, model_path)` and flavor authors should implement `mlflow_load_flavor.mlflow_flavor_{FLAVORNAME}`. The flavor argument is inferred from the inputs of user-facing `mlflow_load_model()` and does not need to be explicitly provided by the user.

Projects:

- Remove and rename some `projects.run` parameters for generality and consistency. (#1222, @sueann)
- In R, the `mlflow_run` API for running MLflow projects has been modified to more closely reflect the Python `mlflow.run` API. In particular, the order of the `uri` and `entry_point` arguments has been reversed and the `param_list` argument has been renamed to `parameters`. (#1265, @smurching)

R:

- Remove `mlflow_snapshot` and `mlflow_restore_snapshot` APIs. Also, the `r_dependencies` argument used to specify the path to a packrat r-dependencies.txt file has been removed from all APIs. (#1263, @smurching)
- The `mlflow_cli` and `crate` APIs are now private. (#1246, @smurching)

Environment variables:

- Prefix environment variables with "MLFLOW\_" (#1268, @aarondav). Affected variables are:

  - [Tracking] `_MLFLOW_SERVER_FILE_STORE`, `_MLFLOW_SERVER_ARTIFACT_ROOT`, `_MLFLOW_STATIC_PREFIX`
  - [SageMaker] `MLFLOW_SAGEMAKER_DEPLOY_IMG_URL`, `MLFLOW_DEPLOYMENT_FLAVOR_NAME`
  - [Scoring] `MLFLOW_SCORING_SERVER_MIN_THREADS`, `MLFLOW_SCORING_SERVER_MAX_THREADS`

More features and improvements

- [Tracking] Non-default driver support for SQLAlchemy backends: `db+driver` is now a valid tracking backend URI scheme (#1297, @drewmcdonald; #1374, @mparkhe)
- [Tracking] Validate backend store URI before starting tracking server (#1218, @luke-zhu, @sueann)
- [Tracking] Add `GetMetricHistory` client API in Python and Java corresponding to the REST API. (#1178, @smurching)
- [Tracking] Add `view_type` argument to `MlflowClient.list_experiments()` in Python. (#1212, @smurching)
- [Tracking] Dictionary values provided to `mlflow.log_params` and `mlflow.set_tags` in Python can now be non-string types (e.g., numbers), and they are automatically converted to strings. (#1364, @aarondav)
- [Tracking] R API additions to be at parity with REST API and Python (#1122, @kevinykuo)
- [Tracking] Limit number of results returned from `SearchRuns` API and UI for faster load (#1125, @mparkhe; #1154, @andrewmchen)
- [Artifacts] To avoid having many copies of large model files in serving, `ArtifactRepository.download_artifacts` no longer copies local artifacts (#1307, @andrewmchen; #1383, @dbczumar)
- [Artifacts/Projects] Support GCS in download utilities. `gs://bucket/path` files are now supported by the `mlflow artifacts download` CLI command and as parameters of type `path` in MLProject files. (#1168, @drewmcdonald)
- [Models] All Python models exported by MLflow now declare `mlflow` as a dependency by default. In addition, we introduce a flag `--install-mlflow` users can pass to `mlflow models serve` and `mlflow models predict` methods to force installation of the latest version of MLflow into the model's environment. (#1308, @tomasatdatabricks)
- [Models] Update model flavors to lazily import dependencies in Python. Modules that define Model flavors now import extra dependencies such as `tensorflow`, `scikit-learn`, and `pytorch` inside individual _methods_, ensuring that these modules can be imported and explored even if the dependencies have not been installed on your system. Also, the `DEFAULT_CONDA_ENVIRONMENT` module variable has been replaced with a `get_default_conda_env()` function for each flavor. (#1238, @dbczumar)
- [Models] It is now possible to pass extra arguments to `mlflow.keras.load_model` that will be passed through to `keras.load_model`. (#1330, @yorickvP)
- [Serving] For better performance, switch to `gunicorn` for serving Python models. This does not change the user interface. (#1322, @tomasatdatabricks)
- [Deployment] For SageMaker, use the uniquely-generated model name as the S3 bucket prefix instead of requiring one. (#1183, @dbczumar)
- [REST API] Add support for API paths without the `preview` component. The `preview` paths will be deprecated in a future version of MLflow. (#1236, @mparkhe)

Bug fixes and documentation updates

- [Tracking] Log metric timestamps in milliseconds by default (#1177, @smurching; #1333, @dbczumar)
- [Tracking] Fix bug when deserializing integer experiment ID for runs in `SQLAlchemyStore` (#1167, @smurching)
- [Tracking] Ensure unique constraint names in MLflow tracking database (#1292, @smurching)
- [Tracking] Fix base64 encoding for basic auth in R tracking client (#1126, @freefrag)
- [Tracking] Correctly handle `file:` URIs for the `-—backend-store-uri` option in `mlflow server` and `mlflow ui` CLI commands (#1171, @eedeleon, @tomasatdatabricks)
- [Artifacts] Update artifact repository download methods to return absolute paths (#1179, @dbczumar)
- [Artifacts] Make FileStore respect the default artifact location (#1332, @dbczumar)
- [Artifacts] Fix `log_artifact` failures due to existing directory on FTP server (#1327, @kafendt)
- [Artifacts] Fix GCS artifact logging of subdirectories (#1285, @jason-huling)
- [Projects] Fix bug not sharing `SQLite` database file with Docker container (#1347, @tomasatdatabricks; #1375, @aarondav)
- [Java] Mark `sendPost` and `sendGet` as experimental (#1186, @aarondav)
- [Python/CLI] Mark `azureml.build_image` as experimental (#1222, #1233 @sueann)
- [Docs] Document public MLflow environment variables (#1343, @aarondav)
- [Docs] Document MLflow system tags for runs (#1342, @aarondav)
- [Docs] Autogenerate CLI documentation to include subcommands and descriptions (#1231, @sueann)
- [Docs] Update run selection description in `mlflow_get_run` in R documentation (#1258, @dbczumar)
- [Examples] Update examples to reflect API changes (#1361, @tomasatdatabricks; #1367, @mparkhe)

Small bug fixes and doc updates (#1359, #1350, #1331, #1301, #1270, #1271, #1180, #1144, #1135, #1131, #1358, #1369, #1368, #1387, @aarondav; #1373, @akarloff; #1287, #1344, #1309, @stbof; #1312, @hchiuzhuo; #1348, #1349, #1294, #1227, #1384, @tomasatdatabricks; #1345, @withsmilo; #1316, @ancasarb; #1313, #1310, #1305, #1289, #1256, #1124, #1097, #1162, #1163, #1137, #1351, @smurching; #1319, #1244, #1224, #1195, #1194, #1328, @dbczumar; #1213, #1200, @Kublai-Jing; #1304, #1320, @andrewmchen; #1311, @Zangr; #1306, #1293, #1147, @mateiz; #1303, @gliptak; #1261, #1192, @eedeleon; #1273, #1259, @kevinykuo; #1277, #1247, #1243, #1182, #1376, @mparkhe; #1210, @vgod-dbx; #1199, @ashtuchkin; #1176, #1138, #1365, @sueann; #1157, @cclauss; #1156, @clemens-db; #1152, @pogil; #1146, @srowen; #875, #1251, @jimthompson5802)

## 0.9.1 (2019-04-21)

MLflow 0.9.1 is a patch release on top of 0.9.0 containing mostly bug fixes and internal improvements. We have also included a one breaking API change in preparation for additions in MLflow 1.0 and later. This release also includes significant improvements to the Search API.

Breaking changes:

- [Tracking] Generalized experiment_id to string (from a long) to be more permissive of different ID types in different backend stores. While breaking for the REST API, this change is backwards compatible for python and R clients. (#1067, #1034 @eedeleon)

More features and improvements:

- [Search/API] Moving search filters into a query string based syntax, with Java client, Python client, and UI support. This also improves quote, period, and special character handling in query strings and adds the ability to search on tags in filter string. (#1042, #1055, #1063, #1068, #1099, #1106 @mparkhe; #1025 @andrewmchen; #1060 @smurching)
- [Tracking] Limits and validations to batch-logging APIs in OSS server (#958 @smurching)
- [Tracking/Java] Java client API for batch-logging (#1081 @mparkhe)
- [Tracking] Improved consistency of handling multiple metric values per timestamp across tracking stores (#972, #999 @dbczumar)

Bug fixes and documentation updates:

- [Tracking/Python] Reintroduces the parent_run_id argument to MlflowClient.create_run. This API is planned for removal in MLflow 1.0 (#1137 @smurching)
- [Tracking/Python] Provide default implementations of AbstractStore log methods (#1051 @acroz)
- [R] (Released on CRAN as MLflow 0.9.0.1) Small bug fixes with R (#1123 @smurching; #1045, #1017, #1019, #1039, #1048, #1098, #1101, #1107, #1108, #1119 @tomasatdatabricks)

Small bug fixes and doc updates (#1024, #1029 @bayethiernodiop; #1075 @avflor; #968, #1010, #1070, #1091, #1092 @smurching; #1004, #1085 @dbczumar; #1033, #1046 @sueann; #1053 @tomasatdatabricks; #987 @hanyucui; #935, #941 @jimthompson5802; #963 @amilbourne; #1016 @andrewmchen; #991 @jaroslawk; #1007 @mparkhe)

## 0.9.0.1 (2019-04-09)

Bugfix release (PyPI only) with the following changes:

- Rebuilt MLflow JS assets to fix an issue where form input was broken in MLflow 0.9.0 (identified
  in #1056, #1113 by @shu-yusa, @timothyjlaurent)

  0.9.0 (2019-03-13)

Major features:

- Support for running MLflow Projects in Docker containers. This allows you to include non-Python dependencies in their project environments and provides stronger isolation when running projects. See the [Projects documentation](https://mlflow.org/docs/latest/projects.html) for more information. (#555, @marcusrehm; #819, @mparkhe; #970, @dbczumar)
- Database stores for the MLflow Tracking Server. Support for a scalable and performant backend store was one of the top community requests. This feature enables you to connect to local or remote SQLAlchemy-compatible databases (currently supported flavors include MySQL, PostgreSQL, SQLite, and MS SQL) and is compatible with file backed store. See the [Tracking Store documentation](https://mlflow.org/docs/latest/tracking.html#storage) for more information. (#756, @AndersonReyes; #800, #844, #847, #848, #860, #868, #975, @mparkhe; #980, @dbczumar)
- Simplified custom Python model packaging. You can easily include custom preprocessing and postprocessing logic, as well as data dependencies in models with the `python_function` flavor using updated `mlflow.pyfunc` Python APIs. For more information, see the [Custom Python Models documentation](https://mlflow.org/docs/latest/models.html#custom-python-models). (#791, #792, #793, #830, #910, @dbczumar)
- Plugin systems allowing third party libraries to extend MLflow functionality. The [proposal document](https://gist.github.com/zblz/9e337a55a7ba73314890be68370fa69a) gives the full detail of the three main changes:

  - You can register additional providers of tracking stores using the `mlflow.tracking_store` entrypoint. (#881, @zblz)
  - You can register additional providers of artifact repositories using the `mlflow.artifact_repository` entrypoint. (#882, @mociarain)
  - The logic generating run metadata from the run context (e.g. `source_name`, `source_version`) has been refactored into an extendable system of run context providers. Plugins can register additional providers using the `mlflow.run_context_provider` entrypoint, which add to or overwrite tags set by the base library. (#913, #926, #930, #978, @acroz)

- Support for HTTP authentication to the Tracking Server in the R client. Now you can connect to secure Tracking Servers using credentials set in environment variables, or provide custom plugins for setting the credentials. As an example, this release contains a Databricks plugin that can detect existing Databricks credentials to allow you to connect to the Databricks Tracking Server. (#938, #959, #992, @tomasatdatabricks)

Breaking changes:

- [Scoring] The `pyfunc` scoring server now expects requests with the `application/json` content type to contain json-serialized pandas dataframes in the split format, rather than the records format. See the [documentation on deployment](https://mlflow.org/docs/latest/models.html#deploy-a-python-function-model-as-a-local-rest-api-endpoint) for more detail. (#960, @dbczumar) Also, when reading the pandas dataframes from JSON, the scoring server no longer automatically infers data types as it can result in unintentional conversion of data types (#916, @mparkhe).
- [API] Remove `GetMetric` & `GetParam` from the REST API as they are subsumed by `GetRun`. (#879, @aarondav)

More features and improvements:

- [UI] Add a button for downloading artifacts (#967, @mateiz)
- [CLI] Add CLI commands for runs: now you can `list`, `delete`, `restore`, and `describe` runs through the CLI (#720, @DorIndivo)
- [CLI] The `run` command now can take `--experiment-name` as an argument, as an alternative to the `--experiment-id` argument. You can also choose to set the `_EXPERIMENT_NAME_ENV_VAR` environment variable instead of passing in the value explicitly. (#889, #894, @mparkhe)
- [Examples] Add Image classification example with Keras. (#743, @tomasatdatabricks )
- [Artifacts] Add `get_artifact_uri()` and `_download_artifact_from_uri` convenience functions (#779)
- [Artifacts] Allow writing Spark models directly to the target artifact store when possible (#808, @smurching)
- [Models] PyTorch model persistence improvements to allow persisting definitions and dependencies outside the immediate scope:
  - Add a `code_paths` parameter to `mlflow.pytorch.save_model` and `mlflow.pytorch.log_model` to allow external module dependencies to be specified as paths to python files. (#842, @dbczumar)
  - Improve `mlflow.pytorch.save_model` to capture class definitions from notebooks and the `__main__` scope (#851, #861, @dbczumar)
- [Runs/R] Allow client to infer context info when creating new run in fluent API (#958, @tomasatdatabricks)
- [Runs/UI] Support Git Commit hyperlink for Gitlab and Bitbucket. Previously the clickable hyperlink was generated only for Github pages. (#901)
- [Search]/API] Allow param value to have any content, not just alphanumeric characters, `.`, and `-` (#788, @mparkhe)
- [Search/API] Support "filter" string in the `SearchRuns` API. Corresponding UI improvements are planned for the future (#905, @mparkhe)
- [Logging] Basic support for LogBatch. NOTE: The feature is currently experimental and the behavior is expected to change in the near future. (#950, #951, #955, #1001, @smurching)

Bug fixes and documentation updates:

- [Artifacts] Fix empty-file upload to DBFS in `log_artifact` and `log_artifacts` (#895, #818, @smurching)
- [Artifacts] S3 artifact store: fix path resolution error when artifact root is bucket root (#928, @dbczumar)
- [UI] Fix a bug with Databricks notebook URL links (#891, @smurching)
- [Export] Fix for missing run name in csv export (#864, @jimthompson5802)
- [Example] Correct missing tensorboardX module error in PyTorch example when running in MLflow Docker container (#809, @jimthompson5802)
- [Scoring/R] Fix local serving of rfunc models (#874, @kevinykuo)
- [Docs] Improve flavor-specific documentation in Models documentation (#909, @dbczumar)

Small bug fixes and doc updates (#822, #899, #787, #785, #780, #942, @hanyucui; #862, #904, #954, #806, #857, #845, @stbof; #907, #872, @smurching; #896, #858, #836, #859, #923, #939, #933, #931, #952, @dbczumar; #880, @zblz; #876, @acroz; #827, #812, #816, #829, @jimthompson5802; #837, #790, #897, #974, #900, @mparkhe; #831, #798, @aarondav; #814, @sueann; #824, #912, @mateiz; #922, #947, @tomasatdatabricks; #795, @KevYuen; #676, @mlaradji; #906, @4n4nd; #777, @tmielika; #804, @alkersan)

## 0.8.2 (2019-01-28)

MLflow 0.8.2 is a patch release on top of 0.8.1 containing only bug fixes and no breaking changes or features.

Bug fixes:

- [Python API] CloudPickle has been added to the set of MLflow library dependencies, fixing missing import errors when attempting to save models (#777, @tmielika)
- [Python API] Fixed a malformed logging call that prevented `mlflow.sagemaker.push_image_to_ecr()` invocations from succeeding (#784, @jackblandin)
- [Models] PyTorch models can now be saved with code dependencies, allowing model classes to be loaded successfully in new environments (#842, #836, @dbczumar)
- [Artifacts] Fixed a timeout when logging zero-length files to DBFS artifact stores (#818, @smurching)

Small docs updates (#845, @stbof; #840, @grahamhealy20; #839, @wilderrodrigues)

## 0.8.1 (2018-12-21)

MLflow 0.8.1 introduces several significant improvements:

- Improved UI responsiveness and load time, especially when displaying experiments containing hundreds to thousands of runs.
- Improved visualizations, including interactive scatter plots for MLflow run comparisons
- Expanded support for scoring Python models as Spark UDFs. For more information, see the [updated documentation for this feature](https://mlflow.org/docs/latest/models.html#export-a-python-function-model-as-an-apache-spark-udf).
- By default, saved models will now include a Conda environment specifying all of the dependencies necessary for loading them in a new environment.

Features:

- [API/CLI] Support for running MLflow projects from ZIP files (#759, @jmorefieldexpe)
- [Python API] Support for passing model conda environments as dictionaries to `save_model` and `log_model` functions (#748, @dbczumar)
- [Models] Default Anaconda environments have been added to many Python model flavors. By default, models produced by `save_model` and `log_model` functions will include an environment that specifies all of the versioned dependencies necessary to load and serve the models. Previously, users had to specify these environments manually. (#705, #707, #708, #749, @dbczumar)
- [Scoring] Support for synchronous deployment of models to SageMaker (#717, @dbczumar)
- [Tracking] Include the Git repository URL as a tag when tracking an MLflow run within a Git repository (#741, @whiletruelearn, @mateiz)
- [UI] Improved runs UI performance by using a react-virtualized table to optimize row rendering (#765, #762, #745, @smurching)
- [UI] Significant performance improvements for rendering run metrics, tags, and parameter information (#764, #747, @smurching)
- [UI] Scatter plots, including run comparison plots, are now interactive (#737, @mateiz)
- [UI] Extended CSRF support by allowing the MLflow UI server to specify a set of expected headers that clients should set when making AJAX requests (#733, @aarondav)

Bug fixes and documentation updates:

- [Python/Scoring] MLflow Python models that produce Pandas DataFrames can now be evaluated as Spark UDFs correctly. Spark UDF outputs containing multiple columns of primitive types are now supported (#719, @tomasatdatabricks)
- [Scoring] Fixed a serialization error that prevented models served with Azure ML from returning Pandas DataFrames (#754, @dbczumar)
- [Docs] New example demonstrating how the MLflow REST API can be used to create experiments and log run information (#750, kjahan)
- [Docs] R documentation has been updated for clarity and style consistency (#683, @stbof)
- [Docs] Added clarification about user setup requirements for executing remote MLflow runs on Databricks (#736, @andyk)

Small bug fixes and doc updates (#768, #715, @smurching; #728, dodysw; #730, mshr-h; #725, @kryptec; #769, #721, @dbczumar; #714, @stbof)

## 0.8.0 (2018-11-08)

MLflow 0.8.0 introduces several major features:

- Dramatically improved UI for comparing experiment run results:

  - Metrics and parameters are by default grouped into a single column, to avoid an explosion of mostly-empty columns. Individual metrics and parameters can be moved into their own column to help compare across rows.
  - Runs that are "nested" inside other runs (e.g., as part of a hyperparameter search or multistep workflow) now show up grouped by their parent run, and can be expanded or collapsed altogether. Runs can be nested by calling `mlflow.start_run` or `mlflow.run` while already within a run.
  - Run names (as opposed to automatically generated run UUIDs) now show up instead of the run ID, making comparing runs in graphs easier.
  - The state of the run results table, including filters, sorting, and expanded rows, is persisted in browser local storage, making it easier to go back and forth between an individual run view and the table.

- Support for deploying models as Docker containers directly to Azure Machine Learning Service Workspace (as opposed to the previously-recommended solution of Azure ML Workbench).

Breaking changes:

- [CLI] `mlflow sklearn serve` has been removed in favor of `mlflow pyfunc serve`, which takes the same arguments but works against any pyfunc model (#690, @dbczumar)

Features:

- [Scoring] pyfunc server and SageMaker now support the pandas "split" JSON format in addition to the "records" format. The split format allows the client to specify the order of columns, which is necessary for some model formats. We recommend switching client code over to use this new format (by sending the Content-Type header `application/json; format=pandas-split`), as it will become the default JSON format in MLflow 0.9.0. (#690, @dbczumar)
- [UI] Add compact experiment view (#546, #620, #662, #665, @smurching)
- [UI] Add support for viewing & tracking nested runs in experiment view (#588, @andrewmchen; #618, #619, @aarondav)
- [UI] Persist experiments view filters and sorting in browser local storage (#687, @smurching)
- [UI] Show run name instead of run ID when present (#476, @smurching)
- [Scoring] Support for deploying Models directly to Azure Machine Learning Service Workspace (#631, @dbczumar)
- [Server/Python/Java] Add `rename_experiment` to Tracking API (#570, @aarondav)
- [Server] Add `get_experiment_by_name` to RestStore (#592, @dmarkhas)
- [Server] Allow passing gunicorn options when starting mlflow server (#626, @mparkhe)
- [Python] Cloudpickle support for sklearn serialization (#653, @dbczumar)
- [Artifacts] FTP artifactory store added (#287, @Shenggan)

Bug fixes and documentation updates:

- [Python] Update TensorFlow integration to match API provided by other flavors (#612, @dbczumar; #670, @mlaradji)
- [Python] Support for TensorFlow 1.12 (#692, @smurching)
- [R] Explicitly loading Keras module at predict time no longer required (#586, @kevinykuo)
- [R] pyfunc serve can correctly load models saved with the R Keras support (#634, @tomasatdatabricks)
- [R] Increase network timeout of calls to the RestStore from 1 second to 60 seconds (#704, @aarondav)
- [Server] Improve errors returned by RestStore (#582, @andrewmchen; #560, @smurching)
- [Server] Deleting the default experiment no longer causes it to be immediately recreated (#604, @andrewmchen; #641, @schipiga)
- [Server] Azure Blob Storage artifact repo supports Windows paths (#642, @marcusrehm)
- [Server] Improve behavior when environment and run files are corrupted (#632, #654, #661, @mparkhe)
- [UI] Improve error page when viewing nonexistent runs or views (#600, @andrewmchen; #560, @andrewmchen)
- [UI] UI no longer throws an error if all experiments are deleted (#605, @andrewmchen)
- [Docs] Include diagram of workflow for multistep example (#581, @dennyglee)
- [Docs] Add reference tags and R and Java APIs to tracking documentation (#514, @stbof)
- [Docs/R] Use CRAN installation (#686, @javierluraschi)

Small bug fixes and doc updates (#576, #594, @javierluraschi; #585, @kevinykuo; #593, #601, #611, #650, #669, #671, #679, @dbczumar; #607, @suzil; #583, #615, @andrewmchen; #622, #681, @aarondav; #625, @pogil; #589, @tomasatdatabricks; #529, #635, #684, @stbof; #657, @mvsusp; #682, @mateiz; #678, vfdev-5; #596, @yutannihilation; #663, @smurching)

## 0.7.0 (2018-10-01)

MLflow 0.7.0 introduces several major features:

- An R client API (to be released on CRAN soon)
- Support for deleting runs (API + UI)
- UI support for adding notes to a run

The release also includes bugfixes and improvements across the Python and Java clients, tracking UI,
and documentation.

Breaking changes:

- [Python] The per-flavor implementation of load_pyfunc has been made private (#539, @tomasatdatabricks)
- [REST API, Java] logMetric now accepts a double metric value instead of a float (#566, @aarondav)

Features:

- [R] Support for R (#370, #471, @javierluraschi; #548 @kevinykuo)
- [UI] Add support for adding notes to Runs (#396, @aadamson)
- [Python] Python API, REST API, and UI support for deleting Runs (#418, #473, #526, #579 @andrewmchen)
- [Python] Set a tag containing the branch name when executing a branch of a Git project (#469, @adrian555)
- [Python] Add a set_experiment API to activate an experiment before starting runs (#462, @mparkhe)
- [Python] Add arguments for specifying a parent run to tracking & projects APIs (#547, @andrewmchen)
- [Java] Add Java set tag API (#495, @smurching)
- [Python] Support logging a conda environment with sklearn models (#489, @dbczumar)
- [Scoring] Support downloading MLflow scoring JAR from Maven during scoring container build (#507, @dbczumar)

Bug fixes:

- [Python] Print errors when the Databricks run fails to start (#412, @andrewmchen)
- [Python] Fix Spark ML PyFunc loader to work on Spark driver (#480, @tomasatdatabricks)
- [Python] Fix Spark ML load_pyfunc on distributed clusters (#490, @tomasatdatabricks)
- [Python] Fix error when downloading artifacts from a run's artifact root (#472, @dbczumar)
- [Python] Fix DBFS upload file-existence-checking logic during Databricks project execution (#510, @smurching)
- [Python] Support multi-line and unicode tags (#502, @mparkhe)
- [Python] Add missing DeleteExperiment, RestoreExperiment implementations in the Python REST API client (#551, @mparkhe)
- [Scoring] Convert Spark DataFrame schema to an MLeap schema prior to serialization (#540, @dbczumar)
- [UI] Fix bar chart always showing in metric view (#488, @smurching)

Small bug fixes and doc updates (#467 @drorata; #470, #497, #508, #518 @dbczumar; #455, #466, #492, #504, #527 @aarondav; #481, #475, #484, #496, #515, #517, #498, #521, #522, #573 @smurching; #477 @parkerzf; #494 @jainr; #501, #531, #532, #552 @mparkhe; #503, #520 @dmatrix; #509, #532 @tomasatdatabricks; #484, #486 @stbof; #533, #534 @javierluraschi; #542 @GCBallesteros; #511 @AdamBarnhard)

## 0.6.0 (2018-09-10)

MLflow 0.6.0 introduces several major features:

- A Java client API, available on Maven
- Support for saving and serving SparkML models as MLeap for low-latency serving
- Support for tagging runs with metadata, during and after the run completion
- Support for deleting (and restoring deleted) experiments

In addition to these features, there are a host of improvements and bugfixes to the REST API, Python API, tracking UI, and documentation. The [examples](https://github.com/mlflow/mlflow/tree/master/examples) subdirectory has also been revamped to make it easier to jump in, and examples demonstrating multistep workflows and hyperparameter tuning have been added.

Breaking changes:

We fixed a few inconsistencies in the the `mlflow.tracking` API, as introduced in 0.5.0:

- `MLflowService` has been renamed `MlflowClient` (#461, @mparkhe)
- You get an `MlflowClient` by calling `mlflow.tracking.MlflowClient()` (previously, this was `mlflow.tracking.get_service()`) (#461, @mparkhe)
- `MlflowService.list_runs` was changed to `MlflowService.list_run_infos` to reflect the information actually returned by the call. It now returns a `RunInfo` instead of a `Run` (#334, @aarondav)
- `MlflowService.log_artifact` and `MlflowService.log_artifacts` now take a `run_id` instead of `artifact_uri`. This now matches `list_artifacts` and `download_artifacts` (#444, @aarondav)

Features:

- Java client API added with support for the MLflow Tracking API (analogous to `mlflow.tracking`), allowing users to create and manage experiments, runs, and artifacts. The release includes a [usage example](https://github.com/mlflow/mlflow/blob/master/mlflow/java/client/src/main/java/org/mlflow/tracking/samples/QuickStartDriver.java>)and [Javadocs](https://mlflow.org/docs/latest/java_api/index.html). The client is published to Maven under `mlflow:mlflow` (#380, #394, #398, #409, #410, #430, #452, @aarondav)
- SparkML models are now also saved in MLeap format (https://github.com/combust/mleap), when applicable. Model serving platforms can choose to serve using this format instead of the SparkML format to dramatically decrease prediction latency. SageMaker now does this by default (#324, #327, #331, #395, #428, #435, #438, @dbczumar)
- [API] Experiments can now be deleted and restored via REST API, Python Tracking API, and MLflow CLI (#340, #344, #367, @mparkhe)
- [API] Tags can now be set via a SetTag API, and they have been moved to `RunData` from `RunInfo` (#342, @aarondav)
- [API] Added `list_artifacts` and `download_artifacts` to `MlflowService` to interact with a run's artifactory (#350, @andrewmchen)
- [API] Added `get_experiment_by_name` to Python Tracking API, and equivalent to Java API (#373, @vfdev-5)
- [API/Python] Version is now exposed via `mlflow.__version__`.
- [API/CLI] Added `mlflow artifacts` CLI to list, download, and upload to run artifact repositories (#391, @aarondav)
- [UI] Added icons to source names in MLflow Experiments UI (#381, @andrewmchen)
- [UI] Added support to view `.log` and `.tsv` files from MLflow artifacts UI (#393, @Shenggan; #433, @whiletruelearn)
- [UI] Run names can now be edited from within the MLflow UI (#382, @smurching)
- [Serving] Added `--host` option to `mlflow serve` to allow listening on non-local addresses (#401, @hamroune)
- [Serving/SageMaker] SageMaker serving takes an AWS region argument (#366, @dbczumar)
- [Python] Added environment variables to support providing HTTP auth (username, password, token) when talking to a remote MLflow tracking server (#402, @aarondav)
- [Python] Added support to override S3 endpoint for S3 artifactory (#451, @hamroune)
- MLflow nightly Python wheel and JAR snapshots are now available and linked from https://github.com/mlflow/mlflow (#352, @aarondav)

Bug fixes and documentation updates:

- [Python] `mlflow run` now logs default parameters, in addition to explicitly provided ones (#392, @mparkhe)
- [Python] `log_artifact` in FileStore now requires a relative path as the artifact path (#439, @mparkhe)
- [Python] Fixed string representation of Python entities, so they now display both their type and serialized fields (#371, @smurching)
- [UI] Entry point name is now shown in MLflow UI (#345, @aarondav)
- [Models] Keras model export now includes TensorFlow graph explicitly to ensure the model can always be loaded at deployment time (#440, @tomasatdatabricks)
- [Python] Fixed issue where FileStore ignored provided Run Name (#358, @adrian555)
- [Python] Fixed an issue where any `mlflow run` failing printed an extraneous exception (#365, @smurching)
- [Python] uuid dependency removed (#351, @antonpaquin)
- [Python] Fixed issues with remote execution on Databricks (#357, #361, @smurching; #383, #387, @aarondav)
- [Docs] Added [comprehensive example](https://github.com/mlflow/mlflow/tree/master/examples/multistep_workflow) of doing a multistep workflow, chaining MLflow runs together and reusing results (#338, @aarondav)
- [Docs] Added [comprehensive example](https://github.com/mlflow/mlflow/tree/master/examples/hyperparam) of doing hyperparameter tuning (#368, @tomasatdatabricks)
- [Docs] Added code examples to `mlflow.keras` API (#341, @dmatrix)
- [Docs] Significant improvements to Python API documentation (#454, @stbof)
- [Docs] Examples folder refactored to improve readability. The examples now reside in `examples/` instead of `example/`, too (#399, @mparkhe)
- Small bug fixes and doc updates (#328, #363, @ToonKBC; #336, #411, @aarondav; #284, @smurching; #377, @mparkhe; #389, gioa; #408, @aadamson; #397, @vfdev-5; #420, @adrian555; #459, #463, @stbof)

## 0.5.2 (2018-08-24)

MLflow 0.5.2 is a patch release on top of 0.5.1 containing only bug fixes and no breaking changes or features.

Bug fixes:

- Fix a bug with ECR client creation that caused `mlflow.sagemaker.deploy()` to fail when searching for a deployment Docker image (#366, @dbczumar)

## 0.5.1 (2018-08-23)

MLflow 0.5.1 is a patch release on top of 0.5.0 containing only bug fixes and no breaking changes or features.

Bug fixes:

- Fix `with mlflow.start_run() as run` to actually set `run` to the created Run (previously, it was None) (#322, @tomasatdatabricks)
- Fixes to DBFS artifactory to throw an exception if logging an artifact fails (#309) and to mimic FileStore's behavior of logging subdirectories (#347, @andrewmchen)
- Fix for Python 3.7 support with tarfiles (#329, @tomasatdatabricks)
- Fix spark.load_model not to delete the DFS tempdir (#335, @aarondav)
- MLflow UI now appropriately shows entrypoint if it's not main (#345, @aarondav)
- Make Python API forward-compatible with newer server versions of protos (#348, @aarondav)
- Improved API docs (#305, #284, @smurching)

## 0.5.0 (2018-08-17)

MLflow 0.5.0 offers some major improvements, including Keras and PyTorch first-class support as models, SFTP support as an artifactory, a new scatterplot visualization to compare runs, and a more complete Python SDK for experiment and run management.

Breaking changes:

- The Tracking API has been split into two pieces, a "basic logging" API and a "tracking service" API. The "basic logging" API deals with logging metrics, parameters, and artifacts to the currently-active active run, and is accessible in `mlflow` (e.g., `mlflow.log_param`). The tracking service API allow managing experiments and runs (especially historical runs) and is available in `mlflow.tracking`. The tracking service API will look analogous to the upcoming R and Java Tracking Service SDKs. Please be aware of the following breaking changes:

  - `mlflow.tracking` no longer exposes the basic logging API, only `mlflow`. So, code that was written like `from mlflow.tracking import log_param` will have to be `from mlflow import log_param` (note that almost all examples were already doing this).
  - Access to the service API goes through the `mlflow.tracking.get_service()` function, which relies on the same tracking server set by either the environment variable `MLFLOW_TRACKING_URI` or by code with `mlflow.tracking.set_tracking_uri()`. So code that used to look like `mlflow.tracking.get_run()` will now have to do `mlflow.tracking.get_service().get_run()`. This does not apply to the basic logging API.
  - `mlflow.ActiveRun` has been converted into a lightweight wrapper around `mlflow.entities.Run` to enable the Python `with` syntax. This means that there are no longer any special methods on the object returned when calling `mlflow.start_run()`. These can be converted to the service API.

  - The Python entities returned by the tracking service API are now accessible in `mlflow.entities` directly. Where previously you may have used `mlflow.entities.experiment.Experiment`, you would now just use `mlflow.entities.Experiment`. The previous version still exists, but is deprecated and may be hidden in a future version.

- REST API endpoint `/ajax-api/2.0/preview/mlflow/artifacts/get` has been moved to `$static_prefix/get-artifact`. This change is coversioned in the JavaScript, so should not be noticeable unless you were calling the REST API directly (#293, @andremchen)

Features:

- [Models] Keras integration: we now support logging Keras models directly in the log_model API, model format, and serving APIs (#280, @ToonKBC)
- [Models] PyTorch integration: we now support logging PyTorch models directly in the log_model API, model format, and serving APIs (#264, @vfdev-5)
- [UI] Scatterplot added to "Compare Runs" view to help compare runs using any two metrics as the axes (#268, @ToonKBC)
- [Artifacts] SFTP artifactory store added (#260, @ToonKBC)
- [Sagemaker] Users can specify a custom VPC when deploying SageMaker models (#304, @dbczumar)
- Pyfunc serialization now includes the Python version, and warns if the major version differs (can be suppressed by using `load_pyfunc(suppress_warnings=True)`) (#230, @dbczumar)
- Pyfunc serve/predict will activate conda environment stored in MLModel. This can be disabled by adding `--no-conda` to `mlflow pyfunc serve` or `mlflow pyfunc predict` (#225, @0wu)
- Python SDK formalized in `mlflow.tracking`. This includes adding SDK methods for `get_run`, `list_experiments`, `get_experiment`, and `set_terminated`. (#299, @aarondav)
- `mlflow run` can now be run against projects with no `conda.yaml` specified. By default, an empty conda environment will be created -- previously, it would just fail. You can still pass `--no-conda` to avoid entering a conda environment altogether (#218, @smurching)

Bug fixes:

- Fix numpy array serialization for int64 and other related types, allowing pyfunc to return such results (#240, @arinto)
- Fix DBFS artifactory calling `log_artifacts` with binary data (#295, @aarondav)
- Fix Run Command shown in UI to reproduce a run when the original run is targeted at a subdirectory of a Git repo (#294, @adrian555)
- Filter out ubiquitous dtype/ufunc warning messages (#317, @aarondav)
- Minor bug fixes and documentation updates (#261, @stbof; #279, @dmatrix; #313, @rbang1, #320, @yassineAlouini; #321, @tomasatdatabricks; #266, #282, #289, @smurching; #267, #265, @aarondav; #256, #290, @ToonKBC; #273, #263, @mateiz; #272, #319, @adrian555; #277, @aadamson; #283, #296, @andrewmchen)

## 0.4.2 (2018-08-07)

Breaking changes: None

Features:

- MLflow experiments REST API and `mlflow experiments create` now support providing `--artifact-location` (#232, @aarondav)
- [UI] Runs can now be sorted by columns, and added a Select All button (#227, @ToonKBC)
- Databricks File System (DBFS) artifactory support added (#226, @andrewmchen)
- databricks-cli version upgraded to >= 0.8.0 to support new DatabricksConfigProvider interface (#257, @aarondav)

Bug fixes:

- MLflow client sends REST API calls using snake_case instead of camelCase field names (#232, @aarondav)
- Minor bug fixes (#243, #242, @aarondav; #251, @javierluraschi; #245, @smurching; #252, @mateiz)

## 0.4.1 (2018-08-03)

Breaking changes: None

Features:

- [Projects] MLflow will use the conda installation directory given by the `$MLFLOW_CONDA_HOME`
  if specified (e.g. running conda commands by invoking `$MLFLOW_CONDA_HOME/bin/conda`), defaulting
  to running "conda" otherwise. (#231, @smurching)
- [UI] Show GitHub links in the UI for projects run from http(s):// GitHub URLs (#235, @smurching)

Bug fixes:

- Fix GCSArtifactRepository issue when calling list_artifacts on a path containing nested directories (#233, @jakeret)
- Fix Spark model support when saving/loading models to/from distributed filesystems (#180, @tomasatdatabricks)
- Add missing mlflow.version import to sagemaker module (#229, @dbczumar)
- Validate metric, parameter and run IDs in file store and Python client (#224, @mateiz)
- Validate that the tracking URI is a remote URI for Databricks project runs (#234, @smurching)
- Fix bug where we'd fetch git projects at SSH URIs into a local directory with the same name as
  the URI, instead of into a temporary directory (#236, @smurching)

## 0.4.0 (2018-08-01)

Breaking changes:

- [Projects] Removed the `use_temp_cwd` argument to `mlflow.projects.run()`
  (`--new-dir` flag in the `mlflow run` CLI). Runs of local projects now use the local project
  directory as their working directory. Git projects are still fetched into temporary directories
  (#215, @smurching)
- [Tracking] GCS artifact storage is now a pluggable dependency (no longer installed by default).
  To enable GCS support, install `google-cloud-storage` on both the client and tracking server via pip.
  (#202, @smurching)
- [Tracking] Clients running MLflow 0.4.0 and above require a server running MLflow 0.4.0
  or above, due to a fix that ensures clients no longer double-serialize JSON into strings when
  sending data to the server (#200, @aarondav). However, the MLflow 0.4.0 server remains
  backwards-compatible with older clients (#216, @aarondav)

Features:

- [Examples] Add a more advanced tracking example: using MLflow with PyTorch and TensorBoard (#203)
- [Models] H2O model support (#170, @ToonKBC)
- [Projects] Support for running projects in subdirectories of Git repos (#153, @juntai-zheng)
- [SageMaker] Support for specifying a compute specification when deploying to SageMaker (#185, @dbczumar)
- [Server] Added --static-prefix option to serve UI from a specified prefix to MLflow UI and server (#116, @andrewmchen)
- [Tracking] Azure blob storage support for artifacts (#206, @mateiz)
- [Tracking] Add support for Databricks-backed RestStore (#200, @aarondav)
- [UI] Enable productionizing frontend by adding CSRF support (#199, @aarondav)
- [UI] Update metric and parameter filters to let users control column order (#186, @mateiz)

Bug fixes:

- Fixed incompatible file structure returned by GCSArtifactRepository (#173, @jakeret)
- Fixed metric values going out of order on x axis (#204, @mateiz)
- Fixed occasional hanging behavior when using the projects.run API (#193, @smurching)

- Miscellaneous bug and documentation fixes from @aarondav, @andrewmchen, @arinto, @jakeret, @mateiz, @smurching, @stbof

## 0.3.0 (2018-07-18)

Breaking changes:

- [MLflow Server] Renamed `--artifact-root` parameter to `--default-artifact-root` in `mlflow server` to better reflect its purpose (#165, @aarondav)

Features:

- Spark MLlib integration: we now support logging SparkML Models directly in the log_model API, model format, and serving APIs (#72, @tomasatdatabricks)
- Google Cloud Storage is now supported as an artifact storage root (#152, @bnekolny)
- Support asychronous/parallel execution of MLflow runs (#82, @smurching)
- [SageMaker] Support for deleting, updating applications deployed via SageMaker (#145, @dbczumar)
- [SageMaker] Pushing the MLflow SageMaker container now includes the MLflow version that it was published with (#124, @sueann)
- [SageMaker] Simplify parameters to SageMaker deploy by providing sane defaults (#126, @sueann)
- [UI] One-element metrics are now displayed as a bar char (#118, @cryptexis)

Bug fixes:

- Require gitpython>=2.1.0 (#98, @aarondav)
- Fixed TensorFlow model loading so that columns match the output names of the exported model (#94, @smurching)
- Fix SparkUDF when number of columns >= 10 (#97, @aarondav)
- Miscellaneous bug and documentation fixes from @emres, @dmatrix, @stbof, @gsganden, @dennyglee, @anabranch, @mikehuston, @andrewmchen, @juntai-zheng

## 0.2.1 (2018-06-28)

This is a patch release fixing some smaller issues after the 0.2.0 release.

- Switch protobuf implementation to C, fixing a bug related to tensorflow/mlflow import ordering (issues #33 and #77, PR #74, @andrewmchen)
- Enable running mlflow server without git binary installed (#90, @aarondav)
- Fix Spark UDF support when running on multi-node clusters (#92, @aarondav)

## 0.2.0 (2018-06-27)

- Added `mlflow server` to provide a remote tracking server. This is akin to `mlflow ui` with new options:

  - `--host` to allow binding to any ports (#27, @mdagost)
  - `--artifact-root` to allow storing artifacts at a remote location, S3 only right now (#78, @mateiz)
  - Server now runs behind gunicorn to allow concurrent requests to be made (#61, @mateiz)

- TensorFlow integration: we now support logging TensorFlow Models directly in the log_model API, model format, and serving APIs (#28, @juntai-zheng)
- Added `experiments.list_experiments` as part of experiments API (#37, @mparkhe)
- Improved support for unicode strings (#79, @smurching)
- Diabetes progression example dataset and training code (#56, @dennyglee)
- Miscellaneous bug and documentation fixes from @Jeffwan, @yupbank, @ndjido, @xueyumusic, @manugarri, @tomasatdatabricks, @stbof, @andyk, @andrewmchen, @jakeret, @0wu, @aarondav

## 0.1.0 (2018-06-05)

- Initial version of mlflow.


--- CONTRIBUTING.md ---
# Contributing to MLflow

We welcome community contributions to MLflow. This page provides useful information about contributing to MLflow.

**Table of Contents**

- [Governance](#governance)
- [Core Members](#core-members)
- [Contribution process](#contribution-process)
- [Contribution guidelines](#contribution-guidelines)
  - [Write designs for significant changes](#write-designs-for-significant-changes)
  - [Make changes backwards compatible](#make-changes-backwards-compatible)
  - [Consider introducing new features as MLflow Plugins](#consider-introducing-new-features-as-mlflow-plugins)
  - [Python Style Guide](#python-style-guide)
- [Setting up the repository](#setting-up-the-repository)
- [Developing and testing MLflow](#developing-and-testing-mlflow)
  - [Environment Setup and Python configuration](#environment-setup-and-python-configuration)
    - [Automated Python development environment configuration](#automated-python-development-environment-configuration)
    - [Manual Python development environment configuration](#manual-python-development-environment-configuration)
  - [JavaScript and UI](#javascript-and-ui)
    - [Install Node Module Dependencies](#install-node-module-dependencies)
    - [Install Node Modules](#install-node-modules)
    - [Launching the Development UI](#launching-the-development-ui)
    - [Running the Javascript Dev Server](#running-the-javascript-dev-server)
    - [Testing a React Component](#testing-a-react-component)
    - [Linting Javascript Code](#linting-javascript-code)
  - [R](#r)
  - [Java](#java)
  - [Python](#python)
    - [Writing Python Tests](#writing-python-tests)
    - [Running Python Tests](#running-python-tests)
    - [Python Client](#python-client)
      - [Python Model Flavors](python-model-flavors)
    - [Python Server](#python-server)
      - [Building Protobuf Files](#building-protobuf-files)
      - [Database Schema Changes](#database-schema-changes)
  - [Writing MLflow Examples](#writing-mlflow-examples)
  - [Building a Distributable Artifact](#building-a-distributable-artifact)
  - [Writing Docs](#writing-docs)
  - [Sign your work](#sign-your-work)
- [Code of Conduct](#code-of-conduct)

## Governance

Governance of MLflow is conducted by the Technical Steering Committee
(TSC), which currently includes the following members:

- Patrick Wendell (<pwendell@gmail.com>)
- Reynold Xin (<reynoldx@gmail.com>)
- Matei Zaharia (<matei@cs.stanford.edu>)

The founding technical charter can be found
[here](https://github.com/mlflow/mlflow/blob/master/mlflow-charter.pdf).

## Core Members

MLflow is currently maintained by the following core members with significant contributions from hundreds of exceptionally talented community members.

- [Harutaka Kawamura](https://github.com/harupy)
- [Weichen Xu](https://github.com/WeichenXu123)
- [Corey Zumar](https://github.com/dbczumar)
- [Ben Wilson](https://github.com/BenWilson2)
- [Serena Ruan](https://github.com/serena-ruan)
- [Yuki Watanabe](https://github.com/B-Step62)
- [Daniel Lok](https://github.com/daniellok-db)
- [Tomu Hirata](https://github.com/TomeHirata)
- [Gabriel Fu](https://github.com/gabrielfu)

## Contribution process

The MLflow contribution process starts with filing a GitHub issue.
MLflow defines four categories of issues: feature requests, bug reports,
documentation fixes, and installation issues. Details about each issue
type and the issue lifecycle are discussed in the [MLflow Issue
Policy](https://github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md).

MLflow committers actively [triage](ISSUE_TRIAGE.rst) and respond to
GitHub issues. In general, we recommend waiting for feedback from an
MLflow committer or community member before proceeding to implement a
feature or patch. This is particularly important for [significant
changes](https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#write-designs-for-significant-changes),
and will typically be labeled during triage with `needs design`.

After you have agreed upon an implementation strategy for your feature
or patch with an MLflow committer, the next step is to introduce your
changes (see [developing
changes](https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#developing-and-testing-mlflow))
as a pull request against the MLflow Repository (we recommend pull
requests be filed from a non-master branch on a repository fork) or as a
standalone MLflow Plugin. MLflow committers actively review pull
requests and are also happy to provide implementation guidance for
Plugins.

Once your pull request against the MLflow Repository has been merged,
your corresponding changes will be automatically included in the next
MLflow release. Every change is listed in the MLflow release notes and
[Changelog](https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md).

Congratulations, you have just contributed to MLflow. We appreciate your
contribution\!

## Contribution guidelines

In this section, we provide guidelines to consider as you develop new
features and patches for MLflow.

### Write designs for significant changes

For significant changes to MLflow, we recommend outlining a design for
the feature or patch and discussing it with an MLflow committer before
investing heavily in implementation. During issue triage, we try to
proactively identify issues require design by labeling them with `needs design`. This is particularly important if your proposed implementation:

- Introduces changes or additions to the [MLflow REST
  API](https://mlflow.org/docs/latest/rest-api.html)
  - The MLflow REST API is implemented by a variety of open source
    and proprietary platforms. Changes to the REST API impact all of
    these platforms. Accordingly, we encourage developers to
    thoroughly explore alternatives before attempting to introduce
    REST API changes.
- Introduces new user-facing MLflow APIs
  - MLflow's API surface is carefully designed to generalize across
    a variety of common ML operations. It is important to ensure
    that new APIs are broadly useful to ML developers, easy to work
    with, and simple yet powerful.
- Adds new library dependencies to MLflow
- Makes changes to critical internal abstractions. Examples include:
  the Tracking Artifact Repository, the Tracking Abstract Store, and
  the Model Registry Abstract Store.

### Make changes backwards compatible

MLflow's users rely on specific platform and API behaviors in their
daily workflows. As new versions of MLflow are developed and released,
it is important to ensure that users' workflows continue to operate as
expected. Accordingly, please take care to consider backwards
compatibility when introducing changes to the MLflow code base. If you
are unsure of the backwards compatibility implications of a particular
change, feel free to ask an MLflow committer or community member for
input.

In addition to public APIs, any Python APIs within MLflow that are designated with the
annotation `@developer_stable` must remain backwards compatible. Any contribution
that adds features, modifies behavior, or otherwise changes the functionality within the
scope of these classes or methods will be closely reviewed by maintainers, and additional
backwards compatibility testing may be requested.

### Consider introducing new features as MLflow Plugins

[MLflow Plugins](https://mlflow.org/docs/latest/plugins.html) enable
integration of third-party modules with many of MLflow's components,
allowing you to maintain and iterate on certain features independently
of the MLflow Repository. Before implementing changes to the MLflow code
base, consider whether your feature might be better structured as an
MLflow Plugin. MLflow Plugins are a great choice for the following types
of changes:

1.  Supporting a new storage platform for MLflow artifacts
2.  Introducing a new implementation of the MLflow Tracking backend
    ([Abstract
    Store](https://github.com/mlflow/mlflow/blob/cdc6a651d5af0f29bd448d2c87a198cf5d32792b/mlflow/store/tracking/abstract_store.py))
    for a particular platform
3.  Introducing a new implementation of the Model Registry backend
    ([Abstract
    Store](https://github.com/mlflow/mlflow/blob/cdc6a651d5af0f29bd448d2c87a198cf5d32792b/mlflow/store/model_registry/abstract_store.py))
    for a particular platform
4.  Automatically capturing and recording information about MLflow Runs
    created in specific environments

MLflow committers and community members are happy to provide assistance
with the development and review of new MLflow Plugins.

Finally, MLflow maintains a list of Plugins developed by community
members, which is located at
<https://mlflow.org/docs/latest/plugins.html#community-plugins>. This is
an excellent way to inform MLflow users about your exciting new Plugins.
To list your plugin, simply introduce a new pull request against the
[corresponding docs section of the MLflow code
base](https://github.com/mlflow/mlflow/blob/cdc6a651d5af0f29bd448d2c87a198cf5d32792b/docs/source/plugins.rst#community-plugins).

For more information about Plugins, see
<https://mlflow.org/docs/latest/plugins.html>.

### Python Style Guide

##### Docstrings

We follow [Google's Python Style Guide](https://google.github.io/styleguide/pyguide.html)
for writing docstrings. Make sure your docstrings adhere to this style
guide.

###### Code Style

We use [prettier](https://prettier.io/),
[blacken-docs](https://pypi.org/project/blacken-docs/), [ruff](https://github.com/astral-sh/ruff), and
a number of custom lint checking scripts in our CI via
pre-commit Git hooks. If your code passes the CI checks, it's
formatted correctly.

To validate that your local versions of the above libraries
match those in the mlflow CI, refer to [lint-requirements.txt](https://github.com/mlflow/mlflow/blob/master/requirements/lint-requirements.txt).
You can compare these versions with your local using pip:

```bash
pip show ruff
```

## Setting up the repository

To set up the MLflow repository, run the following commands:

```bash
# Clone the repository
git clone --recurse-submodules git@github.com:<username>/mlflow.git
# The alternative way of cloning through https may cause permission error during branch push
# git clone --recurse-submodules https://github.com/<username>/mlflow.git

# Add the upstream repository
cd mlflow
git remote add upstream git@github.com:mlflow/mlflow.git
```

If you cloned the repository before without `--recurse-submodules`, run
this command to fetch submodules:

```bash
git submodule update --init --recursive
```

## Developing and testing MLflow

The majority of the MLflow codebase is developed in Python. This
includes the CLI, Tracking Server, Artifact Repositories (e.g., S3 or
Azure Blob Storage backends), and of course the Python fluent, tracking,
and model APIs.

### Environment Setup and Python configuration

Having a standardized development environment is advisable when working
on MLflow. Creating an environment that contains the required Python
packages (and versions), linting tools, and environment configurations
will help to prevent unnecessary CI failures when filing a PR. A
correctly configured local environment will also allow you to run tests
locally in an environment that mimics that of the CI execution
environment.

There are three means of setting up a base Python development environment
for MLflow: GitHub Codespaces, automated (through the
[dev-env-setup.sh](https://github.com/mlflow/mlflow/tree/master/dev/dev-env-setup.sh)
script) or manual. Even in a manual-based approach (i.e., testing
functionality of a specific version of a model flavor's package
version), the automated script can save a great deal of time and reduce
errors in creating the environment.

#### GitHub Codespaces

<img src="./assets/create-codespace.png" width="60%"/>

1. Navigate to https://github.com/mlflow/mlflow.git.
2. Above the file list, click `Code`, then select `Create codespace` and wait for your codespace to be created.

See [Quickstart for GitHub Codespaces](https://docs.github.com/en/codespaces/getting-started/quickstart) for more information.

#### Automated Python development environment configuration

The automated development environment setup script
([dev-env-setup.sh](https://github.com/mlflow/mlflow/tree/master/dev/dev-env-setup.sh))
can be used to setup a development environment that is configured with
all of the dependencies required and the environment configuration
needed to develop and locally test the Python code portions of MLflow.
This CLI tool's readme can be accessed via the root of the mlflow
repository as follows:

```bash
dev/dev-env-setup.sh -h
```

An example usage of this script that will build a development
environment using `virtualenv` and the minimum supported Python version
(to ensure compatibility) is:

```bash
dev/dev-env-setup.sh -d .venvs/mlflow-dev -q
```

The `-q` parameter is to "quiet" the pip install processes preventing
stdout printing during installation.

It is advised to follow all of the prompts to ensure that the
configuration of the environment, as well as git, are completed so that
your PR process is as effortless as possible.

**Note**

Frequently, a specific version of a library is required in order to
validate a feature's compatibility with older versions. Modifying your
primary development environment to test one-off compatibility can be
very error-prone and result in an environment that is significantly
different from that of the CI test environment. To support this use
case, the automated script can be used to create an environment that can
be easily modified to support testing a particular version of a model
flavor in an isolated environment. Simply run the `dev-env-setup.sh`
script, activate the new environment, and install the required version
for testing.

</div>

Example of installing an older version of `scikit-learn` to perform
isolated testing:

```bash
dev/dev-env-setup.sh -d ~/.venvs/sklearn-test -q
source ~/.venvs/sklearn-test/bin/activate
pip freeze | grep "scikit-learn"
>> scikit-learn==1.0.2
pip install scikit-learn==1.0.1
pip freeze | grep "scikit-learn"
>> scikit-learn==1.0.1
```

#### Manual Python development environment configuration

The manual process is recommended if you are going to use Conda or if
you are fond of terminal setup processes. To start with the manual
process, ensure that you have either conda or virtualenv installed.

First, ensure that your name and email are [configured in
git](https://git-scm.com/book/en/v2/Getting-Started-First-Time-Git-Setup)
so that you can [sign your work](#sign-your-work) when committing code
changes and opening pull requests:

```bash
git config --global user.name "Your Name"
git config --global user.email yourname@example.com
```

For convenience, we provide a pre-commit git hook that validates that
commits are signed-off and runs `ruff check --fix` and `ruff format` to ensure the
code will pass the lint check for python. You can enable it by running:

```bash
pre-commit install --install-hooks
```

Then, install the Python MLflow package from source - this is required
for developing & testing changes across all languages and APIs. We
recommend installing MLflow in its own conda environment by running the
following from your checkout of MLflow:

```bash
conda create --name mlflow-dev-env python=3.8
conda activate mlflow-dev-env
pip install -e '.[extras]' # installs mlflow from current checkout with some useful extra utilities
```

If you plan on doing development and testing, you will also need to
install the following into the conda environment:

```bash
pip install -r requirements/dev-requirements.txt
pip install -e '.[extras]'  # installs mlflow from current checkout
pip install -e tests/resources/mlflow-test-plugin # installs `mlflow-test-plugin` that is required for running certain MLflow tests
```

You may need to run `conda install cmake` for the test requirements to
properly install, as `onnx` needs `cmake`.

Ensure [Docker](https://www.docker.com/) is installed.

Finally, we use `pytest` to test all Python contributed code. Install
`pytest`:

```bash
pip install pytest
```

### JavaScript and UI

The MLflow UI is written in JavaScript. `yarn` is required to run the
Javascript dev server and the tracking UI. You can verify that `yarn` is
on the PATH by running `yarn -v`, and [install
yarn](https://classic.yarnpkg.com/lang/en/docs/install) if needed.

#### Install Node Module Dependencies

On OSX, install the following packages required by the node modules:

```bash
brew install pixman cairo pango jpeg
```

Linux/Windows users will need to source these dependencies using the
appropriate package manager on their platforms.

#### Install Node Modules

Before running the Javascript dev server or building a distributable
wheel, install Javascript dependencies via:

```bash
cd mlflow/server/js
yarn install
cd - # return to root repository directory
```

If modifying dependencies in `mlflow/server/js/package.json`, run `yarn upgrade` within `mlflow/server/js` to install the updated dependencies.

#### Launching the Development UI

We recommend [Running the Javascript Dev
Server](#running-the-javascript-dev-server) - otherwise, the tracking
frontend will request files in the `mlflow/server/js/build` directory,
which is not checked into Git. Alternatively, you can generate the
necessary files in `mlflow/server/js/build` as described in [Building a
Distributable Artifact](#building-a-distributable-artifact).

#### Running the Javascript Dev Server

[Install Node Modules](#install-node-modules), then run the following in two separate shells:

In one shell:

```bash
mlflow ui
```

And in another shell:

```bash
cd mlflow/server/js
yarn start
```

The Javascript Dev Server will run at <http://localhost:3000> and the
MLflow server will run at <http://localhost:5000> and show runs logged
in `./mlruns`.

**Note:** On some versions of MacOS, the "Airplay Receiver" process runs on port 5000 by default,
which can cause [network request failures](https://stackoverflow.com/questions/72369320/why-always-something-is-running-at-port-5000-on-my-mac).
If you are encountering such issues, disable the
process via system settings, or specify another port (e.g. `mlflow server --port 8000`).

If specifying a different port, please set the following environment variables before running `yarn start`:

- `MLFLOW_PROXY=<tracking_server_uri>`
- `MLFLOW_DEV_PROXY_MODE=false`

For example:

```
$ mlflow server --port 8000
...

(in a separate shell)
$ export MLFLOW_PROXY=http://127.0.0.1:8000
$ export MLFLOW_DEV_PROXY_MODE=false
$ yarn install
$ yarn start
...

(UI should now be visible at localhost:3000)
```

#### Launching MLflow UI with MLflow AI Gateway for PromptLab

```sh
python dev/server.py
```

#### Testing a React Component

Add a test file in the same directory as the newly created React
component. For example, `CompareRunBox.test.js` should be added in the
same directory as `CompareRunBox.js`. Next, in `mlflow/server/js`, run
the following command to start the test.

```bash
# Run tests in CompareRunBox.test.js
yarn test CompareRunBox.test.js
# Run tests with a name that matches 'plot' in CompareRunBox.test.js
yarn test CompareRunBox.test.js -t 'plot'
# Run all tests
yarn test
```

#### Linting Javascript Code

In `mlflow/server/js`, run the following command to lint your code.

```bash
# Note this command only fixes auto-fixable issues (e.g. remove trailing whitespace)
yarn lint:fix
```

### R

If contributing to MLflow's R APIs, install
[R](https://cloud.r-project.org/) and make sure that you have satisfied
all the [Environment Setup and Python configuration](#environment-setup-and-python-configuration).

The `mlflow/R/mlflow` directory contains R wrappers for the Projects,
Tracking and Models components. These wrappers depend on the Python
package, so first install the Python package in a conda environment:

```bash
# Note that we don't pass the -e flag to pip, as the R tests attempt to run the MLflow UI
# via the CLI, which will not work if we run against the development tracking server
pip install .
```

[Install R](https://cloud.r-project.org/), then run the following to
install dependencies for building MLflow locally:

```bash
cd mlflow/R/mlflow
NOT_CRAN=true Rscript -e 'install.packages("devtools", repos = "https://cloud.r-project.org")'
NOT_CRAN=true Rscript -e 'devtools::install_deps(dependencies = TRUE)'
```

Build the R client via:

```bash
R CMD build .
```

Run tests:

```bash
R CMD check --no-build-vignettes --no-manual --no-tests mlflow*tar.gz
cd tests
NOT_CRAN=true LINTR_COMMENT_BOT=false Rscript ../.run-tests.R
cd -
```

Run linter:

```bash
Rscript -e 'lintr::lint_package()'
```

If opening a PR that makes API changes, please regenerate API
documentation as described in [Writing Docs](#writing-docs) and commit
the updated docs to your PR branch.

When developing, you can make Python changes available in R by running
(from mlflow/R/mlflow):

```bash
Rscript -e 'reticulate::conda_install("r-mlflow", "../../../.", pip = TRUE)'
```

Please also follow the recommendations from the [Advanced R - Style
Guide](http://adv-r.had.co.nz/Style.html) regarding naming and styling.

### Java

If contributing to MLflow's Java APIs or modifying Java documentation,
install [Java](https://www.java.com/) and [Apache
Maven](https://maven.apache.org/download.cgi).

A certain MLflow module is implemented in Java, under the `mlflow/java/`
directory. This is the Java Tracking API client (`mlflow/java/client`).

Other Java functionality (like artifact storage) depends on the Python
package, so first install the Python package in a conda environment as
described in [Environment Setup and Python configuration](#environment-setup-and-python-configuration).
[Install](https://www.oracle.com/technetwork/java/javase/downloads/index.html)
the Java 8 JDK (or above), and
[download](https://maven.apache.org/download.cgi) and
[install](https://maven.apache.org/install.html) Maven. You can then
build and run tests via:

```bash
cd mlflow/java
mvn compile test
```

If opening a PR that makes API changes, please regenerate API
documentation as described in [Writing Docs](#writing-docs) and commit
the updated docs to your PR branch.

### Python

If you are contributing in Python, make sure that you have satisfied all
the [Environment Setup and Python configuration](#environment-setup-and-python-configuration), including installing
`pytest`, as you will need it for the sections described below.

#### Writing Python Tests

If your PR includes code that isn't currently covered by our tests (e.g.
adding a new flavor, adding autolog support to a flavor, etc.), you
should write tests that cover your new code. Your tests should be added
to the relevant file under `tests`, or if there is no appropriate file,
in a new file prefixed with `test_` so that `pytest` includes that file
for testing.

If your tests require usage of a tracking URI, the [pytest
fixture](https://docs.pytest.org/en/3.2.1/fixture.html)
[tracking_uri_mock](https://github.com/mlflow/mlflow/blob/master/tests/conftest.py#L74)
is automatically set up for every tests. It sets up a mock tracking URI
that will set itself up before your test runs and tear itself down
after.

By default, runs are logged under a local temporary directory that's
unique to each test and torn down immediately after test execution. To
disable this behavior, decorate your test function with
`@pytest.mark.notrackingurimock`

#### Running Python Tests

Verify that the unit tests & linter pass before submitting a pull
request by running:

We use [ruff](https://docs.astral.sh/ruff/) to ensure a
consistent code format. You can auto-format your code by running:

```bash
ruff format .
ruff check .
```

Then, verify that the unit tests & linter pass before submitting a pull
request by running:

```bash
pre-commit run --all-files
pytest tests --quiet --requires-ssh --ignore-flavors --serve-wheel \
  --ignore=tests/examples --ignore=tests/evaluate
```

We use [pytest](https://docs.pytest.org/en/latest/contents.html) to run
Python tests. You can run tests for one or more test directories or
files via `pytest [file_or_dir] ... [file_or_dir]`. For example, to run
all pytest tests, you can run:

```bash
pytest tests/pyfunc
```

Note: Certain model tests are not well-isolated (can result in OOMs when
run in the same Python process), so simply invoking `pytest` or `pytest tests` may not work. If you'd like to run multiple model tests, we
recommend doing so via separate `pytest` invocations, e.g. `pytest tests/sklearn && pytest tests/tensorflow`

If opening a PR that changes or adds new APIs, please update or add
Python documentation as described in [Writing Docs](#writing-docs) and
commit the docs to your PR branch.

#### Python Client

For the client, if you are adding new model flavors, follow the
instructions below.

##### Python Model Flavors

If you are adding new framework flavor support, you'll need to modify
`pytest` and Github action configurations so tests for your code can run
properly. Generally, the files you'll have to edit are:

1.  `.github/workflows/master.yml`: lines where pytest runs with `--ignore-flavors` flag

    1. Add your tests to the ignore list, where the other frameworks are
       ignored
    2. Add a pytest command for your tests along with the other framework
       tests (as a separate command to avoid OOM issues)

2.  `requirements/test-requirements.txt`: add your framework and version
    to the list of requirements

You can see an example of a [flavor
PR](https://github.com/mlflow/mlflow/pull/2136/files).

#### Python Server

For the Python server, you can contribute in these two areas described
below.

##### Building Protobuf Files

To build protobuf files, simply run `./dev/generate-protos.sh`. The required
`protoc` version is `3.19.4`. You can find the URL of a
system-appropriate installation of `protoc` at
<https://github.com/protocolbuffers/protobuf/releases/tag/v3.19.4>, e.g.
<https://github.com/protocolbuffers/protobuf/releases/download/v3.19.4/protoc-3.19.4-osx-x86_64.zip>
if you're on 64-bit Mac OSX.

Alternatively, you can comment `/autoformat` on your PR to automatically compile the protobuf files and update the autogenerated code.

Once the autogenerated code is updated, verify that `.proto` files and autogenerated code are in sync by running `./dev/test-generate-protos.sh`.

##### Database Schema Changes

MLflow's Tracking component supports storing experiment and run data in
a SQL backend. To make changes to the tracking database schema, run the
following from your checkout of MLflow:

```bash
# starting at the root of the project
$ pwd
~/mlflow
$ cd mlflow
# MLflow relies on Alembic (https://alembic.sqlalchemy.org) for schema migrations.
$ alembic -c mlflow/store/db_migrations/alembic.ini revision -m "add new field to db"
  Generating ~/mlflow/mlflow/store/db_migrations/versions/b446d3984cfa_add_new_field_to_db.py
# Update schema files
$ ./tests/db/update_schemas.sh
```

These commands generate a new migration script (e.g., at
`~/mlflow/mlflow/alembic/versions/12341123_add_new_field_to_db.py`) that
you should then edit to add migration logic.

### Writing MLflow Examples

The `mlflow/examples` directory has a collection of quickstart tutorials
and various simple examples that depict MLflow tracking, project, model
flavors, model registry, and serving use cases. These examples provide
developers sample code, as a quick way to learn MLflow Python APIs.

To facilitate review, strive for brief examples that reflect real user
workflows, document how to run your example, and follow the recommended
steps below.

If you are contributing a new model flavor, follow these steps:

1.  Follow instructions in [Python Model Flavors](#python-model-flavors)
2.  Create a corresponding directory in
    `mlflow/examples/new-model-flavor`
3.  Implement your Python training `new-model-flavor` code in this
    directory
4.  Convert this directory's content into an [MLflow
    Project](https://mlflow.org/docs/latest/projects.html) executable
5.  Add `README.md`, `MLproject`, and `conda.yaml` files and your code
6.  Read instructions in the `mlflow/test/examples/README.md` and add a
    `pytest` entry in the `test/examples/test_examples.py`
7.  Add a short description in the `mlflow/examples/README.md` file

If you are contributing to the quickstart directory, we welcome changes
to the `quickstart/mlflow_tracking.py` that make it clearer or simpler.

If you'd like to provide an example of functionality that doesn't fit
into the above categories, follow these steps:

1.  Create a directory with meaningful name in
    `mlflow/examples/new-program-name` and implement your Python code
2.  Create `mlflow/examples/new-program-name/README.md` with
    instructions how to use it
3.  Read instructions in the `mlflow/test/examples/README.md`, and add a
    `pytest` entry in the `test/examples/test_examples.py`
4.  Add a short description in the `mlflow/examples/README.md` file

Finally, before filing a pull request, verify all Python tests pass.

### Building a Distributable Artifact

If you would like to build a fully functional version of MLflow from your local branch for testing or a local patch fix, first
[install the Node Modules](#install-node-modules), then run the following:

Generate JS files in `mlflow/server/js/build`:

```bash
cd mlflow/server/js
yarn build
```

Build a pip-installable wheel and a compressed code archive in `dist/`:

```bash
cd -
python -m build
```

### TOML formatting

We use [taplo](https://taplo.tamasfe.dev/) to enforce consistent TOML formatting. You can install it by following the instructions [here](https://taplo.tamasfe.dev/cli/introduction.html).

### Excluding Symlinks from IDE Searches

The `mlflow/skinny` symlink points to `../mlflow` and may cause duplicate entries in search results. To exclude it from searches, follow these steps:

**VSCode:**

1. Open `Settings`.
2. Search for `search.followSymlinks` and set it to `false`.

**PyCharm:**

1. Right-click `skinny/mlflow`.
2. Select `Mark Directory as` -> `Excluded`.

### Writing Docs

There are two separate build systems for the MLflow documentation:

#### API Docs

The [API reference](https://mlflow.org/docs/latest/api_reference/) is managed by [Sphinx](https://www.sphinx-doc.org/en/master/). The content is primarily populated by our Python docstrings, which are written in reStructuredText (RST).

For instructions on how to build the API docs, please check the [README.md](https://github.com/mlflow/mlflow/blob/master/docs/api_reference/README.md) in the `docs/api_reference/` subfolder.

#### Main Docs

The main MLflow docs (e.g. feature docs, tutorials, etc) are written using [Docusaurus](https://docusaurus.io/). The only prerequisite for building these docs is NodeJS >= 18.0. Please check out the [official NodeJS docs](https://nodejs.org/en/download) for platform-specific installation instructions.

To get started, simply run `yarn && yarn start` from the [`docs/`](https://github.com/mlflow/mlflow/blob/master/docs/) folder. This will spin up a development server that can be viewed at `http://localhost:3000/` (by default). The source files (primarily `.MDX`) are located in the [`docs/docs/`](https://github.com/mlflow/mlflow/blob/master/docs/docs/) subfolder. Changes to these files should be automatically reflected in the development server!

There are also some `.ipynb` files which serve as the source for some of our tutorials. These are converted to MDX via a custom script (`yarn convert-notebooks`). If you want to make changes to these, you will need to install the `nbconvert` Python package in order to preview your changes.

For more detailed information, please check the [README.md](https://github.com/mlflow/mlflow/blob/master/docs/README.md) in the `docs/` folder. We're looking forward to your contributions!

### Sign your work

In order to commit your work, you need to sign that you wrote the patch
or otherwise have the right to pass it on as an open-source patch. If
you can certify the below (from developercertificate.org):

    Developer Certificate of Origin
    Version 1.1

    Copyright (C) 2004, 2006 The Linux Foundation and its contributors.
    1 Letterman Drive
    Suite D4700
    San Francisco, CA, 94129

    Everyone is permitted to copy and distribute verbatim copies of this
    license document, but changing it is not allowed.


    Developer's Certificate of Origin 1.1

    By making a contribution to this project, I certify that:

    (a) The contribution was created in whole or in part by me and I
        have the right to submit it under the open source license
        indicated in the file; or

    (b) The contribution is based upon previous work that, to the best
        of my knowledge, is covered under an appropriate open source
        license and I have the right under that license to submit that
        work with modifications, whether created in whole or in part
        by me, under the same open source license (unless I am
        permitted to submit under a different license), as indicated
        in the file; or

    (c) The contribution was provided directly to me by some other
        person who certified (a), (b) or (c) and I have not modified
        it.

    (d) I understand and agree that this project and the contribution
        are public and that a record of the contribution (including all
        personal information I submit with it, including my sign-off) is
        maintained indefinitely and may be redistributed consistent with
        this project or the open source license(s) involved.

Then add a line to every git commit message:

    Signed-off-by: Jane Smith <jane.smith@email.com>

Use your real name (sorry, no pseudonyms or anonymous contributions).
You can sign your commit automatically with `git commit -s` after you
set your `user.name` and `user.email` git configs.

> NOTE: Failing to sign your commits will result in an inability to merge your PR!

## Code of Conduct

Refer to the [MLflow Contributor Covenant Code of
Conduct](./CODE_OF_CONDUCT.rst) for more information.


--- LICENSE.txt ---
Copyright 2018 Databricks, Inc.  All rights reserved.

				Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS
   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.


--- SECURITY.md ---
# Security Policy

MLflow and its community take security bugs seriously. We appreciate efforts to improve the security of MLflow
and follow the [GitHub coordinated disclosure of security vulnerabilities](https://docs.github.com/en/code-security/security-advisories/about-coordinated-disclosure-of-security-vulnerabilities#about-reporting-and-disclosing-vulnerabilities-in-projects-on-github)
for responsible disclosure and prompt mitigation. We are committed to working with security researchers to
resolve the vulnerabilities they discover.

## Supported Versions

The latest version of MLflow has continued support. If a critical vulnerability is found in the current version
of MLflow, we may opt to backport patches to previous versions.

## Reporting a Vulnerability

When finding a security vulnerability in MLflow, please perform the following actions:

- [Open an issue](https://github.com/mlflow/mlflow/issues/new?assignees=&labels=bug&template=bug_report_template.md&title=%5BBUG%5D%20Security%20Vulnerability) on the MLflow repository. Ensure that you use `[BUG] Security Vulnerability` as the title and _do not_ mention any vulnerability details in the issue post.
- Send a notification [email](mailto:mlflow-oss-maintainers@googlegroups.com) to `mlflow-oss-maintainers@googlegroups.com` that contains, at a minimum:
  - The link to the filed issue stub.
  - Your GitHub handle.
  - Detailed information about the security vulnerability, evidence that supports the relevance of the finding and any reproducibility instructions for independent confirmation.

This first stage of reporting is to ensure that a rapid validation can occur without wasting the time and effort of a reporter. Future communication and vulnerability resolution will be conducted after validating
the veracity of the reported issue.

An MLflow maintainer will, after validating the report:

- Acknowledge the [bug](ISSUE_POLICY.md#bug-reports) during [triage](ISSUE_TRIAGE.rst)
- Mark the issue as `priority/critical-urgent`
- Open a draft [GitHub Security Advisory](https://docs.github.com/en/code-security/security-advisories/creating-a-security-advisory)
  to discuss the vulnerability details in private.

The private Security Advisory will be used to confirm the issue, prepare a fix, and publicly disclose it after the fix has been released.


--- dev/create_release_branch.py ---
import argparse
import os
import subprocess

from packaging.version import Version


def main(new_version: str, remote: str, dry_run=False):
    version = Version(new_version)
    release_branch = f"branch-{version.major}.{version.minor}"
    exists_on_remote = (
        subprocess.check_output(
            ["git", "ls-remote", "--heads", remote, release_branch], text=True
        ).strip()
        != ""
    )
    if exists_on_remote:
        print(f"{release_branch} already exists on {remote}, skipping branch creation")
        return

    prev_branch = subprocess.check_output(["git", "branch", "--show-current"], text=True).strip()
    try:
        exists_on_local = (
            subprocess.check_output(["git", "branch", "--list", release_branch], text=True).strip()
            != ""
        )
        if exists_on_local:
            print(f"Deleting existing {release_branch}")
            subprocess.check_call(["git", "branch", "-D", release_branch])

        print(f"Creating {release_branch}")
        subprocess.check_call(["git", "checkout", "-b", release_branch, "master"])
        print(f"Pushing {release_branch} to {remote}")
        subprocess.check_call(
            ["git", "push", remote, release_branch, *(["--dry-run"] if dry_run else [])]
        )
    finally:
        subprocess.check_call(["git", "checkout", prev_branch])


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Create a release branch")
    parser.add_argument("--new-version", required=True, help="New version to release")
    parser.add_argument("--remote", default="origin", help="Git remote to use (default: origin)")
    parser.add_argument(
        "--dry-run",
        action="store_true",
        default=os.environ.get("DRY_RUN", "true").lower() == "true",
        help="Dry run mode (default: True, can be set via DRY_RUN env var)",
    )
    parser.add_argument(
        "--no-dry-run",
        action="store_false",
        dest="dry_run",
        help="Disable dry run mode",
    )
    args = parser.parse_args()
    main(args.new_version, args.remote, args.dry_run)


--- dev/create_release_tag.py ---
"""
How to test this script
-----------------------
# Ensure origin points to your fork
git remote -v | grep origin

# Pretend we're releasing MLflow 9.0.0
git checkout -b branch-9.0

# First, test the dry run mode
python dev/create_release_tag.py --new-version 9.0.0 --dry-run
git tag -d v9.0.0

# Open https://github.com/<username>/mlflow/tree/v9.0.0 and verify that the tag does not exist.

# Then, test the non-dry run mode
python dev/create_release_tag.py --new-version 9.0.0 --no-dry-run
git tag -d v9.0.0

# Open https://github.com/<username>/mlflow/tree/v9.0.0 and verify that the tag exists now.

# Clean up the remote tag
git push --delete origin v9.0.0

# Clean up the local release branch
git checkout master
git branch -D branch-9.0
"""

import argparse
import os
import subprocess


def main(new_version: str, remote: str, dry_run: bool = False):
    release_tag = f"v{new_version}"
    subprocess.run(["git", "tag", release_tag], check=True)
    subprocess.run(
        ["git", "push", remote, release_tag, *(["--dry-run"] if dry_run else [])], check=True
    )


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Create a release tag")
    parser.add_argument("--new-version", required=True, help="New version to release")
    parser.add_argument("--remote", default="origin", help="Git remote to use (default: origin)")
    parser.add_argument(
        "--dry-run",
        action="store_true",
        default=os.environ.get("DRY_RUN", "true").lower() == "true",
        help="Dry run mode (default: True, can be set via DRY_RUN env var)",
    )
    parser.add_argument(
        "--no-dry-run",
        action="store_false",
        dest="dry_run",
        help="Disable dry run mode",
    )
    args = parser.parse_args()
    main(args.new_version, args.remote, args.dry_run)


--- dev/show_package_release_dates.py ---
import asyncio
import json
import subprocess
import sys
import traceback

import aiohttp


def get_distributions() -> list[tuple[str, str]]:
    res = subprocess.check_output(
        [sys.executable, "-m", "pip", "list", "--format", "json"], text=True
    )
    return [(pkg["name"], pkg["version"]) for pkg in json.loads(res)]


async def get_release_date(session: aiohttp.ClientSession, package: str, version: str) -> str:
    try:
        async with session.get(f"https://pypi.python.org/pypi/{package}/json", timeout=10) as resp:
            if resp.status != 200:
                return ""

            resp_json = await resp.json()
            matched = [
                dist_files for ver, dist_files in resp_json["releases"].items() if ver == version
            ]
            if not matched or not matched[0]:
                return ""

            upload_time = matched[0][0]["upload_time"]
            return upload_time.replace("T", " ")  # return year-month-day hour:minute:second
    except Exception:
        traceback.print_exc()
        return ""


def get_longest_string_length(array: list[str]) -> int:
    return len(max(array, key=len))


async def main() -> None:
    distributions = get_distributions()
    async with aiohttp.ClientSession() as session:
        tasks = [get_release_date(session, pkg, ver) for pkg, ver in distributions]
        release_dates = await asyncio.gather(*tasks)

    packages, versions = list(zip(*distributions))
    package_length = get_longest_string_length(packages)
    version_length = get_longest_string_length(versions)
    release_timestamp_length = get_longest_string_length(release_dates)
    print(
        "Package".ljust(package_length),
        "Version".ljust(version_length),
        "Release Timestamp".ljust(release_timestamp_length),
    )
    print("-" * (package_length + version_length + release_timestamp_length + 2))
    for package, version, release_date in sorted(
        zip(packages, versions, release_dates),
        # Sort by release date in descending order
        key=lambda x: x[2],
        reverse=True,
    ):
        print(
            package.ljust(package_length),
            version.ljust(version_length),
            release_date.ljust(release_timestamp_length),
        )


if __name__ == "__main__":
    asyncio.run(main())


--- dev/update_changelog.py ---
import argparse
import os
import re
import subprocess
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Any, NamedTuple

import requests
from packaging.version import Version


def get_header_for_version(version):
    return "## {} ({})".format(version, datetime.now().strftime("%Y-%m-%d"))


def extract_pr_num_from_git_log_entry(git_log_entry):
    m = re.search(r"\(#(\d+)\)$", git_log_entry)
    return int(m.group(1)) if m else None


def format_label(label: str) -> str:
    key = label.split("/", 1)[-1]
    return {
        "model-registry": "Model Registry",
        "uiux": "UI",
    }.get(key, key.capitalize())


class PullRequest(NamedTuple):
    title: str
    number: int
    author: str
    labels: list[str]

    @property
    def url(self):
        return f"https://github.com/mlflow/mlflow/pull/{self.number}"

    @property
    def release_note_labels(self):
        return [l for l in self.labels if l.startswith("rn/")]

    def __str__(self):
        areas = " / ".join(
            sorted(
                map(
                    format_label,
                    filter(lambda l: l.split("/")[0] in ("area", "language"), self.labels),
                )
            )
        )
        return f"[{areas}] {self.title} (#{self.number}, @{self.author})"

    def __repr__(self):
        return str(self)


class Section(NamedTuple):
    title: str
    items: list[Any]

    def __str__(self):
        if not self.items:
            return ""
        return "\n\n".join(
            [
                self.title,
                "\n".join(f"- {item}" for item in self.items),
            ]
        )


def is_shallow():
    return (
        subprocess.check_output(
            [
                "git",
                "rev-parse",
                "--is-shallow-repository",
            ],
            text=True,
        ).strip()
        == "true"
    )


def batch_fetch_prs_graphql(pr_numbers: list[int]) -> list[PullRequest]:
    """
    Batch fetch PR data using GitHub GraphQL API.
    """
    if not pr_numbers:
        return []

    # GitHub GraphQL has query size limits, so batch in chunks
    MAX_PRS_PER_QUERY = 50  # Conservative limit to avoid query size issues
    all_prs: list[PullRequest] = []

    for i in range(0, len(pr_numbers), MAX_PRS_PER_QUERY):
        chunk = pr_numbers[i : i + MAX_PRS_PER_QUERY]
        chunk_prs = _fetch_pr_chunk_graphql(chunk)
        all_prs.extend(chunk_prs)

    return all_prs


def _fetch_pr_chunk_graphql(pr_numbers: list[int]) -> list[PullRequest]:
    """
    Fetch a chunk of PRs using GraphQL.
    """
    # Build GraphQL query with aliases for each PR
    query_parts = [
        "query($owner: String!, $repo: String!) {",
        "  repository(owner: $owner, name: $repo) {",
    ]

    for i, pr_num in enumerate(pr_numbers):
        query_parts.append(f"""
    pr{i}: pullRequest(number: {pr_num}) {{
      number
      title
      author {{
        login
      }}
      labels(first: 100) {{
        nodes {{
          name
        }}
      }}
    }}""")

    query_parts.extend(["  }", "}"])
    query = "\n".join(query_parts)

    # Headers with authentication
    headers = {"Content-Type": "application/json"}
    if token := os.getenv("GITHUB_TOKEN"):
        headers["Authorization"] = f"Bearer {token}"
    print(f"Batch fetching {len(pr_numbers)} PRs with GraphQL...")
    resp = requests.post(
        "https://api.github.com/graphql",
        json={
            "query": query,
            "variables": {"owner": "mlflow", "repo": "mlflow"},
        },
        headers=headers,
    )
    resp.raise_for_status()
    data = resp.json()
    if "errors" in data:
        raise Exception(f"GraphQL errors: {data['errors']}")

    # Extract PR data from response and create PullRequest objects
    repository_data = data["data"]["repository"]
    prs = []
    for i, pr_num in enumerate(pr_numbers):
        pr_info = repository_data.get(f"pr{i}")
        if pr_info and pr_info.get("author"):
            prs.append(
                PullRequest(
                    title=pr_info["title"],
                    number=pr_info["number"],
                    author=pr_info["author"]["login"],
                    labels=[label["name"] for label in pr_info["labels"]["nodes"]],
                )
            )
        else:
            print(f"Warning: Could not fetch data for PR #{pr_num}")

    return prs


def main(prev_version, release_version, remote):
    if is_shallow():
        print("Unshallowing repository to ensure `git log` works correctly")
        subprocess.check_call(["git", "fetch", "--unshallow"])
        print("Modifying .git/config to fetch remote branches")
        subprocess.check_call(
            ["git", "config", "remote.origin.fetch", "+refs/heads/*:refs/remotes/origin/*"]
        )
    release_tag = f"v{prev_version}"
    ver = Version(release_version)
    branch = f"branch-{ver.major}.{ver.minor}"
    subprocess.check_call(["git", "fetch", remote, "tag", release_tag])
    subprocess.check_call(["git", "fetch", remote, branch])
    git_log_output = subprocess.check_output(
        [
            "git",
            "log",
            "--left-right",
            "--graph",
            "--cherry-pick",
            "--pretty=format:%s",
            f"tags/{release_tag}...{remote}/{branch}",
        ],
        text=True,
    )
    logs = [l[2:] for l in git_log_output.splitlines() if l.startswith("> ")]

    # Extract all PR numbers first
    pr_numbers = []
    for log in logs:
        if pr_num := extract_pr_num_from_git_log_entry(log):
            pr_numbers.append(pr_num)

    prs = batch_fetch_prs_graphql(pr_numbers)
    label_to_prs = defaultdict(list)
    author_to_prs = defaultdict(list)
    unlabelled_prs = []
    for pr in prs:
        if pr.author == "mlflow-app":
            continue

        if len(pr.release_note_labels) == 0:
            unlabelled_prs.append(pr)

        for label in pr.release_note_labels:
            if label == "rn/none":
                author_to_prs[pr.author].append(pr)
            else:
                label_to_prs[label].append(pr)

    assert len(unlabelled_prs) == 0, "The following PRs need to be categorized:\n" + "\n".join(
        f"- {pr.url}" for pr in unlabelled_prs
    )

    unknown_labels = set(label_to_prs.keys()) - {
        "rn/highlight",
        "rn/feature",
        "rn/breaking-change",
        "rn/bug-fix",
        "rn/documentation",
        "rn/none",
    }
    assert len(unknown_labels) == 0, f"Unknown labels: {unknown_labels}"

    breaking_changes = Section("Breaking changes:", label_to_prs.get("rn/breaking-change", []))
    highlights = Section("Major new features:", label_to_prs.get("rn/highlight", []))
    features = Section("Features:", label_to_prs.get("rn/feature", []))
    bug_fixes = Section("Bug fixes:", label_to_prs.get("rn/bug-fix", []))
    doc_updates = Section("Documentation updates:", label_to_prs.get("rn/documentation", []))
    small_updates = [
        ", ".join([f"#{pr.number}" for pr in prs] + [f"@{author}"])
        for author, prs in author_to_prs.items()
    ]
    small_updates = "Small bug fixes and documentation updates:\n\n" + "; ".join(small_updates)
    sections = filter(
        str.strip,
        map(
            str,
            [
                get_header_for_version(release_version),
                f"MLflow {release_version} includes several major features and improvements",
                breaking_changes,
                highlights,
                features,
                bug_fixes,
                doc_updates,
                small_updates,
            ],
        ),
    )
    new_changelog = "\n\n".join(sections)
    changelog_header = "# CHANGELOG"
    changelog = Path("CHANGELOG.md")
    old_changelog = changelog.read_text().replace(f"{changelog_header}\n\n", "", 1)
    new_changelog = "\n\n".join(
        [
            changelog_header,
            new_changelog,
            old_changelog,
        ]
    )
    changelog.write_text(new_changelog)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Update CHANGELOG.md")
    parser.add_argument("--prev-version", required=True, help="Previous version")
    parser.add_argument("--release-version", required=True, help="MLflow version to release")
    parser.add_argument("--remote", default="origin", help="Git remote to use (default: origin)")
    args = parser.parse_args()
    main(args.prev_version, args.release_version, args.remote)


--- dev/validate_release_version.py ---
import argparse

from packaging.version import Version


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--version", help="Release version to validate, e.g., '1.2.3'", required=True
    )
    return parser.parse_args()


def main():
    args = parse_args()
    version = Version(args.version)
    msg = (
        f"Invalid release version: '{args.version}', "
        "must be in the format of <major>.<minor>.<micro>"
    )
    assert len(version.release) == 3, msg


if __name__ == "__main__":
    main()


--- libs/skinny/LICENSE.txt ---
../../LICENSE.txt

--- CLAUDE.md ---
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

**For contribution guidelines, code standards, and additional development information not covered here, please refer to [CONTRIBUTING.md](./CONTRIBUTING.md).**

## Repository Overview

MLflow is an open-source platform for managing the end-to-end machine learning lifecycle. It provides tools for:

- Experiment tracking
- Model versioning and deployment
- LLM observability and tracing
- Model evaluation
- Prompt management

## Quick Start: Development Server

### Start the Full Development Environment (Recommended)

```bash
# Kill any existing servers
pkill -f "mlflow server" || true; pkill -f "yarn start" || true

# Start both MLflow backend and React frontend dev servers
nohup uv run bash dev/run-dev-server.sh > /tmp/mlflow-dev-server.log 2>&1 &

# Monitor the logs
tail -f /tmp/mlflow-dev-server.log

# Servers will be available at:
# - MLflow backend: http://localhost:5000
# - React frontend: http://localhost:3000
```

This uses `uv` (fast Python package manager) to automatically manage dependencies and run the development environment.

### Start Development Server with Databricks Backend

To run the MLflow dev server that proxies requests to a Databricks workspace:

```bash
# IMPORTANT: All four environment variables below are REQUIRED for proper Databricks backend operation
# Set them in this exact order:
export DATABRICKS_HOST="https://your-workspace.databricks.com"  # Your Databricks workspace URL
export DATABRICKS_TOKEN="your-databricks-token"                # Your Databricks personal access token
export MLFLOW_TRACKING_URI="databricks"                        # Must be set to "databricks"
export MLFLOW_REGISTRY_URI="databricks-uc"                     # Use "databricks-uc" for Unity Catalog, or "databricks" for workspace model registry

# Start the dev server with these environment variables
nohup uv run bash dev/run-dev-server.sh > /tmp/mlflow-dev-server.log 2>&1 &

# Monitor the logs
tail -f /tmp/mlflow-dev-server.log

# The MLflow server will now proxy tracking and model registry requests to Databricks
# Access the UI at http://localhost:3000 to see your Databricks experiments and models
```

**Note**: The MLflow server acts as a proxy, forwarding API requests to your Databricks workspace while serving the local React frontend. This allows you to develop and test UI changes against real Databricks data.

## Development Commands

### Testing

```bash
# First-time setup: Install test dependencies
uv sync
uv pip install -r requirements/test-requirements.txt

# Run Python tests
uv run pytest tests/

# Run specific test file
uv run pytest tests/test_version.py

# Run tests with specific package versions
uv run --with abc==1.2.3 --with xyz==4.5.6 pytest tests/test_version.py

# Run tests with optional dependencies/extras
uv run --with transformers pytest tests/transformers
uv run --extra gateway pytest tests/gateway

# Run JavaScript tests
yarn --cwd mlflow/server/js test
```

**IMPORTANT**: `uv` may fail initially because the environment has not been set up yet. Follow the instructions to set up the environment and then rerun `uv` as needed.

### Code Quality

```bash
# Python linting and formatting with Ruff
uv run --only-group lint ruff check . --fix         # Lint with auto-fix
uv run --only-group lint ruff format .              # Format code

# Custom MLflow linting with Clint
uv run --only-group lint clint .                    # Run MLflow custom linter

# Check for MLflow spelling typos
uv run --only-group lint bash dev/mlflow-typo.sh .

# JavaScript linting and formatting
yarn --cwd mlflow/server/js lint
yarn --cwd mlflow/server/js prettier:check
yarn --cwd mlflow/server/js prettier:fix

# Type checking
yarn --cwd mlflow/server/js type-check

# Run all checks
yarn --cwd mlflow/server/js check-all
```

### Special Testing

```bash
# Run tests with minimal dependencies (skinny client)
uv run bash dev/run-python-skinny-tests.sh
```

### Documentation

```bash
# Build documentation site (needs gateway extras for API doc generation)
uv run --all-extras bash dev/build-docs.sh --build-api-docs

# Build with R docs included
uv run --all-extras bash dev/build-docs.sh --build-api-docs --with-r-docs

# Serve documentation locally (after building)
cd docs && yarn serve --port 8080
```

## Important Files

- `pyproject.toml`: Package configuration and tool settings
- `.python-version`: Minimum Python version (3.10)
- `requirements/`: Dependency specifications
- `mlflow/ml-package-versions.yml`: Supported ML framework versions

## Common Development Tasks

### Modifying the UI

See `mlflow/server/js/` for frontend development.

## Language-Specific Style Guides

- [Python](/dev/guides/python.md)

## Git Workflow

### Committing Changes

**IMPORTANT**: After making your commits, run pre-commit hooks on your PR changes to ensure code quality:

```bash
# Make your commit first (with DCO sign-off)
git commit -s -m "Your commit message"

# Then check all files changed in your PR
uv run --only-group lint pre-commit run --from-ref origin/master --to-ref HEAD

# Fix any issues and amend your commit if needed
git add <fixed files>
git commit --amend -s

# Re-run pre-commit to verify fixes
uv run --only-group lint pre-commit run --from-ref origin/master --to-ref HEAD

# Only push once all checks pass
git push origin <your-branch>
```

This workflow ensures you only check files you've actually modified in your PR, avoiding false positives from unrelated files.

**IMPORTANT**: You MUST sign all commits with DCO (Developer Certificate of Origin). Always use the `-s` flag:

```bash
# REQUIRED: Always use -s flag when committing
git commit -s -m "Your commit message"

# This will NOT work - missing -s flag
# git commit -m "Your commit message"  ❌
```

Commits without DCO sign-off will be rejected by CI.

**Frontend Changes**: If your PR touches any code in `mlflow/server/js/`, you MUST run `yarn check-all` before committing:

```bash
yarn --cwd mlflow/server/js check-all
```

### Creating Pull Requests

Follow [the PR template](./.github/pull_request_template.md) when creating pull requests. Remove any unused checkboxes from the template to keep your PR clean and focused.

### Checking CI Status

Use GitHub CLI to check for failing CI:

```bash
# Check workflow runs for current branch
gh run list --branch $(git branch --show-current)

# View details of a specific run
gh run view <run-id>

# Watch a run in progress
gh run watch
```

## Pre-commit Hooks

The repository uses pre-commit for code quality. Install hooks with:

```bash
uv run --only-group lint pre-commit install --install-hooks
```

Run pre-commit manually:

```bash
# Run on all files
uv run --only-group lint pre-commit run --all-files

# Run on all files, skipping hooks that require external tools
SKIP=taplo,typos,conftest uv run --only-group lint pre-commit run --all-files

# Run on specific files
uv run --only-group lint pre-commit run --files path/to/file.py

# Run a specific hook
uv run --only-group lint pre-commit run ruff --all-files
```

This runs Ruff, typos checker, and other tools automatically before commits.

**Note about external tools**: Some pre-commit hooks require external tools that aren't Python packages:

- `taplo` - TOML formatter
- `typos` - Spell checker
- `conftest` - Policy testing tool

To install these tools:

```bash
# Install all tools at once (recommended)
uv run --only-group lint bin/install.py
```

This automatically downloads and installs the correct versions of all external tools to the `bin/` directory. The tools work on both Linux and ARM Macs.

These tools are optional. Use `SKIP=taplo,typos,conftest` if they're not installed.

**Note**: If the typos hook fails, you only need to fix typos in code that was changed by your PR, not pre-existing typos in the codebase.


--- CODE_OF_CONDUCT.rst ---
MLflow Contributor Covenant Code of Conduct
===========================================

.. contents:: **Table of Contents**
  :local:
  :depth: 4

Our Pledge
##########

In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to making participation in our project and
our community a harassment-free experience for everyone, regardless of age, body
size, disability, ethnicity, sex characteristics, gender identity and expression,
level of experience, education, socio-economic status, nationality, personal
appearance, race, religion, or sexual identity and orientation.

Our Standards
#############

Examples of behavior that contributes to creating a positive environment
include:

* Using welcoming and inclusive language
* Being respectful of differing viewpoints and experiences
* Gracefully accepting constructive criticism
* Focusing on what is best for the community
* Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery and unwelcome sexual attention or advances
* Trolling, insulting/derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or electronic address, without explicit permission
* Other conduct which could reasonably be considered inappropriate in a professional setting

Our Responsibilities
####################

Project maintainers are responsible for clarifying the standards of acceptable
behavior and are expected to take appropriate and fair corrective action in
response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or
reject comments, commits, code, wiki edits, issues, and other contributions
that are not aligned to this Code of Conduct, or to ban temporarily or
permanently any contributor for other behaviors that they deem inappropriate,
threatening, offensive, or harmful.

Scope
#####

This Code of Conduct applies both within project spaces and in public spaces
when an individual is representing the project or its community. Examples of
representing a project or community include using an official project e-mail
address, posting via an official social media account, or acting as an appointed
representative at an online or offline event. Representation of a project may be
further defined and clarified by project maintainers.

Enforcement
###########

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by contacting the Technical Steering Committee defined `here <https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#governance>`_.
All complaints will be reviewed and investigated and will result in a response that
is deemed necessary and appropriate to the circumstances. The project team is
obligated to maintain confidentiality with regard to the reporter of an incident.
Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good
faith may face temporary or permanent repercussions as determined by other
members of the project's leadership.

Attribution
###########

This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,
available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see
https://www.contributor-covenant.org/faq


--- COMMITTER.md ---
### Evaluation Criteria

When evaluating potential new MLflow committers, the following criteria will be considered:

- **Code Contributions**: Should have multiple non-trivial code contributions accepted and committed to the MLflow codebase. This demonstrates the ability to produce quality code aligned with the project's standards.
- **Technical Expertise**: Should demonstrate a deep understanding of MLflow's architecture and design principles, evidenced by making appropriate design choices and technical recommendations. History of caring about code quality, testing, maintainability, and ability to critically evaluate technical artifacts (PRs, designs, etc.) and provide constructive suggestions for improvement.
- **Subject Matter Breadth**: Contributions and learnings span multiple areas of the codebase, APIs, and integration points rather than a narrow niche.
- **Community Participation**: Active participation for at least 3 months prior to nomination by authoring code contributions and engaging in the code review process. Involvement in mailing lists, Slack channels, Stack Overflow, and GitHub issues is valued but not strictly required.
- **Communication**: Should maintain a constructive tone in communications, be receptive to feedback, and collaborate well with existing committers and other community members.
- **Project Commitment**: Demonstrate commitment to MLflow's long-term success, uphold project principles and values, and willingness to pitch in for "unglamorous" work.

### Committership Nomination

- Any current MLflow committer can nominate a contributor for committership by emailing MLflow's TSC members with a nomination packet.
- The nomination packet should provide details on the nominee's salient contributions, as well as justification on how they meet the evaluation criteria. Links to GitHub activity, mailing list threads, and other artifacts should be included.
- In addition to the nominator, every nomination must have a seconder -- a separate committer who advocates for the nominee. The seconder should be a more senior committer (active committer for >1 year) familiar with the nominee's work.
- It is the nominator's responsibility to identify a willing seconder and include their recommendation in the nomination packet.
- If no eligible seconder is available or interested, it may indicate insufficient support to proceed with the nomination at that time. This ensures there are two supporting committers invested in each nomination - the nominator and the seconder. The seconder's seniority and familiarity with the situation also help build more consensus among the TSC members during evaluation.

### Evaluation Process

- When a committer nomination is made, the TSC members closely review the proposal and evaluate the nominee's qualifications.
- Throughout the review, the nominator is responsible for addressing any questions from the TSC, and providing clarification or additional evidence as requested by TSC members.
- After adequate discussion (~1 week), the nominator calls for a formal consensus check among the TSC.
- A positive consensus requires at least 2 TSC +1 binding votes and no vetoes.
- Any vetoes must be accompanied by a clear rationale that can be debated.
- If consensus is not achieved, the nomination is rejected at that time.
- If consensus fails, the nominator summarizes substantive feedback and remaining gaps to the nominee for their growth and potential re-nomination later. Nomination can be tried again in 3 months after addressing any gaps identified.

### Onboarding a new committer

- Upon a positive consensus being reached, one of the TSC members will extend the formal invitation to the nominee to become a committer. They also field the private initial response from the nominee on willingness to accept.
- If the proposal is accepted, the nominator grants them the commit access and the new committer will be:
  - Added to the committer list in the README.md
  - Announced on the MLflow mailing lists, Slack channels, and the MLflow website
  - Spotlighted through a post on the MLflow LinkedIn and X handles
- The nominator will work with the new committer to identify well-scoped initial areas for the new committer to focus on, such as improvements to a specific component.
- The nominator will also set up periodic 1:1 mentorship check-ins with the new committer over their first month to provide guidance where needed.


--- EXTRA_DEPENDENCIES.rst ---
=========================
Extra MLflow Dependencies
=========================

When you `install the MLflow Python package <https://mlflow.org/docs/latest/quickstart.html#installing-mlflow>`_,
a set of core dependencies needed to use most MLflow functionality (tracking, projects, models APIs)
is also installed.

However, in order to use certain framework-specific MLflow APIs or configuration options,
you need to install additional, "extra" dependencies. For example, the model persistence APIs under
the ``mlflow.sklearn`` module require scikit-learn to be installed. Some of the most common MLflow
extra dependencies can be installed via ``pip install mlflow[extras]``.

The full set of extra dependencies are documented, along with the modules that depend on them,
in the following files:

* extra-ml-requirements.txt: ML libraries needed to use model persistence and inference APIs
* test-requirements.txt: Libraries required to use non-default artifact-logging and tracking server configurations


--- ISSUE_POLICY.md ---
# Issue Policy

The MLflow Issue Policy outlines the categories of MLflow GitHub issues and discusses the guidelines & processes
associated with each type of issue.

Before filing an issue, make sure to [search for related issues](https://github.com/mlflow/mlflow/issues) and check if
they address yours.

For support (ex. "How do I do X?"), please ask on [Stack Overflow](https://stackoverflow.com/questions/tagged/mlflow).

## Issue Categories

Our policy is that GitHub issues fall into one of the following categories:

1. Feature Requests
2. Bug reports
3. Documentation fixes
4. Installation issues

Each category has its own GitHub issue template. Please do not delete the issue template unless you are certain your
issue is outside its scope.

### Feature Requests

#### Guidelines

Feature requests that are likely to be accepted:

- Are minimal in scope (note that it's always easier to add additional functionality later than remove functionality)
- Are extensible (e.g. if adding an integration with an ML framework, is it possible to add similar integrations with other frameworks?)
- Have user impact & value that justifies the maintenance burden of supporting the feature moving forwards. The
  [JQuery contributor guide](https://contribute.jquery.org/open-source/#contributing-something-new) has an excellent discussion on this.

#### Lifecycle

Feature requests typically go through the following lifecycle:

1. A feature request GitHub Issue is submitted, which contains a high-level description of the proposal and its motivation.
   We encourage requesters to provide an overview of the feature's implementation as well, if possible.
2. The [issue is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route feature requests to appropriate committers.
3. The feature request is discussed with a committer. The committer will provide input on the implementation overview or
   ask for a more detailed design, if applicable.
4. After discussion & agreement on the feature request and its implementation, an implementation owner is identified.
5. The implementation owner begins developing the feature and ultimately files associated pull requests against the
   MLflow Repository or packages the feature as an MLflow Plugin.

### Bug reports

#### Guidelines

In order to ensure that maintainers are able to assist in any reported bug:

- Ensure that the bug report template is filled out in its entirety with appropriate levels of detail, particularly in the `Code to reproduce issue` section.
- Verify that the bug you are reporting meets one of the following criteria:
  - A recent release of MLflow does not support the operation you are doing that an earlier release did (a regression).
  - A [documented feature](https://mlflow.org/docs/latest/index.html) or functionality does not work properly by executing a provided example from the docs.
  - Any exception raised is directly from MLflow and is not the result of an underlying package's exception (e.g., don't file an issue that MLflow can't log a model that can't be trained due to a tensorflow Exception)
- Make a best effort to diagnose and troubleshoot the issue prior to filing.
- Verify that the environment that you're experiencing the bug in is supported as defined in the docs.
- Validate that MLflow supports the functionality that you're having an issue with. _A lack of a feature does not constitute a bug_.
- Read the docs on the feature for the issue that you're reporting. If you're certain that you're following documented guidelines, please file a bug report.

Bug reports typically go through the following lifecycle:

1. A bug report GitHub Issue is submitted, which contains a high-level description of the bug and information required to reproduce it.
2. The [bug report is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route to request appropriate committers.
3. An MLflow committer reproduces the bug and provides feedback about how to implement a fix.
4. After an approach has been agreed upon, an owner for the fix is identified. MLflow committers may choose to adopt
   ownership of severe bugs to ensure a timely fix.
5. The fix owner begins implementing the fix and ultimately files associated pull requests.

### Documentation fixes

Documentation issues typically go through the following lifecycle:

1. A documentation GitHub Issue is submitted, which contains a description of the issue and its location(s) in the MLflow documentation.
2. The [issue is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route the request to appropriate committers.
3. An MLflow committer confirms the documentation issue and provides feedback about how to implement a fix.
4. After an approach has been agreed upon, an owner for the fix is identified. MLflow committers may choose to adopt
   ownership of severe documentation issues to ensure a timely fix.
5. The fix owner begins implementing the fix and ultimately files associated pull requests.

### Installation issues

Installation issues typically go through the following lifecycle:

1. An installation GitHub Issue is submitted, which contains a description of the issue and the platforms its affects.
2. The [issue is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route the issue to appropriate committers.
3. An MLflow committer confirms the installation issue and provides feedback about how to implement a fix.
4. After an approach has been agreed upon, an owner for the fix is identified. MLflow committers may choose to adopt
   ownership of severe installation issues to ensure a timely fix.
5. The fix owner begins implementing the fix and ultimately files associated pull requests.


--- ISSUE_TRIAGE.rst ---

This document is a hands-on manual for doing issue and pull request triage for `MLflow issues
on GitHub <https://github.com/mlflow/mlflow/issues>`_ .
The purpose of triage is to speed up issue management and get community members faster responses.

Issue and pull request triage has three steps:

- assign one or more process labels (e.g. ``needs design`` or ``help wanted``),
- mark a priority, and
- label one or more relevant areas, languages, or integrations to help route issues to appropriate contributors or reviewers.

The remainder of the document describes the labels used in each of these steps and how to apply them.

Assign appropriate process labels
#######
Assign at least one process label to every issue you triage.

- ``needs author feedback``: We need input from the author of the issue or PR to proceed.
- | ``needs design``: This feature is large or tricky enough that we think it warrants a design doc
  | and review before someone begins implementation.
- | ``needs committer feedback``: The issue has a design that is ready for committer review, or there is
  | an issue or pull request that needs feedback from a committer about the approach or appropriateness
  | of the contribution.
- | ``needs review``: Use this label for issues that need a more detailed design review or pull
  | requests ready for review (all questions answered, PR updated if requests have been addressed,
  | tests passing).
- ``help wanted``: We would like community help for this issue.
- ``good first issue``: This would make a good first issue.


Assign priority
#######

You should assign a priority to each issue you triage. We use `kubernetes-style <https://github.com/
kubernetes/community/blob/master/contributors/guide/issue-triage.md#define-priority>`_ priority
labels.

- | ``priority/critical-urgent``: This is the highest priority and should be worked on by
  | somebody right now. This should typically be reserved for things like security bugs,
  | regressions, release blockers.
- | ``priority/important-soon``: The issue is worked on by the community currently or will
  | be very soon, ideally in time for the next release.
- | ``priority/important-longterm``: Important over the long term, but may not be staffed or
  | may need multiple releases to complete. Also used for things we know are on a
  | contributor's roadmap in the next few months. We can use this in conjunction with
  | ``help wanted`` to mark issues we would like to get help with. If someone begins actively
  | working on an issue with this label and we think it may be merged by the next release, change
  | the priority to ``priority/important-soon``.
- | ``priority/backlog``: We believe it is useful but don't see it being prioritized in the
  | next few months. Use this for issues that are lower priority than ``priority/important-longterm``.
  | We welcome community members to pick up a ``priority/backlog`` issue, but there may be some
  | delay in getting support through design review or pull request feedback.
- | ``priority/awaiting-more-evidence``: Lowest priority. Possibly useful, but not yet enough
  | support to actually get it done. This is a good place to put issues that could be useful but
  | require more evidence to demonstrate broad value. Don't use it as a way to say no.
  | If we think it doesn't fit in MLflow, we should just say that and why.

Label relevant areas
#######

Assign one more labels for relevant component or interface surface areas, languages, or
integrations. As a principle, we aim to have the minimal set of labels needed to help route issues
and PRs to appropriate contributors. For example, a ``language/python`` label would not be
particularly helpful for routing issues to committers, since most PRs involve Python code.
``language/java`` and ``language/r`` make sense to have, as the clients in these languages differ from the Python client and aren't maintained by many people. As with process labels, we
take inspiration from Kubernetes on naming conventions.

Components
""""""""
- ``area/artifacts``: Artifact stores and artifact logging
- ``area/build``: Build and test infrastructure for MLflow
- ``area/docs``: MLflow documentation pages
- ``area/evaluation``: MLflow model evaluation features, evaluation metrics, and evaluation workflows
- ``area/examples``: Example code
- ``area/gateway``: AI Gateway service, Gateway client APIs, third-party Gateway integrations
- ``area/model-registry``: Model Registry service, APIs, and the fluent client calls for Model Registry
- ``area/models``: MLmodel format, model serialization/deserialization, flavors
- ``area/projects``: MLproject format, project execution backends
- ``area/prompt``: MLflow prompt engineering features, prompt templates, and prompt management
- ``area/scoring``: MLflow Model server, model deployment tools, Spark UDFs
- ``area/server-infra``: MLflow Tracking server backend
- ``area/tracing``: MLflow Tracing features, tracing APIs, and LLM tracing functionality
- ``area/tracking``: Tracking Service, tracking client APIs, autologging

Interface Surface
""""""""
- ``area/uiux``: Front-end, user experience, plotting, JavaScript, JavaScript dev server
- ``area/docker``: Docker use across MLflow's components, such as MLflow Projects and MLflow Models
- ``area/sqlalchemy``: Use of SQLAlchemy in the Tracking Service or Model Registry
- ``area/windows``: Windows support

Language Surface
""""""""
- ``language/r``: R APIs and clients
- ``language/java``: Java APIs and clients
- ``language/new``: Proposals for new client languages

Integrations
""""""""
- ``integrations/azure``: Azure and Azure ML integrations
- ``integrations/sagemaker``: SageMaker integrations
- ``integrations/databricks``: Databricks integrations


--- README.md ---
<h1 align="center" style="border-bottom: none">
    <a href="https://mlflow.org/">
        <img alt="MLflow logo" src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/logo.svg" width="200" />
    </a>
</h1>
<h2 align="center" style="border-bottom: none">Open-Source Platform for Productionizing AI</h2>

MLflow is an open-source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end **experiment tracking**, **observability**, and **evaluations**, all in one integrated platform.

<div align="center">

[![Python SDK](https://img.shields.io/pypi/v/mlflow)](https://pypi.org/project/mlflow/)
[![PyPI Downloads](https://img.shields.io/pypi/dm/mlflow)](https://pepy.tech/projects/mlflow)
[![License](https://img.shields.io/github/license/mlflow/mlflow)](https://github.com/mlflow/mlflow/blob/main/LICENSE)
<a href="https://twitter.com/intent/follow?screen_name=mlflow" target="_blank">
<img src="https://img.shields.io/twitter/follow/mlflow?logo=X&color=%20%23f5f5f5"
      alt="follow on X(Twitter)"></a>
<a href="https://www.linkedin.com/company/mlflow-org/" target="_blank">
<img src="https://custom-icon-badges.demolab.com/badge/LinkedIn-0A66C2?logo=linkedin-white&logoColor=fff"
      alt="follow on LinkedIn"></a>
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/mlflow/mlflow)

</div>

<div align="center">
   <div>
      <a href="https://mlflow.org/"><strong>Website</strong></a> ·
      <a href="https://mlflow.org/docs/latest"><strong>Docs</strong></a> ·
      <a href="https://github.com/mlflow/mlflow/issues/new/choose"><strong>Feature Request</strong></a> ·
      <a href="https://mlflow.org/blog"><strong>News</strong></a> ·
      <a href="https://www.youtube.com/@mlflowoss"><strong>YouTube</strong></a> ·
      <a href="https://lu.ma/mlflow?k=c"><strong>Events</strong></a>
   </div>
</div>

<br>

## 🚀 Installation

To install the MLflow Python package, run the following command:

```
pip install mlflow
```

## 📦 Core Components

MLflow is **the only platform that provides a unified solution for all your AI/ML needs**, including LLMs, Agents, Deep Learning, and traditional machine learning.

### 💡 For LLM / GenAI Developers

<table>
  <tr>
    <td>
    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-tracing.png" alt="Tracing" width=100%>
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/llms/tracing/index.html"><strong>🔍 Tracing / Observability</strong></a>
        <br><br>
        <div>Trace the internal states of your LLM/agentic applications for debugging quality issues and monitoring performance with ease.</div><br>
        <a href="https://mlflow.org/docs/latest/genai/tracing/quickstart/python-openai/">Getting Started →</a>
        <br><br>
    </div>
    </td>
    <td>
    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-llm-eval.png" alt="LLM Evaluation" width=100%>
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/"><strong>📊 LLM Evaluation</strong></a>
        <br><br>
        <div>A suite of automated model evaluation tools, seamlessly integrated with experiment tracking to compare across multiple versions.</div><br>
        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/">Getting Started →</a>
        <br><br>
    </div>
    </td>
  </tr>
  <tr>
    <td>
      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-prompt.png" alt="Prompt Management">
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/prompt-registry/"><strong>🤖 Prompt Management</strong></a>
        <br><br>
        <div>Version, track, and reuse prompts across your organization, helping maintain consistency and improve collaboration in prompt development.</div><br>
        <a href="https://mlflow.org/docs/latest/genai/prompt-registry/create-and-edit-prompts/">Getting Started →</a>
        <br><br>
    </div>
    </td>
    <td>
      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-logged-model.png" alt="MLflow Hero">
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/version-tracking/"><strong>📦 App Version Tracking</strong></a>
        <br><br>
        <div>MLflow keeps track of many moving parts in your AI applications, such as models, prompts, tools, and code, with end-to-end lineage.</div><br>
        <a href="https://mlflow.org/docs/latest/genai/version-tracking/quickstart/">Getting Started →</a>
        <br><br>
    </div>
    </td>
  </tr>
</table>

### 🎓 For Data Scientists

<table>
  <tr>
    <td colspan="2" align="center" >
      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-experiment.png" alt="Tracking" width=50%>
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/ml/tracking/"><strong>📝 Experiment Tracking</strong></a>
        <br><br>
        <div>Track your models, parameters, metrics, and evaluation results in ML experiments and compare them using an interactive UI.</div><br>
        <a href="https://mlflow.org/docs/latest/ml/tracking/quickstart/">Getting Started →</a>
        <br><br>
    </div>
    </td>
  </tr>
  <tr>
    <td>
      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-model-registry.png" alt="Model Registry" width=100%>
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/ml/model-registry/"><strong>💾 Model Registry</strong></a>
        <br><br>
        <div> A centralized model store designed to collaboratively manage the full lifecycle and deployment of machine learning models.</div><br>
        <a href="https://mlflow.org/docs/latest/ml/model-registry/tutorial/">Getting Started →</a>
        <br><br>
    </div>
    </td>
    <td>
      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-deployment.png" alt="Deployment" width=100%>
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/ml/deployment/"><strong>🚀 Deployment</strong></a>
        <br><br>
        <div> Tools for seamless model deployment to batch and real-time scoring on platforms like Docker, Kubernetes, Azure ML, and AWS SageMaker.</div><br>
        <a href="https://mlflow.org/docs/latest/ml/deployment/">Getting Started →</a>
        <br><br>
    </div>
    </td>
  </tr>
</table>

## 🌐 Hosting MLflow Anywhere

<div align="center" >
  <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-providers.png" alt="Providers" width=100%>
</div>

You can run MLflow in many different environments, including local machines, on-premise servers, and cloud infrastructure.

Trusted by thousands of organizations, MLflow is now offered as a managed service by most major cloud providers:

- [Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/)
- [Azure ML](https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow?view=azureml-api-2)
- [Databricks](https://www.databricks.com/product/managed-mlflow)
- [Nebius](https://nebius.com/services/managed-mlflow)

For hosting MLflow on your own infrastructure, please refer to [this guidance](https://mlflow.org/docs/latest/ml/tracking/#tracking-setup).

## 🗣️ Supported Programming Languages

- [Python](https://pypi.org/project/mlflow/)
- [TypeScript / JavaScript](https://www.npmjs.com/package/mlflow-tracing)
- [Java](https://mvnrepository.com/artifact/org.mlflow/mlflow-client)
- [R](https://cran.r-project.org/web/packages/mlflow/readme/README.html)

## 🔗 Integrations

MLflow is natively integrated with many popular machine learning frameworks and GenAI libraries.

![Integrations](https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-integrations.png)

## Usage Examples

### Experiment Tracking ([Doc](https://mlflow.org/docs/latest/ml/tracking/))

The following examples trains a simple regression model with scikit-learn, while enabling MLflow's [autologging](https://mlflow.org/docs/latest/tracking/autolog.html) feature for experiment tracking.

```python
import mlflow

from sklearn.model_selection import train_test_split
from sklearn.datasets import load_diabetes
from sklearn.ensemble import RandomForestRegressor

# Enable MLflow's automatic experiment tracking for scikit-learn
mlflow.sklearn.autolog()

# Load the training dataset
db = load_diabetes()
X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)

rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)
# MLflow triggers logging automatically upon model fitting
rf.fit(X_train, y_train)
```

Once the above code finishes, run the following command in a separate terminal and access the MLflow UI via the printed URL. An MLflow **Run** should be automatically created, which tracks the training dataset, hyper parameters, performance metrics, the trained model, dependencies, and even more.

```
mlflow ui
```

### Evaluating Models ([Doc](https://mlflow.org/docs/latest/model-evaluation/index.html))

The following example runs automatic evaluation for question-answering tasks with several built-in metrics.

```python
import mlflow
import pandas as pd

# Evaluation set contains (1) input question (2) model outputs (3) ground truth
df = pd.DataFrame(
    {
        "inputs": ["What is MLflow?", "What is Spark?"],
        "outputs": [
            "MLflow is an innovative fully self-driving airship powered by AI.",
            "Sparks is an American pop and rock duo formed in Los Angeles.",
        ],
        "ground_truth": [
            "MLflow is an open-source platform for productionizing AI.",
            "Apache Spark is an open-source, distributed computing system.",
        ],
    }
)
eval_dataset = mlflow.data.from_pandas(
    df, predictions="outputs", targets="ground_truth"
)

# Start an MLflow Run to record the evaluation results to
with mlflow.start_run(run_name="evaluate_qa"):
    # Run automatic evaluation with a set of built-in metrics for question-answering models
    results = mlflow.evaluate(
        data=eval_dataset,
        model_type="question-answering",
    )

print(results.tables["eval_results_table"])
```

### Observability ([Doc](https://mlflow.org/docs/latest/llms/tracing/index.html))

MLflow Tracing provides LLM observability for various GenAI libraries such as OpenAI, LangChain, LlamaIndex, DSPy, AutoGen, and more. To enable auto-tracing, call `mlflow.xyz.autolog()` before running your models. Refer to the documentation for customization and manual instrumentation.

```python
import mlflow
from openai import OpenAI

# Enable tracing for OpenAI
mlflow.openai.autolog()

# Query OpenAI LLM normally
response = OpenAI().chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hi!"}],
    temperature=0.1,
)
```

Then navigate to the "Traces" tab in the MLflow UI to find the trace records OpenAI query.

## 💭 Support

- For help or questions about MLflow usage (e.g. "how do I do X?") visit the [documentation](https://mlflow.org/docs/latest).
- In the documentation, you can ask the question to our AI-powered chat bot. Click on the **"Ask AI"** button at the right bottom.
- Join the [virtual events](https://lu.ma/mlflow?k=c) like office hours and meetups.
- To report a bug, file a documentation issue, or submit a feature request, please [open a GitHub issue](https://github.com/mlflow/mlflow/issues/new/choose).
- For release announcements and other discussions, please subscribe to our mailing list (mlflow-users@googlegroups.com)
  or join us on [Slack](https://mlflow.org/slack).

## 🤝 Contributing

We happily welcome contributions to MLflow!

- Submit [bug reports](https://github.com/mlflow/mlflow/issues/new?template=bug_report_template.yaml) and [feature requests](https://github.com/mlflow/mlflow/issues/new?template=feature_request_template.yaml)
- Contribute for [good-first-issues](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) and [help-wanted](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)
- Writing about MLflow and sharing your experience

Please see our [contribution guide](CONTRIBUTING.md) to learn more about contributing to MLflow.

## ⭐️ Star History

<a href="https://star-history.com/#mlflow/mlflow&Date">
 <picture>
   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date&theme=dark" />
   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />
   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />
 </picture>
</a>

## ✏️ Citation

If you use MLflow in your research, please cite it using the "Cite this repository" button at the top of the [GitHub repository page](https://github.com/mlflow/mlflow), which will provide you with citation formats including APA and BibTeX.

## 👥 Core Members

MLflow is currently maintained by the following core members with significant contributions from hundreds of exceptionally talented community members.

- [Ben Wilson](https://github.com/BenWilson2)
- [Corey Zumar](https://github.com/dbczumar)
- [Daniel Lok](https://github.com/daniellok-db)
- [Gabriel Fu](https://github.com/gabrielfu)
- [Harutaka Kawamura](https://github.com/harupy)
- [Serena Ruan](https://github.com/serena-ruan)
- [Tomu Hirata](https://github.com/TomeHirata)
- [Weichen Xu](https://github.com/WeichenXu123)
- [Yuki Watanabe](https://github.com/B-Step62)


--- bin/README.md ---
# bin

Binary tools for MLflow development.

## Installation

```bash
python bin/install.py
```


--- docker-compose/README.md ---
# MLflow with Docker Compose (PostgreSQL + MinIO)

This directory provides a **Docker Compose** setup for running **MLflow** locally with a **PostgreSQL** backend store and **MinIO** (S3-compatible) artifact storage. It's intended for quick evaluation and local development.

---

## Overview

- **MLflow Tracking Server** — exposed on your host (default `http://localhost:5000`).
- **PostgreSQL** — persists MLflow's metadata (experiments, runs, params, metrics).
- **MinIO** — stores run artifacts via an S3-compatible API.

Compose automatically reads configuration from a local `.env` file in this directory.

---

## Prerequisites

- **Git**
- **Docker** and **Docker Compose**
  - Windows/macOS: [Docker Desktop](https://www.docker.com/products/docker-desktop/)
  - Linux: Docker Engine + the `docker compose` plugin

Verify your setup:

```bash
docker --version
docker compose version
```

---

## 1. Clone the Repository

```bash
git clone https://github.com/mlflow/mlflow.git
cd docker-compose
```

---

## 2. Configure Environment

Copy the example environment file and modify as needed:

```bash
cp .env.dev.example .env
```

The `.env` file defines container image tags, ports, credentials, and storage configuration. Open it and review values before starting the stack.

**Common variables** :

- **MLflow**
  - `MLFLOW_PORT=5000` — host port for the MLflow UI/API
  - `MLFLOW_DEFAULT_ARTIFACT_ROOT=s3://mlflow/` — artifact store URI
  - `MLFLOW_S3_ENDPOINT_URL=http://minio:9000` — S3 endpoint (inside the Compose network)
- **PostgreSQL**
  - `POSTGRES_USER=mlflow`
  - `POSTGRES_PASSWORD=mlflow`
  - `POSTGRES_DB=mlflow`
- **MinIO (S3-compatible)**
  - `MINIO_ROOT_USER=minio`
  - `MINIO_ROOT_PASSWORD=minio123`
  - `MINIO_HOST=minio`
  - `MINIO_PORT=9000`
  - `MINIO_BUCKET=mlflow`

---

## 3. Launch the Stack

```bash
docker compose up -d
```

This:

- Builds/pulls images as needed
- Creates a user-defined network
- Starts **postgres**, **minio**, and **mlflow** containers

Check status:

```bash
docker compose ps
```

View logs (useful on first run):

```bash
docker compose logs -f
```

---

## 4. Access MLflow

Open the MLflow UI:

- **URL**: `http://localhost:5000` (or the port set in `.env`)

You can now create experiments, run training scripts, and log metrics, parameters, and artifacts to this local MLflow instance.

---

## 5. Shutdown

To stop and remove the containers and network:

```bash
docker compose down
```

> Data is preserved in Docker **volumes**. To remove volumes as well (irreversible), run:
>
> ```bash
> docker compose down -v
> ```

---

## Tips & Troubleshooting

- **Verify connectivity**  
  If MLflow can't write artifacts, confirm your S3 settings:

  - `MLFLOW_DEFAULT_ARTIFACT_ROOT` points to your MinIO bucket (e.g., `s3://mlflow/`)
  - `MLFLOW_S3_ENDPOINT_URL` is reachable from the MLflow container (often `http://minio:9000`)

- **Resetting the environment**  
  If you want a clean slate, stop the stack and remove volumes:

  ```bash
  docker compose down -v
  docker compose up -d
  ```

- **Logs**

  - MLflow server: `docker compose logs -f mlflow`
  - PostgreSQL: `docker compose logs -f postgres`
  - MinIO: `docker compose logs -f minio`

- **Port conflicts**  
  If `5000` (or any other port) is in use, change it in `.env` and restart:
  ```bash
  docker compose down
  docker compose up -d
  ```

---

## How It Works (at a Glance)

- MLflow uses **PostgreSQL** as the _backend store_ for experiment/run metadata.
- MLflow uses **MinIO** as the _artifact store_ via S3 APIs.
- Docker Compose wires services on a shared network; MLflow talks to PostgreSQL and MinIO by container name (e.g., `postgres`, `minio`).

---

## Next Steps

- Point your training scripts to this server:
  ```bash
  export MLFLOW_TRACKING_URI=http://localhost:5000
  ```
- Start logging runs with `mlflow.start_run()` (Python) or the MLflow CLI.
- Customize the `.env` and `docker-compose.yml` to fit your local workflow (e.g., change image tags, add volumes, etc.).

---

**You now have a fully local MLflow stack with persistent metadata and artifact storage—ideal for development and experimentation.**


--- prettier.config.js ---
module.exports = {
  printWidth: 100,
};


--- .github/workflows/README.md ---
# GitHub Actions workflows

## Testing

| File                      | Role                                                                 |
| :------------------------ | :------------------------------------------------------------------- |
| `cross-version-tests.yml` | Run cross version tests. See `cross-version-testing.md` for details. |
| `examples.yml`            | Run tests for example scripts & projects                             |
| `master.yml `             | Run unit and integration tests                                       |

## Automation

| File                        | Role                                                           |
| :-------------------------- | :------------------------------------------------------------- |
| `autoformat.yml`            | Apply autoformatting when a PR is commented with `autoformat`  |
| `autoformat.js`             | Define utility functions used in the `autoformat.yml` workflow |
| `labeling.yml`              | Automatically apply labels on issues and PRs                   |
| `notify-dco-failure.yml`    | Notify a DCO check failure                                     |
| `notify-dco-failure.js`     | The main script of the `notify-dco-failure.yml` workflow       |
| `release-note-category.yml` | Validate a release-note category label is applied on a PR      |
| `release-note-category.js`  | The main script of the `release-note-category.yml` workflow    |


--- .github/pull_request_template.md ---
### Related Issues/PRs

<!-- Uncomment 'Resolve' if this PR can close the linked items. -->
<!-- Resolve --> #xxx

### What changes are proposed in this pull request?

<!-- Please fill in changes proposed in this PR. -->

### How is this PR tested?

- [ ] Existing unit/integration tests
- [ ] New unit/integration tests
- [ ] Manual tests

<!-- Attach code, screenshot, video used for manual testing here. -->

### Does this PR require documentation update?

- [ ] No. You can skip the rest of this section.
- [ ] Yes. I've updated:
  - [ ] Examples
  - [ ] API references
  - [ ] Instructions

### Release Notes

#### Is this a user-facing change?

- [ ] No. You can skip the rest of this section.
- [ ] Yes. Give a description of this change to be included in the release notes for MLflow users.

<!-- Details in 1-2 sentences. You can just refer to another PR with a description if this PR is part of a larger change. -->

#### What component(s), interfaces, languages, and integrations does this PR affect?

Components

- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging
- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors
- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry
- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs
- [ ] `area/evaluation`: MLflow model evaluation features, evaluation metrics, and evaluation workflows
- [ ] `area/gateway`: MLflow AI Gateway client APIs, server, and third-party integrations
- [ ] `area/prompts`: MLflow prompt engineering features, prompt templates, and prompt management
- [ ] `area/tracing`: MLflow Tracing features, tracing APIs, and LLM tracing functionality
- [ ] `area/projects`: MLproject format, project running backends
- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server
- [ ] `area/build`: Build and test infrastructure for MLflow
- [ ] `area/docs`: MLflow documentation pages

<!--
Insert an empty named anchor here to allow jumping to this section with a fragment URL
(e.g. https://github.com/mlflow/mlflow/pull/123#user-content-release-note-category).
Note that GitHub prefixes anchor names in markdown with "user-content-".
-->

<a name="release-note-category"></a>

#### How should the PR be classified in the release notes? Choose one:

- [ ] `rn/none` - No description will be included. The PR will be mentioned only by the PR number in the "Small Bugfixes and Documentation Updates" section
- [ ] `rn/breaking-change` - The PR will be mentioned in the "Breaking Changes" section
- [ ] `rn/feature` - A new user-facing feature worth mentioning in the release notes
- [ ] `rn/bug-fix` - A user-facing bug fix worth mentioning in the release notes
- [ ] `rn/documentation` - A user-facing documentation change worth mentioning in the release notes

#### Should this PR be included in the next patch release?

`Yes` should be selected for bug fixes, documentation updates, and other small changes. `No` should be selected for new features and larger changes. If you're unsure about the release classification of this PR, leave this unchecked to let the maintainers decide.

<details>
<summary>What is a minor/patch release?</summary>

- Minor release: a release that increments the second part of the version number (e.g., 1.2.0 -> 1.3.0).
  Bug fixes, doc updates and new features usually go into minor releases.
- Patch release: a release that increments the third part of the version number (e.g., 1.2.0 -> 1.2.1).
  Bug fixes and doc updates usually go into patch releases.

</details>

<!-- Do not modify or remove any text inside the parentheses -->

- [ ] Yes (this PR will be cherry-picked and included in the next patch release)
- [ ] No (this PR will be included in the next minor release)


--- .github/workflows/advice.js ---
function sleep(ms) {
  return new Promise((resolve) => setTimeout(resolve, ms));
}

async function getDcoCheck(github, owner, repo, sha) {
  const backoffs = [0, 2, 4, 6, 8];
  const numAttempts = backoffs.length;
  for (const [index, backoff] of backoffs.entries()) {
    await sleep(backoff * 1000);
    const resp = await github.rest.checks.listForRef({
      owner,
      repo,
      ref: sha,
      app_id: 1861, // ID of the DCO check app
    });

    const { check_runs } = resp.data;
    if (check_runs.length > 0 && check_runs[0].status === "completed") {
      return check_runs[0];
    }
    console.log(`[Attempt ${index + 1}/${numAttempts}]`, "The DCO check hasn't completed yet.");
  }
}

module.exports = async ({ context, github }) => {
  const { owner, repo } = context.repo;
  const { number: issue_number } = context.issue;
  const { sha, label } = context.payload.pull_request.head;
  const { user, body } = context.payload.pull_request;
  const messages = [];

  const title = "&#x1F6E0 DevTools &#x1F6E0";
  if (body && !body.includes(title)) {
    const codespacesBadge = `[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/${user.login}/mlflow/pull/${issue_number}?quickstart=1)`;
    const newSection = `
<details><summary>${title}</summary>
<p>

${codespacesBadge}

#### Install mlflow from this PR

\`\`\`
# mlflow
pip install git+https://github.com/mlflow/mlflow.git@refs/pull/${issue_number}/merge
# mlflow-skinny
pip install git+https://github.com/mlflow/mlflow.git@refs/pull/${issue_number}/merge#subdirectory=libs/skinny
\`\`\`

For Databricks, use the following command:

\`\`\`
%sh curl -LsSf https://raw.githubusercontent.com/mlflow/mlflow/HEAD/dev/install-skinny.sh | sh -s pull/${issue_number}/merge
\`\`\`

</p>
</details>
`.trim();
    await github.rest.pulls.update({
      owner,
      repo,
      pull_number: issue_number,
      body: `${newSection}\n\n${body}`,
    });
  }

  const dcoCheck = await getDcoCheck(github, owner, repo, sha);
  if (dcoCheck && dcoCheck.conclusion !== "success") {
    messages.push(
      "#### &#x26a0; DCO check\n\n" +
        "The DCO check failed. " +
        `Please sign off your commit(s) by following the instructions [here](${dcoCheck.html_url}). ` +
        "See https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#sign-your-work for more " +
        "details."
    );
  }

  if (label.endsWith(":master")) {
    messages.push(
      "#### &#x26a0; PR branch check\n\n" +
        "This PR was filed from the master branch in your fork, which is not recommended " +
        "and may cause our CI checks to fail. Please close this PR and file a new PR from " +
        "a non-master branch."
    );
  }

  if (!(body || "").includes("How should the PR be classified in the release notes?")) {
    messages.push(
      "#### &#x26a0; Invalid PR template\n\n" +
        "This PR does not appear to have been filed using the MLflow PR template. " +
        "Please copy the PR template from [here](https://raw.githubusercontent.com/mlflow/mlflow/master/.github/pull_request_template.md) " +
        "and fill it out."
    );
  }

  if (messages.length > 0) {
    const body =
      `@${user.login} Thank you for the contribution! Could you fix the following issue(s)?\n\n` +
      messages.join("\n\n");
    await github.rest.issues.createComment({
      owner,
      repo,
      issue_number,
      body,
    });
  }
};


--- .github/workflows/autoformat.js ---
const createCommitStatus = async (context, github, sha, state) => {
  const { workflow, runId } = context;
  const { owner, repo } = context.repo;
  const target_url = `https://github.com/${owner}/${repo}/actions/runs/${runId}`;
  await github.rest.repos.createCommitStatus({
    owner,
    repo,
    sha,
    state,
    target_url,
    description: sha,
    context: workflow,
  });
};

const shouldAutoformat = (comment) => {
  return comment.body.trim() === "/autoformat";
};

const getPullInfo = async (context, github) => {
  const { owner, repo } = context.repo;
  const pull_number = context.issue.number;
  const pr = await github.rest.pulls.get({ owner, repo, pull_number });
  const {
    sha: head_sha,
    ref: head_ref,
    repo: { full_name },
  } = pr.data.head;
  const { sha: base_sha, ref: base_ref, repo: base_repo } = pr.data.base;
  return {
    repository: full_name,
    pull_number,
    head_sha,
    head_ref,
    base_sha,
    base_ref,
    base_repo: base_repo.full_name,
    author_association: pr.data.author_association,
  };
};

const createReaction = async (context, github) => {
  const { owner, repo } = context.repo;
  const { id: comment_id } = context.payload.comment;
  await github.rest.reactions.createForIssueComment({
    owner,
    repo,
    comment_id,
    content: "rocket",
  });
};

const createStatus = async (context, github, core) => {
  const { head_sha, head_ref, repository } = await getPullInfo(context, github);
  if (repository === "mlflow/mlflow" && head_ref === "master") {
    core.setFailed("Running autoformat bot against master branch of mlflow/mlflow is not allowed.");
  }
  await createCommitStatus(context, github, head_sha, "pending");
};

const updateStatus = async (context, github, sha, needs) => {
  const failed = Object.values(needs).some(({ result }) => result === "failure");
  const state = failed ? "failure" : "success";
  await createCommitStatus(context, github, sha, state);
};

const fetchWorkflowRuns = async ({ context, github, head_sha }) => {
  const { owner, repo } = context.repo;
  const SLEEP_DURATION_MS = 5000;
  const MAX_RETRIES = 5;
  let prevRuns = [];
  for (let i = 0; i < MAX_RETRIES; i++) {
    console.log(`Attempt ${i + 1} to fetch workflow runs`);
    const runs = await github.paginate(github.rest.actions.listWorkflowRunsForRepo, {
      owner,
      repo,
      head_sha,
      status: "action_required",
      actor: "mlflow-app[bot]",
    });

    // If the number of runs has not changed since the last attempt,
    // we can assume that all the workflow runs have been created.
    if (runs.length > 0 && runs.length === prevRuns.length) {
      return runs;
    }

    prevRuns = runs;
    await new Promise((resolve) => setTimeout(resolve, SLEEP_DURATION_MS));
  }
  return prevRuns;
};

const approveWorkflowRuns = async (context, github, head_sha) => {
  const { owner, repo } = context.repo;
  const workflowRuns = await fetchWorkflowRuns({ context, github, head_sha });
  const approvePromises = workflowRuns.map((run) =>
    github.rest.actions.approveWorkflowRun({
      owner,
      repo,
      run_id: run.id,
    })
  );
  const results = await Promise.allSettled(approvePromises);
  for (const result of results) {
    if (result.status === "rejected") {
      console.error(`Failed to approve run: ${result.reason}`);
    }
  }
};

const checkMaintainerAccess = async (context, github) => {
  const { owner, repo } = context.repo;
  const pull_number = context.issue.number;
  const { runId } = context;
  const pr = await github.rest.pulls.get({ owner, repo, pull_number });

  // Skip maintainer access check for copilot bot PRs
  // Copilot bot creates PRs that are owned by the repository and don't need the same permission model
  if (
    pr.data.user?.type?.toLowerCase() === "bot" &&
    pr.data.user?.login?.toLowerCase() === "copilot"
  ) {
    console.log(`Skipping maintainer access check for copilot bot PR #${pull_number}`);
    return;
  }

  const isForkPR = pr.data.head.repo.full_name !== pr.data.base.repo.full_name;
  if (isForkPR && !pr.data.maintainer_can_modify) {
    const workflowRunUrl = `https://github.com/${owner}/${repo}/actions/runs/${runId}`;

    await github.rest.issues.createComment({
      owner,
      repo,
      issue_number: pull_number,
      body: `❌ **Autoformat failed**: The "Allow edits and access to secrets by maintainers" checkbox must be checked for autoformat to work properly.

Please:
1. Check the "Allow edits and access to secrets by maintainers" checkbox on this pull request
2. Comment \`/autoformat\` again

This permission is required for the autoformat bot to push changes to your branch.

**Details:** [View workflow run](${workflowRunUrl})`,
    });

    throw new Error(
      'The "Allow edits and access to secrets by maintainers" checkbox must be checked for autoformat to work properly.'
    );
  }
};

module.exports = {
  shouldAutoformat,
  getPullInfo,
  createReaction,
  createStatus,
  updateStatus,
  approveWorkflowRuns,
  checkMaintainerAccess,
};


--- .github/workflows/autoformat.md ---
# Autoformat

## Testing

1. Checkout a new branch and make changes.
1. Push the branch to your fork (https://github.com/{your_username}/mlflow).
1. Switch the default branch of your fork to the branch you just pushed.
1. Create a GitHub token.
1. Create a new Actions secret with the name `MLFLOW_AUTOMATION_TOKEN` and put the token value.
1. Checkout another new branch and run the following commands to make dummy changes.

   ```shell
   # python
   echo "" >> setup.py
   # js
   echo "" >> mlflow/server/js/src/experiment-tracking/components/App.js
   # protos
   echo "message Foo {}" >> mlflow/protos/service.proto
   ```

1. Create a PR from the branch containing the dummy changes in your fork.
1. Comment `/autoformat` on the PR and ensure the workflow runs successfully.
   The workflow status can be checked at https://github.com/{your_username}/mlflow/actions/workflows/autoformat.yml.
1. Delete the GitHub token and reset the default branch.


--- .github/workflows/cancel.js ---
module.exports = async ({ context, github }) => {
  const owner = context.repo.owner;
  const repo = context.repo.repo;
  const headSha = context.payload.pull_request.head.sha;
  const prRuns = await github.paginate(github.rest.actions.listWorkflowRunsForRepo, {
    owner,
    repo,
    head_sha: headSha,
    event: "pull_request",
    per_page: 100,
  });
  const unfinishedRuns = prRuns.filter(
    ({ status, name }) =>
      // `post-merge` job in `release-note` workflow should not be cancelled
      status !== "completed" && name !== "release-note"
  );
  for (const run of unfinishedRuns) {
    try {
      // Some runs may have already completed, so we need to handle errors.
      await github.rest.actions.cancelWorkflowRun({
        owner,
        repo,
        run_id: run.id,
      });
      console.log(`Cancelled run ${run.id}`);
    } catch (error) {
      console.error(`Failed to cancel run ${run.id}`, error);
    }
  }
};


--- .github/workflows/closing-pr.js ---
// Regular expressions to capture a closing syntax in the PR body
// https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue
const CLOSING_SYNTAX_PATTERNS = [
  /(?:(?:close|fixe|resolve)[sd]?|fix)\s+(?:mlflow\/mlflow)?#(\d+)/gi,
  /(?:(?:close|fixe|resolve)[sd]?|fix)\s+(?:https?:\/\/github.com\/mlflow\/mlflow\/issues\/)(\d+)/gi,
];
const HAS_CLOSING_PR_LABEL = "has-closing-pr";

const getIssuesToClose = (body) => {
  const commentsExcluded = body.replace(/<!--(.+?)-->/gs, ""); // remove comments
  const matches = CLOSING_SYNTAX_PATTERNS.flatMap((pattern) =>
    Array.from(commentsExcluded.matchAll(pattern))
  );
  const issueNumbers = matches.map((match) => match[1]);
  return [...new Set(issueNumbers)].sort();
};

const arraysEqual = (a1, a2) => {
  return JSON.stringify(a1) == JSON.stringify(a2);
};

const assertArrayEqual = (a1, a2) => {
  if (!arraysEqual(a1, a2)) {
    throw `[${a1}] !== [${a2}]`;
  }
};

const capitalizeFirstLetter = (string) => {
  return string.charAt(0).toUpperCase() + string.slice(1);
};

const test = () => {
  ["close", "closes", "closed", "fix", "fixes", "fixed", "resolve", "resolves", "resolved"].forEach(
    (keyword) => {
      assertArrayEqual(getIssuesToClose(`${keyword} #123`), ["123"]);
      assertArrayEqual(getIssuesToClose(`${capitalizeFirstLetter(keyword)} #123`), ["123"]);
    }
  );

  const body2 = `
Fix mlflow/mlflow#123
Resolve https://github.com/mlflow/mlflow/issues/456
`;
  assertArrayEqual(getIssuesToClose(body2), ["123", "456"]);

  const body3 = `
Fix #123
Close #123
`;
  assertArrayEqual(getIssuesToClose(body3), ["123"]);

  const body4 = "Relates to #123";
  assertArrayEqual(getIssuesToClose(body4), []);

  const body5 = "<!-- close #123 -->";
  assertArrayEqual(getIssuesToClose(body5), []);

  const body6 = "Fixs #123 Fixd #456";
  assertArrayEqual(getIssuesToClose(body6), []);
};

// `node .github/workflows/closing-pr.js` runs this block
if (require.main === module) {
  test();
}

module.exports = async ({ context, github }) => {
  const { body } = context.payload.pull_request;
  const { owner, repo } = context.repo;
  for (const issue_number of getIssuesToClose(body || "")) {
    // Ignore PRs
    const { data: issue } = await github.rest.issues.get({
      owner,
      repo,
      issue_number,
    });
    if (issue.pull_request) {
      continue;
    }
    await github.rest.issues.addLabels({
      owner,
      repo,
      issue_number,
      labels: [HAS_CLOSING_PR_LABEL],
    });
  }
};


--- .github/workflows/cross-version-test-runner.js ---
async function main({ context, github }) {
  const { comment } = context.payload;
  const { owner, repo } = context.repo;
  const pull_number = context.issue.number;

  const { data: pr } = await github.rest.pulls.get({ owner, repo, pull_number });
  const flavorsMatch = comment.body.match(/\/(?:cross-version-test|cvt)\s+([^\n]+)\n?/);
  if (!flavorsMatch) {
    return;
  }

  // Run the workflow
  const flavors = flavorsMatch[1];
  const uuid = Array.from({ length: 16 }, () => Math.floor(Math.random() * 16).toString(16)).join(
    ""
  );
  const workflow_id = "cross-version-tests.yml";
  await github.rest.actions.createWorkflowDispatch({
    owner,
    repo,
    workflow_id,
    ref: pr.base.ref,
    inputs: {
      repository: `${owner}/${repo}`,
      ref: pr.merge_commit_sha,
      flavors,
      // The response of create-workflow-dispatch request doesn't contain the ID of the triggered
      // workflow run. We need to pass a unique identifier to the workflow run and find the run by
      // the identifier. See https://github.com/orgs/community/discussions/9752 for more details.
      uuid,
    },
  });

  // Find the triggered workflow run
  let run;
  const maxAttempts = 5;
  for (let i = 0; i < maxAttempts; i++) {
    await new Promise((resolve) => setTimeout(resolve, 5000));

    const { data: runs } = await github.rest.actions.listWorkflowRunsForRepo({
      owner,
      repo,
      workflow_id,
      event: "workflow_dispatch",
    });
    run = runs.workflow_runs.find((run) => run.name.includes(uuid));
    if (run) {
      break;
    }
  }

  if (!run) {
    await github.rest.issues.createComment({
      owner,
      repo,
      issue_number: pull_number,
      body: "Failed to find the triggered workflow run.",
    });
    return;
  }

  await github.rest.issues.createComment({
    owner,
    repo,
    issue_number: pull_number,
    body: `Cross-version test run started: ${run.html_url}`,
  });
}

module.exports = {
  main,
};


--- .github/workflows/cross-version-testing.md ---
# Cross version testing

## What is cross version testing?

Cross version testing is a testing strategy to ensure ML integrations in MLflow such as
`mlflow.sklearn` work properly with their associated packages across various versions.

## Key files

| File (relative path from the root)              | Role                                                           |
| :---------------------------------------------- | :------------------------------------------------------------- |
| [`mlflow/ml-package-versions.yml`][]            | Define which versions to test for each ML package.             |
| [`dev/set_matrix.py`][]                         | Generate a test matrix from `ml-package-versions.yml`.         |
| [`dev/update_ml_package_versions.py`][]         | Update `ml-package-versions.yml` when releasing a new version. |
| [`.github/workflows/cross-version-tests.yml`][] | Define a Github Actions workflow for cross version testing.    |

[`mlflow/ml-package-versions.yml`]: ../../mlflow/ml-package-versions.yml
[`dev/set_matrix.py`]: ../../dev/set_matrix.py
[`dev/update_ml_package_versions.py`]: ../../dev/update_ml_package_versions.py
[`.github/workflows/cross-version-tests.yml`]: ./cross-version-tests.yml

## Configuration keys in `ml-package-versions.yml`

```yml
# Note this is just an example and not the actual sklearn configuration.

# The top-level key specifies the integration name.
sklearn:
  package_info:
    # [Required] `pip_release` specifies the package this integration depends on.
    pip_release: "scikit-learn"

    # [Optional] `install_dev` specifies a set of commands to install the dev version of the package.
    # For example, the command below builds a wheel from the latest main branch of
    # the scikit-learn repository and installs it.
    #
    # The aim of testing the dev version is to spot issues as early as possible before they get
    # piled up, and fix them incrementally rather than fixing them at once when the package
    # releases a new version.
    install_dev: |
      pip install git+https://github.com/scikit-learn/scikit-learn.git

  # [At least one of `models` and `autologging` must be specified]
  # `models` specifies the configuration for model serialization and serving tests.
  # `autologging` specifies the configuration for autologging tests.
  models or autologging:
    # [Optional] `requirements` specifies additional pip requirements required for running tests.
    # For example, '">= 0.24.0": ["xgboost"]' is interpreted as 'if the version of scikit-learn
    # to install is newer than or equal to 0.24.0, install xgboost'.
    requirements:
      ">= 0.24.0": ["xgboost"]

    # [Required] `minimum` specifies the minimum supported version for the latest release of MLflow.
    minimum: "0.20.3"

    # [Required] `maximum` specifies the maximum supported version for the latest release of MLflow.
    maximum: "1.0"

    # [Optional] `unsupported` specifies a list of versions that should NOT be supported due to
    # unacceptable issues or bugs.
    unsupported: ["0.21.3"]

    # [Required] `run` specifies a set of commands to run tests.
    run: |
      pytest tests/sklearn/test_sklearn_model_export.py
```

## How do we determine which versions to test?

We determine which versions to test based on the following rules:

1. Only test [final][] (e.g. `1.0.0`) and [post][] (`1.0.0.post0`) releases.
2. Only test the latest micro version in each minor version.
   For example, if `1.0.0`, `1.0.1`, and `1.0.2` are available, we only test `1.0.2`.
3. The `maximum` version defines the maximum **major** version to test.
   For example, if the value of `maximum` is `1.0.0`, we test `1.1.0` (if available) but not `2.0.0`.
4. Always test the `minimum` version.

[final]: https://www.python.org/dev/peps/pep-0440/#final-releases
[post]: https://www.python.org/dev/peps/pep-0440/#post-releases

The table below describes which `scikit-learn` versions to test for the example configuration in
the previous section:

| Version       | Tested | Comment                                            |
| :------------ | :----- | -------------------------------------------------- |
| 0.20.3        | ✅     | The value of `minimum`                             |
| 0.20.4        | ✅     | The latest micro version of `0.20`                 |
| 0.21rc2       |        |                                                    |
| 0.21.0        |        |                                                    |
| 0.21.1        |        |                                                    |
| 0.21.2        | ✅     | The latest micro version of `0.21` without`0.21.3` |
| 0.21.3        |        | Excluded by `unsupported`                          |
| 0.22rc2.post1 |        |                                                    |
| 0.22rc3       |        |                                                    |
| 0.22          |        |                                                    |
| 0.22.1        |        |                                                    |
| 0.22.2        |        |                                                    |
| 0.22.2.post1  | ✅     | The latest micro version of `0.22`                 |
| 0.23.0rc1     |        |                                                    |
| 0.23.0        |        |                                                    |
| 0.23.1        |        |                                                    |
| 0.23.2        | ✅     | The latest micro version of `0.23`                 |
| 0.24.dev0     |        |                                                    |
| 0.24.0rc1     |        |                                                    |
| 0.24.0        |        |                                                    |
| 0.24.1        |        |                                                    |
| 0.24.2        | ✅     | The latest micro version of `0.24`                 |
| 1.0rc1        |        |                                                    |
| 1.0rc2        |        |                                                    |
| 1.0           |        | The value of `maximum`                             |
| 1.0.1         | ✅     | The latest micro version of `1.0`                  |
| 1.1.dev       | ✅     | The version installed by `install_dev`             |

## Why do we run tests against development versions?

In cross-version testing, we run daily tests against both publicly available and pre-release
development versions for all dependent libraries that are used by MLflow.
This section explains why.

### Without dev version test

First, let's take a look at what would happen **without** dev version test.

```
  |
  ├─ XGBoost merges a change on the master branch that breaks MLflow's XGBoost integration.
  |
  ├─ MLflow 1.20.0 release date
  |
  ├─ XGBoost 1.5.0 release date
  ├─ ❌ We notice the change here and might need to make a patch release if it's critical.
  |
  v
time
```

- We didn't notice the change until after XGBoost 1.5.0 was released.
- MLflow 1.20.0 doesn't work with XGBoost 1.5.0.

### With dev version test

Then, let's take a look at what would happen **with** dev version test.

```
  |
  ├─ XGBoost merges a change on the master branch that breaks MLflow's XGBoost integration.
  ├─ ✅ Tests for the XGBoost integration fail -> We can notice the change and apply a fix for it.
  |
  ├─ MLflow 1.20.0 release date
  |
  ├─ XGBoost 1.5.0 release date
  |
  v
time
```

- We can notice the change **before XGBoost 1.5.0 is released** and apply a fix for it **before releasing MLflow 1.20.0**.
- MLflow 1.20.0 works with XGBoost 1.5.0.

## When do we run cross version tests?

1. Daily at 7:00 UTC using a cron scheduler.
   [README on the repository root](../../README.md) has a badge ([![badge-img][]][badge-target]) that indicates the status of the most recent cron run.
2. When a PR that affects the ML integrations is created. Note we only run tests relevant to
   the affected ML integrations. For example, a PR that affects files in `mlflow/sklearn` triggers
   cross version tests for `sklearn`.

[badge-img]: https://github.com/mlflow/mlflow/workflows/Cross%20version%20tests/badge.svg?event=schedule
[badge-target]: https://github.com/mlflow/mlflow/actions?query=workflow%3ACross%2Bversion%2Btests+event%3Aschedule

## How to run cross version test for dev versions on a pull request

By default, cross version tests for dev versions are disabled on a pull request.
To enable them, the following steps are required.

1. Click `Labels` in the right sidebar.
2. Click the `enable-dev-tests` label and make sure it's applied on the pull request.
3. Push a new commit or re-run the `cross-version-tests` workflow.

See also:

- [GitHub Docs - Applying a label](https://docs.github.com/en/issues/using-labels-and-milestones-to-track-work/managing-labels#applying-a-label)
- [GitHub Docs - Re-running workflows and jobs](https://docs.github.com/en/actions/managing-workflow-runs/re-running-workflows-and-jobs)

## How to run cross version tests manually

The `cross-version-tests.yml` workflow can be run manually without creating a pull request.

1. Open https://github.com/mlflow/mlflow/actions/workflows/cross-version-tests.yml.
2. Click `Run workflow`.
3. Fill in the input parameters.
4. Click `Run workflow` at the bottom of the parameter input form.

See also:

- [GitHub Docs - Manually running a workflow](https://docs.github.com/en/actions/managing-workflow-runs/manually-running-a-workflow)


--- .github/workflows/delete-artifact.js ---
/**
 * Main function to handle documentation preview comments
 * @param {object} params - Parameters object containing context and github
 * @param {object} params.github - GitHub API client
 * @param {object} params.context - GitHub context
 * @param {object} params.env - Environment variables
 */
module.exports = async ({ github, context, env }) => {
  const artifactName = env.ARTIFACT_NAME;
  const runId = env.RUN_ID;

  if (!artifactName || !runId) {
    throw new Error("Missing required parameters: ARTIFACT_NAME, RUN_ID");
  }

  const { owner, repo } = context.repo;

  try {
    // INFO: https://octokit.github.io/rest.js/v22/#actions-list-workflow-run-artifacts
    const {
      data: { artifacts },
    } = await github.rest.actions.listWorkflowRunArtifacts({
      owner,
      repo,
      run_id: runId,
      name: artifactName,
    });

    const [artifact] = artifacts;

    // INFO: https://octokit.github.io/rest.js/v22/#actions-delete-artifact
    await github.rest.actions.deleteArtifact({
      owner,
      repo,
      artifact_id: artifact.id,
    });
  } catch (error) {
    console.error(`Could not find or delete the artifact for ${runId} and ${artifactName}`);
    throw error;
  }
};


--- dev/README.md ---
## MLflow Dev Scripts

This directory contains automation scripts for MLflow developers and the build infrastructure.

## Job Statuses

[![Examples Action Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/examples.yml.svg?branch=master&event=schedule&label=Examples&style=for-the-badge&logo=github)](https://github.com/mlflow/dev/actions/workflows/examples.yml?query=workflow%3AExamples+event%3Aschedule)
[![Cross Version Tests Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/cross-version-tests.yml.svg?branch=master&event=schedule&label=Cross%20version%20tests&style=for-the-badge&logo=github)](https://github.com/mlflow/dev/actions/workflows/cross-version-tests.yml?query=workflow%3A%22Cross+version+tests%22+event%3Aschedule)
[![Cross Version Test Visualization](https://img.shields.io/github/actions/workflow/status/mlflow/dev/xtest-viz.yml.svg?branch=master&event=schedule&label=Test%20Results%20Viz&style=for-the-badge&logo=github)](https://github.com/mlflow/dev/actions/workflows/xtest-viz.yml)
[![R-devel Action Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/r.yml.svg?branch=master&event=schedule&label=r-devel&style=for-the-badge&logo=github)](https://github.com/mlflow/dev/actions/workflows/r.yml?query=workflow%3AR+event%3Aschedule)
[![Test Requirements Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/requirements.yml.svg?branch=master&event=schedule&label=test%20requirements&logo=github&style=for-the-badge)](https://github.com/mlflow/dev/actions/workflows/requirements.yml?query=workflow%3A%22Test+requirements%22+event%3Aschedule)
[![Push Images Status](https://img.shields.io/github/actions/workflow/status/mlflow/mlflow/push-images.yml.svg?event=release&label=push-images&logo=github&style=for-the-badge)](https://github.com/mlflow/mlflow/actions/workflows/push-images.yml?query=event%3Arelease)
[![Slow Tests Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/slow-tests.yml.svg?branch=master&event=schedule&label=slow-tests&logo=github&style=for-the-badge)](https://github.com/mlflow/dev/actions/workflows/slow-tests.yml?query=event%3Aschedule)
[![Website E2E Tests Status](https://img.shields.io/github/actions/workflow/status/mlflow/mlflow-website/e2e.yml.svg?branch=main&event=schedule&label=website-e2e&logo=github&style=for-the-badge)](https://github.com/mlflow/mlflow-website/actions/workflows/e2e.yml?query=event%3Aschedule)


--- dev/clint/README.md ---
# Clint

A custom linter for mlflow to enforce rules that ruff doesn't cover.

## Installation

```
pip install -e dev/clint
```

## Usage

```bash
clint file.py ...
```

## Integrating with Visual Studio Code

1. Install [the Pylint extension](https://marketplace.visualstudio.com/items?itemName=ms-python.pylint)
2. Add the following setting in your `settings.json` file:

```json
{
  "pylint.path": ["${interpreter}", "-m", "clint"]
}
```

## Ignoring Rules for Specific Files or Lines

**To ignore a rule on a specific line (recommended):**

```python
foo()  # clint: disable=<rule_name>
```

Replace `<rule_name>` with the actual rule you want to disable.

**To ignore a rule for an entire file:**

Add the file path to the `exclude` list in your `pyproject.toml`:

```toml
[tool.clint]
exclude = [
  # ...existing entries...
  "path/to/file.py",
]
```

## Testing

```bash
pytest dev/clint
```


--- dev/proto_to_graphql/README.md ---
# MLflow Proto To GraphQL Autogeneration

## What is this

The system in `dev/proto_to_graphql` parses proto rpc definitions and generates graphql schema based on the proto rpc definition. The goal of this system is to quickly generate base GraphQL schema and resolver code so that we can easily take advantage of the data joining functionalities of GraphQL.

The autogenerated schema and resolver are in the following file: `mlflow/server/graphql/autogenerated_graphql_schema.py`

The autogenerated schema and resolvers are referenced and can be extended in this file `mlflow/server/graphql/graphql_schema_extensions.py`

You can run `python ./dev/proto_to_graphql/code_generator.py` or `./dev/generate-protos.sh` to trigger the codegen process.

## FAQs

### How to onboard a new rpc to GraphQL

- In your proto rpc definition, add `option (graphql) = {};` and re-run `./dev/generate-protos.sh`. You should see the changes in the generated schema. [Example](https://github.com/mlflow/mlflow/pull/11215/files#diff-8ab2ad3109b67a713e147edf557d4da88853563398ce354cc895bb5930950dc5R175).
- In `mlflow/server/handlers.py`, identify the handler function for your rpc, for example `_get_run`, make sure there exists a corresponding `get_run_impl` function that takes in a `request_message` and returns a response messages that is of the generated service_pb proto type. If no such function exists, you can easily extract it out like in this [example](https://github.com/mlflow/mlflow/pull/11215/files#diff-5c10a4e2ca47745f06fa9e7201087acfc102849756cb8d85e774a5ac468cb037R1779-R1795).
- Test manually with a localhost server, as well as adding a unit test in `tests/tracking/test_rest_tracking.py`. [Example](https://github.com/mlflow/mlflow/pull/11215/files#diff-2ec8756f67a20ecbaeec2d2c5e7bf33310a50c015fc3aa487e27100fc4c2f9a7R1771-R1802).

### How to customize a generated query/mutation to join multiple rpc endpoints

The proto to graphql autogeneration only supports 1 to 1 mapping from proto rpc to graphql operation. However, the power of GraphQL is to join multiple rpc endpoints together as one query. So we often would like to customize or extend the autogenerated operations to join these multiple endpoints.

For example, we would like to query data about `Experiment`, `ModelVersions` and `Run` in one query by extending the `MlflowRun` object.

```
query testQuery {
    mlflowGetRun(input: {runId: "my-id"}) {
        run {
            experiment {
                name
            }
            modelVersions {
                name
            }
        }
    }
}
```

To achieve joins, follow the steps below:

- Make sure the rpcs you would like to join are already onboarded to GraphQL by following the `How to onboard a new rpc to GraphQL` section
- Identify the class you would like to extend in `autogenerated_graphql_schema.py` and create a new class that inherits the target class, put it in `graphql_schema_extensions.py`. Add the new fields and the resolver function as you intended. [Example](https://github.com/mlflow/mlflow/pull/11173/files#diff-9e4f7bdf4d7f9d362338bed9ce6607a51b8f520ee605e2fd4c9bda5e43cb617cR21-R31)
- Run `python ./dev/proto_to_graphql/code_generator.py` or `./dev/generate-protos.sh`, you should see the autogenerated schema being updated to reference the extension class you just created.
- Add a test case in `tests/tracking/test_rest_tracking.py` [Example](https://github.com/mlflow/mlflow/pull/11173/files#diff-2ec8756f67a20ecbaeec2d2c5e7bf33310a50c015fc3aa487e27100fc4c2f9a7R1771-R1795)

### How to generate typescript types for a GraphQL operation

To generate typescript types, first make sure the generated schema is up-to-date by running `python ./dev/proto_to_graphql/code_generator.py`

Then write your new query or mutation in the mlflow/server/js/src folder, after that run the following commands:

- cd mlflow/server/js
- yarn graphql-codegen

You should be able to see the generated types in `mlflow/server/js/src/graphql/__generated__/`


--- dev/build.py ---
import argparse
import contextlib
import shutil
import subprocess
import sys
from dataclasses import dataclass
from pathlib import Path


@dataclass(frozen=True)
class Package:
    # name of the package on PyPI.
    pypi_name: str
    # type of the package, one of "dev", "skinny", "tracing", "release"
    type: str
    # path to the package relative to the root of the repository
    build_path: str


DEV = Package("mlflow", "dev", ".")
RELEASE = Package("mlflow", "release", ".")
SKINNY = Package("mlflow-skinny", "skinny", "libs/skinny")
TRACING = Package("mlflow-tracing", "tracing", "libs/tracing")

PACKAGES = [
    DEV,
    SKINNY,
    RELEASE,
    TRACING,
]


def parse_args():
    parser = argparse.ArgumentParser(description="Build MLflow package.")
    parser.add_argument(
        "--package-type",
        help="Package type to build. Default is 'dev'.",
        choices=[p.type for p in PACKAGES],
        default="dev",
    )
    parser.add_argument(
        "--sha",
        help="If specified, include the SHA in the wheel name as a build tag.",
    )
    return parser.parse_args()


@contextlib.contextmanager
def restore_changes():
    try:
        yield
    finally:
        subprocess.check_call(
            [
                "git",
                "restore",
                "README.md",
                "pyproject.toml",
            ]
        )


def main():
    args = parse_args()

    # Clean up build artifacts generated by previous builds
    paths_to_clean_up = ["build"]
    for pkg in PACKAGES:
        paths_to_clean_up += [
            f"{pkg.build_path}/dist",
            f"{pkg.build_path}/{pkg.pypi_name}.egg_info",
        ]
    for path in map(Path, paths_to_clean_up):
        if not path.exists():
            continue
        if path.is_file():
            path.unlink()
        else:
            shutil.rmtree(path)

    package = next(p for p in PACKAGES if p.type == args.package_type)

    with restore_changes():
        pyproject = Path("pyproject.toml")
        if package == RELEASE:
            pyproject.write_text(Path("pyproject.release.toml").read_text())

        subprocess.check_call(
            [
                sys.executable,
                "-m",
                "build",
                package.build_path,
            ]
        )

        DIST_DIR = Path("dist")
        DIST_DIR.mkdir(exist_ok=True)
        if package in (SKINNY, TRACING):
            # Move `libs/xyz/dist/*` to `dist/`
            for src in (Path(package.build_path) / "dist").glob("*"):
                print(src)
                dst = DIST_DIR / src.name
                if dst.exists():
                    dst.unlink()
                src.rename(dst)

    if args.sha:
        # If build succeeds, there should be one wheel in the dist directory
        wheel = next(DIST_DIR.glob("mlflow*.whl"))
        name, version, rest = wheel.name.split("-", 2)
        build_tag = f"0.sha.{args.sha}"  # build tag must start with a digit
        wheel.rename(wheel.with_name(f"{name}-{version}-{build_tag}-{rest}"))


if __name__ == "__main__":
    main()


--- dev/check_function_signatures.py ---
from __future__ import annotations

import argparse
import ast
import os
import subprocess
import sys
from dataclasses import dataclass
from pathlib import Path


def is_github_actions() -> bool:
    return os.environ.get("GITHUB_ACTIONS") == "true"


@dataclass
class Error:
    file_path: Path
    line: int
    column: int
    lines: list[str]

    def format(self, github: bool = False) -> str:
        message = " ".join(self.lines)
        if github:
            return f"::warning file={self.file_path},line={self.line},col={self.column}::{message}"
        else:
            return f"{self.file_path}:{self.line}:{self.column}: {message}"


@dataclass
class Parameter:
    name: str
    position: int | None  # None for keyword-only
    is_required: bool
    is_positional_only: bool
    is_keyword_only: bool
    lineno: int
    col_offset: int


@dataclass
class Signature:
    positional: list[Parameter]  # Includes positional-only and regular positional
    keyword_only: list[Parameter]
    has_var_positional: bool  # *args
    has_var_keyword: bool  # **kwargs


@dataclass
class ParameterError:
    message: str
    param_name: str
    lineno: int
    col_offset: int


def parse_signature(args: ast.arguments) -> Signature:
    """Convert ast.arguments to a Signature dataclass for easier processing."""
    parameters_positional: list[Parameter] = []
    parameters_keyword_only: list[Parameter] = []

    # Process positional-only parameters
    for i, arg in enumerate(args.posonlyargs):
        parameters_positional.append(
            Parameter(
                name=arg.arg,
                position=i,
                is_required=True,  # All positional-only are required
                is_positional_only=True,
                is_keyword_only=False,
                lineno=arg.lineno,
                col_offset=arg.col_offset,
            )
        )

    # Process regular positional parameters
    offset = len(args.posonlyargs)
    first_optional_idx = len(args.posonlyargs + args.args) - len(args.defaults)

    for i, arg in enumerate(args.args):
        pos = offset + i
        parameters_positional.append(
            Parameter(
                name=arg.arg,
                position=pos,
                is_required=pos < first_optional_idx,
                is_positional_only=False,
                is_keyword_only=False,
                lineno=arg.lineno,
                col_offset=arg.col_offset,
            )
        )

    # Process keyword-only parameters
    for arg, default in zip(args.kwonlyargs, args.kw_defaults):
        parameters_keyword_only.append(
            Parameter(
                name=arg.arg,
                position=None,
                is_required=default is None,
                is_positional_only=False,
                is_keyword_only=True,
                lineno=arg.lineno,
                col_offset=arg.col_offset,
            )
        )

    return Signature(
        positional=parameters_positional,
        keyword_only=parameters_keyword_only,
        has_var_positional=args.vararg is not None,
        has_var_keyword=args.kwarg is not None,
    )


def check_signature_compatibility(
    old_fn: ast.FunctionDef | ast.AsyncFunctionDef,
    new_fn: ast.FunctionDef | ast.AsyncFunctionDef,
) -> list[ParameterError]:
    """
    Return list of error messages when *new_fn* is not backward-compatible with *old_fn*,
    or None if compatible.

    Compatibility rules
    -------------------
    • Positional / positional-only parameters
        - Cannot be reordered, renamed, or removed.
        - Adding **required** ones is breaking.
        - Adding **optional** ones is allowed only at the end.
        - Making an optional parameter required is breaking.

    • Keyword-only parameters (order does not matter)
        - Cannot be renamed or removed.
        - Making an optional parameter required is breaking.
        - Adding a required parameter is breaking; adding an optional parameter is fine.
    """
    old_sig = parse_signature(old_fn.args)
    new_sig = parse_signature(new_fn.args)
    errors: list[ParameterError] = []

    # ------------------------------------------------------------------ #
    # 1. Positional / pos-only parameters
    # ------------------------------------------------------------------ #

    # (a) existing parameters must line up
    for idx, old_param in enumerate(old_sig.positional):
        if idx >= len(new_sig.positional):
            errors.append(
                ParameterError(
                    message=f"Positional param '{old_param.name}' was removed.",
                    param_name=old_param.name,
                    lineno=old_param.lineno,
                    col_offset=old_param.col_offset,
                )
            )
            continue

        new_param = new_sig.positional[idx]
        if old_param.name != new_param.name:
            errors.append(
                ParameterError(
                    message=(
                        f"Positional param order/name changed: "
                        f"'{old_param.name}' -> '{new_param.name}'."
                    ),
                    param_name=new_param.name,
                    lineno=new_param.lineno,
                    col_offset=new_param.col_offset,
                )
            )
            # Stop checking further positional params after first order/name mismatch
            break

        if (not old_param.is_required) and new_param.is_required:
            errors.append(
                ParameterError(
                    message=f"Optional positional param '{old_param.name}' became required.",
                    param_name=new_param.name,
                    lineno=new_param.lineno,
                    col_offset=new_param.col_offset,
                )
            )

    # (b) any extra new positional params must be optional and appended
    if len(new_sig.positional) > len(old_sig.positional):
        for idx in range(len(old_sig.positional), len(new_sig.positional)):
            new_param = new_sig.positional[idx]
            if new_param.is_required:
                errors.append(
                    ParameterError(
                        message=f"New required positional param '{new_param.name}' added.",
                        param_name=new_param.name,
                        lineno=new_param.lineno,
                        col_offset=new_param.col_offset,
                    )
                )

    # ------------------------------------------------------------------ #
    # 2. Keyword-only parameters (order-agnostic)
    # ------------------------------------------------------------------ #
    old_kw_names = {p.name for p in old_sig.keyword_only}
    new_kw_names = {p.name for p in new_sig.keyword_only}

    # Build mappings for easier lookup
    old_kw_by_name = {p.name: p for p in old_sig.keyword_only}
    new_kw_by_name = {p.name: p for p in new_sig.keyword_only}

    # removed or renamed
    for name in old_kw_names - new_kw_names:
        old_param = old_kw_by_name[name]
        errors.append(
            ParameterError(
                message=f"Keyword-only param '{name}' was removed.",
                param_name=name,
                lineno=old_param.lineno,
                col_offset=old_param.col_offset,
            )
        )

    # optional -> required upgrades
    for name in old_kw_names & new_kw_names:
        if not old_kw_by_name[name].is_required and new_kw_by_name[name].is_required:
            new_param = new_kw_by_name[name]
            errors.append(
                ParameterError(
                    message=f"Keyword-only param '{name}' became required.",
                    param_name=name,
                    lineno=new_param.lineno,
                    col_offset=new_param.col_offset,
                )
            )

    # new required keyword-only params
    for param in new_sig.keyword_only:
        if param.is_required and param.name not in old_kw_names:
            errors.append(
                ParameterError(
                    message=f"New required keyword-only param '{param.name}' added.",
                    param_name=param.name,
                    lineno=param.lineno,
                    col_offset=param.col_offset,
                )
            )

    return errors


def _is_private(n: str) -> bool:
    return n.startswith("_") and not n.startswith("__") and not n.endswith("__")


class FunctionSignatureExtractor(ast.NodeVisitor):
    def __init__(self):
        self.functions: dict[str, ast.FunctionDef | ast.AsyncFunctionDef] = {}
        self.stack: list[ast.ClassDef] = []

    def visit_ClassDef(self, node: ast.ClassDef) -> None:
        self.stack.append(node)
        self.generic_visit(node)
        self.stack.pop()

    def visit_FunctionDef(self, node: ast.FunctionDef) -> None:
        # Is this a private function or a function in a private class?
        # If so, skip it.
        if _is_private(node.name) or (self.stack and _is_private(self.stack[-1].name)):
            return

        names = [*(c.name for c in self.stack), node.name]
        self.functions[".".join(names)] = node

    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef) -> None:
        if _is_private(node.name) or (self.stack and _is_private(self.stack[-1].name)):
            return

        names = [*(c.name for c in self.stack), node.name]
        self.functions[".".join(names)] = node


def get_changed_python_files(base_branch: str = "master") -> list[Path]:
    # In GitHub Actions PR context, we need to fetch the base branch first
    if is_github_actions():
        # Fetch the base branch to ensure we have it locally
        subprocess.check_call(
            ["git", "fetch", "origin", f"{base_branch}:{base_branch}"],
        )

    result = subprocess.check_output(
        ["git", "diff", "--name-only", f"{base_branch}...HEAD"], text=True
    )
    files = [s.strip() for s in result.splitlines()]
    return [Path(f) for f in files if f]


def parse_functions(content: str) -> dict[str, ast.FunctionDef | ast.AsyncFunctionDef]:
    tree = ast.parse(content)
    extractor = FunctionSignatureExtractor()
    extractor.visit(tree)
    return extractor.functions


def get_file_content_at_revision(file_path: Path, revision: str) -> str | None:
    try:
        return subprocess.check_output(["git", "show", f"{revision}:{file_path}"], text=True)
    except subprocess.CalledProcessError as e:
        print(f"Warning: Failed to get file content at revision: {e}", file=sys.stderr)
        return None


def compare_signatures(base_branch: str = "master") -> list[Error]:
    errors: list[Error] = []
    for file_path in get_changed_python_files(base_branch):
        # Ignore non-Python files
        if not file_path.suffix == ".py":
            continue

        # Ignore files not in the mlflow directory
        if file_path.parts[0] != "mlflow":
            continue

        # Ignore private modules
        if any(part.startswith("_") for part in file_path.parts):
            continue

        base_content = get_file_content_at_revision(file_path, base_branch)
        if base_content is None:
            # Find not found in the base branch, likely added in the current branch
            continue

        if not file_path.exists():
            # File not found, likely deleted in the current branch
            continue

        current_content = file_path.read_text()
        base_functions = parse_functions(base_content)
        current_functions = parse_functions(current_content)
        for func_name in set(base_functions.keys()) & set(current_functions.keys()):
            base_func = base_functions[func_name]
            current_func = current_functions[func_name]
            if param_errors := check_signature_compatibility(base_func, current_func):
                # Create individual errors for each problematic parameter
                for param_error in param_errors:
                    errors.append(
                        Error(
                            file_path=file_path,
                            line=param_error.lineno,
                            column=param_error.col_offset + 1,
                            lines=[
                                "[Non-blocking | Ignore if not public API]",
                                param_error.message,
                                f"This change will break existing `{func_name}` calls.",
                                "If this is not intended, please fix it.",
                            ],
                        )
                    )

    return errors


@dataclass
class Args:
    base_branch: str


def parse_args() -> Args:
    parser = argparse.ArgumentParser(
        description="Check for breaking changes in Python function signatures"
    )
    parser.add_argument("--base-branch", default=os.environ.get("GITHUB_BASE_REF", "master"))
    args = parser.parse_args()
    return Args(base_branch=args.base_branch)


def main():
    args = parse_args()
    errors = compare_signatures(args.base_branch)
    for error in errors:
        print(error.format(github=is_github_actions()))


if __name__ == "__main__":
    main()


--- dev/check_init_py.py ---
"""
Pre-commit hook to check for missing `__init__.py` files in mlflow and tests directories.

This script ensures that all directories under the mlflow package and tests directory that contain
Python files also have an `__init__.py` file. This prevents `setuptools` from excluding these
directories during package build and ensures test modules are properly structured.

Usage:
    uv run dev/check_init_py.py

Requirements:
- If `mlflow/foo/bar.py` exists, `mlflow/foo/__init__.py` must exist.
- If `tests/foo/test_bar.py` exists, `tests/foo/__init__.py` must exist.
- Only test files (starting with `test_`) in the tests directory are checked.
- All parent directories of Python files are checked recursively for `__init__.py`.
- Ignore directories that do not contain any Python files (e.g., `mlflow/server/js`).
"""

import subprocess
import sys
from pathlib import Path


def get_tracked_python_files() -> list[Path]:
    try:
        result = subprocess.check_output(
            ["git", "ls-files", "mlflow/**/*.py", "tests/**/*.py"],
            text=True,
        )
        paths = (Path(f) for f in result.splitlines() if f)
        return [p for p in paths if not p.is_relative_to("tests") or p.name.startswith("test_")]
    except subprocess.CalledProcessError as e:
        print(f"Error running git ls-files: {e}", file=sys.stderr)
        sys.exit(1)


def main() -> int:
    python_files = get_tracked_python_files()
    if not python_files:
        return 0

    python_dirs = {p for f in python_files for p in f.parents if p != Path(".")}
    missing_init_files = [d for d in python_dirs if not (d / "__init__.py").exists()]
    if missing_init_files:
        print("Error: The following directories contain Python files but lack __init__.py:")
        for d in sorted(missing_init_files):
            print(f"  {d.as_posix()}/")
        print("Please add __init__.py files to the directories listed above.")
        return 1

    return 0


if __name__ == "__main__":
    sys.exit(main())


--- dev/check_patch_prs.py ---
import argparse
import os
import re
import subprocess
import sys
import tempfile
from dataclasses import dataclass

import requests


def get_release_branch(version):
    major_minor_version = ".".join(version.split(".")[:2])
    return f"branch-{major_minor_version}"


@dataclass(frozen=True)
class Commit:
    sha: str
    pr_num: int


def get_commits(branch: str):
    """
    Get the commits in the release branch.
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        subprocess.check_call(
            [
                "git",
                "clone",
                "--shallow-since=3 months ago",
                "--branch",
                branch,
                "https://github.com/mlflow/mlflow.git",
                tmpdir,
            ],
        )
        log_stdout = subprocess.check_output(
            [
                "git",
                "log",
                "--pretty=format:%H %s",
            ],
            text=True,
            cwd=tmpdir,
        )
        pr_rgx = re.compile(r"([a-z0-9]+) .+\s+\(#(\d+)\)$")
        commits = []
        for commit in log_stdout.splitlines():
            if m := pr_rgx.search(commit.rstrip()):
                commits.append(Commit(sha=m.group(1), pr_num=int(m.group(2))))

    return commits


@dataclass(frozen=True)
class PR:
    pr_num: int
    merged: bool


def is_closed(pr):
    return pr["state"] == "closed" and pr["pull_request"]["merged_at"] is None


def fetch_patch_prs(version):
    """
    Fetch PRs labeled with `v{version}` from the MLflow repository.
    """
    label = f"v{version}"
    per_page = 100
    page = 1
    pulls = []
    while True:
        response = requests.get(
            f'https://api.github.com/search/issues?q=is:pr+repo:mlflow/mlflow+label:"{label}"&per_page={per_page}&page={page}',
        )
        response.raise_for_status()
        data = response.json()
        # Exclude closed PRs that are not merged
        pulls.extend(pr for pr in data["items"] if not is_closed(pr))
        if len(data) < per_page:
            break
        page += 1

    return {pr["number"]: pr["pull_request"].get("merged_at") is not None for pr in pulls}


def main(version, dry_run):
    release_branch = get_release_branch(version)
    commits = get_commits(release_branch)
    patch_prs = fetch_patch_prs(version)
    if not_cherry_picked := set(patch_prs) - {c.pr_num for c in commits}:
        print(f"The following patch PRs are not cherry-picked to {release_branch}:")
        for idx, pr_num in enumerate(sorted(not_cherry_picked)):
            merged = patch_prs[pr_num]
            url = f"https://github.com/mlflow/mlflow/pull/{pr_num} (merged: {merged})"
            line = f"  {idx + 1}. {url}"
            if not merged:
                line = f"\033[91m{line}\033[0m"  # Red color using ANSI escape codes
            print(line)

        master_commits = get_commits("master")
        cherry_picks = [c.sha for c in master_commits if c.pr_num in not_cherry_picked]
        # reverse the order of cherry-picks to maintain the order of PRs
        print("\n# Steps to cherry-pick the patch PRs:")
        print(
            f"1. Make sure your local master and {release_branch} branches are synced with "
            "upstream."
        )
        print(f"2. Cut a new branch from {release_branch} (e.g. {release_branch}-cherry-picks).")
        print("3. Run the following command on the new branch:\n")
        print("git cherry-pick " + " ".join(cherry_picks[::-1]))
        print(f"\n4. File a PR against {release_branch}.")
        sys.exit(0 if dry_run else 1)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--version", required=True, help="The version to release")
    parser.add_argument(
        "--dry-run",
        action="store_true",
        default=os.environ.get("DRY_RUN", "true").lower() == "true",
        help="Dry run mode (default: True, can be set via DRY_RUN env var)",
    )
    parser.add_argument(
        "--no-dry-run",
        action="store_false",
        dest="dry_run",
        help="Disable dry run mode",
    )
    args = parser.parse_args()
    main(args.version, args.dry_run)


--- dev/extract_deps.py ---
import ast
import re
from pathlib import Path


def parse_dependencies(content: str) -> list[str]:
    pattern = r"dependencies\s*=\s*(\[[\s\S]*?\])"
    match = re.search(pattern, content)
    deps_str = match.group(1)
    return ast.literal_eval(deps_str)


def main():
    content = Path("pyproject.toml").read_text()
    dependencies = parse_dependencies(content)
    print("\n".join(dependencies))


if __name__ == "__main__":
    main()


--- dev/format.py ---
import os
import re
import subprocess
import sys

RUFF_FORMAT = [sys.executable, "-m", "ruff", "format"]
MESSAGE_REGEX = re.compile(r"^Would reformat: (.+)$")


def transform(stdout: str, is_maintainer: bool) -> str:
    if not stdout:
        return stdout
    transformed = []
    for line in stdout.splitlines():
        if m := MESSAGE_REGEX.match(line):
            path = m.group(1)
            command = (
                "`ruff format .` or comment `/autoformat`" if is_maintainer else "`ruff format .`"
            )
            # As a workaround for https://github.com/orgs/community/discussions/165826,
            # add fake line:column numbers (1:1)
            line = f"{path}:1:1: Unformatted file. Run {command} to format."

        transformed.append(line)
    return "\n".join(transformed) + "\n"


def main():
    if "NO_FIX" in os.environ:
        with subprocess.Popen(
            [
                *RUFF_FORMAT,
                "--check",
                *sys.argv[1:],
            ],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
        ) as prc:
            stdout, stderr = prc.communicate()
            is_maintainer = os.environ.get("IS_MAINTAINER", "false").lower() == "true"
            sys.stdout.write(transform(stdout, is_maintainer))
            sys.stderr.write(stderr)
            sys.exit(prc.returncode)
    else:
        with subprocess.Popen(
            [
                *RUFF_FORMAT,
                *sys.argv[1:],
            ]
        ) as prc:
            prc.communicate()
            sys.exit(prc.returncode)


if __name__ == "__main__":
    main()


--- dev/generate_protos.py ---
import platform
import shutil
import subprocess
import tempfile
import textwrap
import urllib.request
import zipfile
from pathlib import Path
from typing import Literal

SYSTEM = platform.system()
MACHINE = platform.machine()
CACHE_DIR = Path(".cache/protobuf_cache")
MLFLOW_PROTOS_DIR = Path("mlflow/protos")
TEST_PROTOS_DIR = Path("tests/protos")


def gen_protos(
    proto_dir: Path,
    proto_files: list[Path],
    lang: Literal["python", "java"],
    protoc_bin: Path,
    protoc_include_paths: list[Path],
    out_dir: Path,
) -> None:
    assert lang in ["python", "java"]
    out_dir.mkdir(parents=True, exist_ok=True)

    include_args = []
    for include_path in protoc_include_paths:
        include_args.append(f"-I={include_path}")

    subprocess.check_call(
        [
            protoc_bin,
            *include_args,
            f"-I={proto_dir}",
            f"--{lang}_out={out_dir}",
            *[proto_dir / pf for pf in proto_files],
        ]
    )


def gen_stub_files(
    proto_dir: Path,
    proto_files: list[Path],
    protoc_bin: Path,
    protoc_include_paths: list[Path],
    out_dir: Path,
) -> None:
    include_args = []
    for include_path in protoc_include_paths:
        include_args.append(f"-I={include_path}")

    subprocess.check_call(
        [
            protoc_bin,
            *include_args,
            f"-I={proto_dir}",
            f"--pyi_out={out_dir}",
            *[proto_dir / pf for pf in proto_files],
        ]
    )


def apply_python_gencode_replacement(file_path: Path) -> None:
    content = file_path.read_text()

    for old, new in python_gencode_replacements:
        content = content.replace(old, new)

    file_path.write_text(content, encoding="UTF-8")


def _get_python_output_path(proto_file_path: Path) -> Path:
    return proto_file_path.parent / (proto_file_path.stem + "_pb2.py")


def to_paths(*args: str) -> list[Path]:
    return list(map(Path, args))


basic_proto_files = to_paths(
    "databricks.proto",
    "service.proto",
    "model_registry.proto",
    "databricks_artifacts.proto",
    "mlflow_artifacts.proto",
    "internal.proto",
    "scalapb/scalapb.proto",
    "assessments.proto",
    "datasets.proto",
    "webhooks.proto",
)
uc_proto_files = to_paths(
    "databricks_managed_catalog_messages.proto",
    "databricks_managed_catalog_service.proto",
    "databricks_uc_registry_messages.proto",
    "databricks_uc_registry_service.proto",
    "databricks_filesystem_service.proto",
    "unity_catalog_oss_messages.proto",
    "unity_catalog_oss_service.proto",
    "unity_catalog_prompt_messages.proto",
    "unity_catalog_prompt_service.proto",
)
tracing_proto_files = to_paths("databricks_trace_server.proto", "databricks_tracing.proto")
facet_proto_files = to_paths("facet_feature_statistics.proto")
python_proto_files = basic_proto_files + uc_proto_files + facet_proto_files + tracing_proto_files
test_proto_files = to_paths("test_message.proto")


python_gencode_replacements = [
    (
        "from scalapb import scalapb_pb2 as scalapb_dot_scalapb__pb2",
        "from .scalapb import scalapb_pb2 as scalapb_dot_scalapb__pb2",
    ),
    (
        "import databricks_pb2 as databricks__pb2",
        "from . import databricks_pb2 as databricks__pb2",
    ),
    (
        "import databricks_uc_registry_messages_pb2 as databricks__uc__registry__messages__pb2",
        "from . import databricks_uc_registry_messages_pb2 as databricks_uc_registry_messages_pb2",
    ),
    (
        "import databricks_managed_catalog_messages_pb2 as databricks__managed__catalog__"
        "messages__pb2",
        "from . import databricks_managed_catalog_messages_pb2 as databricks_managed_"
        "catalog_messages_pb2",
    ),
    (
        "import unity_catalog_oss_messages_pb2 as unity__catalog__oss__messages__pb2",
        "from . import unity_catalog_oss_messages_pb2 as unity_catalog_oss_messages_pb2",
    ),
    (
        "import unity_catalog_prompt_messages_pb2 as unity__catalog__prompt__messages__pb2",
        "from . import unity_catalog_prompt_messages_pb2 as unity_catalog_prompt_messages_pb2",
    ),
    (
        "import service_pb2 as service__pb2",
        "from . import service_pb2 as service__pb2",
    ),
    (
        "import assessments_pb2 as assessments__pb2",
        "from . import assessments_pb2 as assessments__pb2",
    ),
    (
        "import datasets_pb2 as datasets__pb2",
        "from . import datasets_pb2 as datasets__pb2",
    ),
    (
        "import webhooks_pb2 as webhooks__pb2",
        "from . import webhooks_pb2 as webhooks__pb2",
    ),
]


def gen_python_protos(protoc_bin: Path, protoc_include_paths: list[Path], out_dir: Path) -> None:
    gen_protos(
        MLFLOW_PROTOS_DIR,
        python_proto_files,
        "python",
        protoc_bin,
        protoc_include_paths,
        out_dir,
    )

    gen_protos(
        TEST_PROTOS_DIR,
        test_proto_files,
        "python",
        protoc_bin,
        protoc_include_paths,
        out_dir,
    )

    for proto_file in python_proto_files:
        apply_python_gencode_replacement(out_dir / _get_python_output_path(proto_file))


def download_file(url: str, output_path: Path) -> None:
    urllib.request.urlretrieve(url, output_path)


def download_opentelemetry_protos(version: str = "v1.7.0") -> Path:
    """
    Download OpenTelemetry proto files from GitHub.
    Returns the path to the opentelemetry-proto directory.
    """
    otel_proto_dir = CACHE_DIR / f"opentelemetry-proto-{version}"

    if not otel_proto_dir.exists():
        print(f"Downloading OpenTelemetry proto files {version}...")
        with tempfile.TemporaryDirectory() as tmpdir:
            zip_path = Path(tmpdir) / "otel-proto.zip"
            download_file(
                f"https://github.com/open-telemetry/opentelemetry-proto/archive/refs/tags/{version}.zip",
                zip_path,
            )
            with zipfile.ZipFile(zip_path, "r") as zip_ref:
                zip_ref.extractall(tmpdir)

            # Move the extracted directory to cache
            extracted_dir = Path(tmpdir) / f"opentelemetry-proto-{version[1:]}"  # Remove 'v' prefix
            shutil.move(str(extracted_dir), str(otel_proto_dir))

    return otel_proto_dir


def download_and_extract_protoc(version: Literal["3.19.4", "26.0"]) -> tuple[Path, Path]:
    """
    Download and extract specific version protoc tool for Linux systems,
    return extracted protoc executable file path and include path.
    """
    assert SYSTEM == "Linux", "This script only supports Linux systems."
    assert MACHINE in ["x86_64", "aarch64"], (
        "This script only supports x86_64 or aarch64 CPU architectures."
    )

    cpu_type = "x86_64" if MACHINE == "x86_64" else "aarch_64"
    protoc_zip_filename = f"protoc-{version}-linux-{cpu_type}.zip"

    downloaded_protoc_bin = CACHE_DIR / f"protoc-{version}" / "bin" / "protoc"
    downloaded_protoc_include_path = CACHE_DIR / f"protoc-{version}" / "include"
    if not (downloaded_protoc_bin.is_file() and downloaded_protoc_include_path.is_dir()):
        with tempfile.TemporaryDirectory() as t:
            zip_path = Path(t) / protoc_zip_filename
            download_file(
                f"https://github.com/protocolbuffers/protobuf/releases/download/v{version}/{protoc_zip_filename}",
                zip_path,
            )
            with zipfile.ZipFile(zip_path, "r") as zip_ref:
                zip_ref.extractall(CACHE_DIR / f"protoc-{version}")

        # Make protoc executable
        downloaded_protoc_bin.chmod(0o755)
    return downloaded_protoc_bin, downloaded_protoc_include_path


def generate_final_python_gencode(
    gencode3194_path: Path, gencode5260_path: Path, out_path: Path
) -> None:
    gencode3194 = gencode3194_path.read_text()
    gencode5260 = gencode5260_path.read_text()

    merged_code = f"""
import google.protobuf
from packaging.version import Version
if Version(google.protobuf.__version__).major >= 5:
{textwrap.indent(gencode5260, "  ")}
else:
{textwrap.indent(gencode3194, "  ")}
"""
    out_path.write_text(merged_code, encoding="UTF-8")


def main() -> None:
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    with tempfile.TemporaryDirectory() as temp_gencode_dir:
        temp_gencode_path = Path(temp_gencode_dir)
        proto3194_out = temp_gencode_path / "3.19.4"
        proto5260_out = temp_gencode_path / "26.0"
        proto3194_out.mkdir(exist_ok=True)
        proto5260_out.mkdir(exist_ok=True)

        protoc3194, protoc3194_include = download_and_extract_protoc("3.19.4")
        protoc5260, protoc5260_include = download_and_extract_protoc("26.0")

        # Download OpenTelemetry proto files
        otel_proto_dir = download_opentelemetry_protos()

        # Build include paths list
        protoc3194_includes = [protoc3194_include, otel_proto_dir]
        protoc5260_includes = [protoc5260_include, otel_proto_dir]

        gen_python_protos(protoc3194, protoc3194_includes, proto3194_out)
        gen_python_protos(protoc5260, protoc5260_includes, proto5260_out)

        for proto_files, protos_dir in [
            (python_proto_files, MLFLOW_PROTOS_DIR),
            (test_proto_files, TEST_PROTOS_DIR),
        ]:
            for proto_file in proto_files:
                gencode_path = _get_python_output_path(proto_file)

                generate_final_python_gencode(
                    proto3194_out / gencode_path,
                    proto5260_out / gencode_path,
                    protos_dir / gencode_path,
                )

    # generate java gencode using pinned protoc 3.19.4 version.
    gen_protos(
        MLFLOW_PROTOS_DIR,
        basic_proto_files,
        "java",
        protoc3194,
        protoc3194_includes,
        Path("mlflow/java/client/src/main/java"),
    )

    gen_stub_files(
        MLFLOW_PROTOS_DIR,
        python_proto_files,
        protoc5260,
        protoc5260_includes,
        Path("mlflow/protos/"),
    )


if __name__ == "__main__":
    main()


--- libs/skinny/README.md ---
# MLflow Skinny

`mlflow-skinny` a lightweight version of MLflow that is designed to be used in environments where you want to minimize the size of the package.

## Core Files

| File               | Description                                                                     |
| ------------------ | ------------------------------------------------------------------------------- |
| `mlflow`           | A symlink that points to the `mlflow` directory in the root of the repository.  |
| `pyproject.toml`   | The package metadata. Autogenerate by [`dev/pyproject.py`](../dev/pyproject.py) |
| `README_SKINNY.md` | The package description. Autogenerate by [`dev/skinny.py`](../dev/pyproject.py) |

## Installation

```sh
# If you have a local clone of the repository
pip install ./libs/skinny

# If you want to install the latest version from GitHub
pip install git+https://github.com/mlflow/mlflow.git#subdirectory=libs/skinny
```


--- libs/tracing/README.md ---
# MLflow Tracing: An Open-Source SDK for Observability and Monitoring GenAI Applications🔍

[![Latest Docs](https://img.shields.io/badge/docs-latest-success.svg?style=for-the-badge)](https://mlflow.org/docs/latest/index.html)
[![Apache 2 License](https://img.shields.io/badge/license-Apache%202-brightgreen.svg?style=for-the-badge&logo=apache)](https://github.com/mlflow/mlflow/blob/master/LICENSE.txt)
[![Slack](https://img.shields.io/badge/slack-@mlflow--users-CF0E5B.svg?logo=slack&logoColor=white&labelColor=3F0E40&style=for-the-badge)](https://mlflow.org/community/#slack)
[![Twitter](https://img.shields.io/twitter/follow/MLflow?style=for-the-badge&labelColor=00ACEE&logo=twitter&logoColor=white)](https://twitter.com/MLflow)

MLflow Tracing (`mlflow-tracing`) is an open-source, lightweight Python package that only includes the minimum set of dependencies and functionality
to instrument your code/models/agents with [MLflow Tracing Feature](https://mlflow.org/docs/latest/tracing). It is designed to be a perfect fit for production environments where you want:

- **⚡️ Faster Deployment**: The package size and dependencies are significantly smaller than the full MLflow package, allowing for faster deployment times in dynamic environments such as Docker containers, serverless functions, and cloud-based applications.
- **🔧 Simplified Dependency Management**: A smaller set of dependencies means less work keeping up with dependency updates, security patches, and breaking changes from upstream libraries.
- **📦 Portability**: With the less number of dependencies, MLflow Tracing can be easily deployed across different environments and platforms, without worrying about compatibility issues.
- **🔒 Fewer Security Risks**: Each dependency potentially introduces security vulnerabilities. By reducing the number of dependencies, MLflow Tracing minimizes the attack surface and reduces the risk of security breaches.

## ✨ Features

- [Automatic Tracing](https://mlflow.org/docs/latest/tracing/integrations/) for AI libraries (OpenAI, LangChain, DSPy, Anthropic, etc...). Follow the link for the full list of supported libraries.
- [Manual instrumentation APIs](https://mlflow.org/docs/latest/tracing/api/manual-instrumentation) such as `@trace` decorator.
- [Production Monitoring](https://mlflow.org/docs/latest/tracing/production)
- Other tracing APIs such as `mlflow.set_trace_tag`, `mlflow.search_traces`, etc.

## 🌐 Choose Backend

The MLflow Trace package is designed to work with a remote hosted MLflow server as a backend. This allows you to log your traces to a central location, making it easier to manage and analyze your traces. There are several different options for hosting your MLflow server, including:

- [Databricks](https://docs.databricks.com/machine-learning/mlflow/managed-mlflow.html) - Databricks offers a FREE, fully managed MLflow server as a part of their platform. This is the easiest way to get started with MLflow tracing, without having to set up any infrastructure.
- [Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/) - MLflow on Amazon SageMaker is a fully managed service offered as part of the SageMaker platform by AWS, including tracing and other MLflow features such as model registry.
- [Nebius](https://nebius.com/) - Nebius, a cutting-edge cloud platform for GenAI explorers, offers a fully managed MLflow server.
- [Self-hosting](https://mlflow.org/docs/latest/tracking) - MLflow is a fully open-source project, allowing you to self-host your own MLflow server and keep your data private. This is a great option if you want to have full control over your data and infrastructure.

## 🚀 Getting Started

### Installation

To install the MLflow Python package, run the following command:

```bash
pip install mlflow-tracing
```

To install from the source code, run the following command:

```bash
pip install git+https://github.com/mlflow/mlflow.git#subdirectory=libs/tracing
```

> **NOTE:** It is **not** recommended to co-install this package with the full MLflow package together, as it may cause version mismatches issues.

### Connect to the MLflow Server

To connect to your MLflow server to log your traces, set the `MLFLOW_TRACKING_URI` environment variable or use the `mlflow.set_tracking_uri` function:

```python
import mlflow

mlflow.set_tracking_uri("databricks")
# Specify the experiment to log the traces to
mlflow.set_experiment("/Path/To/Experiment")
```

### Start Logging Traces

```python
import openai

client = openai.OpenAI(api_key="<your-api-key>")

# Enable auto-tracing for OpenAI
mlflow.openai.autolog()

# Call the OpenAI API as usual
response = client.chat.completions.create(
    model="gpt-4.1-mini",
    messages=[{"role": "user", "content": "Hello, how are you?"}],
)
```

## 📘 Documentation

Official documentation for MLflow Tracing can be found at [here](https://mlflow.org/docs/latest/tracing).

## 🛑 Features _Not_ Included

The following MLflow features are not included in this package.

- MLflow tracking server and UI.
- MLflow's other tracking capabilities such as Runs, Model Registry, Projects, etc.
- Evaluate models/agents and log evaluation results.

To leverage the full feature set of MLflow, install the full package by running `pip install mlflow`.


--- libs/typescript/README.md ---
<h1 align="center" style="border-bottom: none">
    <div>
        <a href="https://mlflow.org/"><picture>
            <img alt="MLflow Logo" src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/logo.svg" width="200" />
        </picture></a>
        <br>
        MLflow TypeScript SDK
    </div>
</h1>
<h2 align="center" style="border-bottom: none"></h2>

<p align="center">
  <a href="https://github.com/mlflow/mlflow"><img src="https://img.shields.io/github/stars/mlflow/mlflow?style=social" alt="stars"></a>
  <a href="https://www.npmjs.com/package/mlflow-tracing"><img src="https://img.shields.io/npm/v/mlflow-tracing.svg" alt="version"></a>
  <a href="https://www.npmjs.com/package/mlflow-tracing"><img src="https://img.shields.io/npm/dt/mlflow-tracing.svg" alt="downloads"></a>
  <a href="https://github.com/mlflow/mlflow/blob/main/LICENSE"><img src="https://img.shields.io/github/license/mlflow/mlflow" alt="license"></a>
</p>

MLflow Typescript SDK is a variant of the [MLflow Python SDK](https://github.com/mlflow/mlflow) that provides a TypeScript API for MLflow.

> [!IMPORTANT]
> MLflow Typescript SDK is catching up with the Python SDK. Currently only support [Tracing]() and [Feedback Collection]() features. Please raise an issue in Github if you need a feature that is not supported.

## Packages

| Package                                | NPM                                                                                                                                         | Description                                                |
| -------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- |
| [mlflow-tracing](./core)               | [![npm package](https://img.shields.io/npm/v/mlflow-tracing?style=flat-square)](https://www.npmjs.com/package/mlflow-tracing)               | The core tracing functionality and manual instrumentation. |
| [mlflow-openai](./integrations/openai) | [![npm package](https://img.shields.io/npm/v/mlflow-tracing-openai?style=flat-square)](https://www.npmjs.com/package/mlflow-tracing-openai) | Auto-instrumentation integration for OpenAI.               |

## Installation

```bash
npm install mlflow-tracing
```

> [!NOTE]
> MLflow Typescript SDK requires Node.js 20 or higher.

## Quickstart

Start MLflow Tracking Server if you don't have one already:

```bash
pip install mlflow
mlflow server --backend-store-uri sqlite:///mlruns.db --port 5000
```

Self-hosting MLflow server requires Python 3.10 or higher. If you don't have one, you can also use [managed MLflow service](https://mlflow.org/#get-started) for free to get started quickly.

Instantiate MLflow SDK in your application:

```typescript
import * as mlflow from 'mlflow-tracing';

mlflow.init({
  trackingUri: 'http://localhost:5000',
  experimentId: '<experiment-id>'
});
```

### Configure with environment variables

The SDK can also read configuration from environment variables so you can avoid
hard-coding connection details. If `MLFLOW_TRACKING_URI` and
`MLFLOW_EXPERIMENT_ID` are set, you can initialize the client without passing
any arguments:

```bash
export MLFLOW_TRACKING_URI=http://localhost:5000
export MLFLOW_EXPERIMENT_ID=123456789
```

```typescript
import * as mlflow from 'mlflow-tracing';

mlflow.init(); // Uses the values from the environment
```

Create a trace:

```typescript
// Wrap a function with mlflow.trace to generate a span when the function is called.
// MLflow will automatically record the function name, arguments, return value,
// latency, and exception information to the span.
const getWeather = mlflow.trace(
  (city: string) => {
    return `The weather in ${city} is sunny`;
  },
  // Pass options to set span name. See https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk
  // for the full list of options.
  { name: 'get-weather' }
);
getWeather('San Francisco');

// Alternatively, start and end span manually
const span = mlflow.startSpan({ name: 'my-span' });
span.end();
```

View traces in MLflow UI:

![MLflow Tracing UI](https://github.com/mlflow/mlflow/blob/891fed9a746477f808dd2b82d3abb2382293c564/docs/static/images/llms/tracing/quickstart/openai-tool-calling-trace-detail.png?raw=true)

## Adding New Integrations

The TypeScript SDK supports pluggable auto-instrumentation packages under [`integrations/`](./integrations). To add a new integration:

1. Create a new workspace package (for example, `integrations/<provider>`), modeled after the [OpenAI integration](./integrations/openai).
2. Implement the instrumentation entry points in `src/`, exporting a `register()` helper that configures tracing for the target client library.
3. Add package metadata (`package.json`, `tsconfig.json`, and optional `README.md`) so the integration can be built and published.
4. Add unit and/or integration tests under `tests/` that exercise the new instrumentation.
5. Update the root [`package.json`](./package.json) `build:integrations` and `test:integrations` scripts if your package requires additional build or test commands.

Once your integration package is ready, run the local workflow outlined in [Running the SDK after changes](#running-the-sdk-after-changes) and open a pull request that describes the new provider support.

## Contributing

We welcome contributions of new features, bug fixes, and documentation improvements. To contribute:

1. Review the project-wide [contribution guidelines](../../CONTRIBUTING.md) and follow the MLflow [Code of Conduct](../../CODE_OF_CONDUCT.rst).
2. Discuss larger proposals in a GitHub issue or the MLflow community channels before investing significant effort.
3. Fork the repository (or use a feature branch) and make your changes with clear, well-structured commits.
4. Ensure your code includes tests and documentation updates where appropriate.
5. Submit a pull request that summarizes the motivation, implementation details, and validation steps. The MLflow team will review and provide feedback.

## Running the SDK after Changes

The TypeScript workspace uses npm workspaces. After modifying the core SDK or any integration:

```bash
npm install        # Install or update workspace dependencies
npm run build      # Build the core package and all integrations
npm run test       # Execute the test suites for the core SDK and integrations
```

You can run package-specific scripts from their respective directories (for example, `cd core && npm run test`) when iterating on a particular feature. Remember to rebuild before consuming the SDK from another project so that the latest TypeScript output is emitted to `dist/`.

## Trace Usage

MLflow Tracing empowers you throughout the end-to-end lifecycle of your application. Here's how it helps you at each step of the workflow, click on each section to learn more:

<details>
<summary><strong>🔍 Build & Debug</strong></summary>

<table>
<tr>
<td width="60%">

#### Smooth Debugging Experience

MLflow's tracing capabilities provide deep insights into what happens beneath the abstractions of your application, helping you precisely identify where issues occur.

[Learn more →](https://mlflow.org/docs/latest/genai/tracing/observe-with-traces)

</td>
<td width="40%">

![Trace Debug](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-trace-debug.png)

</td>
</tr>
</table>

</details>

<details>
<summary><strong>💬 Human Feedback</strong></summary>

<table>
<tr>
<td width="60%">

#### Track Annotation and User Feedback Attached to Traces

Collecting and managing feedback is essential for improving your application. MLflow Tracing allows you to attach user feedback and annotations directly to traces, creating a rich dataset for analysis.

This feedback data helps you understand user satisfaction, identify areas for improvement, and build better evaluation datasets based on real user interactions.

[Learn more →](https://mlflow.org/docs/latest/genai/tracing/collect-user-feedback)

</td>
<td width="40%">

![Human Feedback](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-human-feedback.png)

</td>
</tr>
</table>

</details>

<details>
<summary><strong>📊 Evaluation</strong></summary>

<table>
<tr>
<td width="60%">

#### Systematic Quality Assessment Throughout Your Application

Evaluating the performance of your application is crucial, but creating a reliable evaluation process can be challenging. Traces serve as a rich data source, helping you assess quality with precise metrics for all components.

When combined with MLflow's evaluation capabilities, you get a seamless experience for assessing and improving your application's performance.

[Learn more →](https://mlflow.org/docs/latest/genai/eval-monitor)

</td>
<td width="40%">

![Evaluation](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-trace-evaluation.png)

</td>
</tr>
</table>

</details>

<details>
<summary><strong>🚀 Production Monitoring</strong></summary>

<table>
<tr>
<td width="60%">

#### Monitor Applications with Your Favorite Observability Stack

Machine learning projects don't end with the first launch. Continuous monitoring and incremental improvement are critical to long-term success.

Integrated with various observability platforms such as Databricks, Datadog, Grafana, and Prometheus, MLflow Tracing provides a comprehensive solution for monitoring your applications in production.

[Learn more →](https://mlflow.org/docs/latest/genai/tracing/prod-tracing)

</td>
<td width="40%">

![Monitoring](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-monitoring.png)

</td>
</tr>
</table>

</details>

<details>
<summary><strong>📦 Dataset Collection</strong></summary>

<table>
<tr>
<td width="60%">

#### Create High-Quality Evaluation Datasets from Production Traces

Traces from production are invaluable for building comprehensive evaluation datasets. By capturing real user interactions and their outcomes, you can create test cases that truly represent your application's usage patterns.

This comprehensive data capture enables you to create realistic test scenarios, validate model performance on actual usage patterns, and continuously improve your evaluation datasets.

[Learn more →](https://mlflow.org/docs/latest/genai/tracing/search-traces#creating-evaluation-datasets)

</td>
<td width="40%">

![Dataset Collection](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-trace-dataset.png)

</td>
</tr>
</table>

</details>

## Documentation 📘

Official documentation for MLflow Typescript SDK can be found [here](https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk).

## License

This project is licensed under the [Apache License 2.0](https://github.com/mlflow/mlflow/blob/master/LICENSE.txt).


--- libs/skinny/README_SKINNY.md ---
<!--  Autogenerated by dev/pyproject.py. Do not edit manually.  -->

📣 This is the `mlflow-skinny` package, a lightweight MLflow package without SQL storage, server, UI, or data science dependencies.
Additional dependencies can be installed to leverage the full feature set of MLflow. For example:

- To use the `mlflow.sklearn` component of MLflow Models, install `scikit-learn`, `numpy` and `pandas`.
- To use SQL-based metadata storage, install `sqlalchemy`, `alembic`, and `sqlparse`.
- To use serving-based features, install `flask` and `pandas`.

---

<br>
<br>

<h1 align="center" style="border-bottom: none">
    <a href="https://mlflow.org/">
        <img alt="MLflow logo" src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/logo.svg" width="200" />
    </a>
</h1>
<h2 align="center" style="border-bottom: none">Open-Source Platform for Productionizing AI</h2>

MLflow is an open-source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end **experiment tracking**, **observability**, and **evaluations**, all in one integrated platform.

<div align="center">

[![Python SDK](https://img.shields.io/pypi/v/mlflow)](https://pypi.org/project/mlflow/)
[![PyPI Downloads](https://img.shields.io/pypi/dm/mlflow)](https://pepy.tech/projects/mlflow)
[![License](https://img.shields.io/github/license/mlflow/mlflow)](https://github.com/mlflow/mlflow/blob/main/LICENSE)
<a href="https://twitter.com/intent/follow?screen_name=mlflow" target="_blank">
<img src="https://img.shields.io/twitter/follow/mlflow?logo=X&color=%20%23f5f5f5"
      alt="follow on X(Twitter)"></a>
<a href="https://www.linkedin.com/company/mlflow-org/" target="_blank">
<img src="https://custom-icon-badges.demolab.com/badge/LinkedIn-0A66C2?logo=linkedin-white&logoColor=fff"
      alt="follow on LinkedIn"></a>
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/mlflow/mlflow)

</div>

<div align="center">
   <div>
      <a href="https://mlflow.org/"><strong>Website</strong></a> ·
      <a href="https://mlflow.org/docs/latest"><strong>Docs</strong></a> ·
      <a href="https://github.com/mlflow/mlflow/issues/new/choose"><strong>Feature Request</strong></a> ·
      <a href="https://mlflow.org/blog"><strong>News</strong></a> ·
      <a href="https://www.youtube.com/@mlflowoss"><strong>YouTube</strong></a> ·
      <a href="https://lu.ma/mlflow?k=c"><strong>Events</strong></a>
   </div>
</div>

<br>

## 🚀 Installation

To install the MLflow Python package, run the following command:

```
pip install mlflow
```

## 📦 Core Components

MLflow is **the only platform that provides a unified solution for all your AI/ML needs**, including LLMs, Agents, Deep Learning, and traditional machine learning.

### 💡 For LLM / GenAI Developers

<table>
  <tr>
    <td>
    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-tracing.png" alt="Tracing" width=100%>
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/llms/tracing/index.html"><strong>🔍 Tracing / Observability</strong></a>
        <br><br>
        <div>Trace the internal states of your LLM/agentic applications for debugging quality issues and monitoring performance with ease.</div><br>
        <a href="https://mlflow.org/docs/latest/genai/tracing/quickstart/python-openai/">Getting Started →</a>
        <br><br>
    </div>
    </td>
    <td>
    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-llm-eval.png" alt="LLM Evaluation" width=100%>
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/"><strong>📊 LLM Evaluation</strong></a>
        <br><br>
        <div>A suite of automated model evaluation tools, seamlessly integrated with experiment tracking to compare across multiple versions.</div><br>
        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/">Getting Started →</a>
        <br><br>
    </div>
    </td>
  </tr>
  <tr>
    <td>
      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-prompt.png" alt="Prompt Management">
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/prompt-registry/"><strong>🤖 Prompt Management</strong></a>
        <br><br>
        <div>Version, track, and reuse prompts across your organization, helping maintain consistency and improve collaboration in prompt development.</div><br>
        <a href="https://mlflow.org/docs/latest/genai/prompt-registry/create-and-edit-prompts/">Getting Started →</a>
        <br><br>
    </div>
    </td>
    <td>
      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-logged-model.png" alt="MLflow Hero">
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/version-tracking/"><strong>📦 App Version Tracking</strong></a>
        <br><br>
        <div>MLflow keeps track of many moving parts in your AI applications, such as models, prompts, tools, and code, with end-to-end lineage.</div><br>
        <a href="https://mlflow.org/docs/latest/genai/version-tracking/quickstart/">Getting Started →</a>
        <br><br>
    </div>
    </td>
  </tr>
</table>

### 🎓 For Data Scientists

<table>
  <tr>
    <td colspan="2" align="center" >
      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-experiment.png" alt="Tracking" width=50%>
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/ml/tracking/"><strong>📝 Experiment Tracking</strong></a>
        <br><br>
        <div>Track your models, parameters, metrics, and evaluation results in ML experiments and compare them using an interactive UI.</div><br>
        <a href="https://mlflow.org/docs/latest/ml/tracking/quickstart/">Getting Started →</a>
        <br><br>
    </div>
    </td>
  </tr>
  <tr>
    <td>
      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-model-registry.png" alt="Model Registry" width=100%>
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/ml/model-registry/"><strong>💾 Model Registry</strong></a>
        <br><br>
        <div> A centralized model store designed to collaboratively manage the full lifecycle and deployment of machine learning models.</div><br>
        <a href="https://mlflow.org/docs/latest/ml/model-registry/tutorial/">Getting Started →</a>
        <br><br>
    </div>
    </td>
    <td>
      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-deployment.png" alt="Deployment" width=100%>
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/ml/deployment/"><strong>🚀 Deployment</strong></a>
        <br><br>
        <div> Tools for seamless model deployment to batch and real-time scoring on platforms like Docker, Kubernetes, Azure ML, and AWS SageMaker.</div><br>
        <a href="https://mlflow.org/docs/latest/ml/deployment/">Getting Started →</a>
        <br><br>
    </div>
    </td>
  </tr>
</table>

## 🌐 Hosting MLflow Anywhere

<div align="center" >
  <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-providers.png" alt="Providers" width=100%>
</div>

You can run MLflow in many different environments, including local machines, on-premise servers, and cloud infrastructure.

Trusted by thousands of organizations, MLflow is now offered as a managed service by most major cloud providers:

- [Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/)
- [Azure ML](https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow?view=azureml-api-2)
- [Databricks](https://www.databricks.com/product/managed-mlflow)
- [Nebius](https://nebius.com/services/managed-mlflow)

For hosting MLflow on your own infrastructure, please refer to [this guidance](https://mlflow.org/docs/latest/ml/tracking/#tracking-setup).

## 🗣️ Supported Programming Languages

- [Python](https://pypi.org/project/mlflow/)
- [TypeScript / JavaScript](https://www.npmjs.com/package/mlflow-tracing)
- [Java](https://mvnrepository.com/artifact/org.mlflow/mlflow-client)
- [R](https://cran.r-project.org/web/packages/mlflow/readme/README.html)

## 🔗 Integrations

MLflow is natively integrated with many popular machine learning frameworks and GenAI libraries.

![Integrations](https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-integrations.png)

## Usage Examples

### Experiment Tracking ([Doc](https://mlflow.org/docs/latest/ml/tracking/))

The following examples trains a simple regression model with scikit-learn, while enabling MLflow's [autologging](https://mlflow.org/docs/latest/tracking/autolog.html) feature for experiment tracking.

```python
import mlflow

from sklearn.model_selection import train_test_split
from sklearn.datasets import load_diabetes
from sklearn.ensemble import RandomForestRegressor

# Enable MLflow's automatic experiment tracking for scikit-learn
mlflow.sklearn.autolog()

# Load the training dataset
db = load_diabetes()
X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)

rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)
# MLflow triggers logging automatically upon model fitting
rf.fit(X_train, y_train)
```

Once the above code finishes, run the following command in a separate terminal and access the MLflow UI via the printed URL. An MLflow **Run** should be automatically created, which tracks the training dataset, hyper parameters, performance metrics, the trained model, dependencies, and even more.

```
mlflow ui
```

### Evaluating Models ([Doc](https://mlflow.org/docs/latest/model-evaluation/index.html))

The following example runs automatic evaluation for question-answering tasks with several built-in metrics.

```python
import mlflow
import pandas as pd

# Evaluation set contains (1) input question (2) model outputs (3) ground truth
df = pd.DataFrame(
    {
        "inputs": ["What is MLflow?", "What is Spark?"],
        "outputs": [
            "MLflow is an innovative fully self-driving airship powered by AI.",
            "Sparks is an American pop and rock duo formed in Los Angeles.",
        ],
        "ground_truth": [
            "MLflow is an open-source platform for productionizing AI.",
            "Apache Spark is an open-source, distributed computing system.",
        ],
    }
)
eval_dataset = mlflow.data.from_pandas(
    df, predictions="outputs", targets="ground_truth"
)

# Start an MLflow Run to record the evaluation results to
with mlflow.start_run(run_name="evaluate_qa"):
    # Run automatic evaluation with a set of built-in metrics for question-answering models
    results = mlflow.evaluate(
        data=eval_dataset,
        model_type="question-answering",
    )

print(results.tables["eval_results_table"])
```

### Observability ([Doc](https://mlflow.org/docs/latest/llms/tracing/index.html))

MLflow Tracing provides LLM observability for various GenAI libraries such as OpenAI, LangChain, LlamaIndex, DSPy, AutoGen, and more. To enable auto-tracing, call `mlflow.xyz.autolog()` before running your models. Refer to the documentation for customization and manual instrumentation.

```python
import mlflow
from openai import OpenAI

# Enable tracing for OpenAI
mlflow.openai.autolog()

# Query OpenAI LLM normally
response = OpenAI().chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hi!"}],
    temperature=0.1,
)
```

Then navigate to the "Traces" tab in the MLflow UI to find the trace records OpenAI query.

## 💭 Support

- For help or questions about MLflow usage (e.g. "how do I do X?") visit the [documentation](https://mlflow.org/docs/latest).
- In the documentation, you can ask the question to our AI-powered chat bot. Click on the **"Ask AI"** button at the right bottom.
- Join the [virtual events](https://lu.ma/mlflow?k=c) like office hours and meetups.
- To report a bug, file a documentation issue, or submit a feature request, please [open a GitHub issue](https://github.com/mlflow/mlflow/issues/new/choose).
- For release announcements and other discussions, please subscribe to our mailing list (mlflow-users@googlegroups.com)
  or join us on [Slack](https://mlflow.org/slack).

## 🤝 Contributing

We happily welcome contributions to MLflow!

- Submit [bug reports](https://github.com/mlflow/mlflow/issues/new?template=bug_report_template.yaml) and [feature requests](https://github.com/mlflow/mlflow/issues/new?template=feature_request_template.yaml)
- Contribute for [good-first-issues](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) and [help-wanted](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)
- Writing about MLflow and sharing your experience

Please see our [contribution guide](CONTRIBUTING.md) to learn more about contributing to MLflow.

## ⭐️ Star History

<a href="https://star-history.com/#mlflow/mlflow&Date">
 <picture>
   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date&theme=dark" />
   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />
   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />
 </picture>
</a>

## ✏️ Citation

If you use MLflow in your research, please cite it using the "Cite this repository" button at the top of the [GitHub repository page](https://github.com/mlflow/mlflow), which will provide you with citation formats including APA and BibTeX.

## 👥 Core Members

MLflow is currently maintained by the following core members with significant contributions from hundreds of exceptionally talented community members.

- [Ben Wilson](https://github.com/BenWilson2)
- [Corey Zumar](https://github.com/dbczumar)
- [Daniel Lok](https://github.com/daniellok-db)
- [Gabriel Fu](https://github.com/gabrielfu)
- [Harutaka Kawamura](https://github.com/harupy)
- [Serena Ruan](https://github.com/serena-ruan)
- [Tomu Hirata](https://github.com/TomeHirata)
- [Weichen Xu](https://github.com/WeichenXu123)
- [Yuki Watanabe](https://github.com/B-Step62)


--- libs/typescript/core/README.md ---
# MLflow Typescript SDK - Core

This is the core package of the [MLflow Typescript SDK](https://github.com/mlflow/mlflow/tree/main/libs/typescript). It is a skinny package that includes the core tracing functionality and manual instrumentation.

| Package              | NPM                                                                                                                           | Description                                                |
| -------------------- | ----------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- |
| [mlflow-tracing](./) | [![npm package](https://img.shields.io/npm/v/mlflow-tracing?style=flat-square)](https://www.npmjs.com/package/mlflow-tracing) | The core tracing functionality and manual instrumentation. |

## Installation

```bash
npm install mlflow-tracing
```

## Quickstart

Start MLflow Tracking Server if you don't have one already:

```bash
pip install mlflow
mlflow server --backend-store-uri sqlite:///mlruns.db --port 5000
```

Self-hosting MLflow server requires Python 3.10 or higher. If you don't have one, you can also use [managed MLflow service](https://mlflow.org/#get-started) for free to get started quickly.

Instantiate MLflow SDK in your application:

```typescript
import * as mlflow from 'mlflow-tracing';

mlflow.init({
  trackingUri: 'http://localhost:5000',
  experimentId: '<experiment-id>'
});
```

Create a trace:

```typescript
// Wrap a function with mlflow.trace to generate a span when the function is called.
// MLflow will automatically record the function name, arguments, return value,
// latency, and exception information to the span.
const getWeather = mlflow.trace(
  (city: string) => {
    return `The weather in ${city} is sunny`;
  },
  // Pass options to set span name. See https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk
  // for the full list of options.
  { name: 'get-weather' }
);
getWeather('San Francisco');

// Alternatively, start and end span manually
const span = mlflow.startSpan({ name: 'my-span' });
span.end();
```

## Documentation 📘

Official documentation for MLflow Typescript SDK can be found [here](https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk).

## License

This project is licensed under the [Apache License 2.0](https://github.com/mlflow/mlflow/blob/master/LICENSE.txt).


--- libs/typescript/integrations/openai/README.md ---
# MLflow Typescript SDK - OpenAI

Seamlessly integrate [MLflow Tracing](https://github.com/mlflow/mlflow/tree/main/libs/typescript) with OpenAI to automatically trace your OpenAI API calls.

| Package             | NPM                                                                                                                                         | Description                                  |
| ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| [mlflow-openai](./) | [![npm package](https://img.shields.io/npm/v/mlflow-tracing-openai?style=flat-square)](https://www.npmjs.com/package/mlflow-tracing-openai) | Auto-instrumentation integration for OpenAI. |

## Installation

```bash
npm install mlflow-openai
```

The package includes the [`mlflow-tracing`](https://github.com/mlflow/mlflow/tree/main/libs/typescript) package and `openai` package as peer dependencies. Depending on your package manager, you may need to install these two packages separately.

## Quickstart

Start MLflow Tracking Server if you don't have one already:

```bash
pip install mlflow
mlflow server --backend-store-uri sqlite:///mlruns.db --port 5000
```

Self-hosting MLflow server requires Python 3.10 or higher. If you don't have one, you can also use [managed MLflow service](https://mlflow.org/#get-started) for free to get started quickly.

Instantiate MLflow SDK in your application:

```typescript
import * as mlflow from 'mlflow-tracing';

mlflow.init({
  trackingUri: 'http://localhost:5000',
  experimentId: '<experiment-id>'
});
```

Create a trace:

```typescript
import { OpenAI } from 'openai';
import { tracedOpenAI } from 'mlflow-openai';

// Wrap the OpenAI client with the tracedOpenAI function
const client = tracedOpenAI(new OpenAI());

// Invoke the client as usual
const response = await client.chat.completions.create({
  model: 'o4-mini',
  messages: [
    { role: 'system', content: 'You are a helpful weather assistant.' },
    { role: 'user', content: "What's the weather like in Seattle?" }
  ]
});
```

View traces in MLflow UI:

![MLflow Tracing UI](https://github.com/mlflow/mlflow/blob/891fed9a746477f808dd2b82d3abb2382293c564/docs/static/images/llms/tracing/quickstart/single-openai-trace-detail.png?raw=true)

## Documentation 📘

Official documentation for MLflow Typescript SDK can be found [here](https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk).

## License

This project is licensed under the [Apache License 2.0](https://github.com/mlflow/mlflow/blob/master/LICENSE.txt).


--- libs/typescript/jest.global-server-setup.ts ---
import { spawn } from 'child_process';
import { tmpdir } from 'os';
import { mkdtempSync } from 'fs';
import { join } from 'path';
import { TEST_PORT, TEST_TRACKING_URI } from './core/tests/helper';

/**
 * Start MLflow Python server. This is necessary for testing Typescript SDK because
 * the SDK does not have a server implementation and talks to the Python server instead.
 */
module.exports = async () => {
  const tempDir = mkdtempSync(join(tmpdir(), 'mlflow-test-'));

  const mlflowRoot = join(__dirname, '../..'); // Use the local dev version

  // Only start a server if one is not already running
  try {
    const response = await fetch(TEST_TRACKING_URI);
    if (response.ok) {
      return;
    }
  } catch (error) {
    // Ignore error
  }

  // eslint-disable-next-line no-console
  console.log(`Starting MLflow server on port ${TEST_PORT}. This may take a few seconds...
      To speed up the test, you can manually start the server and keep it running during local development.`);

  const mlflowProcess = spawn(
    'uv',
    ['run', '--directory', mlflowRoot, 'mlflow', 'server', '--port', TEST_PORT.toString()],
    {
      cwd: tempDir,
      stdio: 'inherit',
      // Create a new process group so we can kill the entire group
      detached: true
    }
  );

  try {
    await waitForServer(TEST_PORT);
    // eslint-disable-next-line no-console
    console.log(`MLflow server is ready on port ${TEST_PORT}`);
  } catch (error) {
    console.error('Failed to start MLflow server:', error);
    throw error;
  }

  // Set global variables for cleanup in jest.global-teardown.ts
  const globals = globalThis as any;
  globals.mlflowProcess = mlflowProcess;
  globals.tempDir = tempDir;
};

async function waitForServer(maxAttempts: number = 30): Promise<void> {
  for (let i = 0; i < maxAttempts; i++) {
    try {
      const response = await fetch(TEST_TRACKING_URI);
      if (response.ok) {
        return;
      }
    } catch (error) {
      // Ignore error
    }
    await new Promise((resolve) => setTimeout(resolve, 1000));
  }
  throw new Error('Failed to start MLflow server');
}


--- libs/typescript/jest.global-server-teardown.ts ---
import { rmSync } from 'fs';
import { ChildProcess } from 'child_process';

module.exports = async () => {
  const globals = globalThis as any;
  const mlflowProcess = globals.mlflowProcess as ChildProcess;
  const tempDir = globals.tempDir as string;

  if (mlflowProcess) {
    // Kill the process group to ensure worker processes spawned by uvicorn are terminated
    process.kill(-mlflowProcess.pid!, 'SIGTERM');

    // Wait for 1 second to ensure the process is terminated
    await new Promise((resolve) => setTimeout(resolve, 1000));
  }
  if (tempDir) {
    rmSync(tempDir, { recursive: true, force: true });
  }
};


--- libs/typescript/core/jest.config.js ---
/** @type {import('ts-jest').JestConfigWithTsJest} */
module.exports = {
  preset: 'ts-jest',
  testEnvironment: 'node',
  roots: ['<rootDir>/tests', '<rootDir>/src'],
  testMatch: ['**/*.test.ts'],
  moduleFileExtensions: ['ts', 'js', 'json', 'node'],
  transform: {
    '^.+\\.tsx?$': ['ts-jest', { tsconfig: 'tsconfig.json' }]
  },
  globalSetup: '<rootDir>/../jest.global-server-setup.ts',
  globalTeardown: '<rootDir>/../jest.global-server-teardown.ts',
  testTimeout: 30000,
  forceExit: true,
  detectOpenHandles: true
};


--- libs/typescript/core/tests/helper.ts ---
import { LiveSpan } from '../src/core/entities/span';
import { SpanType } from '../src/core/constants';

/**
 * Port and tracking URI for the local MLflow server used for testing.
 * If the server is not running, jest.global-setup.ts will start it.
 */
export const TEST_PORT = 5000;
export const TEST_TRACKING_URI = `http://localhost:${TEST_PORT}`;

/**
 * Mock OpenTelemetry span class for testing
 */
export class MockOtelSpan {
  name: string;
  attributes: Record<string, any>;
  spanId: string;
  traceId: string;

  constructor(
    name: string = 'test-span',
    spanId: string = 'test-span-id',
    traceId: string = 'test-trace-id'
  ) {
    this.name = name;
    this.spanId = spanId;
    this.traceId = traceId;
    this.attributes = {};
  }

  getAttribute(key: string): any {
    return this.attributes[key];
  }

  setAttribute(key: string, value: any): void {
    this.attributes[key] = value;
  }

  spanContext() {
    return {
      spanId: this.spanId,
      traceId: this.traceId
    };
  }
}

/**
 * Create a mock OpenTelemetry span with the given parameters
 */
export function createMockOtelSpan(
  name: string = 'test-span',
  spanId: string = 'test-span-id',
  traceId: string = 'test-trace-id'
): MockOtelSpan {
  return new MockOtelSpan(name, spanId, traceId);
}

/**
 * Create a test LiveSpan with mock OpenTelemetry span
 */
export function createTestSpan(
  name: string = 'test-span',
  traceId: string = 'test-trace-id',
  spanId: string = 'test-span-id',
  spanType: SpanType = SpanType.UNKNOWN
): LiveSpan {
  const mockOtelSpan = createMockOtelSpan(name, spanId, traceId);
  // eslint-disable-next-line @typescript-eslint/no-unsafe-argument
  return new LiveSpan(mockOtelSpan as any, traceId, spanType);
}


--- mlflow/claude_code/README.md ---
# MLflow Claude Code Integration

This module provides automatic tracing integration between Claude Code and MLflow.

## Module Structure

- **`config.py`** - Configuration management (settings files, environment variables)
- **`hooks.py`** - Claude Code hook setup and management
- **`cli.py`** - MLflow CLI commands (`mlflow autolog claude`)
- **`tracing.py`** - Core tracing logic and processors
- **`hooks/`** - Hook implementation handlers

## Installation

```bash
pip install mlflow
```

## Usage

Set up Claude Code tracing in any project directory:

```bash
# Set up tracing in current directory
mlflow autolog claude

# Set up tracing in specific directory
mlflow autolog claude ~/my-project

# Set up with custom tracking URI
mlflow autolog claude -u file://./custom-mlruns
mlflow autolog claude -u sqlite:///mlflow.db

# Set up with Databricks
mlflow autolog claude -u databricks -e 123456789

# Check status
mlflow autolog claude --status

# Disable tracing
mlflow autolog claude --disable
```

## How it Works

1. **Setup**: The `mlflow autolog claude` command configures Claude Code hooks in a `.claude/settings.json` file
2. **Automatic Tracing**: When you use the `claude` command in the configured directory, your conversations are automatically traced to MLflow
3. **View Traces**: Use `mlflow ui` to view your conversation traces

## Configuration

The setup creates two types of configuration:

### Claude Code Hooks

- **PostToolUse**: Captures tool usage during conversations
- **Stop**: Processes complete conversations into MLflow traces

### Environment Variables

- `MLFLOW_CLAUDE_TRACING_ENABLED=true`: Enables tracing
- `MLFLOW_TRACKING_URI`: Where to store traces (defaults to local `.claude/mlflow/runs`)
- `MLFLOW_EXPERIMENT_ID` or `MLFLOW_EXPERIMENT_NAME`: Which experiment to use

## Examples

### Basic Local Setup

```bash
mlflow autolog claude
cd .
claude "help me write a function"
mlflow ui --backend-store-uri sqlite:///mlflow.db
```

### Databricks Integration

```bash
mlflow autolog claude -u databricks -e 123456789
claude "analyze this data"
# View traces in Databricks
```

### Custom Project Setup

```bash
mlflow autolog claude ~/my-ai-project -u sqlite:///mlflow.db -n "My AI Project"
cd ~/my-ai-project
claude "refactor this code"
mlflow ui --backend-store-uri sqlite:///mlflow.db
```

## Troubleshooting

### Check Status

```bash
mlflow autolog claude --status
```

### Disable Tracing

```bash
mlflow autolog claude --disable
```

### View Raw Configuration

The configuration is stored in `.claude/settings.json`:

```bash
cat .claude/settings.json
```

## Requirements

- Python 3.10+ (required by MLflow)
- MLflow installed (`pip install mlflow`)
- Claude Code CLI installed


--- mlflow/R/mlflow/README.md ---
# mlflow: R interface for MLflow

[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/mlflow)](https://cran.r-project.org/package=mlflow)

- Install [MLflow](https://mlflow.org/) from R to track experiments
  locally.
- Connect to MLflow servers to share experiments with others.
- Use MLflow to export models that can be served locally and remotely.

## Prerequisites

To use the MLflow R API, you must install [the MLflow Python package](https://pypi.org/project/mlflow/).

```bash
pip install mlflow
```

Optionally, you can set the `MLFLOW_PYTHON_BIN` and `MLFLOW_BIN` environment variables to specify
the Python and MLflow binaries to use. By default, the R client automatically finds them using
`Sys.which("python")` and `Sys.which("mlflow")`.

```bash
export MLFLOW_PYTHON_BIN=/path/to/bin/python
export MLFLOW_BIN=/path/to/bin/mlflow
```

## Installation

Install `mlflow` as follows:

```r
devtools::install_github("mlflow/mlflow", subdir = "mlflow/R/mlflow")
```

## Development

Install the `mlflow` package as follows:

```r
devtools::install_github("mlflow/mlflow", subdir = "mlflow/R/mlflow")
```

Then install the latest released `mlflow` runtime.

However, currently, the development runtime of `mlflow` is also
required; which means you also need to download or clone the `mlflow`
GitHub repo:

```bash
git clone https://github.com/mlflow/mlflow
```

And upgrade the runtime to the development version as follows:

```bash
# Upgrade to the latest development version
pip install -e <local github repo>
```

## Tracking

MLflow Tracking allows you to logging parameters, code versions,
metrics, and output files when running R code and for later visualizing
the results.

MLflow allows you to group runs under experiments, which can be useful
for comparing runs intended to tackle a particular task. You can create
and activate a new experiment locally using `mlflow` as follows:

```r
library(mlflow)
mlflow_set_experiment("Test")
```

Then you can list view your experiments from MLflows user interface by
running:

```r
mlflow_ui()
```

<img src="tools/readme/mlflow-user-interface.png" class="screenshot" width=520 />

You can also use a MLflow server to track and share experiments, see
[running a tracking
server](https://www.mlflow.org/docs/latest/tracking.html#running-a-tracking-server),
and then make use of this server by running:

```r
mlflow_set_tracking_uri("http://tracking-server:5000")
```

Once the tracking url is defined, the experiments will be stored and
tracked in the specified server which others will also be able to
access.

## Projects

An MLflow Project is a format for packaging data science code in a
reusable and reproducible way.

MLflow projects can be [explicitly
created](https://www.mlflow.org/docs/latest/projects.html#specifying-projects)
or implicitly used by running `R` with `mlflow` from the terminal as
follows:

```bash
mlflow run examples/r_wine --entry-point train.R
```

Notice that is equivalent to running from `examples/r_wine`,

```bash
Rscript -e "mlflow::mlflow_source('train.R')"
```

and `train.R` performing training and logging as follows:

```r
library(mlflow)

# read parameters
column <- mlflow_log_param("column", 1)

# log total rows
mlflow_log_metric("rows", nrow(iris))

# train model
model <- lm(
  Sepal.Width ~ x,
  data.frame(Sepal.Width = iris$Sepal.Width, x = iris[,column])
)

# log models intercept
mlflow_log_metric("intercept", model$coefficients[["(Intercept)"]])
```

### Parameters

You will often want to parameterize your scripts to support running and
tracking multiple experiments. You can define parameters with type under
a `params_example.R` example as follows:

```r
library(mlflow)

# define parameters
my_int <- mlflow_param("my_int", 1, "integer")
my_num <- mlflow_param("my_num", 1.0, "numeric")

# log parameters
mlflow_log_param("param_int", my_int)
mlflow_log_param("param_num", my_num)
```

Then run `mlflow run` with custom parameters as
follows

    mlflow run tests/testthat/examples/ --entry-point params_example.R -P my_int=10 -P my_num=20.0 -P my_str=XYZ

    === Created directory /var/folders/ks/wm_bx4cn70s6h0r5vgqpsldm0000gn/T/tmpi6d2_wzf for downloading remote URIs passed to arguments of type 'path' ===
    === Running command 'source /miniconda2/bin/activate mlflow-da39a3ee5e6b4b0d3255bfef95601890afd80709 && Rscript -e "mlflow::mlflow_source('params_example.R')" --args --my_int 10 --my_num 20.0 --my_str XYZ' in run with ID '191b489b2355450a8c3cc9bf96cb1aa3' ===
    === Run (ID '191b489b2355450a8c3cc9bf96cb1aa3') succeeded ===

Run results that we can view with `mlflow_ui()`.

## Models

An MLflow Model is a standard format for packaging machine learning
models that can be used in a variety of downstream tools—for example,
real-time serving through a REST API or batch inference on Apache Spark.
They provide a convention to save a model in different "flavors" that
can be understood by different downstream tools.

To save a model use `mlflow_save_model()`. For instance, you can add the
following lines to the previous `train.R` script:

```r
# train model (...)

# save model
mlflow_save_model(
  crate(~ stats::predict(model, .x), model)
)
```

And trigger a run with that will also save your model as follows:

```bash
mlflow run train.R
```

Each MLflow Model is simply a directory containing arbitrary files,
together with an MLmodel file in the root of the directory that can
define multiple flavors that the model can be viewed in.

The directory containing the model looks as follows:

```r
dir("model")
```

    ## [1] "crate.bin" "MLmodel"

and the model definition `model/MLmodel` like:

```r
cat(paste(readLines("model/MLmodel"), collapse = "\n"))
```

    ## flavors:
    ##   crate:
    ##     version: 0.1.0
    ##     model: crate.bin
    ## time_created: 18-10-03T22:18:25.25.55
    ## run_id: 4286a3d27974487b95b19e01b7b3caab

Later on, the R model can be deployed which will perform predictions
using
`mlflow_rfunc_predict()`:

```r
mlflow_rfunc_predict("model", data = data.frame(x = c(0.3, 0.2)))
```

    ## Warning in mlflow_snapshot_warning(): Running without restoring the
    ## packages snapshot may not reload the model correctly. Consider running
    ## 'mlflow_restore_snapshot()' or setting the 'restore' parameter to 'TRUE'.

    ## 3.400381396714573.40656987651099

    ##        1        2
    ## 3.400381 3.406570

## Deployment

MLflow provides tools for deployment on a local machine and several
production environments. You can use these tools to easily apply your
models in a production environment.

You can serve a model by running,

```bash
mlflow rfunc serve model
```

which is equivalent to
running,

```bash
Rscript -e "mlflow_rfunc_serve('model')"
```

<img src="tools/readme/mlflow-serve-rfunc.png" class="screenshot" width=520 />

You can also run:

```bash
mlflow rfunc predict model data.json
```

which is equivalent to running,

```bash
Rscript -e "mlflow_rfunc_predict('model', 'data.json')"
```

## Dependencies

When running a project, `mlflow_snapshot()` is automatically called to
generate a `r-dependencies.txt` file which contains a list of required
packages and versions.

However, restoring dependencies is not automatic since it's usually an
expensive operation. To restore dependencies run:

```r
mlflow_restore_snapshot()
```

Notice that the `MLFLOW_SNAPSHOT_CACHE` environment variable can be set
to a cache directory to improve the time required to restore
dependencies.

## RStudio

To enable fast iteration while tracking with MLflow improvements over a
model, [RStudio 1.2.897](https://dailies.rstudio.com/) an be configured
to automatically trigger `mlflow_run()` when sourced. This is enabled by
including a `# !source mlflow::mlflow_run` comment at the top of the R
script as
follows:

<img src="tools/readme/mlflow-source-rstudio.png" class="screenshot" width=520 />

## Contributing

See the [MLflow contribution guidelines](https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md).


--- mlflow/java/client/README.md ---
# MLflow Java Client

Java client for [MLflow](https://mlflow.org) REST API.
See also the MLflow [Python API](https://mlflow.org/docs/latest/python_api/index.html)
and [REST API](https://mlflow.org/docs/latest/rest-api.html).

## Requirements

- Java 1.8
- Maven
- Run the [MLflow Tracking Server 0.4.2](https://mlflow.org/docs/latest/tracking.html#running-a-tracking-server)

## Build

### Build with tests

The MLflow Java client tests require that MLflow is on the PATH (to start a local server),
so it is recommended to run them from within a development conda environment.

To build a deployable JAR and run tests:

```
mvn package
```

## Run

To run a simple sample.

```
java -cp target/mlflow-java-client-0.4.2.jar \
  com.databricks.mlflow.client.samples.QuickStartDriver http://localhost:5001
```

## JSON Serialization

MLflow Java client uses [Protobuf](https://developers.google.com/protocol-buffers/) 3.6.0 to serialize the JSON payload.

- [service.proto](../mlflow/protos/service.proto) - Protobuf definition of data objects.
- [com.databricks.api.proto.mlflow.Service.java](src/main/java/com/databricks/api/proto/mlflow/Service.java) - Generated Java classes of all data objects.
- [generate_protos.py](generate_protos.py) - One time script to generate Service.java. If service.proto changes you will need to re-run this script.
- Javadoc can be generated by running `mvn javadoc:javadoc`. The output will be in [target/site/apidocs/index.html](target/site/apidocs/index.html).
  Here is the javadoc for [Service.java](target/site/apidocs/com/databricks/api/proto/mlflow/Service.html).

## Java Client API

See [ApiClient.java](src/main/java/org/mlflow/client/ApiClient.java)
and [Service.java domain objects](src/main/java/org/mlflow/api/proto/mlflow/Service.java).

```
Run getRun(String runId)
RunInfo createRun()
RunInfo createRun(String experimentId)
RunInfo createRun(String experimentId, String appName)
RunInfo createRun(CreateRun request)
List<RunInfo> listRunInfos(String experimentId)


List<Experiment> searchExperiments()
GetExperiment.Response getExperiment(String experimentId)
Optional<Experiment> getExperimentByName(String experimentName)
long createExperiment(String experimentName)

void logParam(String runId, String key, String value)
void logMetric(String runId, String key, float value)
void setTerminated(String runId)
void setTerminated(String runId, RunStatus status)
void setTerminated(String runId, RunStatus status, long endTime)
ListArtifacts.Response listArtifacts(String runId, String path)
```

## Usage

### Java Usage

For a simple example see [QuickStartDriver.java](src/main/java/org/mlflow/tracking/samples/QuickStartDriver.java).
For full examples of API coverage see the [tests](src/test/java/org/mlflow/tracking) such as [MlflowClientTest.java](src/test/java/org/mlflow/tracking/MlflowClientTest.java).

```
package org.mlflow.tracking.samples;

import java.util.List;
import java.util.Optional;

import org.apache.log4j.Level;
import org.apache.log4j.LogManager;

import org.mlflow.api.proto.Service.*;
import org.mlflow.tracking.MlflowClient;

/**
 * This is an example application which uses the MLflow Tracking API to create and manage
 * experiments and runs.
 */
public class QuickStartDriver {
  public static void main(String[] args) throws Exception {
    (new QuickStartDriver()).process(args);
  }

  void process(String[] args) throws Exception {
    MlflowClient client;
    if (args.length < 1) {
      client = new MlflowClient();
    } else {
      client = new MlflowClient(args[0]);
    }

    boolean verbose = args.length >= 2 && "true".equals(args[1]);
    if (verbose) {
      LogManager.getLogger("org.mlflow.client").setLevel(Level.DEBUG);
    }

    System.out.println("====== createExperiment");
    String expName = "Exp_" + System.currentTimeMillis();
    String expId = client.createExperiment(expName);
    System.out.println("createExperiment: expId=" + expId);

    System.out.println("====== getExperiment");
    GetExperiment.Response exp = client.getExperiment(expId);
    System.out.println("getExperiment: " + exp);

    System.out.println("====== searchExperiments");
    List<Experiment> exps = client.searchExperiments();
    System.out.println("#experiments: " + exps.size());
    exps.forEach(e -> System.out.println("  Exp: " + e));

    createRun(client, expId);

    System.out.println("====== getExperiment again");
    GetExperiment.Response exp2 = client.getExperiment(expId);
    System.out.println("getExperiment: " + exp2);

    System.out.println("====== getExperiment by name");
    Optional<Experiment> exp3 = client.getExperimentByName(expName);
    System.out.println("getExperimentByName: " + exp3);
  }

  void createRun(MlflowClient client, String expId) {
    System.out.println("====== createRun");

    // Create run
    String sourceFile = "MyFile.java";
    RunInfo runCreated = client.createRun(expId, sourceFile);
    System.out.println("CreateRun: " + runCreated);
    String runId = runCreated.getRunUuid();

    // Log parameters
    client.logParam(runId, "min_samples_leaf", "2");
    client.logParam(runId, "max_depth", "3");

    // Log metrics
    client.logMetric(runId, "auc", 2.12F);
    client.logMetric(runId, "accuracy_score", 3.12F);
    client.logMetric(runId, "zero_one_loss", 4.12F);

    // Update finished run
    client.setTerminated(runId, RunStatus.FINISHED);

    // Get run details
    Run run = client.getRun(runId);
    System.out.println("GetRun: " + run);
    client.close();
  }
}
```


--- mlflow/store/db_migrations/README.md ---
# MLflow Tracking database migrations

This directory contains configuration scripts and database migration logic for MLflow tracking
databases, using the Alembic migration library (https://alembic.sqlalchemy.org). To run database
migrations, use the `mlflow db upgrade` CLI command. To add and modify database migration logic,
see the contributor guide at https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md.

If you encounter failures while executing migrations, please file a GitHub issue at
https://github.com/mlflow/mlflow/issues.

## Migration descriptions

### 89d4b8295536_create_latest_metrics_table

This migration creates a `latest_metrics` table and populates it with the latest metric entry for
each unique `(run_id, metric_key)` tuple. Latest metric entries are computed based on `step`,
`timestamp`, and `value`.

This migration may take a long time for databases containing a large number of metric entries. You
can determine the total number of metric entries using the following query:

```sql
SELECT count(*) FROM metrics GROUP BY metrics.key, run_uuid;
```

Additionally, query join latency during the migration increases with the number of unique
`(run_id, metric_key)` tuples. You can determine the total number of unique tuples using
the following query:

```sql
SELECT count(*) FROM (
   SELECT metrics.key, run_uuid FROM metrics GROUP BY run_uuid, metrics.key
) unique_metrics;
```

For reference, migrating a Tracking database with the following attributes takes roughly
**three seconds** on MySQL 5.7:

- `3702` unique metrics
- `466860` total metric entries
- `186` runs
- An average of `125` entries per unique metric

#### Recovering from a failed migration

If the **create_latest_metrics_table** migration fails, simply delete the `latest_metrics`
table from your Tracking database as follows:

```sql
DROP TABLE latest_metrics;
```

Alembic does not stamp the database with an updated version unless the corresponding migration
completes successfully. Therefore, when this migration fails, the database remains on the
previous version, and deleting the `latest_metrics` table is sufficient to restore the database
to its prior state.

If the migration fails to complete due to excessive latency, please try executing the
`mlflow db upgrade` command on the same host machine where the database is running. This will
reduce the overhead of the migration's queries and batch insert operation.


--- mlflow/server/js/src/shared/web-shared/model-trace-explorer/oss-notebook-renderer/README.md ---
# Jupter Notebook Trace UI Renderer

This directory contains a standalone notebook renderer that is built as a separate entry point from the main MLflow application.

## Architecture

The notebook renderer is configured as a separate webpack entry point that generates its own HTML file and JavaScript bundle, completely independent of the main MLflow application.

### Build Configuration

The webpack configuration in `craco.config.js` handles the dual-entry setup:

1. **Entry Points**:

   - `main`: The main MLflow application (`src/index.tsx`)
   - `ml-model-trace-renderer`: The notebook renderer (`src/shared/web-shared/model-trace-explorer/oss-notebook-renderer/index.ts`)

2. **Output Structure**:

   ```
   build/
   ├── index.html                           # Main app HTML (excludes notebook renderer)
   ├── static/js/main.[hash].js             # Main app bundle
   ├── static/css/main.[hash].css           # Main app styles
   └── lib/notebook-trace-renderer/
       ├── index.html                       # Notebook renderer HTML
       └── js/ml-model-trace-renderer.[hash].js  # Notebook renderer bundle
   ```

3. **Path Resolution**:
   - Main app uses relative paths: `static-files/static/js/...`
   - Notebook renderer uses absolute paths: `/static-files/lib/notebook-trace-renderer/js/...`
   - Dynamic chunks use absolute paths: `/static-files/static/...` (via `__webpack_public_path__`)

### Key Configuration Details

#### Separate Entry Configuration

```javascript
webpackConfig.entry = {
  main: webpackConfig.entry, // Preserve original entry as 'main'
  'ml-model-trace-renderer': path.resolve(
    __dirname,
    'src/shared/web-shared/model-trace-explorer/oss-notebook-renderer/index.ts',
  ),
};
```

#### Output Path Functions

```javascript
webpackConfig.output = {
  filename: (pathData) => {
    return pathData.chunk.name === 'ml-model-trace-renderer'
      ? 'lib/notebook-trace-renderer/js/[name].[contenthash].js'
      : 'static/js/[name].[contenthash:8].js';
  },
  // ... similar for chunkFilename
};
```

#### HTML Plugin Configuration

- **Main app**: Excludes notebook renderer chunks via `excludeChunks: ['ml-model-trace-renderer']`
- **Notebook renderer**: Includes only its own chunks via `chunks: ['ml-model-trace-renderer']`

#### Runtime Path Override

The notebook renderer sets `__webpack_public_path__ = '/static-files/'` at runtime to ensure dynamically loaded chunks use the correct absolute paths.

## Files

- `index.ts`: Entry point that sets webpack public path and bootstraps the renderer
- `bootstrap.tsx`: Main renderer component
- `index.html`: HTML template for the standalone renderer
- `index.css`: Styles for the renderer

## Usage

The notebook renderer is built automatically as part of the main build process:

```bash
yarn build
```

This generates both the main application and the standalone notebook renderer, accessible at:

- Main app: `/static-files/index.html`
- Notebook renderer: `/static-files/lib/notebook-trace-renderer/index.html`

## Development Notes

- The renderer is completely independent of the main app - no shared runtime dependencies
- Uses absolute paths to avoid complex relative path calculations
- Webpack code splitting works correctly for both entry points
- CSS extraction is configured separately for each entry point


--- mlflow/server/js/public/index.html ---
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <link rel="shortcut icon" href="./static-files/favicon.ico" />
    <meta name="theme-color" content="#000000" />
    <!--
      manifest.json provides metadata used when your web app is added to the
      homescreen on Android. See https://developers.google.com/web/fundamentals/engage-and-retain/web-app-manifest/
    -->
    <link rel="manifest" href="./static-files/manifest.json" crossorigin="use-credentials" />
    <title>MLflow</title>
  </head>

  <body>
    <noscript> You need to enable JavaScript to run this app. </noscript>
    <div id="root" class="mlflow-ui-container"></div>
    <div id="modal" class="mlflow-ui-container"></div>
  </body>
</html>


--- mlflow/server/js/src/shared/web-shared/model-trace-explorer/oss-notebook-renderer/index.html ---
<html>
  <head></head>
  <body>
    <div id="root"></div>
  </body>
</html>


--- mlflow/client.py ---
"""
The ``mlflow.client`` module provides a Python CRUD interface to MLflow Experiments, Runs,
Model Versions, and Registered Models. This is a lower level API that directly translates to MLflow
`REST API <../rest-api.html>`_ calls.
For a higher level API for managing an "active run", use the :py:mod:`mlflow` module.
"""

from mlflow.tracking.client import MlflowClient

__all__ = [
    "MlflowClient",
]


--- mlflow/db.py ---
import click


@click.group("db")
def commands():
    """
    Commands for managing an MLflow tracking database.
    """


@commands.command()
@click.argument("url")
def upgrade(url):
    """
    Upgrade the schema of an MLflow tracking database to the latest supported version.

    **IMPORTANT**: Schema migrations can be slow and are not guaranteed to be transactional -
    **always take a backup of your database before running migrations**. The migrations README,
    which is located at
    https://github.com/mlflow/mlflow/blob/master/mlflow/store/db_migrations/README.md, describes
    large migrations and includes information about how to estimate their performance and
    recover from failures.
    """
    import mlflow.store.db.utils

    engine = mlflow.store.db.utils.create_sqlalchemy_engine_with_retry(url)
    mlflow.store.db.utils._upgrade_db(engine)


--- mlflow/environment_variables.py ---
"""
This module defines environment variables used in MLflow.
MLflow's environment variables adhere to the following naming conventions:
- Public variables: environment variable names begin with `MLFLOW_`
- Internal-use variables: For variables used only internally, names start with `_MLFLOW_`
"""

import os
import warnings
from pathlib import Path


class _EnvironmentVariable:
    """
    Represents an environment variable.
    """

    def __init__(self, name, type_, default):
        if type_ == bool and not isinstance(self, _BooleanEnvironmentVariable):
            raise ValueError("Use _BooleanEnvironmentVariable instead for boolean variables")
        self.name = name
        self.type = type_
        self.default = default

    @property
    def defined(self):
        return self.name in os.environ

    def get_raw(self):
        return os.getenv(self.name)

    def set(self, value):
        os.environ[self.name] = str(value)

    def unset(self):
        os.environ.pop(self.name, None)

    def is_set(self):
        return self.name in os.environ

    def get(self):
        """
        Reads the value of the environment variable if it exists and converts it to the desired
        type. Otherwise, returns the default value.
        """
        if (val := self.get_raw()) is not None:
            try:
                return self.type(val)
            except Exception as e:
                raise ValueError(f"Failed to convert {val!r} for {self.name}: {e}")
        return self.default

    def __str__(self):
        return f"{self.name} (default: {self.default})"

    def __repr__(self):
        return repr(self.name)

    def __format__(self, format_spec: str) -> str:
        return self.name.__format__(format_spec)


class _BooleanEnvironmentVariable(_EnvironmentVariable):
    """
    Represents a boolean environment variable.
    """

    def __init__(self, name, default):
        # `default not in [True, False, None]` doesn't work because `1 in [True]`
        # (or `0 in [False]`) returns True.
        if not (default is True or default is False or default is None):
            raise ValueError(f"{name} default value must be one of [True, False, None]")
        super().__init__(name, bool, default)

    def get(self):
        # TODO: Remove this block in MLflow 3.2.0
        if self.name == MLFLOW_CONFIGURE_LOGGING.name and (
            val := os.getenv("MLFLOW_LOGGING_CONFIGURE_LOGGING")
        ):
            warnings.warn(
                "Environment variable MLFLOW_LOGGING_CONFIGURE_LOGGING is deprecated and will be "
                f"removed in a future release. Please use {MLFLOW_CONFIGURE_LOGGING.name} instead.",
                FutureWarning,
                stacklevel=2,
            )
            return val.lower() in ["true", "1"]

        if not self.defined:
            return self.default

        val = os.getenv(self.name)
        lowercased = val.lower()
        if lowercased not in ["true", "false", "1", "0"]:
            raise ValueError(
                f"{self.name} value must be one of ['true', 'false', '1', '0'] (case-insensitive), "
                f"but got {val}"
            )
        return lowercased in ["true", "1"]


#: Specifies the tracking URI.
#: (default: ``None``)
MLFLOW_TRACKING_URI = _EnvironmentVariable("MLFLOW_TRACKING_URI", str, None)

#: Specifies the registry URI.
#: (default: ``None``)
MLFLOW_REGISTRY_URI = _EnvironmentVariable("MLFLOW_REGISTRY_URI", str, None)

#: Specifies the ``dfs_tmpdir`` parameter to use for ``mlflow.spark.save_model``,
#: ``mlflow.spark.log_model`` and ``mlflow.spark.load_model``. See
#: https://www.mlflow.org/docs/latest/python_api/mlflow.spark.html#mlflow.spark.save_model
#: for more information.
#: (default: ``/tmp/mlflow``)
MLFLOW_DFS_TMP = _EnvironmentVariable("MLFLOW_DFS_TMP", str, "/tmp/mlflow")

#: Specifies the maximum number of retries with exponential backoff for MLflow HTTP requests
#: (default: ``7``)
MLFLOW_HTTP_REQUEST_MAX_RETRIES = _EnvironmentVariable(
    "MLFLOW_HTTP_REQUEST_MAX_RETRIES",
    int,
    # Important: It's common for MLflow backends to rate limit requests for more than 1 minute.
    # To remain resilient to rate limiting, the MLflow client needs to retry for more than 1
    # minute. Assuming 2 seconds per retry, 7 retries with backoff will take ~ 4 minutes,
    # which is appropriate for most rate limiting scenarios
    7,
)

#: Specifies the backoff increase factor between MLflow HTTP request failures
#: (default: ``2``)
MLFLOW_HTTP_REQUEST_BACKOFF_FACTOR = _EnvironmentVariable(
    "MLFLOW_HTTP_REQUEST_BACKOFF_FACTOR", int, 2
)

#: Specifies the backoff jitter between MLflow HTTP request failures
#: (default: ``1.0``)
MLFLOW_HTTP_REQUEST_BACKOFF_JITTER = _EnvironmentVariable(
    "MLFLOW_HTTP_REQUEST_BACKOFF_JITTER", float, 1.0
)

#: Specifies the timeout in seconds for MLflow HTTP requests
#: (default: ``120``)
MLFLOW_HTTP_REQUEST_TIMEOUT = _EnvironmentVariable("MLFLOW_HTTP_REQUEST_TIMEOUT", int, 120)

#: Specifies the timeout in seconds for MLflow deployment client HTTP requests
#: (non-predict operations). This is separate from MLFLOW_HTTP_REQUEST_TIMEOUT to allow
#: longer timeouts for LLM calls (default: ``300``)
MLFLOW_DEPLOYMENT_CLIENT_HTTP_REQUEST_TIMEOUT = _EnvironmentVariable(
    "MLFLOW_DEPLOYMENT_CLIENT_HTTP_REQUEST_TIMEOUT", int, 300
)

#: Specifies whether to respect Retry-After header on status codes defined as
#: Retry.RETRY_AFTER_STATUS_CODES or not for MLflow HTTP request
#: (default: ``True``)
MLFLOW_HTTP_RESPECT_RETRY_AFTER_HEADER = _BooleanEnvironmentVariable(
    "MLFLOW_HTTP_RESPECT_RETRY_AFTER_HEADER", True
)

#: Internal-only configuration that sets an upper bound to the allowable maximum
#: retries for HTTP requests
#: (default: ``10``)
_MLFLOW_HTTP_REQUEST_MAX_RETRIES_LIMIT = _EnvironmentVariable(
    "_MLFLOW_HTTP_REQUEST_MAX_RETRIES_LIMIT", int, 10
)

#: Internal-only configuration that sets the upper bound for an HTTP backoff_factor
#: (default: ``120``)
_MLFLOW_HTTP_REQUEST_MAX_BACKOFF_FACTOR_LIMIT = _EnvironmentVariable(
    "_MLFLOW_HTTP_REQUEST_MAX_BACKOFF_FACTOR_LIMIT", int, 120
)

#: Specifies whether MLflow HTTP requests should be signed using AWS signature V4. It will overwrite
#: (default: ``False``). When set, it will overwrite the "Authorization" HTTP header.
#: See https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html for more information.
MLFLOW_TRACKING_AWS_SIGV4 = _BooleanEnvironmentVariable("MLFLOW_TRACKING_AWS_SIGV4", False)

#: Specifies the auth provider to sign the MLflow HTTP request
#: (default: ``None``). When set, it will overwrite the "Authorization" HTTP header.
MLFLOW_TRACKING_AUTH = _EnvironmentVariable("MLFLOW_TRACKING_AUTH", str, None)

#: Specifies the chunk size to use when downloading a file from GCS
#: (default: ``None``). If None, the chunk size is automatically determined by the
#: ``google-cloud-storage`` package.
MLFLOW_GCS_DOWNLOAD_CHUNK_SIZE = _EnvironmentVariable("MLFLOW_GCS_DOWNLOAD_CHUNK_SIZE", int, None)

#: Specifies the chunk size to use when uploading a file to GCS.
#: (default: ``None``). If None, the chunk size is automatically determined by the
#: ``google-cloud-storage`` package.
MLFLOW_GCS_UPLOAD_CHUNK_SIZE = _EnvironmentVariable("MLFLOW_GCS_UPLOAD_CHUNK_SIZE", int, None)

#: Specifies whether to disable model logging and loading via mlflowdbfs.
#: (default: ``None``)
_DISABLE_MLFLOWDBFS = _EnvironmentVariable("DISABLE_MLFLOWDBFS", str, None)

#: Specifies the S3 endpoint URL to use for S3 artifact operations.
#: (default: ``None``)
MLFLOW_S3_ENDPOINT_URL = _EnvironmentVariable("MLFLOW_S3_ENDPOINT_URL", str, None)

#: Specifies whether or not to skip TLS certificate verification for S3 artifact operations.
#: (default: ``False``)
MLFLOW_S3_IGNORE_TLS = _BooleanEnvironmentVariable("MLFLOW_S3_IGNORE_TLS", False)

#: Specifies extra arguments for S3 artifact uploads.
#: (default: ``None``)
MLFLOW_S3_UPLOAD_EXTRA_ARGS = _EnvironmentVariable("MLFLOW_S3_UPLOAD_EXTRA_ARGS", str, None)

#: Specifies the location of a Kerberos ticket cache to use for HDFS artifact operations.
#: (default: ``None``)
MLFLOW_KERBEROS_TICKET_CACHE = _EnvironmentVariable("MLFLOW_KERBEROS_TICKET_CACHE", str, None)

#: Specifies a Kerberos user for HDFS artifact operations.
#: (default: ``None``)
MLFLOW_KERBEROS_USER = _EnvironmentVariable("MLFLOW_KERBEROS_USER", str, None)

#: Specifies extra pyarrow configurations for HDFS artifact operations.
#: (default: ``None``)
MLFLOW_PYARROW_EXTRA_CONF = _EnvironmentVariable("MLFLOW_PYARROW_EXTRA_CONF", str, None)

#: Specifies the ``pool_size`` parameter to use for ``sqlalchemy.create_engine`` in the SQLAlchemy
#: tracking store. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.pool_size
#: for more information.
#: (default: ``None``)
MLFLOW_SQLALCHEMYSTORE_POOL_SIZE = _EnvironmentVariable(
    "MLFLOW_SQLALCHEMYSTORE_POOL_SIZE", int, None
)

#: Specifies the ``pool_recycle`` parameter to use for ``sqlalchemy.create_engine`` in the
#: SQLAlchemy tracking store. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.pool_recycle
#: for more information.
#: (default: ``None``)
MLFLOW_SQLALCHEMYSTORE_POOL_RECYCLE = _EnvironmentVariable(
    "MLFLOW_SQLALCHEMYSTORE_POOL_RECYCLE", int, None
)

#: Specifies the ``max_overflow`` parameter to use for ``sqlalchemy.create_engine`` in the
#: SQLAlchemy tracking store. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.max_overflow
#: for more information.
#: (default: ``None``)
MLFLOW_SQLALCHEMYSTORE_MAX_OVERFLOW = _EnvironmentVariable(
    "MLFLOW_SQLALCHEMYSTORE_MAX_OVERFLOW", int, None
)

#: Specifies the ``echo`` parameter to use for ``sqlalchemy.create_engine`` in the
#: SQLAlchemy tracking store. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.echo
#: for more information.
#: (default: ``False``)
MLFLOW_SQLALCHEMYSTORE_ECHO = _BooleanEnvironmentVariable("MLFLOW_SQLALCHEMYSTORE_ECHO", False)

#: Specifies whether or not to print a warning when `--env-manager=conda` is specified.
#: (default: ``False``)
MLFLOW_DISABLE_ENV_MANAGER_CONDA_WARNING = _BooleanEnvironmentVariable(
    "MLFLOW_DISABLE_ENV_MANAGER_CONDA_WARNING", False
)
#: Specifies the ``poolclass`` parameter to use for ``sqlalchemy.create_engine`` in the
#: SQLAlchemy tracking store. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.poolclass
#: for more information.
#: (default: ``None``)
MLFLOW_SQLALCHEMYSTORE_POOLCLASS = _EnvironmentVariable(
    "MLFLOW_SQLALCHEMYSTORE_POOLCLASS", str, None
)

#: Specifies the ``timeout_seconds`` for MLflow Model dependency inference operations.
#: (default: ``120``)
MLFLOW_REQUIREMENTS_INFERENCE_TIMEOUT = _EnvironmentVariable(
    "MLFLOW_REQUIREMENTS_INFERENCE_TIMEOUT", int, 120
)

#: Specifies the MLflow Model Scoring server request timeout in seconds
#: (default: ``60``)
MLFLOW_SCORING_SERVER_REQUEST_TIMEOUT = _EnvironmentVariable(
    "MLFLOW_SCORING_SERVER_REQUEST_TIMEOUT", int, 60
)

#: (Experimental, may be changed or removed)
#: Specifies the timeout to use when uploading or downloading a file
#: (default: ``None``). If None, individual artifact stores will choose defaults.
MLFLOW_ARTIFACT_UPLOAD_DOWNLOAD_TIMEOUT = _EnvironmentVariable(
    "MLFLOW_ARTIFACT_UPLOAD_DOWNLOAD_TIMEOUT", int, None
)

#: Specifies the timeout for model inference with input example(s) when logging/saving a model.
#: MLflow runs a few inference requests against the model to infer model signature and pip
#: requirements. Sometimes the prediction hangs for a long time, especially for a large model.
#: This timeout limits the allowable time for performing a prediction for signature inference
#: and will abort the prediction, falling back to the default signature and pip requirements.
MLFLOW_INPUT_EXAMPLE_INFERENCE_TIMEOUT = _EnvironmentVariable(
    "MLFLOW_INPUT_EXAMPLE_INFERENCE_TIMEOUT", int, 180
)


#: Specifies the device intended for use in the predict function - can be used
#: to override behavior where the GPU is used by default when available by
#: setting this environment variable to be ``cpu``. Currently, this
#: variable is only supported for the MLflow PyTorch and HuggingFace flavors.
#: For the HuggingFace flavor, note that device must be parseable as an integer.
MLFLOW_DEFAULT_PREDICTION_DEVICE = _EnvironmentVariable(
    "MLFLOW_DEFAULT_PREDICTION_DEVICE", str, None
)

#: Specifies to Huggingface whether to use the automatic device placement logic of
# HuggingFace accelerate. If it's set to false, the low_cpu_mem_usage flag will not be
# set to True and device_map will not be set to "auto".
MLFLOW_HUGGINGFACE_DISABLE_ACCELERATE_FEATURES = _BooleanEnvironmentVariable(
    "MLFLOW_DISABLE_HUGGINGFACE_ACCELERATE_FEATURES", False
)

#: Specifies to Huggingface whether to use the automatic device placement logic of
# HuggingFace accelerate. If it's set to false, the low_cpu_mem_usage flag will not be
# set to True and device_map will not be set to "auto". Default to False.
MLFLOW_HUGGINGFACE_USE_DEVICE_MAP = _BooleanEnvironmentVariable(
    "MLFLOW_HUGGINGFACE_USE_DEVICE_MAP", False
)

#: Specifies to Huggingface to use the automatic device placement logic of HuggingFace accelerate.
#: This can be set to values supported by the version of HuggingFace Accelerate being installed.
MLFLOW_HUGGINGFACE_DEVICE_MAP_STRATEGY = _EnvironmentVariable(
    "MLFLOW_HUGGINGFACE_DEVICE_MAP_STRATEGY", str, "auto"
)

#: Specifies to Huggingface to use the low_cpu_mem_usage flag powered by HuggingFace accelerate.
#: If it's set to false, the low_cpu_mem_usage flag will be set to False.
MLFLOW_HUGGINGFACE_USE_LOW_CPU_MEM_USAGE = _BooleanEnvironmentVariable(
    "MLFLOW_HUGGINGFACE_USE_LOW_CPU_MEM_USAGE", True
)

#: Specifies the max_shard_size to use when mlflow transformers flavor saves the model checkpoint.
#: This can be set to override the 500MB default.
MLFLOW_HUGGINGFACE_MODEL_MAX_SHARD_SIZE = _EnvironmentVariable(
    "MLFLOW_HUGGINGFACE_MODEL_MAX_SHARD_SIZE", str, "500MB"
)

#: Specifies the name of the Databricks secret scope to use for storing OpenAI API keys.
MLFLOW_OPENAI_SECRET_SCOPE = _EnvironmentVariable("MLFLOW_OPENAI_SECRET_SCOPE", str, None)

#: (Experimental, may be changed or removed)
#: Specifies the download options to be used by pip wheel when `add_libraries_to_model` is used to
#: create and log model dependencies as model artifacts. The default behavior only uses dependency
#: binaries and no source packages.
#: (default: ``--only-binary=:all:``).
MLFLOW_WHEELED_MODEL_PIP_DOWNLOAD_OPTIONS = _EnvironmentVariable(
    "MLFLOW_WHEELED_MODEL_PIP_DOWNLOAD_OPTIONS", str, "--only-binary=:all:"
)

# Specifies whether or not to use multipart download when downloading a large file on Databricks.
MLFLOW_ENABLE_MULTIPART_DOWNLOAD = _BooleanEnvironmentVariable(
    "MLFLOW_ENABLE_MULTIPART_DOWNLOAD", True
)

# Specifies whether or not to use multipart upload when uploading large artifacts.
MLFLOW_ENABLE_MULTIPART_UPLOAD = _BooleanEnvironmentVariable("MLFLOW_ENABLE_MULTIPART_UPLOAD", True)

#: Specifies whether or not to use multipart upload for proxied artifact access.
#: (default: ``False``)
MLFLOW_ENABLE_PROXY_MULTIPART_UPLOAD = _BooleanEnvironmentVariable(
    "MLFLOW_ENABLE_PROXY_MULTIPART_UPLOAD", False
)

#: Private environment variable that's set to ``True`` while running tests.
_MLFLOW_TESTING = _BooleanEnvironmentVariable("MLFLOW_TESTING", False)

#: Specifies the username used to authenticate with a tracking server.
#: (default: ``None``)
MLFLOW_TRACKING_USERNAME = _EnvironmentVariable("MLFLOW_TRACKING_USERNAME", str, None)

#: Specifies the password used to authenticate with a tracking server.
#: (default: ``None``)
MLFLOW_TRACKING_PASSWORD = _EnvironmentVariable("MLFLOW_TRACKING_PASSWORD", str, None)

#: Specifies and takes precedence for setting the basic/bearer auth on http requests.
#: (default: ``None``)
MLFLOW_TRACKING_TOKEN = _EnvironmentVariable("MLFLOW_TRACKING_TOKEN", str, None)

#: Specifies whether to verify TLS connection in ``requests.request`` function,
#: see https://requests.readthedocs.io/en/master/api/
#: (default: ``False``).
MLFLOW_TRACKING_INSECURE_TLS = _BooleanEnvironmentVariable("MLFLOW_TRACKING_INSECURE_TLS", False)

#: Sets the ``verify`` param in ``requests.request`` function,
#: see https://requests.readthedocs.io/en/master/api/
#: (default: ``None``)
MLFLOW_TRACKING_SERVER_CERT_PATH = _EnvironmentVariable(
    "MLFLOW_TRACKING_SERVER_CERT_PATH", str, None
)

#: Sets the ``cert`` param in ``requests.request`` function,
#: see https://requests.readthedocs.io/en/master/api/
#: (default: ``None``)
MLFLOW_TRACKING_CLIENT_CERT_PATH = _EnvironmentVariable(
    "MLFLOW_TRACKING_CLIENT_CERT_PATH", str, None
)

#: Specified the ID of the run to log data to.
#: (default: ``None``)
MLFLOW_RUN_ID = _EnvironmentVariable("MLFLOW_RUN_ID", str, None)

#: Specifies the default root directory for tracking `FileStore`.
#: (default: ``None``)
MLFLOW_TRACKING_DIR = _EnvironmentVariable("MLFLOW_TRACKING_DIR", str, None)

#: Specifies the default root directory for registry `FileStore`.
#: (default: ``None``)
MLFLOW_REGISTRY_DIR = _EnvironmentVariable("MLFLOW_REGISTRY_DIR", str, None)

#: Specifies the default experiment ID to create run to.
#: (default: ``None``)
MLFLOW_EXPERIMENT_ID = _EnvironmentVariable("MLFLOW_EXPERIMENT_ID", str, None)

#: Specifies the default experiment name to create run to.
#: (default: ``None``)
MLFLOW_EXPERIMENT_NAME = _EnvironmentVariable("MLFLOW_EXPERIMENT_NAME", str, None)

#: Specified the path to the configuration file for MLflow Authentication.
#: (default: ``None``)
MLFLOW_AUTH_CONFIG_PATH = _EnvironmentVariable("MLFLOW_AUTH_CONFIG_PATH", str, None)

#: Specifies and takes precedence for setting the UC OSS basic/bearer auth on http requests.
#: (default: ``None``)
MLFLOW_UC_OSS_TOKEN = _EnvironmentVariable("MLFLOW_UC_OSS_TOKEN", str, None)

#: Specifies the root directory to create Python virtual environments in.
#: (default: ``~/.mlflow/envs``)
MLFLOW_ENV_ROOT = _EnvironmentVariable(
    "MLFLOW_ENV_ROOT", str, str(Path.home().joinpath(".mlflow", "envs"))
)

#: Specifies whether or not to use DBFS FUSE mount to store artifacts on Databricks
#: (default: ``False``)
MLFLOW_ENABLE_DBFS_FUSE_ARTIFACT_REPO = _BooleanEnvironmentVariable(
    "MLFLOW_ENABLE_DBFS_FUSE_ARTIFACT_REPO", True
)

#: Specifies whether or not to use UC Volume FUSE mount to store artifacts on Databricks
#: (default: ``True``)
MLFLOW_ENABLE_UC_VOLUME_FUSE_ARTIFACT_REPO = _BooleanEnvironmentVariable(
    "MLFLOW_ENABLE_UC_VOLUME_FUSE_ARTIFACT_REPO", True
)

#: Private environment variable that should be set to ``True`` when running autologging tests.
#: (default: ``False``)
_MLFLOW_AUTOLOGGING_TESTING = _BooleanEnvironmentVariable("MLFLOW_AUTOLOGGING_TESTING", False)

#: (Experimental, may be changed or removed)
#: Specifies the uri of a MLflow Gateway Server instance to be used with the Gateway Client APIs
#: (default: ``None``)
MLFLOW_GATEWAY_URI = _EnvironmentVariable("MLFLOW_GATEWAY_URI", str, None)

#: (Experimental, may be changed or removed)
#: Specifies the uri of an MLflow AI Gateway instance to be used with the Deployments
#: Client APIs
#: (default: ``None``)
MLFLOW_DEPLOYMENTS_TARGET = _EnvironmentVariable("MLFLOW_DEPLOYMENTS_TARGET", str, None)

#: Specifies the path of the config file for MLflow AI Gateway.
#: (default: ``None``)
MLFLOW_GATEWAY_CONFIG = _EnvironmentVariable("MLFLOW_GATEWAY_CONFIG", str, None)

#: Specifies the path of the config file for MLflow AI Gateway.
#: (default: ``None``)
MLFLOW_DEPLOYMENTS_CONFIG = _EnvironmentVariable("MLFLOW_DEPLOYMENTS_CONFIG", str, None)

#: Specifies whether to display the progress bar when uploading/downloading artifacts.
#: (default: ``True``)
MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR = _BooleanEnvironmentVariable(
    "MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR", True
)

#: Specifies the conda home directory to use.
#: (default: ``conda``)
MLFLOW_CONDA_HOME = _EnvironmentVariable("MLFLOW_CONDA_HOME", str, None)

#: Specifies the name of the command to use when creating the environments.
#: For example, let's say we want to use mamba (https://github.com/mamba-org/mamba)
#: instead of conda to create environments.
#: Then: > conda install mamba -n base -c conda-forge
#: If not set, use the same as conda_path
#: (default: ``conda``)
MLFLOW_CONDA_CREATE_ENV_CMD = _EnvironmentVariable("MLFLOW_CONDA_CREATE_ENV_CMD", str, "conda")

#: Specifies the flavor to serve in the scoring server.
#: (default ``None``)
MLFLOW_DEPLOYMENT_FLAVOR_NAME = _EnvironmentVariable("MLFLOW_DEPLOYMENT_FLAVOR_NAME", str, None)

#: Specifies the MLflow Run context
#: (default: ``None``)
MLFLOW_RUN_CONTEXT = _EnvironmentVariable("MLFLOW_RUN_CONTEXT", str, None)

#: Specifies the URL of the ECR-hosted Docker image a model is deployed into for SageMaker.
# (default: ``None``)
MLFLOW_SAGEMAKER_DEPLOY_IMG_URL = _EnvironmentVariable("MLFLOW_SAGEMAKER_DEPLOY_IMG_URL", str, None)

#: Specifies whether to disable creating a new conda environment for `mlflow models build-docker`.
#: (default: ``False``)
MLFLOW_DISABLE_ENV_CREATION = _BooleanEnvironmentVariable("MLFLOW_DISABLE_ENV_CREATION", False)

#: Specifies the timeout value for downloading chunks of mlflow artifacts.
#: (default: ``300``)
MLFLOW_DOWNLOAD_CHUNK_TIMEOUT = _EnvironmentVariable("MLFLOW_DOWNLOAD_CHUNK_TIMEOUT", int, 300)

#: Specifies if system metrics logging should be enabled.
MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING = _BooleanEnvironmentVariable(
    "MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING", False
)

#: Specifies the sampling interval for system metrics logging.
MLFLOW_SYSTEM_METRICS_SAMPLING_INTERVAL = _EnvironmentVariable(
    "MLFLOW_SYSTEM_METRICS_SAMPLING_INTERVAL", float, None
)

#: Specifies the number of samples before logging system metrics.
MLFLOW_SYSTEM_METRICS_SAMPLES_BEFORE_LOGGING = _EnvironmentVariable(
    "MLFLOW_SYSTEM_METRICS_SAMPLES_BEFORE_LOGGING", int, None
)

#: Specifies the node id of system metrics logging. This is useful in multi-node (distributed
#: training) setup.
MLFLOW_SYSTEM_METRICS_NODE_ID = _EnvironmentVariable("MLFLOW_SYSTEM_METRICS_NODE_ID", str, None)


# Private environment variable to specify the number of chunk download retries for multipart
# download.
_MLFLOW_MPD_NUM_RETRIES = _EnvironmentVariable("_MLFLOW_MPD_NUM_RETRIES", int, 3)

# Private environment variable to specify the interval between chunk download retries for multipart
# download.
_MLFLOW_MPD_RETRY_INTERVAL_SECONDS = _EnvironmentVariable(
    "_MLFLOW_MPD_RETRY_INTERVAL_SECONDS", int, 1
)

#: Specifies the minimum file size in bytes to use multipart upload when logging artifacts
#: (default: ``524_288_000`` (500 MB))
MLFLOW_MULTIPART_UPLOAD_MINIMUM_FILE_SIZE = _EnvironmentVariable(
    "MLFLOW_MULTIPART_UPLOAD_MINIMUM_FILE_SIZE", int, 500 * 1024**2
)

#: Specifies the minimum file size in bytes to use multipart download when downloading artifacts
#: (default: ``524_288_000`` (500 MB))
MLFLOW_MULTIPART_DOWNLOAD_MINIMUM_FILE_SIZE = _EnvironmentVariable(
    "MLFLOW_MULTIPART_DOWNLOAD_MINIMUM_FILE_SIZE", int, 500 * 1024**2
)

#: Specifies the chunk size in bytes to use when performing multipart upload
#: (default: ``104_857_60`` (10 MB))
MLFLOW_MULTIPART_UPLOAD_CHUNK_SIZE = _EnvironmentVariable(
    "MLFLOW_MULTIPART_UPLOAD_CHUNK_SIZE", int, 10 * 1024**2
)

#: Specifies the chunk size in bytes to use when performing multipart download
#: (default: ``104_857_600`` (100 MB))
MLFLOW_MULTIPART_DOWNLOAD_CHUNK_SIZE = _EnvironmentVariable(
    "MLFLOW_MULTIPART_DOWNLOAD_CHUNK_SIZE", int, 100 * 1024**2
)

#: Specifies whether or not to allow the MLflow server to follow redirects when
#: making HTTP requests. If set to False, the server will throw an exception if it
#: encounters a redirect response.
#: (default: ``True``)
MLFLOW_ALLOW_HTTP_REDIRECTS = _BooleanEnvironmentVariable("MLFLOW_ALLOW_HTTP_REDIRECTS", True)

#: Specifies the client-based timeout (in seconds) when making an HTTP request to a deployment
#: target. Used within the `predict` and `predict_stream` APIs.
#: (default: ``120``)
MLFLOW_DEPLOYMENT_PREDICT_TIMEOUT = _EnvironmentVariable(
    "MLFLOW_DEPLOYMENT_PREDICT_TIMEOUT", int, 120
)

MLFLOW_GATEWAY_RATE_LIMITS_STORAGE_URI = _EnvironmentVariable(
    "MLFLOW_GATEWAY_RATE_LIMITS_STORAGE_URI", str, None
)

#: If True, MLflow fluent logging APIs, e.g., `mlflow.log_metric` will log asynchronously.
MLFLOW_ENABLE_ASYNC_LOGGING = _BooleanEnvironmentVariable("MLFLOW_ENABLE_ASYNC_LOGGING", False)

#: Number of workers in the thread pool used for asynchronous logging, defaults to 10.
MLFLOW_ASYNC_LOGGING_THREADPOOL_SIZE = _EnvironmentVariable(
    "MLFLOW_ASYNC_LOGGING_THREADPOOL_SIZE", int, 10
)

#: Specifies whether or not to have mlflow configure logging on import.
#: If set to True, mlflow will configure ``mlflow.<module_name>`` loggers with
#: logging handlers and formatters.
#: (default: ``True``)
MLFLOW_CONFIGURE_LOGGING = _BooleanEnvironmentVariable("MLFLOW_CONFIGURE_LOGGING", True)

#: If set to True, the following entities will be truncated to their maximum length:
#: - Param value
#: - Tag value
#: If set to False, an exception will be raised if the length of the entity exceeds the maximum
#: length.
#: (default: ``True``)
MLFLOW_TRUNCATE_LONG_VALUES = _BooleanEnvironmentVariable("MLFLOW_TRUNCATE_LONG_VALUES", True)

# Whether to run slow tests with pytest. Default to False in normal runs,
# but set to True in the weekly slow test jobs.
_MLFLOW_RUN_SLOW_TESTS = _BooleanEnvironmentVariable("MLFLOW_RUN_SLOW_TESTS", False)

#: The OpenJDK version to install in the Docker image used for MLflow models.
#: (default: ``11``)
MLFLOW_DOCKER_OPENJDK_VERSION = _EnvironmentVariable("MLFLOW_DOCKER_OPENJDK_VERSION", str, "11")


#: How long a trace can be "in-progress". When this is set to a positive value and a trace is
#: not completed within this time, it will be automatically halted and exported to the specified
#: backend destination with status "ERROR".
MLFLOW_TRACE_TIMEOUT_SECONDS = _EnvironmentVariable("MLFLOW_TRACE_TIMEOUT_SECONDS", int, None)

#: How frequently to check for timed-out traces. For example, if this is set to 10, MLflow will
#: check for timed-out traces every 10 seconds (in a background worker) and halt any traces that
#: have exceeded the timeout. This is only effective if MLFLOW_TRACE_TIMEOUT_SECONDS is set to a
#: positive value.
MLFLOW_TRACE_TIMEOUT_CHECK_INTERVAL_SECONDS = _EnvironmentVariable(
    "MLFLOW_TRACE_TIMEOUT_CHECK_INTERVAL_SECONDS", int, 1
)

# How long a trace can be buffered in-memory at client side before being abandoned.
MLFLOW_TRACE_BUFFER_TTL_SECONDS = _EnvironmentVariable("MLFLOW_TRACE_BUFFER_TTL_SECONDS", int, 3600)

# How many traces to be buffered in-memory at client side before being abandoned.
MLFLOW_TRACE_BUFFER_MAX_SIZE = _EnvironmentVariable("MLFLOW_TRACE_BUFFER_MAX_SIZE", int, 1000)

#: Maximum number of prompt versions to cache in the LRU cache for _load_prompt_version_cached.
#: This cache improves performance by avoiding repeated network calls for the same prompt version.
#: (default: ``128``)
MLFLOW_PROMPT_CACHE_MAX_SIZE = _EnvironmentVariable("MLFLOW_PROMPT_CACHE_MAX_SIZE", int, 128)

#: Private configuration option.
#: Enables the ability to catch exceptions within MLflow evaluate for classification models
#: where a class imbalance due to a missing target class would raise an error in the
#: underlying metrology modules (scikit-learn). If set to True, specific exceptions will be
#: caught, alerted via the warnings module, and evaluation will resume.
#: (default: ``False``)
_MLFLOW_EVALUATE_SUPPRESS_CLASSIFICATION_ERRORS = _BooleanEnvironmentVariable(
    "_MLFLOW_EVALUATE_SUPPRESS_CLASSIFICATION_ERRORS", False
)

#: Maximum number of workers to use for running model prediction and scoring during
#: for each row in the dataset passed to the `mlflow.genai.evaluate` function.
#: (default: ``10``)
MLFLOW_GENAI_EVAL_MAX_WORKERS = _EnvironmentVariable("MLFLOW_GENAI_EVAL_MAX_WORKERS", int, 10)


#: Skip trace validation during GenAI evaluation. By default (False), MLflow will validate if
#: the given predict function generates a valid trace, and otherwise wraps it with @mlflow.trace
#: decorator to make sure a trace is generated. This validation requires running a single
#: prediction. When you are sure that the predict function generates a trace, set this to True
#: to skip the validation and save the time of running a single prediction.
MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION = _BooleanEnvironmentVariable(
    "MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION", False
)

#: Whether to warn (default) or raise (opt-in) for unresolvable requirements inference for
#: a model's dependency inference. If set to True, an exception will be raised if requirements
#: inference or the process of capturing imported modules encounters any errors.
MLFLOW_REQUIREMENTS_INFERENCE_RAISE_ERRORS = _BooleanEnvironmentVariable(
    "MLFLOW_REQUIREMENTS_INFERENCE_RAISE_ERRORS", False
)

# How many traces to display in Databricks Notebooks
MLFLOW_MAX_TRACES_TO_DISPLAY_IN_NOTEBOOK = _EnvironmentVariable(
    "MLFLOW_MAX_TRACES_TO_DISPLAY_IN_NOTEBOOK", int, 10
)

#: Specifies the sampling ratio for traces. Value should be between 0.0 and 1.0.
#: A value of 1.0 means all traces are sampled (default behavior).
#: A value of 0.5 means 50% of traces are sampled.
#: A value of 0.0 means no traces are sampled.
#: (default: ``1.0``)
MLFLOW_TRACE_SAMPLING_RATIO = _EnvironmentVariable("MLFLOW_TRACE_SAMPLING_RATIO", float, 1.0)

#: When OTel export is configured and this is set to true, MLflow will write spans to BOTH
#: MLflow Tracking Server and OpenTelemetry Collector. When false (default), OTel export
#: replaces MLflow export.
#: (default: ``False``)
MLFLOW_TRACE_ENABLE_OTLP_DUAL_EXPORT = _BooleanEnvironmentVariable(
    "MLFLOW_TRACE_ENABLE_OTLP_DUAL_EXPORT", False
)

#: Controls whether MLflow should export traces to OTLP endpoint when
#: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT is set. This allows users to disable MLflow's OTLP
#: export even when the OTEL endpoint is configured for other telemetry clients.
#: (default: ``True``)
MLFLOW_ENABLE_OTLP_EXPORTER = _BooleanEnvironmentVariable("MLFLOW_ENABLE_OTLP_EXPORTER", True)


# Default addressing style to use for boto client
MLFLOW_BOTO_CLIENT_ADDRESSING_STYLE = _EnvironmentVariable(
    "MLFLOW_BOTO_CLIENT_ADDRESSING_STYLE", str, "auto"
)

#: Specify the timeout in seconds for Databricks endpoint HTTP request retries.
MLFLOW_DATABRICKS_ENDPOINT_HTTP_RETRY_TIMEOUT = _EnvironmentVariable(
    "MLFLOW_DATABRICKS_ENDPOINT_HTTP_RETRY_TIMEOUT", int, 500
)

#: Specifies the number of connection pools to cache in urllib3. This environment variable sets the
#: `pool_connections` parameter in the `requests.adapters.HTTPAdapter` constructor. By adjusting
#: this variable, users can enhance the concurrency of HTTP requests made by MLflow.
MLFLOW_HTTP_POOL_CONNECTIONS = _EnvironmentVariable("MLFLOW_HTTP_POOL_CONNECTIONS", int, 10)

#: Specifies the maximum number of connections to keep in the HTTP connection pool. This environment
#: variable sets the `pool_maxsize` parameter in the `requests.adapters.HTTPAdapter` constructor.
#: By adjusting this variable, users can enhance the concurrency of HTTP requests made by MLflow.
MLFLOW_HTTP_POOL_MAXSIZE = _EnvironmentVariable("MLFLOW_HTTP_POOL_MAXSIZE", int, 10)

#: Enable Unity Catalog integration for MLflow AI Gateway.
#: (default: ``False``)
MLFLOW_ENABLE_UC_FUNCTIONS = _BooleanEnvironmentVariable("MLFLOW_ENABLE_UC_FUNCTIONS", False)

#: Specifies the length of time in seconds for the asynchronous logging thread to wait before
#: logging a batch.
MLFLOW_ASYNC_LOGGING_BUFFERING_SECONDS = _EnvironmentVariable(
    "MLFLOW_ASYNC_LOGGING_BUFFERING_SECONDS", int, None
)

#: Whether to enable Databricks SDK. If true, MLflow uses databricks-sdk to send HTTP requests
#: to Databricks endpoint, otherwise MLflow uses ``requests`` library to send HTTP requests
#: to Databricks endpoint. Note that if you want to use OAuth authentication, you have to
#: set this environment variable to true.
#: (default: ``True``)
MLFLOW_ENABLE_DB_SDK = _BooleanEnvironmentVariable("MLFLOW_ENABLE_DB_SDK", True)

#: A flag that's set to 'true' in the child process for capturing modules.
_MLFLOW_IN_CAPTURE_MODULE_PROCESS = _BooleanEnvironmentVariable(
    "MLFLOW_IN_CAPTURE_MODULE_PROCESS", False
)

#: Use DatabricksSDKModelsArtifactRepository when registering and loading models to and from
#: Databricks UC. This is required for SEG(Secure Egress Gateway) enabled workspaces and helps
#: eliminate models exfiltration risk associated with temporary scoped token generation used in
#: existing model artifact repo classes.
MLFLOW_USE_DATABRICKS_SDK_MODEL_ARTIFACTS_REPO_FOR_UC = _BooleanEnvironmentVariable(
    "MLFLOW_USE_DATABRICKS_SDK_MODEL_ARTIFACTS_REPO_FOR_UC", False
)

#: Disable Databricks SDK for run artifacts. We enable this by default since we want to
#: use Databricks SDK for run artifacts in most cases, but this gives us a way to disable
#: it for certain cases if needed.
MLFLOW_DISABLE_DATABRICKS_SDK_FOR_RUN_ARTIFACTS = _BooleanEnvironmentVariable(
    "MLFLOW_DISABLE_DATABRICKS_SDK_FOR_RUN_ARTIFACTS", False
)

#: Skip signature validation check when migrating model versions from Databricks Workspace
#: Model Registry to Databricks Unity Catalog Model Registry.
#: (default: ``False``)
MLFLOW_SKIP_SIGNATURE_CHECK_FOR_UC_REGISTRY_MIGRATION = _BooleanEnvironmentVariable(
    "MLFLOW_SKIP_SIGNATURE_CHECK_FOR_UC_REGISTRY_MIGRATION", False
)

# Specifies the model environment archive file downloading path when using
# ``mlflow.pyfunc.spark_udf``. (default: ``None``)
MLFLOW_MODEL_ENV_DOWNLOADING_TEMP_DIR = _EnvironmentVariable(
    "MLFLOW_MODEL_ENV_DOWNLOADING_TEMP_DIR", str, None
)

# Specifies whether to log environment variable names used during model logging.
MLFLOW_RECORD_ENV_VARS_IN_MODEL_LOGGING = _BooleanEnvironmentVariable(
    "MLFLOW_RECORD_ENV_VARS_IN_MODEL_LOGGING", True
)

#: Specifies the artifact compression method used when logging a model
#: allowed values are "lzma", "bzip2" and "gzip"
#: (default: ``None``, indicating no compression)
MLFLOW_LOG_MODEL_COMPRESSION = _EnvironmentVariable("MLFLOW_LOG_MODEL_COMPRESSION", str, None)


# Specifies whether to convert a {"messages": [{"role": "...", "content": "..."}]} input
# to a List[BaseMessage] object when invoking a PyFunc model saved with langchain flavor.
# This takes precedence over the default behavior of trying such conversion if the model
# is not an AgentExecutor and the input schema doesn't contain a 'messages' field.
MLFLOW_CONVERT_MESSAGES_DICT_FOR_LANGCHAIN = _BooleanEnvironmentVariable(
    "MLFLOW_CONVERT_MESSAGES_DICT_FOR_LANGCHAIN", None
)

#: A boolean flag which enables additional functionality in Python tests for GO backend.
_MLFLOW_GO_STORE_TESTING = _BooleanEnvironmentVariable("MLFLOW_GO_STORE_TESTING", False)

# Specifies whether the current environment is a serving environment.
# This should only be used internally by MLflow to add some additional logic when running in a
# serving environment.
_MLFLOW_IS_IN_SERVING_ENVIRONMENT = _BooleanEnvironmentVariable(
    "_MLFLOW_IS_IN_SERVING_ENVIRONMENT", None
)

#: Secret key for the Flask app. This is necessary for enabling CSRF protection
#: in the UI signup page when running the app with basic authentication enabled
MLFLOW_FLASK_SERVER_SECRET_KEY = _EnvironmentVariable("MLFLOW_FLASK_SERVER_SECRET_KEY", str, None)

#: (MLflow 3.5.0+) Comma-separated list of allowed CORS origins for the MLflow server.
#: Example: "http://localhost:3000,https://app.example.com"
#: Use "*" to allow ALL origins (DANGEROUS - only use for development!).
#: (default: ``None`` - localhost origins only)
MLFLOW_SERVER_CORS_ALLOWED_ORIGINS = _EnvironmentVariable(
    "MLFLOW_SERVER_CORS_ALLOWED_ORIGINS", str, None
)

#: (MLflow 3.5.0+) Comma-separated list of allowed Host headers for the MLflow server.
#: Example: "mlflow.company.com,mlflow.internal:5000"
#: Use "*" to allow ALL hosts (not recommended for production).
#: If not set, defaults to localhost variants and private IP ranges.
#: (default: ``None`` - localhost and private IP ranges)
MLFLOW_SERVER_ALLOWED_HOSTS = _EnvironmentVariable("MLFLOW_SERVER_ALLOWED_HOSTS", str, None)

#: (MLflow 3.5.0+) Disable all security middleware (DANGEROUS - only use for testing!).
#: Set to "true" to disable security headers, CORS protection, and host validation.
#: (default: ``"false"``)
MLFLOW_SERVER_DISABLE_SECURITY_MIDDLEWARE = _EnvironmentVariable(
    "MLFLOW_SERVER_DISABLE_SECURITY_MIDDLEWARE", str, "false"
)

#: (MLflow 3.5.0+) X-Frame-Options header value for clickjacking protection.
#: Options: "SAMEORIGIN" (default), "DENY", or "NONE" (disable).
#: Set to "NONE" to allow embedding MLflow UI in iframes from different origins.
#: (default: ``"SAMEORIGIN"``)
MLFLOW_SERVER_X_FRAME_OPTIONS = _EnvironmentVariable(
    "MLFLOW_SERVER_X_FRAME_OPTIONS", str, "SAMEORIGIN"
)

#: Specifies the max length (in chars) of an experiment's artifact location.
#: The default is 2048.
MLFLOW_ARTIFACT_LOCATION_MAX_LENGTH = _EnvironmentVariable(
    "MLFLOW_ARTIFACT_LOCATION_MAX_LENGTH", int, 2048
)

#: Path to SSL CA certificate file for MySQL connections
#: Used when creating a SQLAlchemy engine for MySQL
#: (default: ``None``)
MLFLOW_MYSQL_SSL_CA = _EnvironmentVariable("MLFLOW_MYSQL_SSL_CA", str, None)

#: Path to SSL certificate file for MySQL connections
#: Used when creating a SQLAlchemy engine for MySQL
#: (default: ``None``)
MLFLOW_MYSQL_SSL_CERT = _EnvironmentVariable("MLFLOW_MYSQL_SSL_CERT", str, None)

#: Path to SSL key file for MySQL connections
#: Used when creating a SQLAlchemy engine for MySQL
#: (default: ``None``)
MLFLOW_MYSQL_SSL_KEY = _EnvironmentVariable("MLFLOW_MYSQL_SSL_KEY", str, None)

#: Specifies the Databricks traffic ID to inject as x-databricks-traffic-id header
#: in HTTP requests to Databricks endpoints
#: (default: ``None``)
_MLFLOW_DATABRICKS_TRAFFIC_ID = _EnvironmentVariable("MLFLOW_DATABRICKS_TRAFFIC_ID", str, None)

#######################################################################################
# Tracing
#######################################################################################

#: Specifies whether to enable async trace logging to Databricks Tracing Server.
#: TODO: Update OSS MLflow Server to logging async by default
#: Default: ``True``.
MLFLOW_ENABLE_ASYNC_TRACE_LOGGING = _BooleanEnvironmentVariable(
    "MLFLOW_ENABLE_ASYNC_TRACE_LOGGING", True
)

#: Maximum number of worker threads to use for async trace logging.
#: (default: ``10``)
MLFLOW_ASYNC_TRACE_LOGGING_MAX_WORKERS = _EnvironmentVariable(
    "MLFLOW_ASYNC_TRACE_LOGGING_MAX_WORKERS", int, 10
)

#: Maximum number of export tasks to queue for async trace logging.
#: When the queue is full, new export tasks will be dropped.
#: (default: ``1000``)
MLFLOW_ASYNC_TRACE_LOGGING_MAX_QUEUE_SIZE = _EnvironmentVariable(
    "MLFLOW_ASYNC_TRACE_LOGGING_MAX_QUEUE_SIZE", int, 1000
)


#: Timeout seconds for retrying trace logging.
#: (default: ``500``)
MLFLOW_ASYNC_TRACE_LOGGING_RETRY_TIMEOUT = _EnvironmentVariable(
    "MLFLOW_ASYNC_TRACE_LOGGING_RETRY_TIMEOUT", int, 500
)

#: Specifies the SQL warehouse ID to use for tracing with Databricks backend.
#: (default: ``None``)
MLFLOW_TRACING_SQL_WAREHOUSE_ID = _EnvironmentVariable("MLFLOW_TRACING_SQL_WAREHOUSE_ID", str, None)


#: Specifies the location to send traces to. This can be either an MLflow experiment ID or a
#: Databricks Unity Catalog (UC) schema (format: `<catalog_name>.<schema_name>`).
#: (default: ``None`` (an active MLflow experiment will be used))
MLFLOW_TRACING_DESTINATION = _EnvironmentVariable("MLFLOW_TRACING_DESTINATION", str, None)


#######################################################################################
# Model Logging
#######################################################################################

#: The default active LoggedModel ID. Traces created while this variable is set (unless overridden,
#: e.g., by the `set_active_model()` API) will be associated with this LoggedModel ID.
#: (default: ``None``)
MLFLOW_ACTIVE_MODEL_ID = _EnvironmentVariable("MLFLOW_ACTIVE_MODEL_ID", str, None)

#: Legacy environment variable for setting the default active LoggedModel ID.
#: This should only by used by MLflow internally. Users should use the
#: public `MLFLOW_ACTIVE_MODEL_ID` environment variable or the `set_active_model`
#: API to set the active LoggedModel, and should not set this environment variable directly.
#: (default: ``None``)
_MLFLOW_ACTIVE_MODEL_ID = _EnvironmentVariable("_MLFLOW_ACTIVE_MODEL_ID", str, None)

#: Maximum number of parameters to include in the initial CreateLoggedModel request.
#: Additional parameters will be logged in separate requests.
#: (default: ``100``)
_MLFLOW_CREATE_LOGGED_MODEL_PARAMS_BATCH_SIZE = _EnvironmentVariable(
    "_MLFLOW_CREATE_LOGGED_MODEL_PARAMS_BATCH_SIZE", int, 100
)


#: Maximum number of parameters to include in each batch when logging parameters
#: for a logged model.
#: (default: ``100``)
_MLFLOW_LOG_LOGGED_MODEL_PARAMS_BATCH_SIZE = _EnvironmentVariable(
    "_MLFLOW_LOG_LOGGED_MODEL_PARAMS_BATCH_SIZE", int, 100
)

#: A boolean flag that enables printing URLs for logged and registered models when
#: they are created.
#: (default: ``True``)
MLFLOW_PRINT_MODEL_URLS_ON_CREATION = _BooleanEnvironmentVariable(
    "MLFLOW_PRINT_MODEL_URLS_ON_CREATION", True
)

#: Maximum number of threads to use when downloading traces during search operations.
#: (default: ``max(32, (# of system CPUs * 4)``)
MLFLOW_SEARCH_TRACES_MAX_THREADS = _EnvironmentVariable(
    # Threads used to download traces during search are network IO-bound (waiting for downloads)
    # rather than CPU-bound, so we want more threads than CPU cores
    "MLFLOW_SEARCH_TRACES_MAX_THREADS",
    int,
    max(32, (os.cpu_count() or 1) * 4),
)

#: Maximum number of traces to fetch in a single BatchGetTraces request during search operations.
#: (default: ``10``)
_MLFLOW_SEARCH_TRACES_MAX_BATCH_SIZE = _EnvironmentVariable(
    "MLFLOW_SEARCH_TRACES_MAX_BATCH_SIZE", int, 10
)

#: Specifies the logging level for MLflow. This can be set to any valid logging level
#: (e.g., "DEBUG", "INFO"). This environment must be set before importing mlflow to take
#: effect. To modify the logging level after importing mlflow, use `importlib.reload(mlflow)`.
#: (default: ``None``).
MLFLOW_LOGGING_LEVEL = _EnvironmentVariable("MLFLOW_LOGGING_LEVEL", str, None)

#: Avoid printing experiment and run url to stdout at run termination
#: (default: ``False``)
MLFLOW_SUPPRESS_PRINTING_URL_TO_STDOUT = _BooleanEnvironmentVariable(
    "MLFLOW_SUPPRESS_PRINTING_URL_TO_STDOUT", False
)

#: If True, MLflow locks both direct and transitive model dependencies when logging a model.
#: (default: ``False``).
MLFLOW_LOCK_MODEL_DEPENDENCIES = _BooleanEnvironmentVariable(
    "MLFLOW_LOCK_MODEL_DEPENDENCIES", False
)

#: If specified, tracking server rejects model `/mlflow/model-versions/create` requests with
#: a source that does not match the specified regular expression.
#: (default: ``None``).
MLFLOW_CREATE_MODEL_VERSION_SOURCE_VALIDATION_REGEX = _EnvironmentVariable(
    "MLFLOW_CREATE_MODEL_VERSION_SOURCE_VALIDATION_REGEX", str, None
)

#: Maximum number of root fields to include in the MLflow server GraphQL request.
#: (default: ``10``)
MLFLOW_SERVER_GRAPHQL_MAX_ROOT_FIELDS = _EnvironmentVariable(
    "MLFLOW_SERVER_GRAPHQL_MAX_ROOT_FIELDS", int, 10
)

#: Maximum number of aliases to include in the MLflow server GraphQL request.
#: (default: ``10``)
MLFLOW_SERVER_GRAPHQL_MAX_ALIASES = _EnvironmentVariable(
    "MLFLOW_SERVER_GRAPHQL_MAX_ALIASES", int, 10
)


#: Whether to disable schema details in error messages for MLflow schema enforcement.
#: (default: ``False``)
MLFLOW_DISABLE_SCHEMA_DETAILS = _BooleanEnvironmentVariable("MLFLOW_DISABLE_SCHEMA_DETAILS", False)


def _split_strip(s: str) -> list[str]:
    return [s.strip() for s in s.split(",")]


# Specifies the allowed schemes for MLflow webhook URLs.
# This environment variable is not intended for production use.
_MLFLOW_WEBHOOK_ALLOWED_SCHEMES = _EnvironmentVariable(
    "MLFLOW_WEBHOOK_ALLOWED_SCHEMES", _split_strip, ["https"]
)


#: Specifies the secret key used to encrypt webhook secrets in MLflow.
MLFLOW_WEBHOOK_SECRET_ENCRYPTION_KEY = _EnvironmentVariable(
    "MLFLOW_WEBHOOK_SECRET_ENCRYPTION_KEY", str, None
)

#: Specifies the timeout in seconds for webhook HTTP requests
#: (default: ``30``)
MLFLOW_WEBHOOK_REQUEST_TIMEOUT = _EnvironmentVariable("MLFLOW_WEBHOOK_REQUEST_TIMEOUT", int, 30)

#: Specifies the maximum number of threads for webhook delivery thread pool
#: (default: ``10``)
MLFLOW_WEBHOOK_DELIVERY_MAX_WORKERS = _EnvironmentVariable(
    "MLFLOW_WEBHOOK_DELIVERY_MAX_WORKERS", int, 10
)

#: Specifies the maximum number of retries for webhook HTTP requests
#: (default: ``3``)
MLFLOW_WEBHOOK_REQUEST_MAX_RETRIES = _EnvironmentVariable(
    "MLFLOW_WEBHOOK_REQUEST_MAX_RETRIES", int, 3
)

#: Specifies the TTL in seconds for webhook list cache
#: (default: ``60``)
MLFLOW_WEBHOOK_CACHE_TTL = _EnvironmentVariable("MLFLOW_WEBHOOK_CACHE_TTL", int, 60)


#: Whether to disable telemetry collection in MLflow. If set to True, no telemetry
#: data will be collected. (default: ``False``)
MLFLOW_DISABLE_TELEMETRY = _BooleanEnvironmentVariable("MLFLOW_DISABLE_TELEMETRY", False)


#: Internal flag to enable telemetry in mlflow tests.
#: (default: ``False``)
_MLFLOW_TESTING_TELEMETRY = _BooleanEnvironmentVariable("_MLFLOW_TESTING_TELEMETRY", False)


#: Internal environment variable to set the telemetry session id when TelemetryClient is initialized
#: This should never be set by users or explicitly.
#: (default: ``None``)
_MLFLOW_TELEMETRY_SESSION_ID = _EnvironmentVariable("_MLFLOW_TELEMETRY_SESSION_ID", str, None)


#: Internal flag to enable telemetry logging
#: (default: ``False``)
_MLFLOW_TELEMETRY_LOGGING = _BooleanEnvironmentVariable("_MLFLOW_TELEMETRY_LOGGING", False)

#: Internal environment variable to indicate which SGI is being used,
#: e.g. "uvicorn" or "gunicorn".
#: This should never be set by users or explicitly.
#: (default: ``None``)
_MLFLOW_SGI_NAME = _EnvironmentVariable("_MLFLOW_SGI_NAME", str, None)

#: Specifies whether to enforce using stdin scoring server in Spark udf.
#: (default: ``True``)
MLFLOW_ENFORCE_STDIN_SCORING_SERVER_FOR_SPARK_UDF = _BooleanEnvironmentVariable(
    "MLFLOW_ENFORCE_STDIN_SCORING_SERVER_FOR_SPARK_UDF", True
)

#: Specifies whether to enable job execution feature for MLflow server.
#: This feature requires "huey" package dependency, and requires MLflow server to configure
#: --backend-store-uri to database URI.
#: (default: ``False``)
MLFLOW_SERVER_ENABLE_JOB_EXECUTION = _BooleanEnvironmentVariable(
    "MLFLOW_SERVER_ENABLE_JOB_EXECUTION", False
)

#: Specifies MLflow server job maximum allowed retries for transient errors.
#: (default: ``3``)
MLFLOW_SERVER_JOB_TRANSIENT_ERROR_MAX_RETRIES = _EnvironmentVariable(
    "MLFLOW_SERVER_JOB_TRANSIENT_ERROR_MAX_RETRIES", int, 3
)

#: Specifies MLflow server job retry base delay in seconds for transient errors.
#: The retry uses exponential backoff strategy, retry delay is computed by
#: `delay = min(base_delay * (2 ** (retry_count - 1)), max_delay)`
#: (default: ``15``)
MLFLOW_SERVER_JOB_TRANSIENT_ERROR_RETRY_BASE_DELAY = _EnvironmentVariable(
    "MLFLOW_SERVER_JOB_TRANSIENT_ERROR_RETRY_BASE_DELAY", int, 15
)

#: Specifies MLflow server job retry maximum delay in seconds for transient errors.
#: The retry uses exponential backoff strategy, retry delay is computed by
#: `delay = min(base_delay * (2 ** (retry_count - 1)), max_delay)`
#: (default: ``60``)
MLFLOW_SERVER_JOB_TRANSIENT_ERROR_RETRY_MAX_DELAY = _EnvironmentVariable(
    "MLFLOW_SERVER_JOB_TRANSIENT_ERROR_RETRY_MAX_DELAY", int, 60
)


#: Specifies the maximum number of completion iterations allowed when invoking
#: judge models. This prevents infinite loops in case of complex traces or
#: issues with the judge's reasoning.
#: (default: ``30``)
MLFLOW_JUDGE_MAX_ITERATIONS = _EnvironmentVariable("MLFLOW_JUDGE_MAX_ITERATIONS", int, 30)


--- requirements/constraints.txt ---
pytest==8.4.0
# transformers 4.51.0 has this issue:
# https://github.com/huggingface/transformers/issues/37326
transformers!=4.51.0
# https://github.com/BerriAI/litellm/issues/10373
litellm!=1.67.4
# https://github.com/run-llama/llama_index/issues/18587
llama-index-core!=0.12.34
# https://github.com/mangiucugna/json_repair/issues/124
json_repair!=0.45.0
# https://github.com/huggingface/transformers/issues/38269
transformers!=4.52.2
transformers!=4.52.1
# TODO(https://github.com/mlflow/mlflow/issues/15847): Remove this constraint when MLflow is ready for pyspark 4.0.0. Pyspark 3.5.6 has the same issue.
pyspark<3.5.6
# https://github.com/pallets/click/issues/3065
# click 8.3.0 has a bug that causes tests to hang
click!=8.3.0
# xgboost 3.1.0 changed base_score format to vector for multi-output models, breaking shap compatibility
# https://xgboost.readthedocs.io/en/latest/changes/v3.1.0.html#multi-target-class-intercept
xgboost<3.1.0


--- requirements/dev-requirements.txt ---
-r extra-ml-requirements.txt
-r test-requirements.txt
-r lint-requirements.txt
-r doc-requirements.txt


--- requirements/doc-min-requirements.txt ---
# Minimum version that works with Python 3.10
sphinx==4.2.0
jinja2==3.0.3
# to be compatible with jinja2==3.0.3
flask<=2.2.5
sphinx-autobuild
sphinx-click
# to be compatible with docutils==0.16
sphinx-tabs==3.2.0
# redirect handling
sphinx-reredirects==0.1.3
# Pin sphinxcontrib packages. Their newer versions are incompatible with sphinx==4.2.0.
sphinxcontrib-applehelp<1.0.8
sphinxcontrib-devhelp<1.0.6
sphinxcontrib-htmlhelp<2.0.4
sphinxcontrib-serializinghtml<1.1.10
sphinxcontrib-qthelp<1.0.7


--- requirements/doc-requirements.txt ---
-r doc-min-requirements.txt
tensorflow-cpu<=2.12.0; platform_system!="Darwin" or platform_machine!="arm64"
tensorflow-macos<=2.12.0; platform_system=="Darwin" and platform_machine=="arm64"
pyspark
datasets
# nbsphinx and ipython are required for jupyter notebook rendering
nbsphinx==0.8.8
# ipython 8.7.0 is an incompatible release
ipython!=8.7.0
keras
torch>=1.11.0
torchvision>=0.12.0
lightning>=1.8.1
scrapy
ipywidgets>=8.1.1
# incremental==24.7.0 requires setuptools>=61.0, which causes https://github.com/mlflow/mlflow/issues/8635
incremental<24.7.0
# this is an extra dependency for the auth app which
# is not included in the core mlflow requirements
Flask-WTF<2
# required for testing polars dataset integration
polars>=1
# required for the genai evaluation example
openai
# required for the haystack
haystack-ai


--- requirements/extra-ml-requirements.txt ---
## This file describes extra ML library dependencies that you, as an end user,
## must install in order to use various MLflow Python modules.
# Required by mlflow.spacy
# TODO: Remove `<3.8` once we bump the minimim supported python version of MLflow to 3.9.
spacy>=3.3.0,<3.8
# Required by mlflow.tensorflow
tensorflow>=2.10.0; platform_system!="Darwin" or platform_machine!="arm64"
tensorflow-macos>=2.10.0; platform_system=="Darwin" and platform_machine=="arm64"
# Required by mlflow.pytorch
torch>=1.11.0
torchvision>=0.12.0
lightning>=1.8.1
# Required by mlflow.xgboost
xgboost>=0.82
# Required by mlflow.lightgbm
lightgbm
# Required by mlflow.catboost
catboost
# Required by mlflow.statsmodels
statsmodels
# Required by mlflow.h2o
h2o
# Required by mlflow.onnx
onnx>=1.17.0
onnxruntime
onnxscript
tf2onnx
# Required by mlflow.spark and using Delta with MLflow Tracking datasets
pyspark
# Required by mlflow.paddle
paddlepaddle
# Required by mlflow.prophet
# NOTE: Prophet's whl build process will fail with dependencies not being present.
#   Installation will default to setup.py in order to install correctly.
#   To install in dev environment, ensure that gcc>=8 is installed to allow pystan
#   to compile the model binaries. See: https://gcc.gnu.org/install/
# Avoid 0.25 due to https://github.com/dr-prodigy/python-holidays/issues/1200
holidays!=0.25
prophet
# Required by mlflow.shap
# and shap evaluation functionality
shap>=0.42.1
# Required by mlflow.pmdarima
pmdarima
# Required by mlflow.diviner
diviner
# Required for using Hugging Face datasets with MLflow Tracking
# Avoid datasets < 2.19.1 due to an incompatibility issue https://github.com/huggingface/datasets/issues/6737
datasets>=2.19.1
# Required by mlflow.transformers
transformers
sentencepiece
setfit
librosa
ffmpeg
accelerate
# Required by mlflow.openai
openai
tiktoken
tenacity
# Required by mlflow.llama_index
llama_index
# Required for an agent example of mlflow.llama_index
llama-index-agent-openai
# Required by mlflow.langchain
langchain
# Required by mlflow.promptflow
promptflow
# Required by mlflow.sentence_transformers
sentence-transformers
# Required by mlflow.anthropic
anthropic
# Required by mlflow.ag2
ag2
# Required by mlflow.dspy
# In dspy 2.6.9, `dspy.__name__` is not 'dspy', but 'dspy.__metadata__',
# which causes auto-logging tests to fail.
dspy!=2.6.9
# Required by mlflow.litellm
litellm
# Required by mlflow.gemini
google-genai
# Required by mlflow.groq
groq
# Required by mlflow.mistral
mistralai
# Required by mlflow.autogen
autogen-agentchat
# Required by mlflow.semantic_kernel
semantic-kernel
# Required by mlflow.agno
agno
# Required by mlflow.strands
strands-agents
# Required by mlflow.haystack
haystack-ai


--- requirements/lint-requirements.txt ---
ruff==0.12.10
black==23.7.0
blacken-docs==1.18.0
pre-commit==4.0.1
toml==0.10.2
mypy==1.17.1
pytest==8.4.0
pydantic==2.11.7
-e ./dev/clint


--- requirements/skinny-test-requirements.txt ---
## Test-only dependencies
pytest
pytest-cov


--- requirements/test-requirements.txt ---
## Dependencies required to run tests
## Test-only dependencies
pytest
pytest-asyncio
pytest-repeat
pytest-cov
pytest-timeout
moto>=4.2.0,<5,!=4.2.5
azure-storage-blob>=12.0.0
azure-storage-file-datalake>=12.9.1
azure-identity>=1.6.1
pillow
plotly
kaleido
# Required by evaluator tests
shap
# Required to evaluate language models in `mlflow.evaluate`
evaluate
nltk
rouge_score
textstat
tiktoken
# Required by progress bar tests
tqdm[notebook]
# Required for LLM eval in `mlflow.evaluate`
openai
# Required for showing pytest stats
psutil
pyspark
# Required for testing the opentelemetry exporter of tracing
opentelemetry-exporter-otlp-proto-grpc
opentelemetry-exporter-otlp-proto-http
# Required for testing mlflow.server.auth
Flask-WTF<2
# required for testing polars dataset integration
polars>=1
# required for testing mlflow.genai.optimize_prompt
dspy
# required for testing mlflow.genai.optimize.optimizers
gepa
# required for testing mlflow.server.jobs
huey<3,>=2.5.0


--- tests/db/README.md ---
# Instructions

This directory contains files to test MLflow tracking operations using the following databases:

- PostgreSQL
- MySQL
- Microsoft SQL Server
- SQLite

## Prerequisites

- Docker
- Docker Compose V2

## Build Services

```bash
# Build a service
service=mlflow-sqlite
./tests/db/compose.sh build --build-arg DEPENDENCIES="$(python dev/extract_deps.py)" $service

# Build all services
./tests/db/compose.sh build --build-arg DEPENDENCIES="$(python dev/extract_deps.py)"
```

## Run Services

```bash
# Run a service (`pytest tests/db` is executed by default)
./tests/db/compose.sh run --rm $service

# Run all services
for service in $(./tests/db/compose.sh config --services | grep '^mlflow-')
do
  ./tests/db/compose.sh run --rm "$service"
done

# Run tests
./tests/db/compose.sh run --rm $service pytest /path/to/directory/or/script

# Run a python script
./tests/db/compose.sh run --rm $service python /path/to/script
```

## Clean Up Services

```bash
# Clean up containers, networks, and volumes
./tests/db/compose.sh down --volumes --remove-orphans

# Clean up containers, networks, volumes, and images
./tests/db/compose.sh down --volumes --remove-orphans --rmi all
```

## Other Useful Commands

```bash
# View database logs
./tests/db/compose.sh logs --follow <database service>
```
