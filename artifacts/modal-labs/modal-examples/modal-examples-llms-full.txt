# llms-full (private-aware)
> Built from GitHub files and website pages. Large files may be truncated.

--- 02_building_containers/install_cuda.py ---
# # Installing the CUDA Toolkit on Modal

# This code sample is intended to quickly show how different layers of the CUDA stack are used on Modal.
# For greater detail, see our [guide to using CUDA on Modal](https://modal.com/docs/guide/cuda).

# All Modal Functions with GPUs already have the NVIDIA CUDA drivers,
# NVIDIA System Management Interface, and CUDA Driver API installed.

import modal

app = modal.App("example-install-cuda")


@app.function(gpu="T4")
def nvidia_smi():
    import subprocess

    subprocess.run(["nvidia-smi"], check=True)


# This is enough to install and use many CUDA-dependent libraries, like PyTorch.


@app.function(gpu="T4", image=modal.Image.debian_slim().uv_pip_install("torch"))
def torch_cuda():
    import torch

    print(torch.cuda.get_device_properties("cuda:0"))


# If your application or its dependencies need components of the CUDA toolkit,
# like the `nvcc` compiler driver, installed as system libraries or command-line tools,
# you'll need to install those manually.

# We recommend the official NVIDIA CUDA Docker images from Docker Hub.
# You'll need to add Python 3 and pip with the `add_python` option because the image
# doesn't have these by default.


ctk_image = modal.Image.from_registry(
    "nvidia/cuda:12.4.0-devel-ubuntu22.04", add_python="3.11"
).entrypoint([])  # removes chatty prints on entry


@app.function(gpu="T4", image=ctk_image)
def nvcc_version():
    import subprocess

    return subprocess.run(["nvcc", "--version"], check=True)


# You can check that all these functions run by invoking this script with `modal run`.


@app.local_entrypoint()
def main():
    nvidia_smi.remote()
    torch_cuda.remote()
    nvcc_version.remote()


## Links discovered
- [guide to using CUDA on Modal](https://modal.com/docs/guide/cuda)

--- 02_building_containers/install_flash_attn.py ---
# # Install Flash Attention on Modal

# FlashAttention is an optimized CUDA library for Transformer
# scaled-dot-product attention. Dao AI Lab now publishes pre-compiled
# wheels, which makes installation quick.  This script shows how to
# 1. Pin an exact wheel that matches CUDA 12 / PyTorch 2.6 / Python 3.13.
# 2. Build a Modal image that installs torch, numpy, and FlashAttention.
# 3. Launch a GPU function to confirm the kernel runs on a GPU.

import modal

app = modal.App("example-install-flash-attn")

# You need to specify an exact release wheel. You can find
# [more on their github](https://github.com/Dao-AILab/flash-attention/releases).

flash_attn_release = (
    "https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/"
    "flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp313-cp313-linux_x86_64.whl"
)

image = modal.Image.debian_slim(python_version="3.13").uv_pip_install(
    "torch==2.6.0", "numpy==2.2.4", flash_attn_release
)


# And here is a demo verifying that it works:


@app.function(gpu="L40S", image=image)
def run_flash_attn():
    import torch
    from flash_attn import flash_attn_func

    batch_size, seqlen, nheads, headdim, nheads_k = 2, 4, 3, 16, 3

    q = torch.randn(batch_size, seqlen, nheads, headdim, dtype=torch.float16).to("cuda")
    k = torch.randn(batch_size, seqlen, nheads_k, headdim, dtype=torch.float16).to(
        "cuda"
    )
    v = torch.randn(batch_size, seqlen, nheads_k, headdim, dtype=torch.float16).to(
        "cuda"
    )

    out = flash_attn_func(q, k, v)
    assert out.shape == (batch_size, seqlen, nheads, headdim)


## Links discovered
- [more on their github](https://github.com/Dao-AILab/flash-attention/releases)

--- 01_getting_started/generators.py ---
# # Run a generator function on Modal

# This example shows how you can run a generator function on Modal. We define a
# function that `yields` values and then call it with the [`remote_gen`](https://modal.com/docs/reference/modal.Function#remote_gen) method. The
# `remote_gen` method returns a generator object that can be used to iterate over
# the values produced by the function.

import modal

app = modal.App("example-generators")


@app.function()
def f(i):
    for j in range(i):
        yield j


@app.local_entrypoint()
def main():
    for r in f.remote_gen(10):
        print(r)


## Links discovered
- [`remote_gen`](https://modal.com/docs/reference/modal.Function#remote_gen)

--- 01_getting_started/get_started.py ---
import modal

app = modal.App("example-get-started")


@app.function()
def square(x):
    print("This code is running on a remote worker!")
    return x**2


@app.local_entrypoint()
def main():
    print("the square is", square.remote(42))


--- 01_getting_started/hello_world.py ---
# # Hello, world!

# This tutorial demonstrates some core features of Modal:

# * You can run functions on Modal just as easily as you run them locally.
# * Running functions in parallel on Modal is simple and fast.
# * Logs and errors show up immediately, even for functions running on Modal.

# ## Importing Modal and setting up

# We start by importing `modal` and creating a `App`.
# We build up this `App` to [define our application](https://modal.com/docs/guide/apps).

import sys

import modal

app = modal.App("example-hello-world")

# ## Defining a function

# Modal takes code and runs it in the cloud.

# So first we've got to write some code.

# Let's write a simple function that takes in an input,
# prints a log or an error to the console,
# and then returns an output.

# To make this function work with Modal, we just wrap it in a decorator,
# [`@app.function`](https://modal.com/docs/reference/modal.App#function).


@app.function()
def f(i):
    if i % 2 == 0:
        print("hello", i)
    else:
        print("world", i, file=sys.stderr)

    return i * i


# ## Running our function locally, remotely, and in parallel

# Now let's see three different ways we can call that function:

# 1. As a regular call on your `local` machine, with `f.local`

# 2. As a `remote` call that runs in the cloud, with `f.remote`

# 3. By `map`ping many copies of `f` in the cloud over many inputs, with `f.map`

# We call `f` in each of these ways inside the `main` function below.


@app.local_entrypoint()
def main():
    # run the function locally
    print(f.local(1000))

    # run the function remotely on Modal
    print(f.remote(1000))

    # run the function in parallel and remotely on Modal
    total = 0
    for ret in f.map(range(200)):
        total += ret

    print(total)


# Enter `modal run hello_world.py` in a shell, and you'll see a Modal app initialize.
# You'll then see the `print`ed logs of
# the `main` function and, mixed in with them, all the logs of `f` as it is run
# locally, then remotely, and then remotely and in parallel.

# That's all triggered by adding the
# [`@app.local_entrypoint`](https://modal.com/docs/reference/modal.App#local_entrypoint)
# decorator on `main`, which defines it as the function to start from locally when we invoke `modal run`.

# ## What just happened?

# When we called `.remote` on `f`, the function was executed
# _in the cloud_, on Modal's infrastructure, not on the local machine.

# In short, we took the function `f`, put it inside a container,
# sent it the inputs, and streamed back the logs and outputs.

# ## But why does this matter?

# Try one of these things next to start seeing the full power of Modal!

# ### You can change the code and run it again

# For instance, change the `print` statement in the function `f`
# to print `"spam"` and `"eggs"` instead and run the app again.
# You'll see that that your new code is run with no extra work from you --
# and it should even run faster!

# Modal's goal is to make running code in the cloud feel like you're
# running code locally. That means no waiting for long image builds when you've just moved a comma,
# no fiddling with container image pushes, and no context-switching to a web UI to inspect logs.

# ### You can map over more data

# Change the `map` range from `200` to some large number, like `1170`. You'll see
# Modal create and run even more containers in parallel this time.

# And it'll happen lightning fast!

# ### You can run a more interesting function

# The function `f` is a bit silly and doesn't do much, but in its place
# imagine something that matters to you, like:

# * Running [language model inference](https://modal.com/docs/examples/vllm_inference)
# or [fine-tuning](https://modal.com/docs/examples/slack-finetune)
# * Manipulating [audio](https://modal.com/docs/examples/musicgen)
# or [images](https://modal.com/docs/examples/diffusers_lora_finetune)
# * [Embedding huge text datasets](https://modal.com/docs/examples/amazon_embeddings) at lightning fast speeds

# Modal lets you parallelize that operation effortlessly by running hundreds or
# thousands of containers in the cloud.


## Links discovered
- [define our application](https://modal.com/docs/guide/apps)
- [`@app.function`](https://modal.com/docs/reference/modal.App#function)
- [`@app.local_entrypoint`](https://modal.com/docs/reference/modal.App#local_entrypoint)
- [language model inference](https://modal.com/docs/examples/vllm_inference)
- [fine-tuning](https://modal.com/docs/examples/slack-finetune)
- [audio](https://modal.com/docs/examples/musicgen)
- [images](https://modal.com/docs/examples/diffusers_lora_finetune)
- [Embedding huge text datasets](https://modal.com/docs/examples/amazon_embeddings)

--- 01_getting_started/inference.py ---
from pathlib import Path

import modal

app = modal.App("example-inference")
image = modal.Image.debian_slim().uv_pip_install("transformers[torch]")


@app.function(gpu="h100", image=image)
def chat(prompt: str | None = None) -> list[dict]:
    from transformers import pipeline

    if prompt is None:
        prompt = f"/no_think Read this code.\n\n{Path(__file__).read_text()}\nIn one paragraph, what does the code do?"

    print(prompt)
    context = [{"role": "user", "content": prompt}]

    chatbot = pipeline(
        model="Qwen/Qwen3-1.7B-FP8", device_map="cuda", max_new_tokens=1024
    )
    result = chatbot(context)
    print(result[0]["generated_text"][-1]["content"])

    return result


--- 01_getting_started/inference_endpoint.py ---
# ---
# cmd: ["modal", "serve", "01_getting_started/inference_endpoint.py"]
# ---
from pathlib import Path

import modal

app = modal.App("example-inference-endpoint")
image = (
    modal.Image.debian_slim()
    .uv_pip_install("transformers[torch]")
    .uv_pip_install("fastapi")
)


@app.function(gpu="h100", image=image)
@modal.fastapi_endpoint(docs=True)
def chat(prompt: str | None = None) -> list[dict]:
    from transformers import pipeline

    if prompt is None:
        prompt = f"/no_think Read this code.\n\n{Path(__file__).read_text()}\nIn one paragraph, what does the code do?"

    print(prompt)
    context = [{"role": "user", "content": prompt}]

    chatbot = pipeline(
        model="Qwen/Qwen3-1.7B-FP8", device_map="cuda", max_new_tokens=1024
    )
    result = chatbot(context)
    print(result[0]["generated_text"][-1]["content"])

    return result


--- 01_getting_started/inference_full.py ---
# ---
# cmd: ["python", "01_getting_started/inference_full.py"]
# deploy: true
# mypy: ignore-errors
# ---
from pathlib import Path

import modal

app = modal.App("example-inference-full")
image = (
    modal.Image.debian_slim()
    .uv_pip_install("transformers[torch]")
    .uv_pip_install("fastapi")
)

with image.imports():
    from transformers import pipeline

weights_cache = {
    "/root/.cache/huggingface": modal.Volume.from_name(
        "example-inference", create_if_missing=True
    )
}


@app.cls(gpu="h100", image=image, volumes=weights_cache, enable_memory_snapshot=True)
class Chat:
    @modal.enter()
    def init(self):
        self.chatbot = pipeline(
            model="Qwen/Qwen3-1.7B-FP8", device_map="cuda", max_new_tokens=1024
        )

    @modal.fastapi_endpoint(docs=True)
    def web(self, prompt: str | None = None) -> list[dict]:
        result = self.run.local(prompt)
        return result

    @modal.method()
    def run(self, prompt: str | None = None) -> list[dict]:
        if prompt is None:
            prompt = f"/no_think Read this code.\n\n{Path(__file__).read_text()}\nIn one paragraph, what does the code do?"

        print(prompt)
        context = [{"role": "user", "content": prompt}]

        result = self.chatbot(context)
        print(result[0]["generated_text"][-1]["content"])

        return result


@app.local_entrypoint()
def main():
    import glob

    chat = Chat()
    root_dir, examples = Path(__file__).parent.parent, []
    for path in glob.glob("**/*.py", root_dir=root_dir):
        examples.append(
            f"/no_think Read this code.\n\n{(root_dir / path).read_text()}\nIn one paragraph, what does the code do?"
        )

    for result in chat.run.map(examples):
        print(result[0]["generated_text"][-1]["content"])


if __name__ == "__main__":
    import json
    import urllib.request
    from datetime import datetime

    ChatCls = modal.Cls.from_name(app.name, "Chat")
    chat = ChatCls()
    print(datetime.now(), "making .remote call to Chat.run")
    print(chat.run.remote())
    print(datetime.now(), "making web request to", url := chat.web.get_web_url())

    with urllib.request.urlopen(url) as response:
        print(datetime.now())
        print(json.loads(response.read().decode("utf-8")))


--- 01_getting_started/inference_map.py ---
from pathlib import Path

import modal

app = modal.App("example-inference-map")
image = modal.Image.debian_slim().uv_pip_install("transformers[torch]")


@app.function(gpu="h100", image=image)
def chat(prompt: str | None = None) -> list[dict]:
    from transformers import pipeline

    if prompt is None:
        prompt = f"/no_think Read this code.\n\n{Path(__file__).read_text()}\nIn one paragraph, what does the code do?"

    print(prompt)
    context = [{"role": "user", "content": prompt}]

    chatbot = pipeline(
        model="Qwen/Qwen3-1.7B-FP8", device_map="cuda", max_new_tokens=1024
    )
    result = chatbot(context)
    print(result[0]["generated_text"][-1]["content"])

    return result


@app.local_entrypoint()
def main():
    import glob

    root_dir, examples = Path(__file__).parent.parent, []
    for path in glob.glob("**/*.py", root_dir=root_dir):
        examples.append(
            f"/no_think Read this code.\n\n{(root_dir / path).read_text()}\nIn one paragraph, what does the code do?"
        )

    for result in chat.map(examples):
        print(result[0]["generated_text"][-1]["content"])


--- 01_getting_started/inference_perf.py ---
# ---
# cmd: ["python", "01_getting_started/inference_perf.py"]
# deploy: true
# mypy: ignore-errors
# ---
from pathlib import Path

import modal

app = modal.App("example-inference-perf")
image = (
    modal.Image.debian_slim()
    .uv_pip_install("transformers[torch]")
    .uv_pip_install("fastapi")
)

with image.imports():
    from transformers import pipeline

weights_cache = {
    "/root/.cache/huggingface": modal.Volume.from_name(
        "example-inference", create_if_missing=True
    )
}


@app.cls(gpu="h100", image=image, volumes=weights_cache, enable_memory_snapshot=True)
class Chat:
    @modal.enter()
    def init(self):
        self.chatbot = pipeline(
            model="Qwen/Qwen3-1.7B-FP8", device_map="cuda", max_new_tokens=1024
        )

    @modal.fastapi_endpoint(docs=True)
    def web(self, prompt: str | None = None) -> list[dict]:
        result = self.run.local(prompt)
        return result

    @modal.method()
    def run(self, prompt: str | None = None) -> list[dict]:
        if prompt is None:
            prompt = f"/no_think Read this code.\n\n{Path(__file__).read_text()}\nIn one paragraph, what does the code do?"

        print(prompt)
        context = [{"role": "user", "content": prompt}]

        result = self.chatbot(context)
        print(result[0]["generated_text"][-1]["content"])

        return result


if __name__ == "__main__":
    import json
    import urllib.request
    from datetime import datetime

    ChatCls = modal.Cls.from_name(app.name, "Chat")
    chat = ChatCls()
    print(datetime.now(), "making .remote call to Chat.run")
    print(chat.run.remote())
    print(datetime.now(), "making web request to", url := chat.web.get_web_url())

    with urllib.request.urlopen(url) as response:
        print(datetime.now())
        print(json.loads(response.read().decode("utf-8")))


--- internal/examples_test.py ---
import importlib
import json
import pathlib
import sys

import pytest
from utils import (
    EXAMPLES_ROOT,
    ExampleType,
    get_examples,
    get_examples_json,
    render_example_md,
)

examples = [ex for ex in get_examples() if ex.type == ExampleType.MODULE]
examples = [ex for ex in examples if ex.metadata.get("pytest", True)]
example_ids = [ex.module for ex in examples]


@pytest.fixture(autouse=True)
def disable_auto_mount(monkeypatch):
    monkeypatch.setenv("MODAL_AUTOMOUNT", "0")
    yield


@pytest.fixture(autouse=False)
def add_root_to_syspath(monkeypatch):
    sys.path.append(str(EXAMPLES_ROOT))
    yield
    sys.path.pop()


@pytest.mark.parametrize("example", examples, ids=example_ids)
def test_filename(example):
    assert not example.repo_filename.startswith("/")
    assert pathlib.Path(example.repo_filename).exists()


@pytest.mark.parametrize("example", examples, ids=example_ids)
def test_import(example, add_root_to_syspath):
    importlib.import_module(example.module)


@pytest.mark.parametrize("example", examples, ids=example_ids)
def test_render(example):
    md = render_example_md(example)
    assert isinstance(md, str)
    assert len(md) > 0


def test_json():
    data = get_examples_json()
    examples = json.loads(data)
    assert isinstance(examples, list)
    assert len(examples) > 0


--- internal/run_example.py ---
import argparse
import os
import random
import subprocess
import sys
import time

from . import utils

MINUTES = 60
DEFAULT_TIMEOUT = 12 * MINUTES


def run_script(example, timeout=DEFAULT_TIMEOUT):
    t0 = time.time()

    print(f"Running example {example.stem} with timeout {timeout}s")

    try:
        print(f"cli args: {example.cli_args}")
        if "runc" in example.runtimes:
            example.env |= {"MODAL_FUNCTION_RUNTIME": "runc"}
        process = subprocess.run(
            [str(x) for x in example.cli_args],
            env=os.environ | example.env | {"MODAL_SERVE_TIMEOUT": "5.0"},
            timeout=timeout,
        )
        total_time = time.time() - t0
        if process.returncode == 0:
            print(f"Success after {total_time:.2f}s :)")
        else:
            print(
                f"Failed after {total_time:.2f}s with return code {process.returncode} :("
            )

        returncode = process.returncode

    except subprocess.TimeoutExpired:
        print(f"Past timeout of {timeout}s :(")
        returncode = 999

    return returncode


def run_single_example(stem, timeout=DEFAULT_TIMEOUT):
    examples = utils.get_examples()
    for example in examples:
        if stem == example.stem and example.metadata.get("lambda-test", True):
            return run_script(example, timeout=timeout)
    else:
        print(f"Could not find example name {stem}")
        return 0


def run_random_example(timeout=DEFAULT_TIMEOUT):
    examples = filter(
        lambda ex: ex.metadata and ex.metadata.get("lambda-test", True),
        utils.get_examples(),
    )
    return run_script(random.choice(list(examples)), timeout=timeout)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("example", nargs="?", default=None)
    parser.add_argument("--timeout", type=int, default=DEFAULT_TIMEOUT)
    args = parser.parse_args()
    print(args)
    if args.example:
        sys.exit(run_single_example(args.example, timeout=args.timeout))
    else:
        sys.exit(run_random_example(timeout=args.timeout))


--- 06_gpu_and_ml/controlnet/controlnet_gradio_demos.py ---
# ---
# cmd: ["modal", "serve", "06_gpu_and_ml/controlnet/controlnet_gradio_demos.py"]
# ---

# # Play with the ControlNet demos

# This example allows you to play with all 10 demonstration Gradio apps from the new and amazing ControlNet project.
# ControlNet provides a minimal interface allowing users to use images to constrain StableDiffusion's generation process.
# With ControlNet, users can easily condition the StableDiffusion image generation with different spatial contexts
# including a depth maps, segmentation maps, scribble drawings, and keypoints!

# <center>
# <video controls autoplay loop muted>
# <source src="https://user-images.githubusercontent.com/12058921/222927911-3ab52dd1-f2ee-4fb8-97e8-dafbf96ed5c5.mp4" type="video/mp4">
# </video>
# </center>

# ## Imports and config preamble

import importlib
import os
import pathlib
from dataclasses import dataclass, field

import modal
from fastapi import FastAPI

# Below are the configuration objects for all **10** demos provided in the original [lllyasviel/ControlNet](https://github.com/lllyasviel/ControlNet) repo.
# The demos each depend on their own custom pretrained StableDiffusion model, and these models are 5-6GB each.
# We can only run one demo at a time, so this module avoids downloading the model and 'detector' dependencies for
# all 10 demos and instead uses the demo configuration object to download only what's necessary for the chosen demo.

# Even just limiting our dependencies setup to what's required for one demo, the resulting container image is *huge*.


@dataclass(frozen=True)
class DemoApp:
    """Config object defining a ControlNet demo app's specific dependencies."""

    name: str
    model_files: list[str]
    detector_files: list[str] = field(default_factory=list)


demos = [
    DemoApp(
        name="canny2image",
        model_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_canny.pth"
        ],
    ),
    DemoApp(
        name="depth2image",
        model_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_depth.pth"
        ],
        detector_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/dpt_hybrid-midas-501f0c75.pt"
        ],
    ),
    DemoApp(
        name="fake_scribble2image",
        model_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_scribble.pth"
        ],
        detector_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/network-bsds500.pth"
        ],
    ),
    DemoApp(
        name="hed2image",
        model_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_hed.pth"
        ],
        detector_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/network-bsds500.pth"
        ],
    ),
    DemoApp(
        name="hough2image",
        model_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_mlsd.pth"
        ],
        detector_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/mlsd_large_512_fp32.pth",
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/mlsd_tiny_512_fp32.pth",
        ],
    ),
    DemoApp(
        name="normal2image",
        model_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_normal.pth"
        ],
    ),
    DemoApp(
        name="pose2image",
        model_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_openpose.pth"
        ],
        detector_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/body_pose_model.pth",
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/hand_pose_model.pth",
        ],
    ),
    DemoApp(
        name="scribble2image",
        model_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_scribble.pth"
        ],
    ),
    DemoApp(
        name="scribble2image_interactive",
        model_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_scribble.pth"
        ],
    ),
    DemoApp(
        name="seg2image",
        model_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_seg.pth"
        ],
        detector_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/upernet_global_small.pth"
        ],
    ),
]
demos_map: dict[str, DemoApp] = {d.name: d for d in demos}

# ## Pick a demo, any demo

# Simply by changing the `DEMO_NAME` below, you can change which ControlNet demo app is setup
# and run by this Modal script.

DEMO_NAME = "scribble2image"  # Change this value to change the active demo app.
selected_demo = demos_map[DEMO_NAME]

# ## Setting up the dependencies

# ControlNet requires *a lot* of dependencies which could be fiddly to setup manually, but Modal's programmatic
# container image building Python APIs handle this complexity straightforwardly and automatically.

# To run any of the 10 demo apps, we need the following:

# 1. a base Python 3 Linux image (we use Debian Slim)
# 2. a bunch of third party PyPi packages
# 3. `git`, so that we can download the ControlNet source code (there's no `controlnet` PyPi package)
# 4. some image process Linux system packages, including `ffmpeg`
# 5. and demo specific pre-trained model and detector `.pth` files

# That's a lot! Fortunately, the code below is already written for you that stitches together a working container image
# ready to produce remarkable ControlNet images.

# **Note:** a ControlNet model pipeline is [now available in Huggingface's `diffusers` package](https://huggingface.co/blog/controlnet). But this does not contain the demo apps.


def download_file(url: str, output_path: pathlib.Path):
    import httpx
    from tqdm import tqdm

    with open(output_path, "wb") as download_file:
        with httpx.stream("GET", url, follow_redirects=True) as response:
            total = int(response.headers["Content-Length"])
            with tqdm(
                total=total, unit_scale=True, unit_divisor=1024, unit="B"
            ) as progress:
                num_bytes_downloaded = response.num_bytes_downloaded
                for chunk in response.iter_bytes():
                    download_file.write(chunk)
                    progress.update(
                        response.num_bytes_downloaded - num_bytes_downloaded
                    )
                    num_bytes_downloaded = response.num_bytes_downloaded


def download_demo_files() -> None:
    """
    The ControlNet repo instructs: 'Make sure that SD models are put in "ControlNet/models".'
    'ControlNet' is just the repo root, so we place in /root/models.

    The ControlNet repo also instructs: 'Make sure that... detectors are put in "ControlNet/annotator/ckpts".'
    'ControlNet' is just the repo root, so we place in /root/annotator/ckpts.
    """
    demo = demos_map[os.environ["DEMO_NAME"]]
    models_dir = pathlib.Path("/root/models")
    for url in demo.model_files:
        filepath = pathlib.Path(url).name
        download_file(url=url, output_path=models_dir / filepath)
        print(f"download complete for {filepath}")

    detectors_dir = pathlib.Path("/root/annotator/ckpts")
    for url in demo.detector_files:
        filepath = pathlib.Path(url).name
        download_file(url=url, output_path=detectors_dir / filepath)
        print(f"download complete for {filepath}")
    print("ðŸŽ‰ finished baking demo file(s) into image.")


image = (
    modal.Image.debian_slim(python_version="3.10")
    .uv_pip_install(
        "fastapi[standard]==0.115.4",
        "pydantic==2.9.1",
        "starlette==0.41.2",
        "gradio==3.16.2",
        "albumentations==1.3.0",
        "opencv-contrib-python",
        "imageio==2.9.0",
        "imageio-ffmpeg==0.4.2",
        "pytorch-lightning==1.5.0",
        "omegaconf==2.1.1",
        "test-tube>=0.7.5",
        "streamlit==1.12.1",
        "einops==0.3.0",
        "transformers==4.19.2",
        "webdataset==0.2.5",
        "kornia==0.6",
        "open_clip_torch==2.0.2",
        "invisible-watermark>=0.1.5",
        "streamlit-drawable-canvas==0.8.0",
        "torchmetrics==0.6.0",
        "timm==0.6.12",
        "addict==2.4.0",
        "yapf==0.32.0",
        "prettytable==3.6.0",
        "safetensors==0.2.7",
        "basicsr==1.4.2",
        "tqdm~=4.64.1",
    )
    # xformers library offers performance improvement.
    .uv_pip_install("xformers", pre=True)
    .apt_install("git")
    # Here we place the latest ControlNet repository code into /root.
    # Because /root is almost empty, but not entirely empty, `git clone` won't work,
    # so this `init` then `checkout` workaround is used.
    .run_commands(
        "cd /root && git init .",
        "cd /root && git remote add --fetch origin https://github.com/lllyasviel/ControlNet.git",
        "cd /root && git checkout main",
    )
    .apt_install("ffmpeg", "libsm6", "libxext6")
    .run_function(
        download_demo_files,
        secrets=[modal.Secret.from_dict({"DEMO_NAME": DEMO_NAME})],
    )
)
app = modal.App(name="example-controlnet-gradio-demos", image=image)

web_app = FastAPI()

# ## Serving the Gradio web UI

# Each ControlNet gradio demo module exposes a `block` Gradio interface running in queue-mode,
# which is initialized in module scope on import and served on `0.0.0.0`. We want the block interface object,
# but the queueing and launched webserver aren't compatible with Modal's serverless web endpoint interface,
# so in the `import_gradio_app_blocks` function we patch out these behaviors.


def import_gradio_app_blocks(demo: DemoApp):
    from gradio import blocks

    # The ControlNet repo demo scripts are written to be run as
    # standalone scripts, and have a lot of code that executes
    # in global scope on import, including the launch of a Gradio web server.
    # We want Modal to control the Gradio web app serving, so we
    # monkeypatch the .launch() function to be a no-op.
    blocks.Blocks.launch = lambda self, server_name: print(
        "launch() has been monkeypatched to do nothing."
    )

    # each demo app module is a file like gradio_{name}.py
    module_name = f"gradio_{demo.name}"
    mod = importlib.import_module(module_name)
    blocks = mod.block
    # disable queueing mode, which is incompatible with our Modal web app setup.
    blocks.enable_queue = False
    return blocks


# Because the ControlNet gradio apps are so time and compute intensive to cold-start,
# the web app function is limited to running just 1 warm container (max_containers=1).
# This way, while playing with the demos we can pay the cold-start cost once and have
# all web requests hit the same warm container.
# Spinning up extra containers to handle additional requests would not be efficient
# given the cold-start time.
# We set the scaledown_window to 600 seconds so the container will be kept
# running for 10 minutes after the last request, to keep the app responsive in case
# of continued experimentation.


@app.function(
    gpu="A10G",
    max_containers=1,
    scaledown_window=600,
)
@modal.asgi_app()
def run():
    from gradio.routes import mount_gradio_app

    # mount for execution on Modal
    return mount_gradio_app(
        app=web_app,
        blocks=import_gradio_app_blocks(demo=selected_demo),
        path="/",
    )


# ## Have fun!

# Serve your chosen demo app with `modal serve controlnet_gradio_demos.py`. If you don't have any images ready at hand,
# try one that's in the `06_gpu_and_ml/controlnet/demo_images/` folder.

# StableDiffusion was already impressive enough, but ControlNet's ability to so accurately and intuitively constrain
# the image generation process is sure to put a big, dumb grin on your face.


## Links discovered
- [lllyasviel/ControlNet](https://github.com/lllyasviel/ControlNet)
- [now available in Huggingface's `diffusers` package](https://huggingface.co/blog/controlnet)

--- 06_gpu_and_ml/dreambooth/instance_example_urls.txt ---
https://modal-public-assets.s3.amazonaws.com/example-dreambooth-app/fkRYgv6.png
https://modal-public-assets.s3.amazonaws.com/example-dreambooth-app/98k9yDg.jpg
https://modal-public-assets.s3.amazonaws.com/example-dreambooth-app/gHlW8Kw.jpg


--- 06_gpu_and_ml/tensorflow/tensorflow_tutorial.py ---
# ---
# args: ["--just-run"]
# ---
# # TensorFlow tutorial

# This is essentially a version of the
# [image classification example in the TensorFlow documentation](https://www.tensorflow.org/tutorials/images/classification)
# running inside Modal on a GPU.
# If you run this script, it will also create an TensorBoard URL you can go to to watch the model train and review the results:

# ![tensorboard](./tensorboard.png)

# ## Setting up the dependencies

# Configuring a system to properly run GPU-accelerated TensorFlow can be challenging.
# Luckily, Modal makes it easy to stand on the shoulders of giants and
# [use a pre-built Docker container image](https://modal.com/docs/guide/custom-container#use-an-existing-container-image-with-from_registry) from a registry like Docker Hub.
# We recommend TensorFlow's [official base Docker container images](https://hub.docker.com/r/tensorflow/tensorflow), which come with `tensorflow` and its matching CUDA libraries already installed.

# If you want to install TensorFlow some other way, check out [their docs](https://www.tensorflow.org/install) for options and instructions.
# GPU-enabled containers on Modal will always have NVIDIA drivers available, but you will need to add higher-level tools like CUDA and cuDNN yourself.
# See the [Modal guide on customizing environments](https://modal.com/docs/guide/custom-container) for options we support.

import time

import modal

dockerhub_image = modal.Image.from_registry(
    "tensorflow/tensorflow:2.15.0-gpu",
)

app = modal.App("example-tensorflow-tutorial", image=dockerhub_image)

# ## Logging data to TensorBoard

# Training ML models takes time. Just as we need to monitor long-running systems like databases or web servers for issues,
# we also need to monitor the training process of our ML models. TensorBoard is a tool that comes with TensorFlow that helps you visualize
# the state of your ML model training. It is packaged as a web server.

# We want to run the web server for TensorBoard at the same time as we are training the
# TensorFlow model. The easiest way to share data between the training function and the
# web server is by creating a
# [Modal Volume](https://modal.com/docs/guide/volumes)
# that we can attach to both
# [Functions](https://modal.com/docs/reference/modal.Function).

volume = modal.Volume.from_name("tensorflow-tutorial", create_if_missing=True)
LOGDIR = "/tensorboard"

# ## Training function

# This is basically the same code as [the official example](https://www.tensorflow.org/tutorials/images/classification) from the TensorFlow docs.
# A few Modal-specific things are worth pointing out:

# * We attach the Volume for sharing data with TensorBoard in the `app.function`
#   decorator.

# * We also annotate this function with `gpu="T4"` to make sure it runs on a GPU.

# * We put all the TensorFlow imports inside the function body.
#   This makes it possible to run this example even if you don't have TensorFlow installed on your local computer -- a key benefit of Modal!

# You may notice some warnings in the logs about certain CPU performance optimizations (NUMA awareness and AVX/SSE instruction set support) not being available.
# While these optimizations can be important for some workloads, especially if you are running ML models on a CPU, they are not critical for most cases.


@app.function(volumes={LOGDIR: volume}, gpu="T4", timeout=600)
def train():
    import pathlib

    import tensorflow as tf
    from tensorflow.keras import layers
    from tensorflow.keras.models import Sequential

    # load raw data from storage
    dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
    data_dir = tf.keras.utils.get_file(
        "flower_photos.tar", origin=dataset_url, extract=True
    )
    data_dir = pathlib.Path(data_dir).with_suffix("")

    # construct Keras datasets from raw data
    batch_size = 32
    img_height = img_width = 180

    train_ds = tf.keras.utils.image_dataset_from_directory(
        data_dir,
        validation_split=0.2,
        subset="training",
        seed=123,
        image_size=(img_height, img_width),
        batch_size=batch_size,
    )

    val_ds = tf.keras.utils.image_dataset_from_directory(
        data_dir,
        validation_split=0.2,
        subset="validation",
        seed=123,
        image_size=(img_height, img_width),
        batch_size=batch_size,
    )

    class_names = train_ds.class_names
    train_ds = (
        train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)  # type: ignore
    )
    val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)  # type: ignore
    num_classes = len(class_names)

    model = Sequential(
        [
            layers.Rescaling(1.0 / 255, input_shape=(img_height, img_width, 3)),
            layers.Conv2D(16, 3, padding="same", activation="relu"),
            layers.MaxPooling2D(),
            layers.Conv2D(32, 3, padding="same", activation="relu"),
            layers.MaxPooling2D(),
            layers.Conv2D(64, 3, padding="same", activation="relu"),
            layers.MaxPooling2D(),
            layers.Flatten(),
            layers.Dense(128, activation="relu"),
            layers.Dense(num_classes),
        ]
    )

    model.compile(
        optimizer="adam",
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=["accuracy"],
    )

    model.summary()

    tensorboard_callback = tf.keras.callbacks.TensorBoard(
        log_dir=LOGDIR,
        histogram_freq=1,
    )

    model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=20,
        callbacks=[tensorboard_callback],
    )


# ## Running TensorBoard

# TensorBoard is compatible with a Python web server standard called [WSGI](https://www.fullstackpython.com/wsgi-servers.html),
# the same standard used by [Flask](https://flask.palletsprojects.com/).
# Modal [speaks WSGI too](https://modal.com/docs/guide/webhooks#wsgi), so it's straightforward to run TensorBoard in a Modal app.

# We will attach the same Volume that we attached to our training function so that
# TensorBoard can read the logs. For this to work with Modal, we will first
# create some
# [WSGI Middleware](https://peps.python.org/pep-3333/)
# to check the Modal Volume for updates any time the page is reloaded.


class VolumeMiddleware:
    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):
        if (route := environ.get("PATH_INFO")) in ["/", "/modal-volume-reload"]:
            try:
                volume.reload()
            except Exception as e:
                print("Exception while re-loading traces: ", e)
            if route == "/modal-volume-reload":
                environ["PATH_INFO"] = "/"  # redirect
        return self.app(environ, start_response)


# The WSGI app isn't exposed directly through the TensorBoard library, but we can build it
# the same way it's built internally --
# [see the TensorBoard source code for details](https://github.com/tensorflow/tensorboard/blob/0c5523f4b27046e1ca7064dd75347a5ee6cc7f79/tensorboard/program.py#L466-L476).

# Note that the TensorBoard server runs in a different container.
# The server does not need GPU support.
# Note that this server will be exposed to the public internet!


@app.function(
    volumes={LOGDIR: volume},
    max_containers=1,  # single replica
    scaledown_window=5 * 60,  # five minute idle time
)
@modal.concurrent(max_inputs=100)  # 100 concurrent request threads
@modal.wsgi_app()
def tensorboard_app():
    import tensorboard

    board = tensorboard.program.TensorBoard()
    board.configure(logdir=LOGDIR)
    (data_provider, deprecated_multiplexer) = board._make_data_provider()
    wsgi_app = tensorboard.backend.application.TensorBoardWSGIApp(
        board.flags,
        board.plugin_loaders,
        data_provider,
        board.assets_zip_provider,
        deprecated_multiplexer,
        experimental_middlewares=[VolumeMiddleware],
    )
    return wsgi_app


# ## Local entrypoint code

# Let's kick everything off.
# Everything runs in an ephemeral "app" that gets destroyed once it's done.
# In order to keep the TensorBoard web server running, we sleep in an infinite loop
# until the user hits ctrl-c.

# The script will take a few minutes to run, although each epoch is quite fast since it runs on a GPU.
# The first time you run it, it might have to build the image, which can take an additional few minutes.


@app.local_entrypoint()
def main(just_run: bool = False):
    train.remote()
    if not just_run:
        print(
            "Training is done, but the app is still running TensorBoard until you hit ctrl-c."
        )
        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            print("Terminating app")


## Links discovered
- [image classification example in the TensorFlow documentation](https://www.tensorflow.org/tutorials/images/classification)
- [tensorboard](https://github.com/modal-labs/modal-examples/blob/main/06_gpu_and_ml/tensorflow/tensorboard.png)
- [use a pre-built Docker container image](https://modal.com/docs/guide/custom-container#use-an-existing-container-image-with-from_registry)
- [official base Docker container images](https://hub.docker.com/r/tensorflow/tensorflow)
- [their docs](https://www.tensorflow.org/install)
- [Modal guide on customizing environments](https://modal.com/docs/guide/custom-container)
- [Modal Volume](https://modal.com/docs/guide/volumes)
- [Functions](https://modal.com/docs/reference/modal.Function)
- [the official example](https://www.tensorflow.org/tutorials/images/classification)
- [WSGI](https://www.fullstackpython.com/wsgi-servers.html)
- [Flask](https://flask.palletsprojects.com/)
- [speaks WSGI too](https://modal.com/docs/guide/webhooks#wsgi)
- [WSGI Middleware](https://peps.python.org/pep-3333/)
- [see the TensorBoard source code for details](https://github.com/tensorflow/tensorboard/blob/0c5523f4b27046e1ca7064dd75347a5ee6cc7f79/tensorboard/program.py#L466-L476)

--- 06_gpu_and_ml/comfyui/essentials/essentials_example.py ---
# ---
# cmd: ["modal", "serve", "06_gpu_and_ml/comfyui/essentials/essentials_example.py"]
# ---

import subprocess

import modal

image = (  # build up a Modal Image to run ComfyUI, step by step
    modal.Image.debian_slim(  # start from basic Linux with Python
        python_version="3.11"
    )
    .apt_install("git")  # install git to clone ComfyUI
    .uv_pip_install("comfy-cli==1.2.7")  # install comfy-cli
    .run_commands(  # use comfy-cli to install the ComfyUI repo and its dependencies
        "comfy --skip-prompt install --nvidia"
    )
    .run_commands(  # download the ComfyUI Essentials custom node pack
        "comfy node install comfyui_essentials"
    )
    .run_commands(
        "comfy --skip-prompt model download --url https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/resolve/main/v1-5-pruned.safetensors --relative-path models/checkpoints"
    )
)

app = modal.App(name="example-essentials", image=image)


# Run ComfyUI as an interactive web server
@app.function(
    max_containers=1,
    scaledown_window=30,
    timeout=1800,
    gpu="A10G",
)
@modal.concurrent(max_inputs=10)
@modal.web_server(8000, startup_timeout=60)
def ui():
    subprocess.Popen("comfy launch -- --listen 0.0.0.0 --port 8000", shell=True)


--- 06_gpu_and_ml/comfyui/impact/impact_example.py ---
# ---
# cmd: ["modal", "serve", "06_gpu_and_ml/comfyui/impact/impact_example.py"]
# ---

import subprocess

import modal

image = (
    modal.Image.debian_slim(  # start from basic Linux with Python
        python_version="3.11"
    )
    .apt_install("git")  # install git to clone ComfyUI
    .uv_pip_install("comfy-cli==1.2.7")  # install comfy-cli
    .run_commands(  # use comfy-cli to install the ComfyUI repo and its dependencies
        "comfy --skip-prompt install --nvidia"
    )
    .run_commands(  # download the Impact pack
        "comfy node install comfyui-impact-pack"
    )
    .uv_pip_install("ultralytics==8.3.26")  # object detection models
    .apt_install(  # opengl dependencies
        "libgl1-mesa-glx", "libglib2.0-0"
    )
    .run_commands(
        "comfy --skip-prompt model download --url https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/resolve/main/v1-5-pruned.safetensors --relative-path models/checkpoints",
    )
)

app = modal.App(name="example-impact", image=image)


# Run ComfyUI as an interactive web server
@app.function(
    max_containers=1,
    scaledown_window=30,
    timeout=1800,
    gpu="A10G",
)
@modal.concurrent(max_inputs=10)
@modal.web_server(8000, startup_timeout=60)
def ui():
    subprocess.Popen("comfy launch -- --listen 0.0.0.0 --port 8000", shell=True)


--- 06_gpu_and_ml/comfyui/ip_adapter/ip_adapter_example.py ---
# ---
# cmd: ["modal", "serve", "06_gpu_and_ml/comfyui/ip_adapter/ip_adapter_example.py"]
# ---

import subprocess

import modal

image = (  # build up a Modal Image to run ComfyUI, step by step
    modal.Image.debian_slim(  # start from basic Linux with Python
        python_version="3.11"
    )
    .apt_install("git")  # install git to clone ComfyUI
    .uv_pip_install("comfy-cli==1.2.7")  # install comfy-cli
    .run_commands(  # use comfy-cli to install the ComfyUI repo and its dependencies
        "comfy --skip-prompt install --nvidia"
    )
    .run_commands(  # download the WAS Node Suite custom node pack
        "comfy node install comfyui_ipadapter_plus"
    )
    .run_commands("apt install -y wget")
    .run_commands(  # the Unified Model Loader node requires these two models to be named a specific way, so we use wget instead of the usual comfy model download command
        "wget -q -O /root/comfy/ComfyUI/models/clip_vision/CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors https://huggingface.co/h94/IP-Adapter/resolve/main/models/image_encoder/model.safetensors",
    )
    .run_commands(
        "wget -q -O /root/comfy/ComfyUI/models/clip_vision/CLIP-ViT-bigG-14-laion2B-39B-b160k.safetensors, https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/image_encoder/model.safetensors",
    )
    .run_commands(  # download the IP-Adapter model
        "comfy --skip-prompt model download --url https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter_sd15.safetensors --relative-path models/ipadapter"
    )
    .run_commands(
        "comfy --skip-prompt model download --url https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/resolve/main/v1-5-pruned.safetensors --relative-path models/checkpoints",
    )
)

app = modal.App(name="example-ip-adapter", image=image)


# Run ComfyUI as an interactive web server
@app.function(
    max_containers=1,
    scaledown_window=30,
    timeout=1800,
    gpu="A10G",
)
@modal.concurrent(max_inputs=10)
@modal.web_server(8000, startup_timeout=60)
def ui():
    subprocess.Popen("comfy launch -- --listen 0.0.0.0 --port 8000", shell=True)


--- 06_gpu_and_ml/comfyui/kjnodes/kjnodes_example.py ---
# ---
# cmd: ["modal", "serve", "06_gpu_and_ml/comfyui/kjnodes/kjnodes_example.py"]
# ---

import subprocess

import modal

image = (  # build up a Modal Image to run ComfyUI, step by step
    modal.Image.debian_slim(  # start from basic Linux with Python
        python_version="3.11"
    )
    .apt_install("git")  # install git to clone ComfyUI
    .uv_pip_install("comfy-cli==1.2.7")  # install comfy-cli
    .run_commands(  # use comfy-cli to install the ComfyUI repo and its dependencies
        "comfy --skip-prompt install --nvidia"
    )
    .run_commands(  # download the ComfyUI Essentials custom node pack
        "comfy node install comfyui-kjnodes"
    )
    .run_commands(
        "comfy --skip-prompt model download --url https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/resolve/main/v1-5-pruned.safetensors --relative-path models/checkpoints"
    )
)

app = modal.App(name="example-kjnodes", image=image)


# Run ComfyUI as an interactive web server
@app.function(
    max_containers=1,
    scaledown_window=30,
    timeout=1800,
    gpu="A10G",
)
@modal.concurrent(max_inputs=10)
@modal.web_server(8000, startup_timeout=60)
def ui():
    subprocess.Popen("comfy launch -- --listen 0.0.0.0 --port 8000", shell=True)


--- 06_gpu_and_ml/comfyui/memory_snapshot/memory_snapshot_example.py ---
# Simple ComfyUI example using memory snapshot to speed up cold starts.

# CAUTION: Some custom nodes may not work with memory snapshots, especially if they make calls to torch (i.e. require a GPU) on initialization.
# Run `modal deploy memory_snapshot_example.py` to deploy with memory snapshot enabled.

# Image building and model downloading is directly taken from the core example: https://modal.com/docs/examples/comfyapp
# The notable changes are copying the custom node in the image and the cls object
import subprocess
from pathlib import Path

import modal

image = (
    modal.Image.debian_slim(python_version="3.11")
    .apt_install("git")
    .uv_pip_install("fastapi[standard]==0.115.4")
    .uv_pip_install("comfy-cli==1.3.8")
    .run_commands("comfy --skip-prompt install --fast-deps --nvidia --version 0.3.10")
)

# Add custom node that patches core ComfyUI so that we can use Modal's [memory snapshot](https://modal.com/docs/guide/memory-snapshot)
image = image.add_local_dir(
    local_path=Path(__file__).parent / "memory_snapshot_helper",
    remote_path="/root/comfy/ComfyUI/custom_nodes/memory_snapshot_helper",
    copy=True,
)


def hf_download():
    from huggingface_hub import hf_hub_download

    flux_model = hf_hub_download(
        repo_id="Comfy-Org/flux1-schnell",
        filename="flux1-schnell-fp8.safetensors",
        cache_dir="/cache",
    )

    subprocess.run(
        f"ln -s {flux_model} /root/comfy/ComfyUI/models/checkpoints/flux1-schnell-fp8.safetensors",
        shell=True,
        check=True,
    )


vol = modal.Volume.from_name("hf-hub-cache", create_if_missing=True)

image = (
    image.uv_pip_install("huggingface_hub[hf_transfer]==0.30.0")
    .env({"HF_XET_HIGH_PERFORMANCE": "1"})
    .run_function(
        hf_download,
        volumes={"/cache": vol},
    )
)


app = modal.App(name="example-comfyui-memory-snapshot", image=image)


@app.cls(
    max_containers=1,
    gpu="L40S",
    volumes={"/cache": vol},
    enable_memory_snapshot=True,  # snapshot container state for faster cold starts
)
@modal.concurrent(max_inputs=10)
class ComfyUIMemorySnapshot:
    port: int = 8000

    # Snapshot ComfyUI server launch state, which includes import torch and custom node initialization (GPU not available during this step)
    @modal.enter(snap=True)
    def launch_comfy_background(self):
        cmd = f"comfy launch --background -- --port {self.port}"
        subprocess.run(cmd, shell=True, check=True)

    # Restore ComfyUI server state. Re-enables the CUDA device for inference.
    @modal.enter(snap=False)
    def restore_snapshot(self):
        import requests

        response = requests.post(f"http://127.0.0.1:{self.port}/cuda/set_device")
        if response.status_code != 200:
            print("Failed to set CUDA device")
        else:
            print("Successfully set CUDA device")

    @modal.web_server(port, startup_timeout=60)
    def ui(self):
        subprocess.Popen(
            f"comfy launch -- --listen 0.0.0.0 --port {self.port}", shell=True
        )


## Links discovered
- [memory snapshot](https://modal.com/docs/guide/memory-snapshot)

--- 07_web_endpoints/fastapi_app.py ---
# ---
# cmd: ["modal", "serve", "07_web_endpoints/fastapi_app.py"]
# ---

# # Deploy FastAPI app with Modal

# This example shows how you can deploy a [FastAPI](https://fastapi.tiangolo.com/) app with Modal.
# You can serve any app written in an ASGI-compatible web framework (like FastAPI) using this pattern or you can server WSGI-compatible frameworks like Flask with [`wsgi_app`](https://modal.com/docs/guide/webhooks#wsgi).

from typing import Optional

import modal
from fastapi import FastAPI, Header
from pydantic import BaseModel

image = modal.Image.debian_slim().uv_pip_install("fastapi[standard]", "pydantic")
app = modal.App("example-fastapi-app", image=image)
web_app = FastAPI()


class Item(BaseModel):
    name: str


@web_app.get("/")
async def handle_root(user_agent: Optional[str] = Header(None)):
    print(f"GET /     - received user_agent={user_agent}")
    return "Hello World"


@web_app.post("/foo")
async def handle_foo(item: Item, user_agent: Optional[str] = Header(None)):
    print(f"POST /foo - received user_agent={user_agent}, item.name={item.name}")
    return item


@app.function()
@modal.asgi_app()
def fastapi_app():
    return web_app


@app.function()
@modal.fastapi_endpoint(method="POST")
def f(item: Item):
    return "Hello " + item.name


if __name__ == "__main__":
    app.deploy("webapp")


## Links discovered
- [FastAPI](https://fastapi.tiangolo.com/)
- [`wsgi_app`](https://modal.com/docs/guide/webhooks#wsgi)

--- 07_web_endpoints/webrtc/yolo/yolo_classes.txt ---
person
bicycle
car
motorcycle
airplane
bus
train
truck
boat
traffic light
fire hydrant
stop sign
parking meter
bench
bird
cat
dog
horse
sheep
cow
elephant
bear
zebra
giraffe
backpack
umbrella
handbag
tie
suitcase
frisbee
skis
snowboard
sports ball
kite
baseball bat
baseball glove
skateboard
surfboard
tennis racket
bottle
wine glass
cup
fork
knife
spoon
bowl
banana
apple
sandwich
orange
broccoli
carrot
hot dog
pizza
donut
cake
chair
couch
potted plant
bed
dining table
toilet
tv
laptop
mouse
remote
keyboard
cell phone
microwave
oven
toaster
sink
refrigerator
book
clock
vase
scissors
teddy bear
hair drier
toothbrush


--- .claude/CLAUDE.md ---
# Modal Examples

For up-to-date Modal usage tips, see https://modal.com/llms.txt

Examples are published at `https://modal.com/docs/examples/{slug}` where `slug`
is the filename without `.py`.

## For maintainers

See @internal/CLAUDE.md for fixing bugs in these examples.


--- .github/pull_request_template.md ---
<!--
  âœï¸ Write a short summary of your work. Screenshots and videos are welcome!
-->

## Type of Change

<!--
  â˜‘ï¸ Check one of the top-level boxes and delete the others.
-->

- [ ] New example for the GitHub repo
  - [ ] New example for the documentation site (Linked from a discoverable page, e.g. via the sidebar in `/docs/examples`)
- [ ] Example updates (Bug fixes, new features, etc.)
- [ ] Other (Changes to the codebase, but not to examples)

## Monitoring Checklist

<!--
  â˜‘ï¸ All examples added to numbered folders in the repo should pass this checklist.
  Otherwise, move the file into `misc/` and delete the checklist.

  See `internal/README.md` for details on the CI.
-->

  - [ ] Example is configured for testing in the synthetic monitoring system, or `lambda-test: false` is provided in the example frontmatter and I have gotten approval from a maintainer
    - [ ] Example is tested by executing with `modal run`, or an alternative `cmd` is provided in the example frontmatter (e.g. `cmd: ["modal", "serve"]`)
    - [ ] Example is tested by running the `cmd` with no arguments, or the `args` are provided in the example frontmatter (e.g. `args: ["--prompt", "Formula for room temperature superconductor:"]`
    - [ ] Example does _not_ require third-party dependencies besides `fastapi` to be installed locally (e.g. does not import `requests` or `torch` in the global scope or other code executed locally)

## Documentation Site Checklist

<!--
  â˜‘ï¸ Review the checklist below if the example is intended for the documentation site.
  All boxes should be checked!
-->

### Content
  - [ ] Example is documented with comments throughout, in a [_Literate Programming_](https://en.wikipedia.org/wiki/Literate_programming) style
  - [ ] All media assets for the example that are rendered in the documentation site page are retrieved from `modal-cdn.com`

### Build Stability
  - [ ] Example pins all dependencies in container images
    - [ ] Example pins container images to a stable tag like `v1`, not a dynamic tag like `latest`
    - [ ] Example specifies a `python_version` for the base image, if it is used 
    - [ ] Example pins all dependencies to at least [SemVer](https://semver.org/) minor version, `~=x.y.z` or `==x.y`, or we expect this example to work across major versions of the dependency and are committed to maintenance across those versions
      - [ ] Example dependencies with `version < 1` are pinned to patch version, `==0.y.z`

## Outside Contributors

You're great! Thanks for your contribution.


## Links discovered
- [_Literate Programming_](https://en.wikipedia.org/wiki/Literate_programming)
- [SemVer](https://semver.org/)

--- 04_secrets/db_to_sheet.py ---
# ---
# deploy: true
# ---

# # Write to Google Sheets from Postgres

# In this tutorial, we'll show how to use Modal to schedule a daily report in a spreadsheet on Google Sheets
# that combines data from a PostgreSQL database with data from an external API.

# In particular, we'll extract the city of each user from the database, look up the current weather in that city,
# and then build a count/histogram of how many users are experiencing each type of weather.

# ## Entering credentials

# We begin by setting up some credentials that we'll need in order to access our database and output
# spreadsheet. To do that in a secure manner, we log in to our Modal account on the web and go to
# the [Secrets](https://modal.com/secrets) section.

# ### Database

# First we will enter our database credentials. The easiest way to do this is to click **New
# secret** and select the **Postgres compatible** Secret preset and fill in the requested
# information. Then we press **Next** and name our Secret `postgres-secret` and click **Create**.

# ### Google Sheets/GCP

# We'll now add another Secret for Google Sheets access through Google Cloud Platform. Click **New
# secret** and select the Google Sheets preset.

# In order to access the Google Sheets API, we'll need to create a *Service Account* in Google Cloud
# Platform. You can skip this step if you already have a Service Account json file.

# 1. Sign up to Google Cloud Platform or log in if you haven't
#    ([https://cloud.google.com/](https://cloud.google.com/)).

# 2. Go to [https://console.cloud.google.com/](https://console.cloud.google.com/).

# 3. In the navigation pane on the left, go to **IAM & Admin** > **Service Accounts**.

# 4. Click the **+ CREATE SERVICE ACCOUNT** button.

# 5. Give the service account a suitable name, like "sheet-access-bot". Click **Done**. You don't
#    have to grant it any specific access privileges at this time.

# 6. Click your new service account in the list view that appears and navigate to the **Keys**
#    section.

# 7. Click **Add key** and choose **Create new key**. Use the **JSON** key type and confirm by
#    clicking **Create**.

# 8. A json key file should be downloaded to your computer at this point. Copy the contents of that
#    file and use it as the value for the `SERVICE_ACCOUNT_JSON` field in your new secret.

# We'll name this other Secret `"gsheets-secret"`.

# Now you can access the values of your Secrets from Modal Functions that you annotate with the
# corresponding `modal.Secret`s, e.g.:

import os

import modal

app = modal.App("example-db-to-sheet")


@app.function(secrets=[modal.Secret.from_name("postgres-secret")])
def show_host():
    # automatically filled from the specified secret
    print("Host is " + os.environ["PGHOST"])


# Because these Secrets are Python objects, you can construct and manipulate them in your code.
# We'll do that below by defining a variable to hold our Secret for accessing Postgres

# You can additionally specify

pg_secret = modal.Secret.from_name(
    "postgres-secret",
    required_keys=["PGHOST", "PGPORT", "PGDATABASE", "PGUSER", "PGPASSWORD"],
)


# In order to connect to the database, we'll use the `psycopg2` Python package. To make it available
# to your Modal Function you need to supply it with an `image` argument that tells Modal how to
# build the container image that contains that package. We'll base it off of the `Image.debian_slim` base
# image that's built into Modal, and make sure to install the required binary packages as well as
# the `psycopg2` package itself:

pg_image = (
    modal.Image.debian_slim(python_version="3.11")
    .apt_install("libpq-dev")
    .uv_pip_install("psycopg2~=2.9.9")
)

# Since the default keynames for a **Postgres compatible** secret correspond to the environment
# variables that `psycopg2` looks for, we can now easily connect to the database even without
# explicit credentials in your code. We'll create a simple function that queries the city for each
# user in the `users` table.


@app.function(image=pg_image, secrets=[pg_secret])
def get_db_rows(verbose=True):
    import psycopg2

    conn = psycopg2.connect()  # no explicit credentials needed
    cur = conn.cursor()
    cur.execute("SELECT city FROM users")
    results = [row[0] for row in cur.fetchall()]
    if verbose:
        print(results)
    return results


# Note that we import `psycopg2` inside our function instead of the global scope. This allows us to
# run this Modal Function even from an environment where `psycopg2` is not installed. We can test run
# this function using the `modal run` shell command: `modal run db_to_sheet.py::app.get_db_rows`.

# To run this function, make sure there is a table called `users` in your database with a column called `city`.
# You can populate the table with some example data using the following SQL commands:

# ```sql
# CREATE TABLE users (city TEXT);
# INSERT INTO users VALUES ('Stockholm,,Sweden');
# INSERT INTO users VALUES ('New York,NY,USA');
# INSERT INTO users VALUES ('Tokyo,,Japan');
# ```

# ## Applying Python logic

# For each row in our source data we'll run an online lookup of the current weather using the
# [http://openweathermap.org](http://openweathermap.org) API. To do this, we'll add the API key to
# another Modal Secret. We'll use a custom secret called "weather-secret" with the key
# `OPENWEATHER_API_KEY` containing our API key for OpenWeatherMap.

requests_image = modal.Image.debian_slim(python_version="3.11").uv_pip_install(
    "requests~=2.31.0"
)


@app.function(
    image=requests_image,
    secrets=[
        modal.Secret.from_name("weather-secret", required_keys=["OPENWEATHER_API_KEY"])
    ],
)
def city_weather(city):
    import requests

    url = "https://api.openweathermap.org/data/2.5/weather"
    params = {"q": city, "appid": os.environ["OPENWEATHER_API_KEY"]}
    response = requests.get(url, params=params)
    weather_label = response.json()["weather"][0]["main"]
    return weather_label


# We'll make use of Modal's built-in `function.map` method to create our report. `function.map`
# makes it really easy to parallelize work by executing a Function on every element in a sequence of
# data. For this example we'll just do a simple count of rows per weather type --
# answering the question "how many of our users are experiencing each type of weather?".

from collections import Counter


@app.function()
def create_report(cities):
    # run city_weather for each city in parallel
    user_weather = city_weather.map(cities)
    count_users_by_weather = Counter(user_weather).items()
    return count_users_by_weather


# Let's try to run this! To make it simple to trigger the function with some
# predefined input data, we create a "local entrypoint" that can be
# run from the command line with

# ```bash
# modal run db_to_sheet.py
# ```


@app.local_entrypoint()
def main():
    cities = [
        "Stockholm,,Sweden",
        "New York,NY,USA",
        "Tokyo,,Japan",
    ]
    print(create_report.remote(cities))


# Running the local entrypoint using `modal run db_to_sheet.py` should print something like:
# `dict_items([('Clouds', 3)])`.
# Note that since this file only has a single app, and the app has only one local entrypoint
# we only have to specify the file to run it - the function/entrypoint is inferred.

# In this case the logic is quite simple, but in a real world context you could have applied a
# machine learning model or any other tool you could build into a container to transform the data.

# ## Sending output to a Google Sheet

# We'll set up a new Google Sheet to send our report to. Using the "Sharing" dialog in Google
# Sheets, share the document to the service account's email address (the value of the `client_email` field in the json file)
# and make the service account an editor of the document.

# You may also need to enable the Google Sheets API for your project in the Google Cloud Platform console.
# If so, the URL will be printed inside the message of a 403 Forbidden error when you run the function.
# It begins with https://console.developers.google.com/apis/api/sheets.googleapis.com/overview.

# Lastly, we need to point our code to the correct Google Sheet. We'll need the *key* of the document.
# You can find the key in the URL of the Google Sheet. It appears after the `/d/` in the URL, like:
# `https://docs.google.com/spreadsheets/d/1wOktal......IJR77jD8Do`.

# We'll make use of the `pygsheets` python package to authenticate with
# Google Sheets and then update the spreadsheet with information from the report we just created:

pygsheets_image = modal.Image.debian_slim(python_version="3.11").uv_pip_install(
    "pygsheets~=2.0.6"
)


@app.function(
    image=pygsheets_image,
    secrets=[
        modal.Secret.from_name("gsheets-secret", required_keys=["SERVICE_ACCOUNT_JSON"])
    ],
)
def update_sheet_report(rows):
    import pygsheets

    gc = pygsheets.authorize(service_account_env_var="SERVICE_ACCOUNT_JSON")
    document_key = "1JxhGsht4wltyPFFOd2hP0eIv6lxZ5pVxJN_ZwNT-l3c"
    sh = gc.open_by_key(document_key)
    worksheet = sh.sheet1
    worksheet.clear("A2")

    worksheet.update_values("A2", [list(row) for row in rows])


# At this point, we have everything we need in order to run the full program. We can put it all together in
# another Modal Function, and add a [`schedule`](https://modal.com/docs/guide/cron) argument so it runs every day automatically:


@app.function(schedule=modal.Period(days=1))
def db_to_sheet():
    rows = get_db_rows.remote()
    report = create_report.remote(rows)
    update_sheet_report.remote(report)
    print("Updated sheet with new weather distribution")
    for weather, count in report:
        print(f"{weather}: {count}")


# This entire app can now be deployed using `modal deploy db_to_sheet.py`. The [apps page](https://modal.com/apps)
# shows our cron job's execution history and lets you navigate to each invocation's logs.
# To trigger a manual run from your local code during development, you can also trigger this function using the cli:
# `modal run db_to_sheet.py::db_to_sheet`

# Note that all of the `@app.function()` annotated functions above run remotely in isolated containers that are specified per
# function, but they are called as seamlessly as if we were using regular Python functions. This is a simple
# showcase of how you can mix and match Modal Functions that use different environments and have them feed
# into each other or even call each other as if they were all functions in the same local program.


## Links discovered
- [Secrets](https://modal.com/secrets)
- [https://cloud.google.com/](https://cloud.google.com/)
- [https://console.cloud.google.com/](https://console.cloud.google.com/)
- [http://openweathermap.org](http://openweathermap.org)
- [`schedule`](https://modal.com/docs/guide/cron)
- [apps page](https://modal.com/apps)

--- README.md ---
<p align="center">
  <a href="https://modal.com">
    <img src="https://modal-cdn.com/Modal-Primary-Logo.png" height="96">
    <h3 align="center">Modal Examples</h3>
  </a>
</p>

This is a collection of examples for [Modal](https://modal.com/). Use these examples to learn Modal and build your own robust and scalable applications.

## Usage

First, sign up for a free account at [modal.com](https://modal.com/) and follow
the setup instructions to install the `modal` package and set your API key.

The examples are organized into several folders based on their category. You can
generally run the files in any folder much like you run ordinary Python programs, with a
command like:

```bash
modal run 01_getting_started/hello_world.py
```

Although these scripts are run on your local machine, they'll communicate with
Modal and run in our cloud, spawning serverless containers on demand.

## Examples

- [**`01_getting_started/`**](01_getting_started) through [**`14_clusters/`**](14_clusters) provide a guided tour through Modal's concepts and capabilities.
- [**`misc/`**](/misc) contains uncategorized, miscellaneous examples.

_These examples are continuously tested for correctness against Python **3.11**._

## License

The [MIT license](LICENSE).


## Links discovered
- [Modal](https://modal.com/)
- [modal.com](https://modal.com/)
- [**`01_getting_started/`**](https://github.com/modal-labs/modal-examples/blob/main/01_getting_started.md)
- [**`14_clusters/`**](https://github.com/modal-labs/modal-examples/blob/main/14_clusters.md)
- [**`misc/`**](https://github.com/modal-labs/modal-examples/blob/main/misc.md)
- [MIT license](https://github.com/modal-labs/modal-examples/blob/main/LICENSE.md)
- [<img src="https://modal-cdn.com/Modal-Primary-Logo.png" height="96"> <h3 align="center">Modal Examples</h3>](https://modal.com)

--- 02_building_containers/import_sklearn.py ---
# # Install scikit-learn in a custom image
#
# This builds a custom image which installs the sklearn (scikit-learn) Python package in it.
# It's an example of how you can use packages, even if you don't have them installed locally.
#
# First, the imports

import time

import modal

# Next, define an app, with a custom image that installs `sklearn`.

app = modal.App(
    "example-import-sklearn",
    image=modal.Image.debian_slim()
    .apt_install("libgomp1")
    .uv_pip_install("scikit-learn"),
)

# The `app.image.imports()` lets us conditionally import in the global scope.
# This is needed because we might not have sklearn and numpy installed locally,
# but we know they are installed inside the custom image.

with app.image.imports():
    import numpy as np
    from sklearn import datasets, linear_model

# Now, let's define a function that uses one of scikit-learn's built-in datasets
# and fits a very simple model (linear regression) to it


@app.function()
def fit():
    print("Inside run!")
    t0 = time.time()
    diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)
    diabetes_X = diabetes_X[:, np.newaxis, 2]
    regr = linear_model.LinearRegression()
    regr.fit(diabetes_X, diabetes_y)
    return time.time() - t0


# Finally, let's trigger the run locally. We also time this. Note that the first time we run this,
# it will build the image. This might take 1-2 min. When we run this subsequent times, the image
# is already build, and it will run much much faster.


if __name__ == "__main__":
    t0 = time.time()
    with app.run():
        t = fit.remote()
        print("Function time spent:", t)
    print("Full time spent:", time.time() - t0)


--- 02_building_containers/urls.txt ---
adobe.com
alibaba.com
aliexpress.com
amazon.com
apple.com
baidu.com
bbc.co.uk
bing.com
blogspot.com
booking.com
craigslist.org
dailymail.co.uk
dropbox.com
ebay.com
facebook.com
github.com
google.com
imdb.com
imgur.com
instagram.com


--- 03_scaling_out/basic_grid_search.py ---
# # Hyperparameter search
#
# This example showcases a simple grid search in one dimension, where we try different
# parameters for a model and pick the one with the best results on a holdout set.
#
# ## Defining the image
#
# First, let's build a custom image and install scikit-learn in it.

import modal

app = modal.App(
    "example-basic-grid-search",
    image=modal.Image.debian_slim().uv_pip_install("scikit-learn~=1.5.0"),
)

# ## The Modal function
#
# Next, define the function. Note that we use the custom image with scikit-learn in it.
# We also take the hyperparameter `k`, which is how many nearest neighbors we use.


@app.function()
def fit_knn(k):
    from sklearn.datasets import load_digits
    from sklearn.model_selection import train_test_split
    from sklearn.neighbors import KNeighborsClassifier

    X, y = load_digits(return_X_y=True)
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

    clf = KNeighborsClassifier(k)
    clf.fit(X_train, y_train)
    score = float(clf.score(X_test, y_test))
    print("k = %3d, score = %.4f" % (k, score))
    return score, k


# ## Parallel search
#
# To do a hyperparameter search, let's map over this function with different values
# for `k`, and then select for the best score on the holdout set:


@app.local_entrypoint()
def main():
    # Do a basic hyperparameter search
    best_score, best_k = max(fit_knn.map(range(1, 100)))
    print("Best k = %3d, score = %.4f" % (best_k, best_score))


--- 03_scaling_out/cls_with_options.py ---
# ---
# mypy: ignore-errors
# ---

# # Override Modal resource options (GPU, scaling) at runtime with `Cls.with_options`

# [`Cls.with_options`](https://modal.com/docs/reference/modal.Cls#with_options)
# lets you override the resource configuration of a
# Modal [Cls](https://modal.com/docs/guide/lifecycle-functions) at runtime.
# This is useful when the same code needs to run
# with different resource allocations -- say, with a GPU or with out,
# or with a large [warm pool of containers](https://modal.com/docs/guide/cold-start)
# -- at different times -- say, when iterating on code and when in production.

# Each call to `with_options` returns a new class handle that scales
# independently from the original.

# ## Setup

import modal

app = modal.App("example-cls-with-options")


# ## Defining the class

# We define a simple class with a method that performs a
# CPU-bound computation. The class is configured with modest defaults.


@app.cls(cpu=1, memory=128, timeout=60)
class Worker:
    @modal.method()
    def compute(self, n: int) -> int:
        import subprocess

        # if GPU available, prints details
        subprocess.Popen("nvidia-smi", shell=True)

        return sum(i * i for i in range(n))


# ## Using `with_options` to override configuration

# We can call `with_options` on the class to get a new handle
# with different resource settings.


@app.local_entrypoint()
def main():
    # Use the default configuration for a light workload
    default_worker = Worker()
    result = default_worker.compute.remote(1_000)
    print(f"Default worker result: {result}")

    # Create a GPU-accelerated variant
    GpuWorker = Worker.with_options(gpu="T4", memory=512)
    gpu_worker = GpuWorker()
    result = gpu_worker.compute.remote(10_000_000)
    print(f"GPU worker result:     {result}")


## Links discovered
- [`Cls.with_options`](https://modal.com/docs/reference/modal.Cls#with_options)
- [Cls](https://modal.com/docs/guide/lifecycle-functions)
- [warm pool of containers](https://modal.com/docs/guide/cold-start)

--- 03_scaling_out/dynamic_batching.py ---
# # Dynamic batching for ASCII and character conversion

# This example demonstrates how to dynamically batch a simple
# application that converts ASCII codes to characters and vice versa.

# For more details about using dynamic batching and optimizing
# the batching configurations for your application, see
# the [dynamic batching guide](https://modal.com/docs/guide/dynamic-batching).

# ## Setup

# Let's start by defining the image for the application.

import modal

app = modal.App(
    "example-dynamic-batching",
    image=modal.Image.debian_slim(python_version="3.11"),
)


# ## Defining a Batched Function

# Now, let's define a function that converts ASCII codes to characters. This
# async Batched Function allows us to convert up to four ASCII codes at once.


@app.function()
@modal.batched(max_batch_size=4, wait_ms=1000)
async def asciis_to_chars(asciis: list[int]) -> list[str]:
    return [chr(ascii) for ascii in asciis]


# If there are fewer than four ASCII codes in the batch, the Function will wait
# for one second, as specified by `wait_ms`, to allow more inputs to arrive before
# returning the result.

# The input `asciis` to the Function is a list of integers, and the
# output is a list of strings. To allow batching, the input list `asciis`
# and the output list must have the same length.

# You must invoke the Function with an individual ASCII input, and a single
# character will be returned in response.

# ## Defining a class with a Batched Method

# Next, let's define a class that converts characters to ASCII codes. This
# class has an async Batched Method `chars_to_asciis` that converts characters
# to ASCII codes.

# Note that if a class has a Batched Method, it cannot have other Batched Methods
# or Methods.


@app.cls()
class AsciiConverter:
    @modal.batched(max_batch_size=4, wait_ms=1000)
    async def chars_to_asciis(self, chars: list[str]) -> list[int]:
        asciis = [ord(char) for char in chars]
        return asciis


# ## ASCII and character conversion

# Finally, let's define the `local_entrypoint` that uses the Batched Function
# and Class Method to convert ASCII codes to characters and
# vice versa.

# We use [`map.aio`](https://modal.com/docs/reference/modal.Function#map) to asynchronously map
# over the ASCII codes and characters. This allows us to invoke the Batched
# Function and the Batched Method over a range of ASCII codes and characters
# in parallel.
#
# Run this script to see which characters correspond to ASCII codes 33 through 38!


@app.local_entrypoint()
async def main():
    ascii_converter = AsciiConverter()
    chars = []
    async for char in asciis_to_chars.map.aio(range(33, 39)):
        chars.append(char)

    print("Characters:", chars)

    asciis = []
    async for ascii in ascii_converter.chars_to_asciis.map.aio(chars):
        asciis.append(ascii)

    print("ASCII codes:", asciis)


## Links discovered
- [dynamic batching guide](https://modal.com/docs/guide/dynamic-batching)
- [`map.aio`](https://modal.com/docs/reference/modal.Function#map)

--- 05_scheduling/hackernews_alerts.py ---
# ---
# lambda-test: false  # missing-secret
# ---

# # Run cron jobs in the cloud to search Hacker News

# In this example, we use Modal to deploy a cron job that periodically queries Hacker News for
# new posts matching a given search term, and posts the results to Slack.

# ## Import and define the app

# Let's start off with imports, and defining a Modal app.

import os
from datetime import datetime, timedelta

import modal

app = modal.App("example-hackernews-alerts")

# Now, let's define an image that has the `slack-sdk` package installed, in which we can run a function
# that posts a slack message.

slack_sdk_image = modal.Image.debian_slim().uv_pip_install("slack-sdk")

# ## Defining the function and importing the secret

# Our Slack bot will need access to a bot token.
# We can use Modal's [Secrets](https://modal.com/secrets) interface to accomplish this.
# To quickly create a Slack bot secret, click the "Create new secret" button.
# Then, select the Slack secret template from the list options,
# and follow the instructions in the "Where to find the credentials?" panel.
# Name your secret `hn-bot-slack.`

# Now, we define the function `post_to_slack`, which simply instantiates the Slack client using our token,
# and then uses it to post a message to a given channel name.


@app.function(
    image=slack_sdk_image,
    secrets=[modal.Secret.from_name("hn-bot-slack", required_keys=["SLACK_BOT_TOKEN"])],
)
async def post_to_slack(message: str):
    import slack_sdk

    client = slack_sdk.WebClient(token=os.environ["SLACK_BOT_TOKEN"])
    client.chat_postMessage(channel="hn-alerts", text=message)


# ## Searching Hacker News

# We are going to use Algolia's [Hacker News Search API](https://hn.algolia.com/api) to query for posts
# matching a given search term in the past X days. Let's define our search term and query period.

QUERY = "serverless"
WINDOW_SIZE_DAYS = 1

# Let's also define an image that has the `requests` package installed, so we can query the API.

requests_image = modal.Image.debian_slim().uv_pip_install("requests")

# We can now define our main entrypoint, that queries Algolia for the term, and calls `post_to_slack`
# on all the results. We specify a [schedule](https://modal.com/docs/guide/cron)
# in the function decorator, which means that our function will run automatically at the given interval.


@app.function(image=requests_image)
def search_hackernews():
    import requests

    url = "http://hn.algolia.com/api/v1/search"

    threshold = datetime.utcnow() - timedelta(days=WINDOW_SIZE_DAYS)

    params = {
        "query": QUERY,
        "numericFilters": f"created_at_i>{threshold.timestamp()}",
    }

    response = requests.get(url, params, timeout=10).json()
    urls = [item["url"] for item in response["hits"] if item.get("url")]

    print(f"Query returned {len(urls)} items.")

    post_to_slack.for_each(urls)


# ## Test running

# We can now test run our scheduled function as follows: `modal run hackernews_alerts.py::app.search_hackernews`

# ## Defining the schedule and deploying

# Let's define a function that will be called by Modal every day


@app.function(schedule=modal.Period(days=1))
def run_daily():
    search_hackernews.remote()


# In order to deploy this as a persistent cron job, you can run `modal deploy hackernews_alerts.py`,

# Once the job is deployed, visit the [apps page](https://modal.com/apps) page to see
# its execution history, logs and other stats.


## Links discovered
- [Secrets](https://modal.com/secrets)
- [Hacker News Search API](https://hn.algolia.com/api)
- [schedule](https://modal.com/docs/guide/cron)
- [apps page](https://modal.com/apps)

--- 05_scheduling/schedule_simple.py ---
# ---
# cmd: ["python", "-m", "05_scheduling.schedule_simple"]
# ---

# # Scheduling remote jobs

# This example shows how you can schedule remote jobs on Modal.
# You can do this either with:
#
# - [`modal.Period`](https://modal.com/docs/reference/modal.Period) - a time interval between function calls.
# - [`modal.Cron`](https://modal.com/docs/reference/modal.Cron) - a cron expression to specify the schedule.

# In the code below, the first function runs every
# 5 seconds, and the second function runs every minute. We use the `schedule`
# argument to specify the schedule for each function. The `schedule` argument can
# take a `modal.Period` object to specify a time interval or a `modal.Cron` object
# to specify a cron expression.

import time
from datetime import datetime

import modal

app = modal.App("example-schedule-simple")


@app.function(schedule=modal.Period(seconds=5))
def print_time_1():
    print(
        f"Printing with period 5 seconds: {datetime.now().strftime('%m/%d/%Y, %H:%M:%S')}"
    )


@app.function(schedule=modal.Cron("* * * * *"))
def print_time_2():
    print(
        f"Printing with cron every minute: {datetime.now().strftime('%m/%d/%Y, %H:%M:%S')}"
    )


if __name__ == "__main__":
    with modal.enable_output():
        with app.run():
            time.sleep(60)


## Links discovered
- [`modal.Period`](https://modal.com/docs/reference/modal.Period)
- [`modal.Cron`](https://modal.com/docs/reference/modal.Cron)

--- 06_gpu_and_ml/embeddings/wikipedia/README.md ---
# Embedding Wikipedia in 15 minutes

This example shows how we can embed the entirety of english wikipedia on Modal in just 15 minutes. We've published a detailed writeup which walks you through the implemenation [here](#todo).

## Description

There are a total of 2 files in this repository

- `download.py` : This showcases how to download the Wikipedia dataset into a `Modal` volume. We can take advantage of `Modal`'s high internet speeds to download large datasets quickly.

- `main.py`: This showcases how to run an embedding job on your downloaded dataset and run a parallelizable job using Modal's inbuilt parallelization abstraction.

## Getting Started

You'll need a few packages to get started - we recommend using a virtual environment to install all of the dependencies listed in the `requirements.txt`

```bash
python3 -m venv venv
source venv/bin/activate
pip3 install modal
```

Once you've done so, you'll need to authenticate with Modal. To do so, run the command `modal token new`.

This will open up a new tab in your default browser and allow you to run, deploy and configure all of your Modal applications from your terminal.

## Downloading Our Dataset

Let's first download our Wikipedia dataset into a Modal volume. We can optimise the download time using the `num_proc ` keyword to parallelize some of the downloads.

From experience, this reduces the amount of time required by around 30-40% as long as we set a number between 4-10.

We can run our Download script using the command

```
modal run download.py
```

## Embedding our Dataset

Now that we've downloaded our wikipedia dataset, we can now embed the entire dataset using our `main.py` script. We can run it using the command

```
modal run main.py
```

Note that we utilize 2 volumes in our dataset script - one for reading from and another to write the files to upload to.

# Debugging

## Verifying that the Dataset has been downloaded

> Note that the `size` of the volume listed in the table for the directories. Our wikipedia directory is listed as having a size of 56B but the multiple .arrow files inside it should tell you that it in fact contains much larger files

Once we've downloaded the dataset, we can confirm that it has been downloaded and saved into our `embedding-wikipedia` volume at the path `/wikipedia` by runnning the command

```
modal volume ls embedding-wikipedia
```

This should produce a table that looks like this.

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ filename                                            â”ƒ type â”ƒ created/modified          â”ƒ size      â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ wikipedia                                           â”‚ dir  â”‚ 2023-12-02 10:57:44+01:00 â”‚ 56 B      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

We can then view what this folder looks like inside by appending the `/wikipedia` to our command

```
modal volume ls embedding-wikipedia /wikipedia
```

This will then show the files inside the `/wikipedia`

```
Directory listing of '/wikipedia' in 'embedding-wikipedia'
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”“
â”ƒ filename                    â”ƒ type â”ƒ created/modified          â”ƒ size    â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”©
â”‚ wikipedia/train             â”‚ dir  â”‚ 2023-12-02 10:58:12+01:00 â”‚ 4.0 KiB â”‚
â”‚ wikipedia/dataset_dict.json â”‚ file â”‚ 2023-12-02 10:57:44+01:00 â”‚ 21 B    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Removing Files

> Note that if you're looking to remove a directory, you need to supply the `--recursive` flag to the command for it to work.

If you'll like to save on storage costs when using volumes, you can use the modal cli to easily remove files.

```
modal volume rm embedding-wikipedia /wikipedia --recursive
```


--- 06_gpu_and_ml/openai_whisper/finetuning/readme.md ---
## Fine-tuning OpenAI's whisper model for improved automatic Hindi speech recognition

The following configuration will finetune the `whisper-small` model for almost 3 hrs,
acheiving a word error rate (WER) of about 55-60. Increasing the number of training
epochs should improve performance, decreasing WER.

You can benchmark this example's performance using Huggingface's [**autoevaluate leaderboard**]https://huggingface.co/spaces/autoevaluate/leaderboards?dataset=mozilla-foundation%2Fcommon_voice_11_0&only_verified=0&task=automatic-speech-recognition&config=hi&split=test&metric=wer).

```bash
modal run -m train.train --num_train_epochs=10
```

### Testing

Use `modal run -m train.end_to_end_check` to do a full train â†’ serialize â†’ save â†’ load â†’ predict
run in less than 5 minutes, checking that the finetuning program is functional.


--- 06_gpu_and_ml/llm-frontend/index.html ---
<html>
  <head>
    <script
      defer
      src="https://cdn.jsdelivr.net/npm/alpinejs@3.x.x/dist/cdn.min.js"
    ></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/marked/5.1.2/marked.min.js"></script>
    <link href="https://unpkg.com/@tailwindcss/typography@0.4.1/dist/typography.min.css" rel="stylesheet">
    <script src="https://cdn.tailwindcss.com"></script>

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Modal | LLM Engine</title>
  </head>
  <body>
    <section x-data="state()" class="max-w-2xl mx-auto pt-16 px-4">
      <div class="text-xs font-semibold tracking-wide uppercase text-center text-black">
        <a
          href="https://modal.com/docs/examples/text_generation_inference"
          class="inline-flex gap-x-1 items-center bg-lime-400 py-0.5 px-3 rounded-full hover:text-lime-400 hover:ring hover:ring-lime-400 hover:bg-white focus:outline-neutral-400"
          target="_blank"
        >
          powered by Modal
          <svg
            xmlns="http://www.w3.org/2000/svg"
            class="w-4 h-4 animate-pulse -mr-1"
            viewBox="0 0 24 24"
            fill="none"
            stroke="currentColor"
            stroke-width="2"
            stroke-linecap="round"
            stroke-linejoin="round"
            class="lucide lucide-chevrons-right"
          >
            <path d="m6 17 5-5-5-5" />
            <path d="m13 17 5-5-5-5" />
          </svg>
        </a>
      </div>
      <div class="text-4xl mt-4 mb-4 font-semibold tracking-tighter text-center">
        Modal LLM Engine
      </div>
      <div x-show="info.loaded && info.model" x-text="info.model" class="text-2xl mb-4 font-medium tracking-tighter text-center">
      </div>

      <div class="flex flex-wrap justify-center items-center mt-4 mb-6">
        <div
          x-init="setInterval(() => refreshInfo(), 1000)"
          class="inline-flex flex-col sm:flex-row justify-center items-center gap-x-4 text-sm text-white px-3 py-1 bg-neutral-600 rounded-full"
        >
          <div x-show="!info.loaded" class="flex items-center gap-x-1">
            <div class="animate-spin w-4 h-4">
              <svg
                xmlns="http://www.w3.org/2000/svg"
                viewBox="0 0 24 24"
                fill="none"
                stroke="currentColor"
                stroke-width="2"
                stroke-linecap="round"
                stroke-linejoin="round"
              >
                <path d="M21 12a9 9 0 1 1-6.219-8.56" />
              </svg>
            </div>
            <span>loading stats</span>
          </div>
          <div x-show="info.loaded && info.backlog > 0">
            <span x-text="info.backlog"></span>
            <span x-text="info.backlog === 1 ? 'input in queue' : 'inputs in queue'"></span>
          </div>
          <div x-show="info.loaded && (info.backlog === 0)">
            <span x-text="info.num_total_runners"></span>
            <span x-text="info.num_total_runners === 1 ? 'container online' : 'containers online'"></span>
          </div>
          <div
            class="flex items-center gap-x-1"
            x-show="info.loaded && info.backlog > 0"
          >
            <div class="animate-spin w-4 h-4">
              <svg
                xmlns="http://www.w3.org/2000/svg"
                viewBox="0 0 24 24"
                fill="none"
                stroke="currentColor"
                stroke-width="2"
                stroke-linecap="round"
                stroke-linejoin="round"
              >
                <path d="M21 12a9 9 0 1 1-6.219-8.56" />
              </svg>
            </div>
            <span> cold-starting </span>
          </div>
        </div>
      </div>

      <form class="relative">
        <input
          x-model="nextPrompt"
          type="text"
          placeholder="Ask something ..."
          class="flex grow w-full h-10 pl-4 pr-12 py-2 text-md bg-white border rounded-3xl border-neutral-300 ring-offset-background placeholder:text-neutral-500 focus:border-neutral-300 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-neutral-400 disabled:cursor-not-allowed disabled:opacity-50"
          @keydown.window.prevent.ctrl.k="$el.focus()"
          @keydown.window.prevent.cmd.k="$el.focus()"
          autofocus
        />
        <div class="absolute top-0 right-0 flex items-center h-full pr-[0.3125rem]">
          <button
            @click.prevent="callApi()"
            class="rounded-full bg-lime-400 p-2 focus:border-neutral-300 focus:outline-neutral-400"
          >
            <svg
              xmlns="http://www.w3.org/2000/svg"
              class="w-4 h-4"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="2"
              stroke-linecap="round"
              stroke-linejoin="round"
              class="lucide lucide-plus"
            >
              <path d="M5 12h14" />
              <path d="M12 5v14" />
            </svg>
          </button>
        </div>
      </form>

      <div class="flex flex-col gap-y-4 my-8">
        <template x-for="(item, index) in [...items].reverse()" :key="index">
          <div class="w-full border px-4 py-2 rounded-3xl">
            <div
              x-data
              class="text-sm mt-2 mb-4 whitespace-pre-line"
              x-text="item.prompt"
              :class="{'animate-pulse': item.loading}"
            ></div>
            <div
              x-show="item.completion.length === 0"
              class="h-4 w-2 mt-2 mb-4 bg-neutral-500 animate-pulse"
            ></div>
            <div
              class="text-sm mt-2 mb-4 text-neutral-500 w-full prose max-w-none prose-neutral-100 leading-6"
              x-show="item.completion.length > 0"
              x-html="item.markdownCompletion"
            >
            </div>
          </div>
        </template>
      </div>

      <script>
        function state() {
          return {
            nextPrompt: "",
            items: [],
            info: { backlog: 0, num_total_runners: 0 },
            callApi() {
              console.log(this.nextPrompt);
              if (!this.nextPrompt) return;

              let item = {
                id: Math.random(),
                prompt: this.nextPrompt,
                completion: "",
                loading: true,
                markdownCompletion: "",
              };
              this.nextPrompt = "";
              this.items.push(item);
              const eventSource = new EventSource(
                `/completion/${encodeURIComponent(item.prompt)}`,
              );

              console.log("Created event source ...");

              eventSource.onmessage = (event) => {
                item.completion += JSON.parse(event.data).text;
                item.markdownCompletion = marked.parse(item.completion, {mangle: false, headerIds: false});
                // Hacky way to notify element to update
                this.items = this.items.map((i) =>
                  i.id === item.id ? { ...item } : i,
                );
              };

              eventSource.onerror = (event) => {
                eventSource.close();
                item.loading = false;
                this.items = this.items.map((i) =>
                  i.id === item.id ? { ...item } : i,
                );
                console.log(item.completion);
              };
            },
            refreshInfo() {
              fetch("/stats")
                .then((response) => response.json())
                .then((data) => {
                  this.info = { ...data, loaded: true };
                })
                .catch((error) => console.log(error));
            },
          };
        }
      </script>
    </section>
  </body>
</html>

## Links discovered
- [powered by Modal <svg xmlns="http://www.w3.org/2000/svg" class="w-4 h-4 animate-pulse -mr-1" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevrons-right" > <path d="m6 17 5-5-5-5" /> <path d="m13 17 5-5-5-5" /> </svg>](https://modal.com/docs/examples/text_generation_inference)

--- 06_gpu_and_ml/image-to-video/frontend/index.html ---
<html>
  <head>
    <script
      defer
      src="https://cdn.jsdelivr.net/npm/alpinejs@3.x.x/dist/cdn.min.js"
    ></script>
    <script src="https://cdn.tailwindcss.com"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>{{ model_name }} â€” Modal</title>
  </head>
  <body x-data="state()">
    <div class="max-w-3xl mx-auto pt-4 pb-8 px-10 sm:py-12 sm:px-6 lg:px-8">
      <h2 class="text-3xl font-medium text-center mb-10">
        {{ model_name }} on Modal
      </h2>

      <form
        @submit.prevent="submitPrompt"
        class="flex flex-col items-center justify-center gap-x-4 gap-y-2 w-full mx-auto mb-4"
      >
        <textarea
          x-data
          x-model="prompt"
          x-init="$nextTick(() => { $el.focus(); });"
          rows="2"
          class="w-full px-3 py-3 mb-3 text-md bg-white border rounded-md border-neutral-300 ring-offset-background placeholder:text-neutral-500 focus:border-neutral-300 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-neutral-400 disabled:cursor-not-allowed disabled:opacity-50 text-center"
        ></textarea>
        <div class="flex w-full justify-between">
          <input
            type="file"
            accept="image/*"
            @change="previewImage"
            @click="$event.target.value = null;"
            class="text-sm text-gray-500 file:mr-4 file:py-2 file:px-4 file:rounded-lg file:border-0 file:text-sm file:font-semibold file:bg-neutral-950 file:text-white hover:file:bg-neutral-900"
          />
          <button
            type="submit"
            class="px-4 py-2 text-sm font-semibold tracking-wide text-white transition-colors duration-200 rounded-md bg-neutral-950 hover:bg-neutral-900 focus:ring-2 focus:ring-offset-2 focus:ring-neutral-900 focus:shadow-outline focus:outline-none"
            :disabled="loading"
          >
            <span x-show="!loading">Submit</span>
            <div class="animate-spin w-6 h-6 mx-3" x-show="loading">
              <svg
                xmlns="http://www.w3.org/2000/svg"
                viewBox="0 0 24 24"
                fill="none"
                stroke="currentColor"
                stroke-width="2"
                stroke-linecap="round"
                stroke-linejoin="round"
                class="lucide lucide-loader-2"
              >
                <path d="M21 12a9 9 0 1 1-6.219-8.56" />
              </svg>
            </div>
          </button>
        </div>
      </form>

      <div class="mx-auto w-full max-w-[768px] relative grid">
        <div
          style="padding-top: 100%"
          x-show="loading"
          class="absolute w-full h-full animate-pulse bg-neutral-100 rounded-md"
        ></div>
        <img
          x-show="imageURL && !videoURL"
          class="rounded-md self-center justify-self-center"
          :src="imageURL"
        />
        <video
          x-bind:src="videoURL"
          x-show="videoURL"
          controls
          class="w-full rounded-md"
        ></video>
      </div>
    </div>

    <script>
      function state() {
        return {
          prompt: "{{ default_prompt }}",
          submitted: "",
          loading: false,
          imageURL: "",
          videoURL: "",
          selectedFile: null,
          previewImage(event) {
            const file = event.target.files[0];
            if (file) {
              this.selectedFile = file;
              this.imageURL = URL.createObjectURL(file);
              this.videoURL = "";
            }
          },
          async submitPrompt() {
            if (!this.prompt || !this.selectedFile) return;
            this.submitted = this.prompt;
            this.loading = true;

            try {
              const formData = new FormData();
              formData.append("image_bytes", this.selectedFile);

              url = `{{ inference_url }}?prompt=${this.prompt}`;
              const res = await fetch(url, {
                method: "POST",
                headers: {
                  accept: "application/json",
                },
                body: formData,
              });

              if (!res.ok) {
                throw new Error("Inference failed");
              }

              const blob = await res.blob();
              this.videoURL = URL.createObjectURL(blob);
              this.imageURL = "";
            } catch (error) {
              console.error("Fetch failed:", error);
              alert("There was an error generating the video.");
            } finally {
              this.loading = false;
            }
          },
        };
      }
    </script>
  </body>
</html>


--- 06_gpu_and_ml/stable_diffusion/frontend/index.html ---
<html>
  <head>
    <script
      defer
      src="https://cdn.jsdelivr.net/npm/alpinejs@3.x.x/dist/cdn.min.js"
    ></script>
    <script src="https://cdn.tailwindcss.com"></script>

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>{{ model_name }} â€” Modal</title>
  </head>
  <body x-data="state()">
    <div class="max-w-3xl mx-auto pt-4 pb-8 px-10 sm:py-12 sm:px-6 lg:px-8">
      <h2 class="text-3xl font-medium text-center mb-10">
        {{ model_name }} on Modal
      </h2>

      <form
        @submit.prevent="submitPrompt"
        class="flex items-center justify-center gap-x-4 gap-y-2 w-full mx-auto mb-4"
      >
        <input
          x-data
          x-model="prompt"
          x-init="$nextTick(() => { $el.focus(); });"
          type="text"
          class="flex w-full px-3 py-3 text-md bg-white border rounded-md border-neutral-300 ring-offset-background placeholder:text-neutral-500 focus:border-neutral-300 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-neutral-400 disabled:cursor-not-allowed disabled:opacity-50 text-center"
        />
        <button
          type="submit"
          class="inline-flex items-center justify-center px-4 py-3 text-sm font-medium tracking-wide text-white transition-colors duration-200 rounded-md bg-neutral-950 hover:bg-neutral-900 focus:ring-2 focus:ring-offset-2 focus:ring-neutral-900 focus:shadow-outline focus:outline-none"
          :disabled="loading"
        >
          <span x-show="!loading">Submit</span>
          <div class="animate-spin w-6 h-6 mx-3" x-show="loading">
            <svg
              xmlns="http://www.w3.org/2000/svg"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="2"
              stroke-linecap="round"
              stroke-linejoin="round"
              class="lucide lucide-loader-2"
            >
              <path d="M21 12a9 9 0 1 1-6.219-8.56" />
            </svg>
          </div>
        </button>
      </form>

      <div class="mx-auto w-full max-w-[768px] relative grid">
        <div
          style="padding-top: 100%"
          x-show="loading"
          class="absolute w-full h-full animate-pulse bg-neutral-100 rounded-md"
        ></div>
        <img
          x-show="imageURL"
          class="rounded-md self-center justify-self-center"
          :src="imageURL"
        />
      </div>
    </div>

    <script>
      function state() {
        return {
          prompt: "{{ default_prompt }}",
          submitted: "",
          loading: false,
          imageURL: "",
          async submitPrompt() {
            if (!this.prompt) return;
            this.submitted = this.prompt;
            this.loading = true;

            try {
              const res = await fetch(
                `{{ inference_url }}?prompt=${this.submitted}`,
              );

              if (!res.ok) {
                throw new Error("Inference failed");
              }

              const blob = await res.blob();
              this.imageURL = URL.createObjectURL(blob);
            } catch (error) {
              console.error("Fetch failed:", error);
              alert("There was an error generating the image.");
            } finally {
              this.loading = false;
              console.log(this.imageURL);
            }
          },
        };
      }
    </script>
  </body>
</html>


--- 06_gpu_and_ml/speech-to-text/multitalker-frontend/index.html ---
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Audio Transcription</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        #controls {
            margin: 20px 0;
        }
        #recordButton {
            padding: 10px 20px;
            font-size: 16px;
            cursor: pointer;
            background-color: #4CAF50;
            color: white;
            border: none;
            border-radius: 4px;
        }
        #recordButton.recording {
            background-color: #f44336;
        }
        #transcription {
            margin-top: 20px;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 4px;
            min-height: 100px;
        }
    </style>
</head>
<body>
    <h1>Parakeet Streaming Transcription</h1>
    <p><em>Tip: Turn your microphone volume up for better transcription quality.</em></p>
    <div id="controls">
        <button id="recordButton">Start Transcribing Mic</button>
    </div>
    <div id="transcription"></div>
    <script src="/static/parakeet.js"></script>
</body>
</html>


--- 06_gpu_and_ml/speech-to-text/streaming-diarization-frontend/index.html ---
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Audio Transcription</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        #controls {
            margin: 20px 0;
        }
        #recordButton {
            padding: 10px 20px;
            font-size: 16px;
            cursor: pointer;
            background-color: #4CAF50;
            color: white;
            border: none;
            border-radius: 4px;
        }
        #recordButton.recording {
            background-color: #f44336;
        }
        #transcription {
            margin-top: 20px;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 4px;
            min-height: 100px;
        }
    </style>
</head>
<body>
    <h1>Streaming Speaker Diarization with nvidia/diar_streaming_sortformer_4spk-v2_1</h1>
    <p><em>Tip: Turn your microphone volume up for better transcription quality.</em></p>
    <div id="controls">
        <button id="recordButton">Start Diarizing</button>
    </div>
    <div id="speakers" style="display: flex; gap: 20px; margin-top: 20px;">
        <div class="speaker-box" id="speaker-0">
            <div class="speaker-label">Speaker 0</div>
            <div class="speaker-indicator" style="width: 100px; height: 100px; background-color: #80EE64; opacity: 0.1;"></div>
            <div class="speaker-prob">0.00</div>
        </div>
        <div class="speaker-box" id="speaker-1">
            <div class="speaker-label">Speaker 1</div>
            <div class="speaker-indicator" style="width: 100px; height: 100px; background-color: #80EE64; opacity: 0.1;"></div>
            <div class="speaker-prob">0.00</div>
        </div>
        <div class="speaker-box" id="speaker-2">
            <div class="speaker-label">Speaker 2</div>
            <div class="speaker-indicator" style="width: 100px; height: 100px; background-color: #80EE64; opacity: 0.1;"></div>
            <div class="speaker-prob">0.00</div>
        </div>
        <div class="speaker-box" id="speaker-3">
            <div class="speaker-label">Speaker 3</div>
            <div class="speaker-indicator" style="width: 100px; height: 100px; background-color: #80EE64; opacity: 0.1;"></div>
            <div class="speaker-prob">0.00</div>
        </div>
    </div>
    <!-- <div id="transcription"></div> -->
    <script src="/static/sortformer2_1.js"></script>
</body>
</html>


--- 06_gpu_and_ml/speech-to-text/streaming-parakeet-frontend/index.html ---
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Audio Transcription</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        #controls {
            margin: 20px 0;
        }
        #recordButton {
            padding: 10px 20px;
            font-size: 16px;
            cursor: pointer;
            background-color: #4CAF50;
            color: white;
            border: none;
            border-radius: 4px;
        }
        #recordButton.recording {
            background-color: #f44336;
        }
        #transcription {
            margin-top: 20px;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 4px;
            min-height: 100px;
        }
    </style>
</head>
<body>
    <h1>Audio Transcription</h1>
    <p><em>Tip: Turn your microphone volume up for better transcription quality.</em></p>
    <div id="controls">
        <button id="recordButton">Start Transcribing Mic</button>
    </div>
    <div id="transcription"></div>
    <script src="/static/parakeet.js"></script>
</body>
</html>


--- 06_gpu_and_ml/gpu_fallbacks.py ---
# # Set "fallback" GPUs
#
# GPU availabilities on Modal can fluctuate, especially for
# tightly-constrained requests, like for eight co-located GPUs
# in a specific region.
#
# If your code can run on multiple different GPUs, you can specify
# your GPU request as a list, in order of preference, and whenever
# your Function scales up, we will try to schedule it on each requested GPU type in order.
#
# The code below demonstrates the usage of the `gpu` parameter with a list of GPUs.

import subprocess

import modal

app = modal.App("example-gpu-fallbacks")


@app.function(
    gpu=["h100", "a100", "any"],  # "any" means any of L4, A10, or T4
    single_use_containers=True,  # new container each input, so we re-roll the GPU dice every time
)
async def remote(_idx):
    gpu = subprocess.run(
        ["nvidia-smi", "--query-gpu=name", "--format=csv,noheader"],
        check=True,
        text=True,
        stdout=subprocess.PIPE,
    ).stdout.strip()
    print(gpu)
    return gpu


@app.local_entrypoint()
def local(count: int = 32):
    from collections import Counter

    gpu_counter = Counter(remote.map([i for i in range(count)], order_outputs=False))
    print(f"ran {gpu_counter.total()} times")
    print(f"on the following {len(gpu_counter.keys())} GPUs:", end="\n")
    print(
        *[f"{gpu.rjust(32)}: {'ðŸ”¥' * ct}" for gpu, ct in gpu_counter.items()],
        sep="\n",
    )


--- 06_gpu_and_ml/gpu_packing.py ---
# ---
# mypy: ignore-errors
# ---
# # Run multiple instances of a model on a single GPU
#
# Many models are small enough to fit multiple instances onto a single GPU.
# Doing so can dramatically reduce the number of GPUs needed to handle demand.
#
# We use `@modal.concurrent` to allow multiple connections into the container
# We load the model instances into a FIFO queue to ensure only one http handler can access it at once

import asyncio
import time
from contextlib import asynccontextmanager

import modal

MODEL_PATH = "/model.bge"


def download_model():
    from sentence_transformers import SentenceTransformer

    model = SentenceTransformer("BAAI/bge-small-en-v1.5")
    model.save(MODEL_PATH)


image = (
    modal.Image.debian_slim(python_version="3.12")
    .uv_pip_install("sentence-transformers==3.2.0")
    .run_function(download_model)
)

app = modal.App("example-gpu-packing", image=image)


# ModelPool holds multiple instances of the model, using a queue
class ModelPool:
    def __init__(self):
        self.pool: asyncio.Queue = asyncio.Queue()

    async def put(self, model):
        await self.pool.put(model)

    # We provide a context manager to easily acquire and release models from the pool
    @asynccontextmanager
    async def acquire_model(self):
        model = await self.pool.get()
        try:
            yield model
        finally:
            await self.pool.put(model)


with image.imports():
    from sentence_transformers import SentenceTransformer


@app.cls(
    gpu="A10G",
    max_containers=1,  # Max one container for this app, for the sake of demoing concurrent_inputs
)
@modal.concurrent(max_inputs=100)  # Allow concurrent inputs into our single container.
class Server:
    n_models: int = modal.parameter(default=10)

    @modal.enter()
    def init(self):
        self.model_pool = ModelPool()

    @modal.enter()
    async def load_models(self):
        # Boot N models onto the gpu, and place into the pool
        t0 = time.time()
        for i in range(self.n_models):
            model = SentenceTransformer("/model.bge", device="cuda")
            await self.model_pool.put(model)

        print(f"Loading {self.n_models} models took {time.time() - t0:.4f}s")

    @modal.method()
    def prewarm(self):
        pass

    @modal.method()
    async def predict(self, sentence):
        # Block until a model is available
        async with self.model_pool.acquire_model() as model:
            # We now have exclusive access to this model instance
            embedding = model.encode(sentence)
            await asyncio.sleep(
                0.2
            )  # Simulate extra inference latency, for demo purposes
        return embedding.tolist()


@app.local_entrypoint()
async def main(n_requests: int = 100):
    # We benchmark with 100 requests in parallel.
    # Thanks to @modal.concurrent(), 100 requests will enter .predict() at the same time.

    sentences = ["Sentence {}".format(i) for i in range(n_requests)]

    # Baseline: a server with a pool size of 1 model
    print("Testing Baseline (1 Model)")
    t0 = time.time()
    server = Server(n_models=1)
    server.prewarm.remote()
    print("Container boot took {:.4f}s".format(time.time() - t0))

    t0 = time.time()
    async for result in server.predict.map.aio(sentences):
        pass
    print(f"Inference took {time.time() - t0:.4f}s\n")

    # Packing: a server with a pool size of 10 models
    # Note: this increases boot time, but reduces inference time
    print("Testing Packing (10 Models)")
    t0 = time.time()
    server = Server(n_models=10)
    server.prewarm.remote()
    print("Container boot took {:.4f}s".format(time.time() - t0))

    t0 = time.time()
    async for result in server.predict.map.aio(sentences):
        pass
    print(f"Inference took {time.time() - t0:.4f}s\n")


--- 07_web_endpoints/webrtc/frontend/index.html ---
<!DOCTYPE html>
<html>
<head>
    <title>WebRTC YOLO Demo</title>
    <style>
        video {
            width: 320px;
            height: 240px;
            margin: 10px;
            border: 1px solid black;
        }
        button {
            margin: 10px;
            padding: 10px;
        }
        #videos {
            display: flex;
            flex-wrap: wrap;
        }
        .radio-group {
            margin: 10px;
            padding: 10px;
            border: 1px solid #ccc;
            border-radius: 4px;
        }
        .radio-group label {
            margin-right: 15px;
        }
        #statusDisplay {
            margin: 10px;
            padding: 10px;
            border: 1px solid #ccc;
            border-radius: 4px;
            background-color: #f5f5f5;
            min-height: 20px;
            max-height: 150px;
            overflow-y: auto;
            font-family: monospace;
            white-space: pre-wrap;
            word-wrap: break-word;
        }
        .status-line {
            margin: 2px 0;
            padding: 2px;
            border-bottom: 1px solid #eee;
        }
    </style>
</head>
<body>
    <div class="radio-group">
        <label>
            <input type="radio" name="iceServer" value="stun" checked> STUN Server
        </label>
        <label>
            <input type="radio" name="iceServer" value="turn"> TURN Server
        </label>
    </div>
    <div id="videos">
        <video id="localVideo" autoplay playsinline muted></video>
        <video id="remoteVideo" autoplay playsinline></video>
    </div>
    <div>
        <button id="startWebcamButton">Start Webcam</button>
        <button id="startStreamingButton" disabled>Stream YOLO</button>
        <button id="stopStreamingButton" disabled>Stop Streaming</button>
    </div>
    <div id="statusDisplay"></div>
    <script type="module" src="/static/webcam_webrtc.js"></script>
</body>
</html> 

--- 07_web_endpoints/badges.py ---
# ---
# cmd: ["modal", "serve", "07_web_endpoints/badges.py"]
# ---

# # Serve a dynamic SVG badge

# In this example, we use Modal's [webhook](https://modal.com/docs/guide/webhooks) capability to host a dynamic SVG badge that shows
# you the current number of downloads for a Python package.

# First let's start off by creating a Modal app, and defining an image with the Python packages we're going to be using:

import modal

image = modal.Image.debian_slim().uv_pip_install(
    "fastapi[standard]", "pybadges", "pypistats"
)

app = modal.App("example-badges", image=image)

# ## Defining the web endpoint

# In addition to using `@app.function()` to decorate our function, we use the
# [`@modal.fastapi_endpoint` decorator](https://modal.com/docs/guide/webhooks)
# which instructs Modal to create a REST endpoint that serves this function.
# Note that the default method is `GET`, but this can be overridden using the `method` argument.


@app.function()
@modal.fastapi_endpoint()
async def package_downloads(package_name: str):
    import json

    import pypistats
    from fastapi import Response
    from pybadges import badge

    stats = json.loads(pypistats.recent(package_name, format="json"))
    svg = badge(
        left_text=f"{package_name} downloads",
        right_text=str(stats["data"]["last_month"]),
        right_color="blue",
    )

    return Response(content=svg, media_type="image/svg+xml")


# In this function, we use `pypistats` to query the most recent stats for our package, and then
# use that as the text for a SVG badge, rendered using `pybadges`.
# Since Modal web endpoints are FastAPI functions under the hood, we return this SVG wrapped in a FastAPI response with the correct media type.
# Also note that FastAPI automatically interprets `package_name` as a [query param](https://fastapi.tiangolo.com/tutorial/query-params/).

# ## Running and deploying

# We can now run an ephemeral app on the command line using:

# ```shell
# modal serve badges.py
# ```

# This will create a short-lived web url that exists until you terminate the script.
# It will also hot-reload the code if you make changes to it.

# If you want to create a persistent URL, you have to deploy the script.
# To deploy using the Modal CLI by running `modal deploy badges.py`,

# Either way, as soon as we run this command, Modal gives us the link to our brand new
# web endpoint in the output:

# ![web badge deployment](./badges_deploy.png)

# We can now visit the link using a web browser, using a `package_name` of our choice in the URL query params.
# For example:
# - `https://YOUR_SUBDOMAIN.modal.run/?package_name=synchronicity`
# - `https://YOUR_SUBDOMAIN.modal.run/?package_name=torch`


## Links discovered
- [webhook](https://modal.com/docs/guide/webhooks)
- [`@modal.fastapi_endpoint` decorator](https://modal.com/docs/guide/webhooks)
- [query param](https://fastapi.tiangolo.com/tutorial/query-params/)
- [web badge deployment](https://github.com/modal-labs/modal-examples/blob/main/07_web_endpoints/badges_deploy.png)

--- 07_web_endpoints/basic_web.py ---
# ---
# cmd: ["modal", "serve", "07_web_endpoints/basic_web.py"]
# ---

# # Hello world wide web!

# Modal makes it easy to turn your Python functions into serverless web services:
# access them via a browser or call them from any client that speaks HTTP, all
# without having to worry about setting up servers or managing infrastructure.

# This tutorial shows the path with the shortest ["time to 200"](https://shkspr.mobi/blog/2021/05/whats-your-apis-time-to-200/):
# [`modal.fastapi_endpoint`](https://modal.com/docs/reference/modal.fastapi_endpoint).

# On Modal, web endpoints have all the superpowers of Modal Functions:
# they can be [accelerated with GPUs](https://modal.com/docs/guide/gpu),
# they can access [Secrets](https://modal.com/docs/guide/secrets) or [Volumes](https://modal.com/docs/guide/volumes),
# and they [automatically scale](https://modal.com/docs/guide/cold-start) to handle more traffic.

# Under the hood, we use the [FastAPI library](https://fastapi.tiangolo.com/),
# which has [high-quality documentation](https://fastapi.tiangolo.com/tutorial/),
# linked throughout this tutorial.

# ## Turn a Modal Function into an API endpoint with a single decorator

# Modal Functions are already accessible remotely -- when you add the `@app.function` decorator to a Python function
# and run `modal deploy`, you make it possible for your [other Python functions to call it](https://modal.com/docs/guide/trigger-deployed-functions).

# That's great, but it's not much help if you want to share what you've written with someone running code in a different language --
# or not running code at all!

# And that's where most of the power of the Internet comes from: sharing information and functionality across different computer systems.

# So we provide the `fastapi_endpoint` decorator to wrap your Modal Functions in the lingua franca of the web: HTTP.
# Here's what that looks like:

import modal

image = modal.Image.debian_slim().uv_pip_install("fastapi[standard]")
app = modal.App(name="example-basic-web", image=image)


@app.function()
@modal.fastapi_endpoint(
    docs=True  # adds interactive documentation in the browser
)
def hello():
    return "Hello world!"


# You can turn this function into a web endpoint by running `modal serve basic_web.py`.
# In the output, you should see a URL that ends with `hello-dev.modal.run`.
# If you navigate to this URL, you should see the `"Hello world!"` message appear in your browser.

# You can also find interactive documentation, powered by OpenAPI and Swagger,
# if you add `/docs` to the end of the URL.
# From this documentation, you can interact with your endpoint, sending HTTP requests and receiving HTTP responses.
# For more details, see the [FastAPI documentation](https://fastapi.tiangolo.com/features/#automatic-docs).

# By running the endpoint with `modal serve`, you created a temporary endpoint that will disappear if you interrupt your terminal.
# These temporary endpoints are great for debugging -- when you save a change to any of your dependent files, the endpoint will redeploy.
# Try changing the message to something else, hitting save, and then hitting refresh in your browser or re-sending
# the request from `/docs` or the command line. You should see the new message, along with logs in your terminal showing the redeploy and the request.

# When you're ready to deploy this endpoint permanently, run `modal deploy basic_web.py`.
# Now, your function will be available even when you've closed your terminal or turned off your computer.

# ## Send data to a web endpoint

# The web endpoint above was a bit silly: it always returns the same message.

# Most endpoints need an input to be useful. There are two ways to send data to a web endpoint:
# - in the URL as a [query parameter](#sending-data-in-query-parameters)
# - in the [body of the request](#sending-data-in-the-request-body) as JSON

# ### Sending data in query parameters

# By default, your function's arguments are treated as query parameters:
# they are extracted from the end of the URL, where they should be added in the form
# `?arg1=foo&arg2=bar`.

# From the Python side, there's hardly anything to do:


@app.function()
@modal.fastapi_endpoint(docs=True)
def greet(user: str) -> str:
    return f"Hello {user}!"


# If you are already running `modal serve basic_web.py`, this endpoint will be available at a URL, printed in your terminal, that ends with `greet-dev.modal.run`.

# We provide Python type-hints to get type information in the docs and
# [automatic validation](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/).
# For example, if you navigate directly to the URL for `greet`, you will get a detailed error message
# indicating that the `user` parameter is missing. Navigate instead to `/docs` to see how to invoke the endpoint properly.

# You can read more about query parameters in the [FastAPI documentation](https://fastapi.tiangolo.com/tutorial/query-params/).


# ### Sending data in the request body

# For larger and more complex data, it is generally preferrable to send data in the body of the HTTP request.
# This body is formatted as [JSON](https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Objects/JSON),
# the most common data interchange format on the web.

# To set up an endpoint that accepts JSON data, add an argument with a `dict` type-hint to your function.
# This argument will be populated with the data sent in the request body.


@app.function()
@modal.fastapi_endpoint(method="POST", docs=True)
def goodbye(data: dict) -> str:
    name = data.get("name") or "world"
    return f"Goodbye {name}!"


# Note that we gave a value of `"POST"` for the `method` argument here.
# This argument defines the HTTP request method that the endpoint will respond to,
# and it defaults to `"GET"`.
# If you head to the URL for the `goodbye` endpoint in your browser,
# you will get a 405 Method Not Allowed error, because browsers only send GET requests by default.
# While this is technically a separate concern from query parameters versus request bodies
# and you can define an endpoint that accepts GET requests and uses data from the body,
# it is [considered bad form](https://stackoverflow.com/a/983458).

# Navigate to `/docs` for more on how to invoke the endpoint properly.
# You will need to send a POST request with a JSON body containing a `name` key.
# To get the same typing and validation benefits as with query parameters,
# use a [Pydantic model](https://fastapi.tiangolo.com/tutorial/body/)
# for this argument.

# You can read more about request bodies in the [FastAPI documentation](https://fastapi.tiangolo.com/tutorial/body/).

# ## Handle expensive startup with `modal.Cls`

# Sometimes your endpoint needs to do something before it can handle its first request,
# like get a value from a database or set the value of a variable.
# If that step is expensive, like [loading a large ML model](https://modal.com/docs/guide/model-weights),
# it'd be a shame to have to do it every time a request comes in!

# Web endpoints can be methods on a [`modal.Cls`](https://modal.com/docs/guide/lifecycle-functions#container-lifecycle-functions-and-parameters),
# which allows you to manage the container's lifecycle independently from processing individual requests.

# This example will only set the `start_time` instance variable once, on container startup.


@app.cls()
class WebApp:
    @modal.enter()
    def startup(self):
        from datetime import datetime, timezone

        print("ðŸ Starting up!")
        self.start_time = datetime.now(timezone.utc)

    @modal.fastapi_endpoint(docs=True)
    def web(self):
        from datetime import datetime, timezone

        current_time = datetime.now(timezone.utc)
        return {"start_time": self.start_time, "current_time": current_time}


# ## Protect web endpoints with proxy authentication

# Sharing your Python functions on the web is great, but it's not always a good idea
# to make those functions available to just anyone.

# For example, you might have a function like the one below that
# is more expensive to run than to call (and so might be abused by your enemies)
# or reveals information that you would rather keep secret.

# To protect your Modal web endpoints so that they can't be triggered except
# by members of your [Modal workspace](https://modal.com/docs/guide/workspaces),
# add the `requires_proxy_auth=True` flag to the `fastapi_endpoint` decorator.


@app.function(gpu="h100")
@modal.fastapi_endpoint(requires_proxy_auth=True, docs=False)
def expensive_secret():
    return "I didn't care for 'The Godfather'. It insists upon itself."


# The `expensive-secret` endpoint URL will still be printed to the output when you `modal serve` or `modal deploy`,
# along with a "ðŸ”‘" emoji indicating that it is secured with proxy authentication.
# If you head to that URL via the browser, you will get a
# [`401 Unauthorized`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/401) error code in response.
# You should also check the dashboard page for this app (at the URL printed at the very top of the `modal` command output)
# so you can see that no containers were spun up to handle the request -- this authorization is handled entirely inside Modal's infrastructure.

# You can trigger the web endpoint by [creating a Proxy Auth Token](https://modal.com/settings/proxy-auth-tokens)
# and then including the token ID and secret in the `Modal-Key` and `Modal-Secret` headers.

# From the command line, that might look like

# ```shell
# export TOKEN_ID=wk-1234abcd
# export TOKEN_SECRET=ws-1234abcd
# curl -H "Modal-Key: $TOKEN_ID" \
#      -H "Modal-Secret: $TOKEN_SECRET" \
#      https://your-workspace-name--expensive-secret.modal.run
# ```

# For more details, see the
# [guide to proxy authentication](https://modal.com/docs/guide/webhook-proxy-auth).

# ## What next?

# Modal's `fastapi_endpoint` decorator is opinionated and designed for relatively simple web applications --
# one or a few independent Python functions that you want to expose to the web.

# Three additional decorators allow you to serve more complex web applications with greater control:
# - [`asgi_app`](https://modal.com/docs/guide/webhooks#asgi) to serve applications compliant with the ASGI standard,
# like [FastAPI](https://fastapi.tiangolo.com/)
# - [`wsgi_app`](https://modal.com/docs/guide/webhooks#wsgi) to serve applications compliant with the WSGI standard,
# like [Flask](https://flask.palletsprojects.com/)
# - [`web_server`](https://modal.com/docs/guide/webhooks#non-asgi-web-servers) to serve any application that listens on a port


## Links discovered
- ["time to 200"](https://shkspr.mobi/blog/2021/05/whats-your-apis-time-to-200/)
- [`modal.fastapi_endpoint`](https://modal.com/docs/reference/modal.fastapi_endpoint)
- [accelerated with GPUs](https://modal.com/docs/guide/gpu)
- [Secrets](https://modal.com/docs/guide/secrets)
- [Volumes](https://modal.com/docs/guide/volumes)
- [automatically scale](https://modal.com/docs/guide/cold-start)
- [FastAPI library](https://fastapi.tiangolo.com/)
- [high-quality documentation](https://fastapi.tiangolo.com/tutorial/)
- [other Python functions to call it](https://modal.com/docs/guide/trigger-deployed-functions)
- [FastAPI documentation](https://fastapi.tiangolo.com/features/#automatic-docs)
- [automatic validation](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/)
- [FastAPI documentation](https://fastapi.tiangolo.com/tutorial/query-params/)
- [JSON](https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Objects/JSON)
- [considered bad form](https://stackoverflow.com/a/983458)
- [Pydantic model](https://fastapi.tiangolo.com/tutorial/body/)
- [FastAPI documentation](https://fastapi.tiangolo.com/tutorial/body/)
- [loading a large ML model](https://modal.com/docs/guide/model-weights)
- [`modal.Cls`](https://modal.com/docs/guide/lifecycle-functions#container-lifecycle-functions-and-parameters)
- [Modal workspace](https://modal.com/docs/guide/workspaces)
- [`401 Unauthorized`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/401)
- [creating a Proxy Auth Token](https://modal.com/settings/proxy-auth-tokens)
- [guide to proxy authentication](https://modal.com/docs/guide/webhook-proxy-auth)
- [`asgi_app`](https://modal.com/docs/guide/webhooks#asgi)
- [FastAPI](https://fastapi.tiangolo.com/)
- [`wsgi_app`](https://modal.com/docs/guide/webhooks#wsgi)
- [Flask](https://flask.palletsprojects.com/)
- [`web_server`](https://modal.com/docs/guide/webhooks#non-asgi-web-servers)

--- 07_web_endpoints/count_faces.py ---
# ---
# cmd: ["modal", "serve", "07_web_endpoints/count_faces.py"]
# ---

# # Run OpenCV face detection on an image

# This example shows how you can use OpenCV on Modal to detect faces in an image. We use
# the `opencv-python` package to load the image and the `opencv` library to
# detect faces. The function `count_faces` takes an image as input and returns
# the number of faces detected in the image.

# The code below also shows how you can create wrap this function
# in a simple FastAPI server to create a web interface.

import os

import modal

app = modal.App("example-count-faces")


open_cv_image = (
    modal.Image.debian_slim(python_version="3.11")
    .apt_install("python3-opencv")
    .uv_pip_install(
        "fastapi[standard]==0.115.4",
        "opencv-python~=4.10.0",
        "numpy<2",
    )
)


@app.function(image=open_cv_image)
def count_faces(image_bytes: bytes) -> int:
    import cv2
    import numpy as np

    # Example borrowed from https://towardsdatascience.com/face-detection-in-2-minutes-using-opencv-python-90f89d7c0f81
    # Load the cascade
    face_cascade = cv2.CascadeClassifier(
        os.path.join(cv2.data.haarcascades, "haarcascade_frontalface_default.xml")
    )
    # Read the input image
    np_bytes = np.frombuffer(image_bytes, dtype=np.uint8)
    img = cv2.imdecode(np_bytes, cv2.IMREAD_COLOR)
    # Convert into grayscale
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    # Detect faces
    faces = face_cascade.detectMultiScale(gray, 1.1, 4)
    return len(faces)


@app.function(
    image=modal.Image.debian_slim(python_version="3.11").uv_pip_install("inflect")
)
@modal.asgi_app()
def web():
    import inflect
    from fastapi import FastAPI, File, HTTPException, UploadFile
    from fastapi.responses import HTMLResponse

    app = FastAPI()

    @app.get("/", response_class=HTMLResponse)
    async def index():
        """
        Render an HTML form for file upload.
        """
        return """
        <html>
            <head>
                <title>Face Counter</title>
            </head>
            <body>
                <h1>Upload an Image to Count Faces</h1>
                <form action="/process" method="post" enctype="multipart/form-data">
                    <input type="file" name="file" id="file" accept="image/*" required />
                    <button type="submit">Upload</button>
                </form>
            </body>
        </html>
        """

    @app.post("/process", response_class=HTMLResponse)
    async def process(file: UploadFile = File(...)):
        """
        Process the uploaded image and return the number of faces detected.
        """
        try:
            file_content = await file.read()
            num_faces = await count_faces.remote.aio(file_content)
            return f"""
            <html>
                <head>
                    <title>Face Counter Result</title>
                </head>
                <body>
                    <h1>{inflect.engine().number_to_words(num_faces).title()} {"Face" if num_faces == 1 else "Faces"} Detected</h1>
                    <h2>{"ðŸ˜€" * num_faces}</h2>
                    <a href="/">Go back</a>
                </body>
            </html>
            """
        except Exception as e:
            raise HTTPException(
                status_code=400, detail=f"Error processing image: {str(e)}"
            )

    return app


## Links discovered
- [Go back](https://github.com/modal-labs/modal-examples/blob/main/.)

--- 07_web_endpoints/discord_bot.py ---
# ---
# deploy: true
# ---

# # Serve a Discord Bot on Modal

# In this example we will demonstrate how to use Modal to build and serve a Discord bot that uses
# [slash commands](https://discord.com/developers/docs/interactions/application-commands).

# Slash commands send information from Discord server members to a service at a URL.
# Here, we set up a simple [FastAPI app](https://fastapi.tiangolo.com/)
# to run that service and deploy it easily  Modalâ€™s
# [`@asgi_app`](https://modal.com/docs/guide/webhooks#serving-asgi-and-wsgi-apps) decorator.

# As our example service, we hit a simple free API:
# the [Free Public APIs API](https://www.freepublicapis.com/api),
# a directory of free public APIs.

# [Try it out on Discord](https://discord.gg/PmG7P47EPQ)!

# ## Set up our App and its Image

# First, we define the [container image](https://modal.com/docs/guide/images)
# that all the pieces of our bot will run in.

# We set that as the default image for a Modal [App](https://modal.com/docs/guide/apps).
# The App is where we'll attach all the components of our bot.

import json
from enum import Enum

import modal

image = modal.Image.debian_slim(python_version="3.11").uv_pip_install(
    "fastapi[standard]==0.115.4", "pynacl~=1.5.0", "requests~=2.32.3"
)

app = modal.App("example-discord-bot", image=image)

# ## Hit the Free Public APIs API

# We start by defining the core service that our bot will provide.

# In a real application, this might be [music generation](https://modal.com/docs/examples/musicgen),
# a [chatbot](https://modal.com/docs/examples/chat_with_pdf_vision),
# or [interacting with a database](https://modal.com/docs/examples/cron_datasette).

# Here, we just hit a simple free public API:
# the [Free Public APIs](https://www.freepublicapis.com) API,
# an "API of APIs" that returns information about free public APIs,
# like the [Global Shark Attack API](https://www.freepublicapis.com/global-shark-attack-api)
# and the [Corporate Bullshit Generator](https://www.freepublicapis.com/corporate-bullshit-generator).
# We convert the response into a Markdown-formatted message.

# We turn our Python function into a Modal Function by attaching the `app.function` decorator.
# We make the function `async` and add `@modal.concurrent()` with a large `max_inputs` value, because
# communicating with an external API is a classic case for better performance from asynchronous execution.
# Modal handles things like the async event loop for us.


@app.function()
@modal.concurrent(max_inputs=100)
async def fetch_api() -> str:
    import aiohttp

    url = "https://www.freepublicapis.com/api/random"

    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(url) as response:
                response.raise_for_status()
                data = await response.json()
                message = (
                    f"# {data.get('emoji') or 'ðŸ¤–'} [{data['title']}]({data['source']})"
                )
                message += f"\n _{''.join(data['description'].splitlines())}_"
        except Exception as e:
            message = f"# ðŸ¤–: Oops! {e}"

    return message


# This core component has nothing to do with Discord,
# and it's nice to be able to interact with and test it in isolation.

# For that, we add a `local_entrypoint` that calls the Modal Function.
# Notice that we add `.remote` to the function's name.

# Later, when you replace this component of the app with something more interesting,
# test it by triggering this entrypoint with  `modal run discord_bot.py`.


@app.local_entrypoint()
def test_fetch_api():
    result = fetch_api.remote()
    if result.startswith("# ðŸ¤–: Oops! "):
        raise Exception(result)
    else:
        print(result)


# ## Integrate our Modal Function with Discord Interactions

# Now we need to map this function onto Discord's interface --
# in particular the [Interactions API](https://discord.com/developers/docs/interactions/overview).

# Reviewing the documentation, we see that we need to send a JSON payload
# to a specific API URL that will include an `app_id` that identifies our bot
# and a `token` that identifies the interaction (loosely, message) that we're participating in.

# So let's write that out. This function doesn't need to live on Modal,
# since it's just encapsulating some logic -- we don't want to turn it into a service or an API on its own.
# That means we don't need any Modal decorators.


async def send_to_discord(payload: dict, app_id: str, interaction_token: str):
    import aiohttp

    interaction_url = f"https://discord.com/api/v10/webhooks/{app_id}/{interaction_token}/messages/@original"

    async with aiohttp.ClientSession() as session:
        async with session.patch(interaction_url, json=payload) as resp:
            print("ðŸ¤– Discord response: " + await resp.text())


# Other parts of our application might want to both hit the Free Public APIs API and send the result to Discord,
# so we both write a Python function for this and we promote it to a Modal Function with a decorator.

# Notice that we use the `.local` suffix to call our `fetch_api` Function. That means we run
# the Function the same way we run all the other Python functions, rather than treating it as a special
# Modal Function. This reduces a bit of extra latency, but couples these two Functions more tightly.


@app.function()
@modal.concurrent(max_inputs=100)
async def reply(app_id: str, interaction_token: str):
    message = await fetch_api.local()
    await send_to_discord({"content": message}, app_id, interaction_token)


# ## Set up a Discord app

# Now, we need to actually connect to Discord.
# We start by creating an application on the Discord Developer Portal.

# 1. Go to the
#    [Discord Developer Portal](https://discord.com/developers/applications) and
#    log in with your Discord account.
# 2. On the portal, go to **Applications** and create a new application by
#    clicking **New Application** in the top right next to your profile picture.
# 3. [Create a custom Modal Secret](https://modal.com/docs/guide/secrets) for your Discord bot.
#    On Modal's Secret creation page, select 'Discord'. Copy your Discord applicationâ€™s
#    **Public Key** and **Application ID** (from the **General Information** tab in the Discord Developer Portal)
#    and paste them as the value of `DISCORD_PUBLIC_KEY` and `DISCORD_CLIENT_ID`.
#    Additionally, head to the **Bot** tab and use the **Reset Token** button to create a new bot token.
#    Paste this in the value of an additional key in the Secret, `DISCORD_BOT_TOKEN`.
#    Name this Secret `discord-secret`.

# We access that Secret in code like so:

discord_secret = modal.Secret.from_name(
    "discord-secret",
    required_keys=[  # included so we get nice error messages if we forgot a key
        "DISCORD_BOT_TOKEN",
        "DISCORD_CLIENT_ID",
        "DISCORD_PUBLIC_KEY",
    ],
)

# ## Register a Slash Command

# Next, weâ€™re going to register a [Slash Command](https://discord.com/developers/docs/interactions/application-commands#slash-commands)
# for our Discord app. Slash Commands are triggered by users in servers typing `/` and the name of the command.

# The Modal Function below will register a Slash Command for your bot named `bored`.
# More information about Slash Commands can be found in the Discord docs
# [here](https://discord.com/developers/docs/interactions/application-commands).

# You can run this Function with

# ```bash
# modal run discord_bot::create_slash_command
# ```


@app.function(secrets=[discord_secret], image=image)
def create_slash_command(force: bool = False):
    """Registers the slash command with Discord. Pass the force flag to re-register."""
    import os

    import requests

    BOT_TOKEN = os.getenv("DISCORD_BOT_TOKEN")
    CLIENT_ID = os.getenv("DISCORD_CLIENT_ID")

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bot {BOT_TOKEN}",
    }
    url = f"https://discord.com/api/v10/applications/{CLIENT_ID}/commands"

    command_description = {
        "name": "api",
        "description": "Information about a random free, public API",
    }

    # first, check if the command already exists
    response = requests.get(url, headers=headers)
    try:
        response.raise_for_status()
    except Exception as e:
        raise Exception("Failed to create slash command") from e

    commands = response.json()
    command_exists = any(
        command.get("name") == command_description["name"] for command in commands
    )

    # and only recreate it if the force flag is set
    if command_exists and not force:
        print(f"ðŸ¤–: command {command_description['name']} exists")
        return

    response = requests.post(url, headers=headers, json=command_description)
    try:
        response.raise_for_status()
    except Exception as e:
        raise Exception("Failed to create slash command") from e
    print(f"ðŸ¤–: command {command_description['name']} created")


# ## Host a Discord Interactions endpoint on Modal

# If you look carefully at the definition of the Slash Command above,
# you'll notice that it doesn't know anything about our bot besides an ID.

# To hook the Slash Commands in the Discord UI up to our logic for hitting the Bored API,
# we need to set up a service that listens at some URL and follows a specific protocol,
# described [here](https://discord.com/developers/docs/interactions/overview#configuring-an-interactions-endpoint-url).

# Here are some of the most important facets:

# 1. We'll need to respond within five seconds or Discord will assume we are dead.
# Modal's fast-booting serverless containers usually start faster than that,
# but it's not guaranteed. So we'll add the `min_containers` parameter to our
# Function so that there's at least one live copy ready to respond quickly at any time.
# Modal charges a minimum of about 2Â¢ an hour for live containers (pricing details [here](https://modal.com/pricing)).
# Note that that still fits within Modal's $30/month of credits on the free tier.

# 2. We have to respond to Discord that quickly, but we don't have to respond to the user that quickly.
# We instead send an acknowledgement so that they know we're alive and they can close their connection to us.
# We also trigger our `reply` Modal Function, which will respond to the user via Discord's Interactions API,
# but we don't wait for the result, we just `spawn` the call.

# 3. The protocol includes some authentication logic that is mandatory
# and checked by Discord. We'll explain in more detail in the next section.

# We can set up our interaction endpoint by deploying a FastAPI app on Modal.
# This is as easy as creating a Python Function that returns a FastAPI app
# and adding the `modal.asgi_app` decorator.
# For more details on serving Python web apps on Modal, see
# [this guide](https://modal.com/docs/guide/webhooks).


@app.function(secrets=[discord_secret], min_containers=1)
@modal.concurrent(max_inputs=100)
@modal.asgi_app()
def web_app():
    from fastapi import FastAPI, HTTPException, Request
    from fastapi.middleware.cors import CORSMiddleware

    web_app = FastAPI()

    # must allow requests from other domains, e.g. from Discord's servers
    web_app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    @web_app.post("/api")
    async def get_api(request: Request):
        body = await request.body()

        # confirm this is a request from Discord
        authenticate(request.headers, body)

        print("ðŸ¤–: parsing request")
        data = json.loads(body.decode())
        if data.get("type") == DiscordInteractionType.PING.value:
            print("ðŸ¤–: acking PING from Discord during auth check")
            return {"type": DiscordResponseType.PONG.value}

        if data.get("type") == DiscordInteractionType.APPLICATION_COMMAND.value:
            print("ðŸ¤–: handling slash command")
            app_id = data["application_id"]
            interaction_token = data["token"]

            # kick off request asynchronously, will respond when ready
            reply.spawn(app_id, interaction_token)

            # respond immediately with defer message
            return {
                "type": DiscordResponseType.DEFERRED_CHANNEL_MESSAGE_WITH_SOURCE.value
            }

        print(f"ðŸ¤–: unable to parse request with type {data.get('type')}")
        raise HTTPException(status_code=400, detail="Bad request")

    return web_app


# The authentication for Discord is a bit involved and there aren't,
# to our knowledge, any good Python libraries for it.

# So we have to implement the protocol "by hand".

# Essentially, Discord sends a header in their request
# that we can use to verify the request comes from them.
# For that, we use the `DISCORD_PUBLIC_KEY` from
# our Application Information page.

# The details aren't super important, but they appear in the `authenticate` function below
# (which defers the real cryptography work to [PyNaCl](https://pypi.org/project/PyNaCl/),
# a Python wrapper for [`libsodium`](https://github.com/jedisct1/libsodium)).

# Discord will also check that we reject unauthorized requests,
# so we have to be sure to get this right!


def authenticate(headers, body):
    import os

    from fastapi.exceptions import HTTPException
    from nacl.exceptions import BadSignatureError
    from nacl.signing import VerifyKey

    print("ðŸ¤–: authenticating request")
    # verify the request is from Discord using their public key
    public_key = os.getenv("DISCORD_PUBLIC_KEY")
    verify_key = VerifyKey(bytes.fromhex(public_key))

    signature = headers.get("X-Signature-Ed25519")
    timestamp = headers.get("X-Signature-Timestamp")

    message = timestamp.encode() + body

    try:
        verify_key.verify(message, bytes.fromhex(signature))
    except BadSignatureError:
        # either an unauthorized request or Discord's "negative control" check
        raise HTTPException(status_code=401, detail="Invalid request")


# The code above used a few enums to abstract bits of the Discord protocol.
# Now that we've walked through all of it,
# we're in a position to understand what those are
# and so the code for them appears below.


class DiscordInteractionType(Enum):
    PING = 1  # hello from Discord during auth check
    APPLICATION_COMMAND = 2  # an actual command


class DiscordResponseType(Enum):
    PONG = 1  # hello back during auth check
    DEFERRED_CHANNEL_MESSAGE_WITH_SOURCE = 5  # we'll send a message later


# ## Deploy on Modal

# You can deploy this app on Modal by running the following commands:

# ``` shell
# modal run discord_bot.py  # checks the API wrapper, little test
# modal run discord_bot.py::create_slash_command  # creates the slash command, if missing
# modal deploy discord_bot.py  # deploys the web app and the API wrapper
# ```

# Copy the Modal URL that is printed in the output and go back to the **General Information** section on the
# [Discord Developer Portal](https://discord.com/developers/applications).
# Paste the URL, making sure to append the path of your `POST` route (here, `/api`), in the
# **Interactions Endpoint URL** field, then click **Save Changes**. If your
# endpoint URL is incorrect or if authentication is incorrectly implemented,
# Discord will refuse to save the URL. Once it saves, you can start
# handling interactions!

# ## Finish setting up Discord bot

# To start using the Slash Command you just set up, you need to invite the bot to
# a Discord server. To do so, go to your application's **Installation** section on the
# [Discord Developer Portal](https://discord.com/developers/applications).
# Copy the **Discored Provided Link** and visit it to invite the bot to your bot to the server.

# Now you can open your Discord server and type `/api` in a channel to trigger the bot.
# You can see a working version [in our test Discord server](https://discord.gg/PmG7P47EPQ).


## Links discovered
- [slash commands](https://discord.com/developers/docs/interactions/application-commands)
- [FastAPI app](https://fastapi.tiangolo.com/)
- [`@asgi_app`](https://modal.com/docs/guide/webhooks#serving-asgi-and-wsgi-apps)
- [Free Public APIs API](https://www.freepublicapis.com/api)
- [Try it out on Discord](https://discord.gg/PmG7P47EPQ)
- [container image](https://modal.com/docs/guide/images)
- [App](https://modal.com/docs/guide/apps)
- [music generation](https://modal.com/docs/examples/musicgen)
- [chatbot](https://modal.com/docs/examples/chat_with_pdf_vision)
- [interacting with a database](https://modal.com/docs/examples/cron_datasette)
- [Free Public APIs](https://www.freepublicapis.com)
- [Global Shark Attack API](https://www.freepublicapis.com/global-shark-attack-api)
- [Corporate Bullshit Generator](https://www.freepublicapis.com/corporate-bullshit-generator)
- [Interactions API](https://discord.com/developers/docs/interactions/overview)
- [Discord Developer Portal](https://discord.com/developers/applications)
- [Create a custom Modal Secret](https://modal.com/docs/guide/secrets)
- [Slash Command](https://discord.com/developers/docs/interactions/application-commands#slash-commands)
- [here](https://discord.com/developers/docs/interactions/application-commands)
- [here](https://discord.com/developers/docs/interactions/overview#configuring-an-interactions-endpoint-url)
- [here](https://modal.com/pricing)
- [this guide](https://modal.com/docs/guide/webhooks)
- [PyNaCl](https://pypi.org/project/PyNaCl/)
- [`libsodium`](https://github.com/jedisct1/libsodium)
- [in our test Discord server](https://discord.gg/PmG7P47EPQ)

--- 07_web_endpoints/fasthtml_app.py ---
# ---
# cmd: ["modal", "serve", "07_web_endpoints/fasthtml_app.py"]
# ---

# # Deploy a FastHTML app with Modal

# This example shows how you can deploy a FastHTML app with Modal.
# [FastHTML](https://www.fastht.ml/) is a Python library built on top of [HTMX](https://htmx.org/)
# which allows you to create entire web applications using only Python.

# The integration is pretty simple, thanks to the ASGI standard.
# You just need to define a function returns your FastHTML app
# and is decorated with `app.function` and `modal.asgi_app`.

import modal

app = modal.App("example-fasthtml-app")


@app.function(
    image=modal.Image.debian_slim(python_version="3.12").uv_pip_install(
        "python-fasthtml==0.5.2"
    )
)
@modal.asgi_app()
def serve():
    import fasthtml.common as fh

    app = fh.FastHTML()

    @app.get("/")
    def home():
        return fh.Div(fh.P("Hello World!"), hx_get="/change")

    return app


## Links discovered
- [FastHTML](https://www.fastht.ml/)
- [HTMX](https://htmx.org/)

--- 07_web_endpoints/fastrtc_flip_webcam.py ---
# ---
# cmd: ["modal", "serve", "07_web_endpoints/fastrtc_flip_webcam.py"]
# deploy: true
# ---

# # Run a FastRTC app on Modal

# [FastRTC](https://fastrtc.org/) is a Python library for real-time communication on the web.
# This example demonstrates how to run a simple FastRTC app in the cloud on Modal.

# It's intended to help you get up and running with real-time streaming applications on Modal
# as quickly as possible. If you're interested in running a production-grade WebRTC app on Modal,
# see [this example](https://modal.com/docs/examples/webrtc_yolo).

# In this example, we stream webcam video from a browser to a container on Modal,
# where the video is flipped, annotated, and sent back with under 100ms of delay.
# You can try it out [here](https://modal-labs-examples--example-fastrtc-flip-webcam-ui.modal.run/)
# or just dive straight into the code to run it yourself.

# ## Set up FastRTC on Modal

# First, we import the `modal` SDK
# and use it to define a [container image](https://modal.com/docs/guide/images)
# with FastRTC and related dependencies.

import modal

web_image = modal.Image.debian_slim(python_version="3.12").uv_pip_install(
    "fastapi[standard]==0.115.4",
    "fastrtc==0.0.23",
    "gradio==5.7.1",
    "opencv-python-headless==4.11.0.86",
)

# Then, we set that as the default Image on our Modal [App](https://modal.com/docs/guide/apps).

app = modal.App("example-fastrtc-flip-webcam", image=web_image)

# ### Configure WebRTC streaming on Modal

# Under the hood, FastRTC uses the WebRTC
# [APIs](https://www.w3.org/TR/webrtc/) and
# [protocols](https://datatracker.ietf.org/doc/html/rfc8825).

# WebRTC provides low latency ("real-time") peer-to-peer communication
# for Web applications, focusing on audio and video.
# Considering that the Web is a platform originally designed
# for high-latency, client-server communication of text and images,
# that's no mean feat!

# In addition to protocols that implement this communication,
# WebRTC includes APIs for describing and manipulating audio/video streams.
# In this demo, we set a few simple parameters, like the direction of the webcam
# and the minimum frame rate. See the
# [MDN Web Docs for `MediaTrackConstraints`](https://developer.mozilla.org/en-US/docs/Web/API/MediaTrackConstraints)
# for more.

TRACK_CONSTRAINTS = {
    "width": {"exact": 640},
    "height": {"exact": 480},
    "frameRate": {"min": 30},
    "facingMode": {  # https://developer.mozilla.org/en-US/docs/Web/API/MediaTrackSettings/facingMode
        "ideal": "user"
    },
}

# In theory, the Internet is designed for peer-to-peer communication
# all the way down to its heart, the Internet Protocol (IP): just send packets between IP addresses.
# In practice, peer-to-peer communication on the contemporary Internet is fraught with difficulites,
# from restrictive firewalls to finicky work-arounds for
# [the exhaustion of IPv4 addresses](https://www.a10networks.com/glossary/what-is-ipv4-exhaustion/),
# like [Carrier-Grade Network Address Translation (CGNAT)](https://en.wikipedia.org/wiki/Carrier-grade_NAT).

# So establishing peer-to-peer connections can be quite involved.
# The protocol for doing so is called Interactive Connectivity Establishment (ICE).
# It is described in [this RFC](https://datatracker.ietf.org/doc/html/rfc8445#section-2).

# ICE involves the peers exchanging a list of connections that might be used.
# We use a fairly simple setup here, where our peer on Modal uses the
# [Session Traversal Utilities for NAT (STUN)](https://datatracker.ietf.org/doc/html/rfc5389)
# server provided by Google. A STUN server basically just reflects back to a client what their
# IP address and port number appear to be when they talk to it. The peer on Modal communicates
# that information to the other peer trying to connect to it -- in this case, a browser trying to share a webcam feed.
# Note the use of `stun` and port `19302` in the URL in place of
# something more familiar, like `http` and port `80`.

RTC_CONFIG = {"iceServers": [{"url": "stun:stun.l.google.com:19302"}]}


# ## Running a FastRTC app on Modal

# FastRTC builds on top of the [Gradio](https://www.gradio.app/docs)
# library for defining Web UIs in Python.
# Gradio in turn is compatible with the
# [Asynchronous Server Gateway Interface (ASGI)](https://asgi.readthedocs.io/en/latest/)
# protocol for asynchronous Python web servers, like
# [FastAPI](https://fastrtc.org/userguide/streams/),
# so we can host it on Modal's cloud platform using the
# [`modal.asgi_app` decorator](https://modal.com/docs/guide/webhooks#serving-asgi-and-wsgi-apps)
# with [Modal Function](https://modal.com/docs/guide/apps).

# But before we do that, we need to consider limits:
# on how many peers can connect to one instance on Modal
# and on how long they can stay connected.
# We picked some sensible defaults to show how they interact
# with the deployment parameters of the Modal Function.
# You'll want to tune these for your application!

MAX_CONCURRENT_STREAMS = 10  # number of peers per instance on Modal

MINUTES = 60  # seconds
TIME_LIMIT = 10 * MINUTES  # time limit


@app.function(
    # gradio requires sticky sessions
    # so we limit the number of concurrent containers to 1
    # and allow that container to handle concurrent streams
    max_containers=1,
    scaledown_window=TIME_LIMIT + 1 * MINUTES,  # add a small buffer to time limit
)
@modal.concurrent(max_inputs=MAX_CONCURRENT_STREAMS)  # inputs per container
@modal.asgi_app()  # ASGI on Modal
def ui():
    import fastrtc  # WebRTC in Gradio
    import gradio as gr  # WebUIs in Python
    from fastapi import FastAPI  # asynchronous ASGI server framework
    from gradio.routes import mount_gradio_app  # connects Gradio and FastAPI

    with gr.Blocks() as blocks:  # block-wise UI definition
        gr.HTML(  # simple HTML header
            "<h1 style='text-align: center'>"
            "Streaming Video Processing with Modal and FastRTC"
            "</h1>"
        )

        with gr.Column():  # a column of UI elements
            fastrtc.Stream(  # high-level media streaming UI element
                modality="video",
                mode="send-receive",
                handler=flip_vertically,  # handler -- handle incoming frame, produce outgoing frame
                ui_args={"title": "Click 'Record' to flip your webcam in the cloud"},
                rtc_configuration=RTC_CONFIG,
                track_constraints=TRACK_CONSTRAINTS,
                concurrency_limit=MAX_CONCURRENT_STREAMS,  # limit simultaneous connections
                time_limit=TIME_LIMIT,  # limit time per connection
            )

    return mount_gradio_app(app=FastAPI(), blocks=blocks, path="/")


# To try this out for yourself, run

# ```bash
# modal serve 07_web_endpoints/fastrtc_flip_webcam.py
# ```

# and head to the `modal.run` URL that appears in your terminal.
# You can also check on the application's dashboard
# via the `modal.com` URL thatappears below it.

# The `modal serve` command produces a hot-reloading development server --
# try editing the `title` in the `ui_args` above and watch the server redeploy.

# This temporary deployment is tied to your terminal session.
# To deploy permanently, run

# ```bash
# modal deploy 07_web_endponts/fastrtc_flip_webcam.py
# ```

# Note that Modal is a serverless platform with [usage-based pricing](https://modal.com/pricing),
# so this application will spin down and cost you nothing when it is not in use.

# ## Addenda

# This FastRTC app is very much the "hello world" or "echo server"
# of FastRTC: it just flips the incoming webcam stream and adds a "hello" message.
# That logic appears below.


def flip_vertically(image):
    import cv2
    import numpy as np

    image = image.astype(np.uint8)

    if image is None:
        print("failed to decode image")
        return

    # flip vertically and caption to show video was processed on Modal
    image = cv2.flip(image, 0)
    lines = ["Hello from Modal!"]
    caption_image(image, lines)

    return image


def caption_image(
    img, lines, font_scale=0.8, thickness=2, margin=10, font=None, color=None
):
    import cv2

    if font is None:
        font = cv2.FONT_HERSHEY_SIMPLEX
    if color is None:
        color = (127, 238, 100, 128)  # Modal Green

    # get text sizes
    sizes = [cv2.getTextSize(line, font, font_scale, thickness)[0] for line in lines]
    if not sizes:
        return

    # position text in bottom right
    pos_xs = [img.shape[1] - size[0] - margin for size in sizes]

    pos_ys = [img.shape[0] - margin]
    for _width, height in reversed(sizes[:-1]):
        next_pos = pos_ys[-1] - 2 * height
        pos_ys.append(next_pos)

    for line, pos in zip(lines, zip(pos_xs, reversed(pos_ys))):
        cv2.putText(img, line, pos, font, font_scale, color, thickness)


## Links discovered
- [FastRTC](https://fastrtc.org/)
- [this example](https://modal.com/docs/examples/webrtc_yolo)
- [here](https://modal-labs-examples--example-fastrtc-flip-webcam-ui.modal.run/)
- [container image](https://modal.com/docs/guide/images)
- [App](https://modal.com/docs/guide/apps)
- [APIs](https://www.w3.org/TR/webrtc/)
- [protocols](https://datatracker.ietf.org/doc/html/rfc8825)
- [MDN Web Docs for `MediaTrackConstraints`](https://developer.mozilla.org/en-US/docs/Web/API/MediaTrackConstraints)
- [the exhaustion of IPv4 addresses](https://www.a10networks.com/glossary/what-is-ipv4-exhaustion/)
- [Carrier-Grade Network Address Translation (CGNAT)](https://en.wikipedia.org/wiki/Carrier-grade_NAT)
- [this RFC](https://datatracker.ietf.org/doc/html/rfc8445#section-2)
- [Session Traversal Utilities for NAT (STUN)](https://datatracker.ietf.org/doc/html/rfc5389)
- [Gradio](https://www.gradio.app/docs)
- [Asynchronous Server Gateway Interface (ASGI)](https://asgi.readthedocs.io/en/latest/)
- [FastAPI](https://fastrtc.org/userguide/streams/)
- [`modal.asgi_app` decorator](https://modal.com/docs/guide/webhooks#serving-asgi-and-wsgi-apps)
- [Modal Function](https://modal.com/docs/guide/apps)
- [usage-based pricing](https://modal.com/pricing)

--- 07_web_endpoints/flask_app.py ---
# ---
# cmd: ["modal", "serve", "07_web_endpoints/flask_app.py"]
# ---

# # Deploy Flask app with Modal

# This example shows how you can deploy a [Flask](https://flask.palletsprojects.com/en/3.0.x/) app with Modal.
# You can serve any app written in a WSGI-compatible web framework (like Flask) on Modal with this pattern. You can serve an app written in an ASGI-compatible framework, like FastAPI, with [`asgi_app`](https://modal.com/docs/guide/webhooks#asgi).

import modal

app = modal.App(
    "example-flask-app",
    image=modal.Image.debian_slim().uv_pip_install("flask"),
)


@app.function()
@modal.wsgi_app()
def flask_app():
    from flask import Flask, request

    web_app = Flask(__name__)

    @web_app.get("/")
    def home():
        return "Hello Flask World!"

    @web_app.post("/foo")
    def foo():
        return request.json

    return web_app


## Links discovered
- [Flask](https://flask.palletsprojects.com/en/3.0.x/)
- [`asgi_app`](https://modal.com/docs/guide/webhooks#asgi)

--- 07_web_endpoints/flask_streaming.py ---
# ---
# cmd: ["modal", "serve", "07_web_endpoints/flask_streaming.py"]
# ---

# # Deploy Flask app with streaming results with Modal

# This example shows how you can deploy a [Flask](https://flask.palletsprojects.com/en/3.0.x/) app with Modal that streams results back to the client.

import modal

app = modal.App(
    "example-flask-streaming",
    image=modal.Image.debian_slim().uv_pip_install("flask"),
)


@app.function()
def generate_rows():
    """
    This creates a large CSV file, about 10MB, which will be streaming downloaded
    by a web client.
    """
    for i in range(10_000):
        line = ",".join(str((j + i) * i) for j in range(128))
        yield f"{line}\n"


@app.function()
@modal.wsgi_app()
def flask_app():
    from flask import Flask

    web_app = Flask(__name__)

    # These web handlers follow the example from
    # https://flask.palletsprojects.com/en/2.2.x/patterns/streaming/

    @web_app.route("/")
    def generate_large_csv():
        # Run the function locally in the web app's container.
        return generate_rows.local(), {"Content-Type": "text/csv"}

    @web_app.route("/remote")
    def generate_large_csv_in_container():
        # Run the function remotely in a separate container,
        # which will stream back results to the web app container,
        # which will stream back to the web client.
        #
        # This is less efficient, but demonstrates how web serving
        # containers can be separated from and cooperate with other
        # containers.
        return generate_rows.remote(), {"Content-Type": "text/csv"}

    return web_app


## Links discovered
- [Flask](https://flask.palletsprojects.com/en/3.0.x/)

--- 07_web_endpoints/streaming.py ---
# ---
# cmd: ["modal", "serve", "07_web_endpoints/streaming.py"]
# ---

# # Deploy a FastAPI app with streaming responses

# This example shows how you can deploy a [FastAPI](https://fastapi.tiangolo.com/) app with Modal that streams results back to the client.

import asyncio
import time

import modal
from fastapi import FastAPI
from fastapi.responses import StreamingResponse

image = modal.Image.debian_slim().uv_pip_install("fastapi[standard]")
app = modal.App("example-streaming", image=image)

web_app = FastAPI()

# This asynchronous generator function simulates
# progressively returning data to the client. The `asyncio.sleep`
# is not necessary, but makes it easier to see the iterative behavior
# of the response.


async def fake_video_streamer():
    for i in range(10):
        yield f"frame {i}: hello world!".encode()
        await asyncio.sleep(1.0)


# ASGI app with streaming handler.

# This `fastapi_app` also uses the fake video streamer async generator,
# passing it directly into `StreamingResponse`.


@web_app.get("/")
async def main():
    return StreamingResponse(fake_video_streamer(), media_type="text/event-stream")


@app.function()
@modal.asgi_app()
def fastapi_app():
    return web_app


# This `hook` web endpoint Modal function calls *another* Modal function,
# and it just works!


@app.function()
def sync_fake_video_streamer():
    for i in range(10):
        yield f"frame {i}: some data\n".encode()
        time.sleep(1)


@app.function()
@modal.fastapi_endpoint()
def hook():
    return StreamingResponse(
        sync_fake_video_streamer.remote_gen(), media_type="text/event-stream"
    )


# This `mapped` web endpoint Modal function does a parallel `.map` on a simple
# Modal function. Using `.starmap` also would work in the same fashion.


@app.function()
def map_me(i):
    time.sleep(i)  # stagger the results for demo purposes
    return f"hello from {i}\n"


@app.function()
@modal.fastapi_endpoint()
def mapped():
    return StreamingResponse(map_me.map(range(10)), media_type="text/event-stream")


# To try for yourself, run

# ```shell
# modal serve streaming.py
# ```

# and then send requests to the URLs that appear in the terminal output.

# Make sure that your client is not buffering the server response
# until it gets newline (\n) characters. By default browsers and `curl` are buffering,
# though modern browsers should respect the "text/event-stream" content type header being set.


## Links discovered
- [FastAPI](https://fastapi.tiangolo.com/)

--- 08_advanced/generators_async.py ---
# # Run async generator function on Modal

# This example shows how you can run an async generator function on Modal.
# Modal natively supports async/await syntax using asyncio.

import modal

app = modal.App("example-generators-async")


@app.function()
def f(i):
    for j in range(i):
        yield j


@app.local_entrypoint()
async def run_async():
    async for r in f.remote_gen.aio(10):
        print(r)


--- 08_advanced/hello_world_async.py ---
# # Async functions
#
# Modal natively supports async/await syntax using asyncio.

# First, let's import some global stuff.

import sys

import modal

app = modal.App("example-hello-world-async")


# ## Defining a function
#
# Now, let's define a function. The wrapped function can be synchronous or
# asynchronous, but calling it in either context will still work.
# Let's stick to a normal synchronous function


@app.function()
def f(i):
    if i % 2 == 0:
        print("hello", i)
    else:
        print("world", i, file=sys.stderr)

    return i * i


# ## Running the app with asyncio
#
# Let's make the main entrypoint asynchronous. In async contexts, we should
# call the function using `await` or iterate over the map using `async for`.
# Otherwise we would block the event loop while our call is being run.


@app.local_entrypoint()
async def run_async():
    # Call the function using .remote.aio() in order to run it asynchronously
    print(await f.remote.aio(1000))

    # Parallel map.
    total = 0
    # Call .map asynchronously using using f.map.aio(...)
    async for ret in f.map.aio(range(20)):
        total += ret

    print(total)


--- 08_advanced/parallel_execution.py ---
# # Parallel execution on Modal with `spawn` and `gather`

# This example shows how you can run multiple functions in parallel on Modal.
# We use the `spawn` method to start a function and return a handle to its result.
# The `get` method is used to retrieve the result of the function call.

import time

import modal

app = modal.App("example-parallel-execution")


@app.function()
def step1(word):
    time.sleep(2)
    print("step1 done")
    return word


@app.function()
def step2(number):
    time.sleep(1)
    print("step2 done")
    if number == 0:
        raise ValueError("custom error")
    return number


@app.local_entrypoint()
def main():
    # Start running a function and return a handle to its result.
    word_call = step1.spawn("foo")
    number_call = step2.spawn(2)

    # Print "foofoo" after 2 seconds.
    print(word_call.get() * number_call.get())

    # Alternatively, use `modal.FunctionCall.gather(...)` as a convenience wrapper,
    # which returns an error if either call fails.
    results = modal.FunctionCall.gather(step1.spawn("bar"), step2.spawn(4))
    assert results == ["bar", 4]

    # Raise exception after 2 seconds.
    try:
        modal.FunctionCall.gather(step1.spawn("bar"), step2.spawn(0))
    except ValueError as exc:
        assert str(exc) == "custom error"


--- 08_advanced/poll_delayed_result.py ---
# ---
# cmd: ["modal", "serve", "08_advanced/poll_delayed_result.py"]
# ---

# # Polling for a delayed result on Modal

# This example shows how you can poll for a delayed result on Modal.

# The function `factor_number` takes a number as input and returns the prime factors of the number. The function could take a long time to run, so we don't want to wait for the result in the web server.
# Instead, we return a URL that the client can poll to get the result.

import fastapi
import modal
from modal.functions import FunctionCall
from starlette.responses import HTMLResponse, RedirectResponse

app = modal.App("example-poll-delayed-result")

web_app = fastapi.FastAPI()


@app.function(image=modal.Image.debian_slim().uv_pip_install("primefac"))
def factor_number(number):
    import primefac

    return list(primefac.primefac(number))  # could take a long time


@web_app.get("/")
async def index():
    return HTMLResponse(
        """
    <form method="get" action="/factors">
        Enter a number: <input name="number" />
        <input type="submit" value="Factorize!"/>
    </form>
    """
    )


@web_app.get("/factors")
async def web_submit(request: fastapi.Request, number: int):
    call = factor_number.spawn(
        number
    )  # returns a FunctionCall without waiting for result
    polling_url = request.url.replace(
        path="/result", query=f"function_id={call.object_id}"
    )
    return RedirectResponse(polling_url)


@web_app.get("/result")
async def web_poll(function_id: str):
    function_call = FunctionCall.from_id(function_id)
    try:
        result = function_call.get(timeout=0)
    except TimeoutError:
        result = "not ready"

    return result


@app.function()
@modal.asgi_app()
def fastapi_app():
    return web_app


--- 09_job_queues/doc_ocr_frontend/index.html ---
<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="icon" href="images/favicon.svg" />
    <title>Receipt Parser</title>

    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>

    <!-- React -->
    <script
      crossorigin
      src="https://unpkg.com/react@18/umd/react.development.js"
    ></script>
    <script
      crossorigin
      src="https://unpkg.com/react-dom@18/umd/react-dom.development.js"
    ></script>

    <!-- Marked - markdown parsing -->
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

    <!-- DOMPurify sanitizing generated markdown -->
    <script src="https://cdn.jsdelivr.net/npm/dompurify/dist/purify.min.js"></script>


    <!-- Babel for JSX -->
    <script
      crossorigin
      src="https://unpkg.com/@babel/standalone/babel.min.js"
    ></script>

    <!-- Loading Spinner -->
    <script crossorigin src="https://spin.js.org/spin.umd.js"></script>
    <link rel="stylesheet" href="https://spin.js.org/spin.css" />

    <!-- Inter Font -->
    <link
      rel="stylesheet"
      href="https://fonts.googleapis.com/css2?family=Inter:wght@200;300;400&display=swap"
    />

    <!-- Tailwind Config -->
    <script>
      tailwind.config = {
        theme: {
          extend: {
            colors: {
              modal: {
                primary: "#4F46E5",
                secondary: "#818CF8",
              },
            },
            fontFamily: {
              inter: ["Inter", "sans-serif"],
              arial: ["Arial", "sans-serif"],
            },
          },
        },
      };
    </script>
  </head>
  <body class="bg-black">
    <noscript>You must have JavaScript enabled to use this app.</noscript>
    <div id="react"></div>
    <script type="text/babel" src="/app.jsx"></script>
  </body>
</html>


--- 09_job_queues/dicts_and_queues.py ---
# ---
# mypy: ignore-errors
# ---

# # Use Modal Dicts and Queues together

# Modal Dicts and Queues store and communicate objects in distributed applications on Modal.

# To illustrate how Dicts and Queues can interact together in a simple distributed
# system, consider the following example program that crawls the web, starting
# from some initial page and traversing links to many sites in breadth-first order.

# The Modal Queue acts as a job queue, accepting new pages to crawl as they are discovered
# by the crawlers and doling them out to be crawled via [`.spawn`](https://modal.com/docs/reference/modal.Function#spawn).

# The Dict is used to coordinate termination once the maximum number of URLs to crawl is reached.

# Starting from Wikipedia, this spawns several dozen containers (auto-scaled on
# demand) and crawls about 100,000 URLs per minute.

import queue
import sys
from datetime import datetime

import modal

app = modal.App(
    "example-dicts-and-queues",
    image=modal.Image.debian_slim().uv_pip_install(
        "requests~=2.32.4", "beautifulsoup4~=4.13.4"
    ),
)


def extract_links(url: str) -> list[str]:
    """Extract links from a given URL."""
    import urllib.parse

    import requests
    from bs4 import BeautifulSoup

    resp = requests.get(url, timeout=10)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")
    links = []
    for link in soup.find_all("a"):
        links.append(urllib.parse.urljoin(url, link.get("href")))
    return links


@app.function()
def crawl_pages(q: modal.Queue, d: modal.Dict, urls: set[str]) -> None:
    for url in urls:
        if "stop" in d:
            return
        try:
            s = datetime.now()
            links = extract_links(url)
            print(f"Crawled: {url} in {datetime.now() - s}, with {len(links)} links")
            q.put_many(links)
        except Exception as exc:
            print(
                f"Failed to crawl: {url} with error {exc}, skipping...", file=sys.stderr
            )


@app.function()
def scrape(url: str, max_urls: int = 50_000):
    start_time = datetime.now()

    # Create ephemeral dicts and queues
    with modal.Dict.ephemeral() as d, modal.Queue.ephemeral() as q:
        # The dict is used to signal the scraping to stop
        # The queue contains the URLs that have been crawled

        # Initialize queue with a starting URL
        q.put(url)

        # Crawl until the queue is empty, or reaching some number of URLs
        visited = set()
        max_urls = min(max_urls, 50_000)
        while True:
            try:
                next_urls = q.get_many(2000, timeout=5)
            except queue.Empty:
                break
            new_urls = set(next_urls) - visited
            visited |= new_urls
            if len(visited) < max_urls:
                crawl_pages.spawn(q, d, new_urls)
            else:
                d["stop"] = True

        elapsed = (datetime.now() - start_time).total_seconds()
        print(f"Crawled {len(visited)} URLs in {elapsed:.2f} seconds")


@app.local_entrypoint()
def main(starting_url=None, max_urls: int = 10_000):
    starting_url = starting_url or "https://www.wikipedia.org/"
    scrape.remote(starting_url, max_urls=max_urls)


## Links discovered
- [`.spawn`](https://modal.com/docs/reference/modal.Function#spawn)

--- 09_job_queues/doc_ocr_jobs.py ---
# ---
# deploy: true
# mypy: ignore-errors
# ---

# # Run a job queue that turns documents into structured data with Datalab Marker

# This tutorial shows you how to use Modal as an infinitely scalable job queue
# that can service async tasks from a web app.

# Our job queue will handle a single task: converting images/PDFs into structured data.
# We'll use [Marker](https://github.com/datalab-to/marker) from [Datalab](https://www.datalab.to),
# which can convert images of documents or PDFs to Markdown, JSON, and HTML. Marker is an open-weights model;
# to learn more about commercial usage, see [here](https://github.com/datalab-to/marker?tab=readme-ov-file#commercial-usage).

# For the purpose of this tutorial, we've also built a [React + FastAPI web app on Modal](https://modal.com/docs/examples/doc_ocr_webapp)
# that works together with it, but note that you don't need a web app running on Modal
# to use this pattern. You can submit async tasks to Modal from any Python
# application (for example, a regular Django app running on Kubernetes).

# Try it out for yourself [here](https://modal-labs-examples--example-doc-ocr-webapp-wrapper.modal.run/).

# ## Define an App

# Let's first import `modal` and define an [`App`](https://modal.com/docs/reference/modal.App).
# Later, we'll use the name provided for our job queue App to find it from our web app and submit tasks to it.

from typing import Optional

import modal
from typing_extensions import Literal

app = modal.App("example-doc-ocr-jobs")

# We also define the dependencies we need by specifying an
# [Image](https://modal.com/docs/guide/images).

inference_image = modal.Image.debian_slim(python_version="3.12").uv_pip_install(
    "marker-pdf[full]==1.9.3", "torch==2.8.0"
)

# ## Cache the pre-trained model on a Modal Volume

# We can obtain the pre-trained model we want to run from Datalab
# by using the Marker library.


def load_models():
    import marker.models

    print("loading models")

    return marker.models.create_model_dict()


# The `create_model_dict` function downloads model weights from Datalab's
# cloud storage (S3 bucket) if they aren't already present in the filesystem.
# However, in Modal's serverless environment, filesystems are ephemeral,
# so using this code alone would mean that models need to be downloaded
# many times (every time a new instance of our Function spins up).

# So instead, we create a Modal [Volume](https://modal.com/docs/guide/volumes)
# to store the models. Each Modal Volume is a durable filesystem that any Modal Function can access.
# You can read more about storing model weights on Modal in [our guide](https://modal.com/docs/guide/model-weights).

marker_cache_path = "/root/.cache/datalab/"
marker_cache_volume = modal.Volume.from_name(
    "marker-models-modal-demo", create_if_missing=True
)
marker_cache = {marker_cache_path: marker_cache_volume}

# ## Run Datalab Marker on Modal

# Now let's set up the actual inference.

# Using the [`@app.function`](https://modal.com/docs/reference/modal.App#function)
# decorator, we set up a Modal [Function](https://modal.com/docs/reference/modal.Function).
# We provide arguments to that decorator to customize the hardware, scaling, and other features
# of the Function.

# Here, we say that this Function should use NVIDIA L40S [GPUs](https://modal.com/docs/guide/gpu),
# automatically [retry](https://modal.com/docs/guide/retries#function-retries) failures up to 3 times,
# and have access to our [shared model cache](https://modal.com/docs/guide/volumes).

# Inside the Function, we write out our inference logic,
# which mostly involves configuring components provided by the `marker` library.


@app.function(gpu="l40s", retries=3, volumes=marker_cache, image=inference_image)
def parse_document(
    document: bytes,
    page_range: str | None = None,
    force_ocr: bool = False,
    paginate_output: bool = False,
    output_format: Literal["markdown", "html", "chunks", "json"] = "markdown",
    use_llm: bool = False,
) -> str | dict:
    """
    Args:
        document: Document data (PDF, JPG, PNG) as bytes.
        page_range: Specify which pages to process. Accepts comma-separated page numbers and ranges.
        force_ocr: Force OCR processing on the entire document, even for pages that might contain extractable text.
                    This will also format inline math properly.
        paginate_output: Paginates the output, using \n\n{PAGE_NUMBER} followed by - * 48, then \n\n
        output_format: Output format. Can be markdown, JSON, HTML, or chunks.
        use_llm: use an llm to improve the marker results.
    """
    from tempfile import NamedTemporaryFile

    import marker.config.parser
    import marker.converters.pdf
    import marker.output

    models = load_models()

    # Set up document "converter"
    config = {
        "page_range": page_range,
        "force_ocr": force_ocr,
        "paginate_output": paginate_output,
        "output_format": output_format,
        "use_llm": use_llm,
    }

    config_parser = marker.config.parser.ConfigParser(config)
    config_dict = config_parser.generate_config_dict()
    config_dict["pdftext_workers"] = 1

    converter = marker.converters.pdf.PdfConverter(
        config=config_dict,
        artifact_dict=models,
        processor_list=config_parser.get_processors(),
        renderer=config_parser.get_renderer(),
        llm_service=config_parser.get_llm_service() if use_llm else None,
    )

    # Run the converter on our document
    with NamedTemporaryFile(delete=False, mode="wb+") as temp_path:
        temp_path.write(document)
        rendered_output = converter(temp_path.name)

    # Format the output and return it
    if output_format == "json":
        result = rendered_output.model_dump_json()
    else:
        text, _, images = marker.output.text_from_rendered(rendered_output)

        result = text

    return result


# ## Testing and debugging remote code

# To make sure this code works, we want a way to kick the tires and debug it.

# We can run it on Modal, with no need to set up separate local testing,
# by adding a [`local_entrypoint`](https://modal.com/docs/reference/modal.App#local_entrypoint)
# that invokes the Function `.remote`ly.


@app.local_entrypoint()
def main(document_filename: Optional[str] = None):
    import urllib.request
    from pathlib import Path

    if document_filename is None:
        document_filename = Path(__file__).parent / "receipt.png"
    else:
        document_filename = Path(document_filename)

    if document_filename.exists():
        image = document_filename.read_bytes()
        print(f"running OCR on {document_filename}")
    else:
        document_url = "https://modal-cdn.com/cdnbot/Brandys-walmart-receipt-8g68_a_hk_f9c25fce.webp"
        print(f"running OCR on sample from URL {document_url}")
        request = urllib.request.Request(document_url)
        with urllib.request.urlopen(request) as response:
            image = response.read()
    print(parse_document.remote(image, output_format="html"))


# You can then run this from the command line with:

# ```shell
# modal run doc_ocr_jobs.py
# ```

# ## Deploying the document conversion service

# Now that we have a Function, we can publish it by deploying the App:

# ```shell
# modal deploy doc_ocr_jobs.py
# ```

# Once it's published, we can [look up](https://modal.com/docs/guide/trigger-deployed-functions) this Function
# from another Python process and submit tasks to it:

# ```python
# fn = modal.Function.from_name("example-doc-ocr-jobs", "parse_document")
# fn.spawn(my_document)
# ```

# Modal will auto-scale to handle all the tasks queued, and
# then scale back down to 0 when there's no work left. To see how you could use this from a Python web
# app, take a look at the [receipt parser frontend](https://modal.com/docs/examples/doc_ocr_webapp)
# tutorial.


## Links discovered
- [Marker](https://github.com/datalab-to/marker)
- [Datalab](https://www.datalab.to)
- [here](https://github.com/datalab-to/marker?tab=readme-ov-file#commercial-usage)
- [React + FastAPI web app on Modal](https://modal.com/docs/examples/doc_ocr_webapp)
- [here](https://modal-labs-examples--example-doc-ocr-webapp-wrapper.modal.run/)
- [`App`](https://modal.com/docs/reference/modal.App)
- [Image](https://modal.com/docs/guide/images)
- [Volume](https://modal.com/docs/guide/volumes)
- [our guide](https://modal.com/docs/guide/model-weights)
- [`@app.function`](https://modal.com/docs/reference/modal.App#function)
- [Function](https://modal.com/docs/reference/modal.Function)
- [GPUs](https://modal.com/docs/guide/gpu)
- [retry](https://modal.com/docs/guide/retries#function-retries)
- [shared model cache](https://modal.com/docs/guide/volumes)
- [`local_entrypoint`](https://modal.com/docs/reference/modal.App#local_entrypoint)
- [look up](https://modal.com/docs/guide/trigger-deployed-functions)
- [receipt parser frontend](https://modal.com/docs/examples/doc_ocr_webapp)

--- 09_job_queues/doc_ocr_webapp.py ---
# ---
# deploy: true
# cmd: ["modal", "serve", "09_job_queues/doc_ocr_webapp.py"]
# ---

# # Serve a receipt parsing web app

# This tutorial shows you how to use Modal to deploy a fully serverless
# [React](https://reactjs.org/) + [FastAPI](https://fastapi.tiangolo.com/) application.

# We're going to build a simple "Receipt Parser" web app that submits document parsing
# tasks to a separate Modal app defined in [another example](https://modal.com/docs/examples/doc_ocr_jobs),
# polls until the task is completed, and displays
# the results. Try it out for yourself
# [here](https://modal-labs-examples--example-doc-ocr-webapp-wrapper.modal.run/).

# It should look something like this:

# [![Webapp frontend](https://modal-cdn.com/doc_ocr_frontend.jpg)](https://modal-labs-examples--example-doc-ocr-webapp-wrapper.modal.run/)

# ## Basic setup

# Let's get the imports out of the way and define an [`App`](https://modal.com/docs/reference/modal.App).

from pathlib import Path

import fastapi
import fastapi.staticfiles
import modal

app = modal.App("example-doc-ocr-webapp")

# Modal works with any [ASGI](https://modal.com/docs/guide/webhooks#serving-asgi-and-wsgi-apps) or
# [WSGI](https://modal.com/docs/guide/webhooks#wsgi) web framework. Here, we choose to use [FastAPI](https://fastapi.tiangolo.com/).

web_app = fastapi.FastAPI()

# ## Define endpoints

# We need two endpoints: one to accept an image and submit it to the Modal job queue,
# and another to poll for the results of the job.

# In `parse`, we're going to submit tasks to the Function defined in the [Job
# Queue tutorial](https://modal.com/docs/examples/doc_ocr_jobs), so we import it first using
# [`Function.lookup`](https://modal.com/docs/reference/modal.Function#lookup).

# We call [`.spawn()`](https://modal.com/docs/reference/modal.Function#spawn) on the Function handle
# we imported above to kick off our Function without blocking on the results. `spawn` returns
# a unique ID for the function call, which we then use
# to poll for its result.


@web_app.post("/parse")
async def parse(request: fastapi.Request):
    parse_receipt = modal.Function.from_name("example-doc-ocr-jobs", "parse_document")

    form = await request.form()
    receipt = await form["receipt"].read()  # type: ignore
    call = parse_receipt.spawn(receipt)
    return {"call_id": call.object_id}


# `/result` uses the provided `call_id` to instantiate a `modal.FunctionCall` object, and attempt
# to get its result. If the call hasn't finished yet, we return a `202` status code, which indicates
# that the server is still working on the job.


@web_app.get("/result/{call_id}")
async def poll_results(call_id: str):
    function_call = modal.functions.FunctionCall.from_id(call_id)
    try:
        result = function_call.get(timeout=0)
    except TimeoutError:
        return fastapi.responses.JSONResponse(content="", status_code=202)

    return result


# Now that we've defined our endpoints, we're ready to host them on Modal.
# First, we specify our dependencies -- here, a basic Debian Linux
# environment with FastAPI installed.

image = modal.Image.debian_slim(python_version="3.12").uv_pip_install(
    "fastapi[standard]==0.115.4"
)

# Then, we add the static files for our front-end. We've made [a simple React
# app](https://github.com/modal-labs/modal-examples/tree/main/09_job_queues/doc_ocr_frontend)
# that hits the two endpoints defined above. To package these files with our app, we use
# `add_local_dir` with the local directory of the assets, and specify that we want them
# in the `/assets` directory inside our container (the `remote_path`). Then, we instruct FastAPI to [serve
# this static file directory](https://fastapi.tiangolo.com/tutorial/static-files/) at our root path.

local_assets_path = Path(__file__).parent / "doc_ocr_frontend"
image = image.add_local_dir(local_assets_path, remote_path="/assets")

# We serve them from our FastAPI app as `StaticFiles`.

# To put our FastAPI app on Modal, we need to return it from a Python function
# that is wrapped with some extra decorators:

# - [`modal.asgi_app`](https://modal.com/docs/reference/modal.asgi_app)
# to ensure the Modal system knows to route web traffic to it (and in what format)
# - [`modal.concurrent`](https://modal.com/docs/reference/modal.concurrent)
# to allow more than one request (e.g. for stylesheet and for HTML) to be served concurrently
# - [`app.function`](https://modal.com/docs/reference/modal.App#function)
# to turn our Python function into a Modal Function and define the infrastructure it needs
# (here, just the dependencies).


@app.function(image=image)
@modal.concurrent(max_inputs=100)
@modal.asgi_app()
def wrapper():
    web_app.mount("/", fastapi.staticfiles.StaticFiles(directory="/assets", html=True))
    return web_app


# ## Running

# While developing, you can run this as an ephemeral app by executing the command

# ```shell
# modal serve doc_ocr_webapp.py
# ```

# If successful, this will print a URL for your app that you can navigate to in
# your browser ðŸŽ‰ .

# The result should look something like this:

# [![Webapp frontend](https://modal-cdn.com/doc_ocr_frontend.jpg)](https://modal-labs-examples--example-doc-ocr-webapp-wrapper.modal.run/)

# Modal watches all the mounted files and updates the app if anything changes.
# See [these docs](https://modal.com/docs/guide/webhooks#developing-with-modal-serve)
# for more details.

# ## Deploy

# To deploy your application, run

# ```shell
# modal deploy doc_ocr_webapp.py
# ```

# That's all!


## Links discovered
- [React](https://reactjs.org/)
- [FastAPI](https://fastapi.tiangolo.com/)
- [another example](https://modal.com/docs/examples/doc_ocr_jobs)
- [here](https://modal-labs-examples--example-doc-ocr-webapp-wrapper.modal.run/)
- [![Webapp frontend](https://modal-cdn.com/doc_ocr_frontend.jpg)
- [`App`](https://modal.com/docs/reference/modal.App)
- [ASGI](https://modal.com/docs/guide/webhooks#serving-asgi-and-wsgi-apps)
- [WSGI](https://modal.com/docs/guide/webhooks#wsgi)
- [Job
# Queue tutorial](https://modal.com/docs/examples/doc_ocr_jobs)
- [`Function.lookup`](https://modal.com/docs/reference/modal.Function#lookup)
- [`.spawn()`](https://modal.com/docs/reference/modal.Function#spawn)
- [a simple React
# app](https://github.com/modal-labs/modal-examples/tree/main/09_job_queues/doc_ocr_frontend)
- [serve
# this static file directory](https://fastapi.tiangolo.com/tutorial/static-files/)
- [`modal.asgi_app`](https://modal.com/docs/reference/modal.asgi_app)
- [`modal.concurrent`](https://modal.com/docs/reference/modal.concurrent)
- [`app.function`](https://modal.com/docs/reference/modal.App#function)
- [these docs](https://modal.com/docs/guide/webhooks#developing-with-modal-serve)

--- 09_job_queues/web_job_queue_wrapper.py ---
# ---
# cmd: ["modal", "run", "09_job_queues/web_job_queue_wrapper.py::test_polling"]
# mypy: ignore-errors
# ---

# # Create a web wrapper for job queue, submission, polling, & results

# This simple tutorial shows you how to create an API endpoint that you can use
# to poll the status of your request.

# Let's first import `modal` and define an [`App`](https://modal.com/docs/reference/modal.App).

import time

import modal

app = modal.App("example-web-job-queue-wrapper")

# Next, we'll create a dummy backend service, in reality you may plug an a LLM or Diffusion model here.
# We'll add artificial delays to simulate a cold boot and a long-running tasks.


@app.cls()
class BackendService:
    @modal.enter()
    def enter(self):
        print("begin cold booting")
        time.sleep(10)
        print("end cold booting")

    @modal.method()
    def run(self, input_val: str):
        print(f"begin run with {input_val}")
        time.sleep(5)
        print(f"end run with {input_val}")
        return input_val[::-1]  # reverse the string


# Then, we can define a web endpoint that will submit a request to the backend service
# as well as other API routes for polling or retrieving results.

# To submit jobs asynchronously, we can use ['spawn'](https://modal.com/docs/reference/modal.Function#spawn),
# which return a [`FunctionCall`](https://modal.com/docs/reference/modal.FunctionCall) object that represents
# the submitted job.
#
# Then we can poll results by checking the ['call graph'](https://modal.com/docs/reference/modal.call_graph)
# of the `FunctionCall` object.


@app.function(
    image=modal.Image.debian_slim().uv_pip_install("fastapi[standard]==0.116.0")
)
@modal.asgi_app()
@modal.concurrent(max_inputs=100)
def web_endpoint():
    from fastapi import FastAPI, Request

    web_app = FastAPI()

    service = BackendService()

    @web_app.post("/run")
    async def submit(request: Request):
        """Asynchronously submit a request to the backend service."""
        input_val = (await request.json())["input_val"]
        fc = service.run.spawn(input_val)
        while len(fc.get_call_graph()) == 0:
            time.sleep(0.1)
        return {"request_id": fc.object_id}

    @web_app.get("/requests/{request_id}/status")
    async def status(request_id: str):
        """Get the status of the request from the call graph."""
        fc = modal.FunctionCall.from_id(request_id)
        fc_input_info = fc.get_call_graph()[0].children[0]
        assert fc_input_info.function_call_id == fc.object_id, "unexpected graph"
        return {"status": fc_input_info.status.name}

    @web_app.get("/requests/{request_id}")
    async def result(request_id: str):
        fc = modal.FunctionCall.from_id(request_id)
        return {"response": fc.get()}

    return web_app


# To test this you can do:
# ```bash
# modal serve web_job_queue_wrapper.py
# ```

# Or run the test locally:
# ```bash
# modal run web_job_queue_wrapper.py::test_polling
# ```


@app.local_entrypoint()
def test_polling():
    """Test the polling job queue by submitting a request and polling for results."""
    import json
    import urllib.parse
    import urllib.request

    # Get the deployed URL
    url = web_endpoint.get_web_url()
    print(f"URL: {url}")

    # Submit request
    print("submitting request")
    data = json.dumps({"input_val": "Hello, world!"}).encode("utf-8")
    headers = {"Content-Type": "application/json"}
    req = urllib.request.Request(
        f"{url}/run", data=data, headers=headers, method="POST"
    )

    try:
        with urllib.request.urlopen(req) as response:
            result = json.loads(response.read().decode("utf-8"))
            request_id = result["request_id"]
            print(f"got request id: {request_id}, polling status")
    except Exception as e:
        print(f"Failed to submit request: {e}")
        return

    # Poll for status
    while True:
        try:
            with urllib.request.urlopen(
                f"{url}/requests/{request_id}/status"
            ) as response:
                data = json.loads(response.read().decode("utf-8"))
                if data["status"] == "SUCCESS":
                    print("request completed successfully")
                    break
                else:
                    print(f"request result is {data['status']}")
        except Exception as e:
            print(f"poll failed: {e}")
        time.sleep(1)

    # Retrieve result
    print("retrieving result")
    try:
        with urllib.request.urlopen(f"{url}/requests/{request_id}") as response:
            result = json.loads(response.read().decode("utf-8"))
            print(f"result is {result}")
            print("done")
    except Exception as e:
        print(f"Failed to retrieve result: {e}")


## Links discovered
- [`App`](https://modal.com/docs/reference/modal.App)
- ['spawn'](https://modal.com/docs/reference/modal.Function#spawn)
- [`FunctionCall`](https://modal.com/docs/reference/modal.FunctionCall)
- ['call graph'](https://modal.com/docs/reference/modal.call_graph)

--- 10_integrations/algolia_indexer.py ---
# ---
# deploy: true
# env: {"MODAL_ENVIRONMENT": "main"}
# ---

# # Algolia docsearch crawler

# This tutorial shows you how to use Modal to run the [Algolia docsearch
# crawler](https://docsearch.algolia.com/docs/legacy/run-your-own/) to index your
# website and make it searchable. This is not just example code - we run the same
# code in production to power search on this page (`Ctrl+K` to try it out!).

# ## Basic setup

# Let's get the imports out of the way.

import json
import os
import subprocess

import modal

# Modal lets you [use and extend existing Docker images](https://modal.com/docs/guide/custom-container#use-an-existing-container-image-with-from_registry),
# as long as they have `python` and `pip` available. We'll use the official crawler image built by Algolia, with a small
# adjustment: since this image has `python` symlinked to `python3.6` and Modal is not compatible with Python 3.6, we
# install Python 3.11 and symlink that as the `python` executable instead.

algolia_image = modal.Image.from_registry(
    "algolia/docsearch-scraper:v1.16.0",
    add_python="3.11",
    setup_dockerfile_commands=["ENTRYPOINT []"],
)

app = modal.App("example-algolia-indexer")

# ## Configure the crawler

# Now, let's configure the crawler with the website we want to index, and which
# CSS selectors we want to scrape. Complete documentation for crawler configuration is available
# [here](https://docsearch.algolia.com/docs/legacy/config-file).

CONFIG = {
    "index_name": "modal_docs",
    "custom_settings": {
        "separatorsToIndex": "._",
        "synonyms": [["cls", "class"]],
    },
    "stop_urls": [
        "https://modal.com/docs/reference/modal.Stub",
        "https://modal.com/gpu-glossary",
        "https://modal.com/docs/reference/changelog",
    ],
    "start_urls": [
        {
            "url": "https://modal.com/docs/guide",
            "selectors_key": "default",
            "page_rank": 2,
        },
        {
            "url": "https://modal.com/docs/examples",
            "selectors_key": "examples",
            "page_rank": 1,
        },
        {
            "url": "https://modal.com/docs/reference",
            "selectors_key": "reference",
            "page_rank": 1,
        },
    ],
    "selectors": {
        "default": {
            "lvl0": {
                "selector": "header .navlink-active",
                "global": True,
            },
            "lvl1": "article h1",
            "lvl2": "article h2",
            "lvl3": "article h3",
            "text": "article p,article ol,article ul",
        },
        "examples": {
            "lvl0": {
                "selector": "header .navlink-active",
                "global": True,
            },
            "lvl1": "article h1",
            "text": "article p,article ol,article ul",
        },
        "reference": {
            "lvl0": {
                "selector": "//div[contains(@class, 'sidebar')]//a[contains(@class, 'active')]//preceding::a[contains(@class, 'header')][1]",
                "type": "xpath",
                "global": True,
                "default_value": "",
                "skip": {"when": {"value": ""}},
            },
            "lvl1": "article h1",
            "lvl2": "article h2",
            "lvl3": "article h3",
            "text": "article p,article ol,article ul",
        },
    },
}

# ## Create an API key

# If you don't already have one, sign up for an account on [Algolia](https://www.algolia.com/). Set up
# a project and create an API key with `write` access to your index, and with the ACL permissions
# `addObject`, `editSettings` and `deleteIndex`. Now, create a Secret on the Modal [Secrets](https://modal.com/secrets)
# page with the `API_KEY` and `APPLICATION_ID` you just created. You can name this anything you want,
# but we named it `algolia-secret` and so that's what the code below expects.

# ## The actual function

# We want to trigger our crawler from our CI/CD pipeline, so we're serving it as a
# [web endpoint](https://modal.com/docs/guide/webhooks) that can be triggered by a `GET` request during deploy.
# You could also consider running the crawler on a [schedule](https://modal.com/docs/guide/cron).

# The Algolia crawler is written for Python 3.6 and needs to run in the `pipenv` created for it,
# so we're invoking it using a subprocess.


@app.function(
    image=algolia_image,
    secrets=[modal.Secret.from_name("algolia-secret")],
)
def crawl():
    # Installed with a 3.6 venv; Python 3.6 is unsupported by Modal, so use a subprocess instead.
    subprocess.run(
        ["pipenv", "run", "python", "-m", "src.index"],
        env={**os.environ, "CONFIG": json.dumps(CONFIG)},
    )


# We want to be able to trigger this function through a webhook.


@app.function(image=modal.Image.debian_slim().uv_pip_install("fastapi[standard]"))
@modal.fastapi_endpoint()
def crawl_webhook():
    crawl.remote()
    return "Finished indexing docs"


# ## Deploy the indexer

# That's all the code we need! To deploy your application, run

# ```shell
# modal deploy algolia_indexer.py
# ```

# If successful, this will print a URL for your new webhook, that you can hit using
# `curl` or a browser. Logs from webhook invocations can be found from the [apps](https://modal.com/apps)
# page.

# The indexed contents can be found at https://www.algolia.com/apps/APP_ID/explorer/browse/, for your
# APP_ID. Once you're happy with the results, you can [set up the `docsearch` package with your
# website](https://docsearch.algolia.com/docs/docsearch-v3/), and create a search component that uses this index.

# ## Entrypoint for development

# To make it easier to test this, we also have an entrypoint for when you run
# `modal run algolia_indexer.py`


@app.local_entrypoint()
def run():
    crawl.remote()


## Links discovered
- [Algolia docsearch
# crawler](https://docsearch.algolia.com/docs/legacy/run-your-own/)
- [use and extend existing Docker images](https://modal.com/docs/guide/custom-container#use-an-existing-container-image-with-from_registry)
- [here](https://docsearch.algolia.com/docs/legacy/config-file)
- [Algolia](https://www.algolia.com/)
- [Secrets](https://modal.com/secrets)
- [web endpoint](https://modal.com/docs/guide/webhooks)
- [schedule](https://modal.com/docs/guide/cron)
- [apps](https://modal.com/apps)
- [set up the `docsearch` package with your
# website](https://docsearch.algolia.com/docs/docsearch-v3/)

--- 10_integrations/cloud_bucket_mount_loras.py ---
# ---
# output-directory: "/tmp/stable-diffusion-xl"
# deploy: true
# ---

# # LoRAs Galore: Create a LoRA Playground with Modal, Gradio, and S3

# This example shows how to mount an S3 bucket in a Modal app using [`CloudBucketMount`](https://modal.com/docs/reference/modal.CloudBucketMount).
# We will download a bunch of LoRA adapters from the [HuggingFace Hub](https://huggingface.co/models) into our S3 bucket
# then read from that bucket, on the fly, when doing inference.

# By default, we use the [IKEA instructions LoRA](https://huggingface.co/ostris/ikea-instructions-lora-sdxl) as an example,
# which produces the following image when prompted to generate "IKEA instructions for building a GPU rig for deep learning":

# ![IKEA instructions for building a GPU rig for deep learning](./ikea-instructions-for-building-a-gpu-rig-for-deep-learning.png)

# By the end of this example, we've deployed a "playground" app where anyone with a browser can try
# out these custom models. That's the power of Modal: custom, autoscaling AI applications, deployed in seconds.
# You can try out our deployment [here](https://modal-labs-examples--example-cloud-bucket-mount-loras-ui.modal.run).

# ## Basic setup

import io
import os
from pathlib import Path
from typing import Optional

import modal

# You will need to have an S3 bucket and AWS credentials to run this example. Refer to the documentation
# for the detailed [IAM permissions](https://modal.com/docs/guide/cloud-bucket-mounts#iam-permissions) those credentials will need.

# After you are done creating a bucket and configuring IAM settings,
# you now need to create a [Modal Secret](https://modal.com/docs/guide/secrets). Navigate to the "Secrets" tab and
# click on the AWS card, then fill in the fields with the AWS key and secret created
# previously. Name the Secret `s3-bucket-secret`.

bucket_secret = modal.Secret.from_name(
    "s3-bucket-secret",
    required_keys=["AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY"],
)

MOUNT_PATH: Path = Path("/mnt/bucket")
LORAS_PATH: Path = MOUNT_PATH / "loras/v5"

BASE_MODEL = "stabilityai/stable-diffusion-xl-base-1.0"
CACHE_DIR = "/hf-cache"

# Modal runs serverless functions inside containers.
# The environments those functions run in are defined by
# the container `Image`. The line below constructs an image
# with the dependencies we need -- no need to install them locally.

image = (
    modal.Image.debian_slim(python_version="3.12")
    .uv_pip_install(
        "huggingface_hub==0.21.4",
        "transformers==4.38.2",
        "diffusers==0.26.3",
        "peft==0.9.0",
        "accelerate==0.27.2",
    )
    .env({"HF_HUB_CACHE": CACHE_DIR})
)

with image.imports():
    # we import these dependencies only inside the container
    import diffusers
    import huggingface_hub
    import torch

# We attach the S3 bucket to all the Modal functions in this app by mounting it on the filesystem they see,
# passing a `CloudBucketMount` to the `volumes` dictionary argument. We can read and write to this mounted bucket
# (almost) as if it were a local directory.

app = modal.App(
    "example-cloud-bucket-mount-loras",
    image=image,
    volumes={
        MOUNT_PATH: modal.CloudBucketMount(
            "modal-s3mount-test-bucket",
            secret=bucket_secret,
        )
    },
)


# For the base model, we'll use a modal.Volume to store the Hugging Face cache.
cache_volume = modal.Volume.from_name("hf-hub-cache", create_if_missing=True)


@app.function(image=image, volumes={CACHE_DIR: cache_volume})
def download_model():
    loc = huggingface_hub.snapshot_download(repo_id=BASE_MODEL)
    print(f"Saved model to {loc}")


# ## Acquiring LoRA weights

# `search_loras()` will use the Hub API to search for LoRAs. We limit LoRAs
# to a maximum size to avoid downloading very large model weights.
# We went with 800 MiB, but feel free to adapt to what works best for you.


@app.function(secrets=[bucket_secret])
def search_loras(limit: int, max_model_size: int = 1024 * 1024 * 1024):
    api = huggingface_hub.HfApi()

    model_ids: list[str] = []
    for model in api.list_models(
        tags=["lora", f"base_model:{BASE_MODEL}"],
        library="diffusers",
        sort="downloads",  # sort by most downloaded
    ):
        try:
            model_size = 0
            for file in api.list_files_info(model.id):
                model_size += file.size

        except huggingface_hub.utils.GatedRepoError:
            print(f"gated model ({model.id}); skipping")
            continue

        # Skip models that are larger than file limit.
        if model_size > max_model_size:
            print(f"model {model.id} is too large; skipping")
            continue

        model_ids.append(model.id)
        if len(model_ids) >= limit:
            return model_ids

    return model_ids


# We want to take the LoRA weights we found and move them from Hugging Face onto S3,
# where they'll be accessible, at short latency and high throughput, for our Modal functions.
# Downloading files in this mount will automatically upload files to S3.
# To speed things up, we will run this function in parallel using Modal's
# [`map`](https://modal.com/docs/reference/modal.Function#map).
@app.function()
def download_lora(repository_id: str) -> Optional[str]:
    os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"

    # CloudBucketMounts will report 0 bytes of available space leading to many
    # unnecessary warnings, so we patch the method that emits those warnings.
    from huggingface_hub import file_download

    file_download._check_disk_space = lambda x, y: False

    repository_path = LORAS_PATH / repository_id
    try:
        # skip models we've already downloaded
        if not repository_path.exists():
            huggingface_hub.snapshot_download(
                repository_id,
                local_dir=repository_path.as_posix().replace(".", "_"),
                allow_patterns=["*.safetensors"],
            )
        downloaded_lora = len(list(repository_path.rglob("*.safetensors"))) > 0
    except OSError:
        downloaded_lora = False
    except FileNotFoundError:
        downloaded_lora = False
    if downloaded_lora:
        return repository_id
    else:
        return None


# ## Inference with LoRAs

# We define a `StableDiffusionLoRA` class to organize our inference code.
# We load Stable Diffusion XL 1.0 as a base model, then, when doing inference,
# we load whichever LoRA the user specifies from the S3 bucket.
# For more on the decorators we use on the methods below to speed up building and booting,
# check out the [container lifecycle hooks guide](https://modal.com/docs/guide/lifecycle-functions).


@app.cls(
    gpu="a10g",  # A10G GPUs are great for inference
    volumes={CACHE_DIR: cache_volume},  # We cache the base model
)
class StableDiffusionLoRA:
    @modal.enter()  # when a new container starts, we load the base model into the GPU
    def load(self):
        self.pipe = diffusers.DiffusionPipeline.from_pretrained(
            BASE_MODEL, torch_dtype=torch.float16
        ).to("cuda")

    @modal.method()  # at inference time, we pull in the LoRA weights and pass the final model the prompt
    def run_inference_with_lora(
        self, lora_id: str, prompt: str, seed: int = 8888
    ) -> bytes:
        for file in (LORAS_PATH / lora_id).rglob("*.safetensors"):
            self.pipe.load_lora_weights(lora_id, weight_name=file.name)
            break

        lora_scale = 0.9
        image = self.pipe(
            prompt,
            num_inference_steps=10,
            cross_attention_kwargs={"scale": lora_scale},
            generator=torch.manual_seed(seed),
        ).images[0]

        buffer = io.BytesIO()
        image.save(buffer, format="PNG")

        return buffer.getvalue()


# ## Try it locally!

# To use our inference code from our local command line, we add a `local_entrypoint` to our `app`.
# Run it using `modal run cloud_bucket_mount_loras.py`, and pass `--help`
# to see the available options.

# The inference code will run on our machines, but the results will be available on yours.


@app.local_entrypoint()
def main(
    limit: int = 100,
    example_lora: str = "ostris/ikea-instructions-lora-sdxl",
    prompt: str = "IKEA instructions for building a GPU rig for deep learning",
    seed: int = 8888,
):
    # Download LoRAs in parallel.
    lora_model_ids = [example_lora]
    lora_model_ids += search_loras.remote(limit)

    downloaded_loras = []
    for model in download_lora.map(lora_model_ids):
        if model:
            downloaded_loras.append(model)

    print(f"downloaded {len(downloaded_loras)} loras => {downloaded_loras}")

    # Run inference using one of the downloaded LoRAs.
    byte_stream = StableDiffusionLoRA().run_inference_with_lora.remote(
        example_lora, prompt, seed
    )
    dir = Path("/tmp/stable-diffusion-xl")
    if not dir.exists():
        dir.mkdir(exist_ok=True, parents=True)

    output_path = dir / f"{as_slug(prompt.lower())}.png"
    print(f"Saving it to {output_path}")
    with open(output_path, "wb") as f:
        f.write(byte_stream)


# ## LoRA Exploradora: A hosted Gradio interface
#
# Command line tools are cool, but we can do better!
# With the Gradio library by Hugging Face, we can create a simple web interface
# around our Python inference function, then use Modal to host it for anyone to try out.
#
# To set up your own, run `modal deploy cloud_bucket_mount_loras.py` and navigate to the URL it prints out.
# If you're playing with the code, use `modal serve` instead to see changes live.

web_image = modal.Image.debian_slim(python_version="3.12").uv_pip_install(
    "fastapi[standard]==0.115.4",
    "gradio~=5.7.1",
    "pillow~=10.2.0",
)


@app.function(
    image=web_image,
    scaledown_window=60 * 20,
    # gradio requires sticky sessions
    # so we limit the number of concurrent containers to 1
    # and allow it to scale to 100 concurrent inputs
    max_containers=1,
)
@modal.concurrent(max_inputs=100)
@modal.asgi_app()
def ui():
    """A simple Gradio interface around our LoRA inference."""
    import io

    import gradio as gr
    from fastapi import FastAPI
    from gradio.routes import mount_gradio_app
    from PIL import Image

    # determine which loras are available
    lora_ids = [
        f"{lora_dir.parent.stem}/{lora_dir.stem}" for lora_dir in LORAS_PATH.glob("*/*")
    ]

    # pick one to be default, set a default prompt
    default_lora_id = (
        "ostris/ikea-instructions-lora-sdxl"
        if "ostris/ikea-instructions-lora-sdxl" in lora_ids
        else lora_ids[0]
    )
    default_prompt = (
        "IKEA instructions for building a GPU rig for deep learning"
        if default_lora_id == "ostris/ikea-instructions-lora-sdxl"
        else "text"
    )

    # the simple path to making an app on Gradio is an Interface: a UI wrapped around a function.
    def go(lora_id: str, prompt: str, seed: int) -> Image:
        return Image.open(
            io.BytesIO(
                StableDiffusionLoRA().run_inference_with_lora.remote(
                    lora_id, prompt, seed
                )
            ),
        )

    iface = gr.Interface(
        go,
        inputs=[  # the inputs to go/our inference function
            gr.Dropdown(choices=lora_ids, value=default_lora_id, label="ðŸ‘‰ LoRA ID"),
            gr.Textbox(default_prompt, label="ðŸŽ¨ Prompt"),
            gr.Number(value=8888, label="ðŸŽ² Random Seed"),
        ],
        outputs=gr.Image(label="Generated Image"),
        # some extra bits to make it look nicer
        title="LoRAs Galore",
        description="# Try out some of the top custom SDXL models!"
        "\n\nPick a LoRA finetune of SDXL from the dropdown, then prompt it to generate an image."
        "\n\nCheck out [the code on GitHub](https://github.com/modal-labs/modal-examples/blob/main/10_integrations/cloud_bucket_mount_loras.py)"
        " if you want to create your own version or just see how it works."
        "\n\nPowered by [Modal](https://modal.com) ðŸš€",
        theme="soft",
        allow_flagging="never",
    )

    return mount_gradio_app(app=FastAPI(), blocks=iface, path="/")


def as_slug(name):
    """Converts a string, e.g. a prompt, into something we can use as a filename."""
    import re

    s = str(name).strip().replace(" ", "-")
    s = re.sub(r"(?u)[^-\w.]", "", s)
    return s


## Links discovered
- [`CloudBucketMount`](https://modal.com/docs/reference/modal.CloudBucketMount)
- [HuggingFace Hub](https://huggingface.co/models)
- [IKEA instructions LoRA](https://huggingface.co/ostris/ikea-instructions-lora-sdxl)
- [IKEA instructions for building a GPU rig for deep learning](https://github.com/modal-labs/modal-examples/blob/main/10_integrations/ikea-instructions-for-building-a-gpu-rig-for-deep-learning.png)
- [here](https://modal-labs-examples--example-cloud-bucket-mount-loras-ui.modal.run)
- [IAM permissions](https://modal.com/docs/guide/cloud-bucket-mounts#iam-permissions)
- [Modal Secret](https://modal.com/docs/guide/secrets)
- [`map`](https://modal.com/docs/reference/modal.Function#map)
- [container lifecycle hooks guide](https://modal.com/docs/guide/lifecycle-functions)
- [the code on GitHub](https://github.com/modal-labs/modal-examples/blob/main/10_integrations/cloud_bucket_mount_loras.py)
- [Modal](https://modal.com)

--- 10_integrations/cron_datasette.py ---
# ---
# deploy: true
# ---

# # Publish interactive datasets with Datasette

# ![Datasette user interface](https://modal-cdn.com/cdnbot/imdb_datasetteqzaj3q9d_a83d82fd.webp)

# Build and deploy an interactive movie database that automatically updates daily with the latest IMDb data.
# This example shows how to serve a Datasette application on Modal with millions of movie and TV show records.

# Try it out for yourself [here](https://modal-labs-examples--example-cron-datasette-ui.modal.run).

# Along the way, we will learn how to use the following Modal features:

# * [Volumes](https://modal.com/docs/guide/volumes): a persisted volume lets us store and grow the published dataset over time.

# * [Scheduled functions](https://modal.com/docs/guide/cron): the underlying dataset is refreshed daily, so we schedule a function to run daily.

# * [Web endpoints](https://modal.com/docs/guide/webhooks): exposes the Datasette application for web browser interaction and API requests.

# ## Basic setup

# Let's get started writing code.
# For the Modal container image we need a few Python packages.

import asyncio
import gzip
import pathlib
import shutil
import tempfile
from datetime import datetime
from urllib.request import urlretrieve

import modal

app = modal.App("example-cron-datasette")
cron_image = modal.Image.debian_slim(python_version="3.12").uv_pip_install(
    "datasette==0.65.1", "sqlite-utils==3.38", "tqdm~=4.67.1", "setuptools<80"
)

# ## Persistent dataset storage

# To separate database creation and maintenance from serving, we'll need the underlying
# database file to be stored persistently. To achieve this we use a
# [Volume](https://modal.com/docs/guide/volumes).

volume = modal.Volume.from_name(
    "example-cron-datasette-cache-vol", create_if_missing=True
)
DB_FILENAME = "imdb.db"
VOLUME_DIR = "/cache-vol"
DATA_DIR = pathlib.Path(VOLUME_DIR, "imdb-data")
DB_PATH = pathlib.Path(VOLUME_DIR, DB_FILENAME)

# ## Getting a dataset

# [IMDb Datasets](https://datasets.imdbws.com/) are available publicly and are updated daily.
# We will download the title.basics.tsv.gz file which contains basic information about all titles (movies, TV shows, etc.).
# Since we are serving an interactive database which updates daily, we will download the files into a temporary directory and then move them to the volume to prevent downtime.

BASE_URL = "https://datasets.imdbws.com/"
IMDB_FILES = [
    "title.basics.tsv.gz",
]


@app.function(
    image=cron_image,
    volumes={VOLUME_DIR: volume},
    retries=2,
    timeout=1800,
)
def download_dataset(force_refresh=False):
    """Download IMDb dataset files."""
    if DATA_DIR.exists() and not force_refresh:
        print(
            f"Dataset already present and force_refresh={force_refresh}. Skipping download."
        )
        return

    TEMP_DATA_DIR = pathlib.Path(VOLUME_DIR, "imdb-data-temp")
    if TEMP_DATA_DIR.exists():
        shutil.rmtree(TEMP_DATA_DIR)

    TEMP_DATA_DIR.mkdir(parents=True, exist_ok=True)

    print("Downloading IMDb dataset...")

    try:
        for filename in IMDB_FILES:
            print(f"Downloading {filename}...")
            url = BASE_URL + filename
            output_path = TEMP_DATA_DIR / filename

            urlretrieve(url, output_path)
            print(f"Successfully downloaded {filename}")

        if DATA_DIR.exists():
            # move the current data to a backup location
            OLD_DATA_DIR = pathlib.Path(VOLUME_DIR, "imdb-data-old")
            if OLD_DATA_DIR.exists():
                shutil.rmtree(OLD_DATA_DIR)
            shutil.move(DATA_DIR, OLD_DATA_DIR)

            # move the new data into place
            shutil.move(TEMP_DATA_DIR, DATA_DIR)

            # clean up the old data
            shutil.rmtree(OLD_DATA_DIR)
        else:
            shutil.move(TEMP_DATA_DIR, DATA_DIR)

        volume.commit()
        print("Finished downloading dataset.")

    except Exception as e:
        print(f"Error during download: {e}")
        if TEMP_DATA_DIR.exists():
            shutil.rmtree(TEMP_DATA_DIR)
        raise


# ## Data processing

# This dataset is no swamp, but a bit of data cleaning is still in order.
# The following function reads a .tsv file, cleans the data and yields batches of records.


def parse_tsv_file(filepath, batch_size=50000, filter_year=None):
    """Parse a gzipped TSV file and yield batches of records."""
    import csv

    with gzip.open(filepath, "rt", encoding="utf-8") as gz_file:
        reader = csv.DictReader(gz_file, delimiter="\t")
        batch = []
        total_processed = 0

        for row in reader:
            # map missing values to None
            row = {k: (None if v == "\\N" else v) for k, v in row.items()}

            # remove nsfw data
            if row.get("isAdult") == "1":
                continue

            if filter_year:
                start_year = int(row.get("startYear", 0) or 0)
                if start_year < filter_year:
                    continue

            batch.append(row)
            total_processed += 1

            if len(batch) >= batch_size:
                yield batch
                batch = []

        # Yield any remaining records
        if batch:
            yield batch

        print(f"Finished processing {total_processed:,} titles.")


# ## Inserting into SQLite

# With the TSV processing out of the way, weâ€™re ready to create a SQLite database and feed data into it.

# Importantly, the `prep_db` function mounts the same volume used by `download_dataset`, and rows are batch inserted with progress logged after each batch,
# as the full IMDb dataset has millions of rows and does take some time to be fully inserted.

# A more sophisticated implementation would only load new data instead of performing a full refresh,
# but weâ€™re keeping things simple for this example!
# We will also create indexes for the titles table to speed up queries.


@app.function(
    image=cron_image,
    volumes={VOLUME_DIR: volume},
    timeout=900,
)
def prep_db(filter_year=None):
    """Process IMDb data files and create SQLite database."""
    import sqlite_utils
    import tqdm

    volume.reload()

    # Create database in a temporary directory first
    with tempfile.TemporaryDirectory() as tmpdir:
        tmpdir_path = pathlib.Path(tmpdir)
        tmp_db_path = tmpdir_path / DB_FILENAME

        db = sqlite_utils.Database(tmp_db_path)

        # Process title.basics.tsv.gz
        titles_file = DATA_DIR / "title.basics.tsv.gz"

        if titles_file.exists():
            titles_table = db["titles"]
            batch_count = 0
            total_processed = 0

            with tqdm.tqdm(desc="Processing titles", unit="batch", leave=True) as pbar:
                for i, batch in enumerate(
                    parse_tsv_file(
                        titles_file, batch_size=50000, filter_year=filter_year
                    )
                ):
                    titles_table.insert_all(batch, batch_size=50000, truncate=(i == 0))
                    batch_count += len(batch)
                    total_processed += len(batch)
                    pbar.update(1)
                    pbar.set_postfix({"titles": f"{total_processed:,}"})

            print(f"Total titles in database: {batch_count:,}")

            # Create indexes for titles so we can query the database faster
            print("Creating indexes...")
            titles_table.create_index(["tconst"], if_not_exists=True, unique=True)
            titles_table.create_index(["primaryTitle"], if_not_exists=True)
            titles_table.create_index(["titleType"], if_not_exists=True)
            titles_table.create_index(["startYear"], if_not_exists=True)
            titles_table.create_index(["genres"], if_not_exists=True)
            print("Created indexes for titles table")

        db.close()

        # Copy the database to the volume
        DB_PATH.parent.mkdir(parents=True, exist_ok=True)
        shutil.copyfile(tmp_db_path, DB_PATH)

    print("Syncing DB with volume.")
    volume.commit()
    print("Volume changes committed.")


# ## Keep it fresh

# IMDb updates their data daily, so we set up
# a [scheduled](https://modal.com/docs/guide/cron) function to automatically refresh the database
# every 24 hours.


@app.function(schedule=modal.Period(hours=24), timeout=4000)
def refresh_db():
    """Scheduled function to refresh the database daily."""
    print(f"Running scheduled refresh at {datetime.now()}")
    download_dataset.remote(force_refresh=True)
    prep_db.remote()


# ## Web endpoint

# Hooking up the SQLite database to a Modal webhook is as simple as it gets.
# The Modal `@asgi_app` decorator wraps a few lines of code: one `import` and a few
# lines to instantiate the `Datasette` instance and return its app server.

# First, let's define a metadata object for the database.
# This will be used to configure Datasette to display a custom UI with some pre-defined queries.

columns = {
    "tconst": "Unique identifier",
    "titleType": "Type (movie, tvSeries, short, etc.)",
    "primaryTitle": "Main title",
    "originalTitle": "Original language title",
    "startYear": "Release year",
    "endYear": "End year (for TV series)",
    "runtimeMinutes": "Runtime in minutes",
    "genres": "Comma-separated genres",
}

queries = {
    "movies_2024": {
        "sql": """
                        SELECT
                            primaryTitle as title,
                            genres,
                            runtimeMinutes as runtime
                        FROM titles
                        WHERE titleType = 'movie'
                        AND startYear = 2024
                        ORDER BY primaryTitle
                        LIMIT 100
                    """,
        "title": "Movies Released in 2024",
    },
    "longest_movies": {
        "sql": """
                        SELECT
                            primaryTitle as title,
                            startYear as year,
                            runtimeMinutes as runtime,
                            genres
                        FROM titles
                        WHERE titleType = 'movie'
                        AND runtimeMinutes IS NOT NULL
                        AND runtimeMinutes > 180
                        ORDER BY runtimeMinutes DESC
                        LIMIT 50
                    """,
        "title": "Longest Movies (3+ hours)",
    },
    "genre_breakdown": {
        "sql": """
                        SELECT
                            genres,
                            COUNT(*) as count
                        FROM titles
                        WHERE titleType = 'movie'
                        AND genres IS NOT NULL
                        GROUP BY genres
                        ORDER BY count DESC
                        LIMIT 25
                    """,
        "title": "Popular Genres",
    },
}


metadata = {
    "title": "IMDb Database Explorer",
    "description": "Explore IMDb movie and TV show data",
    "databases": {
        "imdb": {
            "tables": {
                "titles": {
                    "description": "Basic information about all titles (movies, TV shows, etc.)",
                    "columns": columns,
                }
            },
            "queries": {
                "movies_2024": queries["movies_2024"],
                "longest_movies": queries["longest_movies"],
                "genre_breakdown": queries["genre_breakdown"],
            },
        }
    },
}

# Now we can define the web endpoint that will serve the Datasette application


@app.function(
    image=cron_image,
    volumes={VOLUME_DIR: volume},
)
@modal.concurrent(max_inputs=16)
@modal.asgi_app()
def ui():
    """Web endpoint for Datasette UI."""
    from datasette.app import Datasette

    ds = Datasette(
        files=[DB_PATH],
        settings={
            "sql_time_limit_ms": 60000,
            "max_returned_rows": 10000,
            "allow_download": True,
            "facet_time_limit_ms": 5000,
            "allow_facet": True,
        },
        metadata=metadata,
    )
    asyncio.run(ds.invoke_startup())
    return ds.app()


# ## Publishing to the web

# Run this script using `modal run cron_datasette.py` and it will create the database under 5 minutes!

# If you would like to force a refresh of the dataset, you can use:

# `modal run cron_datasette.py --force-refresh`

# If you would like to filter the data to be after a specific year, you can use:

# `modal run cron_datasette.py --filter-year year`

# You can then use `modal serve cron_datasette.py` to create a short-lived web URL
# that exists until you terminate the script.

# When publishing the interactive Datasette app you'll want to create a persistent URL.
# Just run `modal deploy cron_datasette.py` and your app will be deployed in seconds!


@app.local_entrypoint()
def run(force_refresh: bool = False, filter_year: int = None):
    if force_refresh:
        print("Force refreshing the dataset...")

    if filter_year:
        print(f"Filtering data to be after {filter_year}")

    print("Downloading IMDb dataset...")
    download_dataset.remote(force_refresh=force_refresh)
    print("Processing data and creating SQLite DB...")
    prep_db.remote(filter_year=filter_year)
    print("\nDatabase ready! You can now run:")
    print("  modal serve cron_datasette.py  # For development")
    print("  modal deploy cron_datasette.py  # For production deployment")


# You can explore the data at the [deployed web endpoint](https://modal-labs-examples--example-cron-datasette-ui.modal.run).


## Links discovered
- [Datasette user interface](https://modal-cdn.com/cdnbot/imdb_datasetteqzaj3q9d_a83d82fd.webp)
- [here](https://modal-labs-examples--example-cron-datasette-ui.modal.run)
- [Volumes](https://modal.com/docs/guide/volumes)
- [Scheduled functions](https://modal.com/docs/guide/cron)
- [Web endpoints](https://modal.com/docs/guide/webhooks)
- [Volume](https://modal.com/docs/guide/volumes)
- [IMDb Datasets](https://datasets.imdbws.com/)
- [scheduled](https://modal.com/docs/guide/cron)
- [deployed web endpoint](https://modal-labs-examples--example-cron-datasette-ui.modal.run)

--- 10_integrations/mcp_server_stateless.py ---
# ---
# cmd: ["modal", "run", "10_integrations/mcp_server_stateless.py::test_tool"]
# ---

# # Deploy a remote, stateless MCP server on Modal with FastMCP

# This example demonstrates how to deploy a simple
# [MCP server](https://modelcontextprotocol.io/)
# on Modal.

# The server provides a tool to get the current date and time in a given timezone.
# It is a stateless MCP server, meaning that it does not store any state between requests,
# which is important for mapping onto Modal's serverless Functions.
# It uses the "streamable HTTP" transport type.

# ## Building the MCP server

# First, we define our dependencies.

# We use the [FastMCP library](https://github.com/jlowin/fastmcp) to create the MCP
# server. We wrap with a FastAPI server to expose it to the Internet.

import modal

app = modal.App("example-mcp-server-stateless")

image = modal.Image.debian_slim(python_version="3.12").uv_pip_install(
    "fastapi==0.115.14",
    "fastmcp==2.10.6",
    "pydantic==2.11.10",
)


# Next, we create the MCP server itself using FastMCP and add a tool to it that
# allows LLMs to get the current date and time in a given timezone.


def make_mcp_server():
    from fastmcp import FastMCP

    mcp = FastMCP("Date and Time MCP Server")

    @mcp.tool()
    async def current_date_and_time(timezone: str = "UTC") -> str:
        """Get the current date and time.

        Args:
            timezone: The timezone to get the date and time in (optional). Defaults to UTC.

        Returns:
            The current date and time in the given timezone, in ISO 8601 format.
        """
        from datetime import datetime
        from zoneinfo import ZoneInfo

        try:
            tz = ZoneInfo(timezone)
        except Exception:
            raise ValueError(
                f"Invalid timezone '{timezone}'. Please use a valid timezone like 'UTC', "
                "'America/New_York', or 'Europe/Stockholm'."
            )
        return datetime.now(tz).isoformat()

    return mcp


# We then use FastMCP to create a Starlette app with `streamable-http` as transport
# type, and set `stateless_http=True` to make it stateless.

# This will be mounted by the FastAPI app, which we deploy as a
# [Modal web endpoint](https://modal.com/docs/guide/webhooks)
# using [the `asgi_app` decorator](https://modal.com/docs/reference/modal.asgi_app):


@app.function(image=image)
@modal.asgi_app()
def web():
    """ASGI web endpoint for the MCP server"""
    from fastapi import FastAPI

    mcp = make_mcp_server()
    mcp_app = mcp.http_app(transport="streamable-http", stateless_http=True)

    fastapi_app = FastAPI(lifespan=mcp_app.router.lifespan_context)
    fastapi_app.mount("/", mcp_app, "mcp")

    return fastapi_app


# And we're done!

# ## Testing the MCP server

# Now you can [serve](https://modal.com/docs/reference/cli/serve#modal-serve) the MCP
# server by running:

# ```bash
# modal serve mcp_server_stateless.py
# ```

# Then open the [MCP inspector](https://github.com/modelcontextprotocol/inspector):

# ```bash
# npx @modelcontextprotocol/inspector
# ```

# Enter the URL of the MCP server that was printed by the `modal serve` command above,
# suffixed with `/mcp/` (so for example
# `https://modal-labs-examples--datetime-mcp-server-web-dev.modal.run/mcp/`). Also
# make sure to select "Streamable HTTP" as the "Transport Type".

# After connecting and clicking "List Tools" in the "Tools" tab you should see your
# `current_date_and_time` tool listed, and if you "Run Tool" it should give you the
# current date and time in UTC!

# To automatically test the MCP server, we spin up a client and have it list the tools.


@app.function(image=image)
async def test_tool(tool_name: str | None = None):
    from fastmcp import Client
    from fastmcp.client.transports import StreamableHttpTransport

    if tool_name is None:
        tool_name = "current_date_and_time"

    transport = StreamableHttpTransport(url=f"{web.get_web_url()}/mcp/")
    client = Client(transport)

    async with client:
        tools = await client.list_tools()

        for tool in tools:
            print(tool)
            if tool.name == tool_name:
                result = await client.call_tool(tool_name)
                print(result.data)
                return

    raise Exception(f"could not find tool {tool_name}")


# This test is executed by running the script with `modal run`:

# ```bash
# modal run mcp_server_stateless::test_tool
# ```

# ## Deploying the MCP server

# `modal serve` creates an ephemeral, hot-reloading server,
# which is useful for testing and development.

# When it's time to move to production,
# you can deploy the server with

# ```bash
# modal deploy mcp_server_stateless
# ```


## Links discovered
- [MCP server](https://modelcontextprotocol.io/)
- [FastMCP library](https://github.com/jlowin/fastmcp)
- [Modal web endpoint](https://modal.com/docs/guide/webhooks)
- [the `asgi_app` decorator](https://modal.com/docs/reference/modal.asgi_app)
- [serve](https://modal.com/docs/reference/cli/serve#modal-serve)
- [MCP inspector](https://github.com/modelcontextprotocol/inspector)

--- 10_integrations/multion_news_agent.py ---
# ---
# lambda-test: false  # missing-secret
# ---

# # MultiOn: Twitter News Agent

# In this example, we use Modal to deploy a cron job that periodically checks for AI news everyday and tweets it on Twitter using the MultiOn Agent API.

# ## Import and define the app

# Let's start off with imports, and defining a Modal app.

import os

import modal

app = modal.App("example-multion-news-agent")

# ## Searching for AI News

# Let's also define an image that has the `multion` package installed, so we can query the API.

multion_image = modal.Image.debian_slim().uv_pip_install("multion")

# We can now define our main entrypoint, which uses [MultiOn](https://www.multion.ai/)
# to scrape AI news everyday and post it on our Twitter account.
# We specify a [schedule](https://modal.com/docs/guide/cron) in the function decorator, which
# means that our function will run automatically at the given interval.

# ## Set up MultiOn

# [MultiOn](https://multion.ai/) is a Web Action Agent that can take actions on behalf of the user.
# You can watch it in action [here](https://www.youtube.com/watch?v=Rm67ry6bogw).

# The MultiOn API enables building the next level of web automation & custom AI agents capable of performing complex actions on the internet with just a few lines of code.

# To get started, first create an account with [MultiOn](https://www.multion.ai/),
# install the [MultiOn chrome extension](https://chrome.google.com/webstore/detail/ddmjhdbknfidiopmbaceghhhbgbpenmm)
# and login to your Twitter account in your browser.
# To use the API, create a MultiOn API Key
# and store it as a Modal Secret on [the dashboard](https://modal.com/secrets)


@app.function(image=multion_image, secrets=[modal.Secret.from_name("MULTION_API_KEY")])
def news_tweet_agent():
    # Import MultiOn
    import multion

    # Login to MultiOn using the API key
    multion.login(use_api=True, multion_api_key=os.environ["MULTION_API_KEY"])

    # Enable the Agent to run locally
    multion.set_remote(False)

    params = {
        "url": "https://www.multion.ai",
        "cmd": "Go to twitter (im already signed in). Search for the last tweets i made (check the last 10 tweets). Remember them so then you can go a search for super interesting AI news. Search the news on up to 3 different sources. If you see that the source has not really interesting AI news or i already made a tweet about that, then go to a different one. When you finish the research, go and make a few small and interesting AI tweets with the info you gathered. Make sure the tweet is small but informative and interesting for AI enthusiasts. Don't do more than 5 tweets",
        "maxSteps": 100,
    }

    response = multion.browse(params)

    print(f"MultiOn response: {response}")


# ## Test running

# We can now test run our scheduled function as follows: `modal run multion_news_agent.py.py::app.news_tweet_agent`

# ## Defining the schedule and deploying

# Let's define a function that will be called by Modal every day.


@app.function(schedule=modal.Cron("0 9 * * *"))
def run_daily():
    news_tweet_agent.remote()


# In order to deploy this as a persistent cron job, you can run `modal deploy multion_news_agent.py`.

# Once the job is deployed, visit the [apps page](https://modal.com/apps) page to see
# its execution history, logs and other stats.


## Links discovered
- [MultiOn](https://www.multion.ai/)
- [schedule](https://modal.com/docs/guide/cron)
- [MultiOn](https://multion.ai/)
- [here](https://www.youtube.com/watch?v=Rm67ry6bogw)
- [MultiOn chrome extension](https://chrome.google.com/webstore/detail/ddmjhdbknfidiopmbaceghhhbgbpenmm)
- [the dashboard](https://modal.com/secrets)
- [apps page](https://modal.com/apps)

--- 10_integrations/pushgateway.py ---
# ---
# deploy: true
# cmd: ["modal", "serve", "10_integrations/pushgateway.py"]
# ---

# # Publish custom metrics with Prometheus Pushgateway

# This example shows how to publish custom metrics to a Prometheus instance with Modal.
# Due to a Modal container's ephemeral nature, it's not a good fit for a traditional
# scraping-based Prometheus setup. Instead, we'll use a [Prometheus Pushgateway](https://github.com/prometheus/pushgateway)
# to collect and store metrics from our Modal container. We can run the Pushgateway in Modal
# as a separate process and have our application push metrics to it.

# ![Prometheus Pushgateway diagram](./pushgateway_diagram.png)

# ## Install Prometheus Pushgateway

# Since the official Prometheus pushgateway image does not have Python installed, we'll
# use a custom image that includes Python to push metrics to the Pushgateway. Pushgateway
# ships a single binary, so it's easy to get it into a Modal container.

import os
import subprocess

import modal

PUSHGATEWAY_VERSION = "1.9.0"

gw_image = (
    modal.Image.debian_slim(python_version="3.10")
    .apt_install("wget", "tar")
    .run_commands(
        f"wget https://github.com/prometheus/pushgateway/releases/download/v{PUSHGATEWAY_VERSION}/pushgateway-{PUSHGATEWAY_VERSION}.linux-amd64.tar.gz",
        f"tar xvfz pushgateway-{PUSHGATEWAY_VERSION}.linux-amd64.tar.gz",
        f"cp pushgateway-{PUSHGATEWAY_VERSION}.linux-amd64/pushgateway /usr/local/bin/",
        f"rm -rf pushgateway-{PUSHGATEWAY_VERSION}.linux-amd64 pushgateway-{PUSHGATEWAY_VERSION}.linux-amd64.tar.gz",
        "mkdir /pushgateway",
    )
)

# ## Start the Pushgateway

# We'll start the Pushgateway as a separate Modal app. This way, we can run the Pushgateway
# in the background and have our main app push metrics to it. We'll use the `web_server`
# decorator to expose the Pushgateway's web interface. Note that we must set `max_containers=1`
# as the Pushgateway is a single-process application. If we spin up multiple instances, they'll
# conflict with each other.

# This is an example configuration, but a production-ready configuration will differ in two respects:

# 1. You should set up authentication for the Pushgateway. Pushgateway has support for [basic authentication](https://github.com/prometheus/pushgateway/blob/42c4075fc5e2564031f2852885cdb2f5d570f672/README.md#tls-and-basic-authentication)
#    out of the box. If you need more advanced authentication, consider using a [web endpoint with authentication](https://modal.com/docs/guide/webhooks#authentication)
#    which proxies requests to the Pushgateway.

# 2. The Pushgateway should listen on a [custom domain](https://modal.com/docs/guide/webhook-urls#custom-domains).
#    This will allow you to configure Prometheus to scrape metrics from a predictable URL rather than
#    the autogenerated URL Modal assigns to your app.

gw_app = modal.App(
    "example-pushgateway-server",
    image=gw_image,
)


@gw_app.function(max_containers=1)
@modal.web_server(9091)
def serve():
    subprocess.Popen("/usr/local/bin/pushgateway")


# ## Push metrics to the Pushgateway

# Now that we have the Pushgateway running, we can push metrics to it. We'll use the `prometheus_client`
# library to create a simple counter and push it to the Pushgateway. This example is a simple counter,
# but you can push any metric type to the Pushgateway.

# Note that we use the `grouping_key` argument to distinguish between different instances of the same
# metric. This is useful when you have multiple instances of the same app pushing metrics to the Pushgateway.
# Without this, the Pushgateway will overwrite the metric with the latest value.

client_image = modal.Image.debian_slim().uv_pip_install(
    "prometheus-client==0.20.0", "fastapi[standard]==0.115.4"
)
app = modal.App(
    "example-pushgateway",
    image=client_image,
)

with client_image.imports():
    from prometheus_client import (
        CollectorRegistry,
        Counter,
        delete_from_gateway,
        push_to_gateway,
    )


@app.cls()
class ExampleClientApplication:
    @modal.enter()
    def init(self):
        self.registry = CollectorRegistry()
        self.web_url = serve.get_web_url()
        self.instance_id = os.environ["MODAL_TASK_ID"]
        self.counter = Counter(
            "hello_counter",
            "This is a counter",
            registry=self.registry,
        )

    # We must explicitly clean up the metric when the app exits so Prometheus doesn't
    # keep stale metrics around.
    @modal.exit()
    def cleanup(self):
        delete_from_gateway(
            self.web_url,
            job="hello",
            grouping_key={"instance": self.instance_id},
        )

    @modal.fastapi_endpoint(label="hello-pushgateway")
    def hello(self):
        self.counter.inc()
        push_to_gateway(
            self.web_url,
            job="hello",
            grouping_key={"instance": self.instance_id},
            registry=self.registry,
        )
        return f"Hello world from {self.instance_id}!"


app.include(gw_app)

# Now, we can deploy the app and see the metrics in the Pushgateway's web interface.

# ```shell
# $ modal deploy pushgateway.py
# âœ“ Created objects.
# â”œâ”€â”€ ðŸ”¨ Created mount /home/ec2-user/modal/examples/10_integrations/pushgateway.py
# â”œâ”€â”€ ðŸ”¨ Created function ExampleClientApplication.*.
# â”œâ”€â”€ ðŸ”¨ Created web function serve => https://modal-labs-examples--example-pushgateway-serve.modal.run
# â””â”€â”€ ðŸ”¨ Created web endpoint for ExampleClientApplication.hello => https://modal-labs-examples--hello-pushgateway.modal.run
# âœ“ App deployed! ðŸŽ‰
# ```

# You can now go to both the [client application](https://modal-labs-examples--hello-pushgateway.modal.run)
# and [Pushgateway](https://modal-labs-examples--example-pushgateway-serve.modal.run) URLs to see the metrics being pushed.

# ## Hooking up Prometheus

# Now that we have metrics in the Pushgateway, we can configure Prometheus to scrape them. This
# is as simple as adding a new job to your Prometheus configuration. Here's an example configuration
# snippet:

# ```yaml
# scrape_configs:
# - job_name: 'pushgateway'
#   honor_labels: true # required so that the instance label is preserved
#   static_configs:
#   - targets: ['modal-labs-examples--example-pushgateway-serve.modal.run']
# ```

# Note that the target will be different if you have a custom domain set up for the Pushgateway,
# and you may need to configure authentication.

# Once you've added the job to your Prometheus configuration, Prometheus will start scraping metrics
# from the Pushgateway. You can then use Grafana or another visualization tool to create dashboards
# and alerts based on these metrics!

# ![Grafana example](./pushgateway_grafana.png)


## Links discovered
- [Prometheus Pushgateway](https://github.com/prometheus/pushgateway)
- [Prometheus Pushgateway diagram](https://github.com/modal-labs/modal-examples/blob/main/10_integrations/pushgateway_diagram.png)
- [basic authentication](https://github.com/prometheus/pushgateway/blob/42c4075fc5e2564031f2852885cdb2f5d570f672/README.md#tls-and-basic-authentication)
- [web endpoint with authentication](https://modal.com/docs/guide/webhooks#authentication)
- [custom domain](https://modal.com/docs/guide/webhook-urls#custom-domains)
- [client application](https://modal-labs-examples--hello-pushgateway.modal.run)
- [Pushgateway](https://modal-labs-examples--example-pushgateway-serve.modal.run)
- [Grafana example](https://github.com/modal-labs/modal-examples/blob/main/10_integrations/pushgateway_grafana.png)

--- 10_integrations/s3_bucket_mount.py ---
# ---
# output-directory: "/tmp/s3_bucket_mount"
# ---

# # Analyze NYC yellow taxi data with DuckDB on Parquet files from S3

# This example shows how to use Modal for a classic data science task: loading table-structured data into cloud stores,
# analyzing it, and plotting the results.

# In particular, we'll load public NYC taxi ride data into S3 as Parquet files,
# then run SQL queries on it with DuckDB.

# We'll mount the S3 bucket in a Modal app with [`CloudBucketMount`](https://modal.com/docs/reference/modal.CloudBucketMount).
# We will write to and then read from that bucket, in each case using
# Modal's [parallel execution features](https://modal.com/docs/guide/scale) to handle many files at once.

# ## Basic setup

# You will need to have an S3 bucket and AWS credentials to run this example. Refer to the documentation
# for the exact [IAM permissions](https://modal.com/docs/guide/cloud-bucket-mounts#iam-permissions) your credentials will need.

# After you are done creating a bucket and configuring IAM settings,
# you now need to create a [`Secret`](https://modal.com/docs/guide/secrets) to share
# the relevant AWS credentials with your Modal apps.

from datetime import datetime
from pathlib import Path, PosixPath

import modal

image = modal.Image.debian_slim(python_version="3.12").uv_pip_install(
    "requests==2.31.0", "duckdb==0.10.0", "matplotlib==3.8.3"
)
app = modal.App("example-s3-bucket-mount", image=image)

secret = modal.Secret.from_name(
    "s3-bucket-secret",
    required_keys=["AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY"],
)

MOUNT_PATH = PosixPath("/bucket")
YELLOW_TAXI_DATA_PATH = MOUNT_PATH / "yellow_taxi"

# The dependencies installed above are not available locally. The following block instructs Modal
# to only import them inside the container.

with image.imports():
    import duckdb
    import requests


# ## Download New York City's taxi data

# NYC makes data about taxi rides publicly available. The city's [Taxi & Limousine Commission (TLC)](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)
# publishes files in the Parquet format. Files are organized by year and month.

# We are going to download all available files and store them in an S3 bucket. We do this by
# attaching a `modal.CloudBucketMount` with the S3 bucket name and its respective credentials.
# The files in the bucket will then be available at `MOUNT_PATH`.

# As we'll see below, this operation can be massively sped up by running it in parallel on Modal.


@app.function(
    volumes={
        MOUNT_PATH: modal.CloudBucketMount("modal-s3mount-test-bucket", secret=secret),
    },
)
def download_data(year: int, month: int) -> str:
    filename = f"yellow_tripdata_{year}-{month:02d}.parquet"
    url = f"https://d37ci6vzurychx.cloudfront.net/trip-data/{filename}"
    s3_path = MOUNT_PATH / filename
    # Skip downloading if file exists.
    if not s3_path.exists():
        if not YELLOW_TAXI_DATA_PATH.exists():
            YELLOW_TAXI_DATA_PATH.mkdir(parents=True, exist_ok=True)
            with requests.get(url, stream=True) as r:
                r.raise_for_status()
                print(f"downloading => {s3_path}")
                # It looks like we writing locally, but this is actually writing to S3!
                with open(s3_path, "wb") as file:
                    for chunk in r.iter_content(chunk_size=8192):
                        file.write(chunk)

    return s3_path.as_posix()


# ## Analyze data with DuckDB

# [DuckDB](https://duckdb.org/) is an analytical database with rich support for Parquet files.
# It is also very fast. Below, we define a Modal Function that aggregates yellow taxi trips
# within a month (each file contains all the rides from a specific month).


@app.function(
    volumes={
        MOUNT_PATH: modal.CloudBucketMount(
            "modal-s3mount-test-bucket",
            secret=modal.Secret.from_name("s3-bucket-secret"),
        )
    },
)
def aggregate_data(path: str) -> list[tuple[datetime, int]]:
    print(f"processing => {path}")

    # Parse file.
    year_month_part = path.split("yellow_tripdata_")[1]
    year, month = year_month_part.split("-")
    month = month.replace(".parquet", "")

    # Make DuckDB query using in-memory storage.
    con = duckdb.connect(database=":memory:")
    q = """
    with sub as (
        select tpep_pickup_datetime::date d, count(1) c
        from read_parquet(?)
        group by 1
    )
    select d, c from sub
    where date_part('year', d) = ?  -- filter out garbage
    and date_part('month', d) = ?   -- same
    """
    con.execute(q, (path, year, month))
    return list(con.fetchall())


# ## Plot daily taxi rides

# Finally, we want to plot our results.
# The plot created shows the number of yellow taxi rides per day in NYC.
# This function runs remotely, on Modal, so we don't need to install plotting libraries locally.


@app.function()
def plot(dataset) -> bytes:
    import io

    import matplotlib.pyplot as plt

    # Sorting data by date
    dataset.sort(key=lambda x: x[0])

    # Unpacking dates and values
    dates, values = zip(*dataset)

    # Plotting
    plt.figure(figsize=(10, 6))
    plt.plot(dates, values)
    plt.title("Number of NYC yellow taxi trips by weekday, 2018-2023")
    plt.ylabel("Number of daily trips")
    plt.grid(True)
    plt.tight_layout()

    # Saving plot as raw bytes to send back
    buf = io.BytesIO()

    plt.savefig(buf, format="png")

    buf.seek(0)

    return buf.getvalue()


# ## Run everything

# The `@app.local_entrypoint()` defines what happens when we run our Modal program locally.
# We invoke it from the CLI by calling `modal run s3_bucket_mount.py`.
# We first call `download_data()` and `starmap` (named because it's kind of like `map(*args)`)
# on tuples of inputs `(year, month)`. This will download, in parallel,
# all yellow taxi data files into our locally mounted S3 bucket and return a list of
# Parquet file paths. Then, we call `aggregate_data()` with `map` on that list. These files are
# also read from our S3 bucket. So one function writes files to S3 and the other
# reads files from S3 in; both run across many files in parallel.

# Finally, we call `plot` to generate the following figure:
#
# ![Number of NYC yellow taxi trips by weekday, 2018-2023](./nyc_yellow_taxi_trips_s3_mount.png)

# This program should run in less than 30 seconds.


@app.local_entrypoint()
def main():
    # List of tuples[year, month].
    inputs = [(year, month) for year in range(2018, 2023) for month in range(1, 13)]

    # List of file paths in S3.
    parquet_files: list[str] = []
    for path in download_data.starmap(inputs):
        print(f"done => {path}")
        parquet_files.append(path)

    # List of datetimes and number of yellow taxi trips.
    dataset = []
    for r in aggregate_data.map(parquet_files):
        dataset += r

    dir = Path("/tmp") / "s3_bucket_mount"
    if not dir.exists():
        dir.mkdir(exist_ok=True, parents=True)

    figure = plot.remote(dataset)
    path = dir / "nyc_yellow_taxi_trips_s3_mount.png"
    with open(path, "wb") as file:
        print(f"Saving figure to {path}")
        file.write(figure)


## Links discovered
- [`CloudBucketMount`](https://modal.com/docs/reference/modal.CloudBucketMount)
- [parallel execution features](https://modal.com/docs/guide/scale)
- [IAM permissions](https://modal.com/docs/guide/cloud-bucket-mounts#iam-permissions)
- [`Secret`](https://modal.com/docs/guide/secrets)
- [Taxi & Limousine Commission (TLC)](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)
- [DuckDB](https://duckdb.org/)
- [Number of NYC yellow taxi trips by weekday, 2018-2023](https://github.com/modal-labs/modal-examples/blob/main/10_integrations/nyc_yellow_taxi_trips_s3_mount.png)

--- 10_integrations/webscraper.py ---
# ---
# deploy: true
# ---

# # A simple web scraper

# In this guide we'll introduce you to Modal by writing a simple web scraper.
# We'll explain the foundations of a Modal application step by step.

# ## Set up your first Modal app

# Modal Apps are orchestrated as Python scripts but can theoretically run
# anything you can run in a container. To get you started, make sure to install
# the latest `modal` Python package and set up an API token (the first two steps
# [here](https://modal.com/docs/guide)).

# ## Scrape links locally

# First, we create an empty Python file `webscraper.py`. This file will contain our
# application code. Let's write some basic Python code to fetch the contents of a
# web page and print the links (`href` attributes) it finds in the document:

# ```python
# import re
# import sys
# import urllib.request
#
#
# def get_links(url):
#     response = urllib.request.urlopen(url)
#     html = response.read().decode("utf8")
#     links = []
#     for match in re.finditer('href="(.*?)"', html):
#         links.append(match.group(1))
#     return links
#
#
# if __name__ == "__main__":
#     links = get_links(sys.argv[1])
#     print(links)
# ```

# Now obviously this is just pure standard library Python code, and you can run it
# on your machine:

# ```bash
# $ python webscraper.py http://example.com
# ['https://www.iana.org/domains/example']
# ```

# ## Run it on Modal

# To make the `get_links` function run on Modal instead of your local machine, all
# you need to do is

# - Import `modal`
# - Create a [`modal.App`](/docs/reference/modal.App) instance
# - Add an `@app.function()` annotation to your function
# - Replace the `if __name__ == "__main__":` block with a function decorated with
#   [`@app.local_entrypoint()`](/docs/reference/modal.App#local_entrypoint)
# - Call `get_links` using `get_links.remote`

# ```python
# import re
# import urllib.request
# import modal
#
# app = modal.App(name="example-webscraper")
#
#
# @app.function()
# def get_links(url):
#     response = urllib.request.urlopen(url)
#     html = response.read().decode("utf8")
#     links = []
#     for match in re.finditer('href="(.*?)"', html):
#         links.append(match.group(1))
#     return links
#
#
# @app.local_entrypoint()
# def main(url):
#     links = get_links.remote(url)
#     print(links)
# ```

# You can now run this with the Modal CLI, using `modal run` instead of `python`.
# This time, you'll see additional progress indicators while the script is
# running, something like:

# ```bash
# $ modal run webscraper.py --url http://example.com
# âœ“ Initialized.
# âœ“ Created objects.
# ['https://www.iana.org/domains/example']
# âœ“ App completed.
# ```

# ## Add dependencies

# In the code above we make use of the Python standard library `urllib` library.
# This works great for static web pages, but many pages these days use javascript
# to dynamically load content, which wouldn't appear in the loaded html file.
# Let's use the [Playwright](https://playwright.dev/python/docs/intro) package to
# instead launch a headless Chromium browser which can interpret any javascript
# that might be on the page.

# We can pass [custom container images](/docs/guide/images) (defined using
# [`modal.Image`](/docs/reference/modal.Image)) to the `@app.function()`
# decorator. We'll make use of the `modal.Image.debian_slim` pre-bundled Image add
# the shell commands to install Playwright and its dependencies:

import modal

app = modal.App("example-webscraper")
playwright_image = modal.Image.debian_slim(python_version="3.10").run_commands(
    "apt-get update",
    "apt-get install -y software-properties-common",
    "apt-add-repository non-free",
    "apt-add-repository contrib",
    "pip install playwright==1.42.0",
    "playwright install-deps chromium",
    "playwright install chromium",
)

# Note that we don't have to install Playwright or Chromium on our development
# machine since this will all run in Modal. We can now modify our `get_links`
# function to make use of the new tools.


@app.function(image=playwright_image)
async def get_links(cur_url: str) -> list[str]:
    from playwright.async_api import (
        TimeoutError as PlaywrightTimeoutError,
        async_playwright,
    )

    async with async_playwright() as p:
        browser = await p.chromium.launch()
        page = await browser.new_page()

        try:
            await page.goto(cur_url, timeout=10_000)  # ten seconds
        except PlaywrightTimeoutError:
            print(f"Timeout loading {cur_url}, skipping")
            await browser.close()
            return []

        links = await page.eval_on_selector_all(
            "a[href]", "elements => elements.map(element => element.href)"
        )
        await browser.close()

    print("Links", links)
    return list(set(links))


# Since Playwright has a nice async interface, we'll redeclare our `get_links`
# function as async (Modal works with both sync and async functions).

# The first time you run the function after making this change, you'll notice that
# the output first shows the progress of building the image you specified,
# after which your function runs like before. This image is then cached so that on
# subsequent runs of the function it will not be rebuilt as long as the image
# definition is the same.

# ## Scale out

# So far, our script only fetches the links for a single page. What if we want to
# scrape a large list of links in parallel?

# We can do this easily with Modal, because of some magic: the function we wrapped
# with the `@app.function()` decorator is no longer an ordinary function, but a
# Modal [Function](https://modal.com/docs/reference/modal.Function) object. This
# means it comes with a `map` property built in, that lets us run this function
# for all inputs in parallel, scaling up to as many workers as needed.

# Let's change our code to scrape all urls we feed to it in parallel:

# ```python
# @app.local_entrypoint()
# def main():
#     urls = ["http://modal.com", "http://github.com"]
#     for links in get_links.map(urls):
#         for link in links:
#             print(link)
# ```

# ## Deploy it and run it on a schedule

# Let's say we want to log the scraped links daily. We move the print loop into
# its own Modal function and annotate it with a `modal.Period(days=1)` schedule -
# indicating we want to run it once per day. Since the scheduled function will not
# run from our command line, we also add a hard-coded list of links to crawl for
# now. In a more realistic setting we could read this from a database or other
# accessible data source.

# ```python
# @app.function(schedule=modal.Period(days=1))
# def daily_scrape():
#     urls = ["http://modal.com", "http://github.com"]
#     for links in get_links.map(urls):
#         for link in links:
#             print(link)
# ```

# To deploy App permanently, run the command

# ```
# modal deploy webscraper.py
# ```

# Running this command deploys this function and then closes immediately. We can
# see the deployment and all of its runs, including the printed links, on the
# Modal [Apps page](https://modal.com/apps). Rerunning the script will redeploy
# the code with any changes you have made - overwriting an existing deploy with
# the same name ("example-webscraper" in this case).

# ## Add Secrets and integrate with other systems

# Instead of looking at the links in the run logs of our deployments, let's say we
# wanted to post them to a `#scraped-links` Slack channel. To do this, we can
# make use of the [Slack API](https://api.slack.com/) and the `slack-sdk`
# [PyPI package](https://pypi.org/project/slack-sdk/).

# The Slack SDK WebClient requires an API token to get access to our Slack
# Workspace, and since it's bad practice to hardcode credentials into application
# code we make use of Modal's **Secrets**. Secrets are snippets of data that will
# be injected as environment variables in the containers running your functions.

# The easiest way to create Secrets is to go to the
# [Secrets section of modal.com](https://modal.com/secrets). You can both create a
# free-form secret with any environment variables, or make use of presets for
# common services. We'll use the Slack preset and after filling in the necessary
# information we are presented with a snippet of code that can be used to post to
# Slack using our credentials, which looks something like:

import os

slack_sdk_image = modal.Image.debian_slim(python_version="3.10").uv_pip_install(
    "slack-sdk"
)


@app.function(
    image=slack_sdk_image,
    secrets=[
        modal.Secret.from_name(
            "scraper-slack-secret", required_keys=["SLACK_BOT_TOKEN"]
        )
    ],
    retries=3,
)
def bot_token_msg(channel, message):
    import slack_sdk

    client = slack_sdk.WebClient(token=os.environ["SLACK_BOT_TOKEN"])
    print(f"Posting {message} to #{channel}")
    client.chat_postMessage(channel=channel, text=message)


# Notice the `retries` in the `@app.function` decorator.
# That parameter adds automatic retries when Function calls fail
# due to temporary issues, like rate limits. Read more [here](https://modal.com/docs/guide/retries)

# Copy that code, then amend the `daily_scrape` function to call
# `bot_token_msg`. We also add a per-URL `limit` for good measure.


@app.function(schedule=modal.Period(days=1))
def daily_scrape(limit: int = 50):
    urls = ["http://modal.com", "http://github.com"]

    for links in get_links.map(urls):
        for link in links[:limit]:
            bot_token_msg.remote("scraped-links", link)


@app.local_entrypoint()
def main():
    urls = ["http://modal.com", "http://github.com"]
    for links in get_links.map(urls):
        for link in links:
            print(link)


# Note that we are freely making function calls across completely different
# container images, as if they were regular Python functions in the same program!

# We keep the `local_entrypoint` the same so that we can still `modal run`
# this script to test the scraping behavior without posting to Slack.

# ```bash
# modal run webscraper.py  # runs get_links.map via the local_entrypoint
# ```

# If we want to test the `daily_scrape` or `bot_token_msg` Functions themselves, we can do that too!
# We just add the name of the Function to the end of our `modal run` command:

# ```bash
# modal run webscraper.py::daily_scrape --limit 1  # quick test
# ```

# Now redeploy the script to overwrite the old deploy with our updated code, and
# you'll get a daily feed of scraped links in your Slack channel ðŸŽ‰

# ```bash
# modal deploy webscraper.py
# ```

# ## Summary

# We have shown how you can use Modal to develop distributed Python data
# applications using custom containers. Through simple constructs we were able to
# add parallel execution. With the change of a single line of code were were able
# to go from experimental development code to a deployed application. We hope
# this overview gives you a glimpse of what you are able to build using Modal.


## Links discovered
- [here](https://modal.com/docs/guide)
- [`modal.App`](https://github.com/modal-labs/modal-examples/blob/main/docs/reference/modal.App)
- [`@app.local_entrypoint()`](https://github.com/modal-labs/modal-examples/blob/main/docs/reference/modal.App#local_entrypoint)
- [Playwright](https://playwright.dev/python/docs/intro)
- [custom container images](https://github.com/modal-labs/modal-examples/blob/main/docs/guide/images.md)
- [`modal.Image`](https://github.com/modal-labs/modal-examples/blob/main/docs/reference/modal.Image)
- [Function](https://modal.com/docs/reference/modal.Function)
- [Apps page](https://modal.com/apps)
- [Slack API](https://api.slack.com/)
- [PyPI package](https://pypi.org/project/slack-sdk/)
- [Secrets section of modal.com](https://modal.com/secrets)
- [here](https://modal.com/docs/guide/retries)

--- 10_integrations/webscraper_old.py ---
# # Web Scraping on Modal

# This example shows how you can scrape links from a website and post them to a Slack channel using Modal.

import os

import modal

app = modal.App("example-webscraper")


playwright_image = modal.Image.debian_slim(
    python_version="3.10"
).run_commands(  # Doesn't work with 3.11 yet
    "apt-get update",
    "apt-get install -y software-properties-common",
    "apt-add-repository non-free",
    "apt-add-repository contrib",
    "pip install playwright==1.42.0",
    "playwright install-deps chromium",
    "playwright install chromium",
)


@app.function(image=playwright_image)
async def get_links(url: str) -> set[str]:
    from playwright.async_api import async_playwright

    async with async_playwright() as p:
        browser = await p.chromium.launch()
        page = await browser.new_page()
        await page.goto(url)
        links = await page.eval_on_selector_all(
            "a[href]", "elements => elements.map(element => element.href)"
        )
        await browser.close()

    return set(links)


slack_sdk_image = modal.Image.debian_slim(python_version="3.10").uv_pip_install(
    "slack-sdk==3.27.1"
)


@app.function(
    image=slack_sdk_image,
    secrets=[
        modal.Secret.from_name(
            "scraper-slack-secret", required_keys=["SLACK_BOT_TOKEN"]
        )
    ],
)
def bot_token_msg(channel, message):
    import slack_sdk
    from slack_sdk.http_retry.builtin_handlers import RateLimitErrorRetryHandler

    client = slack_sdk.WebClient(token=os.environ["SLACK_BOT_TOKEN"])
    rate_limit_handler = RateLimitErrorRetryHandler(max_retry_count=3)
    client.retry_handlers.append(rate_limit_handler)

    print(f"Posting {message} to #{channel}")
    client.chat_postMessage(channel=channel, text=message)


@app.function()
def scrape():
    links_of_interest = ["http://modal.com"]

    for links in get_links.map(links_of_interest):
        for link in links:
            bot_token_msg.remote("scraped-links", link)


@app.function(schedule=modal.Period(days=1))
def daily_scrape():
    scrape.remote()


@app.local_entrypoint()
def run():
    scrape.remote()


--- 10_integrations/streamlit/app.py ---
# ---
# lambda-test: false  # auxiliary-file
# ---
# ## Demo Streamlit application.
#
# This application is the example from https://docs.streamlit.io/library/get-started/create-an-app.
#
# Streamlit is designed to run its apps as Python scripts, not functions, so we separate the Streamlit
# code into this module, away from the Modal application code.


def main():
    import numpy as np
    import pandas as pd
    import streamlit as st

    st.title("Uber pickups in NYC!")

    DATE_COLUMN = "date/time"
    DATA_URL = (
        "https://s3-us-west-2.amazonaws.com/"
        "streamlit-demo-data/uber-raw-data-sep14.csv.gz"
    )

    @st.cache_data
    def load_data(nrows):
        data = pd.read_csv(DATA_URL, nrows=nrows)

        def lowercase(x):
            return str(x).lower()

        data.rename(lowercase, axis="columns", inplace=True)
        data[DATE_COLUMN] = pd.to_datetime(data[DATE_COLUMN])
        return data

    data_load_state = st.text("Loading data...")
    data = load_data(10000)
    data_load_state.text("Done! (using st.cache_data)")

    if st.checkbox("Show raw data"):
        st.subheader("Raw data")
        st.write(data)

    st.subheader("Number of pickups by hour")
    hist_values = np.histogram(data[DATE_COLUMN].dt.hour, bins=24, range=(0, 24))[0]
    st.bar_chart(hist_values)

    # Some number in the range 0-23
    hour_to_filter = st.slider("hour", 0, 23, 17)
    filtered_data = data[data[DATE_COLUMN].dt.hour == hour_to_filter]

    st.subheader("Map of all pickups at %s:00" % hour_to_filter)
    st.map(filtered_data)


if __name__ == "__main__":
    main()


--- 11_notebooks/basic.ipynb ---
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade modal\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "\n",
    "assert modal.__version__ > \"0.49.0\"\n",
    "modal.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = modal.App(name=\"example-basic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling standard Python functions\n",
    "\n",
    "Standard Python functions can of course be defined in a notebook and used on their own or be called within Modal functions.\n",
    "Below the `double` function is defined in pure-Python, and called once locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double(x: int) -> int:\n",
    "    return x + x\n",
    "\n",
    "\n",
    "double(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Modal Functions\n",
    "\n",
    "If we wanted to run this trivial doubling function *in the cloud* we can write another function `double_with_modal` and decorate it with `@app.function` to register\n",
    "the function with the Modal app.\n",
    "\n",
    "To demonstrate that Modal functions you define in the notebook can be called by _other_ Modal functions, there's another function, `quadruple`, which uses `double` and `double_with_modal`.\n",
    "For numbers greater than 1 million, this function spins up containers that run in Modal, which is a _very_ inefficient way to multiply a number by four, but you can do it if you please!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.function()\n",
    "def double_with_modal(x: int) -> int:\n",
    "    return x + x\n",
    "\n",
    "\n",
    "@app.function()\n",
    "def quadruple(x: int) -> int:\n",
    "    if x <= 1_000_000:\n",
    "        return double(x) + double(x)\n",
    "    else:\n",
    "        return double_with_modal.remote(x) + double_with_modal.remote(x)\n",
    "\n",
    "\n",
    "with app.run():\n",
    "    print(quadruple.local(100))  # running locally\n",
    "    print(quadruple.remote(100))  # run remotely\n",
    "    print(\"Doing a very inefficient remote multiplication just for fun!\")\n",
    "    result = quadruple.remote(10_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the result created in above cell\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU-powered notebook cells!\n",
    "\n",
    "Thanks to Modal's remote execution capabilities, your notebook can be running on your laptop or a cheap CPU-only instance and take advantage of serverless GPU container execution. Here's the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Modal function with a GPU attached.\n",
    "@app.function(gpu=\"any\")\n",
    "def hello_gpu():\n",
    "    import subprocess\n",
    "\n",
    "    subprocess.run(\"nvidia-smi\", shell=True, check=True)\n",
    "    return \"hello from a remote GPU!\"\n",
    "\n",
    "\n",
    "# Start and run an ephemeral modal.App and execute the GPU-powered modal Function!\n",
    "with app.run():\n",
    "    result = hello_gpu.remote()\n",
    "    assert result == \"hello from a remote GPU!\"\n",
    "\n",
    "# After the app is finished you can continue executing other function's defined in your notebook and\n",
    "# use the results of your GPU functions!\n",
    "\"This is the remote GPU's return value: \" + result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "41aa4f5b72d46326b95133582f60c55f8bcca2a8619d8a82d21027f6cbc11af9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}


--- 11_notebooks/jupyter_inside_modal.py ---
# ---
# args: ["--timeout", 10]
# ---

# ## Overview
#
# Quick snippet showing how to connect to a Jupyter notebook server running inside a Modal container,
# especially useful for exploring the contents of Modal Volumes.
# This uses [Modal Tunnels](https://modal.com/docs/guide/tunnels#tunnels-beta)
# to create a tunnel between the running Jupyter instance and the internet.
#
# If you want to your Jupyter notebook to run _locally_ and execute remote Modal Functions in certain cells, see the `basic.ipynb` example :)

import os
import subprocess
import time

import modal

app = modal.App(
    "example-jupyter-inside-modal",
    image=modal.Image.debian_slim(python_version="3.12").uv_pip_install(
        "jupyter", "bing-image-downloader~=1.1.2"
    ),
)
volume = modal.Volume.from_name(
    "modal-examples-jupyter-inside-modal-data", create_if_missing=True
)

CACHE_DIR = "/root/cache"
JUPYTER_TOKEN = "1234"  # Change me to something non-guessable!


@app.function(volumes={CACHE_DIR: volume})
def seed_volume():
    # Bing it!
    from bing_image_downloader import downloader

    # This will save into the Modal volume and allow you view the images
    # from within Jupyter at a path like `/root/cache/modal labs/Image_1.png`.
    downloader.download(
        query="modal labs",
        limit=10,
        output_dir=CACHE_DIR,
        force_replace=False,
        timeout=60,
        verbose=True,
    )
    volume.commit()


# This is all that's needed to create a long-lived Jupyter server process in Modal
# that you can access in your Browser through a secure network tunnel.
# This can be useful when you want to interactively engage with Volume contents
# without having to download it to your host computer.


@app.function(max_containers=1, volumes={CACHE_DIR: volume}, timeout=1_500)
def run_jupyter(timeout: int):
    jupyter_port = 8888
    with modal.forward(jupyter_port) as tunnel:
        jupyter_process = subprocess.Popen(
            [
                "jupyter",
                "notebook",
                "--no-browser",
                "--allow-root",
                "--ip=0.0.0.0",
                f"--port={jupyter_port}",
                "--NotebookApp.allow_origin='*'",
                "--NotebookApp.allow_remote_access=1",
            ],
            env={**os.environ, "JUPYTER_TOKEN": JUPYTER_TOKEN},
        )

        print(f"Jupyter available at => {tunnel.url}")

        try:
            end_time = time.time() + timeout
            while time.time() < end_time:
                time.sleep(5)
            print(f"Reached end of {timeout} second timeout period. Exiting...")
        except KeyboardInterrupt:
            print("Exiting...")
        finally:
            jupyter_process.kill()


@app.local_entrypoint()
def main(timeout: int = 10_000):
    # Write some images to a volume, for demonstration purposes.
    seed_volume.remote()
    # Run the Jupyter Notebook server
    run_jupyter.remote(timeout=timeout)


# Doing `modal run jupyter_inside_modal.py` will run a Modal app which starts
# the Juypter server at an address like https://u35iiiyqp5klbs.r3.modal.host.
# Visit this address in your browser, and enter the security token
# you set for `JUPYTER_TOKEN`.


## Links discovered
- [Modal Tunnels](https://modal.com/docs/guide/tunnels#tunnels-beta)

--- 12_datasets/coco.py ---
# ---
# lambda-test: false  # long-running
# ---
#
# This script demonstrates ingestion of the [COCO](https://cocodataset.org/#download) (Common Objects in Context)
# dataset.
#
# It is recommended to iterate on this code from a modal.Function running Jupyter server.
# This better supports experimentation and maintains state in the face of errors:
# 11_notebooks/jupyter_inside_modal.py
import os
import pathlib
import shutil
import subprocess
import sys
import threading
import time
import zipfile

import modal

bucket_creds = modal.Secret.from_name(
    "aws-s3-modal-examples-datasets", environment_name="main"
)
bucket_name = "modal-examples-datasets"
volume = modal.CloudBucketMount(
    bucket_name,
    secret=bucket_creds,
)
image = modal.Image.debian_slim().apt_install("wget").uv_pip_install("tqdm")
app = modal.App(
    "example-coco",
    image=image,
    secrets=[],
)


def start_monitoring_disk_space(interval: int = 120) -> None:
    """Start monitoring the disk space in a separate thread."""
    task_id = os.environ["MODAL_TASK_ID"]

    def log_disk_space(interval: int) -> None:
        while True:
            statvfs = os.statvfs("/")
            free_space = statvfs.f_frsize * statvfs.f_bavail
            print(
                f"{task_id} free disk space: {free_space / (1024**3):.2f} GB",
                file=sys.stderr,
            )
            time.sleep(interval)

    monitoring_thread = threading.Thread(target=log_disk_space, args=(interval,))
    monitoring_thread.daemon = True
    monitoring_thread.start()


def extractall(fzip, dest, desc="Extracting"):
    from tqdm.auto import tqdm
    from tqdm.utils import CallbackIOWrapper

    dest = pathlib.Path(dest).expanduser()
    with (
        zipfile.ZipFile(fzip) as zipf,
        tqdm(
            desc=desc,
            unit="B",
            unit_scale=True,
            unit_divisor=1024,
            total=sum(getattr(i, "file_size", 0) for i in zipf.infolist()),
        ) as pbar,
    ):
        for i in zipf.infolist():
            if not getattr(i, "file_size", 0):  # directory
                zipf.extract(i, os.fspath(dest))
            else:
                full_path = dest / i.filename
                full_path.parent.mkdir(exist_ok=True, parents=True)
                with zipf.open(i) as fi, open(full_path, "wb") as fo:
                    shutil.copyfileobj(CallbackIOWrapper(pbar.update, fi), fo)


def copy_concurrent(src: pathlib.Path, dest: pathlib.Path) -> None:
    from multiprocessing.pool import ThreadPool

    class MultithreadedCopier:
        def __init__(self, max_threads):
            self.pool = ThreadPool(max_threads)

        def copy(self, source, dest):
            self.pool.apply_async(shutil.copy2, args=(source, dest))

        def __enter__(self):
            return self

        def __exit__(self, exc_type, exc_val, exc_tb):
            self.pool.close()
            self.pool.join()

    with MultithreadedCopier(max_threads=48) as copier:
        shutil.copytree(src, dest, copy_function=copier.copy, dirs_exist_ok=True)


# This script uses wget to download ZIP files over HTTP because while the official
# website recommends using gsutil to download from a bucket (https://cocodataset.org/#download)
# that bucket no longer exists.


@app.function(
    volumes={"/vol/": volume},
    timeout=60 * 60 * 5,  # 5 hours
    ephemeral_disk=600 * 1024,  # 600 GiB,
)
def _do_part(url: str) -> None:
    start_monitoring_disk_space()
    part = url.replace("http://images.cocodataset.org/", "")
    name = pathlib.Path(part).name.replace(".zip", "")
    zip_path = pathlib.Path("/tmp/") / pathlib.Path(part).name
    extract_tmp_path = pathlib.Path("/tmp", name)
    dest_path = pathlib.Path("/vol/coco/", name)

    print(f"Downloading {name} from {url}")
    command = f"wget {url} -O {zip_path}"
    subprocess.run(command, shell=True, check=True)
    print(f"Download of {name} completed successfully.")
    extract_tmp_path.mkdir()
    extractall(
        zip_path, extract_tmp_path, desc=f"Extracting {name}"
    )  # extract into /tmp/
    zip_path.unlink()  # free up disk space by deleting the zip
    print(f"Copying extract {name} data to volume.")
    copy_concurrent(extract_tmp_path, dest_path)  # copy from /tmp/ into mounted volume


# We can process each part of the dataset in parallel, using a 'parent' Function just to execute
# the map and wait on completion of all children.


@app.function(
    timeout=60 * 60 * 5,  # 5 hours
)
def import_transform_load() -> None:
    print("Starting import, transform, and load of COCO dataset")
    list(
        _do_part.map(
            [
                "http://images.cocodataset.org/zips/train2017.zip",
                "http://images.cocodataset.org/zips/val2017.zip",
                "http://images.cocodataset.org/zips/test2017.zip",
                "http://images.cocodataset.org/zips/unlabeled2017.zip",
                "http://images.cocodataset.org/annotations/annotations_trainval2017.zip",
                "http://images.cocodataset.org/annotations/stuff_annotations_trainval2017.zip",
                "http://images.cocodataset.org/annotations/image_info_test2017.zip",
                "http://images.cocodataset.org/annotations/image_info_unlabeled2017.zip",
            ]
        )
    )
    print("âœ… Done")


## Links discovered
- [COCO](https://cocodataset.org/#download)

--- 12_datasets/imagenet.py ---
# ---
# lambda-test: false  # long-running
# ---
#
# This scripts demonstrates how to ingest the famous ImageNet (https://www.image-net.org/)
# dataset into a mounted volume.
#
# It requires a Kaggle account's API token stored as a modal.Secret in order to download part
# of the dataset from Kaggle's servers using the `kaggle` CLI.
#
# It is recommended to iterate on this code from a modal.Function running Jupyter server.
# This better supports experimentation and maintains state in the face of errors:
# 11_notebooks/jupyter_inside_modal.py
import os
import pathlib
import shutil
import subprocess
import sys
import threading
import time
import zipfile

import modal

bucket_creds = modal.Secret.from_name(
    "aws-s3-modal-examples-datasets", environment_name="main"
)
bucket_name = "modal-examples-datasets"
volume = modal.CloudBucketMount(
    bucket_name,
    secret=bucket_creds,
)
image = modal.Image.debian_slim().apt_install("tree").uv_pip_install("kaggle", "tqdm")
app = modal.App(
    "example-imagenet",
    image=image,
    secrets=[modal.Secret.from_name("kaggle-api-token")],
)


def start_monitoring_disk_space(interval: int = 30) -> None:
    """Start monitoring the disk space in a separate thread."""
    task_id = os.environ["MODAL_TASK_ID"]

    def log_disk_space(interval: int) -> None:
        while True:
            statvfs = os.statvfs("/")
            free_space = statvfs.f_frsize * statvfs.f_bavail
            print(
                f"{task_id} free disk space: {free_space / (1024**3):.2f} GB",
                file=sys.stderr,
            )
            time.sleep(interval)

    monitoring_thread = threading.Thread(target=log_disk_space, args=(interval,))
    monitoring_thread.daemon = True
    monitoring_thread.start()


def copy_concurrent(src: pathlib.Path, dest: pathlib.Path) -> None:
    """
    A modified shutil.copytree which copies in parallel to increase bandwidth
    and compensate for the increased IO latency of volume mounts.
    """
    from multiprocessing.pool import ThreadPool

    class MultithreadedCopier:
        def __init__(self, max_threads):
            self.pool = ThreadPool(max_threads)
            self.copy_jobs = []

        def copy(self, source, dest):
            res = self.pool.apply_async(
                shutil.copy2,
                args=(source, dest),
                callback=lambda r: print(f"{source} copied to {dest}"),
                # NOTE: this should `raise` an exception for proper reliability.
                error_callback=lambda exc: print(
                    f"{source} failed: {exc}", file=sys.stderr
                ),
            )
            self.copy_jobs.append(res)

        def __enter__(self):
            return self

        def __exit__(self, exc_type, exc_val, exc_tb):
            self.pool.close()
            self.pool.join()

    with MultithreadedCopier(max_threads=24) as copier:
        shutil.copytree(src, dest, copy_function=copier.copy, dirs_exist_ok=True)


def extractall(fzip, dest, desc="Extracting"):
    from tqdm.auto import tqdm
    from tqdm.utils import CallbackIOWrapper

    dest = pathlib.Path(dest).expanduser()
    with (
        zipfile.ZipFile(fzip) as zipf,
        tqdm(
            desc=desc,
            unit="B",
            unit_scale=True,
            unit_divisor=1024,
            total=sum(getattr(i, "file_size", 0) for i in zipf.infolist()),
        ) as pbar,
    ):
        for i in zipf.infolist():
            if not getattr(i, "file_size", 0):  # directory
                zipf.extract(i, os.fspath(dest))
            else:
                full_path = dest / i.filename
                full_path.parent.mkdir(exist_ok=True, parents=True)
                with zipf.open(i) as fi, open(full_path, "wb") as fo:
                    shutil.copyfileobj(CallbackIOWrapper(pbar.update, fi), fo)


@app.function(
    volumes={"/mnt/": volume},
    timeout=60 * 60 * 8,  # 8 hours,
    ephemeral_disk=1000 * 1024,  # 1TB
)
def import_transform_load() -> None:
    start_monitoring_disk_space()
    kaggle_api_token_data = os.environ["KAGGLE_API_TOKEN"]
    kaggle_token_filepath = pathlib.Path.home() / ".kaggle" / "kaggle.json"
    kaggle_token_filepath.parent.mkdir(exist_ok=True)
    kaggle_token_filepath.write_text(kaggle_api_token_data)

    tmp_path = pathlib.Path("/tmp/imagenet/")
    vol_path = pathlib.Path("/mnt/imagenet/")
    filename = "imagenet-object-localization-challenge.zip"
    dataset_path = vol_path / filename
    if dataset_path.exists():
        dataset_size = dataset_path.stat().st_size
        if dataset_size < (150 * 1024 * 1024 * 1024):
            dataset_size_gib = dataset_size / (1024 * 1024 * 1024)
            raise RuntimeError(
                f"Partial download of dataset .zip. It is {dataset_size_gib}GiB but should be > 150GiB"
            )
    else:
        subprocess.run(
            f"kaggle competitions download -c imagenet-object-localization-challenge --path {tmp_path}",
            shell=True,
            check=True,
        )
        vol_path.mkdir(exist_ok=True)
        shutil.copy(tmp_path / filename, dataset_path)

    # Extract dataset
    extracted_dataset_path = tmp_path / "extracted"
    extracted_dataset_path.mkdir(parents=True, exist_ok=True)
    print(f"Extracting .zip into {extracted_dataset_path}...")
    extractall(dataset_path, extracted_dataset_path)
    print(f"Extracted {dataset_path} to {extracted_dataset_path}")
    subprocess.run(f"tree -L 3 {extracted_dataset_path}", shell=True, check=True)

    final_dataset_path = vol_path / "extracted"
    final_dataset_path.mkdir(exist_ok=True)
    copy_concurrent(extracted_dataset_path, final_dataset_path)
    subprocess.run(f"tree -L 3 {final_dataset_path}", shell=True, check=True)
    print("Dataset is loaded âœ…")


--- 12_datasets/laion400.py ---
# ---
# lambda-test: false  # long-running
# ---
#
# https://laion.ai/blog/laion-400-open-dataset/
#
# LAION-400 is a large dataset of 400M English (image, text) pairs.
#
# As described on the dataset's homepage, it consists of 32 .parquet files
# containing dataset metadata *but not* the image data itself.
#
# After downloading the .parquet files, this script fans out 32 worker jobs
# to process a single .parquet file. Processing involves fetch and transform
# of image data into 256 * 256 square JPEGs.
#
# This script is loosely based off the following instructions:
# https://github.com/rom1504/img2dataset/blob/main/dataset_examples/laion400m.md
#
# It is recommended to iterate on this code from a modal.Function running Jupyter server.
# This better supports experimentation and maintains state in the face of errors:
# 11_notebooks/jupyter_inside_modal.py
import os
import pathlib
import shutil
import subprocess
import sys
import threading
import time

import modal

bucket_creds = modal.Secret.from_name(
    "aws-s3-modal-examples-datasets", environment_name="main"
)

bucket_name = "modal-examples-datasets"

volume = modal.CloudBucketMount(
    bucket_name,
    secret=bucket_creds,
)

image = (
    modal.Image.debian_slim().apt_install("wget").uv_pip_install("img2dataset~=1.45.0")
)

app = modal.App("example-laion400", image=image)


def start_monitoring_disk_space(interval: int = 30) -> None:
    """Start monitoring the disk space in a separate thread, printing info to stdout"""
    task_id = os.environ["MODAL_TASK_ID"]

    def log_disk_space(interval: int) -> None:
        while True:
            statvfs = os.statvfs("/")
            free_space = statvfs.f_frsize * statvfs.f_bavail
            print(
                f"{task_id} free disk space: {free_space / (1024**3):.2f} GB",
                file=sys.stderr,
            )
            time.sleep(interval)

    monitoring_thread = threading.Thread(target=log_disk_space, args=(interval,))
    monitoring_thread.daemon = True
    monitoring_thread.start()


def copy_concurrent(src: pathlib.Path, dest: pathlib.Path) -> None:
    """
    A modified shutil.copytree which copies in parallel to increase bandwidth
    and compensate for the increased IO latency of volume mounts.
    """
    from multiprocessing.pool import ThreadPool

    class MultithreadedCopier:
        def __init__(self, max_threads):
            self.pool = ThreadPool(max_threads)
            self.copy_jobs = []

        def copy(self, source, dest):
            res = self.pool.apply_async(
                shutil.copy2,
                args=(source, dest),
                callback=lambda r: print(f"{source} copied to {dest}"),
                # NOTE: this should `raise` an exception for proper reliability.
                error_callback=lambda exc: print(
                    f"{source} failed: {exc}", file=sys.stderr
                ),
            )
            self.copy_jobs.append(res)

        def __enter__(self):
            return self

        def __exit__(self, exc_type, exc_val, exc_tb):
            self.pool.close()
            self.pool.join()

    with MultithreadedCopier(max_threads=24) as copier:
        shutil.copytree(src, dest, copy_function=copier.copy, dirs_exist_ok=True)


@app.function(
    volumes={"/mnt": volume},
    # 20 hours â€” img2dataset is extremely slow to work through all images.
    timeout=60 * 60 * 20,
    ephemeral_disk=512 * 1024,
)
def run_img2dataset_on_part(
    i: int,
    partfile: str,
) -> None:
    start_monitoring_disk_space(interval=60)
    while not pathlib.Path(partfile).exists():
        print(f"{partfile} not yet visible...", file=sys.stderr)
        time.sleep(1)
    # Each part works in its own subdirectory because img2dataset creates a working
    # tmpdir at <output_folder>/_tmp and we don't want consistency issues caused by
    # all concurrently processing parts read/writing from the same temp directory.
    tmp_laion400m_data_path = pathlib.Path(f"/tmp/laion400/laion400m-data/{i}/")
    tmp_laion400m_data_path.mkdir(exist_ok=True, parents=True)
    # Increasing retries comes at a *large* performance cost.
    retries = 0
    # TODO: Support --incremental mode. https://github.com/rom1504/img2dataset?tab=readme-ov-file#incremental-mode
    command = (
        f'img2dataset --url_list {partfile} --input_format "parquet" '
        '--url_col "URL" --caption_col "TEXT" --output_format webdataset '
        f"--output_folder {tmp_laion400m_data_path} --processes_count 16 --thread_count 128 --image_size 256 "
        f'--retries={retries} --save_additional_columns \'["NSFW","similarity","LICENSE"]\' --enable_wandb False'
    )
    print(f"Running img2dataset command: \n\n{command}")
    subprocess.run(command, shell=True, check=True)
    print("Completed img2dataset, copying into mounted volume...")
    laion400m_data_path = pathlib.Path("/mnt/laion400/laion400m-data/")
    copy_concurrent(tmp_laion400m_data_path, laion400m_data_path)


@app.function(
    volumes={"/mnt": volume},
    timeout=60 * 60 * 16,  # 16 hours
)
def import_transform_load() -> None:
    start_monitoring_disk_space()
    # We initially download into a tmp directory outside of the volume to avoid
    # any filesystem incompatibilities between the `wget` application and the bucket
    # filesystem mount.
    tmp_laion400m_meta_path = pathlib.Path("/tmp/laion400/laion400m-meta")
    laion400m_meta_path = pathlib.Path("/mnt/laion400/laion400m-meta")
    if not laion400m_meta_path.exists():
        laion400m_meta_path.mkdir(parents=True, exist_ok=True)
        # WARNING: We skip the certificate check for the-eye.eu because its TLS certificate expired as of mid-May 2024.
        subprocess.run(
            f"wget -l1 -r --no-check-certificate --no-parent https://the-eye.eu/public/AI/cah/laion400m-met-release/laion400m-meta/ -P {tmp_laion400m_meta_path}",
            shell=True,
            check=True,
        )

        parquet_files = list(tmp_laion400m_meta_path.glob("**/*.parquet"))
        print(
            f"Downloaded {len(parquet_files)} parquet files into {tmp_laion400m_meta_path}."
        )
        # Perform a simple copy operation to move the data into the bucket.
        copy_concurrent(tmp_laion400m_meta_path, laion400m_meta_path)

    parquet_files = list(laion400m_meta_path.glob("**/*.parquet"))
    print(f"Stored {len(parquet_files)} parquet files into {laion400m_meta_path}.")
    print(f"Spawning {len(parquet_files)} to enrich dataset...")
    list(run_img2dataset_on_part.starmap((i, f) for i, f in enumerate(parquet_files)))


--- 12_datasets/rosettafold.py ---
# ---
# lambda-test: false  # long-running
# ---
#
# This script demonstrated how to ingest the https://github.com/RosettaCommons/RoseTTAFold protein-folding
# model's dataset into a mounted volume.

# The dataset is over 2 TiB when decompressed to the runtime of this script is quite long.
# ref: https://github.com/RosettaCommons/RoseTTAFold/issues/132.
#
# It is recommended to iterate on this code from a modal.Function running Jupyter server.
# This better supports experimentation and maintains state in the face of errors:
# 11_notebooks/jupyter_inside_modal.py
import os
import pathlib
import shutil
import subprocess
import sys
import tarfile
import threading
import time

import modal

bucket_creds = modal.Secret.from_name(
    "aws-s3-modal-examples-datasets", environment_name="main"
)
bucket_name = "modal-examples-datasets"
volume = modal.CloudBucketMount(
    bucket_name,
    secret=bucket_creds,
)
image = modal.Image.debian_slim().apt_install("wget")
app = modal.App("example-rosettafold", image=image)


def start_monitoring_disk_space(interval: int = 30) -> None:
    """Start monitoring the disk space in a separate thread."""
    task_id = os.environ["MODAL_TASK_ID"]

    def log_disk_space(interval: int) -> None:
        while True:
            statvfs = os.statvfs("/")
            free_space = statvfs.f_frsize * statvfs.f_bavail
            print(
                f"{task_id} free disk space: {free_space / (1024**3):.2f} GB",
                file=sys.stderr,
            )
            time.sleep(interval)

    monitoring_thread = threading.Thread(target=log_disk_space, args=(interval,))
    monitoring_thread.daemon = True
    monitoring_thread.start()


def decompress_tar_gz(file_path: pathlib.Path, extract_dir: pathlib.Path) -> None:
    print(f"Decompressing {file_path} into {extract_dir}...")
    with tarfile.open(file_path, "r:gz") as tar:
        tar.extractall(path=extract_dir)
        print(f"Decompressed {file_path} to {extract_dir}")


def copy_concurrent(src: pathlib.Path, dest: pathlib.Path) -> None:
    """
    A modified shutil.copytree which copies in parallel to increase bandwidth
    and compensate for the increased IO latency of volume mounts.
    """
    from multiprocessing.pool import ThreadPool

    class MultithreadedCopier:
        def __init__(self, max_threads):
            self.pool = ThreadPool(max_threads)
            self.copy_jobs = []

        def copy(self, source, dest):
            res = self.pool.apply_async(
                shutil.copy2,
                args=(source, dest),
                callback=lambda r: print(f"{source} copied to {dest}"),
                # NOTE: this should `raise` an exception for proper reliability.
                error_callback=lambda exc: print(
                    f"{source} failed: {exc}", file=sys.stderr
                ),
            )
            self.copy_jobs.append(res)

        def __enter__(self):
            return self

        def __exit__(self, exc_type, exc_val, exc_tb):
            self.pool.close()
            self.pool.join()

    with MultithreadedCopier(max_threads=24) as copier:
        shutil.copytree(src, dest, copy_function=copier.copy, dirs_exist_ok=True)


@app.function(
    volumes={"/mnt/": volume},
    timeout=60 * 60 * 24,
    ephemeral_disk=2560 * 1024,
)
def _do_part(url: str) -> None:
    name = url.split("/")[-1].replace(".tar.gz", "")
    print(f"Downloading {name}")
    compressed = pathlib.Path("/tmp", name)
    cmd = f"wget {url} -O {compressed}"
    p = subprocess.Popen(cmd, shell=True)
    returncode = p.wait()
    if returncode != 0:
        raise RuntimeError(f"Error in downloading. {p.args!r} failed {returncode=}")
    decompressed = pathlib.Path("/tmp/rosettafold/", name)

    # Decompression is much faster against the container's local SSD disk
    # compared with against the mounted volume. So we first compress into /tmp/.
    print(f"Decompressing {compressed} into {decompressed}.")
    decompress_tar_gz(compressed, decompressed)
    print(
        f"âœ… Decompressed {compressed} into {decompressed}. Now deleting it to free up disk.."
    )
    compressed.unlink()  # delete compressed file to free up disk

    # Finally, we move the decompressed data from /tmp/ into the mounted volume.
    # There are a large mount of files to copy so this step takes a while.
    dest = pathlib.Path("/mnt/rosettafold/")
    copy_concurrent(decompressed, dest)
    shutil.rmtree(decompressed, ignore_errors=True)  # free up disk
    print(f"Dataset part {url} is loaded âœ…")


@app.function(
    volumes={"/mnt/": volume},
    # Timeout for this Function is set at the maximum, 24 hours,
    # because downloading, decompressing and storing almost 2 TiB of
    # files takes a long time.
    timeout=60 * 60 * 24,
)
def import_transform_load() -> None:
    # NOTE:
    # The mmseq.com server upload speed is quite slow so this download takes a while.
    # The download speed is also quite variable, sometimes taking over 5 hours.
    list(
        _do_part.map(
            [
                "http://wwwuser.gwdg.de/~compbiol/uniclust/2020_06/UniRef30_2020_06_hhsuite.tar.gz",
                "https://bfd.mmseqs.com/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt.tar.gz",
                "https://files.ipd.uw.edu/pub/RoseTTAFold/pdb100_2021Mar03.tar.gz",
            ]
        )
    )
    print("Dataset is loaded âœ…")


--- 13_sandboxes/codelangchain/README.md ---
# Deploying code agents without all the agonizing pain

This example deploys a "code agent": a language model that can write and execute
code in a flexible control flow aimed at completing a task or goal.

It is implemented in LangChain, using the LangGraph library to structure the
agent and the LangServe framework to turn it into a FastAPI app.

We use Modal to turn that app into a web endpoint. We also use Modal to
"sandbox" the agent's code execution, so that it can't accidentally (or when
prompt injected!) damage the application by executing some inadvisable code.

Modal's Charles Frye and LangChain's Lance Martin did a
[walkthrough webinar](https://www.youtube.com/watch?v=X3yzWtAkaeo) explaining
the project's context and implementation. Check it out if you're curious!

## How to run

To run this app, you need to `pip install modal` and then create the following
[secrets](https://modal.com/docs/guide/secrets):

- `openai-secret` with an OpenAI API key, so that we can query OpenAI's models
  to power the agent,
- and `langsmith-secret` with a LangSmith API key, so that we can monitor the
  agent's behavior with LangSmith.

Head to the [secret creation dashboard](https://modal.com/secrets/) and follow
the instructions for each secret type.

Then, you can deploy the app with:

```bash
modal deploy codelangchain.py
```

Navigate to the URL that appears in the output and you'll be dropped into an
interactive "playground" interface where you can send queries to the agent and
receive responses. You should expect it to take about a minute to respond.

You can also navigate to the `/docs` path to see OpenAPI/Swagger docs, for
everything you'd need to see how to incorporate the agent into your downstream
applications via API requests.

When developing the app, use `modal serve codelangchain.py` to get a
hot-reloading server.

## Repo structure

The web application is defined in `codelangchain.py`.

It wraps the `agent.py` module, which contains the LangChain agent's definition.
To test the agent in isolation, run `modal run agent.py` in the terminal and
provide a `--question` about Python programming as input.

Because the agent is a graph, it is defined by specifying nodes and edges, which
are found in `nodes.py` and `edges.py`, respectively.

The retrieval logic is very simple: all of the data from the relevant docs is
retrieved and put at the beginning of the language model's prompt. You can find
it in `retrieval.py`.

The definition of the Modal container images and a few other shared utilities
can be found in `common.py`.


## Links discovered
- [walkthrough webinar](https://www.youtube.com/watch?v=X3yzWtAkaeo)
- [secrets](https://modal.com/docs/guide/secrets)
- [secret creation dashboard](https://modal.com/secrets/)

--- 13_sandboxes/anthropic_computer_use.py ---
# ---
# cmd: ["python", "13_sandboxes/anthropic_computer_use.py"]
# pytest: false
# ---

# # Run Anthropic's computer use demo in a Modal Sandbox

# This example demonstrates how to run Anthropic's [Computer Use demo](https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo)
# in a Modal [Sandbox](https://modal.com/docs/guide/sandbox).

# ## Sandbox Setup

# All Sandboxes are associated with an App.

# We start by looking up an existing App by name, or creating one if it doesn't exist.

import time
import urllib.request

import modal
import modal.experimental

app = modal.App.lookup("example-anthropic-computer-use", create_if_missing=True)

# The Computer Use [quickstart](https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo)
# provides a prebuilt Docker image. We use this hosted image to create our sandbox environment.

sandbox_image = (
    modal.experimental.raw_registry_image(
        "ghcr.io/anthropics/anthropic-quickstarts:computer-use-demo-latest",
    )
    .env({"WIDTH": "1920", "HEIGHT": "1080"})
    .workdir("/home/computeruse")
    .entrypoint([])
)

# We'll provide the Anthropic API key via a Modal [Secret](https://modal.com/docs/guide/secrets)
# which the sandbox can access at runtime.

secret = modal.Secret.from_name("anthropic-secret", required_keys=["ANTHROPIC_API_KEY"])

# Now, we can start our Sandbox.
# We use `modal.enable_output()` to print the Sandbox's image build logs to the console.
# We'll also expose the ports required for the demo's interfaces:

# - Port 8501 serves the Streamlit UI for interacting with the agent loop
# - Port 6080 serves the VNC desktop view via a browser-based noVNC client

with modal.enable_output():
    sandbox = modal.Sandbox.create(
        "sudo",
        "--preserve-env=ANTHROPIC_API_KEY,DISPLAY_NUM,WIDTH,HEIGHT,PATH",
        "-u",
        "computeruse",
        "./entrypoint.sh",
        app=app,
        image=sandbox_image,
        secrets=[secret],
        encrypted_ports=[8501, 6080],
        timeout=60 * 60,  # stay alive for one hour, maximum one day
    )

print(f"ðŸ–ï¸  Sandbox ID: {sandbox.object_id}")

# After starting the sandbox, we retrieve the public URLs for the exposed ports.

tunnels = sandbox.tunnels()
for port, tunnel in tunnels.items():
    print(f"Waiting for service on port {port} to start at {tunnel.url}")

# We can check on each server's status by making an HTTP request to the server's URL
# and verifying that it responds with a 200 status code.


def is_server_up(url):
    try:
        response = urllib.request.urlopen(url)
        return response.getcode() == 200
    except Exception:
        return False


timeout = 60  # seconds
start_time = time.time()
up_ports = set()
while time.time() - start_time < timeout:
    for port, tunnel in tunnels.items():
        if port not in up_ports and is_server_up(tunnel.url):
            print(f"ðŸ–ï¸  Server is up and running on port {port}!")
            up_ports.add(port)
    if len(up_ports) == len(tunnels):
        break
    time.sleep(1)
else:
    print("ðŸ–ï¸  Timed out waiting for server to start.")


# You can now open the URLs in your browser to interact with the demo!
# Note: The sandbox logs may mention `localhost:8080`.
# Ignore this and use the printed tunnel URLs instead.

# When finished, you can terminate the sandbox from your [Modal dashboard](https://modal.com/containers)
# or by running `Sandbox.from_id(sandbox.object_id).terminate()`.
# The Sandbox will also spin down after one hour.


## Links discovered
- [Computer Use demo](https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo)
- [Sandbox](https://modal.com/docs/guide/sandbox)
- [quickstart](https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo)
- [Secret](https://modal.com/docs/guide/secrets)
- [Modal dashboard](https://modal.com/containers)

--- 13_sandboxes/jupyter_sandbox.py ---
# ---
# cmd: ["python", "13_sandboxes/jupyter_sandbox.py"]
# pytest: false
# ---

# # Run a Jupyter notebook in a Modal Sandbox

# This example demonstrates how to run a Jupyter notebook in a Modal
# [Sandbox](https://modal.com/docs/guide/sandbox).

# ## Setting up the Sandbox

# All Sandboxes are associated with an App.

# We look up our app by name, creating it if it doesn't exist.

import json
import secrets
import time
import urllib.request

import modal

app = modal.App.lookup("example-jupyter-sandbox", create_if_missing=True)

# We define a custom Docker image that has Jupyter and some other dependencies installed.
# Using a pre-defined image allows us to avoid re-installing packages on every Sandbox startup.

image = (
    modal.Image.debian_slim(python_version="3.12").uv_pip_install("jupyter~=1.1.0")
    # .uv_pip_install("pandas", "numpy", "seaborn")  # Any other deps
)

# ## Starting a Jupyter server in a Sandbox

# Since we'll be exposing a Jupyter server over the Internet, we need to create a password.
# We'll use `secrets` from the standard library to create a token
# and then store it in a Modal [Secret](https://modal.com/docs/guide/secrets).

token = secrets.token_urlsafe(13)
token_secret = modal.Secret.from_dict({"JUPYTER_TOKEN": token})

# Now, we can start our Sandbox. Note our use of the `encrypted_ports` argument, which
# allows us to securely expose the Jupyter server to the public Internet. We use
# `modal.enable_output()` to print the Sandbox's image build logs to the console.

JUPYTER_PORT = 8888

print("ðŸ–ï¸  Creating sandbox")

with modal.enable_output():
    sandbox = modal.Sandbox.create(
        "jupyter",
        "notebook",
        "--no-browser",
        "--allow-root",
        "--ip=0.0.0.0",
        f"--port={JUPYTER_PORT}",
        "--NotebookApp.allow_origin='*'",
        "--NotebookApp.allow_remote_access=1",
        encrypted_ports=[JUPYTER_PORT],
        secrets=[token_secret],
        timeout=5 * 60,  # 5 minutes
        image=image,
        app=app,
        gpu=None,  # add a GPU if you need it!
    )

print(f"ðŸ–ï¸  Sandbox ID: {sandbox.object_id}")

# ## Communicating with a Jupyter server

# Next, we print out a URL that we can use to connect to our Jupyter server.
# Note that we have to call [`Sandbox.tunnels`](https://modal.com/docs/reference/modal.Sandbox#tunnels)
# to get the URL. The Sandbox is not publicly accessible until we do so.

tunnel = sandbox.tunnels()[JUPYTER_PORT]
url = f"{tunnel.url}/?token={token}"
print(f"ðŸ–ï¸  Jupyter notebook is running at: {url}")

# Jupyter servers expose a [REST API](https://jupyter-server.readthedocs.io/en/latest/developers/rest-api.html)
# that you can use for programmatic manipulation.

# For example, we can check the server's status by
# sending a GET request to the `/api/status` endpoint.


def is_jupyter_up():
    try:
        response = urllib.request.urlopen(f"{tunnel.url}/api/status?token={token}")
        if response.getcode() == 200:
            data = json.loads(response.read().decode())
            return data.get("started", False)
    except Exception:
        return False
    return False


# We'll now wait for the Jupyter server to be ready by hitting that endpoint.

timeout = 60  # seconds
start_time = time.time()
while time.time() - start_time < timeout:
    if is_jupyter_up():
        print("ðŸ–ï¸  Jupyter is up and running!")
        break
    time.sleep(1)
else:
    print("ðŸ–ï¸  Timed out waiting for Jupyter to start.")


# You can now open this URL in your browser to access the Jupyter notebook!

# When you're done, terminate the sandbox using your [Modal dashboard](https://modal.com/sandboxes)
# or by running `Sandbox.from_id(sandbox.object_id).terminate()`.


## Links discovered
- [Sandbox](https://modal.com/docs/guide/sandbox)
- [Secret](https://modal.com/docs/guide/secrets)
- [`Sandbox.tunnels`](https://modal.com/docs/reference/modal.Sandbox#tunnels)
- [REST API](https://jupyter-server.readthedocs.io/en/latest/developers/rest-api.html)
- [Modal dashboard](https://modal.com/sandboxes)

--- 13_sandboxes/opencode_server.py ---
# ---
# cmd: ["python", "13_sandboxes/opencode_server.py"]
# pytest: false
# ---

# # Run OpenCode in a Modal Sandbox

# This example demonstrates how to run [OpenCode](https://opencode.ai/docs/)
# remotely and connect to it from your local terminal or browser.

# Combine self-hosted OpenCode with [serving a big, smart model](https://modal.com/docs/examples/very_large_models)
# on Modal and you've got "coding agents at home"!

# Coding agents are more useful when they have more context and more tools,
# so this example also demonstrates some patterns for passing local data and setting up OpenCode.
# Here, we pass in [this Modal examples repository](https://github.com/modal-labs/modal-examples)
# and give the agent the ability to run and debug the examples -- including this one! Meta.

# ![A screenshot of the OpenCode Web UI showing this coding agent running its own code](https://modal-cdn.com/examples-opencode-server-webui.png)

# ## Set up OpenCode on Modal

import os
import secrets
from pathlib import Path

import modal

app = modal.App.lookup("example-opencode-server", create_if_missing=True)
here = Path(__file__)

# First, we define a Modal container [Image](https://modal.com/docs/guide/images)
# with OpenCode installed.

image = (
    modal.Image.debian_slim()
    .apt_install("curl")
    .run_commands("curl -fsSL https://opencode.ai/install | bash")  # install opencode
    .env({"PATH": "/root/.opencode/bin:${PATH}"})  # post-installation step
)

# ## Add OpenCode configuration

# Next, we need to add the tools our agent needs to work with the code it's operating on.
# Examples in our repo should run with nothing more than `modal` installed --
# except for a few that use `fastapi`.

image = image.uv_pip_install("modal", "fastapi~=0.128.0")

# We bring the global default OpenCode configuration along for the ride.

CONFIG_PATH = (Path("~") / ".config" / "opencode" / "opencode.json").expanduser()
if CONFIG_PATH.exists():
    print("ðŸ–ï¸  Including config from", CONFIG_PATH)
    image = image.add_local_file(CONFIG_PATH, "/root/.config/opencode/")

# And, because we are developing code against Modal,
# we also grant our OpenCode agent our Modal permissions.

MODAL_PATH = (Path("~") / ".modal.toml").expanduser()
if not MODAL_PATH.exists():
    modal_token_id = os.environ.get("MODAL_TOKEN_ID")
    modal_token_secret = os.environ.get("MODAL_TOKEN_SECRET")
    if modal_token_id is None or modal_token_secret is None:
        raise FileNotFoundError(
            "Modal configuration file not found. Make sure you set up Modal with `modal setup` first!"
        )
    image = image.env(
        {"MODAL_TOKEN_ID": modal_token_id, "MODAL_TOKEN_SECRET": modal_token_secret}
    )
else:
    image = image.add_local_file(MODAL_PATH, "/root/.modal.toml")


# Finally, we copy over the code we want to work on.

repo_root = here.parent.parent
remote_repo_root = f"/root/{repo_root.name}"
image = image.add_local_dir(repo_root, remote_repo_root)

# Let's also secure the server. This code uses a temporary password
# generated with the `secrets` library from the Python stdlib.
# We create an ephemeral [Modal Secret](https://modal.com/docs/guide/secrets)
# to pass this to our Modal infrastructure.

password = secrets.token_urlsafe(13)
password_secret = modal.Secret.from_dict({"OPENCODE_SERVER_PASSWORD": password})

# ## Starting a Modal Sandbox with OpenCode in it

# Now, we create a [Modal Sandbox](https://modal.com/docs/guide/sandboxes)
# to run our coding agent session.
# This Sandbox has our environment Image and our password Secret.

# We open up the `OPENCODE_PORT` so that it can be accessed
# over the Internet.

print("ðŸ–ï¸  Creating sandbox")

MINUTES = 60  # seconds
HOURS = 60 * MINUTES
OPENCODE_PORT = 4096

with modal.enable_output():
    sandbox = modal.Sandbox.create(
        "opencode",
        "serve",
        "--hostname=0.0.0.0",
        f"--port={OPENCODE_PORT}",
        "--log-level=DEBUG",
        "--print-logs",
        encrypted_ports=[OPENCODE_PORT],
        secrets=[password_secret],
        timeout=12 * HOURS,
        image=image,
        app=app,
        workdir=remote_repo_root,
    )

# ## Talking to OpenCode running remotely on Modal

# OpenCode is truly open -- there are many interfaces to the underlying
# coding agent server, and it's even super easy to add your own.
# That's one reason why [Ramp chose OpenCode on Modal](https://builders.ramp.com/post/why-we-built-our-background-agent)
# to deploy their in-house background agent platform.

# The commands below will print the information you need to
# - directly access the underlying Modal Sandbox for debugging or "pair coding" with the agent
# - access the Web UI from a local browser (with authentication!)
# - acess the TUI from your local terminal

print("ðŸ–ï¸  Access the sandbox directly:", f"modal shell {sandbox.object_id}", sep="\n\t")

tunnel = sandbox.tunnels()[OPENCODE_PORT]
print(
    "ðŸ–ï¸  Access the WebUI",
    f"{tunnel.url}",
    "Username: opencode",
    f"Password: {password}",
    sep="\n\t",
)
print(
    "ðŸ–ï¸  Access the TUI:",
    f"OPENCODE_SERVER_PASSWORD={password} opencode attach {tunnel.url}",
    sep="\n\t",
)

# Try it yourself by running this code with

# ```
# python 13_sandboxes/opencode_server.py
# ```


## Links discovered
- [OpenCode](https://opencode.ai/docs/)
- [serving a big, smart model](https://modal.com/docs/examples/very_large_models)
- [this Modal examples repository](https://github.com/modal-labs/modal-examples)
- [A screenshot of the OpenCode Web UI showing this coding agent running its own code](https://modal-cdn.com/examples-opencode-server-webui.png)
- [Image](https://modal.com/docs/guide/images)
- [Modal Secret](https://modal.com/docs/guide/secrets)
- [Modal Sandbox](https://modal.com/docs/guide/sandboxes)
- [Ramp chose OpenCode on Modal](https://builders.ramp.com/post/why-we-built-our-background-agent)

--- 13_sandboxes/safe_code_execution.py ---
# ---
# cmd: ["python", "13_sandboxes/safe_code_execution.py"]
# pytest: false
# ---

# # Run arbitrary code in a sandboxed environment

# This example demonstrates how to run arbitrary code
# in multiple languages in a Modal [Sandbox](https://modal.com/docs/guide/sandbox).

# ## Setting up a multi-language environment

# Sandboxes allow us to run any kind of code in a safe environment.
# We'll use an image with a few different language runtimes to demonstrate this.

import modal

image = modal.Image.debian_slim(python_version="3.11").apt_install(
    "nodejs", "ruby", "php"
)
app = modal.App.lookup("example-safe-code-execution", create_if_missing=True)

# We'll now create a Sandbox with this image. We'll also enable output so we can see the image build
# logs. Note that we don't pass any commands to the Sandbox, so it will stay alive, waiting for us
# to send it commands.

with modal.enable_output():
    sandbox = modal.Sandbox.create(app=app, image=image)

print(f"Sandbox ID: {sandbox.object_id}")

# ## Running bash, Python, Node.js, Ruby, and PHP in a Sandbox

# We can now use [`Sandbox.exec`](https://modal.com/docs/reference/modal.Sandbox#exec) to run a few different
# commands in the Sandbox.

bash_ps = sandbox.exec("echo", "hello from bash")
python_ps = sandbox.exec("python", "-c", "print('hello from python')")
nodejs_ps = sandbox.exec("node", "-e", 'console.log("hello from nodejs")')
ruby_ps = sandbox.exec("ruby", "-e", "puts 'hello from ruby'")
php_ps = sandbox.exec("php", "-r", "echo 'hello from php';")

print(bash_ps.stdout.read(), end="")
print(python_ps.stdout.read(), end="")
print(nodejs_ps.stdout.read(), end="")
print(ruby_ps.stdout.read(), end="")
print(php_ps.stdout.read(), end="")
print()

# The output should look something like

# ```
# hello from bash
# hello from python
# hello from nodejs
# hello from ruby
# hello from php
# ```

# We can use multiple languages in tandem to build complex applications.
# Let's demonstrate this by piping data between Python and Node.js using bash. Here
# we generate some random numbers with Python and sum them with Node.js.

combined_process = sandbox.exec(
    "bash",
    "-c",
    """python -c 'import random; print(\" \".join(str(random.randint(1, 100)) for _ in range(10)))' |
    node -e 'const readline = require(\"readline\");
    const rl = readline.createInterface({input: process.stdin});
    rl.on(\"line\", (line) => {
      const sum = line.split(\" \").map(Number).reduce((a, b) => a + b, 0);
      console.log(`The sum of the random numbers is: ${sum}`);
      rl.close();
    });'""",
)

result = combined_process.stdout.read().strip()
print(result)

# For long-running processes, you can use stdout as an iterator to stream the output.

slow_printer = sandbox.exec(
    "ruby",
    "-e",
    """
    10.times do |i|
      puts "Line #{i + 1}: #{Time.now}"
      STDOUT.flush
      sleep(0.5)
    end
    """,
)

for line in slow_printer.stdout:
    print(line, end="")

# This should print something like

# ```
# Line 1: 2024-10-21 15:30:53 +0000
# Line 2: 2024-10-21 15:30:54 +0000
# ...
# Line 10: 2024-10-21 15:30:58 +0000
# ```

# Since Sandboxes are safely separated from the rest of our system,
# we can run very dangerous code in them!

sandbox.exec("rm", "-rfv", "/", "--no-preserve-root")

# This command has deleted the entire filesystem, so we can't run any more commands.
# Let's terminate the Sandbox to clean up after ourselves.

sandbox.terminate()


## Links discovered
- [Sandbox](https://modal.com/docs/guide/sandbox)
- [`Sandbox.exec`](https://modal.com/docs/reference/modal.Sandbox#exec)

--- 13_sandboxes/sandbox_agent.py ---
# ---
# cmd: ["python", "13_sandboxes/sandbox_agent.py"]
# pytest: false
# ---

# # Run Claude Code in a Modal Sandbox

# This example demonstrates how to run Claude Code in a Modal
# [Sandbox](https://modal.com/docs/guide/sandbox) to analyze a GitHub repository.
# The Sandbox provides an isolated environment where the agent can safely execute code
# and examine files.

import modal
from modal.container_process import ContainerProcess

app = modal.App.lookup("example-sandbox-agent", create_if_missing=True)

# First, we create a custom [Image](https://modal.com/docs/images) that has Claude Code
# and git installed.

image = (
    modal.Image.debian_slim(python_version="3.12")
    .apt_install("curl", "git")
    .env({"PATH": "/root/.local/bin:$PATH"})  # add claude to path
    .run_commands(
        "curl -fsSL https://claude.ai/install.sh | bash",
    )
)

# Then we create our Sandbox.

with modal.enable_output():
    sandbox = modal.Sandbox.create(app=app, image=image)
print(f"Sandbox ID: {sandbox.object_id}")

# Next we'll clone the repository that Claude Code will work on.
# We'll use [the Modal examples repo](https://github.com/modal-labs/modal-examples)
# that this example is a part of.

# We trigger the clone by [`exec`](https://modal.com/docs/reference/modal.Sandbox#exec)uting
# `git` as a process inside the Sandbox. We then `.wait` for it to finish.
# You can read more about the interface for managing
# `ContainerProcess`es in Sandboxes [here](https://modal.com/docs/reference/modal.container_process).

repo_url = "https://github.com/modal-labs/modal-examples"
git_ps: ContainerProcess = sandbox.exec(
    "git", "clone", "--depth", "1", repo_url, "/repo"
)
git_ps.wait()
print(f"Cloned '{repo_url}' into /repo.")

# Finally we'll use `exec` again to run Claude Code to analyze the repository.
# Here, we pass the `pty` flag to give the process a
# [pseudo-terminal](https://unix.stackexchange.com/questions/21147/what-are-pseudo-terminals-pty-tty).

claude_cmd = ["claude", "-p", "What is in this repository?"]

print("\nRunning command:", *claude_cmd)

claude_ps = sandbox.exec(
    *claude_cmd,
    pty=True,  # Adding a PTY is important, since Claude requires it
    secrets=[
        modal.Secret.from_name("anthropic-secret", required_keys=["ANTHROPIC_API_KEY"])
    ],
    workdir="/repo",
)
claude_ps.wait()

# Once the command finishes, we read the `stdout` and `stderr`.

print("\nAgent stdout:\n")
print(claude_ps.stdout.read())

stderr = claude_ps.stderr.read()
if stderr != "":
    print("Agent stderr:", stderr)


## Links discovered
- [Sandbox](https://modal.com/docs/guide/sandbox)
- [Image](https://modal.com/docs/images)
- [the Modal examples repo](https://github.com/modal-labs/modal-examples)
- [`exec`](https://modal.com/docs/reference/modal.Sandbox#exec)
- [here](https://modal.com/docs/reference/modal.container_process)
- [pseudo-terminal](https://unix.stackexchange.com/questions/21147/what-are-pseudo-terminals-pty-tty)

--- 13_sandboxes/sandbox_pool.py ---
# ---
# cmd: ["python", "13_sandboxes/sandbox_pool.py", "demo"]
# pytest: false
# ---

# # Maintain a pool of warm Sandboxes that are healthy and ready to serve requests
#
# This example demonstrates how to build a pool of "warm"
# [Modal Sandboxes](https://modal.com/docs/guide/sandbox), and deploy a
# [Modal web endpoint](https://modal.com/docs/guide/webhook-urls) that let's you claim
# a Sandbox from the pool, getting a URL to the server running in the Sandbox.
#
# Maintaining a pool of warm Sandboxes is useful for example if your Sandboxes need
# to do significant work after being created, like downloading code, installing
# dependencies, or running tests, before they are ready to serve requests.
#
# It uses a [Modal Queue](https://modal.com/docs/guide/dicts-and-queues#modal-queues)
# to store references to the warm Sandboxes, and functionality to maintain the pool
# by adding and removing Sandboxes, checking the current size, etc.
#
# The pool keeps track of the time to live for each Sandbox, and will always return
# a Sandbox with enough time left.
#
# It's structured into two Apps:
# - `example-sandbox-pool` is the main App that contains all the control logic for maintaining
#   the pool, exposing ways to claim Sandboxes, etc.
# - `example-sandbox-pool-sandboxes` houses all the actual Sandboxes, and nothing else.
#
# The implementation borrows from [pawalt](https://github.com/pawalt)'s [Sandbox pool
# example gist](https://gist.github.com/pawalt/7a505c38bba75cafae0780a5dd40e8b8). ðŸ™


import argparse
import time
from dataclasses import dataclass
from datetime import datetime

import modal

app = modal.App("example-sandbox-pool")

server_image = modal.Image.debian_slim(python_version="3.11").uv_pip_install(
    "fastapi[standard]~=0.115.14",
    "requests~=2.32.4",
)

## Configuration of the pool

# Here we define the image that will be used to run the server that runs in the
# Sandbox. In this simple example, we just run the built in Python HTTP server that
# returns a directory listing.
sandbox_image = modal.Image.debian_slim(python_version="3.11")
SANDBOX_SERVER_PORT = 8080
HEALTH_CHECK_TIMEOUT_SECONDS = 10

# In this example Sandboxes live for 5 minutes, and we assume that they are used for
# 2 minutes, meaning that if a Sandbox has less than 2 minutes left it's considered
# to be expiring too soon and will be terminated.
#
# You'll want to adjust these values depending on your use case.
SANDBOX_TIMEOUT_SECONDS = 5 * 60
SANDBOX_USE_DURATION_SECONDS = 2 * 60
POOL_SIZE = 3
POOL_MAINTENANCE_SCHEDULE = modal.Period(minutes=2)


# ## Main implementation

# We keep track of all warm Sandboxes in a Modal Queue of `SandboxReference` objects.
pool_queue = modal.Queue.from_name(
    "example-sandbox-pool-sandboxes", create_if_missing=True
)


@dataclass
class SandboxReference:
    id: str
    url: str
    expires_at: int


# ### Health check
#
# We add a simple health check that just ensures that the server in the Sandbox is
# running and responding to requests.
#
# If you just want to ensure the sandbox is running you could for example check
# `sb.poll() is not None` instead.
def is_healthy(url: str, wait_for_container_start: bool) -> bool:
    """Check if a Sandbox is healthy.

    When the Sandbox is first created, the server may not imemediately accept
    connections, so if `wait_for_container_start` is True, we retry if we fail to
    connect to the server URL.
    """
    import requests

    start_time = time.time()
    while time.time() - start_time < HEALTH_CHECK_TIMEOUT_SECONDS:
        try:
            response = requests.get(url, timeout=HEALTH_CHECK_TIMEOUT_SECONDS)
            response.raise_for_status()
            return True
        except requests.RequestException:
            if (
                not wait_for_container_start
                or time.time() - start_time >= HEALTH_CHECK_TIMEOUT_SECONDS
            ):
                return False
            time.sleep(0.1)

    return False


def is_still_good(sr: SandboxReference, check_health: bool) -> bool:
    """Check if a Sandbox is still good to use.

    It assumes that it's already been added to the pool, so we don't wait for the
    container to start.
    """
    if sr.expires_at < time.time() + SANDBOX_USE_DURATION_SECONDS:
        return False

    if check_health and not is_healthy(sr.url, wait_for_container_start=False):
        return False

    return True


# ### Adding a Sandbox to the pool
#
# This function creates and adds a new Sandbox to the pool. It runs a health check on
# the Sandbox before adding it.
#
# We deploy the Sandboxes in a separate Modal App called `example-sandbox-pool-sandboxes`,
# to separate the control app (logs, etc.) from the Sandboxes.
@app.function(image=server_image, retries=3)
@modal.concurrent(max_inputs=20)
def add_sandbox_to_queue() -> None:
    sandbox_app = modal.App.lookup(
        "example-sandbox-pool-sandboxes", create_if_missing=True
    )

    sandbox_cmd = ["python", "-m", "http.server", "8080"]
    sb = modal.Sandbox.create(
        *sandbox_cmd,
        app=sandbox_app,
        image=sandbox_image,
        encrypted_ports=[SANDBOX_SERVER_PORT],
        timeout=SANDBOX_TIMEOUT_SECONDS,
    )
    expires_at = int(time.time()) + SANDBOX_TIMEOUT_SECONDS
    url = sb.tunnels()[SANDBOX_SERVER_PORT].url

    if not is_healthy(url, wait_for_container_start=True):
        raise Exception("Health check failed")

    pool_queue.put(SandboxReference(id=sb.object_id, url=url, expires_at=expires_at))


# We also have a utility function that can be `.spawn()`ed to terminate Sandboxes.
@app.function()
def terminate_sandboxes(sandbox_ids: list[str]) -> int:
    num_terminated = 0
    for id in sandbox_ids:
        sb = modal.Sandbox.from_id(id)
        sb.terminate()
        num_terminated += 1

    print(f"Terminated {num_terminated} Sandboxes")
    return num_terminated


# ### Claiming a Sandbox from the pool
#
# We expose two ways to claim a Sandbox from the pool and get a URL to the server:
#
# - a web endpoint
# - a Function that can be called using the Modal SDK for [Python][1], [Go, or JS][2].
#
# [1]: https://github.com/modal-labs/modal-client
# [2]: https://github.com/modal-labs/libmodal
#
# The web endpoint is deployed as a [Modal web endpoint][3], and calls the
# `claim_sandbox` Function using `claim_sandbox.local()`, meaning that it's called in
# the same process as the web endpoint.
#
# The Function can be called using the Modal SDK for [Python][1], [Go, or JS][2].
#
# [1]: https://github.com/modal-labs/modal-client
# [2]: https://github.com/modal-labs/libmodal
# [3]: https://modal.com/docs/guide/webhook-urls
@app.function(image=server_image)
@modal.fastapi_endpoint()
@modal.concurrent(max_inputs=20)
def claim_sandbox_web_endpoint(check_health: bool = True) -> str:
    return claim_sandbox.local(check_health=check_health)


@app.function(image=server_image)
def claim_sandbox(check_health: bool = True) -> str:
    to_terminate: list[str] = []

    # Remove any expiring or unhealthy sandboxes, and return the first good one:
    while True:
        print(
            "Adding a new Sandbox to the pool to backfill "
            "(and ensure we have at least one)..."
        )
        add_sandbox_to_queue.spawn()

        # timeout=None here means we block in case we need to wait for the backfill:
        sr = pool_queue.get(timeout=None)
        if sr is None:
            continue

        if not is_still_good(sr, check_health):
            print(f"Sandbox '{sr.id}' was not good - terminating and trying another...")
            to_terminate.append(sr.id)
            continue

        break

    if to_terminate:
        terminate_sandboxes.spawn(to_terminate)

    print(f"Claimed Sandbox '{sr.id}', with URL: {sr.url}")
    return sr.url


# ### Maintaining the pool
#
# This function grows or shrinks the pool to SANDBOX_POOL_SIZE. It first removes any
# expiring or unhealthy sandboxes, then adjusts the pool size to reach the target.
#
# It runs on a schedule to ensure the pool doesn't drift too far from the target size.
@app.function(
    image=server_image,
    schedule=POOL_MAINTENANCE_SCHEDULE,
)
def maintain_pool():
    to_terminate: list[str] = []

    # First remove expiring and unhealthy sandboxes
    while True:
        sr = pool_queue.get(block=False)

        if sr is None:
            break

        if not is_still_good(sr, check_health=True):
            to_terminate.append(sr.id)
            continue

        # Found first good sandbox, but don't put it back in the queue to preserve
        # queue ordering.
        to_terminate.append(sr.id)
        break

    if to_terminate:
        print(f"Terminating {len(to_terminate)} expiring/unhealthy sandboxes...")
        terminate_sandboxes.spawn(to_terminate)

    # Now resize to target
    diff = POOL_SIZE - pool_queue.len()

    if diff > 0:
        for _ in add_sandbox_to_queue.starmap(() for _ in range(diff)):
            pass
    elif diff < 0:
        terminate_sandboxes.spawn(
            [sr.id for sr in pool_queue.get_many(n_values=-diff, timeout=0)]
        )

    print(f"Pool size after maintenance: {pool_queue.len()}")


# ## Local commands for interacting with the pool
#
# ### Deploy the app
#
# This also runs the `maintain_pool` function to ensure the pool is at the correct size
# without having to wait for the first scheduled maintenance run.
#
# Run it with `python 13_sandboxes/sandbox_pool.py deploy`.
def deploy():
    print("Deploying the app...")
    app.deploy()
    print("Done.")

    print("\nRunning initial pool maintenance...")
    maintain_pool.remote()
    print("Done.")


# ### Check the current state of the pool
#
# Run it with `python 13_sandboxes/sandbox_pool.py check`.
def check():
    print(f"Number of Sandboxes in the pool: {pool_queue.len()}")

    for sr in pool_queue.iterate():
        seconds_left = sr.expires_at - time.time()
        print(
            f"- Sandbox '{sr.id}' is at {sr.url} and expires at "
            f"{datetime.fromtimestamp(sr.expires_at).isoformat()} "
            f"({int(seconds_left)} seconds left)"
        )


# ### Claiming a Sandbox from the pool and print its URL
#
# This is implemented as if you wanted to call the Function from a Python backend
# application using the Modal SDK, i.e. using `.from_name()` to get the Function, etc.
#
# Run it with `python 13_sandboxes/sandbox_pool.py claim`.
def claim() -> None:
    deployed_claim_sandbox = modal.Function.from_name(
        "example-sandbox-pool", "claim_sandbox"
    )
    print(deployed_claim_sandbox.remote())


# ### Run a demo of the Sandbox pool.
#
# This is implemented as if you wanted to call the Function from a Python backend
# application using the Modal SDK, i.e. using `.from_name()` to get the Function, etc.
#
# Run it with `python 13_sandboxes/sandbox_pool.py demo`.
def demo():
    import urllib.request

    deploy()

    check()

    print("\nClaiming a Sandbox using the `claim_sandbox` Function...")
    deployed_claim_sandbox = modal.Function.from_name(
        "example-sandbox-pool", "claim_sandbox"
    )
    sandbox_url = deployed_claim_sandbox.remote()
    print(f"Claimed Sandbox URL: {sandbox_url}")

    print("\nCall the server in the Sandbox...")
    with urllib.request.urlopen(sandbox_url) as response:
        result = response.read().decode("utf-8")
        print(f"Sandbox server response:\n{result}")

    time.sleep(2)  # wait for the pool to be backfilled in the background
    check()

    deployed_web_endpoint = modal.Function.from_name(
        "example-sandbox-pool", "claim_sandbox_web_endpoint"
    )
    web_endpoint_url = deployed_web_endpoint.get_web_url()
    print(f"\nClaiming a Sandbox using the web endpoint at '{web_endpoint_url}'...")
    with urllib.request.urlopen(web_endpoint_url) as response:
        sandbox_url = response.read().decode("utf-8").strip(' "')
        print(f"Claimed Sandbox URL: {sandbox_url}")

    print("\nCall the server in the Sandbox...")
    with urllib.request.urlopen(sandbox_url) as response:
        result = response.read().decode("utf-8")
        print(f"Sandbox server response:\n{result}")

    time.sleep(2)
    check()


def main():
    parser = argparse.ArgumentParser(description="Manage Sandbox pool")
    parser.add_argument(
        "command",
        choices=["check", "deploy", "claim", "demo"],
        help="Command to execute",
    )
    args = parser.parse_args()

    if args.command == "check":
        check()
    elif args.command == "claim":
        claim()
    elif args.command == "deploy":
        deploy()
    elif args.command == "demo":
        demo()
    else:
        parser.print_help()


if __name__ == "__main__":
    main()


## Links discovered
- [Modal Sandboxes](https://modal.com/docs/guide/sandbox)
- [Modal web endpoint](https://modal.com/docs/guide/webhook-urls)
- [Modal Queue](https://modal.com/docs/guide/dicts-and-queues#modal-queues)
- [pawalt](https://github.com/pawalt)
- [Sandbox pool
# example gist](https://gist.github.com/pawalt/7a505c38bba75cafae0780a5dd40e8b8)

--- 13_sandboxes/simple_code_interpreter.py ---
# ---
# cmd: ["python", "13_sandboxes/simple_code_interpreter.py"]
# pytest: false
# ---

# # Build a stateful, sandboxed code interpreter

# This example demonstrates how to build a stateful code interpreter using a Modal
# [Sandbox](https://modal.com/docs/guide/sandbox).

# We'll create a Modal Sandbox that listens for code to execute and then
# executes the code in a Python interpreter. Because we're running in a sandboxed
# environment, we can safely use the "unsafe" `exec()` to execute the code.

# ## Setting up a code interpreter in a Modal Sandbox

# Our code interpreter uses a Python "driver program" to listen for code
# sent in JSON format to its standard input (`stdin`), execute the code,
# and then return the results in JSON format on standard output (`stdout`).

import inspect
import json
import sys
from typing import Any, Iterator

import modal


def driver_program():
    import json
    import sys
    from contextlib import redirect_stderr, redirect_stdout
    from io import StringIO

    # When you `exec` code in Python, you can pass in a dictionary
    # that defines the global variables the code has access to.

    # We'll use that to store state.

    globals: dict[str, Any] = {}
    while True:
        command = json.loads(input())  # read a line of JSON from stdin
        if (code := command.get("code")) is None:
            print(json.dumps({"error": "No code to execute"}))
            continue

        # Capture the executed code's outputs
        stdout_io, stderr_io = StringIO(), StringIO()
        with redirect_stdout(stdout_io), redirect_stderr(stderr_io):
            try:
                exec(code, globals)
            except Exception as e:
                print(f"Execution Error: {e}", file=sys.stderr)

        print(
            json.dumps(
                {"stdout": stdout_io.getvalue(), "stderr": stderr_io.getvalue()}
            ),
            flush=True,
        )


# We run this driver program in a [Modal Sandbox](https://modal.com/docs/guide/sandboxes).

app = modal.App.lookup("example-simple-code-interpreter", create_if_missing=True)
sb = modal.Sandbox.create(app=app)

# We have to convert the driver program to a string to pass it to the Sandbox.
# Here we use `inspect.getsource` to get the source code as a string,
# but you could also keep the driver program in a separate file and read it in.

driver_program_text = inspect.getsource(driver_program)
driver_program_command = f"""{driver_program_text}\n\ndriver_program()"""

# We then kick off the program with [`Sandbox.exec`](https://modal.com/docs/reference/modal.Sandbox#exec),
# which creates a process inside the Sandbox (see [`modal.container_process`](https://modal.com/docs/reference/modal.container_process)
# for details).

p = sb.exec("python", "-c", driver_program_command, bufsize=1)

# ## Running code in a Modal Sandbox

# Now we need a way to run code inside that running driver process.
# Our driver program already defined a JSON interface on its `stdin` and `stdout`,
# so we just need to write a quick wrapper to write to the remote `stdin`
# and read from the remote `stdout`.

reader, writer = p.stdin, iter(p.stdout)


def run_code(writer: modal.io_streams.StreamWriter, reader: Iterator[str], code: str):
    writer.write(json.dumps({"code": code}) + "\n")
    writer.drain()
    result = json.loads(next(reader))
    print(result["stdout"], end="")
    if result["stderr"]:
        print("\033[91m" + result["stderr"] + "\033[0m", end="", file=sys.stderr)


# Now we can execute some code in the Sandbox!

run_code(reader, writer, "print('hello, world!')")  # hello, world!

# The Sandbox and our code interpreter are stateful,
# so we can define variables and use them in subsequent code.

run_code(reader, writer, "x = 10")
run_code(reader, writer, "y = 5")
run_code(reader, writer, "result = x + y")
run_code(reader, writer, "print(f'The result is: {result}')")  # The result is: 15

# We can also see errors when code fails.

run_code(reader, writer, "print('Attempting to divide by zero...')")
run_code(reader, writer, "1 / 0")  # Execution Error: division by zero

# Finally, let's clean up after ourselves and terminate the Sandbox.

sb.terminate()


## Links discovered
- [Sandbox](https://modal.com/docs/guide/sandbox)
- [Modal Sandbox](https://modal.com/docs/guide/sandboxes)
- [`Sandbox.exec`](https://modal.com/docs/reference/modal.Sandbox#exec)
- [`modal.container_process`](https://modal.com/docs/reference/modal.container_process)

--- 13_sandboxes/test_case_generator.py ---
# ---
# cmd: ["modal", "run", "-m", "13_sandboxes.test_case_generator"]
# args: ["--gh-owner", "modal-labs", "--gh-repo-name", "password-analyzer", "--gh-module-path", "src/password_strength", "--gh-tests-path", "tests", "--gh-branch", "main"]
# ---
import subprocess
import time

import modal

app = modal.App(
    name="sandbox-test-case-generator",
)
model_volume = modal.Volume.from_name("deepseek-model-volume", create_if_missing=True)
files_volume = modal.Volume.from_name("files-volume", create_if_missing=True)

MODEL_NAME = "deepseek-ai/deepseek-coder-6.7b-instruct"
MODEL_REVISION = "e5d64addd26a6a1db0f9b863abf6ee3141936807"


model_image = (
    modal.Image.from_registry("lmsysorg/sglang:v0.4.9.post3-cu126", add_python="3.12")
    .uv_pip_install(
        "sglang[all]==0.4.9.post3",
        "accelerate==1.8.1",
        "hf_transfer==0.1.9",
    )
    .env(
        {
            "HF_HUB_ENABLE_HF_TRANSFER": "1",
            "HF_HOME": "/cache",
        }
    )
    .entrypoint([])  # silence noisy logs
)


@app.cls(
    image=model_image,
    volumes={
        "/cache": model_volume,
        "/data": files_volume,
    },
    gpu="L40S",
    timeout=600,
)
@modal.concurrent(max_inputs=3)  # Each container runs up to 3 requests at once.
class TestCaseServer:
    @modal.enter()
    def download_model(self):
        from huggingface_hub import snapshot_download

        snapshot_download(
            MODEL_NAME,
            local_dir=f"/cache/{MODEL_NAME}",  # similar to cache_dir, but with less unused metadata
            revision=MODEL_REVISION,
            ignore_patterns=["*.pt", "*.bin"],
        )

    @modal.enter()
    def start_model_server(self):
        import subprocess

        serve_params = {
            "host": "0.0.0.0",
            "port": 8000,
            "model": f"/cache/{MODEL_NAME}",
            "log-level": "error",
        }
        serve_cmd = "python -m sglang.launch_server " + " ".join(
            [f"--{k} {v}" for k, v in serve_params.items()]
        )

        self.serve_process = subprocess.Popen(serve_cmd, shell=True)
        wait_for_port(self.serve_process, 8000)

        print("SGLang server is ready!")

    @modal.web_server(port=8000, startup_timeout=240)
    def serve(self):
        return


@app.cls(
    image=modal.Image.debian_slim(python_version="3.12").uv_pip_install(
        "openai==1.97.1"
    ),
    volumes={
        "/data": files_volume,
    },
)
class TestCaseClient:
    url: str = modal.parameter()

    def load_inputs(self, file_name: str) -> tuple[str, str]:
        import os

        if not os.path.exists("/data/inputs"):
            raise Exception(
                "Inputs directory does not exist. Make sure to run download_files_to_volume first."
            )

        with open(f"/data/inputs/{file_name}", "r") as f:
            file_contents = f.read()

        with open(f"/data/inputs/test_{file_name}", "r") as f:
            test_file_contents = f.read()
        return file_contents, test_file_contents

    def write_outputs(self, output_file_name: str, output_contents: str) -> str:
        import os

        os.makedirs("/data/outputs", exist_ok=True)
        with open(f"/data/outputs/{output_file_name}", "w") as f:
            f.write(output_contents)
        return output_file_name

    @modal.method()
    def generate(self, file_name: str) -> str:
        import json

        import openai

        file_contents, test_file_contents = self.load_inputs(file_name)

        system_prompt = get_system_prompt()
        user_prompt = get_user_prompt(file_contents, test_file_contents)

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        client = openai.Client(base_url=f"{self.url}/v1", api_key="EMPTY")

        json_schema = {
            "type": "object",
            "properties": {"file_contents": {"type": "string"}},
            "required": ["file_contents"],
        }

        response = client.chat.completions.create(
            model="default",
            messages=messages,
            temperature=0,
            max_tokens=1024,
            response_format={
                "type": "json_schema",
                "json_schema": {
                    "name": "test_file",
                    "schema": json_schema,
                },
            },
        )
        output = response.choices[0].message.content
        try:
            output_contents = json.loads(output)["file_contents"]
            return self.write_outputs(f"test_{file_name}", output_contents)
        except Exception as e:
            print(f"Error generating test file: {e}")
            return None


@app.function(
    image=modal.Image.debian_slim(python_version="3.12").uv_pip_install(
        "requests==2.32.3"
    ),
    volumes={"/data": files_volume},
)
def download_files_to_volume(
    folder_paths: list[str],
    gh_owner: str,
    gh_repo_name: str,
    gh_branch: str,
) -> list[str]:
    import os

    import requests

    os.makedirs("/data/inputs", exist_ok=True)
    all_files = []
    for folder_path in folder_paths:
        response = requests.get(
            f"https://api.github.com/repos/{gh_owner}/{gh_repo_name}/contents/{folder_path}?ref={gh_branch}"
        )
        files = response.json()
        all_files.extend(files)

    file_to_download_urls = []
    for _file in all_files:
        if (
            _file["type"] == "file"
            and _file["name"].endswith(".py")
            and _file["name"] != "__init__.py"
        ):
            file_to_download_urls.append((_file["name"], _file["download_url"]))

    file_to_text = {}
    for name, url in file_to_download_urls:
        response = requests.get(url)
        file_to_text[name] = response.text

    for name, text in file_to_text.items():
        with open(f"/data/inputs/{name}", "w") as f:
            f.write(text)
    print("Files downloaded to volume!")
    return [name for name in file_to_text.keys() if not name.startswith("test_")]


def get_sandbox_image(gh_owner: str, gh_repo_name: str):
    ALLURE_VERSION = "2.34.1"
    MODULE_URL = f"https://github.com/{gh_owner}/{gh_repo_name}"

    image = (
        modal.Image.debian_slim()
        .apt_install("git", "curl", "tar", "default-jre")
        .uv_pip_install("webdiff")
        .run_commands(
            f"git clone {MODULE_URL}",
            "curl -sSL https://install.python-poetry.org | python3 -",
            "mkdir -p /opt/allure",
            f"curl -sL https://github.com/allure-framework/allure2/releases/download/{ALLURE_VERSION}/allure-{ALLURE_VERSION}.tgz | tar xz -C /opt/allure --strip-components=1",
        )
        .env({"PATH": "$PATH:/root/.local/bin:/opt/allure/bin"})
    )

    return image


def run_sandbox(image: modal.Image, file_name: str):
    new_file_name = file_name.replace(".py", "_llm.py")

    cmd = (
        f"webdiff password-analyzer/tests/{file_name} /data/outputs/{file_name}  --host 0.0.0.0 --port 8001 &&"
        + "cd password-analyzer && "
        + "poetry install --no-root && "
        + "poetry run pytest --alluredir allure-results || true && "
        + f"cp /data/outputs/{file_name} tests/{new_file_name} && "
        + "poetry run pytest --alluredir allure-results || true && "
        + "allure serve allure-results --host 0.0.0.0 --port 8000"
    )

    sb = modal.Sandbox.create(
        "sh",
        "-c",
        cmd,
        app=app,
        image=image,
        volumes={
            "/data": files_volume,
        },
        encrypted_ports=[8000, 8001],
        timeout=300,  # 5 minutes
    )
    return sb


@app.local_entrypoint()
async def main(
    gh_owner: str,
    gh_repo_name: str,
    gh_module_path: str,
    gh_tests_path: str,
    gh_branch: str,
):
    import asyncio

    # Start server
    sg_lang_server = TestCaseServer()

    # Download files to volume
    input_files = download_files_to_volume.remote(
        folder_paths=[gh_module_path, gh_tests_path],
        gh_owner=gh_owner,
        gh_repo_name=gh_repo_name,
        gh_branch=gh_branch,
    )

    # Initialize client and generate test files
    generator = TestCaseClient(url=sg_lang_server.serve.get_web_url())  # type: ignore
    output_generator = generator.generate.map.aio(input_files)
    output_files = []
    async for f in output_generator:
        if f is not None:
            output_files.append(f)
    print("Test case files generated successfully! Creating sandboxes...")

    # Create sandboxes to run the generated test files
    sandboxes = create_sandboxes(output_files, gh_owner, gh_repo_name)
    await asyncio.gather(
        *[sb.wait.aio(raise_on_termination=False) for sb in sandboxes],
        return_exceptions=True,
    )


# # Addenda
# The below functions are utility functions.
def create_sandboxes(filenames: list[str], gh_owner: str, gh_repo_name: str):
    file_to_sandbox: dict[str, modal.Sandbox] = {}
    for filename in filenames:
        print(f"Running sandbox for {filename}")
        image = get_sandbox_image(gh_owner, gh_repo_name)
        sb = run_sandbox(image, filename)
        file_to_sandbox[filename] = sb
    time.sleep(20)  # Hack to make sure URLs show up at the very end

    for filename, sb in file_to_sandbox.items():
        tunnel1 = sb.tunnels()[8000]
        tunnel2 = sb.tunnels()[8001]
        print(f"Sandbox created and run for generated test file: {filename}")
        print(f"âœ¨ View diff: {tunnel2.url}")
        print(f"âœ¨ View test results: {tunnel1.url}\n")

    return file_to_sandbox.values()


def get_user_prompt(file_text: str, test_file_text: str) -> str:
    return f"""
    Your task is to improve an existing test file using `pytest`.

    Step-by-step:
    1. Carefully read the existing test file (below) and understand the current test cases.
    2. Then read the source file (also below) and understand the function behavior, focusing on docstrings, edge cases, and argument types.
    3. Based on that understanding, **add** new test cases to the test file to increase coverageâ€”especially edge cases, boundary conditions, and untested branches.
    4. Use `pytest` idioms, but do **not** add or change import statementsâ€”**use only what is already imported**.
    5. Do **not** explain your reasoningâ€”just return the final modified test file.

    ### Requirements:
    - Your output must be a valid, complete Python file with the added test cases.
    - Do not modify existing test logic unless necessary to support your new test cases.
    - Do not import any additional modules.
    - Limit each line to a maximum of 100 characters to avoid output truncation or formatting errors.
    - Limit your output to around 25 lines. Make sure to complete any functions or blocks you start.


    --- BEGIN TEST FILE ---
    {test_file_text}
    --- END TEST FILE ---

    --- BEGIN SOURCE FILE ---
    {file_text}
    --- END SOURCE FILE ---
    """


def get_system_prompt():
    return (
        "You are a senior software engineer with expertise in test-driven development and Python unit testing. "
        "Your task is to enhance an existing test file by adding more test cases. "
        "Do not change or add import statements. Do not explain your reasoning. Output only a complete, valid Python file. "
        "Do not change existing code and only add new test cases that follow the same formatting as the existing test cases. "
        "Limit each line to a maximum of 100 characters to avoid output truncation or formatting errors."
        "Limit your output to around 25 lines. Make sure to complete any functions or blocks you start."
    )


def wait_for_port(process: subprocess.Popen, port: int):
    import socket

    while True:
        try:
            with socket.create_connection(("0.0.0.0", port), timeout=1):
                break
        except (ConnectionRefusedError, OSError):
            if process.poll() is not None:
                raise Exception(
                    f"Process {process.pid} exited with code {process.returncode}"
                )


--- 13_sandboxes/codelangchain/agent.py ---
# ---
# cmd: ["modal", "run", "-m", "13_sandboxes.codelangchain.agent", "--question", "Use gpt2 and transformers to generate text"]
# pytest: false
# ---

# # Build a coding agent with Modal Sandboxes and LangGraph

# This example demonstrates how to build an LLM coding "agent" that can generate and execute Python code, using
# documentation from the web to inform its approach.

# Naturally, we use the agent to generate code that runs language models.

# The agent is built with [LangGraph](https://github.com/langchain-ai/langgraph), a library for building
# directed graphs of computation popular with AI agent developers,
# and uses models from the OpenAI API.

# ## Setup

import modal

from .src import edges, nodes, retrieval
from .src.common import COLOR, PYTHON_VERSION, image

# You will need two [Modal Secrets](https://modal.com/docs/guide/secrets) to run this example:
# one to access the OpenAI API and another to access the LangSmith API for logging the agent's behavior.

# To create them, head to the [Secrets dashboard](https://modal.com/secrets), select "Create new secret",
# and use the provided templates for OpenAI and LangSmith.

app = modal.App(
    "example-agent",
    image=image,
    secrets=[
        modal.Secret.from_name("openai-secret", required_keys=["OPENAI_API_KEY"]),
        modal.Secret.from_name("langsmith-secret", required_keys=["LANGCHAIN_API_KEY"]),
    ],
)

# ## Creating a Sandbox

# We execute the agent's code in a Modal [Sandbox](https://modal.com/docs/guide/sandbox), which allows us to
# run arbitrary code in a safe environment. In this example, we will use the [`transformers`](https://huggingface.co/docs/transformers/index)
# library to generate text with a pre-trained model. Let's create a Sandbox with the necessary dependencies.


def create_sandbox(app) -> modal.Sandbox:
    # Change this image (and the retrieval logic in the retrieval module)
    # if you want the agent to give coding advice on other libraries!
    agent_image = modal.Image.debian_slim(python_version=PYTHON_VERSION).uv_pip_install(
        "torch==2.5.0",
        "transformers==4.46.0",
    )

    return modal.Sandbox.create(
        image=agent_image,
        timeout=60 * 10,  # 10 minutes
        app=app,
        # Modal sandboxes support GPUs!
        gpu="T4",
        # you can also pass secrets here -- note that the main app's secrets are not shared
    )


# We also need a way to run our code in the sandbox. For this, we'll write a simple wrapper
# around the Modal Sandbox `exec` method. We use `exec` because it allows us to run code without spinning up a
# new container. And we can reuse the same container for multiple runs, preserving state.


def run(code: str, sb: modal.Sandbox) -> tuple[str, str]:
    print(
        f"{COLOR['HEADER']}ðŸ“¦: Running in sandbox{COLOR['ENDC']}",
        f"{COLOR['GREEN']}{code}{COLOR['ENDC']}",
        sep="\n",
    )

    exc = sb.exec("python", "-c", code)
    exc.wait()

    stdout = exc.stdout.read()
    stderr = exc.stderr.read()

    if exc.returncode != 0:
        print(
            f"{COLOR['HEADER']}ðŸ“¦: Failed with exitcode {sb.returncode}{COLOR['ENDC']}"
        )

    return stdout, stderr


# ## Constructing the agent's graph

# Now that we have the sandbox to execute code in, we can construct our agent's graph. Our graph is
# defined in the `edges` and `nodes` modules
# [associated with this example](https://github.com/modal-labs/modal-examples/tree/main/13_sandboxes/codelangchain).
# Nodes are actions that change the state. Edges are transitions between nodes.

# The idea is simple: we start at the node `generate`, which invokes the LLM to generate code based off documentation.
# The generated code is executed (in the sandbox) as part of an edge called `check_code_execution`
# and then the outputs are passed to the LLM for evaluation (the `evaluate_execution` node).
# If the LLM determines that the code has executed correctly -- which might mean that the code raised an exception! --
# we pass along the `decide_to_finish` edge and finish.


def construct_graph(sandbox: modal.Sandbox, debug: bool = False):
    from langgraph.graph import StateGraph

    from .src.common import GraphState

    # Crawl the transformers documentation to inform our code generation
    context = retrieval.retrieve_docs(debug=debug)

    graph = StateGraph(GraphState)

    # Attach our nodes to the graph
    graph_nodes = nodes.Nodes(context, sandbox, run, debug=debug)
    for key, value in graph_nodes.node_map.items():
        graph.add_node(key, value)

    # Construct the graph by adding edges
    graph = edges.enrich(graph)

    # Set the starting and ending nodes of the graph
    graph.set_entry_point(key="generate")
    graph.set_finish_point(key="finish")

    return graph


# We now set up the graph and compile it. See the `src` module for details
# on the content of the graph and the nodes we've defined.

DEFAULT_QUESTION = "How do I generate Python code using a pre-trained model from the transformers library?"


@app.function()
def go(
    question: str = DEFAULT_QUESTION,
    debug: bool = False,
):
    """Compiles the Python code generation agent graph and runs it, returning the result."""
    sb = create_sandbox(app)

    graph = construct_graph(sb, debug=debug)
    runnable = graph.compile()
    result = runnable.invoke(
        {"keys": {"question": question, "iterations": 0}},
        config={"recursion_limit": 50},
    )

    sb.terminate()

    return result["keys"]["response"]


# ## Running the Graph

# Now let's call the agent from the command line!

# We define a `local_entrypoint` that runs locally and triggers execution on Modal.

# You can invoke it by executing following command from a folder that contains the `codelangchain` directory
# [from our examples repo](https://github.com/modal-labs/modal-examples/tree/main/13_sandboxes/codelangchain):

# ```bash
# modal run -m codelangchain.agent --question "How do I run a pre-trained model from the transformers library?"
# ```


@app.local_entrypoint()
def main(
    question: str = DEFAULT_QUESTION,
    debug: bool = False,
):
    """Sends a question to the Python code generation agent.

    Switch to debug mode for shorter context and smaller model."""
    if debug:
        if question == DEFAULT_QUESTION:
            question = "hi there, how are you?"

    print(go.remote(question, debug=debug))


# If things are working properly, you should see output like the following:

# ```bash
# $ modal run -m codelangchain.agent --question "generate some cool output with transformers"
# ---DECISION: FINISH---
# ---FINISHING---
# To generate some cool output using transformers, we can use a pre-trained language model from the Hugging Face Transformers library. In this example, we'll use the GPT-2 model to generate text based on a given prompt. The GPT-2 model is a popular choice for text generation tasks due to its ability to produce coherent and contextually relevant text. We'll use the pipeline API from the Transformers library, which simplifies the process of using pre-trained models for various tasks, including text generation.
#
# from transformers import pipeline
# # Initialize the text generation pipeline with the GPT-2 model
# generator = pipeline('text-generation', model='gpt2')
#
# # Define a prompt for the model to generate text from
# prompt = "Once upon a time in a land far, far away"
#
# # Generate text using the model
# output = generator(prompt, max_length=50, num_return_sequences=1)
#
# # Print the generated text
# print(output[0]['generated_text'])
#
# Result of code execution:
# Once upon a time in a land far, far away, and still inhabited even after all the human race, there would be one God: a perfect universal God who has always been and will ever be worshipped. All His acts and deeds are immutable,
# ```


## Links discovered
- [LangGraph](https://github.com/langchain-ai/langgraph)
- [Modal Secrets](https://modal.com/docs/guide/secrets)
- [Secrets dashboard](https://modal.com/secrets)
- [Sandbox](https://modal.com/docs/guide/sandbox)
- [`transformers`](https://huggingface.co/docs/transformers/index)
- [associated with this example](https://github.com/modal-labs/modal-examples/tree/main/13_sandboxes/codelangchain)
- [from our examples repo](https://github.com/modal-labs/modal-examples/tree/main/13_sandboxes/codelangchain)

--- 14_clusters/simple_torch_cluster.py ---
# # Simple PyTorch cluster

# This example shows how you can perform distributed computation with PyTorch.
# It is a kind of 'hello world' example for distributed ML training: setting up a cluster
# and executing a broadcast operation to share a single tensor.

# ## Basic setup: Imports, dependencies, and a script

# Let's get the imports out of the way first.
# We need to import `modal.experimental` to use this feature, since it's still under development.
# Let us know if you run into any issues!

import os
from pathlib import Path

import modal
import modal.experimental

# Communicating between nodes in a cluster requires communication libraries.
# We'll use `torch`, so we add it to our container's [Image](https://modal.com/docs/guide/images) here.

image = modal.Image.debian_slim(python_version="3.12").uv_pip_install(
    "torch~=2.5.1", "numpy~=2.2.1"
)

# The approach we're going to take is to use a Modal [Function](https://modal.com/docs/reference/modal.Function)
# to launch the underlying script we want to distribute over the cluster nodes.
# The script is located in another file in the same directory
# of [our examples repo](https://github.com/modal-labs/modal-examples/).
# In order to use it in our remote Modal Function,
# we need to duplicate it remotely, which we do with `add_local_file`.

this_directory = Path(__file__).parent

image = image.add_local_file(
    this_directory / "simple_torch_cluster_script.py",
    remote_path="/root/script.py",
)

app = modal.App("example-simple-torch-cluster", image=image)

# ## Configuring a test cluster

# First, we set the size of the cluster in containers/nodes. This can be between 1 and 8.
# This is part of our Modal configuration, since Modal is responsible for spinning up our cluster.

n_nodes = 4

# Next, we set the number of processes we run per node.
# The usual practice is to run one process per GPU,
# so we set those two values to be equal.
# Note that `N_GPU` is Modal configuration ("how many GPUs should we spin up for you?")
# while `nproc_per_node` is `torch.distributed` configuration ("how many processes should we spawn for you?").

n_proc_per_node = N_GPU = 1
GPU_CONFIG = f"A100:{N_GPU}"

# Lastly, we need to select our communications library: the software that will handle
# sending messages between nodes in our cluster.
# Since we are running on GPUs, we use the
# [NVIDIA Collective Communications Library](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html)
# (`nccl`, pronounced "nickle").

# This is part of `torch.distributed` configuration --
# Modal handles the networking infrastructure but not the communication protocol.

backend = "nccl"  # or "gloo" on CPU, see https://pytorch.org/docs/stable/distributed.html#which-backend-to-use

# This cluster configurations is nice for testing, but typically
# you'll want to run a cluster with the maximum number of GPUs per container --
# 8 if you're running on H100s, the beefiest GPUs we offer on Modal.

# ## Launching the script

# Our Modal Function is merely a 'launcher' that sets up the distributed
# cluster environment and then calls `torch.distributed.run`,
# the underlying Python code exposed by the [`torchrun`](https://pytorch.org/docs/stable/elastic/run.html)
# command line tool.

# So executing this distributed job is easy! Just run

# ```bash
# modal run simple_torch_cluster.py
# ```

# in your terminal.

# In addition to the values set in code above, you can pass additional arguments to `torch.distributed.run`
# via the command line:

# ```bash
# modal run simple_torch_cluster.py --max-restarts=1
# ```


@app.function(gpu=GPU_CONFIG)
@modal.experimental.clustered(size=n_nodes)
def dist_run_script(*args):
    from torch.distributed.run import parse_args, run

    cluster_info = (  # we populate this data for you
        modal.experimental.get_cluster_info()
    )
    # which container am I?
    container_rank = cluster_info.rank
    # how many containers are in this cluster?
    world_size = len(cluster_info.container_ips)
    # what's the leader/master/main container's address?
    main_addr = cluster_info.container_ips[0]
    # what's the identifier of this cluster task in Modal?
    task_id = os.environ["MODAL_TASK_ID"]
    print(f"hello from {container_rank=}")
    if container_rank == 0:
        print(
            f"reporting cluster state from rank0/main: {main_addr=}, {world_size=}, {task_id=}"
        )

    run(
        parse_args(
            [
                f"--nnodes={n_nodes}",
                f"--node_rank={cluster_info.rank}",
                f"--master_addr={main_addr}",
                f"--nproc-per-node={n_proc_per_node}",
                "--master_port=1234",
            ]
            + list(args)
            + ["/root/script.py", "--backend", backend]
        )
    )


## Links discovered
- [Image](https://modal.com/docs/guide/images)
- [Function](https://modal.com/docs/reference/modal.Function)
- [our examples repo](https://github.com/modal-labs/modal-examples/)
- [NVIDIA Collective Communications Library](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html)
- [`torchrun`](https://pytorch.org/docs/stable/elastic/run.html)

--- 14_clusters/simple_torch_cluster_script.py ---
# ---
# lambda-test: false  # auxiliary-file
# pytest: false
# ---
import argparse
import os
from contextlib import contextmanager

import torch
import torch.distributed as dist

# Environment variables set by torch.distributed.run.
LOCAL_RANK = int(os.environ["LOCAL_RANK"])
WORLD_SIZE = int(os.environ["WORLD_SIZE"])
WORLD_RANK = int(os.environ["RANK"])
# The master (or leader) rank is always 0 with torch.distributed.run.
MASTER_RANK = 0

# This `run` function performs a simple distributed data transfer between containers
# using the specified distributed communication backend.

# An example topology of the cluster when WORLD_SIZE=4 is shown below:
#
#        +---------+
#        | Master  |
#        | Rank 0  |
#        +----+----+
#             |
#             |
#    +--------+--------+
#    |        |        |
#    |        |        |
# +--+--+  +--+--+  +--+--+
# |Rank 1| |Rank 2| |Rank 3|
# +-----+  +-----+  +-----+

# A broadcast operation (https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#broadcast)
# is performed between the master container (rank 0) and all other containers.

# The master container (rank 0) sends a tensor to all other containers.
# Each container then receives that tensor from the master container.


def run(backend):
    # Helper function providing a vanity name for each container based on its world (i.e. global) rank.
    def container_name(wrld_rank: int) -> str:
        return (
            f"container-{wrld_rank} (main)"
            if wrld_rank == 0
            else f"container-{wrld_rank}"
        )

    tensor = torch.zeros(1)

    # Need to put tensor on a GPU device for NCCL backend.
    if backend == "nccl":
        device = torch.device("cuda:{}".format(LOCAL_RANK))
        tensor = tensor.to(device)

    if WORLD_RANK == MASTER_RANK:
        print(f"{container_name(WORLD_RANK)} sending data to all other containers...\n")
        for rank_recv in range(1, WORLD_SIZE):
            dist.send(tensor=tensor, dst=rank_recv)
            print(
                f"{container_name(WORLD_RANK)} sent data to {container_name(rank_recv)}\n"
            )
    else:
        dist.recv(tensor=tensor, src=MASTER_RANK)
        print(
            f"{container_name(WORLD_RANK)} has received data from {container_name(MASTER_RANK)}\n"
        )


# In order for the broadcast operation to happen across the cluster, we need to have the master container (rank 0)
# learn the network addresses of all other containers.

# This is done by calling `dist.init_process_group` with the specified backend.

# See https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group for more details.


@contextmanager
def init_processes(backend):
    try:
        dist.init_process_group(backend, rank=WORLD_RANK, world_size=WORLD_SIZE)
        yield
    finally:
        dist.barrier()  # ensure any async work is done before cleaning up
        # Remove this if it causes program to hang. ref: https://github.com/pytorch/pytorch/issues/75097.
        dist.destroy_process_group()


if __name__ == "__main__":
    # This is a minimal CLI interface adhering to the requirements of torch.distributed.run (torchrun).
    #
    # Our Modal Function will use torch.distributed.run to launch this script.
    #
    # See https://pytorch.org/docs/stable/elastic/run.html for more details on the CLI interface.
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--local-rank",
        "--local_rank",
        type=int,
        help="Local rank. Necessary for using the torch.distributed.launch utility.",
    )
    parser.add_argument("--backend", type=str, default="gloo", choices=["nccl", "gloo"])
    args = parser.parse_args()

    with init_processes(backend=args.backend):
        run(backend=args.backend)


--- internal/readme.md ---
## `Internal/`

This folder contains internal repository and documentation management code.
It does not contain examples.

### Continuous Integration and Continuous Deployment

Modal cares deeply about the correctness of our examples -- we have also
suffered from janky, poorly-maintained documentation and we do our best to
ensure that our examples don't pay that forward.

This document explains the CI/CD process we use. It is primarily intended for
Modal engineers, but if you're contributing an example and have the bandwidth to
set up the testing as well, we appreciate it!

#### Frontmatter

Examples can include a small frontmatter block in YAML format that controls
testing and deployment behavior.

Fields include:

- `deploy`: If `true`, the example is deployed as a Modal application with
  `modal deploy`. If `false`, it is not. Default is `false`. Examples should be
  deployed only if they are a live demo or they are consumed as a service by
  other examples.
- `cmd`: The command to run the example for testing. Default is
  `["modal", "run", "<path>"]`. All `path`s should be relative to the
  root directory of the repository.
- `args`: Arguments to pass to the command. Default is `[]`.
- `lambda-test`: If `true`, the example is tested with the cli command provided
  in `cmd`. If `false`, it is not. Default is `true`. Note that this controls
  execution in the CI/CD of this repo _and_ in the internal AWS Lambda monitor
  as part of `synthetic_monitoring`.
- `env`: A dictionary of environment variables to include when testing.
  Default is `{}`, but note that the environment can be modified in the CI/CD of
  this repo or in the monitor-based testing.

Below is an example frontmatter for deploying a web app. Note that here we
`modal serve` in the test so as to not deploy to prod when testing. Note that in
testing environments, the `MODAL_SERVE_TIMEOUT` environment variable is set so
that the command terminates.

```yaml
---
deploy: true
cmd: ["modal", "serve", "10_integrations/pushgateway.py"]
---
# example prose and code begins here
```

#### Testing in GitHub Actions

When a PR is opened or updated, any changed examples are run via GitHub Actions.
We also create a preview of the documentation site and share the URL in the PR.

You can find the commands used to execute tests in the `.github/workflows`
directory. These can be used to run the tests locally. You may need to install
the `requirements.txt` in this folder to do so.

This workflow is intended to catch errors at the time a PR is made -- including
both errors in the example and issues with the execution of the example in the
monitoring system, like file imports.

#### Continual Monitoring

Examples are executed regularly and at random to check for regressions. The
results are monitored.

Modal engineers, see `synthetic_monitoring` in the `modal` repo for details.

### Previewing the Documentation Site

Modal engineers can preview the documentation site with a fast-reloading
development server (`inv just-frontend`) when iterating or with a shareable
Web deployment with one week TTL (`inv frontend-preview`). See the `modal`
repo for details.

You can find the process for creating a preview in the GitHub Action.


--- internal/CLAUDE.md ---
# Internal Development Guide for Modal Examples

This guide is for fixing bugs in the Modal examples repository. These bugs are
typically triggered by synthetic monitoring (synmon) alerts.

## Prime Directive

**The code must run.** If example code does not run, we have failed the user.
Examples are tested regularly because code rots â€” external dependencies change,
APIs evolve, and artifacts move. Your job is to make the code run again.

## Scope

- Fix bugs in numbered example directories (`01_getting_started/` through
  `14_clusters/`)
- Ignore the `misc/` folder entirely â€” it is not continuously tested
- Do not change anything in the `internal/` folder â€” it contains CI
  infrastructure, not examples. Use its contents, including this file, to guide
  your investigation and testing

## Common Bug Types

Most bugs fall into one of two categories:

1. **Dependency changes** â€” upstream packages release breaking changes or
   deprecate APIs; other artifacts or APIs change in breaking ways
2. **Modal SDK changes** â€” the Modal API evolves and examples need updates

If the root cause is outside this repository (e.g., a bug in the Modal SDK
itself), report it rather than implementing a workaround. Include the error,
affected example, and any relevant context.

## Testing Examples

Run examples using the internal test runner:

```bash
MODAL_ENVIRONMENT=examples python -m internal.run_example <example_stem>
```

Where `<example_stem>` is the filename without `.py` (e.g., `hello_world` for
`01_getting_started/hello_world.py`).

This uses the `cmd` and `args` from the example's frontmatter, or defaults to
`modal run <path>`. The test environment automatically sets
`MODAL_SERVE_TIMEOUT=5.0`.

## Updating Dependencies

When fixing dependency-related bugs:

1. **Check PyPI for the latest version** of the problematic package
2. **Update to the latest working version** â€” not just any version that fixes
   the immediate bug
3. **Confirm the new dependencies work** by running the example
4. **Fix warnings, not just errors** â€” examples should run cleanly without
   deprecation warnings or other noise

### Pinning Requirements

Follow these pinning rules for container image dependencies:

- Pin to SemVer minor version: `~=x.y.z` or `==x.y`
- For packages with `version < 1.0`: pin to patch version `==0.y.z`
- Pin container base images to stable tags (e.g., `v1`), never `latest`
- Always specify `python_version` when using base images that support it

Example:

```python
image = (
    modal.Image.from_registry("nvidia/cuda:12.8.0-devel-ubuntu22.04", add_python="3.12")
    .uv_pip_install(
        "vllm==0.13.0",
        "huggingface-hub==0.36.0",
    )
)
```

## Updating Prose

Examples use literate programming style â€” prose is written as markdown in
comments on their own lines:

```python
# # This is a Title
#
# This is a paragraph of prose that becomes documentation.
# It continues on multiple lines.

import modal  # This comment stays with the code

# ## This is a Section Header
#
# More prose explaining the next code block.

app = modal.App("example-name")
```

**Critical rules:**

- **Always update prose when code changes affect what it describes** â€” the prose
  is the documentation and is equally important as the code
- **Do not rename files** â€” file paths are referenced elsewhere and renaming
  breaks links

## Code Style

### Time constants

Define `MINUTES = 60` (seconds) at module level for readable timeouts:

```python
MINUTES = 60  # seconds

@app.function(timeout=10 * MINUTES)
```

### Pin model revisions

Always pin model revisions to avoid surprises when upstream repos update:

```python
MODEL_NAME = "Qwen/Qwen3-4B-Thinking-2507-FP8"
MODEL_REVISION = "953532f942706930ec4bb870569932ef63038fdf"  # pin to avoid surprises!
```

### Hugging Face downloads

Use the high-performance download flag and pin `huggingface-hub`:

```python
.uv_pip_install("huggingface-hub==0.36.0")
.env({"HF_XET_HIGH_PERFORMANCE": "1"})  # faster downloads
```

### Volume conventions

- Use descriptive kebab-case names: `"huggingface-cache"`, `"vllm-cache"`
- Always use `create_if_missing=True` for first-run convenience

```python
cache_vol = modal.Volume.from_name("huggingface-cache", create_if_missing=True)
```

### App naming

Use `example-` prefix with kebab-case:

```python
app = modal.App("example-vllm-inference")
```

### Path references

Use `Path(__file__).parent` for paths relative to the script:

```python
here = Path(__file__).parent
input_path = here / "data" / "config.yaml"
```

### Clean up warnings and logs

Examples should run cleanly. Fix deprecation warnings and suppress noisy logs â€”
unhandled warnings are amateurish and make debugging harder.

- Run `.entrypoint([])` on NVIDIA container images to disable license logging:
  ```python
  modal.Image.from_registry("nvidia/cuda:12.8.0-devel-ubuntu22.04", add_python="3.12")
      .entrypoint([])  # disable noisy NVIDIA license logging
  ```

### Use fully-qualified Modal names

Always use `modal.X` instead of importing names directly:

```python
# âœ… Good
import modal

image = modal.Image.debian_slim()
vol = modal.Volume.from_name("my-vol", create_if_missing=True)

# âŒ Bad
from modal import Image, Volume

image = Image.debian_slim()
```

### No local dependencies except `modal` and `fastapi`

Examples must not require local Python dependencies beyond `modal` (and
`fastapi` if needed). Use the Python standard library for HTTP requests and
other utilities. Dependencies inside Modal Functions are fine and encouraged.

### Each line should spark joy

Prefer clarity and economy. Use `pathlib.Path` over `os.path`. Use meaningful
variable names. Remove code that foregrounds machine concerns over reader
concerns.

## Prose Style

### Capitalize Modal product features

Modal's features are proper nouns: Image, Volume, Function, Cls, App, Secret.
Use `monospace` only when referring to the actual Python object.

```python
# âœ… "Modal Volumes provide distributed storage."
# âœ… "`modal.Volume` has a `from_name` method."
# âŒ "Modal volumes provide distributed storage."
```

### Use active, descriptive headers

Headers should express purpose, not just content:

```python
# âœ… ## Cache model weights
# âŒ ## Modal Volume
# âŒ ## Model weights loading
```

### Use blank lines for visual separation

Add a blank line before code blocks to match the visual separation in rendered
docs:

```python
# âœ… Good

# We define an image with our dependencies.

image = modal.Image.debian_slim().pip_install("torch")

# âŒ Bad

# We define an image with our dependencies.
image = modal.Image.debian_slim().pip_install("torch")
```

### Defer detailed explanations

Don't duplicate Guide content. Keep explanations brief and link to docs:

```python
# âœ… Good
# [Modal Volumes](https://modal.com/docs/guide/volumes) add distributed storage.
# Here, we use one to cache compiler artifacts.

# âŒ Bad (too much detail)
# Modal Volumes provide a high-performance distributed file system...
# (paragraph of text duplicating the Guide)
```

### Link to Modal docs

Link to relevant Modal documentation using full URLs:

```python
# We wrap inference in a Modal [Cls](https://modal.com/docs/guide/lifecycle-functions)
# that ensures models are loaded once when a new container starts.
```

Common links:

- Images: `https://modal.com/docs/guide/images`
- Volumes: `https://modal.com/docs/guide/volumes`
- GPUs: `https://modal.com/docs/guide/gpu`
- Model weights: `https://modal.com/docs/guide/model-weights`
- Lifecycle: `https://modal.com/docs/guide/lifecycle-functions`

## Linting

Before committing changes:

```bash
source venv/bin/activate
ruff check --fix <modified_files>
ruff format <modified_files>
```

See the repository root `CLAUDE.md` for full Python development rules.

## Pull Request Guidelines

See `.github/pull_request_template.md` for the full checklist. Key points:

- Example must be testable with `modal run` or have a custom `cmd` in
  frontmatter
- Example must run with no arguments or have `args` defined in frontmatter
- All container dependencies must be pinned
- No local third-party dependencies required (except `fastapi`)


## Links discovered
- [Modal Volumes](https://modal.com/docs/guide/volumes)
- [Cls](https://modal.com/docs/guide/lifecycle-functions)

--- internal/conftest.py ---
import pytest


@pytest.fixture(autouse=True)
def disable_auto_mount(monkeypatch):
    monkeypatch.setenv("MODAL_AUTOMOUNT", "0")
    yield


--- internal/deploy.py ---
import argparse
import os
import re
import shlex
import subprocess
import sys
from pathlib import Path
from typing import NamedTuple, Optional

from utils import ExampleType, get_examples


class DeployError(NamedTuple):
    stdout: str
    stderr: str
    code: int


def deploy(
    deployable: bool,
    module_with_app: Path,
    dry_run: bool,
    filter_pttrn: Optional[str],
    env: Optional[dict[str, str]],
) -> Optional[DeployError]:
    if filter_pttrn and not re.match(filter_pttrn, module_with_app.name):
        return None

    if not deployable:
        print(f"â© skipping: '{module_with_app.name}' is not marked for deploy")
        return None

    deploy_command = f"modal deploy {module_with_app.name}"
    if dry_run:
        print(f"ðŸŒµ  dry-run: '{module_with_app.name}' would have deployed")
    else:
        print(f"â›´ deploying: '{module_with_app.name}' ...")
        r = subprocess.run(
            shlex.split(deploy_command),
            cwd=module_with_app.parent,
            capture_output=True,
            env=os.environ | (env or {}),
        )
        if r.returncode != 0:
            print(
                f"âš ï¸ deployment failed: '{module_with_app.name}'",
                file=sys.stderr,
            )
            print(r.stderr)
            return DeployError(stdout=r.stdout, stderr=r.stderr, code=r.returncode)
        else:
            print(f"âœ”ï¸ deployed '{module_with_app.name}")
    return None


def main(argv: Optional[list[str]] = None) -> int:
    parser = argparse.ArgumentParser(
        description="Deploy Modal example programs to our Modal organization.",
        add_help=True,
    )
    parser.add_argument(
        "--dry-run",
        default=True,
        action="store_true",
        help="show what apps be deployed without deploying them.",
    )
    parser.add_argument("--no-dry-run", dest="dry_run", action="store_false")
    parser.add_argument(
        "--filter",
        default=None,
        help="Filter which apps are deployed with basic pattern matching. eg. 'cron' matches 'say_hello_cron.py'.",
    )
    arguments = parser.parse_args()

    if arguments.dry_run:
        print(
            "INFO: dry-run is active. Intended deployments will be displayed to console."
        )

    example_modules = (ex for ex in get_examples() if ex.type == ExampleType.MODULE)
    filter_pttrn = (r".*" + arguments.filter + r".*") if arguments.filter else None
    results = [
        deploy(
            deployable=bool(ex_mod.metadata.get("deploy")),
            module_with_app=Path(ex_mod.module),
            dry_run=arguments.dry_run,
            filter_pttrn=filter_pttrn,
            env=ex_mod.metadata.get("env"),
        )
        for ex_mod in example_modules
    ]

    failures = [r for r in results if r]
    if any(failures):
        print(f"ERROR: {len(failures)} deployment failures.")
        return 1
    return 0


if __name__ == "__main__":
    raise SystemExit(main())


--- internal/generate_diff_matrix.py ---
import json
import os
import subprocess
import sys


def load_event():
    event_path = os.environ.get("GITHUB_EVENT_PATH")
    if not event_path:
        print("GITHUB_EVENT_PATH not set", file=sys.stderr)
        sys.exit(1)
    try:
        with open(event_path, "r") as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading event JSON: {e}", file=sys.stderr)
        sys.exit(1)


def determine_diff_range(event, event_name):
    if event_name == "pull_request":
        try:
            base = event["pull_request"]["base"]["sha"]
            head = event["pull_request"]["head"]["sha"]
        except KeyError as e:
            print(f"Missing key in pull_request event: {e}", file=sys.stderr)
            sys.exit(1)
    elif event_name == "push":
        base = event.get("before")
        head = event.get("after")

    elif event_name == "workflow_dispatch":
        try:
            subprocess.run(["git", "fetch", "origin", "main"], check=True)

            base = (
                subprocess.check_output(["git", "rev-parse", "origin/main"])
                .decode()
                .strip()
            )

            head = (
                subprocess.check_output(["git", "rev-parse", "HEAD"]).decode().strip()
            )
        except subprocess.CalledProcessError as e:
            print(f"Git error while determining diff range: {e}", file=sys.stderr)
            sys.exit(1)

    else:
        print(f"Unsupported event type: {event_name}", file=sys.stderr)
        sys.exit(1)

    if not base or not head:
        print("Could not determine base and head commits", file=sys.stderr)
        sys.exit(1)
    return base, head


def get_changed_files(base, head):
    try:
        result = subprocess.run(
            ["git", "diff", "--name-only", base, head],
            capture_output=True,
            text=True,
            check=True,
        )
        return result.stdout.splitlines()
    except subprocess.CalledProcessError as e:
        print(f"Error running git diff: {e}", file=sys.stderr)
        sys.exit(1)


def filter_files(files):
    return [
        f
        for f in files
        if f.endswith(".py")
        and not (f.startswith("internal/") or f.startswith("misc/"))
    ]


def write_output(key, value):
    github_output = os.environ.get("GITHUB_OUTPUT")
    if github_output:
        try:
            with open(github_output, "a") as out:
                out.write(f"{key}={value}\n")
        except Exception as e:
            print(f"Error writing to GITHUB_OUTPUT: {e}", file=sys.stderr)


def main():
    event = load_event()
    event_name = event.get("event_name") or os.environ.get("GITHUB_EVENT_NAME")
    base, head = determine_diff_range(event, event_name)
    changed_files = get_changed_files(base, head)
    filtered_files = filter_files(changed_files)
    json_output = json.dumps(filtered_files)
    write_output("all_changed_files", json_output)
    print(json_output)


if __name__ == "__main__":
    main()


--- internal/requirements.txt ---
pytest
jupyter
ipython
nbconvert
jupytext~=1.16.1
pydantic~=1.10.14
mypy==1.2.0
ruff==0.9.6
fastapi


--- internal/test_generate_diff_matrix.py ---
import json
import subprocess

import generate_diff_matrix as gdm
import pytest


def test_determine_diff_range_push():
    event = {"before": "commit1", "after": "commit2"}
    base, head = gdm.determine_diff_range(event, "push")
    assert base == "commit1"
    assert head == "commit2"


def test_determine_diff_range_pull():
    event = {
        "pull_request": {
            "base": {"sha": "base_sha"},
            "head": {"sha": "head_sha"},
        }
    }
    base, head = gdm.determine_diff_range(event, "pull_request")
    assert base == "base_sha"
    assert head == "head_sha"


def test_determine_diff_range_invalid_event():
    event = {}
    with pytest.raises(SystemExit):
        gdm.determine_diff_range(event, "unsupported_event")


def test_filter_files():
    files = [
        "example.py",
        "internal/test.py",
        "misc/skip.py",
        "script.js",
        "dir/another.py",
    ]
    filtered = gdm.filter_files(files)
    assert filtered == ["example.py", "dir/another.py"]


def test_get_changed_files(monkeypatch):
    class DummyCompletedProcess:
        def __init__(self, stdout):
            self.stdout = stdout

    def fake_run(args, capture_output, text, check):
        return DummyCompletedProcess("file1.py\nfile2.py\n")

    monkeypatch.setattr(subprocess, "run", fake_run)
    files = gdm.get_changed_files("base", "head")
    assert files == ["file1.py", "file2.py"]


def test_write_output(tmp_path, monkeypatch):
    temp_output = tmp_path / "github_output.txt"
    monkeypatch.setenv("GITHUB_OUTPUT", str(temp_output))

    gdm.write_output("test_key", "test_value")

    with open(temp_output, "r") as f:
        content = f.read()
    assert "test_key=test_value" in content


def test_main_push(monkeypatch, tmp_path):
    # simulate a push event by creating a temporary event JSON
    event_data = {"before": "commit1", "after": "commit2"}
    event_file = tmp_path / "event.json"
    event_file.write_text(json.dumps(event_data))

    monkeypatch.setenv("GITHUB_EVENT_PATH", str(event_file))
    monkeypatch.setenv("GITHUB_EVENT_NAME", "push")

    output_file = tmp_path / "output.txt"
    monkeypatch.setenv("GITHUB_OUTPUT", str(output_file))

    # override get_changed_files to simulate a git diff call
    def fake_get_changed_files(base, head):
        return ["file1.py", "internal/ignore.py", "misc/skip.py", "dir/keep.py"]

    monkeypatch.setattr(gdm, "get_changed_files", fake_get_changed_files)

    gdm.main()

    with open(output_file, "r") as f:
        output_content = f.read().strip()
    expected = json.dumps(["file1.py", "dir/keep.py"])
    assert f"all_changed_files={expected}" in output_content


def test_main_pull(monkeypatch, tmp_path):
    # simulate a pull_request event
    event_data = {
        "pull_request": {
            "base": {"sha": "base_commit"},
            "head": {"sha": "head_commit"},
        }
    }
    event_file = tmp_path / "event.json"
    event_file.write_text(json.dumps(event_data))

    monkeypatch.setenv("GITHUB_EVENT_PATH", str(event_file))
    monkeypatch.setenv("GITHUB_EVENT_NAME", "pull_request")

    output_file = tmp_path / "output.txt"
    monkeypatch.setenv("GITHUB_OUTPUT", str(output_file))

    def fake_get_changed_files(base, head):
        return [
            "pull_file.py",
            "internal/not_this.py",
            "misc/also_not.py",
            "folder/keep_this.py",
        ]

    monkeypatch.setattr(gdm, "get_changed_files", fake_get_changed_files)

    gdm.main()

    with open(output_file, "r") as f:
        output_content = f.read().strip()
    expected = json.dumps(["pull_file.py", "folder/keep_this.py"])
    assert f"all_changed_files={expected}" in output_content


--- internal/typecheck.py ---
"""
MyPy type-checking script.
Unvalidated, incorrect type-hints are worse than no type-hints!
"""

import concurrent
import os
import pathlib
import subprocess
import sys
from concurrent.futures import ProcessPoolExecutor

import mypy.api


def fetch_git_repo_root() -> pathlib.Path:
    return pathlib.Path(
        subprocess.check_output(["git", "rev-parse", "--show-toplevel"])
        .decode("ascii")
        .strip()
    )


def run_mypy(pkg: str, config_file: pathlib.Path) -> list[str]:
    args = [
        pkg,
        "--no-incremental",
        "--namespace-packages",
        "--config-file",
        str(config_file),
    ]
    result = mypy.api.run(args)
    return result[0].splitlines()


def extract_errors(output: list[str]) -> list[str]:
    if len(output) > 0 and "success" in output[0].lower():
        print(output[0], file=sys.stderr)
        return []
    return [l for l in output if "error" in l]


def main() -> int:
    repo_root = fetch_git_repo_root()
    config_file = repo_root / "pyproject.toml"
    errors = []

    # Type-check scripts
    topic_dirs = sorted([d for d in repo_root.iterdir() if d.name[:2].isdigit()])

    with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
        future_to_path = {}
        for topic_dir in topic_dirs:
            for pth in topic_dir.iterdir():
                if not (pth.is_file() and pth.name.endswith(".py")):
                    continue
                elif "__pycache__" in pth.parts:
                    continue
                else:
                    print(f"âŒ›ï¸ spawning mypy on '{pth}'", file=sys.stderr)
                    future = executor.submit(
                        run_mypy, pkg=str(pth), config_file=config_file
                    )
                    future_to_path[future] = pth

        for future in concurrent.futures.as_completed(future_to_path, timeout=90):
            pth = future_to_path[future]
            try:
                output = future.result()
                topic_errors = extract_errors(output)
                if topic_errors:
                    print(f"\nfound {len(topic_errors)} errors in '{pth}'")
                    print("\n".join(topic_errors))
                    errors.extend(topic_errors)
            except Exception as exc:
                print(f"Error on file {pth}: {exc}")
                errors.append(exc)

    # Type-check packages
    # Getting mypy running successfully with a monorepo of heterogenous packaging structures
    # is a bit fiddly, so we expect top-level packages to opt-in to type-checking by placing a
    # `py.typed` file inside themselves. https://peps.python.org/pep-0561/
    for py_typed in repo_root.glob("**/py.typed"):
        if "site-packages" in py_typed.parts:
            continue
        toplevel_pkg = py_typed.parent
        print(f"âŒ›ï¸ running mypy on '{toplevel_pkg}'", file=sys.stderr)
        package_errors = extract_errors(
            run_mypy(
                pkg=str(toplevel_pkg),
                config_file=config_file,
            )
        )
        if package_errors:
            print(
                f"found {len(package_errors)} errors in '{toplevel_pkg}'",
                file=sys.stderr,
            )
            print("\n".join(package_errors))
            errors.extend(package_errors)

    if errors:
        return 1
    return 0


if __name__ == "__main__":
    raise SystemExit(main())


--- internal/utils.py ---
import json
import re
import warnings
from enum import Enum
from pathlib import Path
from typing import Iterator, Optional

from pydantic import BaseModel

EXAMPLES_ROOT = Path(__file__).parent.parent


with warnings.catch_warnings():
    # This triggers some dumb warning in jupyter_core
    warnings.simplefilter("ignore")
    import jupytext
    import jupytext.config


class ExampleType(int, Enum):
    MODULE = 1
    ASSET = 2


class Example(BaseModel):
    type: ExampleType
    filename: str  # absolute filepath to example file
    module: Optional[str] = (
        None  # python import path, or none if file is not a py module.
    )
    # TODO(erikbern): don't think the module is used (by docs or monitors)?
    metadata: Optional[dict] = None
    repo_filename: str  # git repo relative filepath
    cli_args: Optional[list] = None  # Full command line args to run it
    stem: Optional[str] = None  # stem of path
    tags: Optional[list[str]] = None  # metadata tags for the example
    env: Optional[dict[str, str]] = None  # environment variables for the example
    runtimes: Optional[list[str]] = None  # list of Modal Function runtimes


_RE_NEWLINE = re.compile(r"\r?\n")
_RE_FRONTMATTER = re.compile(r"^---$", re.MULTILINE)
_RE_CODEBLOCK = re.compile(r"\s*```[^`]+```\s*", re.MULTILINE)


def render_example_md(example: Example) -> str:
    """Render a Python code example to Markdown documentation format."""

    with open(example.filename) as f:
        content = f.read()

    lines = _RE_NEWLINE.split(content)
    markdown: list[str] = []
    code: list[str] = []
    for line in lines:
        if line == "#" or line.startswith("# "):
            if code:
                markdown.extend(["```python", *code, "```", ""])
                code = []
            markdown.append(line[2:])
        else:
            if markdown and markdown[-1]:
                markdown.append("")
            if code or line:
                code.append(line)

    if code:
        markdown.extend(["```python", *code, "```", ""])

    text = "\n".join(markdown)
    if _RE_FRONTMATTER.match(text):
        # Strip out frontmatter from text.
        if match := _RE_FRONTMATTER.search(text, 4):
            text = text[match.end() + 1 :]

    if match := _RE_CODEBLOCK.match(text):
        filename = Path(example.filename).name
        if match.end() == len(text):
            # Special case: The entire page is a single big code block.
            text = f"""# Example ({filename})

This is the source code for **{example.module}**.
{text}"""

    return text


def gather_example_files(
    parents: list[str], subdir: Path, ignored: list[str], recurse: bool
) -> Iterator[Example]:
    config = jupytext.config.JupytextConfiguration(
        root_level_metadata_as_raw_cell=False
    )

    for filename in sorted(list(subdir.iterdir())):
        if filename.is_dir() and recurse:
            # Gather two-subdirectories deep, but no further.
            yield from gather_example_files(
                parents + [str(subdir.stem)], filename, ignored, recurse=False
            )
        else:
            filename_abs: str = str(filename.resolve())
            ext: str = filename.suffix
            if parents:
                repo_filename: str = (
                    f"{'/'.join(parents)}/{subdir.name}/{filename.name}"
                )
            else:
                repo_filename: str = f"{subdir.name}/{filename.name}"

            if ext == ".py" and filename.stem != "__init__":
                if parents:
                    parent_mods = ".".join(parents)
                    module = f"{parent_mods}.{subdir.stem}.{filename.stem}"
                else:
                    module = f"{subdir.stem}.{filename.stem}"
                data = jupytext.read(open(filename_abs), config=config)
                metadata = data["metadata"]["jupytext"].get("root_level_metadata", {})
                cmd = metadata.get("cmd", ["modal", "run", repo_filename])
                args = metadata.get("args", [])
                tags = metadata.get("tags", [])
                env = metadata.get("env", dict())
                runtimes = metadata.get("runtimes", ["gvisor"])
                yield Example(
                    type=ExampleType.MODULE,
                    filename=filename_abs,
                    metadata=metadata,
                    module=module,
                    repo_filename=repo_filename,
                    cli_args=(cmd + args),
                    stem=Path(filename_abs).stem,
                    tags=tags,
                    env=env,
                    runtimes=runtimes,
                )
            elif ext in [".png", ".jpeg", ".jpg", ".gif", ".mp4"]:
                yield Example(
                    type=ExampleType.ASSET,
                    filename=filename_abs,
                    repo_filename=repo_filename,
                )
            else:
                ignored.append(str(filename))


def get_examples() -> Iterator[Example]:
    """Yield all Python module files and asset files relevant to building modal.com/docs."""
    if not EXAMPLES_ROOT.exists():
        raise Exception(
            f"Can't find directory {EXAMPLES_ROOT}. You might need to clone the modal-examples repo there."
        )

    ignored = []
    for subdir in sorted(
        p
        for p in EXAMPLES_ROOT.iterdir()
        if p.is_dir()
        and not p.name.startswith(".")
        and not p.name.startswith("internal")
        and not p.name.startswith("misc")
    ):
        yield from gather_example_files(
            parents=[], subdir=subdir, ignored=ignored, recurse=True
        )


def get_examples_json():
    examples = list(ex.dict() for ex in get_examples())
    return json.dumps(examples)


if __name__ == "__main__":
    for example in get_examples():
        print(example.model_dump_json())


--- misc/README.md ---
# Miscellaneous Examples

This directory contains a variety of examples of ways to use Modal.

Unlike the examples in the rest of this repository, these examples are not
continually monitored for correctness, so it is possible that they may become
out of date or incorrect over time.

If you find an error in one of these examples, please report it in the issues
tab or, even better, submit a pull request to fix it.
