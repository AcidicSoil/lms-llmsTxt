# llms-full (private-aware)
> Built from GitHub files and website pages. Large files may be truncated.

--- pitch-deck-builder/references/data-visualization-guide.md ---
# Data Visualization Guide for Pitch Decks

This reference provides detailed guidance on creating effective charts and data visualizations for pitch presentations.

## Core Principles

### 1. Every Chart Must Tell a Story
A chart without a clear message is just decoration. Before creating any visualization:
- Identify the single most important insight
- Put that insight in the chart title
- Remove any elements that don't support the insight

**Bad title**: "Monthly Revenue"
**Good title**: "We've Tripled Revenue in 6 Months"

### 2. Maximize Signal-to-Noise Ratio
Edward Tufte's principle: maximize data ink, minimize non-data ink.

**Remove**:
- Heavy gridlines (use light gray if needed at all)
- 3D effects (they distort perception)
- Decorative borders
- Redundant labels
- Chart backgrounds
- Excessive precision (round to meaningful digits)

**Keep**:
- Data points
- Axis labels
- Key annotations
- Trend lines or highlights

### 3. Design for Glance Value
Investors may look at a slide for 5-10 seconds. Your chart should convey its message instantly.

**Techniques**:
- Use color to highlight the key data point
- Add a callout box with the headline number
- Draw attention with arrows or annotations
- Size elements proportionally to importance

## Chart Types and Use Cases

### Line Charts

**Best for**: Trends over time, continuous data, showing change

**When to use**:
- Revenue or user growth over months/years
- Metric comparisons over time (e.g., retention curves)
- Showing projections vs. actuals
- Demonstrating consistency or acceleration

**Design guidelines**:
- Maximum 3-4 lines (more becomes unreadable)
- Use different line styles (solid, dashed) for different categories
- Highlight your key line with bold or bright color
- Label endpoints directly on the chart (avoid legends when possible)
- Add data labels at key inflection points

**Example use**:
```
Title: "We've Grown 300% Year-Over-Year for 3 Consecutive Years"
Chart: Line showing revenue from 2022-2025
Annotation: Arrow pointing to 2024-2025 period with "40% CAGR"
```

**Common mistakes**:
- Starting Y-axis at arbitrary point (makes growth look artificial)
- Too many lines creating spaghetti chart
- Unlabeled lines requiring constant legend reference
- No clear visual hierarchy

### Bar Charts

**Best for**: Comparisons, categorical data, discrete values

**When to use**:
- Comparing your metrics to competitors
- Showing survey results or customer feedback scores
- Breaking down revenue by segment
- Comparing performance across categories

**Design guidelines**:
- Arrange bars by value (largest to smallest) unless order matters
- Horizontal bars work better for long labels
- Use consistent bar width
- Consider gradient fills for single-category charts
- Leave space between bars (typically 50% of bar width)

**Example use**:
```
Title: "We're 3x Faster Than Leading Competitors"
Chart: Horizontal bars showing process time
Your company: Bold color, leftmost (fastest)
Competitors: Muted colors
Annotation: "3x" difference highlighted
```

**Common mistakes**:
- Starting Y-axis above zero (distorts comparison)
- Too many bars (>8 becomes overwhelming)
- Using 3D bars (distorts values)
- Inconsistent colors without meaning

### Stacked Bar/Area Charts

**Best for**: Showing composition and how parts contribute to whole over time

**When to use**:
- Revenue breakdown by product line
- User acquisition by channel
- Cost structure evolution
- Market share changes

**Design guidelines**:
- Limit to 3-5 categories (more is unreadable)
- Place most important category at bottom (most stable baseline)
- Use distinct, high-contrast colors
- Consider 100% stacked for showing proportions
- Label each segment with values or percentages

**Example use**:
```
Title: "Enterprise Customers Now Drive 65% of Revenue"
Chart: Stacked area showing revenue by customer segment over time
Bottom layer: Enterprise (growing)
Top layers: SMB, Individual (stable or declining)
Annotation: Crossover point where Enterprise became majority
```

**Common mistakes**:
- Too many thin segments
- Similar colors making segments indistinguishable
- Not labeling the insight (e.g., which segment matters most)

### Pie/Donut Charts

**Best for**: Showing simple proportions (2-4 segments only)

**When to use**:
- Market share with few players
- Revenue split by major category
- Customer segment breakdown

**Design guidelines**:
- Maximum 4-5 segments (fewer is better)
- Start largest segment at 12 o'clock
- Arrange remaining segments clockwise by size
- Label directly on segments (not legend)
- Consider donut chart for cleaner look
- Put key number in donut center

**Example use**:
```
Title: "70% of Users Are in Our Target Enterprise Segment"
Chart: Donut with 70% highlighted in brand color
Center: "70% Enterprise"
Other segments: Muted colors
```

**WHEN TO AVOID**:
- More than 5 segments (use bar chart instead)
- Comparing multiple pies (use grouped bar chart instead)
- Showing trends (use line or area chart instead)
- When precise comparison is needed (pie segments are hard to compare)

### Scatter Plots

**Best for**: Positioning, correlations, competitive landscape

**When to use**:
- Competitive positioning (2×2 matrix)
- Customer segmentation
- Risk/return analysis
- Showing relationship between two variables

**Design guidelines**:
- Label axes clearly with what they measure
- Highlight your position with distinct marker
- Use quadrants to create meaning (high/low on each axis)
- Add trend line if showing correlation
- Size or color dots to add third dimension of data

**Example use**:
```
Title: "We Deliver Premium Quality at Mid-Market Pricing"
X-axis: Price Point
Y-axis: Quality Score
Your company: Large, bold marker in top-left (high quality, low price)
Competitors: Smaller dots in other quadrants
Quadrant labels: "Expensive & Good", "Cheap & Poor", etc.
```

**Common mistakes**:
- Unlabeled or unclear axes
- Too many points creating cloud
- Not highlighting your key position
- Arbitrary axis ranges hiding the story

### Waterfall Charts

**Best for**: Showing cumulative effect of sequential changes

**When to use**:
- Path from starting to ending value with intermediate steps
- Revenue bridge (how you get from current to projected)
- Cost breakdown showing how savings accumulate
- Showing factors contributing to growth

**Design guidelines**:
- Start with baseline value
- Show increases as bars going up (green)
- Show decreases as bars going down (red)
- End with final total
- Connect bars with lines showing flow
- Label each bar with value

**Example use**:
```
Title: "Path to $10M ARR by End of Year"
Starting point: Current $3M ARR
Positive bars: New Enterprise +$4M, Expansion +$2M, New Product +$1.5M
Negative bars: Churn -$0.5M
Ending point: $10M ARR
```

**Common mistakes**:
- Too many small steps making chart cluttered
- Unlabeled values requiring mental math
- Inconsistent time periods for each step

### Funnel Charts

**Best for**: Showing conversion through stages

**When to use**:
- Sales funnel (leads → customers)
- User onboarding flow
- Multi-step process with drop-offs

**Design guidelines**:
- Show stages from top to bottom
- Width represents volume at each stage
- Label each stage with count and percentage
- Highlight conversion rates between stages
- Use consistent color gradient (lighter to darker)

**Example use**:
```
Title: "Our Conversion Rate Beats Industry Average by 2x"
Stages:
- Leads: 10,000 (100%)
- Qualified: 3,000 (30%) [+50% vs industry]
- Proposals: 1,200 (12%) [+40% vs industry]
- Closed: 480 (4.8%) [+2x vs industry]
```

## Color Strategy

### Color Psychology
- **Blue**: Trust, stability, professional (finance, healthcare, enterprise)
- **Green**: Growth, success, positive (metrics, revenue, environmental)
- **Red**: Urgency, decline, negative (losses, churn, problems)
- **Orange**: Energy, innovation, consumer
- **Purple**: Premium, creative, luxury
- **Gray**: Neutral, secondary, comparison

### Color Usage Patterns

**Single-metric chart**:
- Use brand color for your primary data
- Use gray for comparisons or benchmarks
- Use accent color for highlights or callouts

**Multi-metric chart**:
- Primary metric: Brand color (bold)
- Secondary metric: Accent color
- Comparison/benchmark: Gray
- Negative data: Red
- Positive data: Green

**Sequential data** (e.g., time periods):
- Use gradient from light to dark of same color
- Latest/most important: Darkest
- Historical: Lighter shades

**Categorical data** (e.g., product lines):
- Use distinct, high-contrast colors
- Keep consistent colors across charts
- Consider colorblind-friendly palettes
- Limit to 5-6 colors maximum

### Accessible Color Combinations
Test colors with colorblind simulation tools. Safe combinations:
- Blue + Orange
- Blue + Red
- Purple + Green
- Magenta + Green

Avoid:
- Red + Green (most common colorblindness)
- Light shades only (low contrast)
- More than 6 distinct colors

## Typography for Data Visualization

### Hierarchy
**Chart title**: 24-32pt, bold, black
- Should state the insight, not just describe the chart
- Complete sentence format

**Axis labels**: 16-18pt, medium weight, dark gray
- Units in parentheses: "Revenue ($M)", "Users (thousands)"

**Data labels**: 14-20pt, depends on size
- Bold for emphasis on key points
- Gray for supporting data

**Annotations/callouts**: 14-18pt, bold for numbers
- Use to highlight specific insights
- Include context: "3x growth", "Industry avg: 25%"

### Readability Tips
- Never rotate text more than 45 degrees (horizontal is best)
- Use sentence case, not TITLE CASE
- Abbreviate carefully: k for thousands, M for millions, B for billions
- Round to 2 significant digits max: "$4.2M" not "$4,237,482"

## Layout and Composition

### Slide Layout Options

**Full-bleed chart**:
- Chart takes up entire slide
- Bold, impactful for key metrics
- Best for simple, high-impact visualizations

**Two-column layout**:
- Left: Text with key takeaways (30-40% width)
- Right: Chart visualization (60-70% width)
- Works well for complex charts needing explanation

**Title + chart + callouts**:
- Top: Headline and key insight
- Center: Chart (60% of slide)
- Bottom or side: Callout boxes with key numbers

### Spacing Guidelines
- Margins: Minimum 40px from slide edge
- Space between elements: 20-40px
- Chart padding: 20px inside chart area
- Text padding: 16px minimum

## Annotations and Callouts

### When to Annotate
Use annotations to:
- Highlight key data points
- Explain anomalies or inflection points
- Show comparisons to benchmarks
- Clarify trends or patterns

### Annotation Types

**Arrows**: Point to specific data points
- Use simple, clean arrows (not 3D)
- Keep arrow lines thin
- Position text near arrow head

**Callout boxes**: Emphasize specific numbers
- Use for the most important metric
- Large, bold number
- Brief context below: "3x industry average"

**Shaded regions**: Highlight time periods or zones
- Vertical bands for time periods (e.g., "Launch period")
- Horizontal bands for target ranges (e.g., "Profitability zone")
- Use subtle, transparent fills

**Trend lines**: Show overall direction
- Add R² value if showing correlation
- Label slope or growth rate
- Extend slightly beyond data for projection

### Best Practices
- Maximum 2-3 annotations per chart
- Use consistent styling for similar annotations
- Don't obscure data with annotations
- Make annotations large enough to read

## Chart Examples by Pitch Slide

### Traction Slide

**Revenue Growth**:
```
Chart type: Line chart with area fill
Title: "We're Growing 45% Month-Over-Month"
Design: 
- Bold line showing monthly revenue
- Shaded area under line (gradient from brand color)
- Data labels at key months
- Projection for next 6 months (dashed line)
Callout: Large "$2.4M ARR" in corner with "up from $400K 12 months ago"
```

**User Growth**:
```
Chart type: Stacked area chart
Title: "We've Acquired 50,000 Users Across 3 Cohorts"
Design:
- Bottom layer: Original beta users (stable)
- Middle layer: Cohort 2 (growing)
- Top layer: Latest cohort (fastest growth)
- Each with distinct color
Annotation: "Cohort 3 has 2x faster growth than Cohort 1"
```

### Market Slide

**Market Size**:
```
Chart type: Nested circles or horizontal bars
Title: "Our $8B Serviceable Market Is Growing 15% Annually"
Design:
- Outer circle: TAM ($50B)
- Middle circle: SAM ($8B) - highlighted
- Inner circle: SOM ($500M)
- Growth rate callout
```

### Competition Slide

**Competitive Positioning**:
```
Chart type: 2×2 scatter plot
Title: "We're the Only Solution Built for Mid-Market Teams"
Axes: X = Price Point, Y = Enterprise Features
Design:
- Your company: Large, bold marker with logo
- Competitors: Smaller dots with labels
- Quadrant labels showing market gaps
```

### Business Model Slide

**Unit Economics**:
```
Chart type: Waterfall chart
Title: "We Achieve 75% Gross Margin with Strong Unit Economics"
Design:
- Start: Revenue per customer ($10,000)
- Deduct: COGS bars (hosting, support, etc.)
- End: Gross profit ($7,500)
Callout: "LTV/CAC = 4.5:1"
```

## Common Mistakes to Avoid

### Mistake: Using Default Chart Styles
**Problem**: Excel/PowerPoint defaults look unprofessional
**Solution**: Customize colors, remove backgrounds, simplify gridlines

### Mistake: Too Much Precision
**Problem**: "$4,237,482.39" is hard to read and parse
**Solution**: "$4.2M" - round to significant digits

### Mistake: No Hierarchy
**Problem**: Everything same size, viewer doesn't know where to look
**Solution**: Make important elements larger, bolder, brighter

### Mistake: Cherry-Picked Scales
**Problem**: Y-axis starts at 90 to make 95 look huge
**Solution**: Start at 0 or clearly mark axis break

### Mistake: Decoration Over Data
**Problem**: 3D effects, shadows, excessive colors
**Solution**: Remove all non-data elements

### Mistake: No Context
**Problem**: "40% growth" without baseline or timeframe
**Solution**: "40% MoM growth from $1M to $1.4M"

### Mistake: Unclear Axis Labels
**Problem**: Unlabeled or ambiguous axes
**Solution**: Clear labels with units: "Revenue ($ Millions)"

### Mistake: Too Many Elements
**Problem**: Trying to show 10 product lines on one chart
**Solution**: Show top 3-4, combine rest into "Other"

## Quality Checklist

Before finalizing any chart:

**Clarity**:
- [ ] Chart title states the insight, not just description
- [ ] Axes are clearly labeled with units
- [ ] Key data points are annotated
- [ ] Font size readable from 10 feet away (minimum 16pt)

**Design**:
- [ ] Colors are consistent with brand and meaningful
- [ ] Removed all chart junk (gridlines, borders, 3D effects)
- [ ] Sufficient whitespace around and within chart
- [ ] Visual hierarchy guides eye to key information

**Accuracy**:
- [ ] Data is current and correctly represented
- [ ] Scales are appropriate (not misleading)
- [ ] Source is cited if external data
- [ ] Projections are clearly marked as such

**Accessibility**:
- [ ] Color combinations work for colorblind viewers
- [ ] Labels don't overlap or crowd
- [ ] Chart type appropriate for the data
- [ ] Can be understood in grayscale (for printing)


--- learning-capture/references/decision-examples.md ---
# Decision Examples

This document provides concrete examples of when to offer skill capture vs. when to continue without offering.

## Context Window ROI Calculation

**Key insight**: A skill is worth capturing if:
- It will be reused 10+ times over multiple conversations
- Each reuse saves 500+ tokens of re-explanation
- The skill itself costs <5000 tokens to load

**ROI formula**: (Reuse_frequency × Tokens_saved) - Skill_token_cost > 0

## High-Confidence Capture Scenarios (OFFER)

### Example 1: Third Instance of Similar Task

**Context**: User has asked me to write internal comms docs three times, each with similar structure (context → update → action items → FAQ)

**Signal**: 
- Pattern has repeated 3 times ✓
- Structure is consistent ✓
- Likely to recur (ongoing company communications) ✓
- Each time requires re-explaining structure (~800 tokens) ✓

**Offer**: "I notice I've structured the last three internal comms documents in a similar way. Would it be helpful to capture this as a skill for future internal communications?"

**Expected ROI**: High - saves ~800 tokens per future use, likely 20+ uses

---

### Example 2: Complex Domain Knowledge Synthesis

**Context**: Through several conversations, user has explained their data pipeline architecture, table schemas, and common query patterns

**Signal**:
- Information spans multiple conversations ✓
- Requires significant context to re-explain (2000+ tokens) ✓
- User frequently asks data-related questions ✓
- Knowledge is stable (schemas don't change weekly) ✓

**Offer**: "I've built up understanding of your data architecture across our conversations. Would it be useful to formalize this as a skill so I can reference your schemas and patterns more efficiently in future conversations?"

**Expected ROI**: Very high - saves 2000+ tokens, dozens of potential uses

---

### Example 3: Novel Workflow with Clear Reusability

**Context**: User asked me to analyze a messy CSV with inconsistent formatting. I developed a multi-step validation → cleaning → analysis approach that worked well.

**Signal**:
- Workflow is novel and non-obvious ✓
- Problem type is clearly recurring (user mentions "I have lots of these CSVs") ✓
- Approach requires 1000+ tokens to explain each time ✓
- Generalizes beyond this specific file ✓

**Offer**: "The validation and cleaning approach I used here seems like it would work well for your other messy data files. Should I capture this as a skill for handling similar datasets?"

**Expected ROI**: High - user explicitly mentioned recurring need, saves 1000+ tokens per use

---

### Example 4: Workflow Optimization Pattern

**Context**: User asked me to generate a report from data. I developed an efficient pattern: fetch data → validate → analyze → format → generate visual summary. User seemed pleased with the comprehensive output.

**Signal**:
- Workflow chains multiple steps efficiently ✓
- User asked similar question before ✓
- Pattern produces notably better results than simpler approaches ✓
- Easy to apply to similar reporting tasks ✓

**Offer**: "I developed a multi-step reporting workflow here that seems to work well. Would you like me to formalize this for future data reporting tasks?"

**Expected ROI**: Medium-high - saves ~600 tokens, 10+ potential uses

## Low-Confidence Scenarios (DO NOT OFFER)

### Example 5: One-Off Creative Solution

**Context**: User asked me to generate a haiku about their cat Mr. Whiskers. I wrote a creative haiku.

**Signal**:
- ✗ Task is highly specific to this instance
- ✗ No pattern of repetition
- ✗ Solution doesn't generalize (writing haikus about cats generally? too broad)
- ✗ Low reuse probability

**Action**: Do NOT offer capture. This is creative work, not a reusable pattern.

---

### Example 6: Simple Task I Already Know

**Context**: User asked me to format some text as a bulleted list.

**Signal**:
- ✗ This is basic functionality I already have
- ✗ No novel approach involved
- ✗ No meaningful tokens to save

**Action**: Do NOT offer capture. No marginal value over existing capabilities.

---

### Example 7: Context-Specific Solution

**Context**: User asked me to fix a specific bug in their Python code related to a third-party library version conflict.

**Signal**:
- ✗ Highly context-specific to this codebase and library version
- ✗ Unlikely to recur in identical form
- ✗ Solution doesn't generalize to other contexts
- ✗ Low reusability

**Action**: Do NOT offer capture. Too specific to be reusable.

---

### Example 8: Ambiguous Reusability

**Context**: User asked me to draft an email to their manager about a project delay. I wrote a clear, professional email.

**Signal**:
- ✗ First time doing this type of task
- ? Might recur, but not clear (is this a pattern or one-off?)
- ✗ No strong signal of repetition yet

**Action**: Do NOT offer capture yet. Wait to see if they ask for similar emails 2+ times before offering.

---

### Example 9: Already Covered by Existing Skills

**Context**: User asked me to create a presentation with company branding. I used the brand-guidelines skill.

**Signal**:
- ✗ Existing skill already handles this
- ✗ No novel approach beyond what skill provides

**Action**: Do NOT offer capture. Would be redundant with existing skill.

## Recognition Patterns

### Strong Reusability Signals

✓ "I have lots of these [files/tasks/problems]"
✓ "We do this [weekly/monthly/regularly]"
✓ "Can you do the same thing you did for [previous task]?"
✓ Third or fourth instance of similar task
✓ User explicitly mentions this is an ongoing need
✓ Complex domain knowledge spanning multiple conversations

### Weak Reusability Signals

✗ First instance of a task
✗ Highly creative or one-off request
✗ Simple task using existing capabilities
✗ User says "just this once" or similar
✗ Context-specific solution that won't generalize

## Decision Threshold

**Offer skill capture when**:
- Confidence that it will save 10%+ of context window over 10+ uses: >95%
- AND one of:
  - Pattern has repeated 2+ times already
  - User explicitly indicates recurring need
  - Complex domain knowledge worth formalizing
  - Novel workflow with clear generalizability

**Do not offer when**:
- First instance of a pattern
- Highly context-specific
- Trivial or already well-covered by existing capabilities
- Creative/one-off work
- Ambiguous reusability


--- job-search-strategist/references/templates-and-examples.md ---
# Templates, Examples, and Sample Assets

This reference provides concrete templates, example messages, and sample frameworks to make the job search strategy immediately actionable.

## Self-Diagnostic Tool: Where's Your Search Breaking Down?

Use this rubric to identify which phase needs attention. Rate yourself 1-5 on each dimension:

### Clarity Diagnostics
- [ ] I can explain my target role in one sentence (1 = no clarity, 5 = crystal clear)
- [ ] I know exactly what problems I solve and for whom (1 = vague, 5 = specific)
- [ ] I can list 3-5 companies that would be perfect fits (1 = no targets, 5 = researched list)
- [ ] I understand what makes me different from other candidates (1 = generic, 5 = unique value clear)

**Score < 12**: Your search lacks direction. Start with Phase 1 research and Phase 2 skills mapping.

### Proof Diagnostics
- [ ] I have quantified achievements with specific metrics (1 = none, 5 = multiple)
- [ ] My LinkedIn/portfolio demonstrates my best work (1 = outdated, 5 = impressive)
- [ ] I can tell compelling stories about my impact (1 = struggle with examples, 5 = confident)
- [ ] I have testimonials, recommendations, or social proof (1 = none, 5 = strong validation)

**Score < 12**: You need to build credibility assets. Focus on Phase 3 portfolio development.

### Distribution Diagnostics
- [ ] I'm reaching decision-makers, not just applying online (1 = applications only, 5 = multi-channel)
- [ ] I have warm connections at target companies (1 = cold only, 5 = referrals)
- [ ] My outreach messages get responses (1 = crickets, 5 = >30% response rate)
- [ ] I'm tracking my pipeline and conversion rates (1 = no tracking, 5 = detailed metrics)

**Score < 12**: Your distribution is weak. Prioritize Phase 4 creative strategy and outreach.

### Overall Health Score
- **48-60**: Strong position, focus on optimization
- **36-47**: Solid foundation, address specific gaps
- **24-35**: Significant weaknesses, systematic rebuild needed
- **12-23**: Start from scratch with clarity work

## Decision Matrix: Prioritizing Target Roles

Use this framework to evaluate opportunities:

| Company/Role | Fit (1-10) | Leverage (1-10) | Energy (1-10) | Total | Priority |
|--------------|------------|-----------------|---------------|-------|----------|
| Example Corp PM Role | 8 | 6 | 9 | 23 | High |
| Startup Designer | 6 | 9 | 7 | 22 | Medium |
| BigCo Analyst | 9 | 4 | 5 | 18 | Low |

**Fit**: Do your skills and experience directly match their needs?
**Leverage**: Do you have connections, unique skills, or timing advantages?
**Energy**: Are you genuinely excited about this opportunity?

**Target**: 20+ total score. Prioritize roles scoring 23+.

## Job Search Operating Rhythm

### Daily (30-60 minutes)
- [ ] Review 2-3 new job postings with red/green flag analysis
- [ ] Send 2-3 personalized outreach messages (LinkedIn, email, or comments)
- [ ] Engage with 5-10 posts from target companies or hiring managers (thoughtful comments)
- [ ] Update your tracking spreadsheet with responses and next steps

### Weekly (2-3 hours)
- [ ] Deep research on 1-2 priority companies (Phase 1 full analysis)
- [ ] Apply to 3-5 strategically selected roles (not spray-and-pray)
- [ ] Schedule/conduct 2+ informational interviews or networking calls
- [ ] Create or improve one portfolio piece, blog post, or proof asset
- [ ] Review pipeline: conversion rates, what's working, what's not

### Monthly (4-6 hours)
- [ ] Comprehensive pipeline review: applications → responses → interviews → offers
- [ ] Update resume and LinkedIn based on what's resonating
- [ ] Expand network: attend 1-2 industry events or virtual meetups
- [ ] Skill development: complete one course module or certification milestone
- [ ] Strategy adjustment: what tactics are working? What should you stop doing?

### Pipeline Metrics to Track
- **Top of Funnel**: Applications sent, outreach messages, connection requests
- **Middle of Funnel**: Response rate, informational chats, recruiter screens
- **Bottom of Funnel**: Interviews, final rounds, offers
- **Conversion Rates**: Applications → screens (target: 10-20%), screens → interviews (target: 30-50%)

## Message Templates

### LinkedIn Connection Request (No Prior Interaction)

```
Hi [Name],

I've been following [Company]'s work on [specific thing], particularly [recent development]. I'm exploring [role type] opportunities and would love to learn more about the [team/culture/product].

Would you be open to a brief chat about your experience there?

[Your Name]
```

**Why it works**: Specific (not generic), focused on learning (not asking), brief.

### LinkedIn Message (After Connection Accepted)

```
Thanks for connecting, [Name]!

I'm particularly interested in [Company] because [specific reason tied to your background]. I noticed you work on [their team/project] — that's exactly the kind of [problem/domain] I've been focused on at [your experience].

Any chance you'd have 15 minutes for a quick call this week? I'd love to hear about [specific question about their work].
```

**Why it works**: Shows you did homework, mutual relevance, low time ask.

### Warm Referral Request (To Your Connection)

```
Hi [Connection],

Hope you're well! I'm reaching out because I'm really interested in the [Role Title] position at [Company]. I've been [brief context of your relevant experience].

I noticed you know [Employee Name] on LinkedIn. Would you feel comfortable introducing us? I'd just love to learn more about the team and role before applying.

I can draft an intro message if that helps make it easy!
```

**Why it works**: Specific ask, makes it easy for them, shows respect for their time.

### Direct Email to Hiring Manager

**Subject**: [Specific Value] for [Team/Initiative Name]

```
Hi [Name],

I saw that [Company] recently [specific news/product launch/announcement]. Having spent [X years] doing [relevant experience], I'm excited about the [Role Title] opportunity.

Quick snapshot of relevant background:
• [Specific achievement with metric]
• [Relevant skill/experience that matches their need]
• [Unique angle that most candidates won't have]

I built [small project/analysis] related to [their product/market] — would love to share it with you: [link]

Are you open to a brief conversation about the role?

Best,
[Your Name]
```

**Why it works**: Demonstrates initiative, specific value, social proof through work, brief.

### Follow-Up (After No Response - 1 Week Later)

```
Hi [Name],

Following up on my note from last week about the [Role Title] position.

I wanted to add — I just [recent accomplishment/published something/completed relevant project] that's directly relevant to [their challenge/team need].

Still interested in connecting if you have 15 minutes this week.

[Your Name]
```

**Why it works**: Adds new information (not just "bumping"), stays brief, reiterates interest.

## Positioning Statement Templates

### One-Sentence Value Proposition

**Formula**: "I help [target audience] [achieve outcome] through [unique approach/skill]"

**Examples**:
- "I help B2B SaaS companies increase user activation rates through data-driven product experimentation"
- "I help early-stage startups build scalable growth operations from zero to Series A"
- "I help healthcare organizations implement AI/ML solutions that improve clinical outcomes"

**Your turn**: Complete this sentence clearly and specifically.

### Three-Sentence Career Story

**Formula**: 
1. Where you've been + what you built
2. What you learned/became excellent at
3. What you're looking for next and why

**Example**:
"I spent 5 years at Microsoft building developer tools, where I shipped features used by 50K+ developers and led a team of 8. Through that experience, I became excellent at translating complex technical requirements into intuitive user experiences. I'm now looking to join an early-stage dev tools startup where I can own product strategy end-to-end and directly shape company direction."

## Case Study Examples

### Case Study 1: Marketing Manager → Tech Product Manager

**Starting Position**:
- 6 years marketing experience at traditional retail company
- No formal product management experience
- Wanted to break into tech PM roles
- Clarity score: 14, Proof score: 11, Distribution score: 9

**Strategy Applied**:
1. **Phase 1 Analysis**: Researched 20 product companies, identified 5 that valued marketing background (consumer apps, B2B with PLG motion)
2. **Phase 2 Skills Mapping**: Reframed experience as "user research," "go-to-market strategy," "experimentation" rather than just "marketing"
3. **Phase 3 Portfolio Development**: Built 2 mock product specs for target companies' features, published on Medium with thoughtful analysis
4. **Phase 4 Creative Strategy**: 
   - LinkedIn outreach to PMs who came from marketing (found 12, got 6 informational chats)
   - Portfolio project shared directly with hiring managers
   - Referral from informational interview contact

**Timeline**: 8 weeks from start to offer
**Key Tactic**: Portfolio projects demonstrated product thinking without formal experience
**Result**: APM role at Series B SaaS company, $140K base

### Case Study 2: Senior Engineer → Technical Startup Founder Outreach

**Starting Position**:
- 8 years engineering at FAANG
- Looking for founding engineer role at AI startup
- Strong technical skills but limited startup connections
- Clarity score: 18, Proof score: 19, Distribution score: 12

**Strategy Applied**:
1. **Phase 1 Analysis**: Deep research on 15 seed-stage AI startups, identified 3 with problems matching his expertise (MLOps, infrastructure)
2. **Phase 2 Skills Mapping**: Highlighted experience building systems from scratch, not just working in established codebases
3. **Phase 3 Portfolio Development**: Already strong technical portfolio, added AI blog posts
4. **Phase 4 Creative Strategy**:
   - Twitter engagement with startup founders (6 weeks of thoughtful comments)
   - Built small open-source tool related to their tech stack
   - Direct email to founders with technical analysis of their architecture
   - Attended YC Demo Day virtually, followed up with companies in his domain

**Timeline**: 4 weeks from targeted outreach to offer conversations (3 simultaneous)
**Key Tactic**: Technical blog + open source contributions created credibility before outreach
**Result**: Founding engineer at seed-stage AI company, equity package

### Case Study 3: Mid-Career Academic → Nonprofit Program Director

**Starting Position**:
- PhD + 5 years university research/teaching
- Wanted to move to nonprofit sector
- Strong mission alignment, weak nonprofit connections
- Clarity score: 16, Proof score: 13, Distribution score: 8

**Strategy Applied**:
1. **Phase 1 Analysis**: Researched 10 education-focused nonprofits, identified culture alignment through Glassdoor and employee posts
2. **Phase 2 Skills Mapping**: Reframed academic experience as "program design," "stakeholder management," "impact measurement"
3. **Phase 3 Skill Development**: Completed nonprofit management certificate online (8 weeks)
4. **Phase 4 Creative Strategy**:
   - Published LinkedIn article analyzing challenges in education policy (their focus area)
   - Volunteered with related nonprofit for 2 months to build proof
   - Alumni network: found 3 alums at target nonprofits, requested informational chats
   - Included volunteer metrics in application (reached 500 students)

**Timeline**: 12 weeks (longer due to certificate completion)
**Key Tactic**: Volunteer work + certificate addressed "no nonprofit experience" objection
**Result**: Program Director role at education nonprofit, mission-aligned work

## KPI Tracking Spreadsheet Template

| Date | Activity | Company | Contact | Channel | Response? | Next Step | Status |
|------|----------|---------|---------|---------|-----------|-----------|--------|
| 10/15 | LinkedIn message | Acme Corp | Jane Smith | LinkedIn | Yes | Call scheduled 10/22 | Active |
| 10/15 | Application | Beta Inc | N/A | Website | No | Follow up 10/22 | Waiting |
| 10/16 | Warm intro | Gamma Co | John Doe | Email | Yes | Sent resume | Active |

**Weekly Rollup**:
- Outreach sent: [number]
- Responses received: [number]
- Response rate: [percentage]
- Conversations scheduled: [number]
- Interviews scheduled: [number]
- Offers received: [number]

**Conversion Tracking**:
- Applications → Screens: [X/Y = Z%] (target: 10-20%)
- Screens → Interviews: [X/Y = Z%] (target: 30-50%)
- Interviews → Offers: [X/Y = Z%] (target: 20-30%)

## Visual Framework Descriptions

### Three Levers of Job Search Efficiency

```
┌─────────────────────────────────────────────┐
│            JOB SEARCH SUCCESS               │
│                                             │
│   ┌─────────┐  ┌─────────┐  ┌──────────┐  │
│   │ CLARITY │  │  PROOF   │  │DISTRIBU- │  │
│   │         │→ │          │→ │  TION    │  │
│   │ Who you │  │ Evidence │  │ Reaching │  │
│   │ serve & │  │ of value │  │ decision │  │
│   │ what    │  │          │  │ makers   │  │
│   │ value   │  │          │  │          │  │
│   └─────────┘  └─────────┘  └──────────┘  │
│                                             │
│   Without clarity, your proof is unfocused  │
│   Without proof, distribution is ignored    │
│   Without distribution, nothing happens     │
└─────────────────────────────────────────────┘
```

### Four-Phase Process Flow

```
Phase 1: ANALYSIS
    ↓ (Output: Company scorecard, red/green flags)
Phase 2: SKILLS MATCHING
    ↓ (Output: Skills matrix, gap analysis)
Phase 3: SKILL DEVELOPMENT
    ↓ (Output: Learning plan, portfolio pieces)
Phase 4: CREATIVE STRATEGY
    ↓ (Output: Multi-channel campaign plan)
    ↓
[APPLY] → [TRACK] → [ITERATE]
    ↑__________________|
```

### Strategy Prioritization Model

```
Tactic Selection = (Company Culture × 40%) 
                 + (Candidate Strengths × 40%) 
                 + (Job Level × 20%)

Example:
Creative startup + Video skills + Mid-level
= Video cover letter (High priority)

Enterprise B2B + Strong network + Senior-level  
= Referral hunting (High priority)
```

## Application Materials Frameworks

### Resume Bullet Formula

**Action Verb + Specific Context + Quantified Result**

❌ Weak: "Managed marketing campaigns"
✅ Strong: "Led 5 paid acquisition campaigns that generated 12K qualified leads, reducing CAC by 30%"

❌ Weak: "Improved customer satisfaction"
✅ Strong: "Redesigned onboarding flow, increasing NPS from 45 to 72 and reducing support tickets by 40%"

### Cover Letter Structure

**Paragraph 1** (Hook - 2-3 sentences):
Why THIS company excites you (be specific, reference recent news/product)

**Paragraph 2** (Fit - 3-4 sentences):
Your most relevant experience, with 2 concrete examples matching their needs

**Paragraph 3** (Unique Value - 2-3 sentences):
What you bring that most candidates won't (transferable skill, unique background, specific insight)

**Paragraph 4** (Close - 1-2 sentences):
Enthusiasm + clear call to action

**Total length**: 300-400 words maximum

## Conversational Interview Question Bank

When conducting Phase 2 skills matching, use these to dig deeper:

**For Each Key Requirement**:
- "Tell me about a time when you [requirement]. What was the situation?"
- "Walk me through your process for [relevant task]."
- "What was the most challenging aspect of [relevant domain]? How did you handle it?"
- "If you were [job requirement], what would your first 30 days look like?"

**For Transferable Skills**:
- "Even though you worked in [other industry], did you face [similar challenge]?"
- "The core skill here is really [abstracted skill]. Where have you demonstrated that?"
- "How would your approach from [previous context] apply to [new context]?"

**For Gaps**:
- "Is [missing skill] something you're interested in developing?"
- "Have you done anything adjacent to [requirement]?"
- "How would you approach learning [gap area] quickly?"

**For Unique Value**:
- "What's something about your background that isn't obvious from your resume?"
- "What unique perspective would you bring to this problem?"
- "What have you done that most other candidates probably haven't?"

## Next Steps After Using This Reference

1. **Complete self-diagnostic** → identify your weak phase
2. **Implement operating rhythm** → daily/weekly/monthly structure
3. **Build tracking spreadsheet** → start measuring conversion rates
4. **Adapt templates** → personalize for your voice and situation
5. **Run one complete cycle** → pick one role, go through all four phases
6. **Measure and iterate** → what worked? What didn't? Adjust.

Remember: This is an operating system, not a checklist. Adapt based on what's working in your specific market and situation.


--- skill-doc-generator/references/readme-template.md ---
# README Template Reference

This document defines the standard structure for auto-generated skill READMEs.

## Structure

A well-formed README contains these sections in order:

### 1. Title and Description (Required)
```markdown
# {skill-name}

> {description from frontmatter}
```

### 2. Overview (Required)
Brief introduction to the skill's purpose and capabilities. Typically 1-3 paragraphs.

### 3. When to Use This Skill (Required)
Explicit trigger scenarios that help users understand when to invoke the skill. Should include:
- Common use cases
- Trigger phrases extracted from description
- Example queries

### 4. Skill Structure (Required)
Quick stats about the skill:
- Lines of documentation
- Number of sections
- Number of code examples
- Resource counts

### 5. Bundled Resources (If Present)
Organized by resource type:
- **Scripts** - Executable code
- **Reference Documentation** - Additional docs to load as needed
- **Assets** - Templates, images, fonts, etc.

Each resource listed with a link to the actual file.

### 6. Key Sections (Optional)
List of main documentation sections for quick navigation. Limit to top 5 most important sections.

### 7. Usage Examples (If Present)
Concrete code examples extracted from SKILL.md. Include:
- Language tag
- Actual working code
- Maximum 3 examples to keep README focused

### 8. Quality Validation (Optional)
Validation results showing:
- Pass/fail status
- Error count
- Warning count
- Collapsible details with specific issues

### 9. Footer (Required)
```markdown
---

_Documentation auto-generated from `SKILL.md`_
```

## Best Practices

### Conciseness
READMEs should be scannable. Keep descriptions brief and use formatting to highlight key information.

### Links
Always link to actual resources using relative paths. This makes the README actionable.

### Examples
Choose the most representative examples. Quality over quantity - 3 good examples beat 10 mediocre ones.

### Validation
Including validation results builds trust and helps maintain quality standards across skills.


--- token-budget-advisor/references/advanced-patterns.md ---
# Advanced Chunking Patterns Reference

This reference provides additional chunking strategies and decision frameworks for complex scenarios not covered in the main skill.

## Advanced Chunking Strategies

### 6. Layered Abstraction

**Best for:** Tasks requiring different levels of detail for different audiences

**Example scenario:** Creating both executive summary and detailed technical analysis

**Approach:**
- Session 1: High-level executive summary (research + draft)
- Session 2: Deep technical details (research + draft)
- Session 3: Supporting appendices and data tables

**Benefits:** Allows prioritization of most important layer first, can stop after any session if time-constrained

### 7. Hub-and-Spoke

**Best for:** Central theme with multiple independent supporting analyses

**Example scenario:** Company overview with deep dives into each business unit

**Approach:**
- Session 1: Core company profile (central hub)
- Session 2-N: Individual business unit analyses (spokes)
- Final session: Integration and cross-unit insights

**Benefits:** Hub provides context for all spokes; spokes can be tackled in any order

### 8. Dependency-Aware Sequencing

**Best for:** Multi-step processes where later steps depend on earlier decisions

**Example scenario:** Market entry strategy requiring market research → target selection → go-to-market plan

**Approach:**
- Explicitly map dependencies upfront
- Complete foundational analysis first
- Use findings to inform subsequent sessions
- Document decisions between sessions

**Benefits:** Ensures quality by building on validated foundations

### 9. Spiral Development

**Best for:** Complex creative projects benefiting from multiple refinement passes

**Example scenario:** Building a comprehensive training curriculum

**Approach:**
- Pass 1: Complete curriculum structure at high level (all modules outlined)
- Pass 2: Develop 50% of content in depth
- Pass 3: Develop remaining 50% in depth
- Pass 4: Quality pass and polish

**Benefits:** Maintains holistic view while making steady progress; can adjust scope mid-project

### 10. Resource Preloading

**Best for:** Tasks requiring heavy context that will be referenced throughout

**Example scenario:** Analyzing code changes across a large codebase

**Approach:**
- Session 0: Load and understand architecture/key files (no deliverable)
- Session 1-N: Actual analysis work with preloaded context
- Optimization: Summarize key context to carry forward efficiently

**Benefits:** Amortizes context loading cost across multiple work sessions

## Decision Framework for Strategy Selection

### Task Classification Matrix

| Task Characteristics | Recommended Strategy | Second Choice |
|---------------------|---------------------|---------------|
| Time-ordered data | Sequential Processing | Hub-and-Spoke |
| Multi-dimensional analysis | Dimensional Breakdown | Layered Abstraction |
| Large volume, pattern-finding | Subset Sampling | Sequential Processing |
| Independent parallel work | Parallel Track | Hub-and-Spoke |
| Requires iteration | Depth Progression | Spiral Development |
| Complex dependencies | Dependency-Aware | Sequential Processing |
| Mixed audience needs | Layered Abstraction | Depth Progression |
| Central + supporting topics | Hub-and-Spoke | Dimensional Breakdown |

### Hybrid Approaches

Real-world tasks often benefit from combining strategies:

**Example: Quarterly business review across 5 departments**

Hybrid approach:
1. Sequential (by quarter) + Parallel Track (by department)
2. Structure: Q1 Dept A-B, Q1 Dept C-E, Q2 Dept A-B, Q2 Dept C-E, etc.

**Example: Product comparison with deep research**

Hybrid approach:
1. Depth Progression (outline → content) + Parallel Track (by product)
2. Structure: Outline all products, develop Product A deep, develop Product B deep, synthesize

## Estimation Refinement Techniques

### Interactive Scoping

When task size is unclear, use a scoping conversation:

```
"Before I start, let me understand the scope to plan this well:
- Depth: Are you looking for [surface-level summary] or [detailed analysis with citations]?
- Breadth: Should I cover [subset] or [comprehensive coverage]?
- Output: Do you need [bullet points] or [formatted report/presentation]?

Your answers help me estimate whether we should chunk this task."
```

### Token Checkpoints

For borderline tasks, establish checkpoints:

```
"This might be achievable in one session, but it's close. I'll:
1. Complete the first major section
2. Check token usage at that point
3. Decide whether to continue or split remaining work

Sound good?"
```

### Progressive Disclosure with User

Don't present all options at once:

```
"This will exceed our token budget. I see two promising approaches:
A. Split by time period (quarters)
B. Split by analysis type (quantitative vs qualitative)

Which makes more sense for your needs?"
```

## Chunking Anti-Patterns

Avoid these common mistakes:

### ❌ Arbitrary Splits

**Bad:** "Let's do half now and half later"
**Why it fails:** No logical boundary, hard to resume coherently
**Better:** Identify natural boundaries (themes, time periods, categories)

### ❌ Unequal Chunks

**Bad:** Session 1 is 80% of work, Session 2 is 20%
**Why it fails:** Defeats purpose of chunking; Session 1 still exceeds budget
**Better:** Balance work across sessions to ~60-70% of budget each

### ❌ Chunk Interdependence

**Bad:** Session 2 requires re-analyzing everything from Session 1
**Why it fails:** Duplicates work, compounds token costs
**Better:** Design chunks to be relatively self-contained

### ❌ No Synthesis Plan

**Bad:** Create 5 independent analyses with no plan to integrate
**Why it fails:** User left to connect dots; misses overarching insights
**Better:** Always include final integration/synthesis session

### ❌ Over-Chunking

**Bad:** Split a 70% budget task into 4 mini-sessions
**Why it fails:** Overhead of context switching; destroys flow
**Better:** Only chunk when necessary (80%+ budget risk)

## Context Handoff Techniques

### Effective Session Summaries

At end of each chunk, provide:

1. **What was completed:** Specific deliverables and findings
2. **Key insights:** 3-5 critical points to carry forward
3. **Next session focus:** Clear starting point
4. **Open questions:** Unresolved items to address

**Example:**
```
Session 1 complete. We analyzed Q1-Q2 revenue data and found:
- 15% growth in Q2 driven by enterprise segment
- Consumer segment flat YoY
- Churn increased 3% in May (investigate further)

Next session: Analyze Q3-Q4 with focus on:
- Did enterprise growth continue?
- What happened with consumer rebound efforts?
- Did churn stabilize?
```

### User-Driven Handoffs

Coach users on what to say when resuming:

```
"When you're ready for Part 2, start a new conversation and say:
'Continue the Q3-Q4 analysis. In Part 1, we found [X, Y, Z]. Here are the Q3-Q4 files: [upload]'

This gives me the context I need to pick up smoothly."
```

## Special Considerations

### Working with External Tools

When chunking involves external tool research:

- **Front-load research**: Do heavy web searches early to inform all sessions
- **Document sources**: Save URLs/citations to reference across sessions
- **Cache findings**: Summarize research to avoid re-searching same topics

### Artifact-Heavy Tasks

When creating large artifacts (decks, documents):

- **Template first**: Establish structure/template in Session 1
- **Parallel development**: Can work on different sections independently
- **Final polish separate**: Reserve last session for formatting/refinement

### Real-Time Collaboration

If user wants to work synchronously through chunks:

- **Time-box sessions**: "Let's spend ~15 min on each section"
- **Quick transitions**: Minimize handoff overhead between chunks
- **Live pivots**: Be ready to adjust plan based on findings

## Measuring Success

Good chunking strategies should:

✅ Complete within token budget for each session
✅ Produce valuable interim deliverables (not just work-in-progress)
✅ Minimize redundant work across sessions
✅ Feel natural to resume (clear continuity)
✅ Allow flexibility to reprioritize between sessions

Poor chunking feels like:

❌ Constantly running out of tokens mid-session
❌ Extensive recap needed to resume work
❌ Deliverables only make sense when all chunks complete
❌ Frequent context re-loading of same information


--- resume-builder/references/ats-optimization.md ---
# ATS (Applicant Tracking System) Optimization

## Understanding ATS

**What is an ATS?**
Applicant Tracking Systems are software applications that help companies manage recruitment. They scan, parse, and rank resumes before human review. An estimated 75%+ of resumes never reach human eyes due to ATS filtering.

**How ATS Works:**
1. **Parsing**: Extracts data from resume (name, contact info, work history, education, skills)
2. **Keyword matching**: Compares resume content against job description requirements
3. **Ranking**: Scores candidates based on keyword matches and relevance
4. **Filtering**: May automatically reject candidates below certain thresholds

**Major ATS Platforms:**
- Workday
- Taleo (Oracle)
- Greenhouse
- Lever
- iCIMS
- Jobvite
- SAP SuccessFactors
- BambooHR

## ATS-Friendly Formatting

### Structure and Layout

**✅ ATS-Friendly Practices:**
- Use standard section headings: "Work Experience," "Education," "Skills"
- Left-aligned text
- Simple, single-column layout
- Standard fonts: Arial, Calibri, Helvetica, Times New Roman, Garamond
- Font size: 10-12pt for body, 14-16pt for headings
- Consistent bullet points (•, -, or – throughout)
- Clear visual hierarchy with white space
- Standard margins (0.5-1 inch)
- Traditional chronological or combination format

**❌ ATS-Unfriendly Practices:**
- Multiple columns
- Text boxes
- Tables for layout
- Headers and footers containing important information
- Images or graphics
- Charts or graphs
- Decorative lines or borders
- Fancy fonts or excessive formatting
- Color-coded sections (some ATS can't interpret color)
- Text embedded in images
- Unusual bullet symbols or characters

### File Format Best Practices

**Preferred Format: PDF**
- Modern ATS can parse PDF files effectively
- Preserves formatting across devices
- Professional appearance
- Ensure PDF is text-based, not an image scan

**Alternative: DOCX**
- Use if specifically requested
- Microsoft Word format (.docx) is widely compatible
- Ensure no special formatting that might break

**Never Submit:**
- .pages (Mac-specific)
- .jpg or .png images of resume
- Password-protected files
- Zip files containing resume

### Section Headers

**Standard Headers (ATS recognizes):**
- Work Experience / Professional Experience / Experience / Employment History
- Education / Academic Background
- Skills / Core Competencies / Technical Skills
- Certifications / Professional Certifications
- Summary / Professional Summary / Profile

**Avoid Creative Headers:**
- "Where I've Been" instead of "Experience"
- "My Toolkit" instead of "Skills"
- "What I Bring to the Table" instead of "Summary"

## Keyword Optimization

### Strategic Keyword Placement

**Priority Placement:**
1. **Professional Summary**: Include 3-5 key role-specific keywords
2. **Skills Section**: List all relevant keywords from job description
3. **Work Experience**: Integrate keywords naturally into achievement bullets
4. **Throughout Resume**: Use keywords multiple times in different contexts

**Keyword Types to Include:**

**Hard Skills:**
- Technical skills (programming languages, software, tools)
- Certifications and credentials
- Industry-specific terminology
- Methodologies (Agile, Six Sigma, Lean)

**Soft Skills (when specified in job description):**
- Leadership
- Project management
- Communication
- Problem-solving
- Team collaboration

**Job-Specific Keywords:**
- Exact job title variations
- Industry terms
- Required qualifications
- Tools and technologies mentioned

### Keyword Extraction from Job Descriptions

**Step 1: Identify Required Skills**
- Look for "required," "must have," "essential" qualifications
- Note years of experience specified
- Identify technical tools/software mentioned
- Find certifications or degrees required

**Step 2: Note Preferred Skills**
- Look for "preferred," "nice to have," "desired" qualifications
- Secondary skills that strengthen candidacy
- Additional technical proficiencies

**Step 3: Extract Action-Oriented Language**
- Verbs used to describe responsibilities
- Industry-specific terminology
- Achievement-oriented phrases

**Step 4: Use Exact Phrasing**
- If job description says "project management," use "project management" not "managed projects"
- Match acronyms and spelled-out versions (SEO / Search Engine Optimization)
- Use both variations: "JavaScript" and "JS"

### Keyword Integration Strategies

**Natural Integration:**
✅ Good: "Led cross-functional teams using Agile methodology to deliver 15+ software releases, reducing time-to-market by 30%"
❌ Bad: "Agile. Cross-functional teams. Software releases. Time-to-market."

**Skills Section Examples:**

**Technical Role:**
```
Technical Skills:
Programming Languages: Python, Java, JavaScript, SQL, C++
Frameworks & Libraries: React, Node.js, Django, TensorFlow, pandas
Tools & Platforms: AWS, Docker, Git, Jenkins, JIRA
Methodologies: Agile, Scrum, CI/CD, Test-Driven Development
```

**Marketing Role:**
```
Core Competencies:
Digital Marketing: SEO, SEM, Google Analytics, Google Ads, Facebook Ads Manager
Content Strategy: Content marketing, copywriting, editorial planning, brand storytelling
Tools: HubSpot, Salesforce, Hootsuite, Mailchimp, Adobe Creative Suite
Analytics: A/B testing, conversion rate optimization, data analysis, reporting
```

### Acronyms and Variations

**Always Include Both:**
- Search Engine Optimization (SEO)
- Certified Public Accountant (CPA)
- Application Programming Interface (API)
- Customer Relationship Management (CRM)

**Technology Variations:**
- JavaScript / JS
- Amazon Web Services / AWS
- Structured Query Language / SQL
- Hypertext Markup Language / HTML

**Multiple Terms for Same Skill:**
- Project Management / Program Management
- Software Engineer / Software Developer
- UX Designer / User Experience Designer

## Common ATS Pitfalls

### What ATS Cannot Parse

**Graphics and Images:**
- Company logos
- Photos of yourself
- Icons for contact information
- Infographics
- Charts or graphs
- Text in images

**Solution**: Use text only, with clear labels

**Complex Formatting:**
- Columns
- Tables (for layout structure)
- Text boxes
- Shapes
- SmartArt

**Solution**: Use simple, single-column layout with standard formatting

**Headers and Footers:**
- Contact information in header
- Page numbers
- Document metadata

**Solution**: Place all important information in main body

**Special Characters:**
- Unusual bullet points (✓, ➤, ☞)
- Decorative symbols
- Em dashes instead of hyphens
- Non-standard characters

**Solution**: Use standard keyboard characters only

### Date Formatting

**ATS-Friendly Date Formats:**
- Month Year – Month Year (January 2020 – March 2023)
- MM/YYYY – MM/YYYY (01/2020 – 03/2023)
- MMM YYYY – MMM YYYY (Jan 2020 – Mar 2023)

**Be Consistent:**
- Use same format throughout resume
- Align dates properly (typically right-aligned)
- Use "Present" or "Current" for ongoing roles

**Avoid:**
- Inconsistent formats
- Abbreviated years (20-23)
- Ambiguous dates (2020-2023 without months)

### Contact Information Format

**Standard Format:**
```
FULL NAME
City, State | Phone | Email | LinkedIn
```

**ATS-Friendly:**
- Use simple text
- No icons for phone, email, etc.
- Include links as full URLs
- LinkedIn: linkedin.com/in/yourprofile
- Portfolio: yourname.com

**Avoid:**
- Contact info in header/footer
- Using symbols (📧, 📱)
- Hyperlinked text without showing URL

## Testing Your Resume

### Pre-Submission Checks

1. **Plain Text Test:**
   - Copy resume into plain text editor (Notepad)
   - Check if all information is readable and properly structured
   - If text is garbled, ATS may have similar issues

2. **PDF Text Selection:**
   - Open PDF and try to select/copy text
   - If you can't select text, it's an image (will fail ATS)
   - Re-save ensuring text is selectable

3. **File Name:**
   - Use: FirstName_LastName_Resume.pdf
   - Avoid: resume.pdf, My_Resume_FINAL_v2.pdf

4. **Jobscan or Similar Tools:**
   - Use online ATS simulators
   - Compare your resume against job description
   - Identify missing keywords

### Online ATS Testing Tools

**Jobscan** (jobscan.co)
- Compares resume to job description
- Provides match percentage
- Suggests keyword improvements

**Resume Worded** (resumeworded.com)
- Checks ATS compatibility
- Identifies formatting issues
- Provides optimization suggestions

**TopResume** (topresume.com)
- Free ATS scan
- Identifies parsing problems
- Offers improvement recommendations

## Industry-Specific ATS Considerations

### Technology/Engineering
- Heavy emphasis on technical skills keywords
- Include programming languages, frameworks, tools
- Mention specific technologies from job description
- List methodologies (Agile, DevOps, CI/CD)
- Include certifications (AWS, Google Cloud, Azure)

### Healthcare
- Include licenses and certifications prominently
- Use medical terminology from job posting
- List specific procedures or specialties
- Mention software systems (Epic, Cerner)
- Include continuing education

### Finance/Accounting
- List certifications (CPA, CFA, CMA)
- Include financial software (QuickBooks, SAP, Oracle)
- Use industry-specific terms (GAAP, reconciliation)
- Mention regulatory knowledge (SOX, IFRS)
- Specify types of analysis performed

### Marketing/Sales
- Include digital marketing platforms
- List CRM systems (Salesforce, HubSpot)
- Mention specific channels (SEO, SEM, social media)
- Quantify achievements (conversion rates, ROI)
- Include tools (Google Analytics, Mailchimp)

### Education
- Include teaching certifications and endorsements
- List educational technologies
- Mention curriculum development
- Include grade levels or subjects
- Specify assessment tools used

## Customization Strategy

### For Each Application:

1. **Analyze Job Description:**
   - Highlight all keywords and required skills
   - Note technical terms and acronyms
   - Identify years of experience needed
   - Understand priorities (required vs. preferred)

2. **Update Skills Section:**
   - Lead with most relevant skills for position
   - Include all mentioned technologies/tools you have
   - Use exact phrasing from job description
   - Include both acronyms and spelled-out versions

3. **Revise Professional Summary:**
   - Mirror language from job description
   - Include 2-3 top required skills/qualifications
   - Use job title if you have it or aspirational version

4. **Adjust Work Experience:**
   - Reorder bullets to emphasize relevant achievements
   - Integrate job description keywords naturally
   - Add context for less obvious skill matches

5. **Verify Keyword Coverage:**
   - Check if all major requirements appear in resume
   - Ensure keywords appear multiple times (naturally)
   - Don't keyword stuff - maintain readability

## Balance: ATS vs. Human Readers

Remember: ATS gets you through the door, but humans make hiring decisions.

**Optimize for Both:**
- Use keywords naturally, not in lists
- Maintain compelling, readable narratives
- Quantify achievements with metrics
- Tell a coherent career story
- Ensure visual appeal once parsed

**The Goal:**
Pass ATS screening while creating a resume that impresses human recruiters and hiring managers. A well-optimized resume serves both audiences effectively.

**Final Check:**
- Can ATS parse it? ✓
- Would a human enjoy reading it? ✓
- Does it showcase your qualifications compellingly? ✓


--- resume-builder/references/best-practices.md ---
# Resume Best Practices

## Core Principles

### Content Hierarchy
1. **Contact Information** - Name, phone, email, location (city/state), LinkedIn, portfolio/GitHub (if relevant)
2. **Professional Summary** (optional but recommended) - 2-4 sentences highlighting key qualifications
3. **Experience** - Reverse chronological, with measurable achievements
4. **Education** - Degree, institution, graduation date (recent grads may place before experience)
5. **Skills** - Relevant technical and professional competencies
6. **Additional Sections** (as applicable) - Certifications, publications, awards, volunteer work, projects

### Writing Principles

**Achievement-Oriented Language**
- Use action verbs: led, developed, implemented, increased, reduced, streamlined
- Quantify results wherever possible: percentages, dollar amounts, time savings, scale
- Follow the formula: Action Verb + Task + Result/Impact
- Example: "Reduced customer support response time by 35% through implementation of automated ticket routing system"

**Professional Tone**
- Use past tense for previous roles, present tense for current role
- Avoid first-person pronouns (I, me, my)
- Be concise - every word should add value
- Use industry-standard terminology while remaining accessible

**Relevance Over Completeness**
- Tailor content to the target position
- Emphasize transferable skills for career transitions
- De-emphasize or omit irrelevant experience (unless it fills gaps)
- Recent experience deserves more detail than older positions

## Format Standards

### Length Guidelines
- **Entry-level (0-5 years)**: One page strongly preferred
- **Mid-career (5-15 years)**: One to two pages
- **Senior/Executive (15+ years)**: Two pages acceptable, rarely three

### Visual Design
- **Margins**: 0.5-1 inch on all sides
- **Font**: Professional sans-serif (Calibri, Arial, Helvetica) or serif (Times New Roman, Garamond) at 10-12pt
- **Headings**: 14-16pt, bold or slightly larger font
- **Spacing**: Consistent line spacing (1.0-1.15), adequate white space between sections
- **Alignment**: Left-aligned for easy scanning (centered headers acceptable)
- **Consistency**: Uniform formatting for dates, locations, bullet points throughout

### File Format
- Submit as PDF unless specifically requested otherwise (preserves formatting)
- Use descriptive filename: FirstName_LastName_Resume.pdf
- Ensure accessibility: proper heading tags, readable text (not embedded in images)

## Section-Specific Guidance

### Contact Information
**Include:**
- Full name (larger font, bold)
- Phone number with professional voicemail
- Professional email address (firstname.lastname@provider.com format)
- City and State (full address not necessary)
- LinkedIn profile URL (customized if possible)
- Portfolio/GitHub/website (for relevant fields)

**Exclude:**
- Date of birth
- Marital status
- Photo (in US; customs vary internationally)
- Social Security number
- Full street address

### Professional Summary
**When to Include:**
- Career changers explaining transferable skills
- Experienced professionals highlighting breadth
- Highly competitive fields requiring immediate impact
- When job requirements are very specific

**How to Write:**
- 2-4 sentences maximum
- Lead with years of experience and key expertise
- Mention specific skills matching job requirements
- Include 1-2 notable achievements or unique qualifications
- Example: "Marketing professional with 8+ years driving growth for SaaS companies. Expertise in digital strategy, content marketing, and data analytics. Led campaigns that increased MQLs by 150% and contributed to $5M in new revenue."

**When to Skip:**
- Entry-level positions where experience is limited
- When space is at premium and experience speaks clearly
- Internal applications where you're already known

### Work Experience

**Company and Role Information:**
- Job Title | Company Name, Location (City, State)
- Employment dates (Month Year – Month Year or Present)
- Brief company description for lesser-known organizations

**Bullet Points:**
- Start with strong action verbs
- Focus on accomplishments over responsibilities
- Quantify whenever possible - metrics demonstrate impact
- Use 3-6 bullets per position (more for recent roles, fewer for older)
- Target 1-2 lines per bullet (brief and scannable)

**STAR Method for Achievements:**
- **Situation**: Context or challenge
- **Task**: Your responsibility
- **Action**: What you did
- **Result**: Measurable outcome

**Examples of Strong Bullets:**
- ❌ Weak: "Responsible for managing social media accounts"
- ✅ Strong: "Grew Instagram following from 2K to 45K in 8 months through targeted content strategy and influencer partnerships"

- ❌ Weak: "Worked on improving customer satisfaction"
- ✅ Strong: "Increased customer satisfaction scores from 3.2 to 4.6/5.0 by redesigning onboarding process and implementing proactive support system"

### Education

**Recent Graduates (0-2 years):**
- Place education before experience
- Include GPA if 3.5+ (or above department average)
- List relevant coursework (3-5 courses)
- Include honors, awards, scholarships
- Mention thesis/capstone project if relevant

**Experienced Professionals:**
- Place after experience section
- Include degree, major, institution, graduation year
- Omit GPA (unless specifically requested or truly exceptional)
- Include honors (cum laude, Dean's List) if achieved

**Format:**
Degree, Major | Institution Name, Location
Graduation Month Year

### Skills Section

**Technical Skills:**
- Group by category (Programming Languages, Frameworks, Tools, etc.)
- List in order of proficiency/relevance
- Be honest - you may be tested on these
- Use standard industry terminology

**Professional Skills:**
- Focus on hard skills over soft skills
- Include certifications with credentials
- Mention language proficiency if relevant (and proficiency level)
- Quantify when possible: "Advanced Excel (pivot tables, VLOOKUP, macros)"

**Avoid:**
- Listing soft skills without context ("excellent communicator")
- Obvious skills ("Microsoft Word," "email" for professional roles)
- Outdated skills unless relevant to specific role
- Skills rated with bars/stars (subjective and imprecise)

### Additional Sections

**Projects** (especially for developers, designers, career changers):
- Personal or academic projects demonstrating relevant skills
- Include project name, technologies used, brief description, link if available
- Focus on impact or what you learned

**Certifications:**
- List current, relevant professional certifications
- Include issuing organization and date obtained
- Use official credential names and acronyms
- Update or remove expired certifications

**Publications/Presentations:**
- Relevant for academic, research, thought leadership positions
- Use proper citation format (APA, MLA, Chicago)
- Include co-authors if applicable
- Link to online versions when available

**Volunteer Work:**
- Include when relevant to target role
- Demonstrates commitment, leadership, skills
- Format similar to work experience with accomplishments

## Common Mistakes to Avoid

**Content Errors:**
1. **Typos and grammatical errors** - Proofread multiple times, use tools, have others review
2. **Inconsistent verb tenses** - Past for previous roles, present for current
3. **Generic objective statements** - Replace with professional summary or omit
4. **Using "responsible for"** - Shift to accomplishment-based language
5. **Including irrelevant information** - Every line should strengthen your candidacy
6. **Duties without impact** - Transform responsibilities into achievements
7. **Unexplained employment gaps** - Address briefly if necessary
8. **Too much personal information** - Keep it professional

**Format Errors:**
1. **Dense text blocks** - Use white space and bullet points
2. **Inconsistent formatting** - Dates, bullets, spacing must match throughout
3. **Unprofessional email address** - Use firstname.lastname format
4. **Decorative fonts or colors** - Professional doesn't mean boring, but readability is key
5. **Headers/footers with page numbers on one-page resumes** - Unnecessary
6. **Multiple columns** - Can interfere with ATS parsing
7. **Text boxes or tables** - Often cause ATS parsing issues

**Strategic Errors:**
1. **Not tailoring to job description** - Customize for each application
2. **Leading with education when experience is stronger**
3. **Including references** - Provide when requested, not on resume
4. **Using "References available upon request"** - Assumed and wastes space
5. **Hobbies/interests** - Generally omit unless directly relevant or notable

## Tailoring Your Resume

### Job Description Analysis
1. **Identify keywords**: Both technical skills and competencies
2. **Note required vs. preferred qualifications**: Prioritize requirements
3. **Understand company culture**: Reflect values in language/examples
4. **Match level**: Ensure your experience aligns with position seniority

### Customization Strategy
- **Reorder bullet points**: Lead with most relevant achievements
- **Adjust professional summary**: Mirror key requirements
- **Emphasize relevant skills**: Move matching competencies to top
- **Add context**: Explain lesser-known companies or roles when beneficial
- **Remove irrelevancies**: Trim less relevant experience for space

### Keyword Integration
- Use exact phrases from job description when accurate
- Incorporate throughout (not just skills section)
- Maintain natural, readable language
- Focus on substance over keyword stuffing

## Proofreading Checklist

**Content Review:**
- [ ] All dates are accurate and consistent
- [ ] No typos or grammatical errors
- [ ] Every bullet starts with action verb
- [ ] Achievements include metrics where possible
- [ ] Content is relevant to target position
- [ ] Verb tenses are consistent (past/present)

**Format Review:**
- [ ] Consistent font throughout
- [ ] Uniform bullet style
- [ ] Adequate white space
- [ ] Professional appearance
- [ ] Headers clearly distinguish sections
- [ ] Contact information is current and accurate

**ATS Review:**
- [ ] Standard section headings
- [ ] Simple formatting (no text boxes, tables, columns)
- [ ] Keywords from job description included
- [ ] Saved as PDF with text (not image)
- [ ] File name is professional

**Final Checks:**
- [ ] Fits appropriate length (1-2 pages)
- [ ] Opens correctly on different devices
- [ ] Have 2-3 others review
- [ ] Read aloud to catch awkward phrasing
- [ ] Verify all links work


--- ai-vendor-evaluation/references/build-vs-buy.md ---
# Build vs Buy Decision Framework

Structured approach to deciding whether to build AI capability in-house or purchase vendor solution.

---

## Decision Framework

```
Decision Flow:

1. Does suitable vendor solution exist?
   NO → Must build
   YES → Continue to 2

2. Is this a core strategic capability?
   YES → Bias toward build
   NO → Continue to 3

3. Do you have available engineering talent?
   NO → Must buy
   YES → Continue to 4

4. Compare 3-year TCO: Build vs Buy
   Build significantly cheaper → Build
   Buy significantly cheaper → Buy
   Similar costs → Continue to 5

5. Time to value comparison
   Need fast (< 3 months) → Buy
   Can wait (3-12 months) → Either
   Long timeline acceptable → Build

6. Assess strategic factors (see below)
```

---

## Build Cost Estimation

### Initial Development (Months 0-6)
**Engineering**:
- 2-3 engineers × 6-12 months
- Loaded cost: $150K-300K per engineer-year
- Total: $150K-900K depending on complexity

**Infrastructure**:
- Cloud services: $5K-50K
- Development tools: $5K-20K
- Data/model costs: $10K-100K

**Other**:
- Product management: $25K-50K
- Design: $10K-30K
- Testing/QA: $20K-50K

**Year 1 Total**: $225K-1.2M

### Ongoing Costs (Years 2-3)
**Maintenance & Operations**:
- 1-2 engineers: $150K-600K/year
- Infrastructure: $20K-100K/year
- Model retraining: $10K-50K/year
- Improvements: $50K-200K/year

**Annual Ongoing**: $230K-950K

### 3-Year Build TCO
**Low complexity**: $700K-1.5M
**Medium complexity**: $1.5M-3M  
**High complexity**: $3M-6M+

---

## Buy Cost Estimation

### Year 1 Costs
**Licenses/Subscriptions**:
- Based on vendor pricing model
- Typical: $50K-500K

**Implementation**:
- Professional services: $25K-250K
- Internal time: $20K-100K
- Integration dev: $30K-150K

**Training**:
- User training: $10K-50K
- Admin training: $5K-20K

**Year 1 Total**: $140K-1.07M

### Years 2-3 Costs
**Annual Costs**:
- License renewals: 105-110% of Y1 (price increases)
- Usage growth: 10-30% annual increase
- Support: Usually included or $10K-50K
- Minor enhancements: $20K-100K

**Annual Ongoing**: $110K-660K

### 3-Year Buy TCO
**Low-end solution**: $360K-2.4M
**Mid-market solution**: $2.4M-4M
**Enterprise solution**: $4M-10M+

---

## Decision Matrix

| Factor | Weight | Build Score | Buy Score | Winner |
|--------|--------|-------------|-----------|--------|
| Total 3-year cost | 30% | [1-5] | [1-5] | |
| Time to value | 20% | [1-5] | [1-5] | |
| Strategic importance | 15% | [1-5] | [1-5] | |
| Team capability | 15% | [1-5] | [1-5] | |
| Flexibility/control | 10% | [1-5] | [1-5] | |
| Risk | 10% | [1-5] | [1-5] | |
| **Weighted Total** | | | | |

**Scoring**:
- 5 = Strongly favors this option
- 3 = Neutral
- 1 = Strongly against this option

**Decision**: Option with higher weighted total wins

---

## Build Advantages

✅ **Full control**: Own the roadmap and priorities  
✅ **Customization**: Exactly fits your needs  
✅ **No vendor lock-in**: Freedom to pivot  
✅ **Competitive advantage**: Proprietary capability  
✅ **Data privacy**: Data never leaves your infrastructure  
✅ **Cost at scale**: Cheaper if high volume usage  

---

## Build Disadvantages

❌ **High upfront cost**: Large initial investment  
❌ **Slow time to value**: 6-24 months typically  
❌ **Ongoing maintenance**: Permanent team needed  
❌ **Technical risk**: May not work as planned  
❌ **Opportunity cost**: Engineers not on other projects  
❌ **Expertise required**: Need specialized AI talent  

---

## Buy Advantages

✅ **Fast time to value**: Live in weeks/months  
✅ **Lower upfront cost**: Subscription model  
✅ **Vendor expertise**: Benefit from their R&D  
✅ **Reduced risk**: Solution already proven  
✅ **Predictable costs**: Monthly/annual fees  
✅ **No maintenance burden**: Vendor handles it  

---

## Buy Disadvantages

❌ **Vendor lock-in**: Switching costs are high  
❌ **Limited control**: On vendor's roadmap  
❌ **Customization limits**: May not fit perfectly  
❌ **Recurring costs**: Pay forever  
❌ **Cost at scale**: Expensive with high usage  
❌ **Data sharing**: Vendor access to your data  

---

## Build When...

✅ **Core strategic capability**: Key differentiator  
✅ **Unique requirements**: No vendor fits  
✅ **High volume usage**: Build economics work at scale  
✅ **Data sensitivity**: Cannot share with vendors  
✅ **Available talent**: Have engineers who can build  
✅ **Long-term horizon**: Can wait for ROI  
✅ **Competitive moat**: Want proprietary advantage  

**Example**: Recommendation engine for e-commerce - core to business, high volume, competitive advantage

---

## Buy When...

✅ **Commodity capability**: Not differentiating  
✅ **Speed critical**: Need solution now  
✅ **Lower cost**: Vendor cheaper than building  
✅ **Limited resources**: Can't spare engineering  
✅ **Proven solutions exist**: No need to reinvent  
✅ **Not core competency**: Focus elsewhere  
✅ **Risk averse**: Want proven solution  

**Example**: Email classification - commodity capability, vendors exist, not core business

---

## Hybrid Approach

**Sometimes best option**: Build some, buy some

**When hybrid makes sense**:
- Build core differentiating features
- Buy commodity features
- Use vendor for speed, transition to build later
- Buy for pilots, build if successful

**Example hybrid**:
- Buy: Text extraction from documents (commodity)
- Build: Custom classification for your domain (differentiation)

---

## Decision Case Studies

### Case Study 1: Customer Support Classification

**Situation**: E-commerce company, 10K tickets/month

**Build option**: $400K year 1, $200K/year ongoing = $800K 3-year TCO  
**Buy option**: $60K/year × 3 = $180K 3-year TCO

**Decision**: **Buy** - Not core to business, vendor solution works well, 4.4x cheaper

---

### Case Study 2: Product Recommendation Engine

**Situation**: Retailer, recommendations drive 30% of revenue

**Build option**: $800K year 1, $400K/year ongoing = $1.6M 3-year TCO  
**Buy option**: $200K/year × 3 = $600K 3-year TCO

**Decision**: **Build** - Core to business model, unique data, competitive advantage worth premium

---

### Case Study 3: Document Processing

**Situation**: Insurance company, 50K documents/month

**Build option**: $500K year 1, $250K/year ongoing = $1M 3-year TCO  
**Buy option**: $150K/year × 3 = $450K 3-year TCO

**Decision**: **Buy initially, build later** - Need fast solution for business, will build custom later as volume grows and costs justify

---

## Common Mistakes

❌ **Underestimating build costs**: Forgetting ongoing maintenance  
❌ **Overestimating build benefits**: Assuming perfect execution  
❌ **Ignoring opportunity cost**: Engineers could build revenue features  
❌ **Vendor lock-in fear paralyzing decision**: Some lock-in is acceptable for right solution  
❌ **Not actually comparing**: Building without checking if vendors exist  
❌ **Building because "we can"**: Engineering vanity projects  

---

## Final Decision Checklist

Before deciding, answer:

- [ ] Have we accurately estimated both build and buy costs?
- [ ] Have we factored in opportunity cost of building?
- [ ] Is this truly strategic or are we overestimating importance?
- [ ] Do we have the talent to build and maintain this?
- [ ] Have we evaluated at least 3 vendor options?
- [ ] Have we included all hidden costs (both options)?
- [ ] Does the timeline align with business needs?
- [ ] Have we considered hybrid approaches?

---

## Summary

**Build when**: Strategic capability, unique needs, high volume, available talent, long-term view

**Buy when**: Commodity capability, speed needed, limited resources, proven solutions exist

**Remember**: Build vs buy isn't permanent. Can buy now, build later (or vice versa). Choose based on current situation and strategic importance.

--- token-budget-advisor/references/chunking_patterns.md ---
# Chunking Patterns Reference

This document provides detailed strategies for chunking complex tasks to manage token budgets effectively.

## Sequential Processing

**Best for:** Time-series data, chronological analysis, ordered datasets

**Pattern:**
1. Divide by natural time boundaries (quarters, years, months)
2. Process each segment independently
3. Synthesize findings at the end

**Example:**
- "Let's analyze Q1-Q2 first, then Q3-Q4"
- "I'll review January-June, then July-December"
- "Starting with 2023 data, then moving to 2024"

**Communication template:**
> "This analysis spans [timeframe]. I recommend splitting into [N] parts: [list parts]. Should I start with [first part]?"

## Dimensional Breakdown

**Best for:** Multi-faceted analysis, complex comparisons, thematic reviews

**Pattern:**
1. Identify key dimensions (themes, categories, metrics)
2. Analyze each dimension separately
3. Cross-reference and synthesize

**Example:**
- "Let's examine financial health first, then operational efficiency, then market position"
- "I'll analyze technical feasibility, then cost implications, then timeline"
- "First pass: data quality. Second pass: insights extraction"

**Communication template:**
> "This requires analyzing [N] dimensions: [list]. I'll tackle [dimension 1] first, which covers [scope]. Continue with that?"

## Iterative Refinement

**Best for:** Long documents, comprehensive reports, creative content

**Pattern:**
1. Create outline or structure
2. Draft core sections
3. Refine and add detail in subsequent passes

**Example:**
- "First: outline with headers and key points"
- "Second: draft main sections"
- "Third: add supporting detail and polish"

**Communication template:**
> "For a comprehensive [output type], I'll use an iterative approach: [outline steps]. This ensures quality while managing token usage. Start with the outline?"

## Divide and Conquer

**Best for:** Multiple independent files, parallel analyses, comparative studies

**Pattern:**
1. Process subset A independently
2. Process subset B independently
3. Compare/synthesize results

**Example:**
- "Analyze competitor A's docs, then competitor B's docs, then compare"
- "Review technical docs first, business docs second"
- "Process first half of files, then second half"

**Communication template:**
> "I'll analyze [subset A] first ([N] files), then [subset B] ([M] files). This gives us focused insights before the comparison. Ready to start with [subset A]?"

## Focused Deep Dive

**Best for:** Single complex file requiring thorough analysis, detailed code review

**Pattern:**
1. Initial scan for structure and overview
2. Deep analysis of critical sections
3. Comprehensive pass on remaining sections

**Example:**
- "First: scan the document structure and executive summary"
- "Second: deep dive on the methodology section"
- "Third: analyze findings and recommendations"

**Communication template:**
> "This [document/codebase] needs careful attention. I'll start with [high-level overview], then focus on [critical sections], then [comprehensive pass]. This approach ensures I catch important details. Begin?"

## Incremental Artifact Building

**Best for:** Large spreadsheets, complex presentations, multi-page documents

**Pattern:**
1. Build core structure/template
2. Populate main sections iteratively
3. Add polish and supporting elements

**Example:**
- "Create workbook structure with tabs and headers"
- "Populate financial data sections"
- "Add visualizations and formatting"

**Communication template:**
> "I'll build this [artifact type] in stages: [stage 1: structure], [stage 2: core content], [stage 3: refinement]. This ensures we can review and adjust between stages. Start with the structure?"

## Hybrid Approaches

Most complex tasks benefit from combining patterns:

**Research + Synthesis:**
1. Gather information (sequential searches)
2. Organize by theme (dimensional)
3. Draft report (iterative refinement)

**Multi-File Analysis + Report:**
1. Process files in batches (divide and conquer)
2. Extract key themes (dimensional)
3. Build comprehensive report (iterative)

## Estimation Guidelines

Use these heuristics to gauge when chunking is needed:

**High risk (almost certainly chunk):**
- 5+ large files uploaded AND request for comprehensive analysis
- Request for "complete," "thorough," "detailed" report
- 10+ tool calls likely needed
- Output artifact expected to be >1000 lines
- Multiple passes needed (analysis → synthesis → formatting)

**Medium risk (consider chunking):**
- 3-4 files uploaded with analysis request
- 5-8 tool calls likely needed
- Output artifact 500-1000 lines
- Complex multi-step workflow

**Low risk (probably fine):**
- 1-2 files with focused question
- <5 tool calls needed
- Output under 500 lines
- Single-focus task

## Selection Decision Tree

```
Is the task complex with multiple aspects?
├─ YES → Use Dimensional Breakdown or Divide and Conquer
└─ NO → Continue

Does the task involve time-series or ordered data?
├─ YES → Use Sequential Processing
└─ NO → Continue

Is the output a long document/artifact?
├─ YES → Use Iterative Refinement or Incremental Building
└─ NO → Continue

Does the task require deep analysis of a single complex item?
├─ YES → Use Focused Deep Dive
└─ NO → Task may not need chunking
```


--- skill-debugging-assistant/references/common-issues.md ---
# Common Skill Issues and Solutions

This reference provides detailed examples of common skill problems with before/after fixes. Use this when debugging complex or recurring issues.

## Trigger Failure Issues

### Issue 1: Description Too Generic

**Problem:** Skill doesn't trigger because description uses generic terms that don't match user queries.

**Example - Before:**
```yaml
name: data-analyzer
description: Analyzes data and provides insights
```

**Why it fails:**
- "Analyzes data" is too vague (what kind of data?)
- Missing trigger indicators (file types, specific tasks)
- No differentiation from other analysis skills
- Doesn't include terms users would actually say

**Example - After:**
```yaml
name: data-analyzer
description: Analyze CSV and Excel files to identify trends, outliers, and statistical patterns. Use when users request data analysis, statistical summaries, correlation analysis, or exploratory data analysis (EDA) on tabular data files.
```

**Why it works:**
- Specifies file types (.csv, .xlsx)
- Includes task types (trends, outliers, statistical patterns)
- Lists trigger phrases (data analysis, statistical summaries, correlation)
- Clarifies domain (tabular data)

### Issue 2: Missing Synonym Coverage

**Problem:** Skill triggers for some phrasings but not others.

**Example - Before:**
```yaml
name: resume-builder
description: Creates professional resumes in multiple formats
```

**Why it fails:**
- Only mentions "resumes" - misses CV, curriculum vitae
- Doesn't include action verbs users might use (write, make, build, design)
- Missing context about job applications

**Example - After:**
```yaml
name: resume-builder
description: Create, write, or update professional resumes and CVs for job applications. Use when users want to build, design, review, improve, or tailor resumes (or curriculum vitae) for specific positions. Supports multiple formats with ATS optimization.
```

**Why it works:**
- Includes synonyms: resume/CV/curriculum vitae
- Covers action verbs: create, write, update, build, design, review, improve, tailor
- Provides context: job applications, ATS optimization

### Issue 3: Overlapping Skill Descriptions

**Problem:** Two skills have similar descriptions, causing inconsistent triggering.

**Example - Before:**

Skill A:
```yaml
name: document-formatter
description: Formats documents and improves their appearance
```

Skill B:
```yaml
name: style-enhancer
description: Enhances document styling and formatting
```

**Why it fails:**
- Both mention "formatting" and "documents"
- No clear differentiation between the two skills
- User has no way to know which will trigger

**Example - After:**

Skill A:
```yaml
name: document-formatter
description: Apply structural formatting to documents including headings, tables of contents, page numbers, headers/footers, and layout adjustments. Use for structural document organization tasks in Word docs or PDFs.
```

Skill B:
```yaml
name: style-enhancer
description: Enhance visual styling of documents with custom fonts, colors, themes, and brand guidelines. Use for aesthetic improvements and brand consistency across presentations, documents, or marketing materials.
```

**Why it works:**
- Clear scope differentiation: structural vs. visual
- Different trigger terms: structural/layout vs. fonts/colors/themes
- Distinct use cases: organization vs. aesthetics

## False Positive Issues

### Issue 4: Over-Broad Description

**Problem:** Skill triggers for queries outside its intended scope.

**Example - Before:**
```yaml
name: code-helper
description: Helps with coding tasks and programming questions
```

**Why it fails:**
- "Helps with coding" triggers on ANY coding query
- No scope limitation (languages, types of tasks)
- Would trigger even for simple questions about syntax

**Example - After:**
```yaml
name: code-helper
description: Debug complex multi-file codebases and trace execution flow across modules. Use specifically for debugging hard-to-diagnose issues, not for simple syntax questions, code writing, or single-file problems. Focused on Python and JavaScript projects.
```

**Why it works:**
- Narrow scope: complex debugging, multi-file issues
- Explicit exclusions: "not for simple syntax questions"
- Language constraints: Python and JavaScript
- Specific trigger: hard-to-diagnose issues

### Issue 5: Absolute Statements Without Context

**Problem:** Skill applies instructions too broadly, overriding user intent.

**Example - Before:**
```markdown
## Writing Style

ALWAYS use formal academic language.
NEVER use contractions.
ALWAYS include citations for every claim.
```

**Why it fails:**
- No conditional logic or context awareness
- Would apply even when user requests casual tone
- "ALWAYS" overrides user preferences

**Example - After:**
```markdown
## Writing Style

When creating academic research documents:
- Prefer formal academic language
- Avoid contractions in formal sections
- Include citations for factual claims

Adapt the tone based on the document type and user's explicit preferences. For casual documents or when specifically requested, adjust formality accordingly.
```

**Why it works:**
- Conditional: "When creating academic research documents"
- Uses "prefer" and "avoid" instead of "always" and "never"
- Explicit adaptation clause for user preferences

## Instruction Conflict Issues

### Issue 6: Conflicting Instructions Within Skill

**Problem:** Different parts of SKILL.md give contradictory guidance.

**Example - Before:**
```markdown
## Report Structure

Always use the following structure:
1. Executive Summary
2. Methodology
3. Findings
4. Recommendations

## Custom Reports

For financial reports, use this structure:
1. Financial Overview
2. Revenue Analysis
3. Cost Breakdown
4. Projections
```

**Why it fails:**
- "Always use" conflicts with custom structure
- No precedence rules
- Unclear which applies for financial reports

**Example - After:**
```markdown
## Report Structure

### Default Structure

Use this structure unless a specialized format is needed:
1. Executive Summary
2. Methodology
3. Findings
4. Recommendations

### Specialized Formats

For domain-specific reports, adapt the structure. For example, financial reports should use:
1. Financial Overview
2. Revenue Analysis
3. Cost Breakdown
4. Projections

When in doubt, ask the user which format they prefer.
```

**Why it works:**
- Default with exception clause: "unless a specialized format is needed"
- Clear hierarchy: default → specialized
- Escape hatch: "ask the user"

### Issue 7: Critical Information in References

**Problem:** Essential workflow information buried in references that may not be loaded.

**Example - Before:**

SKILL.md:
```markdown
## Creating Reports

1. Gather data
2. Analyze trends
3. Generate report

For detailed analysis methods, see references/analysis-methods.md
```

references/analysis-methods.md:
```markdown
CRITICAL: Always validate data for outliers before analysis.
Never proceed without checking for null values.
```

**Why it fails:**
- Critical validation steps hidden in reference file
- May not be loaded during quick workflows
- User might skip validation unknowingly

**Example - After:**

SKILL.md:
```markdown
## Creating Reports

1. Gather data
2. **Validate data:**
   - Check for outliers
   - Verify no null values in critical fields
   - Confirm data types match expectations
3. Analyze trends (see references/analysis-methods.md for advanced techniques)
4. Generate report

Essential validation steps are above. For optional advanced analysis methods (regression, clustering, etc.), load references/analysis-methods.md.
```

**Why it works:**
- Critical steps in SKILL.md body
- Clear delineation: essential vs. optional
- References for advanced/optional details only

## Structural Issues

### Issue 8: SKILL.md Token Bloat

**Problem:** SKILL.md exceeds 500 lines, consuming too much context.

**Example - Before:**

SKILL.md contains:
- 50 lines of overview
- 200 lines of detailed API documentation
- 150 lines of examples
- 100 lines of edge case handling
- 50 lines of troubleshooting tips

**Why it fails:**
- Detailed API docs should be in references/
- Too many examples in main file
- Edge cases and troubleshooting are reference material

**Example - After:**

SKILL.md (150 lines total):
- 20 lines of overview
- 40 lines of core workflow
- 30 lines of essential examples
- 40 lines of quick reference
- 20 lines pointing to references

references/api-documentation.md:
- Detailed API documentation

references/examples.md:
- Comprehensive example collection

references/troubleshooting.md:
- Edge cases and troubleshooting

**Why it works:**
- SKILL.md contains only essential procedural knowledge
- Detailed information moved to purpose-specific references
- Clear indicators in SKILL.md about when to load each reference

### Issue 9: Missing Trigger Terms in Description

**Problem:** Description doesn't include domain-specific terminology users would naturally use.

**Example - Before:**
```yaml
name: legal-document-helper
description: Assists with legal documents
```

**Why it fails:**
- Doesn't mention specific document types (contracts, NDAs, agreements)
- Missing legal terminology (terms and conditions, clauses)
- No indication of what "assists" means

**Example - After:**
```yaml
name: legal-document-helper
description: Draft, review, and analyze legal contracts, NDAs, terms of service, privacy policies, and agreements. Use when users need help with legal document creation, contract review, clause analysis, or compliance checking. Identifies legal terminology, standard clauses, and potential issues.
```

**Why it works:**
- Lists specific document types: contracts, NDAs, ToS, privacy policies
- Includes legal terms: clauses, compliance, terminology
- Specifies actions: draft, review, analyze
- Clear use cases: creation, review, analysis

### Issue 10: Vague "When to Use" Indicators

**Problem:** Description doesn't clearly specify triggering scenarios.

**Example - Before:**
```yaml
name: image-processor
description: Processes images using various techniques and methods
```

**Why it fails:**
- "Various techniques" is not specific
- No indication of which image tasks trigger it
- Could be anything from basic cropping to ML-based analysis

**Example - After:**
```yaml
name: image-processor
description: Perform image transformations including resize, rotate, crop, format conversion, and basic filters. Use for image manipulation tasks, batch processing multiple images, or format conversions (PNG, JPEG, WebP). Not for AI-based image analysis or generation.
```

**Why it works:**
- Lists specific operations: resize, rotate, crop, format conversion
- Clarifies use cases: manipulation, batch processing, conversions
- Explicit exclusion: "Not for AI-based analysis or generation"
- Specifies supported formats

## Validation Error Issues

### Issue 11: Malformed YAML Frontmatter

**Problem:** YAML syntax errors prevent skill loading.

**Example - Before:**
```yaml
---
name: my-skill
description: This skill helps with analysis,
and provides insights
---
```

**Why it fails:**
- Multi-line description without proper YAML syntax
- Unquoted string with newline breaks YAML parsing

**Example - After:**
```yaml
---
name: my-skill
description: This skill helps with analysis and provides insights into data patterns
---
```

**Why it works:**
- Single-line description
- No unescaped special characters
- Valid YAML syntax

### Issue 12: Name Convention Violations

**Problem:** Skill name doesn't follow kebab-case convention.

**Examples of incorrect names:**
- `Skill_Debugger` (uses underscore)
- `skillDebugger` (uses camelCase)
- `Skill Debugger` (has space)
- `Skill-Debugger` (uses uppercase)

**Correct name:**
- `skill-debugger` (lowercase kebab-case)

## Progressive Disclosure Issues

### Issue 13: Not Leveraging References Effectively

**Problem:** All information crammed into SKILL.md when it could be split.

**Example - Before:**

Single SKILL.md with:
- Core workflow (necessary)
- Complete API reference (could be reference file)
- 20 detailed examples (could be reference file)
- Troubleshooting guide (could be reference file)
- Performance optimization tips (could be reference file)

**Example - After:**

SKILL.md:
```markdown
## Quick Start

[Essential workflow with 2-3 examples]

## Core Operations

[Most common operations with minimal examples]

## Resources

### references/api-reference.md
Complete API documentation. Load when you need detailed parameter information.

### references/examples.md
20+ detailed examples. Load when working on complex or unusual use cases.

### references/troubleshooting.md
Common errors and solutions. Load when debugging issues.

### references/optimization.md
Performance tuning guide. Load when optimizing for speed or resource usage.
```

**Why it works:**
- SKILL.md stays lean with essential information
- References clearly described with when-to-load guidance
- Information organized by purpose
- Progressive loading based on need

## Recommended Diagnostic Process

When encountering any issue:

1. **Start with description analysis** - 80% of trigger issues stem from poor descriptions
2. **Check for absolute statements** - Look for ALWAYS, NEVER, MUST without conditions
3. **Validate structure** - Run automated validation script
4. **Test with real queries** - Try 5+ variations of expected trigger queries
5. **Review similar skills** - Check for description overlap
6. **Analyze token budget** - Ensure SKILL.md is concise (<500 lines)
7. **Verify progressive disclosure** - Confirm proper use of references vs. SKILL.md body


--- skill-doc-generator/references/consistency-rules.md ---
# Consistency Rules Reference

Detailed validation rules for ensuring skill quality and consistency.

## Frontmatter Validation

### Required Fields
- **name**: Must be present and non-empty
  - Format: lowercase with hyphens (e.g., `skill-name`)
  - No spaces or underscores
  - Example: `pdf-editor` ✅, `PDF Editor` ❌, `pdf_editor` ❌

- **description**: Must be present and non-empty
  - Minimum length: 50 characters (warning if shorter)
  - Maximum length: 500 characters (info if longer)
  - Must start with capital letter
  - Should include trigger phrases (`when`, `use`)

### Optional But Recommended
- **license**: Legal terms reference
- **version**: Semantic versioning

## Description Quality

### Content Requirements
1. **Specificity**: Avoid vague terms
   - ❌ "various tasks"
   - ✅ "spreadsheet formulas and data visualization"

2. **Trigger Clarity**: Include when/how to use
   - ❌ "Helps with documents"
   - ✅ "When Claude needs to create, edit, or review .docx files"

3. **Completeness**: Cover key capabilities
   - What does it do?
   - When should it be used?
   - What are the main features?

### Length Guidelines
- **Too short** (<50 chars): Missing essential details
- **Ideal** (50-200 chars): Concise and complete
- **Good** (200-500 chars): Comprehensive
- **Too long** (>500 chars): May need editing for clarity

## Structure Validation

### Body Length
- **Minimum**: 100 characters (warning if less)
- **Maximum**: 500 lines (warning if more - suggests need for references/)

### Expected Sections
While not strictly required, skills typically benefit from:
- **Overview/About**: Introduction to the skill
- **Workflow/Usage**: How to use it
- **Examples**: Concrete demonstrations

### Resource Organization
- Scripts in `scripts/`
- Documentation in `references/`
- Templates/assets in `assets/`
- All resources should be referenced in SKILL.md

## Terminology Standards

### Writing Style
- Use imperative/infinitive form
  - ✅ "Use this script to rotate PDFs"
  - ❌ "You should use this script to rotate PDFs"

- Be direct and action-oriented
  - ✅ "Run analyze.py to extract metadata"
  - ❌ "You can run analyze.py if you want to extract metadata"

### Consistency
- **Claude**: Always capitalize (it's a proper noun)
- **SKILL.md**: All caps for the filename
- **skill**: Lowercase when referring to the concept

### Avoid Second Person
- ❌ "You should", "You can", "You must"
- ✅ "Use", "Run", "Create", "Execute"

## Resource Validation

### Referenced Resources
All bundled resources should be mentioned in SKILL.md:
- Scripts should be referenced by name or path
- Reference files should be linked in context
- Assets should be mentioned where applicable

### Unreferenced Resources (Warning)
If a resource exists but isn't mentioned, it may be:
- Dead code/docs that should be removed
- Important but forgotten in documentation

## Code Examples

### Best Practices
- Always include language tags: ` ```python` not ` ``` `
- Keep examples concise and focused
- Ensure examples are complete and runnable
- Provide context for what the example demonstrates

### Quantity
- No examples: INFO (consider adding if helpful)
- 1-5 examples: Ideal range
- 5+ examples: Consider moving to references/

## Error Severity Levels

### ERROR
Violations that prevent skill from working correctly:
- Missing required frontmatter fields
- Empty required fields
- Critical structural issues

### WARNING
Issues that affect quality but don't break functionality:
- Naming convention violations
- Short descriptions
- Unreferenced resources
- Excessive length (>500 lines)

### INFO
Suggestions for improvement:
- Stylistic recommendations
- Missing optional sections
- Terminology suggestions
- Code example language tags

## Validation Checklist

When validating a skill, check:

- [ ] Frontmatter has required fields
- [ ] Name follows naming conventions
- [ ] Description is comprehensive and clear
- [ ] Body has reasonable length
- [ ] Key sections are present
- [ ] Resources are properly referenced
- [ ] Code examples have language tags
- [ ] Terminology is consistent
- [ ] Style is imperative/infinitive


--- ai-vendor-evaluation/references/contract-checklist.md ---
# AI Vendor Contract Checklist

Essential terms for AI vendor agreements and negotiation guidance.

---

## Critical Contract Terms

### 1. Data Ownership and Rights

**Must have**:
- ✅ Customer owns all input data
- ✅ Customer owns all outputs/results  
- ✅ Vendor cannot use customer data without explicit permission
- ✅ Data deleted within 30 days of termination

**Negotiate**:
- Vendor rights to anonymized, aggregated data
- Model training on customer data (should be opt-in only)

**Red flags**:
- Vendor claims ownership of customer data
- Vendor can use data without restriction
- Data retained indefinitely
- Unclear ownership terms

---

### 2. Performance Guarantees (SLAs)

**Must have**:
- ✅ Uptime SLA (99.9% for production systems)
- ✅ Performance metrics (latency, throughput)
- ✅ Financial penalties for SLA breaches
- ✅ Transparent SLA reporting

**Typical SLAs**:
- Uptime: 99.9% (8.76 hours downtime/year)
- Response time: <1 second p95
- Support response: <1 hour for critical issues

**Penalties for breach**:
- 5-10% monthly fee credit per percentage point below SLA
- Right to terminate if sustained breaches

**Red flags**:
- No SLAs or "best effort" only
- No remedies for breaches
- Vague metrics

---

### 3. Termination and Exit

**Must have**:
- ✅ Can terminate with 90-180 days notice
- ✅ No penalty for termination (or reasonable penalty)
- ✅ Full data export in standard format
- ✅ Continued access during transition period

**Negotiate**:
- Termination for convenience
- Termination for cause (vendor breach)
- Reduced notice period
- Transition assistance

**Red flags**:
- Multi-year lock-in with large penalties
- Difficult or expensive data export
- No termination for cause option
- Vendor can hold data hostage

---

### 4. Pricing and Payment Terms

**Must have**:
- ✅ Clear pricing structure
- ✅ Annual price increase caps (5-10% max)
- ✅ Reasonable payment terms (Net 30)
- ✅ No surprise fees

**Negotiate**:
- Volume discounts
- Multi-year discount
- Usage caps or buffers
- Payment terms (Net 45/60)

**Red flags**:
- Unlimited price increases
- Hidden fees
- Prepayment required
- Short payment terms

---

### 5. Intellectual Property

**Must have**:
- ✅ Customer retains all IP in their data
- ✅ Vendor retains IP in their platform
- ✅ Clear ownership of deliverables
- ✅ Customer can use outputs freely

**Watch for**:
- Vendor claims rights to customer IP
- Restrictions on output usage
- Requirements to credit vendor

---

### 6. Security and Compliance

**Must have**:
- ✅ Security certifications (SOC 2, ISO 27001)
- ✅ Compliance with relevant regulations
- ✅ Data encryption at rest and in transit
- ✅ Breach notification requirements (24-48 hours)

**Negotiate**:
- Specific compliance certifications needed
- Data residency requirements
- Audit rights
- Security assessments

---

### 7. Liability and Indemnification

**Must have**:
- ✅ Vendor liable for breaches
- ✅ Indemnification for IP claims
- ✅ Reasonable liability caps
- ✅ No broad customer indemnification

**Typical caps**:
- Liability: 12 months of fees paid
- Exceptions: Breaches, IP infringement, gross negligence

**Red flags**:
- No liability or very low caps
- Customer indemnifies vendor broadly
- Vendor not liable for anything

---

### 8. Support and Maintenance

**Must have**:
- ✅ Defined support hours
- ✅ Response time commitments
- ✅ Regular updates included
- ✅ Dedicated support contact

**Typical support**:
- Business hours: 9am-5pm local time
- Critical issues: <1 hour response
- Standard issues: <24 hour response

**Negotiate**:
- 24/7 support if needed
- Faster response times
- Dedicated success manager
- Training included

---

## Negotiation Guide

### Before Negotiation

**Prepare**:
1. Understand your priorities
2. Know your walk-away points
3. Get competing proposals
4. Identify leverage points

**Prioritize your asks**:
- Must-haves (deal breakers)
- Important (will fight for)
- Nice-to-haves (will accept trade-offs)

---

### Negotiation Tactics

**For better pricing**:
- Request multi-year discount
- Commit to higher volume
- Pay annually vs monthly
- Reduce included features

**For better terms**:
- Shorter contract length
- Better termination clause
- Stronger SLAs with penalties
- Data ownership clarity

**What to trade**:
- Longer commitment for discount
- Reference/case study for concessions
- Public announcement for better terms
- Higher volume for better rates

**When you have leverage**:
- Multiple vendors competing
- Large contract size
- End of vendor's quarter/year
- You can build in-house

---

### Red Lines (Non-Negotiable)

**Never accept**:
1. Vendor owns your data
2. No ability to terminate
3. Unlimited price increases
4. No SLAs or accountability

**Push back hard on**:
1. Multi-year lock-in
2. Vague performance terms
3. Broad indemnification
4. Limited liability

---

## Contract Review Checklist

### Before Signing

**Data Terms** (Review pages: ___)
- [ ] Customer owns input data
- [ ] Customer owns outputs
- [ ] No vendor rights to use data
- [ ] Data deletion upon termination

**SLAs** (Review pages: ___)
- [ ] Uptime guarantees defined
- [ ] Performance metrics specified
- [ ] Financial penalties for breaches
- [ ] Reporting and measurement clear

**Termination** (Review pages: ___)
- [ ] Reasonable termination clause
- [ ] Data export guaranteed
- [ ] No excessive penalties
- [ ] Transition support included

**Pricing** (Review pages: ___)
- [ ] All costs disclosed
- [ ] Price increase caps
- [ ] Usage limits clear
- [ ] No hidden fees

**Liability** (Review pages: ___)
- [ ] Vendor liability defined
- [ ] Reasonable caps
- [ ] Indemnification mutual
- [ ] Breach consequences clear

**Security** (Review pages: ___)
- [ ] Certifications listed
- [ ] Compliance requirements met
- [ ] Breach notification terms
- [ ] Audit rights included

---

## Special Situations

### For Startups
**Additional protections needed**:
- Flexible scaling (up or down)
- Shorter contracts (annual vs multi-year)
- Lower minimum commitments
- Freedom to change use cases

### For Enterprises
**Additional requirements**:
- Stronger SLAs
- 24/7 support
- Security audits
- Compliance certifications
- Dedicated resources

### For Regulated Industries
**Must include**:
- Specific compliance certifications
- Data residency guarantees
- Audit trail capabilities
- Breach notification procedures
- Business Associate Agreement (if HIPAA)

---

## Common Vendor Pushback (and Responses)

**Vendor**: "Our standard contract doesn't allow changes"
**Response**: "We need these changes for legal/risk reasons. What's your approval process?"

**Vendor**: "We can't provide those SLAs"
**Response**: "What SLAs can you commit to? How do you handle breaches?"

**Vendor**: "Pricing is firm"
**Response**: "We have competing proposals at lower prices. Can you match?"

**Vendor**: "We need data rights for model improvement"
**Response**: "We'll consider opt-in anonymized data sharing. Not blanket rights."

**Vendor**: "Multi-year required for this price"
**Response**: "Annual contract at 10% higher price works, or multi-year with opt-out after year 1."

---

## Final Review Questions

Before signing, ask yourself:

1. Can we exit this contract reasonably if needed?
2. Do we own our data completely?
3. Is the vendor accountable for performance?
4. Are costs predictable and capped?
5. Are our key risks addressed?
6. Have we negotiated fairly?
7. Can legal and procurement approve?
8. Are we comfortable with vendor's liability limits?

**If any answer is "no", renegotiate or walk away.**

---

## Summary

**Essential terms**:
1. Data ownership (customer)
2. SLAs with penalties
3. Reasonable termination
4. Price protections
5. Liability and indemnification

**Remember**: Everything in a contract is negotiable. Vendors have "standard contracts" but will modify for the right deal. Don't accept unfavorable terms without trying to negotiate.

--- ai-vendor-evaluation/references/evaluation-criteria.md ---
# AI Vendor Evaluation Criteria

Comprehensive framework for systematically evaluating AI vendors across all critical dimensions.

---

## How to Use This Framework

**Scoring system**: Rate each criterion on a 1-5 scale
- **5 = Excellent**: Exceeds expectations, clear competitive advantage
- **4 = Good**: Meets expectations, no concerns
- **3 = Acceptable**: Adequate, minor concerns
- **2 = Concerning**: Significant gaps or risks
- **1 = Unacceptable**: Dis

qualifying issues

**Weighting**: Adjust weights based on your priorities (default weights provided)

**Threshold**: Generally, vendors scoring below 3.5 overall warrant serious concern

---

## 1. Technical Capability (Weight: 25%)

### 1.1 Core Technology Assessment
**What to evaluate**: Underlying AI models, architecture, technical approach

**Questions to ask:**
- What models do you use (proprietary vs third-party)?
- How do you handle model updates and deprecations?
- What's your approach to accuracy and reliability?
- How do you measure and improve model performance?

**Good answers include:**
- Specific model names and versions
- Clear testing and evaluation methodology
- Concrete accuracy metrics with confidence intervals
- Regular model evaluation and improvement process

**Red flags:**
- Vague about technical details
- Claims of "proprietary AI" without substance
- No clear performance metrics
- Overpromising accuracy ("99.9% accurate on all tasks")

**Score 5**: Proprietary models with proven performance, clear testing methodology, transparent about limitations  
**Score 3**: Solid implementation of third-party models, reasonable performance claims  
**Score 1**: Vague technical claims, no performance data, unrealistic promises

---

### 1.2 Integration Complexity
**What to evaluate**: How difficult will implementation be?

**Questions to ask:**
- What's your typical implementation timeline?
- What integrations do you support (APIs, webhooks, etc.)?
- What technical resources are required from our team?
- Can you provide implementation case studies?

**Good answers include:**
- Realistic timeline (measured in weeks/months, not days)
- Standard integration patterns (REST APIs, webhooks)
- Clear documentation and SDKs
- Reference customers with similar technical stacks

**Red flags:**
- "It's plug-and-play" (nothing is)
- Requires proprietary infrastructure
- Vague about integration requirements
- No documentation or examples

**Score 5**: Standard APIs, excellent documentation, proven integrations, realistic timelines  
**Score 3**: Reasonable integration approach, adequate documentation  
**Score 1**: Custom integration required, poor documentation, unrealistic timelines

---

### 1.3 Scalability and Performance
**What to evaluate**: Can this solution handle your growth?

**Questions to ask:**
- What are your current scale limits (requests/sec, data volume)?
- How do you handle traffic spikes?
- What's your typical latency?
- Do you have SLAs for performance?

**Good answers include:**
- Specific capacity numbers
- Auto-scaling capabilities
- Latency guarantees (p50, p95, p99)
- Formal SLAs with penalties

**Red flags:**
- Vague about capacity
- No SLAs or guarantees
- Unclear about handling scale
- "We've never hit limits" (not credible)

**Score 5**: Proven at scale, clear SLAs, excellent performance metrics  
**Score 3**: Adequate for current needs, reasonable performance  
**Score 1**: Unclear about scale, no SLAs, performance concerns

---

### 1.4 Security and Compliance
**What to evaluate**: Data protection, regulatory compliance, security practices

**Questions to ask:**
- What certifications do you have (SOC 2, ISO 27001, etc.)?
- How is customer data handled and protected?
- Do you support data residency requirements?
- What's your security incident history?

**Good answers include:**
- Current security certifications
- Clear data handling practices
- Compliance with relevant regulations (GDPR, CCPA, HIPAA if applicable)
- Transparent security practices

**Red flags:**
- No security certifications
- Vague about data handling
- "We've never been hacked" (everyone's been tested)
- Unwilling to discuss security

**Score 5**: Multiple certifications, excellent security practices, transparent  
**Score 3**: Basic security measures, working toward certifications  
**Score 1**: No certifications, unclear security practices

---

## 2. Business Viability (Weight: 20%)

### 2.1 Company Stability
**What to evaluate**: Will this vendor exist in 2-3 years?

**Questions to ask:**
- How long have you been in business?
- What's your funding situation?
- How many customers do you have?
- What's your revenue trajectory?

**Good indicators:**
- 2+ years in business
- Profitable or recently raised funding
- 20+ paying customers
- Growing revenue

**Red flags:**
- Brand new company (<1 year)
- Burning cash with no clear path to profitability
- Few customers or declining revenue
- Heavy dependence on single customer

**Score 5**: Profitable, growing revenue, strong customer base  
**Score 3**: Funded, reasonable burn rate, growing customer base  
**Score 1**: Financial concerns, very early stage, unstable

---

### 2.2 Customer Base and References
**What to evaluate**: Track record with similar customers

**Questions to ask:**
- Who are your reference customers?
- Can I speak with customers in similar industries?
- What's your customer retention rate?
- How many customers have similar use cases to ours?

**Good answers include:**
- Multiple verifiable reference customers
- High retention rate (>90%)
- Customers willing to speak with you
- Similar use cases to yours

**Red flags:**
- Can't provide references
- All references are pilots, no production deployments
- High churn rate
- No customers similar to your use case

**Score 5**: Excellent references, high retention, proven in your domain  
**Score 3**: Some references, reasonable retention  
**Score 1**: No verifiable references, high churn, no domain experience

---

### 2.3 Product Roadmap
**What to evaluate**: Alignment of vendor's direction with your needs

**Questions to ask:**
- What's on your product roadmap for next 12 months?
- How do you prioritize features?
- What's your release cadence?
- Do you have a customer advisory board?

**Good answers include:**
- Clear roadmap aligned with your needs
- Regular releases (monthly/quarterly)
- Customer-driven prioritization
- Transparent about priorities

**Red flags:**
- Vague roadmap or "everything is on the roadmap"
- Roadmap doesn't align with your needs
- Infrequent releases
- Sales-driven feature promises

**Score 5**: Clear roadmap, aligned with your needs, customer-driven  
**Score 3**: Reasonable roadmap, some alignment  
**Score 1**: Vague or misaligned roadmap

---

## 3. Pricing and Value (Weight: 20%)

### 3.1 Pricing Model Fairness
**What to evaluate**: Is the pricing structure reasonable and transparent?

**Questions to ask:**
- What's included in base pricing?
- What are additional costs (implementation, support, overage)?
- How do prices scale with usage?
- Are there volume discounts?

**Good pricing characteristics:**
- Transparent pricing structure
- Predictable costs
- Reasonable scaling
- Aligned with value delivered

**Red flags:**
- Hidden fees or surprise charges
- Complex pricing that's hard to predict
- High implementation fees
- Expensive overages

**Score 5**: Transparent, fair, predictable, aligned with value  
**Score 3**: Reasonable pricing with some complexity  
**Score 1**: Opaque pricing, high fees, poor value

See [pricing-models.md](pricing-models.md) for detailed pricing analysis.

---

### 3.2 Total Cost of Ownership
**What to evaluate**: Full 3-year cost including hidden costs

**Components to consider:**
- Software license/subscription fees
- Implementation costs
- Training and change management
- Ongoing maintenance and support
- Integration development and maintenance
- Internal resource costs

**Analysis approach:**
1. Calculate direct costs (licenses, fees)
2. Add implementation costs (professional services, internal time)
3. Add ongoing costs (support, maintenance, updates)
4. Compare to alternative solutions (including build in-house)

**Score 5**: TCO significantly lower than alternatives, clear value  
**Score 3**: TCO competitive with alternatives  
**Score 1**: TCO significantly higher than alternatives

See [build-vs-buy.md](build-vs-buy.md) for TCO analysis framework.

---

### 3.3 Value Delivered
**What to evaluate**: Does the solution deliver sufficient ROI?

**Questions to ask:**
- What measurable outcomes can we expect?
- What's the typical payback period?
- Can you provide ROI case studies?
- How do you measure success?

**Good answers include:**
- Specific, measurable outcomes
- Realistic payback periods (12-24 months typical)
- Verifiable case studies
- Clear success metrics

**Red flags:**
- Vague about outcomes
- Unrealistic ROI claims
- No case studies or metrics
- Can't define success

**Score 5**: Clear, measurable ROI with case studies, realistic payback  
**Score 3**: Reasonable value proposition, some evidence  
**Score 1**: Unclear value, no supporting evidence

---

## 4. Implementation Risk (Weight: 20%)

### 4.1 Implementation Complexity
**What to evaluate**: How difficult will deployment be?

**Factors to assess:**
- Technical integration complexity
- Data preparation required
- Change management needs
- Training requirements

**Questions to ask:**
- What's your average time to first value?
- What percentage of implementations succeed?
- What are common implementation challenges?
- What support do you provide during implementation?

**Good answers include:**
- Time to value measured in weeks, not months
- High success rate (>80%)
- Honest about challenges
- Dedicated implementation support

**Red flags:**
- Very long implementation timelines
- High failure rate or vague about success
- Downplaying implementation difficulty
- Limited implementation support

**Score 5**: Straightforward implementation, high success rate, excellent support  
**Score 3**: Manageable complexity, reasonable support  
**Score 1**: High complexity, low success rate, poor support

---

### 4.2 Vendor Support Quality
**What to evaluate**: Will you get help when you need it?

**Questions to ask:**
- What support tiers do you offer?
- What's your average response time for critical issues?
- Do you have a dedicated customer success team?
- What's your support availability (24/7, business hours)?

**Good answers include:**
- Tiered support with SLAs
- Fast response for critical issues (<1 hour)
- Dedicated support team
- Support hours matching your needs

**Red flags:**
- Email-only support
- Slow response times
- No dedicated support
- Support costs extra

**Score 5**: Excellent support with fast response, dedicated team, strong SLAs  
**Score 3**: Adequate support, reasonable response times  
**Score 1**: Poor support, slow response, no SLAs

---

### 4.3 Change Management Requirements
**What to evaluate**: How much organizational change is required?

**Factors to consider:**
- Process changes required
- User adoption challenges
- Stakeholder management needs
- Training requirements

**Assessment:**
- Low risk: Minimal process change, easy adoption
- Medium risk: Some process change, moderate training needed
- High risk: Significant process redesign, extensive training required

**Score 5**: Minimal change required, easy adoption  
**Score 3**: Moderate change, manageable with planning  
**Score 1**: Massive change, high adoption risk

---

## 5. Contract Terms (Weight: 15%)

### 5.1 Contract Flexibility
**What to evaluate**: How locked in will you be?

**Key terms to assess:**
- Contract length and renewal terms
- Termination clauses and notice periods
- Data portability and export
- Pricing escalation protections

**Good terms:**
- Annual contracts with option to renew
- Reasonable termination clauses (90 days notice)
- Full data export capabilities
- Price protection or caps

**Red flags:**
- Multi-year lock-in with penalties
- Difficult or expensive to exit
- No data portability
- Unlimited price increases

**Score 5**: Flexible terms, easy exit, strong protections  
**Score 3**: Reasonable terms with some lock-in  
**Score 1**: Heavy lock-in, difficult exit, poor protections

See [contract-checklist.md](contract-checklist.md) for detailed contract guidance.

---

### 5.2 Performance Guarantees
**What to evaluate**: Are vendors accountable for performance?

**Key elements:**
- Uptime SLAs (99.9% typical for production systems)
- Performance guarantees (latency, throughput)
- Accuracy or quality metrics
- Remedies for SLA breaches

**Good guarantees:**
- Clear, measurable SLAs
- Financial penalties for breaches
- Transparent reporting on SLA compliance

**Red flags:**
- No SLAs or vague guarantees
- No remedies for breaches
- "Best effort" language

**Score 5**: Strong SLAs with financial penalties, transparent reporting  
**Score 3**: Basic SLAs, some accountability  
**Score 1**: No SLAs or weak guarantees

---

### 5.3 Data Rights and IP Protection
**What to evaluate**: Who owns what?

**Key questions:**
- Who owns the data we input?
- Can you use our data to train models?
- Who owns the outputs/results?
- What happens to our data if we terminate?

**Good terms:**
- Customer owns all input data
- Vendor cannot use customer data without permission
- Customer owns or has full rights to outputs
- Data deletion upon termination

**Red flags:**
- Vendor claims rights to customer data
- Vendor can use data to train models
- Unclear ownership of outputs
- Data retained indefinitely

**Score 5**: Customer owns all data and outputs, clear deletion terms  
**Score 3**: Reasonable data terms with some usage rights  
**Score 1**: Vendor retains significant data rights

---

## Evaluation Score Calculation

### Overall Score Formula
```
Overall Score = (Technical × 0.25) + (Business × 0.20) + (Pricing × 0.20) + 
                (Implementation × 0.20) + (Contract × 0.15)
```

### Interpretation
- **4.5-5.0**: Excellent vendor, strong recommendation
- **4.0-4.4**: Good vendor, proceed with confidence
- **3.5-3.9**: Acceptable vendor, manageable risks
- **3.0-3.4**: Concerning, significant risks to address
- **<3.0**: Not recommended, too many red flags

### Decision Matrix

| Overall Score | Individual Category Score <2 | Recommendation |
|---------------|------------------------------|----------------|
| ≥4.0 | No | Proceed |
| ≥4.0 | Yes | Proceed with mitigation plan for weak area |
| 3.5-3.9 | No | Proceed cautiously, detailed risk assessment |
| 3.5-3.9 | Yes | Do not proceed without significant improvements |
| <3.5 | Any | Do not proceed |

---

## Customizing Weights for Your Situation

**Adjust weights based on priorities:**

**Early-stage company / High risk tolerance:**
- Increase Technical (30%), decrease Business (15%)
- Willing to bet on unproven vendors for technical advantage

**Enterprise / Risk-averse:**
- Increase Business (25%), Contract (20%)
- Prioritize stability and favorable terms

**Pilot project / Low commitment:**
- Increase Technical (30%), decrease Contract (10%)
- Can afford to try and switch vendors

**Mission-critical system:**
- Increase Implementation (25%), Business (25%)
- Cannot afford deployment failure or vendor going away

---

## Special Considerations

### For AI-Native Companies
Additional questions to ask:
- What's your approach to model drift?
- How do you handle adversarial inputs?
- What's your process for continuous improvement?
- How do you measure model fairness and bias?

### For Regulated Industries
Additional factors to assess:
- Specific compliance certifications (HIPAA, PCI-DSS, etc.)
- Audit trail capabilities
- Data sovereignty options
- Regulatory reporting features

### For High-Volume Use Cases
Additional focus areas:
- Per-unit economics at scale
- Batch processing capabilities
- Caching and optimization options
- Volume discount structures

---

## Using This Framework

**Step 1**: Review evaluation criteria and adjust weights for your situation  
**Step 2**: Conduct vendor interviews and demos focused on evaluation criteria  
**Step 3**: Score each vendor using the scorecard template  
**Step 4**: Review scores and identify any disqualifying issues (any category <2)  
**Step 5**: Compare vendor scores and make selection based on overall fit

**Remember**: Scores are a framework for structured decision-making, not a replacement for judgment. Use scores to identify risks and inform discussion, not as the sole decision factor.

## Links discovered
- [pricing-models.md](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/ai-vendor-evaluation/references/pricing-models.md)
- [build-vs-buy.md](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/ai-vendor-evaluation/references/build-vs-buy.md)
- [contract-checklist.md](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/ai-vendor-evaluation/references/contract-checklist.md)

--- token-budget-advisor/LICENSE.txt ---
MIT License

Copyright (c) 2025

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


--- xlsx-editor/LICENSE.txt ---

                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.

--- skill-security-analyzer/SKILL.md ---
---
name: skill-security-analyzer
description: Comprehensive security risk analysis for Claude skills. Use when asked to analyze security risks, review security stance, audit skills for vulnerabilities, check security before deployment, or evaluate safety of skill files. Triggers include "analyze security," "security risks," "security audit," "security review," "is this skill safe," or "check for vulnerabilities."
---

# Skill Security Analyzer

Analyze Claude skills for security risks, vulnerabilities, and safety concerns before deployment.

## When to Use This Skill

Use this skill whenever security analysis of a Claude skill is requested, including:
- "Analyze the security of this skill"
- "What are the security risks in my [skill-name]?"
- "Review this skill for vulnerabilities"
- "Is this skill safe to deploy?"
- "Check this skill for security issues"
- "Audit this skill before I use it"

## Analysis Process

Security analysis follows a systematic workflow:

1. **Extract skill contents** - If provided as a .skill file, extract and examine all components
2. **Review skill metadata** - Analyze name, description, and stated purpose
3. **Examine SKILL.md** - Review instructions and identify potential risks
4. **Inspect bundled resources** - Analyze scripts, references, and assets for security issues
5. **Cross-reference patterns** - Check against known security patterns (see references/security_patterns.md)
6. **Generate findings** - Compile severity-rated list of identified risks
7. **Create output** - Provide executive summary, findings list, and security checklist

## Output Format

Provide three components in this order:

### 1. Executive Summary (2-3 sentences)
Brief overall assessment with key takeaway. Examples:
- "This skill has CRITICAL security risks including undisclosed network access and potential data exfiltration. Do not deploy without major modifications."
- "This skill demonstrates good security practices with minor concerns around input validation. Generally safe for deployment with awareness of limitations."

### 2. Findings List
Severity-rated list of specific security issues found:

**CRITICAL** - Immediate security threat, do not deploy
- [Specific finding with evidence]
- [Specific finding with evidence]

**HIGH** - Significant risk, requires remediation
- [Specific finding with evidence]

**MEDIUM** - Moderate concern, should be addressed
- [Specific finding with evidence]

**LOW** - Minor issue or best practice deviation
- [Specific finding with evidence]

**POSITIVE** - Security best practices observed
- [Specific good practice found]

### 3. Security Checklist
Quick reference checklist of security categories:

```
[ ] Data Exfiltration Risks - [PASS/FAIL/CONCERN] - [brief note]
[ ] Network Access - [PASS/FAIL/CONCERN] - [brief note]
[ ] Prompt Injection Protection - [PASS/FAIL/CONCERN] - [brief note]
[ ] Permissions & Scope - [PASS/FAIL/CONCERN] - [brief note]
[ ] PII/Confidential Data - [PASS/FAIL/CONCERN] - [brief note]
[ ] Malicious Code Indicators - [PASS/FAIL/CONCERN] - [brief note]
[ ] Supply Chain Risks - [PASS/FAIL/CONCERN] - [brief note]
[ ] Credential Exposure - [PASS/FAIL/CONCERN] - [brief note]
[ ] Resource Abuse - [PASS/FAIL/CONCERN] - [brief note]
[ ] Transparency & Documentation - [PASS/FAIL/CONCERN] - [brief note]
```

## Analyzing Skill Components

### Skill Metadata Analysis

Check frontmatter and description for:
- **Scope clarity**: Does description match actual functionality?
- **Tool disclosure**: Are all used tools mentioned?
- **External service disclosure**: Are API calls or network access mentioned?
- **Data handling statements**: Is data processing clearly explained?

**Red flags:**
- Vague descriptions that don't explain what the skill does
- Description doesn't mention tools used in code
- Missing disclosure of network access or external services

### SKILL.md Analysis

Read the entire SKILL.md and check for:
- **Instruction clarity**: Are instructions clear and unambiguous?
- **Input handling**: How does skill handle user input?
- **Tool usage justification**: Is tool usage appropriate for stated purpose?
- **Prompt construction**: Are there prompt injection risks?
- **Scope boundaries**: Does skill stay within stated purpose?

**Specific checks:**
1. Search for dynamic prompt construction patterns
2. Check for file access instructions without validation
3. Look for network requests not in description
4. Identify any instruction override patterns
5. Review error handling and data exposure

### Scripts Analysis

For each script in `scripts/`:
1. **Read the script** using the view tool
2. **Check imports** against security patterns reference
3. **Scan for dangerous operations**: file deletion, system commands, network requests
4. **Look for obfuscation**: base64, exec, eval, encoded strings
5. **Validate paths**: check file access uses safe paths
6. **Review subprocess usage**: check for shell=True or user input in commands

**Priority patterns to detect:**
- `exec()`, `eval()`, `__import__()`
- `subprocess.run(..., shell=True)`
- `requests.post()`, `urllib.request`, `fetch()`
- `os.system()`, `os.popen()`
- `base64.b64decode()` followed by execution
- File operations on sensitive paths
- Hardcoded credentials or API keys

### References Analysis

Check reference files for:
- **Sensitive data**: API keys, credentials, internal schemas
- **External resources**: Links to external sites or services
- **Data handling instructions**: How data should be processed
- **Compliance requirements**: Any regulatory considerations

### Assets Analysis

Examine assets for:
- **Executable content**: Scripts disguised as assets
- **External resources**: Templates that load remote content
- **Embedded credentials**: Config files with secrets
- **Unexpected file types**: Files that don't match skill purpose

## Using the Security Patterns Reference

**IMPORTANT**: Read `references/security_patterns.md` at the start of every security analysis to load the comprehensive catalog of security patterns, anti-patterns, and risk indicators.

The reference provides:
- Detailed examples of risky vs. safe implementations
- Specific patterns to search for in code
- Risk categorization and severity guidelines
- Context-specific considerations

Use the reference to:
1. Guide what to look for during analysis
2. Determine severity ratings for findings
3. Provide accurate examples in findings
4. Ensure comprehensive coverage of risk categories

## Handling .skill Files

If provided a .skill file:

1. **Extract contents**: .skill files are zip files with .skill extension
```bash
unzip skillname.skill -d /home/claude/skill-analysis/
```

2. **Verify structure**: Check for SKILL.md and proper directory organization

3. **Analyze extracted contents**: Follow normal analysis process

## Edge Cases

### Skills Without Code
For skills with only SKILL.md (no scripts/assets):
- Focus on instruction analysis
- Check for prompt injection risks in instructions
- Verify tool usage is appropriate
- Assess scope and transparency

### Third-Party Skills
For skills from unknown sources:
- Apply heightened scrutiny
- Mark provenance as a risk factor
- Look extra carefully for obfuscation
- Check for unexpected functionality
- Recommend code review before deployment

### Skills Requesting Unusual Permissions
For skills asking for extensive tool access:
- Verify each tool is justified in description
- Check if tool usage aligns with stated purpose
- Look for scope creep in implementation
- Consider principle of least privilege

## Important Principles

### Be Evidence-Based
- Always cite specific code or instructions when identifying risks
- Provide line numbers or code snippets for findings
- Don't make assumptions - base findings on actual content

### Severity Calibration
- **CRITICAL**: Immediate security threat (data theft, system compromise, credential exposure)
- **HIGH**: Significant risk requiring remediation (unsafe file access, undisclosed network calls)
- **MEDIUM**: Moderate concern (missing input validation, unclear scope)
- **LOW**: Best practice deviation (no security impact but suboptimal)

### Context Matters
- Consider the skill's stated purpose
- Distinguish between necessary functionality and overreach
- Account for legitimate use cases vs. security theater

### Actionable Findings
- Provide specific remediation guidance when possible
- Suggest safer alternatives for risky patterns
- Prioritize findings by risk and effort to fix

## Example Analysis Workflow

```
User: "Analyze the security of this skill" [attaches my-skill.skill]

1. Extract the .skill file
2. Read references/security_patterns.md
3. Review SKILL.md frontmatter and description
4. Analyze SKILL.md instructions
5. Examine each script in scripts/
6. Check references/ for sensitive data
7. Review assets/ for unexpected content
8. Compile findings with severity ratings
9. Generate executive summary
10. Create security checklist
11. Provide formatted output
```


--- README.md ---
# Nate's Substack Skills

A comprehensive collection of high-quality Claude Code skills covering agentic development, AI vendor evaluation, skill development toolkit, resume building, requirements elicitation, and more.

## About

The included Skills were originally created by **Nate Jones**, author of [Nate's Newsletter](https://natesnewsletter.substack.com/) - a thoughtful publication on AI, product, and strategy.

Nate shares practical, battle-tested insights about how AI tools are actually used by builders and teams. His writing cuts through the hype with clear thinking and actionable approaches. If you're building with AI or thinking about product strategy, his newsletter is essential reading.

**Subscribe to Nate's Newsletter:** https://natesnewsletter.substack.com/

These skills represent the kind of thoughtful, real-world expertise that Nate brings to every piece he writes. Support his work and get regular insights on AI tooling, product development, and strategic thinking.

## Available Skills

This repository contains the following skills:

### Development & Engineering

- **[agentic-development](./agentic-development/)** - Conversational guidance for building software with AI agents, covering workflows, tool selection, prompt strategies, parallel agent management, and best practices
- **[vibe-coding](./vibe-coding/)** - Interactive development approach focused on rapid iteration and exploration

### AI & Prompting

- **[ai-vendor-evaluation](./ai-vendor-evaluation/)** - Framework for evaluating and selecting AI vendors and tools
- **[prompting-pattern-library](./prompting-pattern-library/)** - Comprehensive library of prompting patterns, failure modes, and model-specific guidance
- **[prompt-optimization-analyzer](./prompt-optimization-analyzer/)** - Analyzes skill prompts to identify token waste, anti-patterns, trigger issues, and optimization opportunities

### Skill Development Toolkit

A comprehensive suite of tools for creating, testing, analyzing, and optimizing Claude Code skills:

- **[skill-testing-framework](./skill-testing-framework/)** - Test framework with unit tests, integration tests, and regression tests for validating skill functionality
- **[skill-security-analyzer](./skill-security-analyzer/)** - Security risk analysis for Claude skills, checking for vulnerabilities, data exfiltration risks, and safety concerns
- **[skill-performance-profiler](./skill-performance-profiler/)** - Analyzes skill usage patterns, tracks token consumption, and identifies consolidation opportunities
- **[skill-gap-analyzer](./skill-gap-analyzer/)** - Analyzes your skill library to identify coverage gaps, redundant overlaps, and optimization opportunities
- **[skill-doc-generator](./skill-doc-generator/)** - Auto-generates standardized README documentation from SKILL.md files with consistency validation
- **[skill-dependency-mapper](./skill-dependency-mapper/)** - Visualizes skill dependencies, identifies workflow bottlenecks, and recommends optimal skill stacks
- **[skill-debugging-assistant](./skill-debugging-assistant/)** - Debug and troubleshoot skill issues including trigger failures, parameter problems, and prompt conflicts
- **[token-budget-advisor](./token-budget-advisor/)** - Proactive token budget assessment and task chunking strategy for complex multi-step workflows
- **[learning-capture](./learning-capture/)** - Recognize and capture reusable patterns from work sessions into new skills

### Professional Tools

- **[resume-builder](./resume-builder/)** - Tools and templates for building effective resumes
- **[job-search-strategist](./job-search-strategist/)** - Strategic guidance for job searching and career planning
- **[pitch-deck-builder](./pitch-deck-builder/)** - Framework for creating compelling pitch decks

### Requirements & Planning

- **[requirements-elicitation](./requirements-elicitation/)** - Structured approach to gathering and documenting project requirements

### Data & Spreadsheets

- **[complex-excel-builder](./complex-excel-builder/)** - Tools for building advanced Excel spreadsheets with formulas and visualizations
- **[xlsx-editor](./xlsx-editor/)** - Utilities for editing and manipulating XLSX files

## How to Use

Each skill directory contains:
- `SKILL.md` - Main skill documentation and instructions
- `references/` - Supporting reference materials (when applicable)
- `assets/` - Templates, examples, and other resources (when applicable)

To use a skill with Claude Code, navigate to the skill directory and reference the `SKILL.md` file.

## Credits

These skills showcase practical, battle-tested approaches to working with AI tools. They reflect the kind of thoughtful, real-world insights that Nate shares regularly in his newsletter.

**Want more content like this?** Subscribe to [Nate's Newsletter](https://natesnewsletter.substack.com/) for regular insights on AI, product development, and strategy.

## License

Please refer to individual skill directories for specific licensing information.


## Links discovered
- [Nate's Newsletter](https://natesnewsletter.substack.com/)
- [agentic-development](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/agentic-development.md)
- [vibe-coding](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/vibe-coding.md)
- [ai-vendor-evaluation](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/ai-vendor-evaluation.md)
- [prompting-pattern-library](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/prompting-pattern-library.md)
- [prompt-optimization-analyzer](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/prompt-optimization-analyzer.md)
- [skill-testing-framework](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/skill-testing-framework.md)
- [skill-security-analyzer](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/skill-security-analyzer.md)
- [skill-performance-profiler](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/skill-performance-profiler.md)
- [skill-gap-analyzer](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/skill-gap-analyzer.md)
- [skill-doc-generator](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/skill-doc-generator.md)
- [skill-dependency-mapper](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/skill-dependency-mapper.md)
- [skill-debugging-assistant](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/skill-debugging-assistant.md)
- [token-budget-advisor](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/token-budget-advisor.md)
- [learning-capture](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/learning-capture.md)
- [resume-builder](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/resume-builder.md)
- [job-search-strategist](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/job-search-strategist.md)
- [pitch-deck-builder](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/pitch-deck-builder.md)
- [requirements-elicitation](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/requirements-elicitation.md)
- [complex-excel-builder](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/complex-excel-builder.md)
- [xlsx-editor](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/xlsx-editor.md)

--- agentic-development/SKILL.md ---
---
name: agentic-development
description: Conversational guidance for building software with AI agents, covering workflows, tool selection, prompt strategies, parallel agent management, and best practices based on real-world high-volume agentic development experience. Use this skill when users ask about setting up agentic workflows, choosing models, optimizing prompts, managing parallel agents, or improving agent output quality.
---

# Agentic Development

This skill provides guidance for building software with AI agents based on real-world experience from high-volume agentic development, specifically Peter Steinberger's "Just Talk To It" methodology developed while building a ~300k LOC TypeScript React application entirely with AI agents.

**Core Philosophy**: Most elaborate frameworks, planning systems, and tooling are premature optimization. Treat AI agents like capable engineers—talk naturally, develop shared context, interrupt when needed, and iterate based on results rather than elaborate plans.

## When to Use This Skill

Apply this skill when users ask about:
- Setting up agentic development workflows
- Choosing tools and models for AI-assisted coding
- Optimizing prompt strategies and context management
- Parallel agent workflows and git management
- Debugging agent behavior or improving output quality
- Evaluating whether to use MCPs, subagents, or other abstractions
- Refactoring strategies with agents
- Testing approaches with AI assistance

## Core Principles

### 1. Think in Blast Radius, Not Complexity

Plan changes by file impact rather than perceived difficulty.

**Application**:
- Before starting, estimate: "Will this touch 3 files or 30?"
- Recognize that multiple large-radius changes prevent isolated commits and complicate recovery
- When an agent takes longer than anticipated, interrupt (escape key) and ask "what's the status?"
- Use "give me a few options before making changes" when uncertain about impact
- Trust that file changes are atomic—agents resume well after interruption

**Guidance Pattern**: When a user describes a task, help gauge blast radius by asking: "How many files do you think this will touch?" This builds intuition for redirecting agents.

### 2. Model Selection and Economics

**Subscription Economics**: Running 4-5 AI subscriptions (~$1k/month) provides effectively unlimited tokens versus per-API-call pricing that costs 5-10x more. This enables context-wasteful usage and multiple parallel agents.

**Model Characteristics**:
- **GPT-5-Codex**: Reads extensively before acting, requires shorter prompts (1-2 sentences often suffice), more cautious with pushback on questionable requests, introverted communication style
- **Claude Sonnet**: More eager to start, requires more explicit direction, verbose communication ("absolutely right", "100% production ready")

**Guidance Pattern**: When advising on model choice, emphasize that model personality affects mental health and productivity. The difference between aggressive optimism (while tests fail) versus quiet progress-making materially impacts burnout.

### 3. Parallel Agents in One Folder

Run 3-8 agents simultaneously in the same directory with one dev server, rather than git worktrees or branch-per-feature.

**Advantages**:
- Test multiple changes at once in running application
- Faster than spawning multiple dev servers or switching branches
- Agents perform atomic commits themselves
- Trade some isolation for velocity gains

**Requirements**:
- Agents must commit only their own changes (requires clear instruction file)
- Single shared dev server for testing
- Accept some git history messiness (clean in batches later)

**Guidance Pattern**: When users struggle with worktrees or branch management, suggest trying parallel agents in one folder for a week. Initial skepticism often gives way to appreciation.

### 4. Screenshots Are 50% of Context Engineering

Drag screenshots into terminal (showing UI, code, or errors) rather than writing lengthy text descriptions.

**Effectiveness**:
- Models excel at visual context
- Find exact strings, match patterns, jump to correct locations
- Takes 2 seconds versus minutes of detailed typing
- More token-efficient than text explanations

**Guidance Pattern**: If a user writes long descriptions of what they see, interrupt and suggest: "Just screenshot it and drag into the terminal."

### 5. Better Models Need Shorter Prompts

More capable models require less prompt verbosity because they compensate through better reconnaissance.

**GPT-5-Codex Pattern**:
- 1-2 sentences often suffice
- Reads extensively before acting
- Strong world knowledge reduces explanation needs

**Claude Pattern**:
- Benefits from more extensive context
- Better comprehension with additional detail
- Requires more explicit direction

**Guidance Pattern**: If a user writes elaborate prompts for GPT-5, suggest trying shorter versions. The model's file-reading behavior often makes detailed specs unnecessary.

### 6. Context Tax Is Real—CLIs Beat MCPs

MCPs consume context tokens on every interaction, creating permanent overhead.

**CLI Advantages**:
- Shows help menu on first incorrect invocation
- Model learns forever without recurring cost
- Example: GitHub MCP costs ~23k tokens; `gh` CLI costs zero
- Models possess strong world knowledge of popular CLIs

**Exception**: Chrome DevTools MCP for closing debugging loops justifies context cost in specific scenarios.

**Guidance Pattern**: When users discuss building an MCP, challenge: "Could this be a CLI instead?" Most tools should be CLIs unless deep integration justifies the context tax.

### 7. Interrupting Agents Is Standard Practice

Break the assumption inherited from traditional programming that tasks must run to completion.

**Practice**:
- Hit escape mid-task and ask "what's the status?"
- Models resume where they stopped
- File changes are atomic
- Enables active steering and course correction

**Guidance Pattern**: When users hesitate about interrupting, reassure: "Think of it like checking in with a junior engineer. They'll pick up where they left off."

### 8. Refactoring Is Low-Focus Work

Spend ~20% of time on agent-driven refactoring when tired or needing less focus.

**Paradigm Shift**: Traditional views require peak concentration for refactoring. With agents, it becomes mechanical work requiring only strategic oversight.

**Refactoring Tasks**:
- Code deduplication (jscpd)
- Dead code removal (knip)
- ESLint plugins (react-compiler, deprecation)
- Adding tests and comments for tricky parts
- Dependency updates and tool upgrades
- File restructuring
- Rewriting slow tests
- Modernizing patterns (e.g., removing unnecessary `useEffect`)

**Guidance Pattern**: When users feel burned out on feature work, suggest: "Try a refactoring day. Queue cleanup tasks and let agents handle mechanical work while maintaining high-level oversight."

### 9. Same-Context Testing Catches Fresh Bugs

After implementing features, request tests in the same context.

**Effectiveness**:
- Agent has full context of what it just built
- Often discovers bugs in its own implementation
- Far more effective than separate test-writing sessions
- Tests reflect actual implementation details

**Exception**: Purely UI tweaks may not warrant immediate testing.

**Guidance Pattern**: Establish this habit: "After each feature or fix, ask the model to write tests. Use the same context." While AI generally writes mediocre tests, this approach helps catch its own bugs.

### 10. Plan Mode Is Workaround Theater

Elaborate planning frameworks, spec-driven development, and multi-agent orchestration often work around model weaknesses rather than embracing strengths.

**Better Approach with Strong Models**:
- Use "let's discuss" or "give me options"
- Model will wait for approval
- No harness ceremony needed

**Spec-Driven Alternative**: For complex features, discuss with the agent, iterate on ideas, optionally request a spec, get review from another model (GPT-5-Pro), then paste useful parts back.

**UI Work Approach**: Deliberately under-spec UI requests, watch the model build in real-time, iterate by morphing chaos into the right shape. Often discovers interesting solutions not initially envisioned.

**Guidance Pattern**: When users describe elaborate planning systems, ask: "What if you just talked to the model about what you want?" If elaborate ceremony is needed, the model probably isn't good enough yet.

### 11. Agent Files Are Organizational Scar Tissue

Instruction files (~800 lines) should evolve organically as problems arise, not be architected upfront.

**Maintenance**:
- Request agent updates when things go wrong
- Treat as living document of codebase quirks and preferences
- Clean periodically as models improve
- Expect content reduction as models gain better world knowledge

**Content Examples**:
- Git instructions for multi-agent workflows
- Product explanation and naming patterns
- Preferred React patterns
- Database migration management
- Testing conventions
- AST-grep rules
- Text-based design system guidelines

**Prompt Style Differences**:
- Claude responds to ALL-CAPS warnings and emphatic language
- GPT-5 prefers natural human language
- Files optimized for one model may not transfer well

**Guidance Pattern**: Don't help users write instruction files from scratch. Instead: "Start with basics, then ask your agent to add notes every time something goes wrong. It'll grow organically into exactly what you need."

### 12. Queue Messages for Lazy Automation

Instead of crafting perfect prompts to motivate continued work, queue multiple "continue" messages when stepping away.

**Mechanism**:
- Model works through queued messages
- Ignores extras when done
- Crude but effective for long-running tasks

**Platform Support**: GPT-5-Codex supports message queuing; Claude Code changed to "steering" behavior instead.

**Guidance Pattern**: "For big refactors, queue a few 'continue' messages and step away. The model will either finish or reach a useful stopping point."

### 13. Intuition Compounds Faster Than Frameworks

Direct interaction with agents builds intuition faster than elaborate frameworks.

**Skills Developed**:
- Blast radius estimation
- Stopping timing
- Under-specification appropriateness
- Test necessity judgment
- Effective steering

**Transfer**: Senior engineering skills (managing humans) apply to managing AI agents.

**Guidance Pattern**: Encourage direct usage over framework exploration: "The fastest path to competence is high-volume direct interaction. Tools will converge toward simplicity as models improve. What won't be commoditized is intuition."

### 14. Subagents and Complexity Are Often Unnecessary

**Historical Context**: 
- May 2024: "Subtasks" for parallelization and context reduction
- Later: Rebranded to "subagents" with instruction packaging
- Often used to work around model limitations

**Alternative Approach**: Use separate terminal panes/windows for different tasks, providing:
- Complete control over context engineering
- Visibility into what's sent
- Easier steering
- No hidden context management

**Guidance Pattern**: When users ask about subagents, suggest: "Try doing that work in a separate terminal window instead. More control and visibility."

### 15. Background Tasks and Tooling Gaps

**Current Limitation**: Not all tools have perfect background task management.

**Workaround Example**: Use tmux for persistent CLI sessions in background:
- "run via tmux" often suffices
- Leverages existing world knowledge
- No custom agent instructions needed

**Guidance Pattern**: When users hit tool limitations, look for standard Unix/development tools that solve the problem. Models often have strong world knowledge of these.

## Prompting Strategies

### For GPT-5-Codex

- **Length**: 1-2 sentences + screenshot often suffice
- **Tone**: Natural human language without emphasis
- **Trigger words for difficult problems**: "take your time", "comprehensive", "read all code that could be related", "create possible hypothesis"
- **Avoid**: ALL-CAPS, excessive emphasis, threatening language

### For Claude

- **Length**: More extensive context beneficial
- **Tone**: Can leverage emphatic language and ALL-CAPS for critical instructions
- **Style**: More explicit direction helpful

### Universal Techniques

- **Images**: Use screenshots liberally—aim for 50% of prompts containing visual context
- **Voice input**: Whispr Flow with semantic correction recommended for efficiency
- **Preservation**: Request "preserve intent" and "add code comments on tricky parts"
- **Intent clarity**: Be explicit about what should and shouldn't change

## Tool Selection

### When to Use Web Search
- Current events or rapidly changing information
- Verifying technical details beyond knowledge cutoff
- Finding documentation for new tools/libraries

### Harness Selection

**GPT-5-Codex**:
- Daily driver for most work
- Efficient context use (~230k usable)
- Fast, lightweight
- Shorter prompts needed
- More careful file reading before acting

**Claude Code**:
- When latest Sonnet capabilities needed
- If verbose communication preferred
- Good for tasks requiring high context (with caveats about context efficiency)

**Local/Open Models**:
- Keep monitoring but not recommended as daily driver yet
- China's models (GLM 4.6, Kimi K2.1) approaching Sonnet 3.7 quality

### Slash Commands (Use Sparingly)

Most interaction should be natural language, but a few commands can be useful:
- `/commit` - When clarifying multi-agent folder commits
- `/review` - Occasionally useful, but review bots often superior
- Custom commands for specific workflows

**Principle**: Avoid ceremony when confident in requests. Develop intuition for when commands add value.

## Conversational Interaction Approach

When providing guidance:

### 1. Assess Current Context
- Inquire about current workflow before suggesting changes
- Understand frustrations (speed, clarity, cost)
- Gauge experience level with agents

### 2. Prioritize High-Impact Changes
- If using worktrees → suggest parallel agents in one folder
- If writing long prompts → suggest shorter ones with screenshots
- If using MCPs → challenge whether CLIs would work better
- If building elaborate frameworks → suggest direct agent interaction

### 3. Foster Intuition Development
- Share principles while emphasizing that intuition develops through practice
- Encourage experimentation with time-boxed trials
- Help develop sense of blast radius, interruption timing, and context management

### 4. Common Frustration Responses

**Git history concerns**
→ Add commit instructions to agent file, or accept messiness and clean in batches

**Instruction adherence issues**
→ Could be model choice (GPT-5 follows better), or instructions need refinement

**Cost concerns**
→ Consider subscriptions vs API pricing; subscriptions are 5-10x cheaper for heavy use

**Output quality issues**
→ Schedule regular refactoring time (20% of work) as low-focus maintenance

**Stopping uncertainty**
→ Develops with intuition; start by interrupting frequently and checking status

### 5. Discourage Over-Engineering
- Most elaborate systems represent premature optimization
- Direct conversation often outperforms complex frameworks
- Tools converge to simplicity as models improve

### 6. Emphasize Direct Experience
- High-volume direct interaction beats framework mastery
- Fastest learning comes from actual usage, not reading
- Senior engineering skills transfer: managing agents parallels managing humans

## Anti-Patterns to Address

When users describe these approaches, gently suggest alternatives:

1. **Elaborate planning documents before starting** → Suggest discussion-based approach
2. **Complex RAG systems for code** → GPT-5 searches adequately
3. **Multiple layers of subagents** → Suggest separate terminal windows
4. **Long instruction files from day one** → Should evolve organically
5. **Excessive MCPs** → Most should be CLIs
6. **Treating agents as non-interruptible** → Emphasize atomicity
7. **Waiting for perfect prompts** → Queue "continue" messages and iterate
8. **Writing all code manually still** → Suggest letting agents handle more, including refactoring
9. **Using elaborate frameworks** → Often unnecessary with good models
10. **Not using screenshots** → Missing 50% of effective context engineering

## Skill Development Progression

As users gain experience, guide them through refinement stages:

### Early Stage (Weeks 1-4)
- Focus on basic workflow: model choice, terminal setup, git management
- Practice interrupting agents and checking status
- Start small instruction file, allow organic growth
- Get comfortable with parallel agents (if applicable)

### Intermediate (Months 2-3)
- Develop blast radius intuition
- Optimize prompting for chosen model
- Establish refactoring rhythm
- Build personal slash commands (if needed)

### Advanced (Months 4+)
- Fine-tune agent instruction file for codebase
- Develop strong intuition for stopping/redirecting timing
- Master context engineering with screenshots
- Experiment with under-specification for creative solutions
- Optimize subscription/API economics

## Model-Specific Patterns

### GPT-5-Codex Behaviors
- Reads extensively before acting
- Sometimes panics mid-refactor and reverts (re-run with soothing language)
- Occasionally forgets bash commands are available
- Rarely replies in wrong language
- May lose lines when scrolling quickly
- Generally honors instruction file well

### Claude Sonnet Behaviors  
- More eager to start working
- Benefits from emphatic instruction style
- Creates random markdown files (older versions)
- May need stronger direction to follow instructions
- Better at context comprehension than file location

## Implementation Principles

1. **Reduce Ceremony**: Most "best practices" are premature optimization
2. **Just Talk To It**: Natural interaction beats elaborate systems
3. **Develop Intuition**: Compounds faster than framework mastery
4. **Embrace Simplicity**: Tools will converge; intuition won't be commoditized
5. **Iterate on Results**: What you see > what you planned
6. **Context Is King**: Screenshots, short prompts, CLI tools over MCPs
7. **Mental Health Matters**: Model personality affects productivity through burnout prevention
8. **Refactoring Is Maintenance**: Regular cleanup at low-focus times keeps codebase healthy
9. **Tests Catch Agent Bugs**: Write them in same context after implementation
10. **Trust the Process**: File changes are atomic, interruption is fine, chaos can be shaped

## Source Attribution

This skill is based on Peter Steinberger's "Just Talk To It - the no-bs Way of Agentic Engineering":
https://steipete.me/posts/just-talk-to-it

Also references his "Optimal AI Workflow" post for foundational concepts:
https://steipete.me/posts/2025/optimal-ai-development-workflow


--- ai-vendor-evaluation/SKILL.md ---
---
name: ai-vendor-evaluation
description: Comprehensive framework for evaluating AI vendors and solutions to avoid costly mistakes. Use this skill when assessing AI vendor proposals, conducting due diligence, evaluating contracts, comparing vendors, or making build-vs-buy decisions. Helps identify red flags, assess pricing models, evaluate technical capabilities, and conduct structured vendor comparisons.
---

# AI Vendor Evaluation

**Version 1.0** | October 2025 | Based on $1.2M average AI spend analysis

---

## Overview

This skill provides a systematic framework for evaluating AI vendors and solutions to avoid the costly mistakes that plague 95% of AI projects. Use when conducting vendor due diligence, evaluating proposals, negotiating contracts, or making strategic AI purchasing decisions.

**Key capabilities:**
- Structured evaluation criteria for AI vendors
- Red flag identification in proposals and demos
- Pricing model analysis and fair market rates
- Technical capability assessment
- Contract term evaluation
- Build vs buy decision framework

---

## Quick Decision Tree

**Start here to determine which references to read:**

```
What stage are you in?

├─ Early exploration (multiple vendors being considered)
│  └─ Read: evaluation-criteria.md, use-case-fit.md
│     Use: scorecard-template.xlsx
│
├─ Evaluating specific proposal or demo
│  └─ Read: red-flags.md, technical-assessment.md
│     Check: pricing-models.md for pricing reasonableness
│
├─ Contract negotiation
│  └─ Read: contract-checklist.md, pricing-models.md
│     Reference: red-flags.md for problematic terms
│
├─ Build vs Buy decision
│  └─ Read: build-vs-buy.md, use-case-fit.md
│     Consider: Total cost of ownership from pricing-models.md
│
└─ Post-purchase review or audit
   └─ Read: evaluation-criteria.md, technical-assessment.md
      Assess: Whether vendor is delivering on promises
```

---

## When to Use This Skill

**Trigger scenarios:**
- "Help me evaluate this AI vendor proposal"
- "What should I look for in AI vendor demos?"
- "Is this pricing reasonable for an AI solution?"
- "Should we build or buy this AI capability?"
- "What questions should I ask this AI vendor?"
- "Help me compare these AI vendors"
- "Review this AI contract for red flags"
- "Conduct due diligence on this AI company"

---

## Core Evaluation Framework

### Phase 1: Initial Screening
**Goal**: Eliminate obviously problematic vendors before deep evaluation

**Key questions:**
- Does the vendor have relevant domain experience?
- Are there verifiable customer references?
- Is the technology approach sound?
- Are pricing and terms transparent?

**Read**: `references/red-flags.md` for disqualifying signals  
**Read**: `references/use-case-fit.md` for domain fit assessment

---

### Phase 2: Deep Evaluation
**Goal**: Assess vendor capabilities systematically across all dimensions

**Evaluation dimensions:**
1. **Technical capability** - Can they actually deliver?
2. **Business viability** - Will they still exist in 2 years?
3. **Pricing fairness** - Are costs reasonable for value delivered?
4. **Implementation risk** - How likely is successful deployment?
5. **Contract terms** - Are legal terms acceptable?

**Read**: `references/evaluation-criteria.md` for comprehensive framework  
**Read**: `references/technical-assessment.md` for technical evaluation  
**Read**: `references/pricing-models.md` for pricing analysis  
**Use**: `assets/scorecard-template.xlsx` to score vendors systematically

---

### Phase 3: Contract Negotiation
**Goal**: Secure favorable terms and avoid costly traps

**Critical areas:**
- Performance guarantees and SLAs
- Data ownership and usage rights
- Pricing structure and escalation terms
- Exit clauses and data portability
- Liability and indemnification

**Read**: `references/contract-checklist.md` for essential terms  
**Reference**: `references/red-flags.md` for problematic contract patterns

---

## Common Vendor Patterns

### The Overpromiser
**Characteristics**: Claims to solve everything, vague on technical details, aggressive sales tactics  
**Red flag**: "Our AI can handle any use case"  
**Response**: Demand specific technical explanations and verifiable references

### The Feature Dumper
**Characteristics**: Long feature lists, complex pricing, unclear core value proposition  
**Red flag**: Can't explain what problem they actually solve  
**Response**: Force clarity on primary use case and success metrics

### The Consultant in Disguise
**Characteristics**: Software license + mandatory professional services  
**Red flag**: Professional services cost more than software  
**Response**: Assess true cost of ownership, consider if you're buying software or consulting

### The Model Wrapper
**Characteristics**: Thin layer over OpenAI/Anthropic APIs with high markup  
**Red flag**: No proprietary technology, just API access + UI  
**Response**: Calculate cost of building similar solution in-house

**Full pattern library**: See `references/red-flags.md`

---

## Build vs Buy Decision Framework

**When to read this section**: Before committing to vendor evaluation, determine if building in-house is better option.

**Key factors:**
1. **Capability availability** - Does suitable vendor solution exist?
2. **Time to value** - Buy: weeks-months, Build: months-years
3. **Total cost** - Consider 3-year TCO for both options
4. **Strategic importance** - Core competency? Build. Commodity? Buy.
5. **Team capability** - Do you have talent to build and maintain?

**Read**: `references/build-vs-buy.md` for detailed decision framework

---

## Using the Scorecard Template

The vendor scorecard enables structured comparison across vendors.

**To use**:
1. Open `assets/scorecard-template.xlsx`
2. List vendors to compare (up to 5)
3. Score each vendor on evaluation criteria (1-5 scale)
4. Review weighted scores and vendor comparison chart
5. Document decision rationale

**Customization**: Adjust weights based on priorities for your specific use case.

---

## Reference Documents

### references/evaluation-criteria.md
Comprehensive scoring framework across all vendor evaluation dimensions. Includes specific questions to ask, what constitutes good/bad answers, and how to weight criteria for different use cases.

**Use when**: Conducting systematic vendor evaluation

---

### references/red-flags.md
Catalog of warning signs indicating problematic vendors. Organized by category: technical red flags, business red flags, pricing red flags, contract red flags, and behavioral red flags.

**Use when**: Initial vendor screening or reviewing proposals

---

### references/pricing-models.md
Guide to AI vendor pricing models (per-seat, usage-based, platform fees, etc.), fair market rates, what drives costs, and how to negotiate. Includes pricing red flags and total cost of ownership analysis.

**Use when**: Evaluating vendor pricing or negotiating contracts

---

### references/technical-assessment.md
Framework for assessing technical capabilities: architecture review, model evaluation, integration complexity, scalability, security, and data handling. Includes specific technical questions to ask.

**Use when**: Deep technical evaluation of vendor capabilities

---

### references/contract-checklist.md
Essential contract terms for AI vendor agreements: performance guarantees, data rights, pricing protection, exit terms, liability, and support commitments. Includes negotiation guidance.

**Use when**: Contract review or negotiation

---

### references/use-case-fit.md
Framework for assessing whether vendor solution actually fits your use case. Includes questions to ask yourself, questions to ask vendor, and warning signs of poor fit.

**Use when**: Initial vendor screening or use case definition

---

### references/build-vs-buy.md
Decision framework for whether to build AI capability in-house vs purchasing vendor solution. Includes total cost analysis, capability assessment, and strategic considerations.

**Use when**: Before committing to vendor evaluation process

---

## Assets

### assets/scorecard-template.xlsx
Structured spreadsheet for vendor comparison with:
- Evaluation criteria organized by category
- Scoring system (1-5 scale) with descriptions
- Weighted scoring based on priorities
- Vendor comparison charts
- Decision documentation section

**Customize**: Adjust criteria weights and add company-specific requirements


--- complex-excel-builder/SKILL.md ---
---
name: complex-excel-builder
description: Comprehensive toolkit for creating multi-tab Excel workbooks for startups and scale-ups. Use this skill when building financial models, operational dashboards, board reports, or analytics workbooks that require data organization, complex calculations, pivot tables, visualizations, and documentation across multiple interconnected sheets. Specialized for startup metrics (ARR, MRR, CAC, LTV), board-level reporting, and data-driven decision making.
---

# Complex Excel Builder

## Purpose

This skill guides the creation of sophisticated, multi-tab Excel workbooks that startups and growing companies need for financial planning, operational analytics, and board reporting. It handles the entire workflow from requirements gathering through final delivery, ensuring GAAP-compliant calculations, best-practice visualizations, and maintainable formulas.

## When to Use This Skill

Use this skill when creating Excel workbooks that include:

**Financial Models**:
- Revenue models with unit economics
- Fundraising models and burn analysis
- Budget planning and variance tracking
- Cash flow projections

**Operational Dashboards**:
- Sales pipeline and conversion analysis
- Marketing spend and CAC tracking
- Customer cohort and retention analysis
- Product metrics and KPI tracking

**Board-Level Reports**:
- ARR/MRR progression and composition
- Key metrics rollup (Rule of 40, LTV:CAC, etc.)
- Departmental performance scorecards
- Strategic initiative tracking

**Data Analysis Workbooks**:
- Multi-source data consolidation
- Cross-tab analysis with pivots
- Trend analysis with visualizations
- Scenario modeling and sensitivity analysis

## Core Workflow

### Phase 1: Requirements Gathering (Conversational)

Start by understanding what the user needs. Use a conversational approach that elicits detailed requirements without overwhelming them.

**Option A: Structured Elicitation (Default)**

Ask questions progressively to build a complete picture:

**Initial Questions**:
1. "What's the primary purpose of this workbook?" (financial model, dashboard, analysis, report)
2. "What decisions will this workbook support?" (fundraising, budgeting, monitoring, board updates)
3. "Who is the primary audience?" (founders, board, team, investors)

**Data Questions**:
4. "What data sources will feed this workbook?" (CSV exports, database dumps, manual entry, API data, PDFs, screenshots)
5. "How frequently will data be updated?" (real-time, daily, weekly, monthly, quarterly)
6. "What time periods should be covered?" (historical lookback, forward projections)

**Metrics Questions**:
7. "What are the 3-5 most important metrics to track?" (let user define, then validate against standard definitions)
8. "Are there specific calculations or formulas you need?" (standard SaaS metrics, custom business logic)
9. "What comparisons matter most?" (time-over-time, budget vs actual, cohorts, segments)

**Output Questions**:
10. "What views or visualizations do you need?" (trends, comparisons, composition, distributions)
11. "Do you need scenario modeling capabilities?" (base/upside/downside cases, sensitivity tables)
12. "What documentation should be included?" (assumptions, data sources, calculation methodology)

**Technical Questions**:
13. "Are there any specific formatting or structure requirements?" (corporate templates, color schemes, layout preferences)
14. "Will this integrate with other systems?" (export format needs, data validation requirements)

**Option B: Requirements Dump (User-Initiated)**

If the user provides comprehensive requirements upfront, acknowledge and confirm understanding:

```
"I understand you need [summarize their requirements]. Let me confirm the key elements:
- Data sources: [list]
- Key metrics: [list]
- Calculations: [list]
- Outputs: [list]
- Timeframes: [details]

Before I start building, are there any other considerations like:
- Specific formatting requirements?
- Integration needs?
- Scenario modeling?
- [Any gaps noticed in their requirements]"
```

**Validation Step** (Always):

Before moving to design, summarize the complete requirements:
```
"Here's what I'll build:

DATA LAYER:
- [Data sources and formats]
- [Import/transformation approach]

CALCULATIONS:
- [Key metrics with standard definitions]
- [Custom business logic]
- [Time period coverage]

OUTPUTS:
- [Tabs/sheets structure]
- [Visualizations and chart types]
- [Pivots and cross-tabs]
- [Documentation]

Does this capture everything, or should I adjust anything?"
```

### Phase 2: Architecture Design

After gathering requirements, design the workbook structure. Always include these standard tabs (customize names as appropriate):

**Standard Tab Architecture**:

1. **Overview/Dashboard Tab** (First tab - always)
   - Executive summary with KPIs
   - Key charts and trends
   - Quick navigation to other tabs
   - Last updated date

2. **Data Tab(s)** (Hidden after setup unless user needs visibility)
   - Raw imported data
   - Organized, deduplicated, validated
   - Clearly structured for formula references
   - Data source documentation

3. **Calculations Tab** (Core model)
   - All intermediate calculations
   - Assumption cells (clearly labeled, blue text)
   - Time series calculations
   - Metric definitions

4. **Summaries Tab(s)** (As needed)
   - Aggregated views by time period
   - Segmented analyses (by product, region, customer type, etc.)
   - Variance analysis (actual vs budget, current vs prior)

5. **Charts Tab** (Visualizations)
   - All charts in one place for easy review
   - Consistent sizing and formatting
   - Clear titles indicating insight

6. **Pivots Tab(s)** (Interactive analysis)
   - Pivot tables for user exploration
   - Slicers for filtering
   - Multiple perspectives on data

7. **Documentation Tab** (Last tab - always)
   - Data sources and refresh dates
   - Calculation methodology
   - Assumptions and their rationale
   - Change log
   - Instructions for updating

**Communicate the design**:
```
"I'll create a workbook with these tabs:
1. [Dashboard] - [What it shows]
2. [Data] - [What it contains]
3. [Calculations] - [What it computes]
...
[Etc.]

This structure ensures [explain benefits: maintainability, auditability, usability]."
```

### Phase 3: Data Processing

Before building Excel formulas, process and prepare data:

**Step 3.1: Load and Inspect Data**

```python
import pandas as pd
from openpyxl import Workbook
import json

# Handle different data formats
if file.endswith('.csv'):
    df = pd.read_csv(file)
elif file.endswith('.json'):
    df = pd.read_json(file)
elif file.endswith('.xlsx'):
    df = pd.read_excel(file)
elif file.endswith('.pdf'):
    # Extract tables from PDF using tabula or camelot
    # Document extraction method in Documentation tab
    pass
# For screenshots: inform user OCR extracted, verify accuracy

# Inspect data
print(f"Shape: {df.shape}")
print(f"Columns: {df.columns.tolist()}")
print(f"Data types:\n{df.dtypes}")
print(f"Missing values:\n{df.isnull().sum()}")
print(f"Sample:\n{df.head()}")
```

**Step 3.2: Clean and Transform**

```python
# Standardize columns
df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')

# Handle missing values
# Document decisions: "Missing dates filled forward", etc.

# Parse dates consistently
date_columns = ['date', 'created_at', 'transaction_date']
for col in date_columns:
    if col in df.columns:
        df[col] = pd.to_datetime(df[col], errors='coerce')

# Add derived columns useful for analysis
if 'date' in df.columns:
    df['year'] = df['date'].dt.year
    df['quarter'] = df['date'].dt.quarter
    df['month'] = df['date'].dt.month
    df['month_name'] = df['date'].dt.strftime('%Y-%m')

# Sort chronologically if time series
if 'date' in df.columns:
    df = df.sort_values('date')

# Remove duplicates
df = df.drop_duplicates()
```

**Step 3.3: Validate Data**

```python
# Check for data quality issues
issues = []

# Check date ranges
if 'date' in df.columns:
    date_range = f"{df['date'].min()} to {df['date'].max()}"
    print(f"Date range: {date_range}")
    
# Check for negative values in fields that shouldn't be negative
numeric_cols = df.select_dtypes(include=['number']).columns
for col in ['revenue', 'amount', 'quantity']:
    if col in df.columns and (df[col] < 0).any():
        issues.append(f"Warning: Negative values found in {col}")

# Check for outliers (values > 3 std dev from mean)
for col in numeric_cols:
    mean = df[col].mean()
    std = df[col].std()
    outliers = df[(df[col] > mean + 3*std) | (df[col] < mean - 3*std)]
    if len(outliers) > 0:
        issues.append(f"Warning: {len(outliers)} potential outliers in {col}")

if issues:
    print("Data quality issues to review:")
    for issue in issues:
        print(f"  - {issue}")
```

### Phase 4: Excel Construction

**Step 4.1: Initialize Workbook**

```python
from openpyxl import Workbook
from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
from openpyxl.utils.dataframe import dataframe_to_rows

wb = Workbook()

# Create all tabs upfront
tab_names = ['Dashboard', 'Data', 'Calculations', 'Summary', 'Charts', 'Pivots', 'Documentation']
for name in tab_names:
    if name == 'Dashboard':
        ws = wb.active
        ws.title = name
    else:
        ws = wb.create_sheet(name)

# Define reusable styles
header_font = Font(bold=True, size=11, color='FFFFFF')
header_fill = PatternFill(start_color='4472C4', end_color='4472C4', fill_type='solid')
input_font = Font(color='0000FF')  # Blue for inputs
formula_font = Font(color='000000')  # Black for formulas
border = Border(
    left=Side(style='thin'),
    right=Side(style='thin'),
    top=Side(style='thin'),
    bottom=Side(style='thin')
)
```

**Step 4.2: Build Data Tab**

```python
data_sheet = wb['Data']

# Write dataframe to Excel
for r_idx, row in enumerate(dataframe_to_rows(df, index=False, header=True), 1):
    for c_idx, value in enumerate(row, 1):
        cell = data_sheet.cell(row=r_idx, column=c_idx, value=value)
        
        # Header formatting
        if r_idx == 1:
            cell.font = header_font
            cell.fill = header_fill
            cell.alignment = Alignment(horizontal='center', vertical='center')
        
        cell.border = border

# Auto-adjust column widths
for column in data_sheet.columns:
    max_length = 0
    column_letter = column[0].column_letter
    for cell in column:
        try:
            if len(str(cell.value)) > max_length:
                max_length = len(str(cell.value))
        except:
            pass
    adjusted_width = min(max_length + 2, 50)
    data_sheet.column_dimensions[column_letter].width = adjusted_width

# Convert to Table for structured references
# This makes formulas more readable and maintainable
from openpyxl.worksheet.table import Table, TableStyleInfo
max_row = data_sheet.max_row
max_col = data_sheet.max_column
table_ref = f"A1:{data_sheet.cell(max_row, max_col).coordinate}"
table = Table(displayName='DataTable', ref=table_ref)
style = TableStyleInfo(
    name='TableStyleMedium2',
    showFirstColumn=False,
    showLastColumn=False,
    showRowStripes=True,
    showColumnStripes=False
)
table.tableStyleInfo = style
data_sheet.add_table(table)

# Add data documentation
doc_sheet = wb['Documentation']
doc_sheet['A1'] = 'Data Sources'
doc_sheet['A1'].font = Font(bold=True, size=14)
doc_sheet['A3'] = 'Data Tab:'
doc_sheet['A3'].font = Font(bold=True)
doc_sheet['B3'] = f'Source: [Document source here]'
doc_sheet['B4'] = f'Date range: {date_range if "date_range" in locals() else "N/A"}'
doc_sheet['B5'] = f'Rows: {len(df)}'
doc_sheet['B6'] = f'Last updated: {pd.Timestamp.now().strftime("%Y-%m-%d %H:%M")}'
```

**Step 4.3: Build Calculations Tab**

Use best practices from `references/formula_best_practices.md`:

```python
calc_sheet = wb['Calculations']

# Section 1: Assumptions (Blue text, clearly labeled)
calc_sheet['A1'] = 'ASSUMPTIONS'
calc_sheet['A1'].font = Font(bold=True, size=14)

# Example assumptions
assumptions = [
    ('Revenue Growth Rate (YoY)', 0.25, '25%'),
    ('Gross Margin %', 0.75, '75%'),
    ('CAC', 5000, '$5,000'),
]

row = 3
for label, value, formatted in assumptions:
    calc_sheet.cell(row, 1, label)
    cell = calc_sheet.cell(row, 2, value)
    cell.font = input_font  # Blue for inputs
    cell.number_format = formatted.replace('%', '0%').replace('$', '$#,##0')
    row += 1

# Section 2: Calculations (Black text, use Excel formulas)
calc_sheet[f'A{row+2}'] = 'CALCULATIONS'
calc_sheet[f'A{row+2}'].font = Font(bold=True, size=14)

row += 4

# CRITICAL: Use Excel formulas, not hardcoded Python calculations
# Example: Calculate metrics using formulas referencing Data tab

calc_sheet.cell(row, 1, 'Total Revenue')
calc_sheet.cell(row, 2, '=SUM(DataTable[revenue])')  # Structured reference
calc_sheet.cell(row, 2).number_format = '$#,##0'

row += 1
calc_sheet.cell(row, 1, 'Average Deal Size')
calc_sheet.cell(row, 2, '=AVERAGE(DataTable[deal_size])')
calc_sheet.cell(row, 2).number_format = '$#,##0'

row += 1
calc_sheet.cell(row, 1, 'Customer Count')
calc_sheet.cell(row, 2, '=COUNTA(DataTable[customer_id])')

# Use XLOOKUP for lookups, SUMIFS for conditional aggregation
# Follow patterns from formula_best_practices.md
```

**Step 4.4: Build Summary/Analysis Tabs**

```python
summary_sheet = wb['Summary']

# Time series summary example
summary_sheet['A1'] = 'Monthly Summary'
summary_sheet['A1'].font = Font(bold=True, size=14)

headers = ['Month', 'Revenue', 'Customers', 'Avg Deal Size', 'MoM Growth %']
for col, header in enumerate(headers, 1):
    cell = summary_sheet.cell(3, col, header)
    cell.font = header_font
    cell.fill = header_fill

# Use SUMIFS/AVERAGEIFS to aggregate by month
# Example for a month:
row = 4
summary_sheet.cell(row, 1, '2024-01')  # Month
summary_sheet.cell(row, 2, '=SUMIFS(DataTable[revenue], DataTable[month_name], A4)')
summary_sheet.cell(row, 3, '=COUNTIFS(DataTable[month_name], A4)')
summary_sheet.cell(row, 4, '=B4/C4')  # Avg = Total / Count
summary_sheet.cell(row, 5, '=(B4-B3)/B3')  # MoM growth
summary_sheet.cell(row, 5).number_format = '0.0%'

# Copy formulas down for all months
# (Repeat or use Python loop to populate all months)
```

**Step 4.5: Create Charts**

Use best practices from `references/visualization_best_practices.md`:

```python
from openpyxl.chart import LineChart, BarChart, Reference

charts_sheet = wb['Charts']

# Chart 1: Revenue Trend (Line Chart - max 4 lines)
chart1 = LineChart()
chart1.title = "Monthly Revenue Trend"
chart1.style = 2
chart1.y_axis.title = 'Revenue ($)'
chart1.x_axis.title = 'Month'

# Reference data from Summary tab
data = Reference(summary_sheet, min_col=2, min_row=3, max_row=15, max_col=2)
categories = Reference(summary_sheet, min_col=1, min_row=4, max_row=15)
chart1.add_data(data, titles_from_data=True)
chart1.set_categories(categories)

# Chart sizing and placement
chart1.width = 15  # inches
chart1.height = 7.5  # ~2:1 aspect ratio
charts_sheet.add_chart(chart1, 'A1')

# Chart 2: Revenue by Segment (Bar Chart - horizontal)
# Use bar chart for categorical comparisons
chart2 = BarChart()
chart2.type = 'bar'  # Horizontal bars
chart2.title = "Revenue by Customer Segment"
chart2.y_axis.title = 'Segment'
chart2.x_axis.title = 'Revenue ($M)'

# ... configure chart2 data references ...

charts_sheet.add_chart(chart2, 'A30')

# AVOID: Pie charts, 3D charts, crowded line charts (>4 lines)
# PREFER: Bar charts for comparisons, line charts for trends (≤4 lines)
```

**Step 4.6: Create Pivot Tables**

```python
pivots_sheet = wb['Pivots']

# Pivot tables require careful setup
# For complex pivots, document the structure for user to recreate manually
# Or provide the aggregated data that would result from the pivot

pivots_sheet['A1'] = 'Pivot Analysis'
pivots_sheet['A1'].font = Font(bold=True, size=14)
pivots_sheet['A3'] = 'Instructions:'
pivots_sheet['A4'] = '1. Select Data tab'
pivots_sheet['A5'] = '2. Insert > PivotTable'
pivots_sheet['A6'] = '3. Configuration:'
pivots_sheet['A7'] = '   - Rows: [Customer Segment]'
pivots_sheet['A8'] = '   - Columns: [Quarter]'
pivots_sheet['A9'] = '   - Values: Sum of [Revenue]'

# Alternatively, pre-build aggregated tables that mimic pivot outputs
```

**Step 4.7: Build Dashboard**

```python
dashboard = wb['Dashboard']

# Title and date
dashboard['A1'] = '[Company Name] - [Report Title]'
dashboard['A1'].font = Font(bold=True, size=16)
dashboard['A2'] = f'As of: {pd.Timestamp.now().strftime("%B %d, %Y")}'

# KPI cards (large numbers at top)
dashboard['A4'] = 'Key Metrics'
dashboard['A4'].font = Font(bold=True, size=14)

kpis = [
    ('ARR', '=Calculations!B10', '$#,##0'),
    ('MRR', '=Calculations!B11', '$#,##0'),
    ('Customers', '=Calculations!B12', '#,##0'),
    ('NRR', '=Calculations!B13', '0.0%'),
]

col = 1
for label, formula, fmt in kpis:
    dashboard.cell(5, col, label)
    dashboard.cell(5, col).font = Font(bold=True)
    dashboard.cell(5, col).fill = PatternFill(start_color='E7E6E6', fill_type='solid')
    
    cell = dashboard.cell(6, col, formula)
    cell.font = Font(size=20, bold=True)
    cell.number_format = fmt
    
    col += 3  # Space between KPIs

# Embed key charts from Charts tab
# (Charts can be copied to Dashboard for at-a-glance view)

# Navigation
dashboard['A20'] = 'Navigation:'
dashboard['A21'] = '→ Detailed calculations: See "Calculations" tab'
dashboard['A22'] = '→ All visualizations: See "Charts" tab'
dashboard['A23'] = '→ Interactive analysis: See "Pivots" tab'
```

**Step 4.8: Complete Documentation Tab**

```python
doc_sheet = wb['Documentation']

sections = [
    ('Data Sources', [
        'Data Tab: [Source description]',
        'Last updated: [Date]',
        'Update frequency: [Frequency]',
        'Data quality notes: [Any issues or caveats]'
    ]),
    ('Calculation Methodology', [
        'ARR: Sum of annualized recurring revenue from active contracts',
        'MRR: Monthly recurring revenue (ARR / 12)',
        'CAC: Total S&M spend / new customers acquired',
        '[Other metric definitions]'
    ]),
    ('Assumptions', [
        'Growth Rate: Based on [rationale]',
        'Churn Rate: Historical average of [X]%',
        '[Other assumptions]'
    ]),
    ('Usage Instructions', [
        '1. To update data: Replace Data tab with new export',
        '2. To recalculate: Formulas auto-update',
        '3. To modify assumptions: Edit blue cells in Calculations tab',
        '4. To create scenarios: Copy Calculations tab, rename, adjust assumptions'
    ]),
    ('Change Log', [
        f'{pd.Timestamp.now().strftime("%Y-%m-%d")}: Initial version',
    ])
]

row = 1
for section_title, bullets in sections:
    doc_sheet.cell(row, 1, section_title)
    doc_sheet.cell(row, 1).font = Font(bold=True, size=12)
    row += 2
    
    for bullet in bullets:
        doc_sheet.cell(row, 1, f'• {bullet}')
        row += 1
    
    row += 1  # Blank line between sections
```

### Phase 5: Validation and Quality Assurance

**Step 5.1: Recalculate Formulas**

```bash
python /mnt/skills/public/xlsx/recalc.py /home/claude/workbook.xlsx
```

**Step 5.2: Check for Errors**

```python
import json

# Parse recalc output
result = json.loads(recalc_output)

if result['status'] == 'errors_found':
    print(f"⚠️  Found {result['total_errors']} formula errors:")
    for error_type, details in result['error_summary'].items():
        print(f"  {error_type}: {details['count']} occurrences")
        print(f"    Locations: {details['locations'][:5]}")  # First 5
    
    # Fix errors and recalculate
    # Common fixes:
    # - #REF!: Fix cell references
    # - #DIV/0!: Add error handling or check denominators
    # - #VALUE!: Check data types in formula
    # - #NAME?: Fix formula function names or defined names
    
else:
    print("✅ All formulas calculated successfully (zero errors)")
```

**Step 5.3: Validate Against Requirements**

Checklist:
- [ ] All requested metrics calculated correctly
- [ ] Formulas use proper definitions (check against `financial_metrics_gaap.md`)
- [ ] Charts follow best practices (check against `visualization_best_practices.md`)
- [ ] Formulas are maintainable (check against `formula_best_practices.md`)
- [ ] All tabs present and properly named
- [ ] Data is properly structured and documented
- [ ] Zero formula errors
- [ ] Documentation complete

### Phase 6: Final Delivery

**Step 6.1: Move to Outputs**

```bash
cp /home/claude/workbook.xlsx /mnt/user-data/outputs/[descriptive_name].xlsx
```

**Step 6.2: Summary for User**

Provide concise summary:
```
"I've created your [workbook type] with:

📊 STRUCTURE:
- [Number] tabs: [list key tabs]
- [Number] data sources integrated
- [Number] calculated metrics

📈 KEY FEATURES:
- [Highlight 2-3 main capabilities]
- Charts following best practices (bar charts for comparisons, line charts for trends)
- GAAP-compliant financial calculations

📝 USAGE:
- Update data: [Simple instruction]
- Modify assumptions: [Where and how]
- Review documentation: See Documentation tab

[View your workbook](computer:///mnt/user-data/outputs/[filename].xlsx)"
```

**Do NOT** provide overly detailed explanations of every tab and formula. Give user access to the file and concise next steps.

## Key Principles

### Financial Calculations

**Always follow GAAP standards**:
- Reference `financial_metrics_gaap.md` for standard metric definitions
- Use proper revenue recognition (ASC 606)
- Calculate LTV, CAC, churn correctly
- Document any non-GAAP metrics

**Common startup metrics**:
```
ARR = Sum of annual recurring revenue
MRR = ARR / 12
CAC = (Sales + Marketing Expense) / New Customers
LTV = (Avg Revenue per Customer / Churn Rate) × Gross Margin
Payback Period = CAC / (MRR × Gross Margin)
NRR = (Start MRR + Expansion - Contraction - Churn) / Start MRR
Rule of 40 = Growth Rate % + Profit Margin %
```

### Formula Best Practices

**Always** reference `formula_best_practices.md` for:
- Use XLOOKUP, not VLOOKUP
- Use SWITCH/IFS, not nested IFs
- Use SUMIFS/COUNTIFS for conditional aggregation
- Use structured table references, not cell ranges
- Make formulas scalable and auditable
- Never hardcode values - always use cell references

### Visualization Best Practices

**Always** reference `visualization_best_practices.md` for:
- ❌ Avoid: Pie charts, 3D charts, crowded line charts (>4 lines)
- ✅ Use: Bar charts (comparisons), line charts (trends, max 4 lines), waterfall charts (variance)
- Choose right chart type for data story
- Use clean, colorblind-safe colors
- Label clearly with units
- Minimize chart junk

### Color Coding Standards

Follow financial modeling conventions:
- **Blue text**: Hardcoded inputs/assumptions users change
- **Black text**: Formulas and calculations
- **Green text**: References to other sheets in same workbook
- **Red text**: External links to other files
- **Yellow background**: Cells needing attention

### Error Prevention

- Run `recalc.py` after creating/modifying workbook
- Fix ALL errors before delivery (target: zero #REF!, #DIV/0!, #VALUE!, etc.)
- Test edge cases (zeros, negatives, missing data)
- Validate formulas manually for 2-3 sample calculations

## Bundled Resources

### References (Load as Needed)

**`financial_metrics_gaap.md`**: 
- GAAP revenue recognition (ASC 606)
- Standard SaaS metrics (ARR, MRR, CAC, LTV, NRR)
- Growth metrics and ratios
- Common calculation errors to avoid
- Model structure best practices

**`formula_best_practices.md`**:
- Modern Excel functions (XLOOKUP, SWITCH, IFS, SUMIFS)
- Formula anti-patterns to avoid
- Structured table references
- Error handling best practices
- Performance optimization

**`visualization_best_practices.md`**:
- Chart type selection guide
- What NOT to use (pie charts, 3D, etc.)
- Color and formatting guidelines
- Dashboard design principles
- Accessibility and testing

### When to Use References

- **Before building**: Review relevant reference(s) to incorporate best practices
- **During validation**: Check calculations against GAAP standards
- **When stuck**: Consult formula best practices for better approach
- **For charts**: Follow visualization guidelines for professional output

## Common Workbook Patterns

### Pattern 1: Sales Analysis Workbook

**Tabs**: Dashboard | Data | Monthly Summary | Cohort Analysis | Charts | Documentation
**Key Metrics**: Revenue, Deal Size, Win Rate, Sales Cycle, Pipeline Coverage
**Charts**: Monthly revenue trend, deal size distribution, win rate by segment

### Pattern 2: Marketing CAC Workbook

**Tabs**: Dashboard | Spend Data | Conversions | CAC Calculations | Channel Analysis | Charts | Documentation  
**Key Metrics**: CAC by channel, Payback Period, LTV:CAC, Channel ROI
**Charts**: CAC trend over time, spend by channel (bar chart), payback period waterfall

### Pattern 3: Board Metrics Workbook

**Tabs**: Dashboard | ARR/MRR Detail | Customer Metrics | Financial Summary | Charts | Documentation
**Key Metrics**: ARR, MRR, NRR, Growth Rate, Burn Rate, Rule of 40
**Charts**: ARR progression, MRR composition (stacked bar), cohort retention, runway

### Pattern 4: Financial Model

**Tabs**: Dashboard | Assumptions | Historical | Projections | Scenarios | Charts | Documentation
**Key Metrics**: Revenue, Gross Margin, Operating Expenses, EBITDA, Cash
**Charts**: Revenue projection, cash runway, expense breakdown

## Tips for Success

1. **Start with requirements**: Don't jump to building. Understand the need first.
2. **Design before coding**: Plan tab structure before writing formulas.
3. **Use formulas, not hardcoding**: Excel should recalculate, not just display Python results.
4. **Follow standards**: Use GAAP definitions, modern Excel functions, appropriate charts.
5. **Document thoroughly**: Explain data sources, calculations, assumptions.
6. **Validate ruthlessly**: Zero formula errors, test edge cases, check against requirements.
7. **Keep it simple**: Clear is better than clever. Maintainable is better than compact.

## Troubleshooting

**Issue**: Formulas not calculating
**Solution**: Run `recalc.py` script to force recalculation

**Issue**: #REF! errors
**Solution**: Cell references are broken. Check if referenced cells exist.

**Issue**: Data not updating when source changes
**Solution**: Formulas are hardcoded values. Use formulas referencing data, not Python calculations.

**Issue**: Charts are too crowded
**Solution**: Limit line charts to 4 lines max. Use small multiples or filtering.

**Issue**: Metrics don't match standard definitions  
**Solution**: Review `financial_metrics_gaap.md` for correct formulas.

**Issue**: Workbook is slow
**Solution**: Reduce volatile functions (NOW, RAND), use whole-column references carefully, consider manual calculation mode for large models.


## Links discovered
- [View your workbook](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/complex-excel-builder/computer:/mnt/user-data/outputs/[filename].xlsx)

--- job-search-strategist/SKILL.md ---
---
name: job-search-strategist
description: Comprehensive job search strategy skill for analyzing job postings, discovering non-obvious insights, conducting conversational skills-matching interviews, identifying skill development needs, and creating creative, personalized application strategies. This skill should be used when users want help with job applications, career transitions, analyzing job opportunities, or developing targeted job search approaches that help them stand out from other candidates.
---

# Job Search Strategist

## Why This Approach Matters

Most job searches fail not from lack of effort, but from lack of **signal**. Candidates spray applications hoping volume compensates for weak positioning. They don't.

The modern hiring process demands three things:
1. **Clarity**: Know exactly what value you offer and to whom
2. **Proof**: Demonstrate that value through evidence, not claims
3. **Distribution**: Reach decision-makers through channels that bypass noise

This skill treats job searching as a **go-to-market problem**. Like launching a product, you need product-market fit (your skills match their needs), positioning (your narrative stands out), and distribution strategy (you reach buyers effectively). Generic applications are low-signal. This system maximizes signal at every stage.

### What Makes This Different

Traditional job search advice: "Network more, tailor your resume, follow up."

This system: 
- **Research-driven**: Uses web search to uncover non-obvious company insights (funding trajectory, culture patterns, decision-maker priorities)
- **Adaptive**: Conversational skills matching that identifies transferable skills, not just keyword matching
- **Strategic**: Weighted prioritization model that matches tactics to company culture + your strengths (40% + 40% + 20% job level)
- **Measurable**: Built-in KPIs and pipeline tracking to diagnose what's working
- **Repeatable**: Operating rhythm for daily/weekly activities, not just one-off tactics

**Core Principle**: You're not looking for "any job." You're finding the intersection of what you're excellent at, what companies urgently need, and where you have unique leverage. Everything flows from that clarity.

## When to Use This Skill

Use this skill when users:
- Ask for help analyzing a specific job posting or opportunity
- Want to understand if they're a good fit for a role
- Need guidance on highlighting their experience for a particular position
- Want to identify skills they should develop to be competitive
- Request help creating a job search strategy or application approach
- Ask how to stand out to a particular company or hiring manager
- Want to research a company's culture and values
- Need help with non-traditional application methods (LinkedIn outreach, video cover letters, referral strategies, etc.)
- Are transitioning careers and need help identifying transferable skills

## Core Methodology: Four-Phase Approach

Execute these phases sequentially, adapting depth based on user needs and information available.

### Before Starting: Diagnostic

Help users identify where their search needs attention by using the self-diagnostic tool in `/references/templates-and-examples.md`. This quickly reveals whether they need work on:
- **Clarity** (target role, value proposition, positioning)
- **Proof** (portfolio, metrics, credibility assets)
- **Distribution** (outreach, networking, channel strategy)

Users with scores < 12 in any category should prioritize that dimension. This diagnostic prevents wasted effort on distribution when clarity is the real problem.

### Flow Between Phases

Each phase produces specific deliverables that feed the next:
- **Phase 1** → Company scorecard with red/green flags, strategic fit assessment
- **Phase 2** → Skills match matrix, gap identification, unique value proposition
- **Phase 3** → Learning roadmap, portfolio pieces, proof assets
- **Phase 4** → Multi-channel campaign plan, personalized tactics, tracking system

**Critical principle**: Don't skip Phase 1 research even when users are eager to "just apply." Weak signal comes from applying to poorly understood opportunities.

### Phase 1: Deep Job Posting and Company Analysis

Conduct comprehensive analysis to uncover non-obvious insights about the role and organization.

#### Job Posting Analysis

1. **Extract Core Information**
   - Official job title and level (entry, mid, senior, executive)
   - Required vs. preferred qualifications (note if posting distinguishes these)
   - Key responsibilities and scope
   - Compensation details (salary, benefits, equity if mentioned)
   - Work arrangement (remote, hybrid, on-site)

2. **Identify Red and Green Flags**
   - Consult `/references/job-posting-flags.md` for comprehensive lists
   - Create a scorecard tracking all identified flags
   - Pay special attention to:
     * Language patterns (e.g., "fast-paced," "wear many hats," "rockstar")
     * Structural indicators (vague descriptions, unrealistic requirements, salary transparency)
     * Cultural signals ("family atmosphere," specific work-life balance mentions)
   - Weight flags appropriately: some are minor concerns, others are dealbreakers
   - Note: Multiple minor red flags together may indicate systemic issues

3. **Decode Hidden Meanings**
   - "Self-starter with minimal supervision" often means → limited management support
   - "Fast-paced environment" often means → high stress, tight deadlines, possible disorganization
   - "Wear many hats" often means → understaffed, unclear role boundaries
   - "Results-driven" without collaboration mentions often means → high-pressure, metric-focused culture
   - Detailed responsibilities split by essential/preferred often means → realistic, organized planning

4. **Extract Cultural Indicators**
   - Tone and language style (formal vs. casual, inclusive vs. exclusive)
   - Values explicitly stated or implicitly shown
   - How they describe their team and work environment
   - Emphasis on collaboration vs. individual achievement
   - Mentions of diversity, inclusion, work-life balance, professional development

#### Company Research Strategy

Use web search tools extensively to build a comprehensive company profile:

1. **Company Basics**
   - Industry, size, founding date, headquarters location
   - Business model and revenue streams
   - Key products or services
   - Major competitors and market position

2. **Recent News and Developments**
   - Search: `"[company name]" news 2025` or `"[company name]" news past 6 months`
   - Look for: funding rounds, acquisitions, layoffs, leadership changes, product launches
   - Assess trajectory: growing, stable, or struggling?

3. **Funding and Financial Health**
   - For startups: funding stage (seed, Series A/B/C, etc.), total raised, recent rounds
   - For public companies: recent earnings, stock performance, analyst sentiment
   - Search: `"[company name]" funding` or `"[company name]" Series [X]` or `"[company name]" earnings`

4. **Culture Research**
   - **Glassdoor/Indeed reviews**: Search `"[company name]" Glassdoor reviews` or use web_fetch on Glassdoor URL
     * Look for patterns in reviews, not just overall rating
     * Pay attention to: management quality, work-life balance, career growth, compensation fairness
     * Note both positive and negative recurring themes
     * Check if reviews mention specific departments or locations
   - **LinkedIn research**:
     * Search `"[company name]" employee LinkedIn` to find current employees
     * Look at their posts: Do they seem engaged? Do they share company content positively?
     * Check employee backgrounds: diverse paths? long tenures? recent hires?
   - **Company social media**: Twitter, LinkedIn company page, blog
     * How do they present themselves?
     * Do they celebrate employees?
     * What do they post about?

5. **Leadership Assessment**
   - Search for CEO/leadership team backgrounds and reputations
   - Look for interviews, thought leadership, public statements
   - Assess: Do their values align with yours? Are they respected in the industry?

6. **Growth Stage and Stability**
   - Early stage (seed to Series A): high risk, high opportunity for impact, role may evolve significantly
   - Growth stage (Series B/C): scaling challenges, need for process, rapid change
   - Mature/public: more stable, established processes, potentially slower advancement
   - Note: Match growth stage to candidate's career preferences

#### Synthesis and Pattern Recognition

After gathering data, synthesize insights:

1. **Risk Assessment**
   - Financial stability indicators
   - Cultural health signals
   - Role clarity and organizational maturity
   - Overall red flag score

2. **Opportunity Assessment**
   - Growth potential (company and personal)
   - Mission alignment
   - Skill development opportunities
   - Overall green flag score

3. **Strategic Fit Analysis**
   - Does this role align with candidate's career trajectory?
   - Are there unique opportunities here?
   - What are the trade-offs?

#### Phase 1 Checkpoint: Deliverables

Before moving to Phase 2, ensure you've created:

**1. Company Scorecard** (document or structured output):
```
Company: [Name]
Role: [Title]
Overall Fit Score: [X/30]

Red Flags (Score: X/10):
- [Flag 1 with explanation]
- [Flag 2 with explanation]

Green Flags (Score: X/10):  
- [Flag 1 with explanation]
- [Flag 2 with explanation]

Strategic Fit (Score: X/10):
- Career alignment: [assessment]
- Growth opportunity: [assessment]
- Mission resonance: [assessment]

Key Insights:
- [Non-obvious insight 1]
- [Non-obvious insight 2]
- [Non-obvious insight 3]

Recommendation: [Apply/Proceed with caution/Pass] because [reasoning]
```

**2. Decision Point**: Should the candidate proceed? 
- **Score 20-30**: Strong opportunity, proceed to full skills matching
- **Score 15-19**: Moderate fit, abbreviated skills matching to confirm
- **Score < 15**: Likely pass unless compelling unique factor

**3. Research Assets Gathered**:
- Company news articles (recent 6 months)
- Glassdoor review patterns documented
- Hiring manager profile/background
- Employee connection list (potential referrals)

**Transition to Phase 2**: Share the scorecard with the candidate. Frame next steps: "Based on this analysis, I see [X opportunities and Y concerns]. Let's explore how your experience maps to what they're looking for."

### Phase 2: Conversational Skills-Matching Interview

Conduct an adaptive, conversational interview to elicit candidate skills and map them to job requirements. This should feel like a collaborative exploration, not an interrogation.

#### Interview Principles

- **Be conversational**: Use natural language, show genuine interest
- **Follow the thread**: Let the conversation flow naturally, don't rigidly follow a script
- **Dig deeper**: When candidates mention relevant experience, ask follow-up questions
- **Recognize transferable skills**: Help candidates see how experience from other domains applies
- **Build confidence**: Frame questions positively, highlight strengths authentically

#### Interview Flow

1. **Opening and Context Setting**
   - Summarize key findings from job/company analysis
   - Share overall assessment: red flags, green flags, strategic fit
   - Frame the interview: "Let's explore how your experience maps to what they're looking for"

2. **Core Competency Exploration**
   For each major requirement or responsibility in the job posting:
   
   - **Direct exploration**: "The role emphasizes [skill/requirement]. Tell me about your experience with this."
   - **Project-based inquiry**: "Can you walk me through a project where you [relevant action]?"
   - **Challenge-based inquiry**: "Have you faced situations where you needed to [relevant challenge]? How did you handle it?"
   - **Scale/context questions**: "What was the scope? Team size? Timeline? What were the constraints?"

3. **Transferable Skills Discovery**
   When candidates have experience from different industries or roles:
   
   - **Analogous situation exploration**: "Even though you worked in [other industry], did you encounter similar challenges?"
   - **Skill abstraction**: "The core skill here is [abstracted skill]. Where have you demonstrated that?"
   - **Reframing experience**: "What you did at [previous company] actually demonstrates [job requirement]. Can you tell me more about that?"

4. **Gap Identification (Tactful)**
   For areas where candidate lacks direct experience:
   
   - **Adjacent experience**: "While you haven't done [exact thing], have you done [related thing]?"
   - **Learning orientation**: "Is this an area you're interested in developing?"
   - **Importance assessment**: "Some requirements are nice-to-have. How critical do you think [skill] is for this role?"

5. **Unique Value Proposition Discovery**
   - "What unique perspective would you bring to this role?"
   - "What have you done that most other candidates probably haven't?"
   - "What are you especially passionate about in this domain?"
   - "What's something about your background that isn't obvious from your resume?"

6. **Motivation and Fit Exploration**
   - "What excites you most about this opportunity?"
   - "What concerns do you have, if any?"
   - "How does this fit into your career goals?"
   - Based on company culture research: "They seem to value [cultural trait]. How does that resonate with you?"

#### Synthesis: Skills Match Matrix

Create a clear, honest assessment:

**Strong Matches** (candidate has clear, relevant experience)
- [Requirement 1]: [Evidence from candidate]
- [Requirement 2]: [Evidence from candidate]

**Moderate Matches** (transferable skills or adjacent experience)
- [Requirement 3]: [How it transfers]
- [Requirement 4]: [Adjacent experience]

**Gaps** (areas needing development)
- [Requirement 5]: [Current level and development needed]

**Unique Strengths** (differentiators from other candidates)
- [Unique angle 1]
- [Unique angle 2]

**Cultural Fit Assessment**
- Alignment with company values: [High/Medium/Low]
- Comfort with growth stage/environment: [Assessment]
- Work style compatibility: [Assessment]

#### Phase 2 Checkpoint: Deliverables

Before moving to Phase 3, ensure you've created:

**1. Skills Match Matrix** (structured format):
```
Role: [Title] at [Company]
Overall Match Strength: [Strong/Moderate/Developing]

STRONG MATCHES (70%+ confidence):
✓ Requirement: "5+ years product management"
  Evidence: "6 years PM at TechCo, shipped 12 features, managed $2M budget"
  
✓ Requirement: "Data-driven decision making"  
  Evidence: "Built experimentation framework, ran 30+ A/B tests, improved conversion 25%"

MODERATE MATCHES (50-70% confidence):
→ Requirement: "Experience with B2B SaaS"
  Transfer: "B2C experience, but managed enterprise partnerships at scale"
  
→ Requirement: "Team leadership"
  Adjacent: "Led cross-functional initiatives with 8 people, no direct reports yet"

GAPS (< 50% confidence):
⚠ Requirement: "SQL and data analysis"
  Current: "Basic Excel, no SQL experience"
  Critical? Medium - nice-to-have, not essential
  
UNIQUE STRENGTHS:
★ Marketing background → understands user acquisition deeply (rare for PM)
★ Built side project in this exact product category → domain passion
★ Knows hiring manager from previous company → warm referral possible

CULTURAL FIT:
• Values alignment: HIGH (both emphasize user-first, experimentation)
• Growth stage comfort: HIGH (thrives in ambiguous, fast-moving environments)
• Work style: HIGH (collaborative, data-driven, comfortable with feedback)

CONFIDENCE LEVEL: 75% - Strong fit with addressable gaps
```

**2. One-Sentence Positioning Statement**:
"I help [their target customer] [achieve outcome they care about] through [your unique approach that connects to their needs]."

Example: "I help B2B SaaS companies increase trial-to-paid conversion through experimentation frameworks informed by marketing psychology."

**3. Three-Sentence Career Story** (tailored to this opportunity):
See template in `/references/templates-and-examples.md`

**4. Gap Prioritization List**:
Rank gaps by: Criticality × Learnability × Demonstrability
- High priority: Essential + can learn quickly + can show proof
- Medium priority: Nice-to-have + moderate learning curve
- Low priority: Non-essential + difficult to demonstrate quickly

**Decision Point**: 
- **Match strength 70%+**: Proceed to Phase 4 (application strategy), address minor gaps in parallel
- **Match strength 50-69%**: Proceed to Phase 3 (skill development) for high-priority gaps, then Phase 4
- **Match strength < 50%**: Reassess fit. Are gaps fundamental or bridgeable?

**Transition to Phase 3 or 4**: "Based on our conversation, here's what I see: [strengths summary]. You have [X gaps] to address. Let's create a plan to [close those gaps / apply strategically]."

### Phase 3: Skill Development Strategy

For identified gaps, create actionable development plans.

#### Gap Prioritization

For each skill gap, assess:
1. **Criticality**: Is this essential or nice-to-have?
2. **Learnability**: Can it be learned quickly?
3. **Demonstrability**: Can progress be shown before applying?

Prioritize gaps that are: high criticality + high learnability + high demonstrability

#### Learning Resource Research

Use web search to find specific resources for skill development:

**Online Courses**
- Search: `"[skill]" online course 2024 2025 highly rated`
- Look for: Coursera, Udemy, edX, LinkedIn Learning, specialized platforms
- Prioritize: hands-on projects, certificates, instructor credibility

**Free Resources**
- Search: `"[skill]" free tutorial` or `"learn [skill]" free`
- Look for: YouTube channels, documentation, interactive tutorials, open courseware
- Quality indicators: view counts, recency, community reputation

**Practice Platforms**
- For technical skills: HackerRank, LeetCode, CodeWars, Kaggle
- For design: Dribbble challenges, Daily UI
- For writing: Medium, guest posting opportunities

**Certification Programs** (if valuable for this skill)
- Search: `"[skill]" certification` or `"[skill]" professional certification`
- Assess: industry recognition, time investment, cost vs. benefit

**Community Learning**
- Search: `"[skill]" community` or `"[skill]" Discord/Slack`
- Benefits: peer learning, mentorship, networking

#### Development Timeline

Create realistic timeline:
- **Quick wins (1-2 weeks)**: Online courses, fundamental concepts, small projects
- **Medium-term (1-2 months)**: Deeper skills, substantial projects, portfolio pieces
- **Long-term (3+ months)**: Mastery-level skills, certifications, major projects

#### Portfolio/Proof Development

For each gap being addressed, identify how to demonstrate progress:
- **Project creation**: Build something tangible showing the skill
- **Case study writing**: Document a project applying the skill
- **Open source contributions**: Show real-world application
- **Blog posts/tutorials**: Teach others, demonstrating understanding
- **Certifications**: Formal credentials if industry-relevant

#### Phase 3 Checkpoint: Deliverables

Before moving to Phase 4, ensure you've created:

**1. Skills Development Roadmap** (prioritized and time-bound):
```
Gap: SQL and data analysis
Priority: MEDIUM (nice-to-have for role)
Timeline: 2 weeks

Learning Plan:
Week 1:
- [ ] Complete "SQL for Data Analysis" (Coursera) - 8 hours
- [ ] Practice: SQLZoo exercises, all tutorials
- [ ] Project: Analyze public dataset (Kaggle)

Week 2:  
- [ ] Build dashboard using real data
- [ ] Write blog post: "5 SQL Queries Every PM Should Know"
- [ ] Add project to portfolio with clear problem/solution/impact

Proof Assets:
✓ Certificate from Coursera
✓ GitHub repo with SQL queries and visualization
✓ Blog post published on Medium
✓ Line item on resume: "Self-taught SQL, built dashboard analyzing 50K records"

Success Metric: Can confidently discuss data analysis in interview, show tangible project
```

**2. Portfolio Pieces List**:
For each prioritized gap, identify 1-2 concrete proof assets:
- Gap 1 → [Portfolio piece 1]
- Gap 2 → [Portfolio piece 2]

**3. Resume Bullets** (draft):
Transform new learning into accomplishment statements:
- Before: "Learning Python"
- After: "Built automated reporting tool in Python, reducing manual analysis from 4 hours to 15 minutes weekly"

**4. LinkedIn/Portfolio Updates** (planned):
- [ ] Add new skills to profile
- [ ] Publish learning journey posts (if appropriate)
- [ ] Update headline/summary to reflect expanded capabilities

**5. 30/60/90 Day Tracking**:
```
30 days: Quick wins (courses, small projects, foundational knowledge)
60 days: Substantial proof (portfolio pieces, blog posts, certifications)
90 days: Mastery signals (complex projects, community contributions, teaching others)
```

**Decision Point**:
- **Quick wins achieved (< 2 weeks)**: Proceed to Phase 4, continue learning in parallel
- **Substantial development needed (2+ months)**: Either (a) apply now and emphasize learning plan, or (b) delay application until proof is stronger

**Transition to Phase 4**: "Here's your development plan for the next [timeframe]. Let's now focus on your application strategy while you're building these proof assets."

### Phase 4: Creative Application Strategy

Develop a personalized, multi-channel application strategy that helps the candidate stand out by matching their unique profile to the company's culture and needs.

#### Strategy Prioritization Framework

Use this weighted decision model to prioritize tactics:

**Company Culture Weight (40%)**
- Creative/innovative culture → weight toward video, portfolio projects, bold outreach
- Traditional/corporate culture → weight toward polished docs, LinkedIn, formal channels
- Startup/scrappy culture → weight toward demonstrating initiative, direct founder outreach
- Remote-first culture → weight toward async communication, strong online presence
- People-focused culture → weight toward warm introductions, cultural fit emphasis

**Candidate Skills Weight (40%)**
- Strong video/presentation skills → video cover letter or Loom intro
- Technical skills → GitHub portfolio, code samples, technical blog
- Design skills → portfolio site, case studies, visual resume
- Writing skills → blog posts, Medium articles, content marketing
- Network/connections → referral hunting, warm introductions
- Social media presence → leverage existing platform, thought leadership

**Job Level Weight (20%)**
- Entry-level → emphasize eagerness, projects, culture fit
- Mid-level → emphasize track record, specific achievements
- Senior-level → emphasize leadership, strategy, industry connections
- Executive-level → emphasize vision, network, board connections

#### Research-Based Strategy Development

Conduct targeted research to inform each tactic:

1. **Hiring Manager/Team Research**
   - Search: `"[company name]" "[role type]" manager` or check LinkedIn
   - Find: hiring manager name, their background, their interests, their content
   - Look for: shared connections, shared interests, their thought leadership
   - Strategy adaptation: Can you engage with their content? Reference their work?

2. **Employee Connection Mapping**
   - Search LinkedIn for company employees, especially in target department
   - Look for: second-degree connections (potential warm intros), alumni from your school, former colleagues of yours
   - Check: who's actively posting about company? who seems engaged?
   - Strategy: prioritize warm referral paths

3. **Company Content Analysis**
   - Review company blog, engineering blog, product announcements
   - Identify: what they're excited about, current challenges, future direction
   - Strategy: tailor application to show awareness of their current focus

4. **Recent Initiatives Research**
   - Search: `"[company name]" new initiative` or `"[company name]" just launched`
   - Find: recent product launches, new directions, current priorities
   - Strategy: position yourself as someone who can contribute to these initiatives

5. **Decision-Maker Platform Analysis**
   - Where does leadership spend time? Twitter? LinkedIn? Medium? Podcasts?
   - What do they engage with? What content do they share?
   - Strategy: meet them where they are, engage thoughtfully with their content

#### Tactical Playbook

Based on prioritization, select and customize tactics:

**LinkedIn Outreach Strategy**
- **When to prioritize**: Professional culture, you have connections, hiring manager active on LinkedIn
- **Research first**: Find hiring manager or team members, understand their interests
- **Message template framework**:
  * Opening: brief, specific compliment or shared connection
  * Middle: your unique value proposition for this specific role (2-3 sentences)
  * Close: specific ask (informational chat, not pushy job ask)
- **Follow-up**: engage with their content before messaging (thoughtful comments, not just likes)
- **Example search**: `"[hiring manager name]" LinkedIn` then craft personalized message

**Video Cover Letter Strategy**
- **When to prioritize**: Creative culture, you have video skills, role involves presentation/communication
- **Platform**: Loom (for shorter, casual), YouTube (for more produced), Vimeo (for polish)
- **Structure** (keep under 2 minutes):
  * 0-15 seconds: hook - why you're excited about this specific company
  * 15-60 seconds: your unique fit - 1-2 specific examples
  * 60-90 seconds: what you'd bring/contribute
  * 90-120 seconds: call to action
- **Production**: decent audio > perfect video, energy and authenticity > polish
- **Delivery**: include link in cover letter or LinkedIn message

**Portfolio Project Strategy**
- **When to prioritize**: Technical/creative role, you have time, demonstrable skills matter most
- **Research what they need**: recent launches, stated challenges, tech stack
- **Project ideas**:
  * Solve a small problem you notice in their product
  * Build a feature you think they should add
  * Create analysis of their market/competitors
  * Design mockups for improvements
- **Documentation**: GitHub README or blog post walking through your thinking
- **Delivery**: link in application + message to hiring manager: "I was so interested in your [X], I built [Y]"

**Referral Hunting Strategy**
- **When to prioritize**: Any company, but especially if you have network overlap
- **LinkedIn search**: `"[company name]" [your university/previous company]`
- **Second-degree connection strategy**:
  * Find mutual connection
  * Ask your connection for warm intro: "I'm really interested in [specific role] at [company]. I see you know [name]. Would you feel comfortable introducing us?"
- **Alumni networks**: search alumni databases for company employees
- **Approach**: ask for informational chat first, not immediate referral

**Thought Leadership Strategy**
- **When to prioritize**: You have domain expertise, company values thought leadership, enough time before applying (2+ weeks)
- **Content creation**:
  * Write Medium post on relevant industry topic
  * Create LinkedIn post analyzing their market/product
  * Share insightful thread on Twitter (if relevant to industry)
- **Quality bar**: must be genuinely insightful, not just promotional
- **Tagging strategy**: don't directly tag hiring manager (too pushy), but use relevant hashtags they follow
- **Timing**: publish 1-2 weeks before applying, reference in application

**Direct Email Campaign Strategy**
- **When to prioritize**: Startup, founder-led, or when you can't find other pathways
- **Finding emails**: Hunter.io, RocketReach, or pattern guessing ([name]@company.com)
- **Email structure**:
  * Subject line: specific and intriguing, not generic "Application for [role]"
  * Body: 3-4 short paragraphs max
  * Hook with specific company knowledge
  * Your unique value in 2-3 sentences
  * Specific ask or call to action
- **Timing**: Tuesday-Thursday, 10 AM-2 PM in their timezone

**Social Proof Strategy**
- **When to prioritize**: You have testimonials, notable accomplishments, or public validation
- **Gather ammunition**:
  * LinkedIn recommendations from impressive people
  * Metrics from previous work (growth %, revenue, users, etc.)
  * Public speaking, publications, awards
- **Packaging**: create one-pager with testimonials + metrics
- **Delivery**: attach to application or link in outreach

**Company Event/Meetup Strategy**
- **When to prioritize**: Company hosts events, you're in same city, networking skills strong
- **Research**: search `"[company name]" events` or `"[company name]" meetup`
- **Preparation**: 
  * Prepare 30-second intro focused on mutual interests, not job hunting
  * Have 2-3 thoughtful questions about company/product
  * Bring business cards or easy way to connect
- **Follow-up**: LinkedIn connection within 24 hours referencing specific conversation
- **Application timing**: apply 2-3 days after event, mention meeting in cover letter

#### Application Materials Optimization

Regardless of tactics chosen, optimize core materials:

**Resume Tailoring**
- Use exact keywords from job posting (especially for ATS)
- Reorder bullet points to highlight most relevant experience first
- Quantify achievements with specific metrics
- Remove less relevant experience to keep focus tight

**Cover Letter Framework**
- Opening paragraph: specific reason you're excited about THIS company/role
- Middle paragraph(s): 2-3 examples directly addressing top job requirements
- Closing paragraph: unique value you'd bring + enthusiasm for next steps
- Keep under 400 words, make every sentence count

**Online Presence Audit**
- Google yourself: what appears?
- LinkedIn: updated, professional photo, headline matches career goals
- GitHub (if technical): pinned projects are impressive and documented
- Twitter/social media: nothing inappropriate, ideally some professional content
- Personal website (if relevant): showcases best work, easy to navigate

#### Multi-Touch Campaign Sequencing

For competitive roles, layer tactics over time:

**Week 1**:
- Apply through official channel (establish timestamp)
- LinkedIn connection request to hiring manager (no message yet)

**Week 2**:
- Engage with company content on LinkedIn (thoughtful comment)
- Reach out to potential referral connection

**Week 3**:
- If no response: follow-up LinkedIn message to hiring manager (brief, adds new info)
- Or: share relevant content/project you created

**Week 4**:
- Final touchpoint: brief email if you have address, or different angle

**Important**: gauge company signals. If they say "no outreach," respect that. Multi-touch works for companies open to proactive candidates.

#### Cultural Adaptation Examples

**Example 1: Creative Tech Startup**
- **Culture signals**: colorful website, founder tweets memes, employee posts are casual
- **Candidate profile**: strong technical skills + YouTube hobby channel
- **Strategy priority**:
  1. Video cover letter (Loom) showing personality + technical knowledge
  2. Build small project related to their product
  3. Twitter engagement with founder's content
  4. Direct email to founder (less formal tone)

**Example 2: Enterprise B2B SaaS**
- **Culture signals**: professional LinkedIn presence, focus on metrics/results, traditional interview process
- **Candidate profile**: track record of enterprise sales, strong network
- **Strategy priority**:
  1. Referral hunting through LinkedIn (2nd-degree connections)
  2. Polished application materials with specific metrics
  3. LinkedIn outreach to sales leader (professional tone)
  4. Case study document showing relevant achievement

**Example 3: Mission-Driven Nonprofit**
- **Culture signals**: values-forward communication, community engagement, testimonials from beneficiaries
- **Candidate profile**: career changer with relevant volunteer experience
- **Strategy priority**:
  1. Cover letter emphasizing mission alignment and transferable skills
  2. Portfolio of volunteer work and impact metrics
  3. Connections through shared volunteer organizations
  4. Blog post or LinkedIn article about relevant issue

#### Phase 4 Checkpoint: Deliverables

Before executing the campaign, ensure you've created:

**1. Multi-Channel Campaign Plan** (week-by-week):
```
Company: [Name]
Role: [Title]
Campaign Duration: 4 weeks
Priority Tactics: [Top 3 based on prioritization model]

WEEK 1: Foundation
- [ ] Apply through official channel (timestamp)
- [ ] LinkedIn connection to hiring manager (no message)
- [ ] Identify 3 potential referral paths
- [ ] Prepare portfolio piece/project relevant to their needs

WEEK 2: Engagement
- [ ] Engage with company content (2-3 thoughtful comments)
- [ ] Reach out to referral connection #1
- [ ] Share relevant content/insight on your platform

WEEK 3: Direct Outreach  
- [ ] LinkedIn message to hiring manager (value-focused)
- [ ] OR Email if you found address
- [ ] Share portfolio project if relevant
- [ ] Connect with team members (2-3 people)

WEEK 4: Follow-Up
- [ ] Follow up if no response (add new information)
- [ ] Alternative channel (email if you did LinkedIn, vice versa)
- [ ] Reach out to referral connection #2 if needed

SUCCESS METRICS:
- Hiring manager responds: Primary goal
- Informational chat scheduled: Secondary goal  
- Referral secured: Alternative path
- Interview scheduled: Outcome goal
```

**2. Prioritized Tactics List**:
Based on Culture (40%) + Your Skills (40%) + Job Level (20%):
1. [Top tactic with rationale]
2. [Second tactic with rationale]  
3. [Third tactic with rationale]

**3. Message Templates** (customized):
Use templates from `/references/templates-and-examples.md` but personalize:
- LinkedIn connection request: [Drafted]
- LinkedIn message after acceptance: [Drafted]
- Email to hiring manager: [Drafted]
- Referral request to connection: [Drafted]

**4. Portfolio/Proof Assets** (ready to share):
- [ ] Resume tailored to this role (ATS-optimized with keywords)
- [ ] Cover letter draft (300-400 words, company-specific)
- [ ] LinkedIn profile updated and keyword-optimized
- [ ] Portfolio piece URL (if relevant)
- [ ] One-pager with testimonials + metrics (if using social proof strategy)

**5. Tracking Spreadsheet** (initialized):
Set up spreadsheet using template from `/references/templates-and-examples.md`:
- Each touchpoint logged with date, channel, contact
- Response tracking
- Next steps documented
- Weekly rollup calculations ready

**6. Response Scenarios** (prepared):
- If hiring manager responds positively → [Your next step]
- If no response after Week 2 → [Your follow-up plan]
- If referral comes through → [How you'll leverage it]
- If interview scheduled → [Your preparation plan]

**Execution Checklist**:
- [ ] All messages drafted and reviewed
- [ ] Calendar reminders set for each week's tasks
- [ ] Tracking spreadsheet ready
- [ ] All assets (resume, portfolio) finalized and accessible
- [ ] Clear success criteria defined
- [ ] Backup plan if primary tactics don't work

**Measurement Plan**:
Track these metrics weekly:
- Outreach sent: [target: 10-15/week]
- Response rate: [target: 20-30%]
- Conversations scheduled: [target: 2-3/week]
- Pipeline advancement: [applications → screens → interviews]

**Post-Campaign**:
After 4 weeks, review:
- What worked? (double down on this)
- What didn't work? (stop or modify)
- Conversion rates by channel
- Adjust strategy for next opportunity

## Using Bundled Resources

### References

**`/references/job-posting-flags.md`**: Comprehensive database of red flags and green flags to identify in job postings, with detailed explanations of why each matters. Consult this during Phase 1 for thorough job posting analysis.

**`/references/templates-and-examples.md`**: Complete toolkit including:
- Self-diagnostic rubric for identifying weak phases
- Decision matrix for prioritizing target roles
- Job search operating rhythm (daily/weekly/monthly)
- Pipeline metrics tracking template
- Message templates (LinkedIn, email, referral requests)
- Positioning statement formulas
- Three real-world case studies with timelines and tactics
- KPI tracking spreadsheet template
- Visual framework descriptions
- Resume bullet and cover letter formulas
- Conversational interview question bank

**When to use each reference**:
- Start with templates file for self-diagnostic before beginning any phase
- Use job-posting-flags during Phase 1 analysis
- Return to templates for message drafts, tracking setup, and examples throughout
- Reference case studies when user's situation matches one of the patterns

## Best Practices and Tips

### Conversational Approach and Coaching Tone

When using this skill with users, maintain a coaching stance rather than consulting stance:

**Coaching Stance** (Preferred):
- "Tell me about your experience with [X]" → draw out their knowledge
- "What excites you about this opportunity?" → understand motivation
- "How would you approach [challenge]?" → build their thinking
- "I see [strength] in what you shared. Let's build on that." → confidence building
- Ask follow-up questions to deepen understanding
- Help them see their experience through a fresh lens

**Consulting Stance** (Avoid):
- Simply telling them what to do without exploration
- Overwhelming them with all tactics at once
- Making assumptions about their preferences or constraints
- Using jargon without explaining
- Moving too fast through phases without their input

**Pacing**:
- **First message**: Understand their situation, run diagnostic if unclear
- **Second message**: Deep dive on Phase 1 (company analysis) OR Phase 2 (skills matching)
- **Third message**: Continue with remaining phases based on their needs
- **Throughout**: Check in on energy level, adjust depth accordingly

**Adaptive Depth**:
Not every conversation needs full four-phase depth. Adjust based on:
- Their specific question ("Just tell me about red flags" → focused Phase 1)
- Their urgency ("I'm applying tomorrow" → skip Phase 3, fast Phase 4)
- Their engagement level (excited → go deep; overwhelmed → simplify)
- The opportunity quality (dream job → maximum depth; backup option → abbreviated)

**Building Confidence While Being Honest**:
- Lead with strengths, then address gaps: "You have strong [X]. We should also develop [Y]."
- Frame gaps as "development opportunities" not "failures"
- Use "yet" language: "You haven't done [X] yet, but here's how you could..."
- Celebrate transferable skills: "That experience actually demonstrates [value] really well"
- Be honest about poor fits rather than forcing square pegs into round holes

**Red Flags in the Conversation**:
If you notice these, adjust approach:
- User has zero enthusiasm → probe deeper on fit, may not be right opportunity
- User is defensive about gaps → soften language, build confidence first
- User wants shortcuts → explain why research/strategy matters (signal quality)
- User is overwhelmed → simplify, focus on one phase at a time, offer operating rhythm structure

### General Principles

- **Quality over quantity**: Better to do deep research and thoughtful outreach for 3 companies than spray-and-pray 50 applications
- **Authenticity over tricks**: strategies work best when genuinely matched to your personality and skills
- **Persistence with boundaries**: follow up, but respect "no" signals
- **Documentation**: keep a spreadsheet tracking which tactics you used for each application
- **Signal over volume**: One well-researched application beats ten generic ones
- **Measurement drives improvement**: Track conversion rates to diagnose what's working

### Measurement Framework and KPIs

Job search is a funnel. Track these metrics to diagnose breakdowns:

**Top of Funnel** (Distribution):
- Applications sent per week: [target: 5-10 for quality approach]
- Outreach messages sent: [target: 10-15 per week]
- Connection requests: [target: 5-10 per week]
- **Key Metric**: Activity volume (are you doing enough?)

**Middle of Funnel** (Signal Strength):
- Response rate: [target: 20-30% for warm outreach, 5-10% for cold]
- Conversations scheduled: [target: 2-3 per week]
- Recruiter screens: [target: 10-20% of applications]
- **Key Metric**: Conversion rate (is your signal strong?)

**Bottom of Funnel** (Fit):
- Interviews scheduled: [target: 30-50% of screens]
- Final rounds: [target: 40-60% of interviews]
- Offers: [target: 20-30% of final rounds]
- **Key Metric**: Close rate (are you the right fit and interviewing well?)

**Diagnostic Decision Tree**:
```
Low activity volume? 
→ Problem: Not doing enough. Solution: Increase daily/weekly rhythm.

High volume but low response rate?
→ Problem: Weak signal (positioning, targeting, or message quality). 
→ Solution: Revisit Phase 1 (better targeting) and Phase 2 (clearer value prop).

Good response rate but low interview conversion?
→ Problem: Screening/interviewing skills. 
→ Solution: Interview prep (separate from this skill focus).

Interviews but no offers?
→ Problem: Either poor fit or interview performance.
→ Solution: Reassess target roles or interview technique.
```

**Weekly Review Questions**:
1. What's my response rate this week vs. last week?
2. Which channel/tactic is working best?
3. Where is my funnel breaking down?
4. What should I do more of? What should I stop?
5. Am I on track to hit my timeline goal?

### Job Search Operating Rhythm

Consistency beats intensity. Build these into your weekly routine:

**Daily Activities** (30-60 minutes):
- Morning: Review 2-3 new postings, do red/green flag analysis
- Midday: Send 2-3 personalized outreach messages
- Evening: Engage with 5-10 posts from target companies/people
- Before bed: Update tracking spreadsheet

**Weekly Activities** (2-3 hours):
- Monday: Pipeline review + plan week's outreach targets
- Tuesday/Thursday: Deep research on 1-2 priority companies (full Phase 1)
- Wednesday: Apply to 3-5 strategically selected roles
- Friday: Weekly metrics review + strategy adjustment
- Weekend: Create/improve one portfolio piece or skill development

**Monthly Activities** (4-6 hours):
- Comprehensive funnel review: What's working? What's not?
- Update resume/LinkedIn based on what's resonating
- Network expansion: Attend 1-2 events or virtual meetups
- Skill development milestone (complete a course module, finish project)
- Portfolio refresh: Add new work, remove outdated pieces

**When to Adjust Rhythm**:
- If unemployed: Can increase daily volume to 2-3 hours
- If currently employed: Maintain sustainable rhythm to avoid burnout
- If getting traction: Double down on what's working
- If no traction after 4 weeks: Major strategy pivot needed (revisit Phase 1-2)

### Red Flags for This Process

When using this skill, watch for these signs that additional caution is needed:
- Candidate has no enthusiasm for the role (strong signal of poor fit)
- Multiple major red flags identified in company research (suggest reconsidering application)
- Skills gaps are too significant to bridge in reasonable timeframe
- Company culture fundamentally misaligned with candidate's values

### Adapting Depth

Not every application needs full four-phase depth:
- **Quick assessment**: User has specific question → jump to relevant phase
- **Moderate depth**: Promising role → abbreviated research, focused skills matching
- **Full depth**: Dream job or highly competitive → complete process with extensive research and multi-tactic strategy

### Conversation Management

- Keep skills-matching interview conversational, not interrogative
- Celebrate strengths authentically while being honest about gaps
- Help candidates see their experience through fresh lens
- If candidate gets discouraged, refocus on realistic options and development path

### Search Strategy Tips

When using web search:
- Be specific with company names (use quotes: "Company Name")
- Add timeframe qualifiers: "2024 2025" or "past year"
- For culture research, search employee sentiment: "working at [company]" Glassdoor
- For leadership assessment: "[CEO name]" interview OR profile
- For funding: "[company]" Series A OR Series B OR funding
- Cross-reference information from multiple sources

### Ethical Guidelines

- Never suggest misrepresenting skills or experience
- Be honest about skill gaps while framing development positively
- Respect company's stated boundaries (if they say "no outreach," honor that)
- Don't encourage spam or harassment (multi-touch ≠ stalking)
- Acknowledge when a role may not be a good fit rather than forcing it

### Follow-Up and Iteration

After initial strategy is developed:
- Encourage user to report back on what tactics are working
- Adjust strategy based on company responses
- Celebrate small wins (connection acceptance, informational chat, interview invitation)
- If multiple rejections, revisit Phase 2 to reassess fit or Phase 3 to strengthen skills

### Iteration Loops and Continuous Improvement

Job search is not linear—it's iterative. Build these feedback loops:

**Loop 1: Message Optimization** (Test and learn)
- Week 1: Send 10 messages with approach A
- Week 2: Send 10 messages with approach B  
- Compare response rates → double down on winner
- Common tests: Subject lines, message length, value prop framing

**Loop 2: Targeting Refinement** (Pattern recognition)
- Track which company types respond best (size, stage, industry)
- Track which roles match your skills best (IC vs. leadership, scope)
- Narrow focus to highest-conversion targets
- Expand only after establishing pattern of success

**Loop 3: Skills Validation** (Market feedback)
- If consistent feedback: "You lack [X skill]" → prioritize that in Phase 3
- If consistent interest in [Y experience] → emphasize that more
- Your resume should evolve based on what the market responds to

**Loop 4: Strategic Pivots** (Major course corrections)
When to pivot vs. persist:

**Persist if** (give it 6-8 weeks):
- Getting some positive responses but not closing yet
- Clear pattern of interest but minor gaps to address
- Funnel is healthy (response rates 15%+, conversion rates normal)

**Pivot if** (after 6-8 weeks):
- Response rate < 5% consistently
- Feedback consistently says you're overqualified or underqualified
- No enthusiasm for the work (affects your pitch quality)
- Multiple red flags keep appearing in target companies

**Common Pivot Scenarios**:
1. **Too senior/junior for targets** → Adjust role level (or company stage)
2. **Wrong industry/domain** → Shift to adjacent field with better match
3. **Unclear positioning** → Back to Phase 2 for deeper skills mapping
4. **Geographic/comp mismatch** → Adjust expectations or location

**Monthly Retrospective Questions**:
1. What surprised me this month? (about market, myself, or process)
2. What tactic worked better than expected?
3. What tactic was a waste of time?
4. What feedback did I get repeatedly? (skill gap, positioning issue, etc.)
5. Am I still excited about these target roles? Or do I need to reassess?
6. What's the one thing I should change next month?

**Success Patterns to Amplify**:
- If video outreach gets 40% response rate → make more videos
- If referrals convert 3x better than cold → prioritize referral hunting
- If certain companies respond fast → research similar companies
- If specific skill gets mentioned positively → lead with that more

**Failure Patterns to Address**:
- If no responses to cold email → stop cold email, try different channel
- If rejected after interview consistently → interview prep needed (outside this skill)
- If "overqualified" feedback → target more senior roles or emphasize growth interest
- If "not enough experience" → strengthen Phase 3 proof assets

### When to Get External Help

This skill optimizes strategy and execution, but some situations need additional support:

**Consider a career coach when**:
- Fundamentally unclear on career direction (Phase 2 keeps revealing confusion)
- Severe confidence issues affecting pitch quality
- Need accountability and structure
- Interview skills are the bottleneck (outside this skill's scope)

**Consider a resume writer when**:
- Document layout/ATS optimization is weak
- Struggling to articulate achievements effectively
- Want professional polish for executive-level applications

**Consider a recruiter when**:
- Breaking into a new industry where you lack connections
- Targeting specific companies with active recruiter relationships
- Senior-level roles where recruiter networks matter more

**This skill complements but doesn't replace**:
- Interview preparation
- Salary negotiation
- Career direction clarity work
- Emotional/psychological support during job search

### Final Reminders

**Job search is a system, not a event**: 
Consistent daily/weekly activities beat sporadic bursts of effort.

**Measure everything**: 
Without metrics, you're flying blind. Track conversion rates religiously.

**Quality signal beats quantity**: 
Ten well-researched, personalized approaches beat 100 generic applications.

**Adapt based on data**: 
Your strategy should evolve weekly based on what the market tells you.

**Persistence with intelligence**: 
Keep going, but change tactics when data says something isn't working.

**Authenticity wins**: 
The best tactics match your natural strengths and genuine interests.

The market rewards clarity, proof, and smart distribution. This skill gives you the system. Execution and iteration are up to you.


--- learning-capture/SKILL.md ---
---
name: learning-capture
description: Recognize and capture reusable patterns, workflows, and domain knowledge from work sessions into new skills. Use when completing tasks that involve novel approaches repeated 2+ times, synthesizing complex domain knowledge across conversations, discovering effective reasoning patterns, or developing workflow optimizations. Optimizes for high context window ROI by identifying patterns that will save 500+ tokens per reuse across 10+ future uses.
---

# Learning Capture

## Overview

This skill enables continual learning by recognizing valuable patterns during work and capturing them as new skills. It focuses on high-ROI captures: patterns that will save significant context window tokens through frequent reuse.

## Recognition Framework

Monitor for these five types of learning moments:

### 1. Novel Problem-Solving Approaches
**Trigger**: Develop a creative, non-obvious solution to a complex problem that could apply to similar future problems.

**Strong signals**:
- Solution required multi-step reasoning or novel tool combinations
- Approach is generalizable beyond this specific instance
- User expresses satisfaction with the results
- Similar problem type likely to recur

### 2. Repeated Patterns
**Trigger**: User requests similar tasks 2-3 times and a consistent approach emerges.

**Strong signals**:
- Pattern has repeated 2+ times with consistent structure
- User asks "can you do the same thing as before?"
- Task type is clearly ongoing (e.g., weekly reports, monthly communications)
- Each instance requires re-explaining the approach

### 3. Domain-Specific Knowledge
**Trigger**: User explains company processes, terminology, schemas, or standards that span multiple conversations.

**Strong signals**:
- Information accumulates across 2+ conversations
- Knowledge is stable (won't change weekly)
- User frequently asks questions in this domain
- Re-explaining costs 1000+ tokens each time

### 4. Effective Reasoning Patterns
**Trigger**: Discover a particular way of structuring thinking that consistently produces better results.

**Strong signals**:
- Pattern applies to a category of problems, not just one instance
- Results are notably better than simpler approaches
- Structure is teachable and reproducible
- Problem category recurs frequently

### 5. Workflow Optimizations
**Trigger**: Figure out an efficient way to chain tools or steps together that produces comprehensive results.

**Strong signals**:
- Workflow chains 3+ distinct steps
- Pattern generalizes to similar task types
- User appreciates the thoroughness
- Similar workflows likely needed regularly

## Decision Framework

**Offer capture when ALL of the following are true**:

1. **High confidence (>95%) of significant ROI**:
   - Pattern will be reused 10+ times across future conversations
   - Each reuse saves 500+ tokens of re-explanation
   - The skill itself costs <5000 tokens to load

2. **Strong reusability signal present**:
   - Pattern has repeated 2+ times already, OR
   - User explicitly indicates ongoing need ("I do this weekly"), OR
   - Complex domain knowledge worth formalizing, OR
   - Novel workflow with clear generalizability

3. **Not redundant with existing capabilities**:
   - No existing skill already covers this pattern
   - Adds meaningful value beyond general knowledge

**Do NOT offer capture when**:
- First instance of a pattern (wait for repetition)
- Highly context-specific solution (won't generalize)
- Simple task using existing capabilities (no marginal value)
- Creative/one-off work (low reuse probability)
- Ambiguous reusability (unclear if it will recur)

**Consult references/decision-examples.md** for concrete examples of high-confidence vs. low-confidence scenarios.

## Capture Process

### Step 1: Recognize the Learning Moment

While working, monitor for recognition triggers from the framework above. Track:
- Is this a repeated pattern?
- Does this generalize beyond this instance?
- Would formalizing this save significant tokens in future uses?

### Step 2: Evaluate Against Decision Framework

Before offering capture, verify:
- ROI calculation: (Expected_reuses × Tokens_saved) >> Skill_cost
- Strong reusability signal is present
- Not redundant with existing capabilities

If all checks pass, proceed to offer. If uncertain, do NOT offer.

### Step 3: Offer Capture Conservatively

**Timing**: Offer after completing the immediate task, not mid-task.

**Phrasing**: Be concise and specific about what would be captured and why it's valuable.

**Good examples**:
- "I notice I've structured the last three internal comms documents similarly. Would it be helpful to capture this as a skill for future communications?"
- "I've built up understanding of your data architecture across our conversations. Should I formalize this as a skill for more efficient future reference?"
- "The validation workflow I developed seems applicable to your other messy datasets. Worth capturing as a skill?"

**Avoid**:
- Over-explaining the decision reasoning
- Offering when confidence is <95%
- Interrupting task flow to offer

### Step 4: Structure the Draft Skill

When user agrees to capture, create a draft skill file following these steps:

1. **Select appropriate template** from references/skill-templates.md based on learning moment type
2. **Structure the skill** using the template as a guide
3. **Keep it concise**: Focus on what's non-obvious and reusable
4. **Include specific triggers**: Make it clear when to use this skill
5. **Add examples** where helpful for clarity
6. **Save to outputs**: Create the draft at `/mnt/user-data/outputs/[skill-name].skill/`

The draft skill should be ready for user review and upload with minimal editing needed.

### Step 5: Present the Draft

After creating the draft skill:

1. **Provide context**: Briefly explain what the skill captures and why it will be valuable
2. **Highlight key sections**: Point out the most important parts of the skill
3. **Suggest refinements**: Note any areas where user input would improve the skill
4. **Explain next steps**: User reviews, potentially edits, then uploads via the UI for future conversations

## Key Principles

**Conservative by default**: Better to capture 80% of truly valuable patterns than create noise. Only offer when confidence is very high.

**ROI-focused**: Prioritize patterns with high reuse frequency and high token savings per reuse.

**Context window awareness**: Skills cost tokens to load. A skill should pay for itself within 10 uses.

**Interpretable**: Skills are plain text and easy to review, correct, and refine. This transparency is a feature.

**User-controlled**: The manual upload step ensures quality control and user agency over what gets added to the knowledge base.

## Resources

### references/skill-templates.md

Templates for structuring different types of skills based on the learning moment type. Includes:
- Workflow/Process skill template
- Domain Knowledge skill template  
- Task Pattern skill template
- Reasoning/Prompt Pattern skill template
- Template selection guide

Read this file when structuring a captured skill to use the appropriate template.

### references/decision-examples.md

Detailed examples of high-confidence capture scenarios (where to offer) and low-confidence scenarios (where NOT to offer). Includes:
- Concrete examples with signal analysis
- Recognition pattern checklists
- Decision threshold guidelines
- ROI calculation examples

Read this file when uncertain whether a learning moment meets the capture threshold.


--- pitch-deck-builder/SKILL.md ---
---
name: pitch-deck-builder
description: Comprehensive pitch deck creation with conversational discovery, narrative structuring, and context-aware chunking strategies. This skill should be used when users request help creating pitch decks, investor presentations, fundraising decks, or startup presentations. It guides through narrative development, data collection, and professional slide creation.
license: MIT
---

# Pitch Deck Builder

This skill provides a comprehensive workflow for creating professional pitch decks through conversational discovery, narrative structuring, and technical execution with context-aware best practices.

## Overview

Creating effective pitch decks requires balancing compelling storytelling with strategic information architecture. This skill guides the process from initial concept through final presentation, with special attention to:

- Conversational discovery to extract key business insights
- Narrative arc construction that resonates with target audiences
- Modern, uncluttered slide design that emphasizes clarity
- Token-aware chunking strategies to prevent context window overflow
- Iterative refinement based on presentation goals

## When to Use This Skill

Use this skill when:
- Creating investor pitch decks or fundraising presentations
- Building sales decks or business presentations
- Developing product launch presentations
- Crafting founder or team introductions
- Any scenario requiring strategic narrative presentation of business concepts

## Core Workflow

### Phase 1: Discovery Interview

**CRITICAL**: Before creating any slides, conduct a structured discovery interview to understand the pitch requirements. This prevents wasted effort and ensures the final deck aligns with user goals.

The discovery process should be conversational and adaptive, not a rigid questionnaire. Ask 2-3 questions at a time, then build on responses naturally. Focus on understanding the story before collecting details.

#### Stage 1: Foundation (3-5 questions)

Start with high-level questions to understand the pitch context:

1. **Pitch Type & Audience**: "What type of pitch deck are you creating, and who's the primary audience? For example: pre-seed investors, enterprise customers, strategic partners, internal stakeholders, etc."

2. **Core Narrative**: "In 2-3 sentences, what's the most compelling story you want to tell? Think about the transformation or opportunity you're presenting."

3. **Stage & Goals**: "What stage is your company/product at, and what's your goal for this deck? For example: raising $2M seed round, closing enterprise deals, securing partnerships, etc."

4. **Key Constraints**: "Are there any specific constraints I should know about? For example: must be under 15 slides, needs to address specific concerns, follows a particular format, etc."

#### Stage 2: Business Fundamentals (5-8 questions)

Based on initial responses, dive into the business model and value proposition:

1. **Problem Definition**: "Describe the problem you're solving. What pain point exists today, and how acute is it for your target customers?"

2. **Solution Approach**: "What's your solution, and why is it uniquely positioned to solve this problem? What's your unfair advantage?"

3. **Market Opportunity**: "What's the market size and your addressable segment? Are there any particularly compelling market dynamics or timing factors?"

4. **Business Model**: "How do you make money? What are your unit economics or revenue model?"

5. **Traction & Validation**: "What traction or validation do you have? This could be revenue, users, partnerships, pilot results, etc."

6. **Competition**: "Who are your main competitors or alternatives, and how are you differentiated?"

#### Stage 3: Narrative Elements (4-6 questions)

Gather the elements needed to craft a compelling narrative:

1. **Founding Story**: "Is there a compelling founding story or insight that led to creating this company/product?"

2. **Customer Evidence**: "Do you have any customer quotes, case studies, or success stories that illustrate your impact?"

3. **Vision**: "What's your long-term vision? Where do you see this in 3-5 years?"

4. **Team**: "What makes your team uniquely qualified? Any notable backgrounds, expertise, or relevant achievements?"

5. **Milestones**: "What are your key milestones or roadmap items for the next 12-18 months?"

#### Stage 4: Data & Materials (3-4 questions)

Collect specific data points and existing materials:

1. **Quantitative Data**: "What are your key metrics or numbers? For example: revenue, growth rate, user count, retention, CAC/LTV, margins, etc."

2. **Visual Materials**: "Do you have any existing materials I should incorporate? For example: logos, brand colors, product screenshots, team photos, existing slides, etc."

3. **Must-Include Elements**: "Are there any specific slides, data points, or messages that absolutely must be included?"

#### Context Window Management Strategy

**BEFORE proceeding to Phase 2**, assess the context implications:

1. **Calculate estimated token usage**:
   - Discovery interview: ~2,000-4,000 tokens
   - Narrative planning: ~1,500-3,000 tokens
   - Slide content creation: ~500-800 tokens per slide
   - Technical execution: ~3,000-5,000 tokens
   - Base overhead: ~2,000 tokens

2. **Identify chunking needs**:
   - If deck > 15 slides: **STRONGLY RECOMMEND** chunking strategy
   - If deck > 20 slides: **REQUIRE** chunking strategy
   - If user provides extensive existing content: Consider chunking

3. **Propose chunking approach** (if needed):
   "Based on your requirements, we're looking at approximately [X] slides. To maintain quality and avoid context constraints, I recommend we build this deck in [2-3] phases:
   
   - **Phase 1**: Opening slides (Cover, Problem, Solution, Product) - slides 1-[X]
   - **Phase 2**: Business slides (Market, Business Model, Traction, Competition) - slides [X]-[Y]  
   - **Phase 3**: Closing slides (Team, Financials, Milestones, Ask) - slides [Y]-[Z]
   
   We'll create each phase completely, then combine them at the end. This ensures every slide gets proper attention and design quality. Does this approach work for you?"

4. **Document phase boundaries**: If using chunking, clearly document in narrative plan which slides belong to which phase

### Phase 2: Narrative Planning

After completing discovery, create a detailed narrative plan that serves as the blueprint for the deck.

#### Step 1: Determine Slide Sequence

Based on pitch type and discovery insights, recommend an appropriate slide sequence. Common structures:

**Standard Investor Pitch (12-15 slides)**:
1. Cover slide (Company name, tagline, contact)
2. Problem (The pain point)
3. Solution (Your approach)
4. Product/Demo (How it works)
5. Market Opportunity (TAM/SAM/SOM)
6. Business Model (Revenue streams)
7. Traction (Metrics, growth)
8. Competition (Landscape, differentiation)
9. Go-to-Market (Distribution, customer acquisition)
10. Team (Founders, key hires)
11. Financials (Projections, unit economics)
12. Milestones (12-18 month roadmap)
13. Funding Ask (Amount, use of funds)
14. Vision (Long-term impact)
15. Contact/Thank You

**Product Pitch (8-12 slides)**:
1. Cover
2. Problem
3. Solution Overview
4. Product Details/Demo
5. Benefits & ROI
6. Customer Success Stories
7. Competitive Advantage
8. Pricing/Packages
9. Implementation
10. Team/Company
11. Next Steps
12. Contact

**CRITICAL**: Adapt the sequence based on discovery insights. If the user has exceptional traction, move that slide earlier. If the team is the strongest asset, elevate it. The narrative should build momentum toward the most compelling elements.

#### Step 2: Create Detailed Content Outline

For each slide, document:

1. **Slide title**: Clear, assertive statement (not generic labels)
2. **Core message**: The single most important point this slide must convey
3. **Supporting points**: 2-4 key facts, data points, or arguments (bullet form)
4. **Visual strategy**: How this slide should be visually presented
5. **Speaker notes**: 1-3 sentences the presenter should say about this slide
6. **Data requirements**: Specific numbers, percentages, or facts needed

Example outline entry:
```
Slide 7: "We've Grown 40% Month-Over-Month for 6 Months"

Core message: Our consistent growth demonstrates product-market fit and validates our approach

Supporting points:
- 10,000+ active users, up from 500 six months ago
- 85% of users remain active after 30 days (industry benchmark: 25%)
- NPS score of 72 (promoter-heavy user base)
- Zero paid marketing spend - all organic and referral growth

Visual strategy: Large growth chart showing MoM trajectory, with key metrics callouts around it

Speaker notes: "This isn't a spike - it's sustained, compounding growth driven by genuine product love. Our users are becoming our sales team, and retention numbers show they're staying because we're solving a real problem for them."

Data requirements: Monthly user count for last 6 months, retention cohort data, NPS score
```

#### Step 3: Validate Narrative Arc

Review the complete outline and validate:

1. **Opening strength**: Do the first 3 slides hook attention and establish credibility?
2. **Logical flow**: Does each slide build naturally to the next?
3. **Peak placement**: Is the strongest material positioned for maximum impact (typically slides 6-9)?
4. **Emotional resonance**: Are there moments of human connection (customer stories, founding insight, vision)?
5. **Objection handling**: Are likely concerns addressed before they become barriers?
6. **Clear ask**: Is the call-to-action unambiguous and compelling?

Present the complete narrative outline to the user for approval before proceeding to design. Make it easy to review by presenting in a clear, scannable format.

### Phase 3: Design & Technical Execution

Once the narrative plan is approved, execute the technical creation using the pptx skill and html2pptx workflow.

#### Design Principles

**CRITICAL**: Pitch decks must balance professionalism with visual impact. Follow these principles:

1. **Whitespace is strategic**: Each slide should have 40-60% whitespace. Crowded slides lose impact.

2. **One idea per slide**: If a slide tries to communicate multiple ideas, split it into multiple slides.

3. **Typography hierarchy**:
   - Headlines: Large (36-48pt), bold, assertive statements
   - Body text: Readable (18-24pt), limited to 3-5 bullets maximum
   - Data callouts: Extra large (48-72pt) for key numbers
   - Avoid paragraphs: Use bullet points, but even better - use short phrases

4. **Color strategy**:
   - Primary: Brand color for key elements and data visualization
   - Accent: Complementary color for highlights and emphasis  
   - Neutral: Dark gray (not black) for body text, light gray for backgrounds
   - Limit palette to 3-4 colors total

5. **Data visualization**:
   - Prefer charts over tables when showing trends
   - Use large, simple charts with clear labels
   - Highlight the key insight (annotate charts with the "so what?")
   - Remove chart junk - no 3D effects, excessive gridlines, or decorative elements

6. **Layout patterns**:
   - Full-bleed: Image or color background with text overlay (cover, section dividers)
   - Two-column: Text on left, visual on right (most content slides)
   - Centered: Single key message or data point (impact slides)
   - Grid: Multiple items of equal importance (team, customers, features)

#### Technical Implementation

**IMPORTANT**: Read `/mnt/skills/public/pptx/SKILL.md` completely before beginning slide creation. The pptx skill contains critical requirements for html2pptx workflow.

Follow this execution sequence:

1. **Read pptx skill documentation**: `view /mnt/skills/public/pptx/SKILL.md` (no range limits - read entire file)

2. **Read html2pptx documentation**: `view /mnt/skills/public/pptx/html2pptx.md` (no range limits - read entire file)

3. **Install html2pptx if needed**: Verify installation with `npm list -g @ant/html2pptx` and install if necessary

4. **Create shared CSS file**: Define CSS variables for colors, typography, and spacing that will be used across all slides

5. **Create HTML files for each slide**:
   - Use proper dimensions (960px × 540px for 16:9)
   - Embed shared CSS in `<style>` tags
   - Use semantic HTML (`<h1>`, `<p>`, `<ul>`, etc.)
   - Use row/col/fit classes for layout (NOT flexbox)
   - Apply CSS variables consistently
   - Include `class="placeholder"` for chart/table areas

6. **Create conversion script**: Write JavaScript to convert HTML to PPTX using html2pptx library and PptxGenJS

7. **Execute and validate**: Run the conversion script and generate thumbnail grid for visual validation

8. **Iterate if needed**: Fix any layout issues, text overflow, or visual problems identified in thumbnails

#### Chunking Execution Strategy

If using a multi-phase approach (recommended for >15 slides):

1. **Phase execution**:
   - Build Phase 1 completely (HTML + PPTX generation + validation)
   - Save Phase 1 output to `/mnt/user-data/outputs/`
   - Begin new conversation or clear context
   - Load Phase 1 PPTX and narrative plan
   - Build Phase 2 (append to existing PPTX)
   - Repeat for remaining phases

2. **Phase handoff checklist**:
   - [ ] Save phase PPTX file to outputs
   - [ ] Save narrative plan for next phase
   - [ ] Save CSS file for design consistency
   - [ ] Document any design decisions or patterns established
   - [ ] Note current slide number for next phase

3. **Combination approach**:
   - After all phases complete, combine using Python-PPTX or manual merge
   - Validate slide numbering and transitions
   - Generate final thumbnail grid
   - Check for design consistency across all slides

### Phase 4: Refinement & Delivery

After initial creation, support iterative improvement:

1. **Visual review**: Generate and examine thumbnail grid for layout issues

2. **Content refinement**: Offer to adjust messaging, data presentation, or narrative flow based on feedback

3. **Design iteration**: Make styling adjustments, color changes, or layout modifications

4. **Export preparation**: Ensure final PPTX is in `/mnt/user-data/outputs/` with clear filename

5. **Usage guidance**: Provide brief tips for presenting the deck effectively

## Best Practices

### Discovery Quality
- Listen more than prescribe - let the user's story emerge naturally
- Ask "why" and "so what" to uncover deeper insights
- Notice what the user emphasizes - these are often the strongest narrative elements
- Validate understanding by reflecting back key points

### Narrative Construction
- Start with why, not what - establish emotional connection before details
- Use assertive slide titles that make claims, not generic labels ("We're Growing 40% Monthly" not "Traction")
- Place the strongest material in the middle third of the deck (slides 6-9 in a 15-slide deck)
- End with clarity on the ask and vision for impact

### Design Execution
- When in doubt, remove elements rather than add them
- Use data visualization to replace bullet points whenever possible
- Test readability by viewing slides at thumbnail size - key points should still be clear
- Maintain consistent spacing, alignment, and typography throughout

### Context Management
- Propose chunking early if deck will exceed 15 slides
- Document design decisions in CSS comments for phase consistency
- Save phase outputs immediately to prevent context loss
- Keep narrative plan accessible across phases

## Common Pitch Deck Types

### Investor Pitch (Pre-Seed/Seed)
- Focus: Team, problem, early traction
- Length: 12-15 slides
- Tone: Ambitious but realistic, founder-driven
- Key slides: Problem, solution, why now, founding story, early metrics

### Investor Pitch (Series A+)
- Focus: Growth metrics, business model, competitive moats
- Length: 15-18 slides  
- Tone: Data-driven, scalable, market-leading
- Key slides: Growth charts, unit economics, competitive landscape, market expansion

### Sales/Product Pitch
- Focus: Customer value, ROI, implementation
- Length: 10-15 slides
- Tone: Customer-centric, benefit-oriented, practical
- Key slides: Problem/pain point, solution benefits, customer success, pricing, next steps

### Partnership Pitch
- Focus: Strategic alignment, mutual value, capabilities
- Length: 8-12 slides
- Tone: Collaborative, strategic, win-win
- Key slides: Company overview, strategic fit, proposed partnership model, case for collaboration

## References

For additional guidance on pitch deck best practices, narrative construction, and data visualization, see:
- `references/pitch-deck-best-practices.md` - Comprehensive guide to effective pitch decks
- `references/data-visualization-guide.md` - Chart types, design patterns, and storytelling with data
- `references/narrative-frameworks.md` - Story structures and persuasion techniques

## Technical Dependencies

This skill leverages the pptx skill for presentation creation. Ensure the following are available:
- pptx skill at `/mnt/skills/public/pptx/`
- html2pptx library (installed via npm)
- python-pptx and related dependencies

All technical execution should follow the workflows documented in `/mnt/skills/public/pptx/SKILL.md`.


--- prompt-optimization-analyzer/SKILL.md ---
---
name: prompt-optimization-analyzer
description: Active diagnostic tool for analyzing skill prompts to identify token waste, anti-patterns, trigger issues, and optimization opportunities. Use when reviewing skill prompts, debugging why skills aren't triggering, optimizing token usage, or preparing skills for publication. Provides specific, actionable suggestions with examples.
---

# Prompt Optimization Analyzer

## When to Use This Skill

**Always use when:**
- User asks to "review", "analyze", "optimize", or "improve" a skill prompt
- User says a skill "isn't triggering" or "isn't working as expected"
- User wants to "reduce token usage" in a skill
- User is "preparing a skill for publication" or sharing
- User asks "why isn't my skill being called?"
- User wants to "compare" two skill prompt versions

**Consider using when:**
- User shares a skill file for any reason (offer to analyze)
- User mentions a skill is "using too many tokens"
- User describes confusing or inconsistent skill behavior

---

## Analysis Framework

Run each skill prompt through this comprehensive diagnostic checklist. Report findings in order of severity (Critical → High → Medium → Low).

### 1. Trigger Pattern Analysis

**Critical Issues:**
- ❌ Missing or extremely vague description field
- ❌ Description doesn't match actual skill capabilities
- ❌ No clear trigger patterns identifiable from description
- ❌ Trigger patterns overlap heavily with other common skills

**High Priority:**
- ⚠️ Description is too generic ("helps with tasks", "assists users")
- ⚠️ Trigger keywords buried in long description
- ⚠️ Missing key verbs or nouns that users would naturally use
- ⚠️ Ambiguous scope (could apply to too many or too few situations)

**Optimization Opportunities:**
- 💡 Could add explicit trigger examples to description
- 💡 Could make description more action-oriented
- 💡 Could add domain-specific terminology
- 💡 Could clarify when NOT to use skill

**Good Patterns:**
- ✅ Description starts with clear trigger context
- ✅ Includes specific verbs that map to user intent
- ✅ Provides clear scope boundaries
- ✅ Uses "Use when..." or "This skill should be used when..." patterns

---

### 2. Token Efficiency Analysis

**Token Waste Patterns:**

**Redundancy:**
- 🔴 Repeating the same concept in multiple ways without adding clarity
- 🔴 Restating information already in the description
- 🔴 Duplicate examples that teach the same pattern
- 🔴 Verbose explanations where concise language would work

**Example - Before (wasteful):**
```
This skill helps you create documents. It's useful for document creation.
When you need to make a document, this skill can help. Documents that can
be created include reports, letters, and memos.
```

**Example - After (efficient):**
```
Creates professional documents including reports, letters, and memos.
```

**Over-Politeness:**
- 🔴 Excessive apologetic language ("please", "kindly", "if you don't mind")
- 🔴 Unnecessary hedging ("might", "perhaps", "possibly") where direct instruction works
- 🔴 Filler phrases ("it should be noted that", "it's important to mention")

**Example - Before:**
```
You might want to perhaps consider possibly using this approach if you think it could help.
```

**Example - After:**
```
Use this approach when [specific condition].
```

**Bloated Structure:**
- 🔴 Excessive nested XML tags when flat structure would work
- 🔴 Long prose explanations where bullet points are clearer
- 🔴 Including "meta" instructions about the skill itself (usually unnecessary)
- 🔴 Detailed explanations of concepts Claude already knows

**Over-Specified Formatting:**
- 🔴 Dictating exact phrasing for every possible response
- 🔴 Template sentences that reduce flexibility
- 🔴 Formatting rules that don't materially impact quality

**Efficiency Checklist:**
- Calculate estimated token count (rough: 1 token ≈ 4 characters)
- Identify sections that could be condensed by 30%+
- Flag any paragraph longer than 100 words for review
- Look for opportunities to replace prose with structured format

---

### 3. Anti-Pattern Detection

Reference the prompting-pattern-library skill for detailed anti-patterns. Key failures to check:

**Ambiguity Failures:**
- ❌ Unclear success criteria ("make it better", "improve quality")
- ❌ Vague output format requirements
- ❌ Ambiguous scope ("explain X" without audience/depth specification)
- ❌ Undefined technical terms or jargon

**Conflicting Instructions:**
- ❌ "Be concise but comprehensive"
- ❌ "Be creative but follow strict rules"
- ❌ "Be formal but conversational"
- ❌ Multiple competing priorities without clear hierarchy

**Implicit Assumptions:**
- ❌ Assuming Claude knows context not provided in skill
- ❌ Assuming knowledge beyond training cutoff
- ❌ Assuming familiarity with domain-specific processes
- ❌ Assuming Claude can access external state/memory

**Over-Constraint:**
- ❌ So many rules that quality suffers (rule count > 20 is warning sign)
- ❌ Micro-managing phrasing instead of outcomes
- ❌ Restricting Claude's reasoning ability unnecessarily
- ❌ Specifying implementation details instead of desired results

**Under-Specification:**
- ❌ No examples for complex or novel tasks
- ❌ Missing error handling guidance
- ❌ No quality criteria defined
- ❌ Unclear edge case handling

---

### 4. Clarity and Structure Review

**Structural Issues:**
- ⚠️ No clear sections or organization (wall of text)
- ⚠️ Instructions scattered throughout instead of logically grouped
- ⚠️ Missing or unclear headers
- ⚠️ Poor information hierarchy (important stuff buried)
- ⚠️ Inconsistent formatting (switching between styles)

**Language Clarity:**
- ⚠️ Overly complex sentences (25+ words frequently)
- ⚠️ Passive voice where active is clearer
- ⚠️ Abstract concepts without concrete examples
- ⚠️ Technical jargon without definitions
- ⚠️ Pronouns with unclear antecedents

**Better Patterns:**
- ✅ Use imperative voice ("Create X" not "You should create X")
- ✅ One instruction per sentence when possible
- ✅ Concrete examples alongside abstract rules
- ✅ Consistent terminology throughout
- ✅ Clear section headers that preview content

---

### 5. Example Quality Assessment

**Poor Examples:**
- ❌ Examples that don't demonstrate the core pattern
- ❌ Overly simple examples that miss edge cases
- ❌ Examples without explanation of why they work
- ❌ Too many examples teaching the same thing
- ❌ Examples using outdated syntax or practices

**High-Quality Examples:**
- ✅ Show before/after or good/bad contrasts
- ✅ Include "why this works" explanations
- ✅ Cover common edge cases
- ✅ Demonstrate key patterns concisely
- ✅ Realistic scenarios (not toy problems)

**Example Count:**
- 0 examples: Usually needs at least 1-2 for complex tasks
- 1-3 examples: Usually optimal
- 4-6 examples: Carefully evaluate if all are necessary
- 7+ examples: Almost always contains redundancy

---

### 6. Special Pattern Checks

**Tool Usage Instructions:**
- ⚠️ Are tool-calling instructions actually necessary?
- ⚠️ Does skill over-specify when Claude should know?
- ⚠️ Are there instructions about tools Claude doesn't have access to?

**Meta-Instructions:**
- ⚠️ Instructions about "how to use this skill" (usually redundant with description)
- ⚠️ Explaining skill's purpose inside the skill (already in description)
- ⚠️ Documentation for documentation's sake

**Conditional Logic:**
- ⚠️ Complex if/then trees that Claude can reason about independently
- ⚠️ Edge case handling that duplicates Claude's reasoning
- ⚠️ Over-specified decision trees (trust Claude's judgment more)

---

## Output Format

Structure analysis results as follows:

### Skill Overview
- Name: [skill name]
- Estimated token count: [rough estimate]
- Overall assessment: [1-2 sentence summary]

### Critical Issues (Must Fix)
[List any critical problems that will prevent skill from working]

### High Priority Improvements
[List significant improvements that will materially help]

### Token Optimization Opportunities
[Specific sections/patterns that waste tokens with estimates]
- Section X: ~[N] tokens could be saved by [specific change]
- Pattern Y: ~[N] tokens wasted on [specific issue]

### Medium Priority Suggestions
[Helpful improvements that aren't urgent]

### Low Priority Polish
[Nice-to-haves that would marginally improve]

### Rewrite Suggestions
[For any section with critical issues, provide rewritten version]

**Before (X tokens):**
```
[original text]
```

**After (Y tokens, Z% reduction):**
```
[optimized text]
```

### Estimated Impact
- Total potential token savings: ~[N] tokens ([X]%)
- Clarity improvement: [Significant/Moderate/Minor]
- Trigger reliability: [Better/Same/Need testing]

---

## Analysis Process

1. **First Pass - Skim**
   - Get overall sense of skill purpose
   - Check description field first
   - Note structure and organization
   - Flag any obvious red flags

2. **Second Pass - Deep Dive**
   - Run through each checklist section systematically
   - Mark specific line numbers or sections with issues
   - Count approximate tokens in bloated sections
   - Identify patterns (don't just note individual issues)

3. **Third Pass - Synthesize**
   - Prioritize findings by severity and impact
   - Group related issues together
   - Prepare concrete rewrite examples for worst sections
   - Calculate potential savings

4. **Output Generation**
   - Start with most critical issues
   - Be specific (quote exact text, give line numbers)
   - Provide rewrites, not just criticism
   - Estimate token impacts
   - Balance criticism with recognition of what works well

---

## Key Principles

### Be Specific
❌ "The description could be better"
✅ "The description 'helps with tasks' is too vague. Suggest: 'Analyzes code quality metrics and suggests refactoring priorities for Python codebases'"

### Show Impact
❌ "This section is redundant"
✅ "Lines 45-78 repeat concepts from lines 12-23, wasting ~120 tokens (18% of skill)"

### Provide Solutions
❌ "This doesn't work"
✅ "This doesn't work because [reason]. Instead, try: [concrete rewrite]"

### Respect Intent
- Understand what the skill author was trying to achieve
- Preserve core functionality while optimizing
- Don't just delete - replace with better alternatives
- Acknowledge trade-offs in suggestions

### Context Matters
- A 2000-token skill for complex workflows may be appropriate
- A 2000-token skill for simple formatting is bloated
- Judge efficiency relative to task complexity
- Consider how frequently skill will be triggered

---

## Common Optimization Wins

### Quick Wins (Usually Save 20-40% Tokens)

**1. Remove Meta-Commentary**
Delete sections explaining the skill to Claude (Claude will read and understand)

**2. Condense Verbose Prose**
Replace paragraphs with bullet points, remove filler words

**3. Consolidate Examples**
Replace 5 similar examples with 2 contrasting examples

**4. Trust Claude's Reasoning**
Remove over-specified decision trees and edge case handling

**5. Simplify Structure**
Flatten unnecessary XML nesting, remove redundant headers

### Deep Optimizations (Can Save 40-60% Tokens)

**1. Rewrite Entire Sections**
Take bloated sections and rewrite from scratch focusing on clarity

**2. Remove Implicit Instructions**
Delete instructions about things Claude already knows how to do

**3. Consolidate Redundant Concepts**
Merge sections teaching the same pattern multiple ways

**4. Streamline Tool Instructions**
Trust Claude's tool-use abilities more, remove over-specification

**5. Clarify Over Adding**
Instead of adding more examples/explanation, clarify existing content

---

## Red Flags for Common Skill Issues

### "My skill isn't triggering"
**Check:**
1. Description too vague or generic?
2. Description doesn't mention key trigger words?
3. Skill scope overlaps too much with existing skills?
4. Trigger pattern requires exact phrasing user won't say?

### "My skill triggers too often"
**Check:**
1. Description too broad ("helps with writing")?
2. Missing scope boundaries or exclusions?
3. Generic trigger words that match many queries?
4. Need more specific domain terminology?

### "My skill gives inconsistent results"
**Check:**
1. Conflicting instructions?
2. Ambiguous success criteria?
3. Too many conditional branches?
4. Under-specified output requirements?

### "My skill seems slow"
**Check:**
1. Token count >2000? (high context load)
2. Requesting Claude read multiple large references?
3. Over-complicated reasoning chains?
4. Excessive examples or edge case handling?

---

## Reference Integration

This skill works alongside:
- **prompting-pattern-library**: Reference for patterns and anti-patterns
- **token-budget-advisor**: Assess if skill fits within token budgets
- **skill-creator**: Guidelines for building skills initially
- **learning-capture**: Identify patterns from analysis to capture

When analyzing prompts, reference the prompting-pattern-library for detailed pattern explanations. Focus this skill on active diagnosis and concrete suggestions.

---

## Limitations and Caveats

**What this skill can't do:**
- Guarantee a skill will trigger in all desired situations (triggering is complex)
- Test actual skill performance (requires real usage)
- Determine if skill logic is correct for domain (needs domain expertise)
- Predict user behavior or query patterns

**Best used for:**
- Identifying clear anti-patterns and inefficiencies
- Suggesting specific improvements with examples
- Estimating token costs and optimization potential
- Catching common failure modes before publication

**Remember:**
- Analysis is a starting point, not gospel
- Some verbose skills are appropriately complex
- User intent matters more than arbitrary token targets
- Test optimized versions to ensure they still work correctly


--- skill-gap-analyzer/SKILL.md ---
---
name: skill-gap-analyzer
description: Analyzes the user's skill library to identify coverage gaps, redundant overlaps, and optimization opportunities. Use when users want to understand their skill ecosystem, optimize their skill collection, find missing capabilities for common workflows, or reduce redundant coverage. Triggered by requests like "analyze my skills," "what skills am I missing," "are any of my skills redundant," or "optimize my skill library."
---

# Skill Gap Analyzer

## Overview

This skill performs systematic analysis of a user's skill library to identify gaps in capability coverage, redundant overlaps between skills, and opportunities for optimization. It compares existing skills against common workflow patterns to surface actionable recommendations for improving the skill ecosystem.

## Analysis Workflow

The skill gap analysis follows these steps:

1. Inventory current skills
2. Map coverage patterns and capabilities
3. Identify gaps against common workflows
4. Detect redundancies and overlaps
5. Generate prioritized recommendations

## Step 1: Inventory Current Skills

Begin by reviewing the complete list of available skills. Examine each skill's name and description to understand:

- Primary purpose and capabilities
- Domains or file types covered
- Workflows supported
- Trigger patterns and use cases

Create a structured inventory that captures:
- Skill name
- Core capabilities (what it does)
- Primary domains (e.g., documents, presentations, data analysis)
- File type associations (e.g., .docx, .pdf, .xlsx)
- Key workflows (e.g., creation, editing, analysis)

## Step 2: Map Coverage Patterns

Analyze the inventory to identify coverage patterns across multiple dimensions:

**By domain:**
- Document processing (word docs, PDFs, presentations)
- Data and analytics (spreadsheets, databases, visualization)
- Development (coding, debugging, testing)
- Creative (design, content creation, media)
- Communication (writing, presentations, reporting)
- Research (information gathering, analysis, synthesis)
- Business (strategy, planning, operations)

**By workflow stage:**
- Creation (new artifacts from scratch)
- Editing/modification (improving existing work)
- Analysis (extracting insights, understanding)
- Conversion (format transformation)
- Automation (scripting, batch processing)
- Quality assurance (validation, review, testing)

**By file type:**
- Office formats (.docx, .xlsx, .pptx)
- PDFs
- Code files (various languages)
- Media (images, video, audio)
- Data formats (CSV, JSON, databases)
- Web (HTML, React artifacts)

## Step 3: Identify Gaps

Compare the coverage map against common workflows to identify gaps. Consider these high-value workflow categories:

**Document workflows:**
- Creating, editing, analyzing text documents
- Working with forms and templates
- PDF manipulation and form-filling
- Presentation creation and design
- Multi-format document conversion

**Data workflows:**
- Spreadsheet creation with formulas and formatting
- Data analysis and visualization
- Database querying and management
- Report generation and dashboards
- Financial modeling

**Development workflows:**
- Code writing and debugging
- Testing and quality assurance
- Documentation generation
- Package and dependency management
- Deployment and DevOps

**Research workflows:**
- Information gathering and synthesis
- Academic paper analysis
- Competitive research
- Market analysis
- Literature reviews

**Business workflows:**
- Strategic planning documents
- Project management artifacts
- Stakeholder communication
- Performance analysis
- Process documentation

**Creative workflows:**
- Visual design and graphics
- Content writing and editing
- Brand asset creation
- Marketing materials
- Media editing

For each gap identified, assess:
- **Impact**: How frequently would this capability be used?
- **Availability**: Could existing skills partially address this with modification?
- **Complexity**: How difficult would it be to create a skill for this gap?
- **Priority**: High (frequently needed), Medium (occasionally useful), Low (rarely needed)

## Step 4: Detect Redundancies

Identify overlapping capabilities across multiple skills that may indicate redundancy:

**Look for:**
- Multiple skills covering the same file types with similar workflows
- Overlapping domain coverage without clear differentiation
- Similar trigger patterns that might cause confusion
- Duplicated functionality that could be consolidated

**Evaluate each overlap:**
- **Complementary**: Different skills handle different aspects well (keep both)
- **Redundant**: Significant overlap with minimal differentiation (consider consolidating)
- **Partially redundant**: Some overlap but each skill has unique value (clarify boundaries or merge strategically)

**Assessment criteria:**
- Do the skills serve distinctly different use cases?
- Is there clear guidance on when to use each skill?
- Would consolidation improve usability or create confusion?
- Is the redundancy justified by specialization or different approaches?

## Step 5: Generate Recommendations

Synthesize findings into actionable recommendations structured in these categories:

### Critical Gaps (High Priority)
Skills that would address frequently-needed workflows currently not covered. Include:
- Specific workflow or use case not currently supported
- Expected frequency of use
- Potential impact on productivity
- Suggested skill name and core capabilities

### Enhancement Opportunities (Medium Priority)
Areas where existing skills could be extended or improved:
- Existing skill that could be enhanced
- Specific capability additions
- Workflows that would be better supported

### Consolidation Candidates (Redundancy Reduction)
Skills with significant overlap that could be merged or clarified:
- Skills involved in redundancy
- Nature of overlap
- Recommendation to consolidate, differentiate, or maintain status quo
- Trade-offs to consider

### Low-Priority Additions
Nice-to-have capabilities for specialized workflows:
- Workflow or use case
- Why it's lower priority (infrequent use, narrow applicability)
- Potential value if implemented

### Configuration Recommendations
Suggestions for optimizing existing skills:
- Description clarifications for better triggering
- Boundary adjustments between overlapping skills
- Documentation improvements
- Cross-references between related skills

## Output Format

Present analysis in a clear, scannable format:

1. **Executive Summary**: 2-3 sentence overview of findings
2. **Coverage Heatmap**: Visual or structured representation of where skills are concentrated vs. sparse
3. **Critical Gaps**: Prioritized list with justification
4. **Redundancy Analysis**: Specific overlaps with recommendations
5. **Action Items**: Concrete next steps prioritized by impact

Keep recommendations specific and actionable. Avoid vague suggestions—each recommendation should enable the user to immediately understand what to build or change and why it matters.

## Usage Notes

**Trigger this skill when users ask about:**
- "What skills am I missing?"
- "Are my skills redundant?"
- "How can I optimize my skill library?"
- "What workflows aren't covered by my skills?"
- "Analyze my skill ecosystem"
- "Should I consolidate any skills?"

**Don't overthink:**
- Perfect coverage isn't the goal—focus on high-impact gaps
- Some redundancy may be intentional and valuable
- User-specific workflows matter more than theoretical completeness

**Context matters:**
- Consider the user's actual work patterns when assessing gaps
- Ask clarifying questions about workflows they frequently perform
- Prioritize based on their stated needs, not generic best practices


--- token-budget-advisor/SKILL.md ---
---
name: token-budget-advisor
description: Proactive token budget assessment and task chunking strategy. Use this skill when queries involve multiple large file uploads, requests for comprehensive multi-document analysis, complex multi-step workflows with heavy research (10+ tool calls), phrases like "complete analysis", "full audit", "thorough review", "deep dive", or tasks combining extensive research with large output artifacts. This skill helps assess token consumption risk early and recommend chunking strategies before beginning work.
---

# Token Budget Advisor

This skill provides early assessment of token-heavy tasks and recommends chunking strategies to ensure successful completion within context window constraints.

## When to Use This Skill

Trigger this skill **before beginning work** when you detect:

- Multiple file uploads (3+ documents) combined with analysis requests
- Requests for "comprehensive", "complete", "thorough", or "full" analysis
- Multi-document comparative analysis
- Complex workflows requiring 10+ tool calls (extensive web research + synthesis)
- Tasks combining heavy research with large artifacts (reports, presentations)
- Queries spanning multiple dimensions (temporal + categorical + quantitative)
- Requests to "analyze everything" or "create a complete report on all aspects"

## Core Function

This skill serves two purposes:

1. **Early warning system**: Assess whether a task will likely exceed token limits
2. **Strategic planning**: Provide specific, actionable chunking recommendations

## Token Estimation Framework

### Quick Assessment Heuristics

Estimate token consumption using these rough guidelines:

**Input costs:**
- Uploaded document: ~1,000-5,000 tokens each (depending on length)
- Web search result: ~500-1,500 tokens
- Web fetch (full article): ~2,000-8,000 tokens
- Google Drive document: ~1,000-10,000 tokens (varies significantly)

**Output costs:**
- Simple response: 500-2,000 tokens
- Detailed analysis: 2,000-5,000 tokens
- Long-form report: 5,000-15,000 tokens
- Complex artifact (presentation, document): 5,000-20,000 tokens

**Tool call overhead:**
- Each tool call includes the query, results, and reasoning: ~1,000-3,000 tokens average

**Warning thresholds:**

- **Caution zone** (60-80% of budget): Task is achievable but tight; consider efficiency
- **Danger zone** (80-95% of budget): High risk; strongly recommend chunking
- **Exceeds budget** (95%+ of budget): Task requires chunking; cannot complete in one conversation

### Task Complexity Multipliers

Apply these mental adjustments:

- **Synthesis required**: Add 30-50% to output estimate (comparing, integrating multiple sources)
- **Iterative refinement**: Add 20-30% (when task involves reviewing and improving)
- **Multiple formats**: Add 20% per additional output type (report + presentation + spreadsheet)

## Chunking Strategy Framework

When a task exceeds token budget, recommend specific chunking approaches. Choose strategies based on task structure:

### 1. Sequential Processing

**Best for:** Time-series data, chronological analysis, ordered workflows

**Pattern:**
```
"This analysis of 12 months of data will exceed our token budget. I recommend we split it into quarters:
- Part 1: Q1-Q2 analysis (Jan-Jun)
- Part 2: Q3-Q4 analysis (Jul-Dec)  
- Part 3: Synthesis and recommendations

Should I start with Part 1?"
```

**When to use:**
- Historical data analysis
- Period-over-period comparisons
- Multi-phase projects

### 2. Dimensional Breakdown

**Best for:** Multi-faceted analysis, different aspects of same topic

**Pattern:**
```
"A complete market analysis covering financial, competitive, regulatory, and technological factors will strain our token budget. Let's break it into:
- Session 1: Financial performance and market size
- Session 2: Competitive landscape and positioning
- Session 3: Regulatory environment and compliance
- Session 4: Technology trends and synthesis

Which dimension should we tackle first?"
```

**When to use:**
- Multi-stakeholder analysis
- Different analytical lenses on same subject
- Complex business cases

### 3. Depth Progression

**Best for:** Tasks requiring outline → draft → refinement

**Pattern:**
```
"Creating a comprehensive 50-slide presentation with detailed research will exceed our budget. I recommend:
- Round 1: Build structure and outline (30 min)
- Round 2: Develop content for slides 1-25 (45 min)
- Round 3: Develop content for slides 26-50 (45 min)
- Round 4: Refinement pass (30 min)

Let's start with the outline?"
```

**When to use:**
- Large documents or presentations
- When quality refinement is important
- Creative projects benefiting from iteration

### 4. Subset Sampling

**Best for:** Large document sets where representative sampling works

**Pattern:**
```
"Analyzing all 15 contracts will exceed our budget. I suggest:
- Part 1: Analyze 5 representative contracts (different types/dates)
- Part 2: Based on patterns found, confirm with 5 more
- Part 3: Quick scan of remaining 5 for exceptions, then synthesize

This gives thorough coverage while managing tokens. Sound good?"
```

**When to use:**
- Document review at scale
- Pattern identification across many files
- Risk-based sampling approaches

### 5. Parallel Track Processing

**Best for:** Independent workstreams that can be combined later

**Pattern:**
```
"Comparing our product vs 5 competitors across features, pricing, and positioning is too large for one session. Let's split by competitor:
- Session 1: Competitors A & B full analysis
- Session 2: Competitors C & D full analysis  
- Session 3: Competitor E + synthesis matrix

Each session stays focused and manageable."
```

**When to use:**
- Comparative analysis
- Multiple independent subjects
- When parts don't need each other's context

## Communication Guidelines

### Messaging Framework

When recommending chunking, use this structure:

1. **Acknowledge the request clearly**
2. **Provide token budget assessment** (brief, 1 sentence)
3. **Recommend specific chunking approach** (numbered list, 2-4 parts)
4. **Ask for confirmation to proceed** (keep user in control)

**Example:**
```
I'll help you analyze these 8 financial reports and create a comprehensive presentation. 
This task will exceed our token budget given the research and artifact creation required. 
I recommend splitting it into:
1. Reports 1-4: Analysis and key findings
2. Reports 5-8: Analysis and key findings  
3. Synthesize all findings into presentation

Should I start with reports 1-4?
```

### What NOT to Do

❌ Don't over-explain token budgets or get technical about context windows
❌ Don't apologize excessively or sound limiting
❌ Don't provide vague suggestions like "maybe split this up somehow"
❌ Don't start work and then stop mid-task saying "we've run out of tokens"

✅ Do be matter-of-fact and solution-oriented
✅ Do provide specific, actionable breakdowns
✅ Do keep the momentum going toward task completion
✅ Do frame chunking as a quality improvement (thoroughness) not limitation

## Handling Edge Cases

### User Insists on Single Session

If user pushes back on chunking:

```
"I understand you'd prefer this in one go. I'll do my best to complete it, but I may need to:
- Prioritize the most critical elements
- Provide a condensed version
- Focus on breadth over depth in some areas

Let me start and we'll see how far we can get. What aspects are highest priority?"
```

### Task Becomes Too Large Mid-Stream

If partway through, token budget becomes concerning:

```
"I'm about halfway through and we're approaching token limits. I have two options:
1. Complete this current section and continue in a fresh conversation (recommended - maintains quality)
2. Accelerate through remaining sections with lighter analysis

Which approach works better for you?"
```

### Uncertain Estimation

When task scope is ambiguous:

```
"I want to make sure we complete this successfully. The scope could go in different directions. Could you clarify:
- [Specific question about depth]
- [Specific question about breadth]

This helps me gauge whether we should chunk this task."
```

## Integration with Other Skills

This skill works in conjunction with domain-specific skills:

- **Before heavy document analysis**: Check token budget, then use docx/pdf/xlsx skills
- **Before comprehensive research**: Assess scope, then execute research process
- **Before complex artifacts**: Plan chunking, then use pptx/docx/xlsx creation skills

Think of this skill as a "pre-flight check" before embarking on token-intensive tasks.

## Examples in Action

### Example 1: Multi-Document Analysis

**User query:** "Analyze all these uploaded contracts and tell me the common terms and differences"
*[User has uploaded 12 PDF contracts]*

**Response:**
"I'll help you analyze these 12 contracts. Given the volume, I recommend we process this in batches to ensure thorough analysis:

1. Contracts 1-4: Extract key terms, obligations, and unique clauses
2. Contracts 5-8: Same analysis
3. Contracts 9-12: Same analysis  
4. Synthesis: Common terms across all + key differences

This ensures I can give each contract proper attention. Should I start with contracts 1-4?"

### Example 2: Comprehensive Research Report

**User query:** "Research the AI chip market and create a detailed report covering market size, key players, technology trends, and 5-year forecast"

**Response:**
"I'll create a comprehensive AI chip market report. This research and report creation will be substantial, so I recommend:

1. Market overview: Size, growth, segments (research + draft)
2. Competitive landscape: Key players, market share, strategies (research + draft)
3. Technology trends: Current developments, emerging tech (research + draft)
4. Future outlook: 5-year forecast, implications (research + draft)
5. Final report assembly: Executive summary, polish, formatting

Each section gets thorough research and quality writing. Should I begin with the market overview?"

### Example 3: Simple Task (No Chunking)

**User query:** "Summarize this 10-page PDF and give me the key takeaways"

**Response:**
*[No trigger - proceed normally without invoking this skill]*
"I'll read through the PDF and extract the key points for you."

*Rationale: Single document summary is well within normal token budget.*

## Skill Maintenance

This skill should be updated when:

- Token budget limits change
- New patterns of token-heavy tasks emerge
- Chunking strategies prove ineffective in practice
- User feedback indicates communication could be clearer


--- vibe-coding/SKILL.md ---
---
name: vibe-coding
description: Comprehensive guide for AI-assisted vibe coding. Use when the user wants to build applications through natural language prompts using tools like Lovable, Cursor, Replit, or Bolt. Includes best practices, pitfall awareness, tool-specific guidance, architectural decision support, and MVP scope definition with a bias toward cutting features aggressively to ship faster.
license: Complete terms in LICENSE.txt
---

# Vibe Coding

This skill provides comprehensive guidance for vibe coding—an AI-assisted development approach where developers describe functionality in natural language and let AI tools generate the code.

## What is Vibe Coding

Vibe coding is a development methodology where users describe desired features in conversational language ("vibes") and AI tools translate these into functional code. Popularized by Andrej Karpathy in early 2025, it democratizes software creation by lowering technical barriers while accelerating prototyping.

**Core principle:** Focus on intent and outcomes rather than syntax and implementation details.

**When to use vibe coding:**
- Rapid prototyping and weekend projects
- MVPs needing fast validation
- Simple web apps and tools
- Learning new technologies quickly
- Non-technical founders building initial versions

**When NOT to use vibe coding:**
- Production systems requiring robust security
- Complex distributed systems with critical reliability needs
- Financial or healthcare applications with strict compliance requirements
- Systems requiring deep performance optimization
- Projects where understanding every line of code is mandatory

## Tool Awareness and Selection

### Popular Vibe Coding Tools (2025)

**Always search for current documentation** when working with specific tools, as they update frequently. Use web search to find the latest docs and capabilities.

**Tool Categories:**

1. **Full-Stack AI Platforms** (No-code to low-code):
   - **Lovable** - Design-heavy prototypes, excellent UI/UX, limited free tier with credits
   - **Bolt.new** - Similar to Lovable, good integration ecosystem (Stripe, Supabase)
   - **Replit Agent** - Browser-based, collaborative, integrated deployment, good for beginners
   - **v0.dev** - Vercel's UI generation tool, excellent for React components

2. **AI-Enhanced IDEs** (For developers):
   - **Cursor** - AI-first VS Code fork, best for professional workflows, context-aware
   - **Windsurf** - Alternative to Cursor with autonomous agents
   - **GitHub Copilot** - Integrated into VS Code, good for existing codebases

3. **Specialized Tools**:
   - **Rosebud AI** - Creative apps and games (2D/3D/VR)
   - **Tempo Labs** - Visual React editor
   - **Void Editor** - Open source, bring-your-own-model

### Tool Selection Framework

**For complete beginners:**
Start with Lovable or Replit—lowest barrier to entry, browser-based, immediate results.

**For developers seeking speed:**
Use Cursor or Windsurf—more control, better for scaling beyond prototype.

**For design-focused apps:**
Use Lovable or v0.dev—generate polished UIs quickly.

**For learning/education:**
Use Replit—collaborative features, integrated learning resources.

**Natural progression path:** 
Replit → Lovable/Bolt → Cursor → traditional development

### Searching Tool Documentation

**Always search for current tool documentation before providing specific guidance.** Tools update frequently with new features, pricing changes, and capability improvements.

**Search patterns:**
```
"[tool name] documentation 2025"
"[tool name] getting started guide"
"[tool name] best practices"
"[tool name] API reference"
```

**Example:** Before advising on Lovable features, search: "Lovable AI documentation 2025 features pricing"

## Best Practices for Effective Vibe Coding

### 1. Start with Clear Vision and Planning

**Before any prompting:**
- Define the app's core purpose in one sentence
- Outline 3-5 key user flows
- Identify the single most important feature (your MVP's MVP)
- Create a simple design doc or mind map

**Why:** Vague starts create scattered, unmaintainable code. Planning ensures alignment and reduces rework.

**Template:**
```
Purpose: [One sentence]
Primary User: [Who]
Core Problem: [What problem does this solve]
Key User Flow: [Primary path through the app]
Success Metric: [How to know if it works]
```

### 2. Craft Specific, Iterative Prompts

**Prompt structure:**
- Start simple, add complexity gradually
- Include context (tech stack, constraints, environment)
- Explain intent before requesting code
- Ask for confirmation on approach before generation
- Request explanation of changes and edge cases

**Bad prompt:** "Build a login system"

**Good prompt:** "Create a basic login page using React and Supabase authentication. Include email validation, error handling for incorrect credentials, and a 'forgot password' link. Please confirm this approach aligns with Supabase best practices before generating code."

**Iterative pattern:**
1. Generate simple version
2. Test thoroughly
3. Prompt for specific enhancement
4. Test again
5. Repeat

**Anti-pattern:** Massive prompts requesting complete features—leads to hallucinations and brittle code.

### 3. Choose Modular, Popular Tech Stacks

**Recommended stacks:**
- **Frontend:** React, Next.js, Vue (AI knows these deeply)
- **Backend:** Node.js, Flask, FastAPI
- **Database:** Supabase, Firebase, PostgreSQL
- **Deployment:** Vercel, Netlify, Replit

**Why popular stacks?** AI training data is rich for well-documented frameworks, resulting in better code generation.

**Modularity principle:** Break code into separate files from day one. Avoid monolithic single-file applications.

**File structure example:**
```
/src
  /components
  /utils
  /api
  /styles
```

### 4. Integrate Version Control Immediately

**Day 1 actions:**
- Initialize Git repository
- Commit after each working iteration
- Use meaningful commit messages
- Push to GitHub regularly

**Why:** Tracks evolution in opaque AI-generated codebases, enables rollback when AI makes mistakes.

**Commit pattern:** After accepting AI changes that work, commit immediately before requesting next change.

**Tool integration:** Cursor and other IDEs support Git natively—use it constantly.

### 5. Test Rigorously at Every Step

**Critical principle:** Never assume AI-generated code "just works."

**Testing hierarchy:**
1. **Happy path** - Does basic functionality work?
2. **Edge cases** - Empty inputs, very long inputs, special characters
3. **Error handling** - What happens when things fail?
4. **Security** - Are inputs validated? Are secrets exposed?
5. **Performance** - Does it scale beyond demo data?

**Prompt for tests:** "Generate this function AND its unit tests with edge cases."

**Testing tools:**
- **Frontend:** Jest, React Testing Library
- **Backend:** Pytest, Mocha
- **E2E:** Playwright, Cypress

**Security checklist:**
- [ ] Input validation and sanitization
- [ ] No hardcoded secrets (use environment variables)
- [ ] Authentication properly implemented
- [ ] Authorization checks on sensitive operations
- [ ] SQL injection prevention
- [ ] XSS protection
- [ ] CSRF tokens where needed

### 6. Embrace Iteration and Human Oversight

**80/20 principle:** AI generates 80%, humans curate the critical 20%.

**Review patterns:**
- Skim all generated code initially
- Deep review for critical sections (auth, payments, data handling)
- Use AI to explain suspicious code sections
- Refactor when patterns become repetitive

**Voice tools:** Use tools like SuperWhisper for hands-free prompting during rapid iteration.

**Finish line definition:** Set clear MVP criteria before starting. Examples:
- "3 core features working end-to-end"
- "Deployable and testable by 5 users"
- "Core user flow completable without crashes"

**Avoid:** Endless tweaking without defined completion criteria.

### 7. Documentation as You Go

**Maintain a running changelog:**
Prompt AI to update a CHANGELOG.md or README.md after each major iteration.

**Essential documentation:**
- README with setup instructions
- Core architecture decisions
- Known limitations
- Environment variable requirements
- Deployment steps

**Why:** Future you (or team members) will struggle with AI-generated code without context.

## Common Pitfalls and How to Avoid Them

### Pitfall 1: Vague Prompts → "Illusion of Correctness"

**Problem:** High-level "vibes" produce code that "mostly works" but hides subtle bugs—unhandled edge cases, inefficient algorithms, security gaps.

**Example:** Asking for "user authentication" might generate code that works for valid credentials but fails to handle account lockout after multiple failures.

**Avoidance strategy:**
- Always follow generation with: "Explain your implementation and identify potential edge cases"
- Test beyond happy paths immediately
- Request explicit error handling for each feature

### Pitfall 2: Skipping Version Control and Structure

**Problem:** No Git = lost progress. Unstructured code becomes unmaintainable mid-project.

**Real-world impact:** Projects spiral into "refactor hell" where each change breaks something else.

**Avoidance strategy:**
- Initialize Git on Day 1 (non-negotiable)
- Commit after every successful iteration
- Prompt for modular file structure from the start
- Use AI to refactor early when structure gets messy

### Pitfall 3: Over-Reliance for Scalability

**Problem:** Prototypes shine, production apps falter. Performance issues, integration complexity, and API costs become prohibitive.

**Statistics:** Tools often produce "60-70% solutions" requiring manual refinement for production readiness.

**Avoidance strategy:**
- Treat vibe-coded MVPs as prototypes, not finished products
- Plan manual optimization for the last 20%
- Budget for human review and refactoring
- Monitor API/compute costs continuously
- Set realistic expectations with stakeholders

### Pitfall 4: Security Oversights

**Problem:** AI frequently generates:
- Weak input validation (40% of AI-generated queries vulnerable to SQL injection)
- Hardcoded credentials
- Outdated dependencies
- Generic error handling that exposes system info

**Real example:** User deployed Cursor-built SaaS, discovered security vulnerabilities within days, faced attack attempts.

**Avoidance strategy:**
- **Explicitly prompt for security:** "Implement this with proper input sanitization, use environment variables for secrets, and follow OWASP best practices"
- Run static security scans (OWASP ZAP, Snyk)
- Never deploy without security review
- Use security-focused prompts: "Review this code for security vulnerabilities"

### Pitfall 5: No Clear Finish Line

**Problem:** Endless tweaking without milestones leads to burnout or abandoned projects. Scope creep kills momentum.

**Avoidance strategy:**
- Define success metrics upfront (see MVP Scope Definition section)
- Use time-boxing: "Ship in 1 week with 3 core features"
- Resist the temptation to add "just one more thing"
- Deploy early, iterate based on user feedback

### Pitfall 6: Ignoring Costs and Integration

**Problem:** Token usage adds up fast. Compute charges accumulate during AI mistakes. Legacy system integration fails.

**Real example:** User watched in horror as Replit agent wiped production database, then fabricated fake data to cover mistake.

**Avoidance strategy:**
- Track token/compute usage in real-time
- Set budget alerts
- Test integrations early with small datasets
- Choose open-source stacks to avoid lock-in
- For critical operations, implement human-in-the-loop approval

### Pitfall 7: Failing to Iterate on Errors

**Problem:** Copy-pasting error messages without context creates infinite loops of AI confusion.

**Avoidance strategy:**
- Provide full stack traces with context
- Explain what you were trying to accomplish
- Describe what you expected vs. what happened
- Ask AI to debug step-by-step with explanations

**Good error prompt:** "I'm trying to connect to PostgreSQL database. Here's the full error: [error]. Here's my connection code: [code]. I expected it to connect successfully but instead got this error. Can you debug step-by-step?"

### Pitfall 8: Skill Atrophy

**Problem:** Over-reliance on AI, especially for beginners, leads to:
- Inability to debug without AI help
- Lack of understanding of underlying concepts
- Reduced problem-solving capabilities
- Dependency on tools rather than skills

**Notable:** Research shows students using AI complete more tasks with fewer syntax errors, but don't develop lasting understanding or manual proficiency.

**Avoidance strategy:**
- Use AI to complement, not replace, hands-on learning
- Regularly write code manually to maintain skills
- Deep-dive into generated code to understand how it works
- Practice debugging without AI assistance
- Learn fundamentals even while using vibe coding

### Pitfall 9: Technical Debt Accumulation

**Problem:** Rapid development through vibe coding creates:
- Inconsistent coding patterns (AI solves similar problems differently each time)
- Sparse or nonexistent documentation
- Quick-fix solutions prioritized over maintainable architecture
- Mounting cleanup work that negates initial speed gains

**Avoidance strategy:**
- Regular code reviews specifically for AI-generated code
- Establish coding standards in Cursor Rules or project docs
- Prompt AI to maintain consistent patterns: "Use the same pattern as [existing code]"
- Schedule refactoring sprints every 2-3 iterations
- Treat technical debt as a metric alongside features

## Architectural Decision Support

### Determining Intent Through Socratic Questioning

When users describe vague ideas, help crystallize intent through structured questioning:

**Discovery questions:**
1. "Who is the primary user of this application?"
2. "What is the single most important thing this app must do?"
3. "What happens if that core feature doesn't work?"
4. "How will users discover and access this app?"
5. "What does success look like in 2 weeks? 2 months?"

**Example dialogue:**
```
User: "I want to build a productivity app"
Claude: "What specific productivity problem are you solving? Time tracking, task management, focus/distraction blocking, or something else?"
User: "Task management"
Claude: "Who's the target user—individual professionals, students, or teams? This affects whether we need collaboration features."
```

### Backend vs Frontend Trade-offs

**Help users navigate architectural decisions by presenting clear trade-offs:**

#### Static Frontend (No Backend)

**Pros:**
- Simplest deployment (Netlify, Vercel, GitHub Pages)
- Zero backend costs
- Instant loading
- Easiest to vibe code

**Cons:**
- No user data persistence
- No authentication
- No server-side logic
- Limited to client-side operations

**Best for:** Portfolios, calculators, converters, simple tools, landing pages

#### Frontend + Backend-as-a-Service (BaaS)

**Options:** Supabase, Firebase, Appwrite

**Pros:**
- Authentication built-in
- Database with minimal setup
- Generous free tiers
- Still mostly vibe-codeable
- Real-time capabilities

**Cons:**
- Vendor lock-in
- Limited custom backend logic
- Costs scale with usage
- Less control over data

**Best for:** MVPs, CRUD apps, real-time apps, most startups

#### Frontend + Custom Backend

**Options:** Node.js/Express, Python/Flask, Python/FastAPI

**Pros:**
- Full control over logic
- Custom business rules
- Flexibility for complex operations
- Own your architecture

**Cons:**
- More complexity to vibe code
- Deployment more involved
- Higher maintenance burden
- Requires more technical knowledge

**Best for:** Complex business logic, unique requirements, scalability needs

#### Decision Framework

**Present users with this ladder:**

```
Complexity ↑

[Static HTML/JS] → Can you do everything client-side?
                    ↓ No, need data persistence
[Frontend + Supabase] → Need custom logic or existing backend?
                        ↓ Yes
[Frontend + Custom Backend] → Complex distributed requirements?
                              ↓ Yes
[Microservices/Advanced] → (Beyond typical vibe coding scope)
```

**Guiding principle:** Start at the bottom of the ladder. Only climb when absolutely necessary.

### Technology Selection Discussion

**When users ask "Should I use X or Y?":**

1. **Clarify the actual requirement:** What are you trying to accomplish?
2. **Present the simplest option first:** Bias toward fewer moving parts
3. **Explain trade-offs concretely:** Not abstract principles, but real implications
4. **Recommend based on context:** Their skill level, project timeline, budget
5. **Search current best practices:** Tools and recommendations change rapidly

**Example:**
```
User: "Should I use REST or GraphQL?"
Claude: "For your MVP with 3-4 simple data types and straightforward CRUD operations, REST is simpler and faster to vibe code. GraphQL's flexibility shines with complex, interconnected data and multiple client types—but adds setup complexity you don't need yet. Start with REST; you can always migrate later if needed. Should I search for current best practices for REST APIs in [your stack]?"
```

## MVP Scope Definition (Paul Graham Style)

### Philosophy: Cut Ruthlessly

**Core principle:** The best MVPs are embarrassingly simple. Ship the absolute minimum that demonstrates core value.

**Paul Graham's wisdom:**
- "The most common mistake startups make is to solve problems no one has"
- Build what a small number of people want a large amount
- Do things that don't scale initially

### The Cutting Framework

**When defining MVP scope, be aggressive about cutting features. Use this hierarchy:**

#### Tier 1: Essential (The Core)
**Must have for the app to exist at all**

Questions to identify Tier 1:
- "If this feature doesn't work, can the user accomplish the main goal?"
- "Would the app make sense without this?"
- "Is this the reason the user would come to the app?"

**Example (Todo App):**
- Add task
- Mark task complete
- View task list

**Ruthless cut candidates:**
- Categories/tags (add later)
- Due dates (add later)
- Priority levels (add later)
- Attachments (add later)
- Sharing (add later)

#### Tier 2: Valuable (The Enhancement)
**Significantly improves UX but not required for core function**

**Add these ONLY after Tier 1 is working and tested.**

**Example (Todo App):**
- Edit task
- Delete task
- Filter by status

#### Tier 3: Nice-to-Have (The Polish)
**Makes app more pleasant but not essential**

**Add these after real users request them.**

**Example (Todo App):**
- Drag-to-reorder
- Keyboard shortcuts
- Dark mode
- Animations

#### Tier 4: Future (The Distraction)
**Interesting but not needed now**

**Explicitly put these on a "not now" list.**

**Example (Todo App):**
- Mobile app
- Team collaboration
- Calendar integration
- Analytics dashboard
- AI task suggestions

### The Negotiation Process

**When users want to include many features, use this pattern:**

1. **List everything:** Let them brainstorm freely
2. **Categorize:** Sort into tiers above
3. **Challenge Tier 2+:** For each non-Tier-1 item, ask: "What if we shipped without this first?"
4. **Test the core hypothesis:** "If we just had [Tier 1 features], could someone get value? Could we learn if this idea works?"
5. **Offer the timeline trade-off:** "With just Tier 1, you could ship in 3 days and get feedback. With Tier 1-3, it might take 3 weeks, and you still don't know if people want it."

**Reframe as experiments:** "Let's ship the simplest version that lets us test if people want this at all. If they do, we'll know what to add next because they'll tell us."

### MVP Scope Template

Provide users with this template to define scope:

```markdown
# MVP Scope Definition

## Core Hypothesis
[What are we trying to validate? One sentence.]

## Target User
[Who specifically will use this? The narrower, the better.]

## Tier 1: Must Have (Ship without = pointless)
- [ ] Feature 1
- [ ] Feature 2
- [ ] Feature 3

## Tier 2: Valuable (Add after Tier 1 works)
- [ ] Feature 4
- [ ] Feature 5

## Tier 3+: Nice-to-Have (Explicitly NOT building now)
- Feature 6
- Feature 7
- Feature 8

## Success Metric
[How will we know if this works? Be specific.]

## Timeline Commitment
[Ship Tier 1 by: DATE]
```

### Red Flags in Scope Discussion

**Warn users when you see:**

1. **"And also..."** - Each "and also" doubles complexity
2. **"It needs to..."** - Who says it needs to? Users, or assumptions?
3. **"Professional/polished/perfect"** - These words delay shipping
4. **"Everyone will want..."** - No, focus on someone specifically
5. **"Just one more thing..."** - The path to scope creep

**Responses:**
- "Let's put that in Tier 2 and ship without it first"
- "What if we tested if anyone wants Tier 1 before building that?"
- "That sounds valuable, but can we validate the core idea first?"

### Examples of Ruthless Cutting

**Bad MVP (Todo App):**
- Categories, tags, priorities
- Team sharing and permissions  
- Calendar integration
- Email reminders
- Mobile app
- Analytics
- AI suggestions
- Custom themes
- Export/import
- Recurring tasks

**Estimated time:** 6-8 weeks

**Good MVP (Todo App):**
- Add task (text input)
- Mark complete (checkbox)
- View list (simple list)

**Estimated time:** 1-2 days

**Result:** Ship faster, learn sooner, iterate based on real feedback.

### The "Food Truck" Principle

**Analogy to share with users:**

"Think of your MVP like a food truck, not a fancy restaurant. The food truck serves one amazing dish from a simple setup. If people love it, you can add more dishes, a bigger truck, or even a restaurant. But you validate the core idea first with minimal investment. Your MVP should be that food truck—one core feature done well enough to test if people want it."

## Workflow Integration

### Session Start Checklist

When beginning a vibe coding session:

1. **Clarify the goal:** What are we building today?
2. **Define the scope:** Use MVP framework if needed
3. **Choose tool:** Which platform is best for this?
4. **Search tool docs:** Get current capabilities/limitations
5. **Set the finish line:** What does "done" look like for today?

### Mid-Session Practices

- Commit working changes before requesting new features
- Test each change before moving to next
- Document decisions as you go
- Watch for costs/token usage
- Take breaks to review code quality

### Session End Checklist

1. **Final commit:** Ensure all working code is saved
2. **Document:** Update README with what works and what doesn't
3. **Note next steps:** What's needed in the next session?
4. **Backup:** Push to remote repository
5. **Reflect:** What went well? What struggled?

## Resources and Further Reading

### Reference Files in This Skill

See the `references/` directory for additional detailed documentation:
- `references/tool-comparison.md` - Deep dive into tool selection
- `references/security-checklist.md` - Comprehensive security guidelines
- `references/prompt-patterns.md` - Effective prompting templates

### Web Resources

**Always search for current information** as the vibe coding landscape evolves rapidly.

**Search suggestions:**
- "vibe coding best practices 2025"
- "[tool name] latest features documentation"
- "AI coding security vulnerabilities"
- "MVP development frameworks"

## Core Principles Summary

1. **Start simple:** Favor minimal scope and clear vision
2. **Test constantly:** Never trust AI-generated code without verification  
3. **Cut ruthlessly:** Ship the smallest thing that demonstrates value
4. **Search tool docs:** Get current information before advising
5. **Security first:** Explicitly prompt for secure implementations
6. **Version control:** Commit working changes immediately
7. **Document decisions:** Future you will thank present you
8. **Iterate based on feedback:** Real user input beats assumptions
9. **Balance speed with quality:** Use AI for 80%, human review for 20%
10. **Keep learning:** Don't let vibe coding replace fundamental skills


--- xlsx-editor/SKILL.md ---
---
name: xlsx-editor
description: Specialized guidance for editing EXISTING Excel files with emphasis on preserving formulas, formatting, and structure across multiple tabs. Use this skill when modifying, updating, or adding data to existing .xlsx files where maintaining integrity is critical—particularly for multi-tab workbooks, complex formulas, formatted tables, and data that must maintain sort order and relationships across sheets.
---

# Excel Editor - Editing Existing Files

## Overview

This skill provides specialized workflows for editing existing Excel files while preserving their integrity. It addresses the unique challenges of modifying live workbooks: maintaining formula relationships across tabs, preserving formatting and structure, ensuring data insertion maintains sort order, and guaranteeing completeness of multi-tab updates.

## When to Use This Skill

Use this skill when:
- Modifying existing Excel files (not creating new ones)
- Working with multi-tab workbooks where changes must cascade correctly
- Adding data that needs to integrate with existing tables while maintaining sort order
- Updating formulas that reference cells across multiple sheets
- Preserving complex formatting, merged cells, or conditional formatting
- Ensuring all related tables across tabs are updated completely

## Critical Principles for Editing Existing Files

### 1. Always Analyze Before Editing
Never dive directly into making changes. First understand the file's structure, dependencies, and relationships.

### 2. Preserve, Don't Replace
The existing file has established patterns. Maintain its structure, formulas, and formatting conventions rather than imposing new ones.

### 3. Multi-Tab Completeness is Mandatory
If data appears in multiple tabs, ALL instances must be updated. Missing one tab is a critical failure.

### 4. Validate Exhaustively
After edits, verify formulas, check for blank rows, confirm sort order, and ensure multi-tab consistency.

## Pre-Edit Analysis Workflow

**MANDATORY: Always perform this analysis before making any edits.**

```python
from openpyxl import load_workbook
import pandas as pd

# Load file without modifying it
wb = load_workbook('existing.xlsx', data_only=False)

# 1. INVENTORY ALL SHEETS
print("Sheets in workbook:", wb.sheetnames)
for sheet_name in wb.sheetnames:
    sheet = wb[sheet_name]
    print(f"\n{sheet_name}:")
    print(f"  Dimensions: {sheet.dimensions}")
    print(f"  Max row: {sheet.max_row}, Max col: {sheet.max_column}")

# 2. IDENTIFY DATA STRUCTURES
# For each sheet, identify:
# - Table boundaries (first/last row and column)
# - Header rows
# - Sort keys (which columns determine order)
# - Merged cells
for sheet_name in wb.sheetnames:
    sheet = wb[sheet_name]
    print(f"\nMerged cells in {sheet_name}:", list(sheet.merged_cells))

# 3. MAP FORMULAS AND DEPENDENCIES
# Find all formulas and note cross-sheet references
for sheet_name in wb.sheetnames:
    sheet = wb[sheet_name]
    formulas = []
    for row in sheet.iter_rows():
        for cell in row:
            if cell.value and isinstance(cell.value, str) and cell.value.startswith('='):
                formulas.append({
                    'cell': cell.coordinate,
                    'formula': cell.value,
                    'cross_sheet': '!' in cell.value
                })
    if formulas:
        print(f"\n{sheet_name} has {len(formulas)} formulas")
        cross_sheet = [f for f in formulas if f['cross_sheet']]
        if cross_sheet:
            print(f"  {len(cross_sheet)} have cross-sheet references")
            for f in cross_sheet[:3]:  # Show first 3 examples
                print(f"    {f['cell']}: {f['formula']}")

# 4. UNDERSTAND SORT ORDER
# Read data to understand current ordering
for sheet_name in wb.sheetnames:
    df = pd.read_excel('existing.xlsx', sheet_name=sheet_name)
    if not df.empty:
        print(f"\n{sheet_name} current sort:")
        print(f"  First 5 rows of first column: {df.iloc[:5, 0].tolist()}")
        print(f"  Last 5 rows of first column: {df.iloc[-5:, 0].tolist()}")
```

**Document findings before proceeding:**
- Which sheets contain related data?
- What formulas reference other sheets?
- What determines sort order?
- Where are merged cells or complex formatting?
- What is the established naming/structure pattern?

## Core Editing Workflows

### Workflow 1: Updating Cell Values While Preserving Formulas

**Use case:** Changing data values without breaking formula dependencies.

```python
from openpyxl import load_workbook

# Load with formulas intact
wb = load_workbook('existing.xlsx', data_only=False)
sheet = wb['SheetName']

# CORRECT: Update value, preserve formula context
cell = sheet['A5']
if cell.value and isinstance(cell.value, str) and cell.value.startswith('='):
    print(f"WARNING: A5 contains formula: {cell.value}")
    print("Updating this will DELETE the formula. Proceed? (update dependent formulas instead)")
else:
    cell.value = new_value

# For cells with formulas, update the formula itself or its dependencies
sheet['B10'] = '=SUM(A1:A9)'  # Updates formula, preserving calculation logic

wb.save('existing.xlsx')
```

**Critical checks:**
- Verify you're not overwriting formula cells with values
- Check if other cells reference the cell you're updating
- Test a few formulas to ensure they still work after the change

### Workflow 2: Adding Rows While Maintaining Structure

**Use case:** Inserting new data into existing tables.

```python
from openpyxl import load_workbook
import pandas as pd

# 1. UNDERSTAND CURRENT STRUCTURE
df = pd.read_excel('existing.xlsx', sheet_name='Data')
# Identify sort key(s)
# Example: sorted by date (column A), then by category (column B)

# 2. DETERMINE INSERTION POINT
# Find where new row belongs in sort order
new_data = {'Date': '2024-06-15', 'Category': 'Sales', 'Amount': 1500}

insertion_row = None
for idx, row in df.iterrows():
    if row['Date'] > new_data['Date']:
        insertion_row = idx + 2  # +2 because: +1 for header, +1 for 1-based indexing
        break
if insertion_row is None:
    insertion_row = len(df) + 2  # Append at end

# 3. INSERT ROW
wb = load_workbook('existing.xlsx')
sheet = wb['Data']
sheet.insert_rows(insertion_row)

# 4. POPULATE WITH CORRECT STRUCTURE
# CRITICAL: Copy formatting from adjacent row
source_row = insertion_row - 1 if insertion_row > 2 else insertion_row + 1
for col in range(1, sheet.max_column + 1):
    # Copy formatting
    source_cell = sheet.cell(source_row, col)
    target_cell = sheet.cell(insertion_row, col)
    if source_cell.has_style:
        target_cell.font = source_cell.font.copy()
        target_cell.border = source_cell.border.copy()
        target_cell.fill = source_cell.fill.copy()
        target_cell.number_format = source_cell.number_format
        target_cell.alignment = source_cell.alignment.copy()

# 5. SET VALUES
sheet.cell(insertion_row, 1, new_data['Date'])
sheet.cell(insertion_row, 2, new_data['Category'])
sheet.cell(insertion_row, 3, new_data['Amount'])

# 6. UPDATE FORMULAS THAT REFERENCE THIS RANGE
# If there are SUM formulas like =SUM(A2:A100), they may need adjustment
# Check all sheets for formulas referencing this sheet
for ws_name in wb.sheetnames:
    ws = wb[ws_name]
    for row in ws.iter_rows():
        for cell in row:
            if cell.value and isinstance(cell.value, str) and cell.value.startswith('='):
                if 'Data!' in cell.value:  # References the sheet we modified
                    print(f"REVIEW: {ws_name}!{cell.coordinate} formula may need adjustment: {cell.value}")

wb.save('existing.xlsx')
```

**Critical checks:**
- [ ] New row is in correct sort position
- [ ] Formatting matches surrounding rows
- [ ] No blank rows introduced
- [ ] Formulas referencing the range are still valid

### Workflow 3: Adding Columns While Preserving Formulas

**Use case:** Adding new data columns to existing tables.

```python
from openpyxl import load_workbook

wb = load_workbook('existing.xlsx')
sheet = wb['Data']

# 1. IDENTIFY INSERTION POINT
# Example: Insert after column C (before column D)
insert_col = 4  # Column D

# 2. INSERT COLUMN
sheet.insert_cols(insert_col)

# 3. COPY FORMATTING FROM ADJACENT COLUMN
source_col = insert_col - 1
for row_idx in range(1, sheet.max_row + 1):
    source_cell = sheet.cell(row_idx, source_col)
    target_cell = sheet.cell(row_idx, insert_col)
    if source_cell.has_style:
        target_cell.font = source_cell.font.copy()
        target_cell.border = source_cell.border.copy()
        target_cell.fill = source_cell.fill.copy()
        target_cell.number_format = source_cell.number_format
        target_cell.alignment = source_cell.alignment.copy()

# 4. SET HEADER
sheet.cell(1, insert_col, 'New Column Header')

# 5. ADD FORMULAS OR DATA
for row_idx in range(2, sheet.max_row + 1):
    # Example: Formula referencing other columns in same row
    sheet.cell(row_idx, insert_col, f'=B{row_idx}*C{row_idx}')

wb.save('existing.xlsx')
```

**Critical checks:**
- [ ] Formulas in other columns still reference correct cells
- [ ] Cross-sheet references are not broken
- [ ] Column widths are appropriate

### Workflow 4: Multi-Tab Data Updates (CRITICAL WORKFLOW)

**Use case:** Updating data that appears in multiple sheets (THE most common source of errors).

**THE PROBLEM:** When the same entity (customer, product, date, etc.) appears across multiple tabs, ALL instances must be updated. Missing even one tab creates data inconsistency.

**THE SOLUTION:** Systematic multi-tab update workflow.

```python
from openpyxl import load_workbook
import pandas as pd

# STEP 1: IDENTIFY ALL OCCURRENCES
# Find which sheets contain the data to be updated
target_value = "Customer X"  # Example: updating customer name
occurrences = {}

wb = load_workbook('existing.xlsx', data_only=True)  # Read values
for sheet_name in wb.sheetnames:
    df = pd.read_excel('existing.xlsx', sheet_name=sheet_name)
    
    # Search for target value in all cells
    found_locations = []
    for col in df.columns:
        mask = df[col].astype(str).str.contains(target_value, case=False, na=False)
        if mask.any():
            locations = df[mask].index.tolist()
            found_locations.extend([(col, idx+2) for idx in locations])  # +2 for header and 1-based
    
    if found_locations:
        occurrences[sheet_name] = found_locations
        print(f"Found '{target_value}' in {sheet_name}: {len(found_locations)} locations")

if not occurrences:
    print(f"ERROR: '{target_value}' not found in any sheet!")
else:
    print(f"\nTotal sheets containing '{target_value}': {len(occurrences)}")

# STEP 2: UPDATE ALL OCCURRENCES
wb = load_workbook('existing.xlsx', data_only=False)  # Now load with formulas

new_value = "Customer X - Updated"
update_count = 0

for sheet_name, locations in occurrences.items():
    sheet = wb[sheet_name]
    for col_name, row_idx in locations:
        # Convert column name to index
        df = pd.read_excel('existing.xlsx', sheet_name=sheet_name, nrows=0)
        col_idx = list(df.columns).index(col_name) + 1
        
        cell = sheet.cell(row_idx, col_idx)
        
        # Check if it's a formula
        if cell.value and isinstance(cell.value, str) and cell.value.startswith('='):
            print(f"WARNING: {sheet_name}!{cell.coordinate} is a formula. Skipping.")
        else:
            cell.value = new_value
            update_count += 1
            print(f"Updated {sheet_name}!{cell.coordinate}")

print(f"\nTotal cells updated: {update_count}")

# STEP 3: VERIFY ALL SHEETS WERE UPDATED
print("\nVerifying updates...")
wb.save('existing_updated.xlsx')
wb_verify = load_workbook('existing_updated.xlsx', data_only=True)

for sheet_name in wb_verify.sheetnames:
    df = pd.read_excel('existing_updated.xlsx', sheet_name=sheet_name)
    old_count = df.astype(str).apply(lambda x: x.str.contains(target_value, case=False, na=False)).sum().sum()
    new_count = df.astype(str).apply(lambda x: x.str.contains(new_value, case=False, na=False)).sum().sum()
    
    if old_count > 0:
        print(f"❌ {sheet_name}: Still has {old_count} instances of old value!")
    else:
        print(f"✅ {sheet_name}: All instances updated (found {new_count} new values)")
```

**Critical checks:**
- [ ] ALL sheets searched, not just obvious ones
- [ ] Update count matches expected number of occurrences
- [ ] Verification confirms zero instances of old value remain
- [ ] No formulas were accidentally overwritten

### Workflow 5: Cascading Formula Updates Across Tabs

**Use case:** Changing a formula that must be propagated to other sheets.

```python
from openpyxl import load_workbook

wb = load_workbook('existing.xlsx')

# EXAMPLE: Update formula pattern across all sheets
# Old formula: =SUM(B2:B10)
# New formula: =SUMIF(B2:B10,">0")  # Only sum positive values

for sheet_name in wb.sheetnames:
    sheet = wb[sheet_name]
    
    changes_in_sheet = 0
    for row in sheet.iter_rows():
        for cell in row:
            if cell.value and isinstance(cell.value, str):
                if cell.value.startswith('=SUM(') and not 'SUMIF' in cell.value:
                    # Extract range from SUM formula
                    old_formula = cell.value
                    # Example: =SUM(B2:B10) -> =SUMIF(B2:B10,">0")
                    range_part = old_formula[5:-1]  # Extract "B2:B10"
                    new_formula = f'=SUMIF({range_part},">0")'
                    
                    cell.value = new_formula
                    changes_in_sheet += 1
                    print(f"{sheet_name}!{cell.coordinate}: {old_formula} -> {new_formula}")
    
    if changes_in_sheet > 0:
        print(f"  {sheet_name}: {changes_in_sheet} formulas updated")

wb.save('existing.xlsx')
```

**Critical checks:**
- [ ] Formula pattern correctly identified across all sheets
- [ ] New formula syntax is valid
- [ ] All sheets were checked (not just the first few)
- [ ] Edge cases handled (formulas with different ranges, nested functions)

### Workflow 6: Handling Merged Cells

**Use case:** Working with tables that have merged cells (common in headers/formatting).

```python
from openpyxl import load_workbook

wb = load_workbook('existing.xlsx')
sheet = wb['Data']

# 1. IDENTIFY MERGED CELLS
merged_ranges = list(sheet.merged_cells.ranges)
print(f"Found {len(merged_ranges)} merged cell ranges:")
for merged_range in merged_ranges:
    print(f"  {merged_range}")

# 2. WHEN INSERTING ROWS/COLUMNS NEAR MERGED CELLS
# Merged cells will NOT expand automatically. Must handle manually.

# Example: Inserting row in middle of merged region
insert_row = 5

# Check if insertion point is in a merged range
for merged_range in merged_ranges:
    if merged_range.min_row <= insert_row <= merged_range.max_row:
        print(f"WARNING: Row {insert_row} is within merged range {merged_range}")
        print("Unmerging, inserting, then re-merging with expanded range")
        
        # Unmerge
        sheet.unmerge_cells(str(merged_range))
        
        # Insert row
        sheet.insert_rows(insert_row)
        
        # Re-merge with expanded range
        new_range = f"{merged_range.min_col}{merged_range.min_row}:{merged_range.max_col}{merged_range.max_row+1}"
        sheet.merge_cells(new_range)

wb.save('existing.xlsx')
```

**Critical checks:**
- [ ] Merged cells are preserved or appropriately expanded
- [ ] Values in merged cells are maintained
- [ ] Formatting of merged cells is preserved

## Post-Edit Validation Checklist

**MANDATORY: Complete this checklist after ANY edits.**

### 1. Formula Integrity Check

```python
# Run recalc.py to identify formula errors
import subprocess
import json

result = subprocess.run(['python', 'recalc.py', 'existing.xlsx'], 
                       capture_output=True, text=True)
recalc_result = json.loads(result.stdout)

if recalc_result['status'] == 'errors_found':
    print(f"❌ ERRORS FOUND: {recalc_result['total_errors']} formula errors")
    for error_type, details in recalc_result['error_summary'].items():
        print(f"\n{error_type}: {details['count']} occurrences")
        for location in details['locations'][:5]:  # Show first 5
            print(f"  - {location}")
else:
    print("✅ All formulas calculated successfully (zero errors)")
```

- [ ] Zero formula errors (#REF!, #DIV/0!, #VALUE!, #N/A, #NAME?)
- [ ] All formulas return expected values
- [ ] Cross-sheet references still work

### 2. Structural Integrity Check

```python
from openpyxl import load_workbook
import pandas as pd

wb = load_workbook('existing.xlsx')

# Check for blank rows in data tables
for sheet_name in wb.sheetnames:
    df = pd.read_excel('existing.xlsx', sheet_name=sheet_name)
    
    # Identify blank rows (all NaN)
    blank_rows = df[df.isnull().all(axis=1)]
    if not blank_rows.empty and len(blank_rows) > 0:
        print(f"⚠️  {sheet_name}: Found {len(blank_rows)} blank rows at indices: {blank_rows.index.tolist()}")
    
    # Check for unexpected blank cells in key columns (first column)
    if not df.empty and len(df.columns) > 0:
        blank_cells = df[df.iloc[:, 0].isnull()]
        if not blank_cells.empty and len(blank_cells) > 0:
            print(f"⚠️  {sheet_name}: First column has {len(blank_cells)} blank cells")
```

- [ ] No unexpected blank rows in data tables
- [ ] No blank cells in key identifier columns
- [ ] Table boundaries are clean (no trailing empty rows/columns)

### 3. Sort Order Verification

```python
# Verify data maintains expected sort order
df = pd.read_excel('existing.xlsx', sheet_name='Data')

# Example: Should be sorted by Date ascending, then Category ascending
expected_sort_cols = ['Date', 'Category']

# Check if sorted
is_sorted = df[expected_sort_cols].equals(df[expected_sort_cols].sort_values(expected_sort_cols))

if is_sorted:
    print("✅ Data is in correct sort order")
else:
    print("❌ Data is NOT sorted correctly")
    print("First 10 rows of sort columns:")
    print(df[expected_sort_cols].head(10))
```

- [ ] Data maintains expected sort order
- [ ] New rows inserted at correct position
- [ ] Sort keys (if any) are consistent

### 4. Multi-Tab Consistency Check

```python
# Verify related data is consistent across tabs

# Example: Check that customer names are consistent across Summary and Detail sheets
df_summary = pd.read_excel('existing.xlsx', sheet_name='Summary')
df_detail = pd.read_excel('existing.xlsx', sheet_name='Detail')

customers_summary = set(df_summary['Customer'].dropna().unique())
customers_detail = set(df_detail['Customer'].dropna().unique())

in_summary_not_detail = customers_summary - customers_detail
in_detail_not_summary = customers_detail - customers_summary

if in_summary_not_detail:
    print(f"⚠️  Customers in Summary but not Detail: {in_summary_not_detail}")
if in_detail_not_summary:
    print(f"⚠️  Customers in Detail but not Summary: {in_detail_not_summary}")

if not in_summary_not_detail and not in_detail_not_summary:
    print("✅ Customer lists are consistent across tabs")
```

- [ ] Related data exists in ALL expected sheets
- [ ] Counts/totals are consistent across sheets
- [ ] No orphaned data (exists in one sheet but not related sheets)

### 5. Formatting Preservation Check

```python
# Visual inspection is often needed for formatting
# But can programmatically check some basics

from openpyxl import load_workbook

wb = load_workbook('existing.xlsx')
sheet = wb['Data']

# Check that new rows have formatting
for row_idx in [5, 10, 15]:  # Example rows you inserted
    cell = sheet.cell(row_idx, 1)
    has_font = cell.font is not None and cell.font != sheet.cell(1, 1).font
    has_fill = cell.fill is not None and cell.fill.start_color.index != '00000000'
    
    print(f"Row {row_idx}: Font={has_font}, Fill={has_fill}")
```

- [ ] New rows/columns match formatting of surrounding cells
- [ ] Borders are continuous (no breaks in table borders)
- [ ] Merged cells are intact
- [ ] Conditional formatting still applies correctly

## Common Pitfalls and How to Avoid Them

### Pitfall 1: Incomplete Multi-Tab Updates

**Problem:** Updating data in Sheet1 but forgetting it also appears in Sheet2 and Sheet3.

**Solution:** Always search ALL sheets before updating. Use the Multi-Tab Data Updates workflow.

**Prevention:**
```python
# Before ANY update, run this search
search_term = "value_to_update"
for sheet_name in wb.sheetnames:
    df = pd.read_excel('file.xlsx', sheet_name=sheet_name)
    found = df.astype(str).apply(lambda x: x.str.contains(search_term, case=False, na=False)).any().any()
    if found:
        print(f"'{search_term}' found in {sheet_name}")
```

### Pitfall 2: Breaking Formulas When Inserting Rows

**Problem:** Inserting a row causes formulas like =SUM(A2:A10) to break or not include the new row.

**Solution:** After inserting rows, check if range-based formulas need adjustment.

**Prevention:**
- Insert rows WITHIN existing ranges (not before or after)
- Use entire column references when possible: =SUM(A:A) instead of =SUM(A2:A10)
- After insertion, verify formula ranges programmatically

### Pitfall 3: Inserting Data Out of Sort Order

**Problem:** New row appears at the end instead of in alphabetical/chronological order.

**Solution:** Always determine the correct insertion point based on sort keys BEFORE inserting.

**Prevention:**
```python
# Determine insertion point based on sort logic
insertion_row = None
for idx, row in df.iterrows():
    if should_come_before(new_data, row):  # Define your comparison logic
        insertion_row = idx + 2
        break
```

### Pitfall 4: Introducing Blank Rows

**Problem:** Editing operations create unexpected blank rows in tables.

**Solution:** Never use append() for mid-table insertions. Always use insert_rows() with calculated position.

**Prevention:** Always run blank row check in post-edit validation.

### Pitfall 5: Overwriting Formulas with Values

**Problem:** Updating a cell that contains a formula, replacing it with a hardcoded value.

**Solution:** Always check if cell contains a formula before updating.

**Prevention:**
```python
if cell.value and isinstance(cell.value, str) and cell.value.startswith('='):
    raise ValueError(f"{cell.coordinate} contains formula. Cannot overwrite with value.")
```

### Pitfall 6: Not Recalculating After Formula Changes

**Problem:** Formulas are updated but values shown are stale (from before the edit).

**Solution:** ALWAYS run recalc.py after any formula modifications.

**Prevention:** Make recalc.py part of your standard workflow. Never skip it.

### Pitfall 7: Losing Formatting on New Rows/Columns

**Problem:** New rows have default formatting instead of matching the table style.

**Solution:** Always copy formatting from adjacent rows/columns.

**Prevention:** Use the formatting copy code from Workflow 2 and 3 above.

## Summary: Editing Workflow Template

For any editing task, follow this sequence:

1. **Analyze** (Pre-Edit Analysis Workflow)
   - Map all sheets
   - Identify dependencies
   - Document structure

2. **Plan**
   - Determine which sheets need changes
   - Identify formula impacts
   - Calculate insertion points

3. **Execute**
   - Use appropriate workflow
   - Update ALL relevant sheets
   - Preserve formatting

4. **Validate** (Post-Edit Validation Checklist)
   - Check formulas (zero errors)
   - Verify structure (no blank rows)
   - Confirm sort order
   - Ensure multi-tab consistency
   - Validate formatting

5. **Recalculate**
   - Run recalc.py
   - Fix any errors found
   - Run recalc.py again until zero errors

**Never skip steps.** Each step catches different classes of errors.


--- prompting-pattern-library/README.md ---
# Prompting Pattern Library

A comprehensive, field-tested library of prompting patterns, failure modes, and model-specific guidance for effective LLM interactions.

**Version**: 1.0 (October 2025)  
**Tested with**: Claude 3.5/4, GPT-4/4o (October 2024), Gemini 1.5 Pro

---

## Quick Start

**New to prompting?** Start with [SKILL.md](SKILL.md) for an overview and quick reference.

**Debugging a prompt?** Jump to [Failure Modes](references/failure-modes.md) for diagnosis and fixes.

**Building cross-model systems?** Check [Model Quirks](references/model-quirks.md) for compatibility guidance.

**Need advanced patterns?** See [Orchestration Patterns](references/orchestration-patterns.md) for agentic workflows.

---

## Contents

### Core Documents

#### [SKILL.md](SKILL.md)
**Purpose**: Quick reference and navigation hub  
**Use when**: Starting a new prompting task, teaching prompting basics, or getting oriented  
**Key sections**:
- Common prompting patterns overview
- Core principles for effective prompting
- Model-specific considerations
- Quick reference to all bundled references

#### [references/prompt-patterns.md](references/prompt-patterns.md)
**Purpose**: Comprehensive catalog of 25+ proven prompting patterns  
**Use when**: Implementing specific prompting techniques, teaching prompting, or troubleshooting approach  
**Key sections**:
- Foundational patterns (zero-shot, few-shot, chain-of-thought)
- Structured output patterns (JSON, tables, delimiters)
- Role and persona patterns
- Advanced reasoning patterns (tree-of-thoughts, self-consistency)
- Quality control patterns (reflection, citation)
- Domain-specific patterns (code review, data analysis)
- Pattern combination strategies

**Cross-references**:
- For failures with these patterns → [Failure Modes](references/failure-modes.md)
- For model compatibility → [Model Quirks](references/model-quirks.md)
- For multi-step orchestration → [Orchestration Patterns](references/orchestration-patterns.md)

#### [references/failure-modes.md](references/failure-modes.md)
**Purpose**: Common prompting failures with diagnosis and fixes  
**Use when**: Debugging problematic prompts, preventing common mistakes, or understanding why a prompt failed  
**Key sections**:
- Ambiguity failures (unclear format, vague criteria, scope issues)
- Assumption failures (implicit context, missing constraints)
- Instruction conflict failures (contradictory requirements)
- Reasoning failures (logic errors, insufficient examples)
- Output quality failures (generic responses, inconsistency)
- Context window failures
- Edge case failures
- Model-specific failures

**Cross-references**:
- To fix with specific patterns → [Prompt Patterns](references/prompt-patterns.md)
- For model-specific workarounds → [Model Quirks](references/model-quirks.md)

#### [references/model-quirks.md](references/model-quirks.md)
**Purpose**: Model-specific guidance for Claude, GPT-4, GPT-4o, and Gemini  
**Use when**: Implementing cross-model systems, optimizing for specific models, or understanding model differences  
**Key sections**:
- Claude strengths and optimal patterns
- GPT-4 strengths and optimal patterns
- GPT-4o multimodal guidance
- Gemini strengths and optimal patterns
- Cross-model considerations
- Model selection decision tree
- Testing across models

**Cross-references**:
- For patterns mentioned → [Prompt Patterns](references/prompt-patterns.md)
- For model-specific failures → [Failure Modes](references/failure-modes.md)

#### [references/orchestration-patterns.md](references/orchestration-patterns.md)
**Purpose**: Advanced patterns for multi-step workflows, agentic systems, and prompt chains  
**Use when**: Building agent systems, orchestrating multi-step tasks, or implementing evaluation loops  
**Key sections**:
- Meta-prompting patterns
- Multi-agent orchestration
- Planner-executor patterns
- Evaluation and refinement loops
- Tool integration patterns
- Memory and state management

**Cross-references**:
- Uses foundational patterns from → [Prompt Patterns](references/prompt-patterns.md)
- For debugging orchestration → [Failure Modes](references/failure-modes.md)
- For model compatibility → [Model Quirks](references/model-quirks.md)

---

## Navigation by Use Case

### I want to...

**Learn prompting from scratch**
1. Read [SKILL.md](SKILL.md) overview
2. Study [Prompt Patterns](references/prompt-patterns.md) - start with Foundational Patterns
3. Review [Failure Modes](references/failure-modes.md) to avoid common mistakes

**Improve an existing prompt**
1. Identify issues in [Failure Modes](references/failure-modes.md)
2. Apply fixes from [Prompt Patterns](references/prompt-patterns.md)
3. Check [Model Quirks](references/model-quirks.md) if using specific model

**Build a multi-step AI workflow**
1. Review [Orchestration Patterns](references/orchestration-patterns.md)
2. Select appropriate patterns from [Prompt Patterns](references/prompt-patterns.md)
3. Test across models using [Model Quirks](references/model-quirks.md) guidance

**Teach prompting to others**
1. Use [SKILL.md](SKILL.md) as curriculum outline
2. Draw examples from [Prompt Patterns](references/prompt-patterns.md)
3. Use [Failure Modes](references/failure-modes.md) for "what not to do" lessons

**Debug a failing prompt**
1. Diagnose issue in [Failure Modes](references/failure-modes.md)
2. Apply recommended pattern from [Prompt Patterns](references/prompt-patterns.md)
3. Check model-specific considerations in [Model Quirks](references/model-quirks.md)

**Optimize for a specific model**
1. Read model section in [Model Quirks](references/model-quirks.md)
2. Adapt patterns from [Prompt Patterns](references/prompt-patterns.md)
3. Avoid model-specific anti-patterns from [Failure Modes](references/failure-modes.md)

---

## Pattern Complexity Levels

### Level 1: Foundational (Start Here)
- Zero-shot prompting
- Few-shot prompting
- Basic chain-of-thought
- Simple role assignment
- Format specification

**Documents**: [SKILL.md](SKILL.md), [Prompt Patterns](references/prompt-patterns.md) sections 1-2

### Level 2: Intermediate
- Structured output patterns
- Multi-step reasoning
- Self-consistency
- Reflection patterns
- Context management

**Documents**: [Prompt Patterns](references/prompt-patterns.md) sections 3-5, [Failure Modes](references/failure-modes.md)

### Level 3: Advanced
- Tree-of-thoughts
- Meta-prompting
- Multi-agent orchestration
- Evaluation loops
- Cross-model optimization

**Documents**: [Prompt Patterns](references/prompt-patterns.md) section 6-7, [Orchestration Patterns](references/orchestration-patterns.md), [Model Quirks](references/model-quirks.md)

---

## Model Version Notes

This library is tested with:
- **Claude 3.5 Sonnet** (June 2024) and **Claude 4 Sonnet** (October 2025)
- **GPT-4** and **GPT-4o** (October 2024 versions)
- **Gemini 1.5 Pro** (September 2024)

Pattern effectiveness may vary with newer model versions. When testing with updated models:
1. Start with foundational patterns (most stable across versions)
2. Test model-specific patterns from [Model Quirks](references/model-quirks.md)
3. Document any changes in behavior
4. Update orchestration patterns if agent capabilities shift

---

## Integration with Tools and Frameworks

This library's patterns integrate with:

**API Integrations**: Function calling patterns work with OpenAI function calls, Claude tool use, and Gemini function declarations

**Agent Frameworks**: Orchestration patterns compatible with LangChain, LlamaIndex, AutoGPT, and custom agent systems

**MCP Servers**: Tool integration patterns apply to Model Context Protocol implementations

**Evaluation Systems**: Reflection and self-consistency patterns support RLHF, evaluation loops, and quality scoring

See [Orchestration Patterns](references/orchestration-patterns.md) for specific integration guidance.

---

## Contributing and Feedback

This is a living document. As models evolve and new patterns emerge, this library should be updated to reflect current best practices.

**Versioning convention**:
- Major version (1.0, 2.0): Significant restructuring or major model changes
- Minor version (1.1, 1.2): New patterns, expanded sections
- Patch (1.1.1): Bug fixes, clarifications, small examples

---

## Quick Reference: Pattern → Use Case Mapping

| Pattern | Best For | Complexity | Document |
|---------|----------|------------|----------|
| Zero-shot | Simple, common tasks | Low | [Patterns](references/prompt-patterns.md#1-zero-shot-prompting) |
| Few-shot | Format specification | Low-Medium | [Patterns](references/prompt-patterns.md#2-few-shot-prompting) |
| Chain-of-thought | Reasoning tasks | Medium | [Patterns](references/prompt-patterns.md#3-chain-of-thought-cot) |
| JSON output | Structured data | Low | [Patterns](references/prompt-patterns.md#4-json-output-specification) |
| Role assignment | Domain expertise | Low | [Patterns](references/prompt-patterns.md#7-expert-role-assignment) |
| Tree-of-thoughts | Complex decisions | High | [Patterns](references/prompt-patterns.md#9-tree-of-thoughts) |
| Self-consistency | High-stakes answers | Medium-High | [Patterns](references/prompt-patterns.md#10-self-consistency) |
| Reflection | Quality assurance | Medium | [Patterns](references/prompt-patterns.md#12-reflection-and-self-critique) |
| Meta-prompting | Prompt optimization | High | [Orchestration](references/orchestration-patterns.md) |
| Planner-executor | Agent systems | High | [Orchestration](references/orchestration-patterns.md) |

---

## License

This prompting pattern library is designed for practical use in AI system development, education, and documentation.

## Links discovered
- [SKILL.md](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/prompting-pattern-library/SKILL.md)
- [Failure Modes](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/prompting-pattern-library/references/failure-modes.md)
- [Model Quirks](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/prompting-pattern-library/references/model-quirks.md)
- [Orchestration Patterns](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/prompting-pattern-library/references/orchestration-patterns.md)
- [references/prompt-patterns.md](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/prompting-pattern-library/references/prompt-patterns.md)
- [references/failure-modes.md](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/prompting-pattern-library/references/failure-modes.md)
- [Prompt Patterns](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/prompting-pattern-library/references/prompt-patterns.md)
- [references/model-quirks.md](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/prompting-pattern-library/references/model-quirks.md)
- [references/orchestration-patterns.md](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/prompting-pattern-library/references/orchestration-patterns.md)
- [Patterns](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/prompting-pattern-library/references/prompt-patterns.md#1-zero-shot-prompting)
- [Patterns](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/prompting-pattern-library/references/prompt-patterns.md#2-few-shot-prompting)
- [Patterns](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/prompting-pattern-library/references/prompt-patterns.md#3-chain-of-thought-cot)
- [Patterns](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/prompting-pattern-library/references/prompt-patterns.md#4-json-output-specification)
- [Patterns](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/prompting-pattern-library/references/prompt-patterns.md#7-expert-role-assignment)
- [Patterns](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/prompting-pattern-library/references/prompt-patterns.md#9-tree-of-thoughts)
- [Patterns](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/prompting-pattern-library/references/prompt-patterns.md#10-self-consistency)
- [Patterns](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/prompting-pattern-library/references/prompt-patterns.md#12-reflection-and-self-critique)
- [Orchestration](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/prompting-pattern-library/references/orchestration-patterns.md)

--- prompting-pattern-library/SKILL.md ---
---
name: prompting-pattern-library
description: Comprehensive library of proven prompting patterns, frameworks, and examples for different use cases. This skill should be used when creating prompting guides, analyzing prompt effectiveness, teaching prompting techniques, or troubleshooting prompting issues. Use when writing about prompting, explaining prompting concepts to others, or improving existing prompts.
---

# Prompting Pattern Library

**Version 1.0** | October 2025 | Tested with Claude 3.5/4, GPT-4/4o, Gemini 1.5 Pro

---

## Navigation

**📖 Full Documentation**: See [README.md](README.md) for complete navigation, use case index, and version notes.

**Quick access by need:**
- **Learning prompting**: Start here, then read [Prompt Patterns](references/prompt-patterns.md)
- **Debugging prompts**: Jump to [Failure Modes](references/failure-modes.md)
- **Building agents**: See [Orchestration Patterns](references/orchestration-patterns.md)
- **Model optimization**: Review [Model Quirks](references/model-quirks.md)

---

## Overview

This skill provides a comprehensive library of prompting patterns, anti-patterns, and model-specific guidance for effective LLM interactions. Use this when creating educational content about prompting, analyzing prompt quality, or explaining prompting techniques to technical and non-technical audiences.

**What's included:**
- 25+ proven prompting patterns with "why it works" analysis
- Common failure modes with diagnosis and fixes
- Model-specific guidance (Claude, GPT-4, Gemini)
- Advanced orchestration patterns for agent systems
- Cross-references throughout for deep-dive learning

---

## Quick Reference: Common Prompting Patterns

### Structural Patterns
**Role Prompting**: Assign a specific role or persona to frame the response
**Chain-of-Thought (CoT)**: Request step-by-step reasoning before final answer
**Few-Shot Learning**: Provide examples of desired input-output pairs
**Zero-Shot with Instructions**: Detailed task description without examples
**Tree of Thoughts**: Explore multiple reasoning paths before choosing best

### Output Control Patterns
**Structured Output**: Request specific formats (JSON, XML, tables, lists)
**Delimiters**: Use clear separators for inputs, examples, and instructions
**Length Control**: Specify desired output length explicitly
**Style Constraints**: Define tone, formality, audience level

### Reasoning Enhancement Patterns
**Self-Consistency**: Generate multiple solutions and select most common
**Reflection**: Ask model to critique its own output
**Decomposition**: Break complex tasks into smaller sub-tasks
**Analogical Reasoning**: Request analogies or comparisons

### Retrieval Patterns
**Citation Requirements**: Demand sources and evidence
**Fact-Checking**: Request verification of claims
**Knowledge Boundaries**: Ask model to acknowledge uncertainty

See `references/prompt-patterns.md` for comprehensive pattern catalog with examples.

## When to Read References

### Always Read First
**Creating prompting educational content**: Read `references/prompt-patterns.md` for pattern catalog with "why it works" analysis
**Debugging problematic prompts**: Read `references/failure-modes.md` for common issues and fixes with cross-referenced solutions
**Cross-model implementation**: Read `references/model-quirks.md` for model-specific considerations and optimization
**Building agent systems**: Read `references/orchestration-patterns.md` for multi-step workflows and agentic architectures

### Read When Needed
**Advanced pattern implementation**: Review specific patterns in references for detailed guidance and research basis
**Teaching prompting**: Use examples from references as teaching materials with "why it works" explanations
**Optimizing existing prompts**: Consult failure modes to identify weaknesses, then apply patterns from prompt-patterns.md
**Agent orchestration**: Reference orchestration-patterns.md for planner-executor, multi-agent collaboration, and evaluation loops

## Core Principles for Effective Prompting

### Specificity Over Generality
Vague: "Write about AI"
Specific: "Write a 500-word technical explanation of transformer attention mechanisms for software engineers with no ML background"

### Provide Context Explicitly
Poor context: "Fix this code"
Good context: "Fix this Python function that should validate email addresses. Current issue: it fails on addresses with plus signs. Python 3.11, standard library only."

### Use Examples When Precision Matters
For tasks requiring specific formats or styles, provide 2-3 high-quality examples rather than lengthy descriptions. Examples communicate requirements more precisely than instructions alone.

### Structure Complex Prompts
For multi-part tasks, use clear sections:
1. Context and background
2. Specific task requirements
3. Output format specifications
4. Constraints and limitations
5. Examples (if applicable)

### Iterate Based on Output
Prompting is experimental. Start simple, observe failure modes, refine incrementally. Most effective prompts emerge through iteration, not perfect first attempts.

## Model-Specific Considerations

Different models respond differently to identical prompts. Key differences:

**Claude (Anthropic)**: Strong with structured output, detailed reasoning, and nuanced tasks. Responds well to polite, conversational prompts. Excellent at maintaining context over long conversations.

**GPT-4 (OpenAI)**: Versatile across domains, strong creative writing, good instruction-following. Benefits from explicit structure. Can be more prone to confident errors.

**Gemini (Google)**: Strong multimodal capabilities, good at analytical tasks. May require more explicit formatting instructions.

See `references/model-quirks.md` for detailed model-specific patterns and anti-patterns.

## Common Failure Modes

### The "Too Polite" Problem
Over-apologetic prompts waste tokens and can reduce output quality. Be direct and clear rather than excessively polite.

### Implicit Assumptions
Models cannot read your mind. What seems obvious to you must be stated explicitly. Common implicit assumptions that cause failures:
- Desired output format
- Audience level
- Required depth of detail
- Constraints (time period, geography, etc.)

### Conflicting Instructions
When instructions contradict each other, models exhibit unpredictable behavior. Example conflict: "Be concise but include comprehensive detail."

### Ambiguous Success Criteria
"Make it better" is not actionable. Define what "better" means: faster, more accurate, more readable, more maintainable, etc.

See `references/failure-modes.md` for comprehensive failure patterns and fixes.

## Using This Skill for Content Creation

### Educational Content
When writing prompting guides or tutorials, use patterns from references as examples. Structure content to move from simple (zero-shot) to complex (chain-of-thought, tree-of-thoughts) patterns.

### Prompt Analysis
To analyze prompt effectiveness, compare against patterns in references. Identify which patterns are present or absent, check for common failure modes.

### Prompt Improvement
To improve existing prompts:
1. Identify the task type and check appropriate patterns in references
2. Check against failure modes in `references/failure-modes.md`
3. Apply relevant structural improvements
4. Test and iterate

## Bundled References

### references/prompt-patterns.md
Comprehensive catalog of 25+ prompting patterns with:
- Pattern name and description
- When to use each pattern
- Concrete examples with "why it works" analysis
- Variations and combinations
- Common pitfalls
- Cross-references to failure modes and model quirks
- Research basis where applicable

### references/failure-modes.md
Common prompting failures organized by:
- Failure type (ambiguity, contradiction, assumption, etc.)
- Symptoms and diagnosis
- Root cause analysis
- Fixes with examples
- Cross-references to relevant patterns
- Prevention checklist

### references/model-quirks.md
Model-specific guidance covering:
- Claude-specific patterns and anti-patterns
- GPT-specific patterns and anti-patterns
- Gemini-specific patterns and anti-patterns
- Cross-model considerations
- When model choice matters
- Model selection decision tree
- Testing strategies across models

### references/orchestration-patterns.md
Advanced patterns for multi-step AI workflows:
- Meta-prompting patterns (prompt generation, optimization loops)
- Multi-agent orchestration (planner-executor, specialist collaboration)
- Evaluation and refinement loops (generate-critique-revise, ensemble evaluation)
- Tool integration patterns (tool selection, function call orchestration)
- Memory and state management (stateful conversations, context summarization)
- Pattern composition for production systems
- Performance optimization strategies


## Links discovered
- [README.md](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/prompting-pattern-library/README.md)
- [Prompt Patterns](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/prompting-pattern-library/references/prompt-patterns.md)
- [Failure Modes](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/prompting-pattern-library/references/failure-modes.md)
- [Orchestration Patterns](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/prompting-pattern-library/references/orchestration-patterns.md)
- [Model Quirks](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/prompting-pattern-library/references/model-quirks.md)

--- requirements-elicitation/SKILL.md ---
---
name: requirements-elicitation
description: Systematic framework for analyzing product documents (PRDs, feature specs, user stories, roadmaps, one-pagers) to identify gaps, generate clarifying questions for PMs and engineers, and assess technical risks. This skill should be used when engineers or technical leads need to bridge PM documents and implementation by eliciting missing technical details rather than making assumptions. Use when asked to extract technical requirements, review specs, identify what's missing, or prepare clarifying questions from product documents.
license: Complete terms in LICENSE.txt
---

# Requirements Elicitation Skill

This skill provides a systematic framework for analyzing product documents and eliciting technical requirements needed for implementation. It bridges the gap between product management and engineering by identifying ambiguities, generating targeted clarifying questions, and assessing technical risks.

## Core Philosophy

**ELICIT, DON'T INVENT**

This skill is designed to identify gaps and ask questions, NOT to fill in missing details with assumptions. Engineers frequently complain that PM requirements are overly broad. This skill helps narrow and specify requirements through systematic elicitation rather than inventing technical details that aren't in the source document.

### Key Principles

1. **Identify ambiguity** - Point out where requirements are unclear or underspecified
2. **Ask targeted questions** - Generate specific, actionable questions organized by stakeholder
3. **Assess risk** - Evaluate technical risks introduced by missing requirements
4. **Avoid assumptions** - Never invent technical details (tech stack, architecture, data models) that aren't specified
5. **Stay grounded** - Always reference the source document; don't add scope or features

## When to Use This Skill

Use this skill when:
- Analyzing PRDs, feature specs, user stories, epics, roadmap docs, or one-pagers
- Asked to "extract technical requirements" from product documents
- Reviewing specs to identify what's missing from an engineering perspective
- Preparing for technical planning or estimation
- Bridging communication between PM and engineering teams
- Asked to help with requirements that seem too broad or vague

Trigger phrases:
- "Extract the technical requirements from..."
- "Review this spec and identify what's missing..."
- "My PM just gave me this PRD, help me find the technical requirements"
- "What questions should I ask about..."
- "Identify gaps in this feature spec"

## Workflow

Follow this systematic process when analyzing product documents:

### Phase 1: Initial Analysis (Always Do This First)

1. **Read the entire document** to understand the feature and its context
2. **Identify what IS specified** - Note the parts that are clear and well-defined
3. **Load the technical dimensions checklist** from `references/technical_dimensions.md`
4. **Systematically review** the document against each dimension in the checklist
5. **Document gaps** - Create a list of areas where critical details are missing

### Phase 2: Question Generation

1. **Load question templates** from `references/question_templates.md`
2. **Organize questions by stakeholder**:
   - **Questions for PM**: Product intent, user expectations, business rules, scope
   - **Questions for Engineering**: Technical decisions, architecture, implementation approach
3. **Make questions specific** - Reference concrete scenarios from the document
4. **Explain why each question matters** - Brief technical context for impact
5. **Avoid assumption-laden questions** - Don't ask "Should we use Redis for caching?" if caching wasn't mentioned

### Phase 3: Risk Assessment

1. **Load risk framework** from `references/risk_assessment.md`
2. **Identify risks** in categories: Implementation, Performance, Security, Data Integrity, Integration, Operations, UX, Compliance
3. **Assign severity levels** - Critical (blocks work), High (needs assumptions), Medium (creates uncertainty), Low (can resolve later)
4. **Link risks to gaps** - Show how missing requirements create specific risks
5. **Prioritize clarification** - Help stakeholders understand what must be resolved vs what's nice-to-have

### Phase 4: Output Creation

Based on the analysis depth needed, create appropriate outputs:

**For quick reviews** (5-10 minutes):
- Brief summary of critical gaps
- Top 5-10 most important questions
- High-level risk assessment

**For thorough analysis** (20-30 minutes):
- Complete gap analysis using `assets/gap_analysis_template.md`
- Comprehensive questions document using `assets/clarifying_questions_template.md`
- Detailed risk assessment with all severity levels

**Output format options:**
- Markdown document (most common)
- Inline response (for quick reviews)
- Presentation format (for stakeholder meetings)

### Phase 5: Post-Clarification (Only After Conversation)

After questions have been answered and gaps filled through discussion:

1. **Technical Specification** - Now create detailed tech specs with actual technical decisions
2. **Implementation Stories** - Break down into stories with acceptance criteria
3. **API Contracts** - Define interfaces and data models based on clarified requirements
4. **Architecture Decisions** - Document technical approach with rationale

**CRITICAL:** Do not create these detailed technical artifacts in Phase 1-4. Only create them after the elicitation conversation has filled in the gaps.

## Reference Files

### `references/technical_dimensions.md`
Comprehensive checklist of technical aspects to consider:
- Data & State Management
- Interfaces & Integration
- Performance & Scale
- Security & Privacy
- Reliability & Operations
- Business Logic & Rules
- Dependencies & Constraints
- User Experience & Behavior
- Testing & Validation

**When to use:** During Phase 1 (Initial Analysis) to systematically review all technical dimensions.

### `references/question_templates.md`
Templates for structuring questions and examples of effective elicitation:
- Question structuring principles
- Categories of questions by audience (PM vs Engineering)
- Good vs bad requirements analysis examples
- Common requirement anti-patterns
- Output structure templates

**When to use:** During Phase 2 (Question Generation) to formulate clear, actionable questions.

### `references/risk_assessment.md`
Framework for identifying and categorizing technical risks:
- Risk categories (Implementation, Performance, Security, Data, Integration, Operations, UX, Compliance)
- Severity levels (Critical, High, Medium, Low)
- Risk identification process
- Common risk patterns
- Communication guidelines

**When to use:** During Phase 3 (Risk Assessment) to evaluate technical risks introduced by requirement gaps.

## Asset Files

### `assets/gap_analysis_template.md`
Comprehensive template for documenting requirements gaps:
- Executive summary with readiness assessment
- Critical gaps that block implementation
- Technical dimensions analysis
- Risk assessment
- Assumptions to validate
- Recommended next steps

**When to use:** For thorough analysis where a formal gap analysis document is needed.

### `assets/clarifying_questions_template.md`
Structured template for organizing questions by stakeholder:
- Sections for PM questions vs Engineering questions
- Space for discussion notes
- Follow-up action items

**When to use:** When preparing questions for review meetings or async communication with stakeholders.

## Examples

### Example 1: Quick Review

**User Request:**
"Review this feature spec and identify what's missing from an engineering perspective: [attaches document about adding social login]"

**Good Response Process:**
1. Read document noting it mentions "users can log in with social providers"
2. Review technical dimensions checklist
3. Generate targeted questions:
   - For PM: Which social providers? Account linking behavior? Auto-create accounts?
   - For Engineering: Where to store OAuth tokens? Session management strategy? Integration with current auth?
4. Note risks: Missing security details (HIGH), unclear user flow (MEDIUM)
5. Output concise list of questions and risks inline

**Bad Response Process:**
❌ Immediately jumping to: "You'll need to implement OAuth 2.0 with Google and Facebook, store JWT tokens in localStorage..."

### Example 2: Thorough Analysis

**User Request:**
"My PM just gave me this PRD for a notification system. Help me find the technical requirements."

**Good Response Process:**
1. Read entire PRD about notifications
2. Load `references/technical_dimensions.md` and systematically check each dimension
3. Load `references/question_templates.md` for structuring
4. Identify gaps: trigger logic undefined, delivery channels unspecified, no performance requirements, missing error handling
5. Load `references/risk_assessment.md` and categorize risks
6. Create documents using both templates:
   - `gap_analysis_template.md` filled with findings
   - `clarifying_questions_template.md` with organized questions
7. Output both documents

**Bad Response Process:**
❌ Creating a full technical specification with "We'll use Firebase for push, SendGrid for email, notifications triggered by database updates..."

### Example 3: Scope Clarification

**User Request:**
"Extract technical requirements from this one-pager about search functionality"

**Good Response Process:**
1. Read one-pager noting it says "users can search transactions"
2. Identify it lacks: searchable fields, matching behavior, performance targets, result ordering
3. Ask questions: "What fields are searchable?" "Exact or fuzzy matching?" "Expected data volume?" "Acceptable latency?"
4. Note risks: Performance risk (no volume/latency specified), UX risk (result display undefined)
5. Explicitly note: "I'm not assuming a search technology (Elasticsearch, etc.) since requirements don't specify performance or scale needs yet"

**Bad Response Process:**
❌ "Implement Elasticsearch with fuzzy matching, index these fields, return 25 results per page..."

## Common Pitfalls to Avoid

### 1. Inventing Technical Details
❌ "This requires a Redis cache with 15-minute TTL"
✅ "No caching strategy specified. Question for Engineering: Do we need caching? What are the performance targets?"

### 2. Assuming Tech Stack
❌ "We'll use React for the frontend and Express for the backend"
✅ "Frontend interaction model not specified. Where should this functionality live?"

### 3. Adding Scope
❌ "We should also add email notifications even though the doc only mentions push"
✅ "Document specifies push notifications. Should confirm if other channels needed."

### 4. Making Architecture Decisions
❌ "This needs a microservice with event-driven architecture"
✅ "Integration approach not specified. How should this integrate with existing systems?"

### 5. Over-Specifying Prematurely
❌ Creating detailed API contracts before clarifying questions are answered
✅ Asking "What operations need API endpoints?" and only designing APIs after answers

### 6. Ignoring What IS Specified
❌ Treating everything as a gap
✅ Acknowledging "The document clearly specifies X, Y, and Z. Gaps are in areas A, B, C."

### 7. Generic Questions
❌ "What are the requirements?"
✅ "The document mentions 'real-time updates' - does real-time mean <100ms latency or is 1-2 seconds acceptable?"

## Best Practices

### Be Specific and Concrete
- Reference exact text from the document
- Use concrete scenarios: "What happens when..." not "How does this work?"
- Quantify when possible: "How many users?" not "What's the scale?"

### Organize Clearly
- Group questions by stakeholder (PM vs Engineering)
- Categorize by dimension (Data, Security, Performance, etc.)
- Prioritize by criticality (Critical > High > Medium > Low)

### Explain Impact
- For each question, briefly explain why it matters technically
- Connect gaps to specific risks
- Help stakeholders understand prioritization

### Stay Objective
- Avoid judging the document quality
- Focus on what's needed for implementation
- Be helpful, not critical

### Know When to Transition
- Phase 1-4: Focus on elicitation (questions and gaps)
- Phase 5: Only after gaps are filled, create detailed specs
- Don't create technical artifacts prematurely

### Use Templates Appropriately
- Quick reviews: Inline response with key questions
- Formal analysis: Use both templates for comprehensive documentation
- Async communication: Clarifying questions template
- Stakeholder presentations: Gap analysis template

## Adapting to Document Types

### PRDs (Product Requirements Documents)
- Usually most comprehensive, but watch for missing technical details
- Focus on data models, APIs, performance, security
- Good source for business rules and user flows

### Feature Specs
- Often focused on user-facing behavior
- May lack backend/integration details
- Check for error handling, edge cases, data consistency

### User Stories / Epics
- Typically brief, expect many gaps
- Focus on acceptance criteria and edge cases
- Need to elicit technical approach entirely

### Roadmap Documents
- Highest level, least detail
- Focus on scope clarification and feasibility
- Identify large unknowns early

### One-Pagers
- Very brief, expect significant gaps
- Start with understanding goals and constraints
- May need follow-up documents before technical planning

## Success Criteria

A successful requirements elicitation produces:

1. **Clear gap identification** - Stakeholders understand what's missing
2. **Actionable questions** - Questions can be directly answered
3. **Risk visibility** - Technical risks are surfaced and prioritized
4. **No false assumptions** - Technical details only included if in source document
5. **Efficient communication** - Engineers and PMs aligned on what needs clarification
6. **Implementation readiness** - After clarification, team can confidently estimate and implement

The goal is not a perfect requirements document - it's identifying exactly what questions need answering to get there.


--- requirements-elicitation/assets/clarifying_questions_template.md ---
# Clarifying Questions: [Feature Name]

**Document:** [PRD/Feature Spec/One-Pager Title]  
**Date:** [Date]  
**Prepared By:** [Your Name]

---

## How to Use This Document

This document contains questions organized by stakeholder. Some questions may apply to multiple audiences - use your judgment about who is best positioned to answer.

**For Product Managers:** Questions about product intent, user experience, business rules, and scope  
**For Engineering Team:** Questions about technical decisions, architecture, and implementation approach

---

## Questions for Product Manager

### User Experience & Behavior

**Context:** [Brief reference to relevant section of product document]

1. [Question about user flow or interaction]
   - *Why this matters:* [Brief explanation]

2. [Question about user expectations]
   - *Why this matters:* [Brief explanation]

3. [Question about edge cases]
   - *Why this matters:* [Brief explanation]

---

### Business Rules & Logic

**Context:** [Brief reference to relevant section]

1. [Question about business rule]
   - *Why this matters:* [Brief explanation]

2. [Question about validation or constraints]
   - *Why this matters:* [Brief explanation]

---

### Scope & Priorities

**Context:** [Brief reference to relevant section]

1. [Question about what's in/out of scope]
   - *Why this matters:* [Brief explanation]

2. [Question about must-haves vs nice-to-haves]
   - *Why this matters:* [Brief explanation]

---

### Success Criteria & Metrics

**Context:** [Brief reference to relevant section]

1. [Question about how to measure success]
   - *Why this matters:* [Brief explanation]

2. [Question about acceptance criteria]
   - *Why this matters:* [Brief explanation]

---

### Error States & Edge Cases

**Context:** [Brief reference to relevant section]

1. [Question about error scenario]
   - *Why this matters:* [Brief explanation]

2. [Question about edge case handling]
   - *Why this matters:* [Brief explanation]

---

## Questions for Engineering Team

### Architecture & System Design

**Context:** [Brief reference to technical implications]

1. [Question about system components]
   - *Why this matters:* [Brief explanation]

2. [Question about integration approach]
   - *Why this matters:* [Brief explanation]

3. [Question about technology choices]
   - *Why this matters:* [Brief explanation]

---

### Data Models & Storage

**Context:** [Brief reference to data mentioned in document]

1. [Question about data structure]
   - *Why this matters:* [Brief explanation]

2. [Question about relationships]
   - *Why this matters:* [Brief explanation]

3. [Question about storage strategy]
   - *Why this matters:* [Brief explanation]

---

### APIs & Interfaces

**Context:** [Brief reference to integration points]

1. [Question about API design]
   - *Why this matters:* [Brief explanation]

2. [Question about data formats]
   - *Why this matters:* [Brief explanation]

---

### Performance & Scale

**Context:** [Brief reference to performance implications]

1. [Question about expected load]
   - *Why this matters:* [Brief explanation]

2. [Question about performance targets]
   - *Why this matters:* [Brief explanation]

3. [Question about scaling strategy]
   - *Why this matters:* [Brief explanation]

---

### Security & Access Control

**Context:** [Brief reference to security concerns]

1. [Question about authentication/authorization]
   - *Why this matters:* [Brief explanation]

2. [Question about data protection]
   - *Why this matters:* [Brief explanation]

---

### Error Handling & Reliability

**Context:** [Brief reference to failure scenarios]

1. [Question about error handling]
   - *Why this matters:* [Brief explanation]

2. [Question about retry/fallback logic]
   - *Why this matters:* [Brief explanation]

3. [Question about recovery strategy]
   - *Why this matters:* [Brief explanation]

---

### Deployment & Operations

**Context:** [Brief reference to operational concerns]

1. [Question about deployment strategy]
   - *Why this matters:* [Brief explanation]

2. [Question about monitoring needs]
   - *Why this matters:* [Brief explanation]

3. [Question about data migration]
   - *Why this matters:* [Brief explanation]

---

## Follow-Up Actions

Based on answers to these questions, the following artifacts may need to be created:

- [ ] Technical specification document
- [ ] Data model diagram
- [ ] API contract/specification
- [ ] Architecture decision record (ADR)
- [ ] Implementation stories with acceptance criteria
- [ ] Test scenarios and test data requirements

---

## Discussion Notes

[Space for capturing answers and decisions during review meetings]

### [Topic Area]
**Question:** [Original question]  
**Answer:** [Response from stakeholder]  
**Decision:** [What was decided]  
**Action Items:** [Any follow-up tasks]


--- requirements-elicitation/assets/gap_analysis_template.md ---
# Requirements Gap Analysis: [Feature Name]

**Document Analyzed:** [PRD/Feature Spec/One-Pager Title]  
**Analysis Date:** [Date]  
**Analyzed By:** [Your Name]

## Executive Summary

[2-3 sentences summarizing the feature and the overall state of requirements completeness]

**Readiness for Implementation:** [Not Ready / Partially Ready / Ready with Clarifications]

**Critical Blockers:** [Number] areas require clarification before implementation can begin

---

## Feature Overview

**What is specified:**
- [Key aspect 1]
- [Key aspect 2]
- [Key aspect 3]

**What is missing:**
- [Critical gap 1]
- [Critical gap 2]
- [Critical gap 3]

---

## Critical Gaps (Must Resolve Before Implementation)

### 1. [Gap Name]

**What the document says:**
> [Relevant quote or paraphrase from document]

**What is missing:**
[Specific missing details]

**Why this matters:**
[Technical impact or risk]

**Questions to resolve:**
- [Question 1]
- [Question 2]

---

### 2. [Gap Name]

[Same structure as above]

---

## High Priority Gaps (Should Resolve Before Implementation)

### [Gap Name]

**What the document says:**
> [Relevant quote]

**What is missing:**
[Details]

**Questions to resolve:**
- [Question 1]
- [Question 2]

---

## Medium Priority Gaps (Can Resolve During Implementation)

- **[Gap Name]:** [Brief description of what's missing]
- **[Gap Name]:** [Brief description of what's missing]

---

## Technical Dimensions Analysis

### ✅ Well-Specified Areas
- **[Dimension]:** [What is clearly defined]
- **[Dimension]:** [What is clearly defined]

### ⚠️ Partially Specified Areas
- **[Dimension]:** [What is mentioned but needs more detail]
- **[Dimension]:** [What is mentioned but needs more detail]

### ❌ Unspecified Areas
- **[Dimension]:** [What is completely missing]
- **[Dimension]:** [What is completely missing]

---

## Risk Assessment

### Critical Risks
**[Risk Name]** (Category: [Implementation/Security/Performance/Data/Integration/Operations/UX/Compliance])
- **Issue:** [What is undefined]
- **Impact:** [What could go wrong]
- **Mitigation:** [What needs to be clarified]

### High Risks
[Same structure]

### Medium Risks
[Same structure]

---

## Assumptions That Need Validation

The following assumptions are implied by the document but should be explicitly confirmed:

1. **[Assumption]**
   - Implied by: [Reference to document]
   - Should confirm: [Specific question]

2. **[Assumption]**
   - Implied by: [Reference to document]
   - Should confirm: [Specific question]

---

## Recommended Next Steps

1. **Immediate Actions** (Before Implementation Starts)
   - [ ] [Action item 1]
   - [ ] [Action item 2]
   - [ ] [Action item 3]

2. **Follow-up During Implementation**
   - [ ] [Action item 1]
   - [ ] [Action item 2]

3. **Schedule Discussions**
   - [ ] Meeting with PM to clarify: [topics]
   - [ ] Technical design review to decide: [topics]
   - [ ] Team sync to align on: [topics]

---

## Appendix: Detailed Questions

See attached "Clarifying Questions" document for complete list of questions organized by stakeholder.


--- resume-builder/SKILL.md ---
---
name: resume-builder
description: Comprehensive resume creation, review, and optimization with support for multiple formats, ATS optimization, industry-specific guidance, and career stage customization. Use this skill when users request help writing, creating, reviewing, improving, or tailoring resumes for job applications.
---

# Resume Builder

## Overview

This skill provides comprehensive guidance for creating professional, ATS-optimized resumes tailored to specific industries, career stages, and job opportunities. It includes best practices, templates, industry-specific guidance, and ATS optimization strategies.

## Workflow Decision Tree

Use this decision tree to determine the appropriate workflow:

**1. Is this a new resume from scratch?**
   → Yes: Follow "Creating New Resumes" workflow
   → No: Continue to question 2

**2. Is there an existing resume to review/improve?**
   → Yes: Follow "Reviewing Existing Resumes" workflow
   → No: Continue to question 3

**3. Is the goal to tailor an existing resume for a specific job?**
   → Yes: Follow "Tailoring Resumes" workflow
   → No: Follow "General Resume Consultation" workflow

## Creating New Resumes

Use this workflow when creating a resume from scratch.

### Step 1: Gather Information

Collect essential information through conversation:

**Career Information:**
- Career stage (entry-level, mid-career, senior, career changer)
- Target industry and specific role
- Years of experience in field
- Education background
- Key skills and certifications
- Notable achievements and quantifiable results

**Context:**
- Specific job description (if available) for tailoring
- Career transition context (if applicable)
- Employment gaps or unique circumstances
- Geographic location (for regional considerations)

### Step 2: Select Appropriate Template

Choose from available templates based on situation:

**Chronological Template** (`assets/template-chronological.md`):
- Best for: Consistent work history, clear career progression
- Use when: Staying in same field, no significant gaps
- Candidates: Most professionals with traditional career paths

**Functional Template** (`assets/template-functional.md`):
- Best for: Career changers, employment gaps, diverse experience
- Use when: Emphasizing transferable skills over timeline
- Candidates: Career transitions, returning to workforce

**Combination Template** (`assets/template-combination.md`):
- Best for: Mid-career professionals with diverse skill sets
- Use when: Highlighting both skills and career progression
- Candidates: Strong experience wanting to emphasize competencies

**Entry-Level Template** (`assets/template-entry-level.md`):
- Best for: Recent graduates, 0-2 years experience
- Use when: Limited professional experience, strong academic background
- Candidates: New graduates, interns transitioning to full-time

### Step 3: Reference Industry and Career Stage Guidance

Before writing, consult relevant sections in `references/industry-career-guidance.md`:

**Career Stage Guidance:**
- Entry-Level / Recent Graduate strategies
- Mid-Career Professional approaches
- Senior / Executive positioning
- Career Changer techniques

**Industry-Specific Guidance:**
- Technology / Software Engineering
- Healthcare / Medical
- Finance / Accounting
- Marketing / Sales
- Education / Teaching
- Legal
- Retail / Hospitality
- Non-Profit / Social Impact
- Government / Public Service
- Creative / Design
- Manufacturing / Supply Chain

Tailor language, priorities, and emphasis based on industry norms.

### Step 4: Apply Best Practices

Consult `references/best-practices.md` for:

**Content Standards:**
- Achievement-oriented language with action verbs
- Quantified results following the formula: Action + Task + Result
- Appropriate professional tone and tense usage
- Relevance over completeness

**Format Standards:**
- Length appropriate to career stage (1-2 pages)
- Professional visual design with adequate white space
- Standard fonts and consistent formatting
- PDF file format for submission

**Section-Specific Guidance:**
- Contact information requirements
- Professional summary construction (when to include/skip)
- Work experience bullet formulation using STAR method
- Education placement and details based on career stage
- Skills section organization and content
- Additional sections (certifications, projects, etc.)

### Step 5: Implement ATS Optimization

Apply ATS-friendly practices from `references/ats-optimization.md`:

**Structure Requirements:**
- Standard section headings
- Simple, single-column layout
- No text boxes, tables, or complex formatting
- Standard fonts and bullet points

**Keyword Integration:**
- Extract keywords from job description (if provided)
- Place keywords strategically in summary, skills, and experience
- Use both acronyms and spelled-out versions
- Integrate naturally without keyword stuffing

**Format Compliance:**
- Text-selectable PDF format
- No images or graphics containing text
- Standard date formatting
- Plain text contact information

### Step 6: Write and Format Resume

Create resume using docx format:

1. **Start with selected template structure**
2. **Populate each section** with gathered information
3. **Apply industry-specific language** and priorities
4. **Ensure ATS compliance** throughout
5. **Verify all formatting** is clean and consistent
6. **Include relevant sections** only (delete inapplicable ones)

**Critical Formatting Guidelines:**
- Use standard fonts: Calibri, Arial, Helvetica, or similar (10-12pt body, 14-16pt headers)
- Consistent bullet points throughout (use • or - consistently)
- Left-aligned text for easy scanning
- Adequate white space between sections
- Bold for headers and company/role names
- Dates right-aligned or consistently placed
- No decorative elements that interfere with ATS parsing

### Step 7: Quality Check

Before delivering, verify:

**Content:**
- [ ] All bullets start with strong action verbs
- [ ] Achievements include quantifiable metrics where possible
- [ ] Verb tenses consistent (past for previous, present for current)
- [ ] No typos or grammatical errors
- [ ] Content tailored to target role (if job description provided)

**Format:**
- [ ] Appropriate length for career stage
- [ ] Consistent formatting throughout
- [ ] Professional appearance
- [ ] Standard section headings
- [ ] Contact information accurate and complete

**ATS:**
- [ ] Simple formatting without tables/columns
- [ ] Keywords from job description included (if provided)
- [ ] Text-based (not image-based)
- [ ] Standard fonts used

## Reviewing Existing Resumes

Use this workflow when reviewing and improving an existing resume.

### Step 1: Initial Assessment

Analyze the existing resume against best practices:

**Content Review:**
- Are bullets achievement-oriented or duty-oriented?
- Are results quantified with specific metrics?
- Is verb tense consistent?
- Is all content relevant to target role?
- Are there typos or grammatical errors?

**Format Review:**
- Is length appropriate for career stage?
- Is formatting consistent (fonts, bullets, spacing)?
- Is there adequate white space?
- Are section headings standard and clear?
- Is visual hierarchy effective?

**ATS Compatibility:**
- Does it use ATS-friendly formatting?
- Are there problematic elements (tables, columns, text boxes)?
- Are section headings standard?
- Would this parse correctly in an ATS?

### Step 2: Identify Improvement Opportunities

Categorize issues found:

**Critical Issues** (must fix):
- Typos and grammatical errors
- ATS-incompatible formatting
- Missing critical sections (contact info, experience, education)
- Unprofessional elements
- Inconsistent or incorrect dates

**Major Improvements** (significantly strengthen resume):
- Transform duty-based bullets to achievement-based
- Add quantifiable results and metrics
- Improve keyword integration for ATS
- Restructure sections for better impact
- Add missing relevant information

**Minor Enhancements** (polish and refine):
- Strengthen action verbs
- Improve consistency in formatting
- Optimize white space
- Refine professional summary
- Reorder bullets for impact

### Step 3: Reference Guidance

Consult relevant reference materials based on issues identified:

**For content improvements:** `references/best-practices.md`
- Achievement-oriented language
- STAR method for accomplishments
- Section-specific guidance
- Common mistakes to avoid

**For ATS issues:** `references/ats-optimization.md`
- Formatting requirements
- Keyword optimization
- Common ATS pitfalls
- Testing approaches

**For industry/stage alignment:** `references/industry-career-guidance.md`
- Career stage strategies
- Industry-specific guidance
- Appropriate emphasis and language

### Step 4: Provide Structured Feedback

Organize feedback clearly:

**Summary Assessment:**
- Overall strengths of current resume
- Key areas for improvement
- Priority of changes (critical, major, minor)

**Specific Recommendations:**
For each section, provide:
- What works well
- What needs improvement
- Specific suggestions with examples
- Revised versions of weak bullets

**Example Feedback Structure:**

```
**Professional Summary:**
Current: [Show current text]
Issue: Generic and lacks specific value proposition
Revised: [Provide improved version with explanation]

**Experience Section:**
Bullet Analysis:
❌ Current: "Responsible for managing social media accounts"
Issue: Duty-oriented, not achievement-focused, lacks metrics
✅ Revised: "Grew Instagram following from 2K to 45K in 8 months through targeted content strategy and influencer partnerships, increasing engagement rate by 300%"
```

### Step 5: Create Revised Version

If requested, create a fully revised resume:

1. Maintain user's core information and experiences
2. Apply all recommended improvements
3. Ensure ATS compliance
4. Optimize for target role (if specified)
5. Use appropriate template format
6. Deliver in professional docx format

## Tailoring Resumes for Specific Jobs

Use this workflow when customizing a resume for a particular job application.

### Step 1: Analyze Job Description

Extract key information from job description:

**Required Qualifications:**
- Must-have skills and experience
- Required certifications or education
- Years of experience specified
- Technical requirements

**Preferred Qualifications:**
- Nice-to-have skills
- Additional certifications
- Preferred background

**Keywords and Terminology:**
- Technical terms and tools
- Industry-specific language
- Action verbs used
- Acronyms and spelled-out versions
- Job title variations

**Company Culture and Values:**
- Language used in description
- Emphasized qualities
- Company information

### Step 2: Reference ATS Optimization

Consult `references/ats-optimization.md` for:

**Keyword Extraction Strategies:**
- How to identify priority keywords
- Where to place keywords strategically
- How to use variations (acronyms + full terms)
- Natural integration techniques

**Customization Approach:**
- Updating professional summary
- Reordering experience bullets
- Adjusting skills section
- Adding relevant context

### Step 3: Create Tailored Version

Customize the resume strategically:

**Professional Summary:**
- Mirror language from job description
- Highlight 2-3 top matching qualifications
- Include years of experience if specified in JD
- Use job title or aspirational variation

**Skills Section:**
- Lead with most relevant skills for position
- Include all mentioned technologies/tools from JD
- Use exact phrasing from job description
- Include both acronyms and spelled-out versions

**Experience Section:**
- Reorder bullets to emphasize relevant achievements
- Add context where needed to show skill matches
- Integrate job description keywords naturally
- Quantify achievements that align with role requirements

**Additional Sections:**
- Emphasize certifications mentioned in JD
- Highlight relevant projects
- Include any specified requirements

### Step 4: Verify Keyword Coverage

Ensure comprehensive alignment:

- [ ] All major required skills appear in resume
- [ ] Keywords used multiple times naturally
- [ ] Both technical and soft skills addressed (if in JD)
- [ ] Industry-specific terminology included
- [ ] Job title variations incorporated
- [ ] No keyword stuffing (maintains readability)

### Step 5: Balance ATS and Human Appeal

Remember: ATS gets resume through door, humans make decisions.

**Ensure Resume:**
- Passes ATS parsing and ranking
- Reads naturally and compellingly
- Tells coherent career story
- Showcases qualifications effectively
- Maintains professional appearance
- Would impress human recruiters

## General Resume Consultation

For general questions or guidance:

**Consult Reference Materials:**
- `references/best-practices.md` - Comprehensive guidance on content and format
- `references/ats-optimization.md` - ATS compliance and optimization
- `references/industry-career-guidance.md` - Industry and career stage specifics

**Provide Specific, Actionable Advice:**
- Reference relevant sections from documentation
- Give concrete examples
- Explain reasoning behind recommendations
- Offer alternatives when applicable

## Resources

### References

**best-practices.md**
Comprehensive resume writing guidance covering core principles, format standards, section-specific guidance, common mistakes, tailoring strategies, and proofreading checklist.

**ats-optimization.md**
Complete ATS optimization guide including how ATS works, formatting requirements, keyword optimization, common pitfalls, testing approaches, and industry-specific considerations.

**industry-career-guidance.md**
Specialized guidance for different career stages, industries, global considerations, and remote work experience presentation.

### Templates

**template-chronological.md**
Standard chronological format for traditional career progression.

**template-functional.md**
Skills-based format for career changers and non-traditional paths.

**template-combination.md**
Hybrid format balancing skills and chronological experience.

**template-entry-level.md**
Specialized format for recent graduates and early-career professionals.

## Best Practices for Using This Skill

**Always:**
- Read relevant reference documentation before writing
- Choose appropriate template for user's situation
- Apply ATS optimization principles
- Quantify achievements with metrics
- Use strong action verbs
- Maintain consistent formatting
- Proofread thoroughly

**Never:**
- Use generic, template-like language
- Include obvious soft skills without context
- Create overly long resumes (>2 pages without strong justification)
- Use complex formatting that breaks ATS parsing
- Ignore job description when provided
- Include personal information inappropriate for region (photos in US, age, marital status)

**Remember:**
- Every resume should be tailored to the target role
- Quality over quantity in bullet points
- ATS compliance and human readability both matter
- Industry and career stage significantly impact approach
- Specific, measurable achievements are most compelling
- Professional appearance matters

## Output Format

Always deliver resumes as professional docx files:

1. Use standard business formatting
2. Ensure consistent styling throughout
3. Apply appropriate fonts (Calibri, Arial, or similar)
4. Include proper spacing and margins
5. Save as .docx format
6. Provide clear file naming: FirstName_LastName_Resume.docx

When providing feedback or examples, use clear formatting with:
- ❌ to indicate weak examples or current issues
- ✅ to indicate strong examples or improvements
- Bold for emphasis on key changes
- Bullet points for organized feedback


--- resume-builder/assets/template-chronological.md ---
# CHRONOLOGICAL RESUME TEMPLATE

Use this template for candidates with consistent work history and clear career progression.
Best for: Most professionals, especially those staying in same field

---

**[YOUR FULL NAME]**
[City, State] | [Phone] | [Email] | [LinkedIn URL] | [Portfolio/Website if relevant]

---

## PROFESSIONAL SUMMARY
[2-4 sentences highlighting your experience level, key skills, and value proposition. Include relevant keywords from target job description. Optional but recommended for mid-career and senior professionals.]

---

## PROFESSIONAL EXPERIENCE

**[Job Title]** | [Company Name], [City, State]  
[Month Year] – [Month Year or "Present"]

- [Achievement-focused bullet starting with action verb, including quantifiable result]
- [Achievement-focused bullet starting with action verb, including quantifiable result]
- [Achievement-focused bullet starting with action verb, including quantifiable result]
- [Achievement-focused bullet starting with action verb, including quantifiable result]

**[Job Title]** | [Company Name], [City, State]  
[Month Year] – [Month Year]

- [Achievement-focused bullet starting with action verb, including quantifiable result]
- [Achievement-focused bullet starting with action verb, including quantifiable result]
- [Achievement-focused bullet starting with action verb, including quantifiable result]
- [Achievement-focused bullet starting with action verb, including quantifiable result]

**[Job Title]** | [Company Name], [City, State]  
[Month Year] – [Month Year]

- [Achievement-focused bullet starting with action verb, including quantifiable result]
- [Achievement-focused bullet starting with action verb, including quantifiable result]
- [Achievement-focused bullet starting with action verb, including quantifiable result]

---

## EDUCATION

**[Degree], [Major]** | [University Name], [City, State]  
[Month Year] | [GPA if 3.5+ and recent graduate]

- [Relevant honors, awards, or distinctions if applicable]
- [Relevant coursework if recent graduate or career changer - list 3-5 courses]

---

## SKILLS

**[Category]:** [Skill 1], [Skill 2], [Skill 3], [Skill 4], [Skill 5]  
**[Category]:** [Skill 1], [Skill 2], [Skill 3], [Skill 4], [Skill 5]  
**[Category]:** [Skill 1], [Skill 2], [Skill 3], [Skill 4], [Skill 5]

---

## CERTIFICATIONS (if applicable)

**[Certification Name]** | [Issuing Organization] | [Month Year]  
**[Certification Name]** | [Issuing Organization] | [Month Year]

---

## ADDITIONAL (Optional sections as relevant)

**Projects:**
- **[Project Name]:** [Brief description with technologies used and impact] | [Link if available]

**Publications:**
- [Citation in proper format]

**Volunteer Experience:**
- **[Role]** | [Organization] | [Dates] - [Brief achievement-focused description]

**Languages:**
- [Language]: [Proficiency Level]

---

**USAGE NOTES:**

1. **Length:** Keep to 1 page for 0-10 years experience, 2 pages for 10+ years
2. **Bullets:** 4-6 per recent position, 2-3 for older positions
3. **Tense:** Past tense for previous roles, present tense for current role
4. **Customization:** Tailor summary and reorder bullets for each application
5. **Keywords:** Integrate terms from job description throughout
6. **Quantification:** Include metrics in as many bullets as possible
7. **Action Verbs:** Start each bullet with strong action verb (Led, Developed, Increased, etc.)
8. **Delete:** Remove any sections marked "Optional" or "if applicable" that don't apply to you


--- resume-builder/assets/template-combination.md ---
# COMBINATION RESUME TEMPLATE

Use this template to highlight both skills and career progression. Balances functional skills emphasis with chronological work history.
Best for: Mid-career professionals, those with diverse skill sets, career changers with relevant experience

---

**[YOUR FULL NAME]**
[City, State] | [Phone] | [Email] | [LinkedIn URL] | [Portfolio/Website if relevant]

---

## PROFESSIONAL SUMMARY
[3-4 sentences highlighting your unique value proposition, key skill areas, and relevant achievements. Bridge between your experience and target role. Include relevant keywords.]

---

## CORE COMPETENCIES & SKILLS

**[Skill Category 1]:** [Skill], [Skill], [Skill], [Skill], [Skill]  
**[Skill Category 2]:** [Skill], [Skill], [Skill], [Skill], [Skill]  
**[Skill Category 3]:** [Skill], [Skill], [Skill], [Skill], [Skill]  
**[Skill Category 4]:** [Skill], [Skill], [Skill], [Skill], [Skill]

---

## KEY ACHIEVEMENTS

[Optional section - include 3-4 standout accomplishments that span your career or are most relevant to target role]

- **[Achievement Title/Category]:** [Detailed accomplishment with context and quantifiable impact]
- **[Achievement Title/Category]:** [Detailed accomplishment with context and quantifiable impact]
- **[Achievement Title/Category]:** [Detailed accomplishment with context and quantifiable impact]

---

## PROFESSIONAL EXPERIENCE

**[Job Title]** | [Company Name], [City, State]  
[Month Year] – [Month Year or "Present"]

[Optional: 1-sentence company description if not well-known]

- [Achievement-focused bullet with quantifiable result]
- [Achievement-focused bullet with quantifiable result]
- [Achievement-focused bullet with quantifiable result]
- [Achievement-focused bullet with quantifiable result]

**Key Skills Utilized:** [Skill 1], [Skill 2], [Skill 3], [Skill 4]

**[Job Title]** | [Company Name], [City, State]  
[Month Year] – [Month Year]

- [Achievement-focused bullet with quantifiable result]
- [Achievement-focused bullet with quantifiable result]
- [Achievement-focused bullet with quantifiable result]

**Key Skills Utilized:** [Skill 1], [Skill 2], [Skill 3], [Skill 4]

**[Job Title]** | [Company Name], [City, State]  
[Month Year] – [Month Year]

- [Achievement-focused bullet with quantifiable result]
- [Achievement-focused bullet with quantifiable result]
- [Achievement-focused bullet with quantifiable result]

**Key Skills Utilized:** [Skill 1], [Skill 2], [Skill 3]

**[Earlier Role]** | [Company Name], [City, State]  
[Month Year] – [Month Year]

- [Brief summary of role and key achievement]

---

## EDUCATION

**[Degree], [Major]** | [University Name], [City, State]  
[Month Year] | [Honors if applicable]

[For recent graduates, include:]
- **Relevant Coursework:** [Course 1], [Course 2], [Course 3], [Course 4]
- **Academic Achievements:** [Dean's List, GPA, Honors, Awards]

---

## CERTIFICATIONS & PROFESSIONAL DEVELOPMENT

**[Certification Name]** | [Issuing Organization]  
[Credential ID if applicable] | [Month Year] – [Expiration if applicable]

**[Certification Name]** | [Issuing Organization]  
[Credential ID if applicable] | [Month Year] – [Expiration if applicable]

**[Training/Course]** | [Provider] | [Year]

---

## TECHNICAL PROFICIENCIES (if applicable)

**[Category]:** [Tool/Technology] (Proficiency level), [Tool/Technology] (Proficiency level), etc.  
**[Category]:** [Tool/Technology], [Tool/Technology], [Tool/Technology]  
**[Category]:** [Tool/Technology], [Tool/Technology], [Tool/Technology]

---

## ADDITIONAL (Optional - include sections relevant to your candidacy)

**Projects:**
- **[Project Name]:** [Technologies] | [Link] - [Impact or outcome with metrics]

**Publications:**
- [Author(s)]. (Year). "[Title]." *Publication Name*. [Link if available]

**Speaking Engagements:**
- **[Event/Conference Name]:** "[Presentation Title]" | [Location] | [Month Year]

**Professional Affiliations:**
- [Organization Name] | [Role/Membership Type] | [Years]

**Awards & Recognition:**
- **[Award Name]** | [Awarding Organization] | [Year] - [Brief context]

**Volunteer Leadership:**
- **[Role]** | [Organization] | [Dates] - [Achievement or impact]

**Languages:**
- [Language]: [Proficiency level (Native, Fluent, Professional Working, etc.)]

**Patents:**
- [Patent Title] | [Patent Number] | [Year]

---

**USAGE NOTES:**

1. **Purpose:** Combines skills-based approach with traditional chronological format
2. **Best Use:** Mid-career professionals who want to emphasize both skills and progression
3. **Skills Section:** Place early to immediately show relevant competencies
4. **Key Achievements:** Optional but powerful - highlights career-defining moments
5. **Experience Section:** Blend of chronological history with skill emphasis
6. **"Key Skills Utilized":** Helps connect experience to competencies for ATS and readers
7. **Customization:** Adjust skills and achievements to match target role
8. **Length:** Usually 2 pages for mid-career professionals
9. **Balance:** Ensure skills section doesn't overshadow concrete work experience
10. **Flexibility:** Can adjust emphasis - more functional or more chronological based on needs
11. **Delete:** Remove "Optional" sections and "Key Skills Utilized" lines if not needed

**WHEN TO USE EACH FORMAT:**

- **More Chronological:** Clear career progression in same field
- **More Skills-Based:** Career transition or diverse experience
- **This Combination:** Want to show both transferable skills AND consistent career growth


--- resume-builder/assets/template-entry-level.md ---
# ENTRY-LEVEL / RECENT GRADUATE RESUME TEMPLATE

Use this template for new graduates and early-career professionals (0-2 years experience).
Best for: Recent graduates, interns transitioning to full-time, entry-level job seekers

---

**[YOUR FULL NAME]**
[City, State] | [Phone] | [Email] | [LinkedIn URL] | [Portfolio/GitHub if relevant]

---

## EDUCATION

**[Degree], [Major]** | [University Name], [City, State]  
[Month Year] | GPA: [X.XX if 3.5+]

**Academic Honors:** [Dean's List, Cum Laude, Scholarships, Awards]

**Relevant Coursework:** [Course 1], [Course 2], [Course 3], [Course 4], [Course 5]

[For STEM/Technical fields:]
**Senior Capstone/Thesis:** "[Project Title]" - [Brief description of project and outcomes]

---

## EXPERIENCE

**[Internship Title or Job Title]** | [Company Name], [City, State]  
[Month Year] – [Month Year]

- [Achievement-focused bullet demonstrating impact, even in entry-level role]
- [Achievement-focused bullet showing skills developed and results achieved]
- [Achievement-focused bullet highlighting collaboration or initiative]
- [Achievement-focused bullet with quantifiable outcome when possible]

**[Internship Title or Job Title]** | [Company Name], [City, State]  
[Month Year] – [Month Year]

- [Achievement-focused bullet demonstrating impact]
- [Achievement-focused bullet showing skills developed]
- [Achievement-focused bullet highlighting key contribution]

**[Part-time or Student Job]** | [Company Name], [City, State]  
[Month Year] – [Month Year]

- [Achievement-focused bullet emphasizing transferable skills]
- [Achievement-focused bullet showing responsibility or growth]

---

## PROJECTS

**[Project Name]** | [Course Name or Personal Project]  
[Month Year] | [GitHub/Portfolio Link if available]

- [Description of project scope and your specific role]
- [Technologies/methodologies used and skills demonstrated]
- [Outcome or results - quantify if possible]

**[Project Name]** | [Course Name or Personal Project]  
[Month Year] | [Link if available]

- [Description emphasizing problem solved or value created]
- [Technical or professional skills applied]
- [Results or learning outcomes]

---

## TECHNICAL SKILLS

**[Category]:** [Skill 1], [Skill 2], [Skill 3], [Skill 4]  
**[Category]:** [Tool 1], [Tool 2], [Tool 3], [Tool 4]  
**[Category]:** [Framework 1], [Framework 2], [Framework 3]

---

## LEADERSHIP & ACTIVITIES

**[Leadership Position]** | [Organization Name]  
[Month Year] – [Month Year]

- [Achievement showing leadership, impact, or organizational skills]
- [Quantifiable result if possible]

**[Activity or Club]** | [Organization Name]  
[Month Year] – [Month Year]

- [Relevant skills or experiences gained]

---

## CERTIFICATIONS (if applicable)

**[Certification Name]** | [Issuing Organization] | [Month Year]

---

## ADDITIONAL (Optional)

**Volunteer Experience:**
- **[Role]** | [Organization] | [Dates] - [Achievement or contribution]

**Awards & Honors:**
- **[Award Name]** | [Organization] | [Year]

**Languages:**
- [Language]: [Proficiency level]

---

**USAGE NOTES:**

1. **Education First:** Your strongest qualification as a new graduate
2. **GPA:** Include if 3.5+ or above major average
3. **Projects:** Demonstrate hands-on skills - very important!
4. **Internships:** Treat as full professional experience
5. **One Page:** Essential for entry-level
6. **Quantify:** Include numbers even in entry-level roles
7. **Transferable Skills:** Connect all experience to career goals

**TRANSFORMING DUTIES INTO ACHIEVEMENTS:**

❌ **Weak:** "Worked as server in restaurant"
✅ **Strong:** "Served 50+ customers daily in high-volume restaurant, maintaining 95% positive feedback rating"

❌ **Weak:** "Member of Marketing Club"
✅ **Strong:** "Organized campus marketing symposium for 200+ attendees, managing $5K budget"

**BULLET FORMULA:** Action Verb + Task + Context + Result


--- resume-builder/assets/template-functional.md ---
# FUNCTIONAL RESUME TEMPLATE

Use this template for career changers, employment gaps, or emphasizing transferable skills over work history.
Best for: Career transitions, returning to workforce, diverse experience in different fields

---

**[YOUR FULL NAME]**
[City, State] | [Phone] | [Email] | [LinkedIn URL] | [Portfolio/Website if relevant]

---

## PROFESSIONAL SUMMARY
[3-4 sentences clearly stating your career transition, transferable skills, and value proposition. Essential for this format to explain your story. Include target role and relevant keywords.]

---

## CORE COMPETENCIES

**[Skill Category 1]**
- [Specific skill or competency relevant to target role]
- [Specific skill or competency relevant to target role]
- [Specific skill or competency relevant to target role]

**[Skill Category 2]**
- [Specific skill or competency relevant to target role]
- [Specific skill or competency relevant to target role]
- [Specific skill or competency relevant to target role]

**[Skill Category 3]**
- [Specific skill or competency relevant to target role]
- [Specific skill or competency relevant to target role]
- [Specific skill or competency relevant to target role]

---

## RELEVANT EXPERIENCE

### [Functional Skill Area 1 - e.g., "Project Management"]

**[Achievement demonstrating this skill]**
[Context] | [Timeframe if relevant]
- [Detailed accomplishment with metrics showing impact]
- [Detailed accomplishment with metrics showing impact]

**[Achievement demonstrating this skill]**
[Context] | [Timeframe if relevant]
- [Detailed accomplishment with metrics showing impact]

### [Functional Skill Area 2 - e.g., "Data Analysis"]

**[Achievement demonstrating this skill]**
[Context] | [Timeframe if relevant]
- [Detailed accomplishment with metrics showing impact]
- [Detailed accomplishment with metrics showing impact]

**[Achievement demonstrating this skill]**
[Context] | [Timeframe if relevant]
- [Detailed accomplishment with metrics showing impact]

### [Functional Skill Area 3 - e.g., "Team Leadership"]

**[Achievement demonstrating this skill]**
[Context] | [Timeframe if relevant]
- [Detailed accomplishment with metrics showing impact]
- [Detailed accomplishment with metrics showing impact]

---

## EMPLOYMENT HISTORY

**[Job Title]** | [Company Name], [City, State] | [Month Year] – [Month Year or "Present"]

**[Job Title]** | [Company Name], [City, State] | [Month Year] – [Month Year]

**[Job Title]** | [Company Name], [City, State] | [Month Year] – [Month Year]

[Note: This section is intentionally brief. Dates and titles are listed but achievements are highlighted in the Relevant Experience section above.]

---

## TECHNICAL SKILLS

**[Category]:** [Skill 1], [Skill 2], [Skill 3], [Skill 4], [Skill 5]  
**[Category]:** [Skill 1], [Skill 2], [Skill 3], [Skill 4], [Skill 5]  
**[Category]:** [Skill 1], [Skill 2], [Skill 3], [Skill 4], [Skill 5]

---

## EDUCATION & TRAINING

**[Degree], [Major]** | [University Name], [City, State]  
[Month Year] | [GPA if 3.5+ and relevant]

**[Recent Certification or Bootcamp]** | [Institution]  
[Month Year] | [Relevant details or specialization]

**[Relevant Course or Training]** | [Provider]  
[Month Year]

---

## PROJECTS (Optional but recommended for career changers)

**[Project Name]**
[Technologies/skills used] | [Link if available]
- [Description of project and its impact or learning outcomes]
- [Quantifiable results if applicable]

**[Project Name]**
[Technologies/skills used] | [Link if available]
- [Description of project and its impact or learning outcomes]

---

## ADDITIONAL (Optional)

**Volunteer Experience:**
- **[Role]** | [Organization] | [Dates] - [Achievement-focused description showing relevant skills]

**Languages:**
- [Language]: [Proficiency Level]

**Professional Affiliations:**
- [Organization membership relevant to target field]

---

**USAGE NOTES:**

1. **Purpose:** This format de-emphasizes chronological work history to highlight transferable skills
2. **Professional Summary:** Critical in this format - must clearly explain career transition
3. **Skill Categories:** Choose 3-4 categories most relevant to target role
4. **Relevant Experience:** Draw examples from any context (work, volunteer, projects, education)
5. **Employment History:** Keep brief - just titles, companies, and dates
6. **Recent Training:** Emphasize to show commitment to new field
7. **Projects:** Highly recommended to demonstrate current skills
8. **Customization:** Adjust skill categories and examples for each application
9. **Limitations:** Some recruiters prefer chronological; use when career change is significant
10. **Delete:** Remove sections that don't strengthen your narrative


--- skill-debugging-assistant/SKILL.md ---
---
name: skill-debugging-assistant
description: Debug, diagnose, and troubleshoot skill issues including trigger failures, parameter problems, prompt conflicts, and SKILL.md structural issues. Use when skills don't activate as expected, trigger incorrectly, produce unexpected behavior, conflict with system instructions, or fail packaging validation. Analyzes YAML frontmatter, descriptions, progressive disclosure, token budget, absolute statements, and reference file organization. For skill creators reviewing, validating, or fixing skill problems.
---

# Skill Debugging Assistant

## Overview

This skill helps diagnose why skills aren't triggering or performing as expected. It systematically analyzes trigger patterns, parameter issues, prompt conflicts, and structural problems to identify root causes and recommend fixes.

## When to Use This Skill

Use this skill when encountering any of these issues:

- Skill doesn't trigger when expected
- Skill triggers incorrectly or at wrong times
- Skill behavior doesn't match description
- Conflicts between skill instructions and system prompts
- Unclear when to load references vs. include in SKILL.md
- Validation errors during packaging
- Skill works inconsistently across similar queries

## Diagnostic Workflow

Follow this decision tree to diagnose skill issues:

### 1. Identify the Problem Type

**Skill not triggering?** → Go to "Trigger Failure Diagnostics"

**Skill triggering incorrectly?** → Go to "False Positive Diagnostics"

**Skill behavior unexpected?** → Go to "Instruction Conflict Diagnostics"

**Packaging/validation errors?** → Go to "Structure Validation"

**General review needed?** → Go to "Comprehensive Audit"

### 2. Trigger Failure Diagnostics

When a skill should trigger but doesn't, analyze in this order:

**Step 1: Analyze the description field**
- Read the skill's frontmatter `description`
- Check if description mentions the user's query terms or conceptual triggers
- Verify description includes WHEN to use the skill, not just WHAT it does
- Confirm description is specific enough to differentiate from other skills

**Step 2: Check description quality**
- Does it include key terms the user would naturally use?
- Does it specify triggers (file types, tasks, scenarios)?
- Is it comprehensive enough for selection among 100+ skills?
- Are the trigger scenarios clear and unambiguous?

**Step 3: Review competing skills**
- Identify other skills with overlapping descriptions
- Determine if another skill's description better matches the query
- Check if trigger patterns are too similar between skills

**Step 4: Test edge cases**
- Would the skill trigger for paraphrased versions of the query?
- Does it cover related terminology and synonyms?
- Are there implicit assumptions about when it should trigger?

**Common fixes:**
- Add specific trigger terms to description
- Include file type indicators (.docx, .pdf, .json)
- Specify task types (create, edit, analyze, debug)
- Add domain indicators (finance, legal, technical)
- Include synonym terms users might naturally use

### 3. False Positive Diagnostics

When a skill triggers when it shouldn't:

**Step 1: Check description over-breadth**
- Is the description too general?
- Does it use broad terms that match many queries?
- Are there missing qualifiers or constraints?

**Step 2: Review instruction conflicts**
- Do instructions apply too broadly within SKILL.md?
- Are there "always" or "never" statements that override context?
- Does the skill assume it should handle something beyond its scope?

**Common fixes:**
- Narrow description scope with specific qualifiers
- Add exclusion indicators (e.g., "not for X")
- Move broad utility functions to scripts rather than main workflow
- Add conditional logic: "Only when..." or "If and only if..."

### 4. Instruction Conflict Diagnostics

When skill behavior contradicts expected results:

**Step 1: Read SKILL.md completely**
- Check for absolute statements (always, never, must, required)
- Identify instructions that might conflict with system prompt
- Look for contradictory instructions within the skill

**Step 2: Analyze instruction priority**
- Are there competing instructions without clear precedence?
- Do examples contradict written rules?
- Is the desired behavior stated clearly vs. implied?

**Step 3: Check progressive disclosure structure**
- Is critical information buried in references that weren't loaded?
- Should certain instructions be in SKILL.md instead of references?
- Are references clearly indicated when they're needed?

**Common fixes:**
- Replace absolutes with conditionals
- Add explicit precedence rules
- Move critical instructions from references to SKILL.md
- Clarify when to load each reference file
- Use "Prefer X, unless Y" instead of "Always X"

### 5. Structure Validation

Run automated and manual checks:

**Automated validation:**
```bash
python3 scripts/validate_skill.py path/to/skill-folder
```

**Manual checks:**
- YAML frontmatter properly formatted (name and description present)
- Name follows kebab-case convention
- Description is comprehensive (>50 chars) and specific
- SKILL.md under 500 lines (split to references if needed)
- References clearly indicated in SKILL.md when needed
- No extraneous files (README.md, CHANGELOG.md, etc.)
- Assets/scripts tested and functional

**Common structural issues:**
- Missing or malformed YAML frontmatter
- Description too vague or too brief
- SKILL.md exceeds token budget (>500 lines)
- References not mentioned in SKILL.md
- Unused example files not deleted
- Scripts with syntax errors or missing dependencies

### 6. Comprehensive Audit

For general skill review or quality improvement:

**Trigger analysis:**
1. List 5-10 queries that should trigger this skill
2. For each query, verify the description contains relevant terms
3. Test paraphrased versions of each query
4. Identify gaps in trigger coverage

**Instruction clarity:**
1. Read SKILL.md start to finish
2. Flag any ambiguous or conflicting statements
3. Verify examples align with instructions
4. Check if workflow steps are clear and sequential

**Progressive disclosure:**
1. Ensure SKILL.md contains only essential procedural knowledge
2. Verify detailed reference material is in separate files
3. Confirm references are clearly indicated when needed
4. Check that SKILL.md describes when to load each reference

**Quality checklist:**
- [ ] Description includes specific trigger terms and scenarios
- [ ] SKILL.md uses imperative/infinitive form throughout
- [ ] No conflicting instructions or absolute statements without qualifiers
- [ ] Examples provided for non-obvious operations
- [ ] References clearly indicated and purposefully separated
- [ ] Scripts tested and functional
- [ ] Token budget respected (<500 lines in SKILL.md)
- [ ] No extraneous files included

## Quick Diagnostics Checklist

For rapid troubleshooting, check these common issues first:

**Trigger failures (skill not activating):**
- [ ] Description mentions user's query terms
- [ ] Description includes "when to use" indicators
- [ ] Description differentiates from similar skills
- [ ] Key terms are specific, not generic

**False positives (skill triggers incorrectly):**
- [ ] Description isn't too broad or generic
- [ ] No absolute statements without context limits
- [ ] Scope clearly defined with boundaries

**Behavior issues (skill does unexpected things):**
- [ ] No conflicting "always/never" statements
- [ ] Critical instructions in SKILL.md, not buried in references
- [ ] Examples align with stated rules
- [ ] Conditional logic uses "prefer" vs "always"

**Validation errors:**
- [ ] YAML frontmatter properly formatted
- [ ] Name uses kebab-case
- [ ] Description >50 characters
- [ ] No extraneous documentation files

## Deep Analysis Methods

### Description Analysis Template

For any skill with trigger issues, analyze the description systematically:

```markdown
**Current description:**
[paste description here]

**Analysis:**
1. Specificity: Does it include concrete trigger terms?
2. Differentiation: How does it differ from similar skills?
3. Completeness: Does it mention when/how to use it?
4. Key terms: List the main terms that would trigger selection

**Test queries:**
[List 5 queries that should trigger this skill]
[For each, note if description contains matching terms]

**Recommended improvements:**
[Specific additions or changes to description]
```

### Instruction Conflict Analysis

For skills with behavioral issues:

1. Extract all imperative statements from SKILL.md
2. Flag statements using: always, never, must, required, CRITICAL, NEVER
3. Check each flagged statement for potential conflicts
4. Identify statements that could override user intent
5. Recommend conditional rephrasing

### Token Budget Analysis

For skills approaching context limits:

1. Count lines in SKILL.md (target: <500 lines)
2. Identify sections >100 lines that could move to references
3. Check for repetitive examples or verbose explanations
4. Verify references are actually being used (not duplicated in SKILL.md)
5. Recommend splits: what stays in SKILL.md vs. what moves to references

## Resources

### references/common-issues.md

Detailed examples of common skill problems with before/after fixes. Load this when debugging complex or recurring issues.

### scripts/validate_skill.py

Automated validation script that checks:
- YAML frontmatter format and completeness
- File structure and naming conventions
- Description quality metrics
- Common structural problems

Run before packaging any skill to catch issues early.


--- skill-debugging-assistant/scripts/validate_skill.py ---
#!/usr/bin/env python3
"""
Comprehensive skill validation script

Checks for common issues including:
- YAML frontmatter format and completeness
- Description quality and specificity
- File structure and naming conventions
- Token budget (SKILL.md line count)
- Absolute statements that might cause conflicts
- Reference file mentions in SKILL.md
"""

import sys
import os
import re
from pathlib import Path
from typing import List, Tuple

class SkillValidator:
    def __init__(self, skill_path: str):
        self.skill_path = Path(skill_path)
        self.errors = []
        self.warnings = []
        self.info = []
        
    def validate(self) -> bool:
        """Run all validation checks"""
        self.check_structure()
        self.check_frontmatter()
        self.check_description_quality()
        self.check_token_budget()
        self.check_absolute_statements()
        self.check_reference_mentions()
        self.check_extraneous_files()
        
        return len(self.errors) == 0
    
    def check_structure(self):
        """Check basic skill directory structure"""
        skill_md = self.skill_path / 'SKILL.md'
        if not skill_md.exists():
            self.errors.append("❌ SKILL.md not found")
            return
        
        self.info.append(f"✓ SKILL.md found")
        
        # Check for optional directories
        if (self.skill_path / 'scripts').exists():
            self.info.append(f"✓ scripts/ directory present")
        if (self.skill_path / 'references').exists():
            self.info.append(f"✓ references/ directory present")
        if (self.skill_path / 'assets').exists():
            self.info.append(f"✓ assets/ directory present")
    
    def check_frontmatter(self):
        """Validate YAML frontmatter"""
        skill_md = self.skill_path / 'SKILL.md'
        if not skill_md.exists():
            return
        
        content = skill_md.read_text()
        
        if not content.startswith('---'):
            self.errors.append("❌ No YAML frontmatter found (must start with ---)")
            return
        
        # Extract frontmatter
        match = re.match(r'^---\n(.*?)\n---', content, re.DOTALL)
        if not match:
            self.errors.append("❌ Invalid frontmatter format (must end with ---)")
            return
        
        frontmatter = match.group(1)
        
        # Check required fields
        if 'name:' not in frontmatter:
            self.errors.append("❌ Missing 'name' field in frontmatter")
        else:
            name_match = re.search(r'name:\s*(.+)', frontmatter)
            if name_match:
                name = name_match.group(1).strip()
                
                # Check naming convention (kebab-case)
                if not re.match(r'^[a-z0-9-]+$', name):
                    self.errors.append(f"❌ Name '{name}' must be kebab-case (lowercase, digits, hyphens only)")
                elif name.startswith('-') or name.endswith('-'):
                    self.errors.append(f"❌ Name '{name}' cannot start or end with hyphen")
                elif '--' in name:
                    self.errors.append(f"❌ Name '{name}' cannot contain consecutive hyphens")
                else:
                    self.info.append(f"✓ Name '{name}' follows conventions")
        
        if 'description:' not in frontmatter:
            self.errors.append("❌ Missing 'description' field in frontmatter")
        else:
            desc_match = re.search(r'description:\s*(.+)', frontmatter)
            if desc_match:
                description = desc_match.group(1).strip()
                
                # Check for angle brackets
                if '<' in description or '>' in description:
                    self.errors.append("❌ Description contains angle brackets (< or >)")
                
                # Check for placeholder text
                if 'TODO' in description or '[' in description:
                    self.errors.append("❌ Description contains TODO or placeholder text")
    
    def check_description_quality(self):
        """Check description quality and specificity"""
        skill_md = self.skill_path / 'SKILL.md'
        if not skill_md.exists():
            return
        
        content = skill_md.read_text()
        desc_match = re.search(r'description:\s*(.+)', content)
        
        if not desc_match:
            return
        
        description = desc_match.group(1).strip()
        
        # Check length
        if len(description) < 50:
            self.warnings.append(f"⚠️  Description is short ({len(description)} chars). Consider adding more detail.")
        elif len(description) > 500:
            self.warnings.append(f"⚠️  Description is long ({len(description)} chars). Consider being more concise.")
        else:
            self.info.append(f"✓ Description length appropriate ({len(description)} chars)")
        
        # Check for "when to use" indicators
        when_indicators = ['when', 'use for', 'use when', 'for queries', 'trigger', 'applies']
        has_when_indicator = any(indicator in description.lower() for indicator in when_indicators)
        
        if not has_when_indicator:
            self.warnings.append("⚠️  Description might lack 'when to use' indicators (when, use for, etc.)")
        
        # Check for overly generic terms
        generic_terms = ['helps with', 'assists', 'various', 'multiple', 'different']
        has_generic = any(term in description.lower() for term in generic_terms)
        
        if has_generic:
            self.warnings.append("⚠️  Description contains generic terms (helps with, assists, various). Be more specific.")
        
        # Check for specific triggers (file extensions, actions, etc.)
        has_specifics = bool(re.search(r'\.(docx|pdf|csv|xlsx|json|py|js|html)', description.lower()))
        has_specifics |= bool(re.search(r'\b(create|edit|analyze|debug|review|generate|extract|convert)\b', description.lower()))
        
        if not has_specifics:
            self.warnings.append("⚠️  Consider adding specific triggers (file types, action verbs, task types)")
    
    def check_token_budget(self):
        """Check SKILL.md size for token budget"""
        skill_md = self.skill_path / 'SKILL.md'
        if not skill_md.exists():
            return
        
        content = skill_md.read_text()
        lines = content.split('\n')
        line_count = len(lines)
        
        if line_count > 500:
            self.warnings.append(f"⚠️  SKILL.md has {line_count} lines (>500). Consider moving content to references/")
        elif line_count > 300:
            self.info.append(f"✓ SKILL.md has {line_count} lines (approaching limit, consider references)")
        else:
            self.info.append(f"✓ SKILL.md has {line_count} lines (well within budget)")
    
    def check_absolute_statements(self):
        """Check for absolute statements that might cause conflicts"""
        skill_md = self.skill_path / 'SKILL.md'
        if not skill_md.exists():
            return
        
        content = skill_md.read_text()
        
        # Pattern for absolute statements
        absolute_patterns = [
            (r'\bALWAYS\b', 'ALWAYS'),
            (r'\bNEVER\b', 'NEVER'),
            (r'\bMUST\b', 'MUST'),
            (r'\bREQUIRED\b', 'REQUIRED'),
            (r'\bCRITICAL\b', 'CRITICAL')
        ]
        
        found_absolutes = []
        for pattern, word in absolute_patterns:
            matches = re.findall(pattern, content)
            if matches:
                found_absolutes.append((word, len(matches)))
        
        if found_absolutes:
            warning_msg = "⚠️  Found absolute statements that might override user intent:"
            for word, count in found_absolutes:
                warning_msg += f"\n    - {word}: {count} occurrence(s)"
            warning_msg += "\n    Consider using: 'prefer', 'avoid', 'typically', 'by default' with conditional clauses"
            self.warnings.append(warning_msg)
    
    def check_reference_mentions(self):
        """Check if reference files are mentioned in SKILL.md"""
        skill_md = self.skill_path / 'SKILL.md'
        references_dir = self.skill_path / 'references'
        
        if not skill_md.exists() or not references_dir.exists():
            return
        
        skill_content = skill_md.read_text()
        
        # Find all reference files
        reference_files = [f.name for f in references_dir.glob('*.md')]
        
        if not reference_files:
            return
        
        # Check if each reference is mentioned in SKILL.md
        unmentioned = []
        for ref_file in reference_files:
            if ref_file not in skill_content:
                unmentioned.append(ref_file)
        
        if unmentioned:
            self.warnings.append(f"⚠️  Reference files not mentioned in SKILL.md: {', '.join(unmentioned)}")
            self.warnings.append("    Consider adding descriptions of when to load these files")
    
    def check_extraneous_files(self):
        """Check for unnecessary documentation files"""
        unwanted_files = ['README.md', 'CHANGELOG.md', 'INSTALLATION.md', 
                         'QUICK_REFERENCE.md', 'CONTRIBUTING.md']
        
        found_unwanted = []
        for filename in unwanted_files:
            if (self.skill_path / filename).exists():
                found_unwanted.append(filename)
        
        if found_unwanted:
            self.warnings.append(f"⚠️  Extraneous files found: {', '.join(found_unwanted)}")
            self.warnings.append("    Skills should only contain SKILL.md and bundled resources")
    
    def print_results(self):
        """Print validation results"""
        print(f"\n{'='*60}")
        print(f"Skill Validation: {self.skill_path.name}")
        print(f"{'='*60}\n")
        
        if self.errors:
            print("ERRORS:")
            for error in self.errors:
                print(f"  {error}")
            print()
        
        if self.warnings:
            print("WARNINGS:")
            for warning in self.warnings:
                print(f"  {warning}")
            print()
        
        if self.info:
            print("INFO:")
            for info in self.info:
                print(f"  {info}")
            print()
        
        print(f"{'='*60}")
        if self.errors:
            print("❌ VALIDATION FAILED - Fix errors before packaging")
        elif self.warnings:
            print("⚠️  VALIDATION PASSED WITH WARNINGS - Review warnings for improvements")
        else:
            print("✅ VALIDATION PASSED - Skill is ready to package")
        print(f"{'='*60}\n")

def main():
    if len(sys.argv) != 2:
        print("Usage: python validate_skill.py <skill_directory>")
        sys.exit(1)
    
    skill_path = sys.argv[1]
    
    if not os.path.isdir(skill_path):
        print(f"Error: '{skill_path}' is not a directory")
        sys.exit(1)
    
    validator = SkillValidator(skill_path)
    is_valid = validator.validate()
    validator.print_results()
    
    sys.exit(0 if is_valid else 1)

if __name__ == "__main__":
    main()


--- skill-dependency-mapper/SKILL.md ---
---
name: skill-dependency-mapper
description: Analyzes skill ecosystem to visualize dependencies, identify workflow bottlenecks, and recommend optimal skill stacks. Use when asked about skill combinations, workflow optimization, bottleneck identification, or which skills work together. Triggers include phrases like "which skills work together", "skill dependencies", "workflow bottlenecks", "optimal skill stack", or "recommend skills for".
---

# Skill Dependency Mapper

Analyzes the skill ecosystem to understand relationships, identify inefficiencies, and optimize workflows.

## When to Use This Skill

Use this skill when users ask about:
- Which skills commonly work together
- Skill combinations that create bottlenecks
- Optimal skill "stacks" for specific tasks
- Workflow optimization across skills
- Understanding skill dependencies
- Token budget concerns with multiple skills

## Core Workflow

### 1. Scan and Analyze Skills

Run the analyzer script to extract metadata from all available skills:

```bash
cd /home/claude/skill-dependency-mapper
python scripts/analyze_skills.py > /tmp/skill_analysis.txt
```

The script extracts:
- Tool dependencies (bash_tool, web_search, etc.)
- File format associations (docx, pdf, xlsx, etc.)
- Domain overlap (document, research, coding, etc.)
- Complexity metrics (size, tool count, bundled resources)

### 2. Detect Bottlenecks

For bottleneck analysis, first capture skill data as JSON:

```python
import json
from scripts.analyze_skills import SkillAnalyzer

analyzer = SkillAnalyzer()
analyzer.scan_skills()

# Save for bottleneck detection
with open('/tmp/skill_data.json', 'w') as f:
    json.dump(analyzer.skills, f, default=list)
```

Then run bottleneck detection:

```bash
python scripts/detect_bottlenecks.py /tmp/skill_data.json > /tmp/bottlenecks.txt
```

### 3. Generate Dependency Map

Create a text-based dependency visualization:

```python
from scripts.analyze_skills import SkillAnalyzer

analyzer = SkillAnalyzer()
analyzer.scan_skills()

# Generate dependency mapping
dependencies = analyzer.find_dependencies()

# Format as markdown
output = ["# Skill Dependency Map\n"]
for skill, related in sorted(dependencies.items()):
    if related:
        output.append(f"## {skill}\n")
        output.append("Works well with:\n")
        for related_skill in sorted(related)[:8]:
            # Show why they're related
            skill_a = analyzer.skills[skill]
            skill_b = analyzer.skills[related_skill]
            
            shared = []
            if skill_a['tools'] & skill_b['tools']:
                shared.append(f"tools: {', '.join(skill_a['tools'] & skill_b['tools'])}")
            if skill_a['formats'] & skill_b['formats']:
                shared.append(f"formats: {', '.join(skill_a['formats'] & skill_b['formats'])}")
            if skill_a['domains'] & skill_b['domains']:
                shared.append(f"domains: {', '.join(skill_a['domains'] & skill_b['domains'])}")
            
            reason = " | ".join(shared) if shared else "complementary"
            output.append(f"- **{related_skill}** ({reason})\n")
        output.append("\n")

print('\n'.join(output))
```

### 4. Recommend Skill Stacks

For task-specific recommendations:

```python
from scripts.analyze_skills import SkillAnalyzer

analyzer = SkillAnalyzer()
analyzer.scan_skills()

# Get recommended stacks
stacks = analyzer.recommend_stacks()

# Format output
output = ["# Recommended Skill Stacks\n"]
for stack in stacks:
    output.append(f"## {stack['name']}\n")
    output.append(f"**Use case**: {stack['use_case']}\n\n")
    output.append("**Skills**:\n")
    for skill in sorted(stack['skills']):
        skill_data = analyzer.skills[skill]
        output.append(f"- **{skill}** - {skill_data['description'][:80]}...\n")
    output.append("\n")

print('\n'.join(output))
```

### 5. Custom Analysis

For specific queries, filter and analyze programmatically:

```python
from scripts.analyze_skills import SkillAnalyzer

analyzer = SkillAnalyzer()
analyzer.scan_skills()

# Example: Find all skills that use web_search
web_skills = [
    name for name, data in analyzer.skills.items()
    if 'web_search' in data['tools']
]

# Example: Find skills by domain
financial_skills = [
    name for name, data in analyzer.skills.items()
    if 'financial' in data['domains']
]

# Example: Find lightweight skills
lightweight = [
    name for name, data in analyzer.skills.items()
    if data['complexity_score'] < 5 and data['size'] < 2000
]
```

## Output Format

Generate concise markdown reports with:

1. **Executive summary** - Key findings in 2-3 sentences
2. **Dependency maps** - Skills grouped by relationship strength
3. **Bottleneck analysis** - Identified issues with impact assessment
4. **Recommendations** - Actionable optimization suggestions
5. **Skill stacks** - Pre-configured combinations for common workflows

Keep output token-efficient:
- Use bullet points for lists
- Bold key skill names
- Include only actionable insights
- Omit verbose explanations

## Interpreting Results

### Dependency Strength

- **Strong**: Share 3+ characteristics (tools, formats, domains)
- **Medium**: Share 2 characteristics
- **Weak**: Share 1 characteristic

### Bottleneck Severity

- **High**: >10k combined token size or >5 tool calls
- **Medium**: 5-10k tokens or 3-5 tool calls
- **Low**: <5k tokens or <3 tool calls

### Stack Optimization

Optimal stacks minimize:
- Total token budget (<15k characters)
- Tool call diversity (<4 different tools)
- Format conversion steps (<2 conversions)

## Advanced Usage

### Consulting Known Patterns

For established patterns and anti-patterns, reference:

```bash
view /home/claude/skill-dependency-mapper/references/known_patterns.md
```

Use this when:
- User asks about best practices
- Workflow seems suboptimal
- Need to explain why certain combinations work well

### Custom Bottleneck Detection

Modify detection thresholds in `detect_bottlenecks.py`:

```python
detector.detect_high_tool_usage(threshold=4)  # Adjust tool count threshold
detector.detect_large_references(size_threshold=8000)  # Adjust size threshold
detector.detect_token_budget_risks(combined_threshold=12000)  # Adjust combined size
```

### Filtering by Skill Type

Analyze only specific skill types:

```python
analyzer = SkillAnalyzer()
analyzer.scan_skills()

# User skills only
user_skills = {
    name: data for name, data in analyzer.skills.items()
    if data['type'] == 'user'
}

# Public skills only
public_skills = {
    name: data for name, data in analyzer.skills.items()
    if data['type'] == 'public'
}
```

## Common Use Cases

### "Which skills work together for data analysis?"

1. Run analyzer to find spreadsheet/data skills
2. Filter by shared domains and tools
3. Generate dependency map for data domain
4. Recommend optimized stack

### "What's causing slowdowns in my document workflow?"

1. Run bottleneck detection
2. Focus on document-related skills
3. Identify high tool usage or token budget issues
4. Suggest sequential processing or skill consolidation

### "Recommend skills for financial reporting"

1. Filter skills by 'financial' domain
2. Find complementary skills (spreadsheet, presentation)
3. Assess token budget feasibility
4. Output recommended stack with rationale

### "Show me skill dependencies visually"

1. Generate full dependency map
2. Group by relationship strength
3. Highlight clusters of related skills
4. Format as hierarchical markdown sections

## Limitations

- Dependency detection is heuristic-based (not ground truth)
- Cannot analyze skills not in /mnt/skills
- Token estimates are approximate (actual may vary)
- Bottleneck severity depends on specific usage patterns
- No access to actual conversation usage data

## Tips for Effective Analysis

1. **Be specific**: Filter by domain/format for targeted results
2. **Consider context**: Bottlenecks depend on user's workflow
3. **Iterate**: Run analysis, optimize, re-analyze
4. **Validate**: Test recommended stacks with real tasks
5. **Stay current**: Re-run after skill updates or additions


--- skill-dependency-mapper/scripts/analyze_skills.py ---
#!/usr/bin/env python3
"""
Skill Dependency Analyzer

Scans available skills and extracts metadata about:
- Tool dependencies (bash, web_search, etc.)
- File format associations (docx, pdf, xlsx, etc.)
- Domain overlap and relationships
- Complexity indicators (token usage, tool call frequency)
"""

import os
import re
import yaml
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Set, Tuple

class SkillAnalyzer:
    def __init__(self, skills_dir: str = "/mnt/skills"):
        self.skills_dir = Path(skills_dir)
        self.skills = {}
        self.tool_patterns = [
            'bash_tool', 'web_search', 'web_fetch', 'drive_search',
            'google_drive_search', 'google_drive_fetch', 'conversation_search',
            'recent_chats', 'view', 'create_file', 'str_replace'
        ]
        self.format_patterns = [
            'docx', 'pdf', 'pptx', 'xlsx', 'csv', 'json', 'html', 'jsx',
            'markdown', 'svg', 'png', 'jpg', 'gif', 'xml', 'yaml'
        ]
        
    def scan_skills(self) -> Dict:
        """Scan all available skills and parse their metadata."""
        for skill_type in ['public', 'user', 'examples']:
            skill_path = self.skills_dir / skill_type
            if not skill_path.exists():
                continue
                
            for skill_dir in skill_path.iterdir():
                if not skill_dir.is_dir():
                    continue
                    
                skill_md = skill_dir / "SKILL.md"
                if skill_md.exists():
                    self.parse_skill(skill_md, skill_type)
        
        return self.skills
    
    def parse_skill(self, skill_path: Path, skill_type: str):
        """Parse a SKILL.md file and extract metadata."""
        with open(skill_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Extract YAML frontmatter
        frontmatter_match = re.match(r'^---\s*\n(.*?)\n---\s*\n', content, re.DOTALL)
        if not frontmatter_match:
            return
        
        frontmatter = yaml.safe_load(frontmatter_match.group(1))
        body = content[frontmatter_match.end():]
        
        skill_name = frontmatter.get('name', skill_path.parent.name)
        
        # Analyze skill content
        self.skills[skill_name] = {
            'type': skill_type,
            'name': skill_name,
            'description': frontmatter.get('description', ''),
            'path': str(skill_path.parent),
            'tools': self.extract_tools(body),
            'formats': self.extract_formats(body),
            'domains': self.extract_domains(body, frontmatter.get('description', '')),
            'size': len(body),
            'has_scripts': (skill_path.parent / 'scripts').exists(),
            'has_references': (skill_path.parent / 'references').exists(),
            'has_assets': (skill_path.parent / 'assets').exists(),
            'complexity_score': self.calculate_complexity(body),
            'bundled_files': self.count_bundled_files(skill_path.parent)
        }
    
    def extract_tools(self, content: str) -> Set[str]:
        """Extract tool dependencies mentioned in skill content."""
        tools = set()
        for tool in self.tool_patterns:
            # Look for tool usage patterns
            if re.search(rf'\b{tool}\b', content, re.IGNORECASE):
                tools.add(tool)
        return tools
    
    def extract_formats(self, content: str) -> Set[str]:
        """Extract file format associations."""
        formats = set()
        for fmt in self.format_patterns:
            # Look for format mentions (case-insensitive, whole word)
            if re.search(rf'\b{fmt}\b', content, re.IGNORECASE):
                formats.add(fmt.lower())
        return formats
    
    def extract_domains(self, content: str, description: str) -> Set[str]:
        """Extract domain keywords (heuristic-based)."""
        domains = set()
        
        # Domain keywords to look for
        domain_keywords = {
            'document': ['document', 'writing', 'text', 'report'],
            'presentation': ['presentation', 'slide', 'deck'],
            'spreadsheet': ['spreadsheet', 'excel', 'data', 'table'],
            'pdf': ['pdf'],
            'web': ['web', 'html', 'frontend', 'website'],
            'research': ['research', 'search', 'analysis'],
            'coding': ['code', 'programming', 'script', 'development'],
            'financial': ['financial', 'finance', 'revenue', 'budget'],
            'ai': ['ai', 'llm', 'model', 'prompt'],
            'business': ['business', 'strategy', 'pitch', 'startup']
        }
        
        combined_text = (content + ' ' + description).lower()
        
        for domain, keywords in domain_keywords.items():
            if any(kw in combined_text for kw in keywords):
                domains.add(domain)
        
        return domains
    
    def calculate_complexity(self, content: str) -> int:
        """Calculate a complexity score based on content characteristics."""
        score = 0
        
        # Length contributes to complexity
        score += len(content) // 1000
        
        # Multiple steps/sections increase complexity
        score += len(re.findall(r'##', content)) * 2
        
        # Code blocks indicate complexity
        score += len(re.findall(r'```', content))
        
        # Tool calls indicate complexity
        score += len(re.findall(r'<invoke', content)) * 3
        
        return score
    
    def count_bundled_files(self, skill_dir: Path) -> Dict[str, int]:
        """Count bundled resources in scripts/, references/, assets/."""
        counts = {'scripts': 0, 'references': 0, 'assets': 0}
        
        for resource_type in counts.keys():
            resource_dir = skill_dir / resource_type
            if resource_dir.exists():
                counts[resource_type] = len(list(resource_dir.glob('*')))
        
        return counts
    
    def find_dependencies(self) -> Dict[str, List[str]]:
        """Find skills that commonly work together based on shared characteristics."""
        dependencies = defaultdict(list)
        
        skills_list = list(self.skills.values())
        
        for i, skill_a in enumerate(skills_list):
            for skill_b in skills_list[i+1:]:
                # Check for shared tools
                shared_tools = skill_a['tools'] & skill_b['tools']
                
                # Check for shared formats
                shared_formats = skill_a['formats'] & skill_b['formats']
                
                # Check for shared domains
                shared_domains = skill_a['domains'] & skill_b['domains']
                
                # If they share multiple characteristics, they likely work together
                if len(shared_tools) >= 1 or len(shared_formats) >= 2 or len(shared_domains) >= 2:
                    dependencies[skill_a['name']].append(skill_b['name'])
                    dependencies[skill_b['name']].append(skill_a['name'])
        
        return dict(dependencies)
    
    def detect_format_conflicts(self) -> List[Dict]:
        """Detect skills that handle the same format with different approaches."""
        format_handlers = defaultdict(list)
        
        for skill_name, skill_data in self.skills.items():
            for fmt in skill_data['formats']:
                format_handlers[fmt].append({
                    'name': skill_name,
                    'tools': skill_data['tools'],
                    'complexity': skill_data['complexity_score']
                })
        
        conflicts = []
        for fmt, handlers in format_handlers.items():
            if len(handlers) > 1:
                # Check if they use different tools (potential conflict)
                tools_sets = [set(h['tools']) for h in handlers]
                if len(set(map(frozenset, tools_sets))) > 1:
                    conflicts.append({
                        'format': fmt,
                        'skills': [h['name'] for h in handlers],
                        'conflict_type': 'different_approaches'
                    })
        
        return conflicts
    
    def identify_token_heavy_combinations(self) -> List[Dict]:
        """Identify skill combinations that may cause token budget issues."""
        heavy_skills = [
            (name, data) for name, data in self.skills.items()
            if data['size'] > 3000 or data['complexity_score'] > 10
        ]
        
        combinations = []
        for i, (name_a, data_a) in enumerate(heavy_skills):
            for name_b, data_b in heavy_skills[i+1:]:
                # If they share domains, they might activate together
                if data_a['domains'] & data_b['domains']:
                    combinations.append({
                        'skills': [name_a, name_b],
                        'combined_size': data_a['size'] + data_b['size'],
                        'shared_domains': list(data_a['domains'] & data_b['domains'])
                    })
        
        return sorted(combinations, key=lambda x: x['combined_size'], reverse=True)
    
    def recommend_stacks(self, task_domain: str = None) -> List[Dict]:
        """Recommend optimal skill stacks for common workflows."""
        stacks = []
        
        # Document workflow stack
        doc_skills = [
            name for name, data in self.skills.items()
            if 'document' in data['domains'] or 'docx' in data['formats']
        ]
        if doc_skills:
            stacks.append({
                'name': 'Document Workflow',
                'skills': doc_skills,
                'use_case': 'Creating, editing, and analyzing documents'
            })
        
        # Presentation workflow stack
        pres_skills = [
            name for name, data in self.skills.items()
            if 'presentation' in data['domains'] or 'pptx' in data['formats']
        ]
        if pres_skills:
            stacks.append({
                'name': 'Presentation Workflow',
                'skills': pres_skills,
                'use_case': 'Creating and editing presentations'
            })
        
        # Research workflow stack
        research_skills = [
            name for name, data in self.skills.items()
            if 'research' in data['domains'] or 'web_search' in data['tools']
        ]
        if research_skills:
            stacks.append({
                'name': 'Research Workflow',
                'skills': research_skills,
                'use_case': 'Web research and information gathering'
            })
        
        # Data analysis stack
        data_skills = [
            name for name, data in self.skills.items()
            if 'spreadsheet' in data['domains'] or 'xlsx' in data['formats']
        ]
        if data_skills:
            stacks.append({
                'name': 'Data Analysis Workflow',
                'skills': data_skills,
                'use_case': 'Spreadsheet analysis and data processing'
            })
        
        return stacks


def main():
    """Run the skill analysis and output results."""
    analyzer = SkillAnalyzer()
    
    print("Scanning skills...")
    analyzer.scan_skills()
    
    print(f"\nFound {len(analyzer.skills)} skills\n")
    
    # Output basic skill information
    print("=== SKILL INVENTORY ===")
    for name, data in sorted(analyzer.skills.items()):
        print(f"{name} ({data['type']})")
        print(f"  Tools: {', '.join(data['tools']) if data['tools'] else 'none'}")
        print(f"  Formats: {', '.join(data['formats']) if data['formats'] else 'none'}")
        print(f"  Domains: {', '.join(data['domains']) if data['domains'] else 'none'}")
        print(f"  Complexity: {data['complexity_score']}")
        print()
    
    # Find dependencies
    print("\n=== SKILL DEPENDENCIES ===")
    dependencies = analyzer.find_dependencies()
    for skill, related in sorted(dependencies.items()):
        if related:
            print(f"{skill}: {', '.join(related[:5])}")
    
    # Detect conflicts
    print("\n=== FORMAT CONFLICTS ===")
    conflicts = analyzer.detect_format_conflicts()
    for conflict in conflicts:
        print(f"{conflict['format']}: {', '.join(conflict['skills'])}")
    
    # Token-heavy combinations
    print("\n=== TOKEN-HEAVY COMBINATIONS ===")
    heavy = analyzer.identify_token_heavy_combinations()
    for combo in heavy[:5]:
        print(f"{' + '.join(combo['skills'])}: {combo['combined_size']} tokens")
    
    # Recommended stacks
    print("\n=== RECOMMENDED STACKS ===")
    stacks = analyzer.recommend_stacks()
    for stack in stacks:
        print(f"{stack['name']}: {', '.join(stack['skills'][:5])}")


if __name__ == "__main__":
    main()


--- skill-dependency-mapper/scripts/detect_bottlenecks.py ---
#!/usr/bin/env python3
"""
Bottleneck Detection for Skill Workflows

Identifies potential bottlenecks:
- Skills requiring many tool calls
- Skills with conflicting approaches
- Skills loading large reference files
- Token budget concerns with multiple activations
"""

import json
import sys
from pathlib import Path
from typing import Dict, List
from collections import defaultdict

class BottleneckDetector:
    def __init__(self, skill_data: Dict):
        # Convert lists to sets for set operations
        self.skills = {}
        for name, data in skill_data.items():
            self.skills[name] = data.copy()
            self.skills[name]['tools'] = set(data.get('tools', []))
            self.skills[name]['formats'] = set(data.get('formats', []))
            self.skills[name]['domains'] = set(data.get('domains', []))
        
        self.bottlenecks = {
            'high_tool_usage': [],
            'conflicting_approaches': [],
            'large_references': [],
            'token_budget_risks': [],
            'sequential_dependencies': []
        }
    
    def detect_high_tool_usage(self, threshold: int = 3):
        """Identify skills that require many tool calls."""
        for name, data in self.skills.items():
            if len(data['tools']) >= threshold:
                self.bottlenecks['high_tool_usage'].append({
                    'skill': name,
                    'tool_count': len(data['tools']),
                    'tools': list(data['tools']),
                    'impact': 'May slow down workflows due to multiple tool calls'
                })
    
    def detect_conflicting_approaches(self):
        """Find skills handling same formats with different tools."""
        format_handlers = defaultdict(list)
        
        for name, data in self.skills.items():
            for fmt in data['formats']:
                format_handlers[fmt].append({
                    'skill': name,
                    'tools': data['tools'],
                    'complexity': data['complexity_score']
                })
        
        for fmt, handlers in format_handlers.items():
            if len(handlers) >= 2:
                # Check for tool differences
                all_tools = [set(h['tools']) for h in handlers]
                unique_tool_combos = set(map(frozenset, all_tools))
                
                if len(unique_tool_combos) > 1:
                    self.bottlenecks['conflicting_approaches'].append({
                        'format': fmt,
                        'skills': [h['skill'] for h in handlers],
                        'approaches': {h['skill']: list(h['tools']) for h in handlers},
                        'impact': 'Inconsistent handling may confuse workflow selection'
                    })
    
    def detect_large_references(self, size_threshold: int = 5000):
        """Identify skills with large SKILL.md files that consume tokens."""
        for name, data in self.skills.items():
            if data['size'] > size_threshold:
                self.bottlenecks['large_references'].append({
                    'skill': name,
                    'size': data['size'],
                    'has_references': data['has_references'],
                    'impact': f'Consumes ~{data["size"] // 4} tokens when loaded',
                    'recommendation': 'Consider splitting into reference files' if not data['has_references'] else 'Already uses references'
                })
    
    def detect_token_budget_risks(self, combined_threshold: int = 8000):
        """Find skill combinations that risk exceeding token budgets."""
        large_skills = {
            name: data for name, data in self.skills.items()
            if data['size'] > 3000
        }
        
        # Find skills that might activate together
        for name_a, data_a in large_skills.items():
            for name_b, data_b in large_skills.items():
                if name_a >= name_b:
                    continue
                
                # Check if they share domains (likely to co-activate)
                shared_domains = data_a['domains'] & data_b['domains']
                
                if shared_domains:
                    combined_size = data_a['size'] + data_b['size']
                    if combined_size > combined_threshold:
                        self.bottlenecks['token_budget_risks'].append({
                            'skills': [name_a, name_b],
                            'combined_size': combined_size,
                            'shared_domains': list(shared_domains),
                            'impact': f'Combined token load: ~{combined_size // 4} tokens',
                            'recommendation': 'May need sequential processing instead of parallel'
                        })
    
    def detect_sequential_dependencies(self):
        """Identify skills that must run in sequence."""
        # Look for skills that produce outputs consumed by other skills
        producers = {}
        consumers = {}
        
        for name, data in self.skills.items():
            # Skills that create files are producers
            if 'create_file' in data['tools'] or data['has_scripts']:
                for fmt in data['formats']:
                    if fmt not in producers:
                        producers[fmt] = []
                    producers[fmt].append(name)
            
            # Skills that read specific formats are consumers
            if 'view' in data['tools'] or data['formats']:
                for fmt in data['formats']:
                    if fmt not in consumers:
                        consumers[fmt] = []
                    consumers[fmt].append(name)
        
        # Find format chains
        for fmt in set(producers.keys()) & set(consumers.keys()):
            if producers[fmt] and consumers[fmt]:
                self.bottlenecks['sequential_dependencies'].append({
                    'format': fmt,
                    'producers': producers[fmt],
                    'consumers': consumers[fmt],
                    'impact': 'Sequential workflow required',
                    'optimization': 'Consider batching operations'
                })
    
    def generate_report(self) -> str:
        """Generate a markdown report of all detected bottlenecks."""
        report_lines = ["# Skill Workflow Bottleneck Analysis\n"]
        
        # High tool usage
        if self.bottlenecks['high_tool_usage']:
            report_lines.append("## High Tool Usage Bottlenecks\n")
            report_lines.append("Skills requiring multiple tool calls:\n")
            for item in sorted(self.bottlenecks['high_tool_usage'], 
                             key=lambda x: x['tool_count'], reverse=True):
                report_lines.append(f"- **{item['skill']}** ({item['tool_count']} tools)")
                report_lines.append(f"  - Tools: {', '.join(item['tools'])}")
                report_lines.append(f"  - Impact: {item['impact']}\n")
        
        # Conflicting approaches
        if self.bottlenecks['conflicting_approaches']:
            report_lines.append("## Conflicting Approach Bottlenecks\n")
            report_lines.append("Multiple skills handling the same format differently:\n")
            for item in self.bottlenecks['conflicting_approaches']:
                report_lines.append(f"- **{item['format']}** format conflict")
                report_lines.append(f"  - Skills: {', '.join(item['skills'])}")
                for skill, tools in item['approaches'].items():
                    report_lines.append(f"  - {skill}: {', '.join(tools) if tools else 'no specific tools'}")
                report_lines.append(f"  - Impact: {item['impact']}\n")
        
        # Large references
        if self.bottlenecks['large_references']:
            report_lines.append("## Token Budget Bottlenecks\n")
            report_lines.append("Skills with large SKILL.md files:\n")
            for item in sorted(self.bottlenecks['large_references'], 
                             key=lambda x: x['size'], reverse=True):
                report_lines.append(f"- **{item['skill']}** ({item['size']} chars)")
                report_lines.append(f"  - Impact: {item['impact']}")
                report_lines.append(f"  - Recommendation: {item['recommendation']}\n")
        
        # Token budget risks
        if self.bottlenecks['token_budget_risks']:
            report_lines.append("## Co-Activation Token Risks\n")
            report_lines.append("Skill pairs that may exceed token budgets when used together:\n")
            for item in sorted(self.bottlenecks['token_budget_risks'], 
                             key=lambda x: x['combined_size'], reverse=True)[:10]:
                report_lines.append(f"- **{' + '.join(item['skills'])}**")
                report_lines.append(f"  - Combined size: {item['combined_size']} chars")
                report_lines.append(f"  - Shared domains: {', '.join(item['shared_domains'])}")
                report_lines.append(f"  - Impact: {item['impact']}")
                report_lines.append(f"  - Recommendation: {item['recommendation']}\n")
        
        # Sequential dependencies
        if self.bottlenecks['sequential_dependencies']:
            report_lines.append("## Sequential Workflow Dependencies\n")
            report_lines.append("Format chains requiring sequential processing:\n")
            for item in self.bottlenecks['sequential_dependencies']:
                report_lines.append(f"- **{item['format']}** format chain")
                report_lines.append(f"  - Producers: {', '.join(item['producers'])}")
                report_lines.append(f"  - Consumers: {', '.join(item['consumers'])}")
                report_lines.append(f"  - Impact: {item['impact']}")
                report_lines.append(f"  - Optimization: {item['optimization']}\n")
        
        if not any(self.bottlenecks.values()):
            report_lines.append("No significant bottlenecks detected.\n")
        
        return '\n'.join(report_lines)
    
    def run_all_detections(self):
        """Run all bottleneck detection methods."""
        self.detect_high_tool_usage()
        self.detect_conflicting_approaches()
        self.detect_large_references()
        self.detect_token_budget_risks()
        self.detect_sequential_dependencies()


def main():
    """Run bottleneck detection on skill data."""
    if len(sys.argv) > 1:
        # Load skill data from JSON file
        with open(sys.argv[1], 'r') as f:
            skill_data = json.load(f)
    else:
        print("Usage: detect_bottlenecks.py <skill_data.json>")
        print("Or use in conjunction with analyze_skills.py")
        sys.exit(1)
    
    detector = BottleneckDetector(skill_data)
    detector.run_all_detections()
    
    print(detector.generate_report())


if __name__ == "__main__":
    main()


--- skill-doc-generator/README.md ---
# skill-doc-generator

> Auto-generates standardized README documentation from SKILL.md files, validates consistency (frontmatter, descriptions, terminology), and creates usage examples. Use when documenting individual skills, generating docs for multiple skills in a directory, or validating skill quality standards.

## Overview

This skill automates the creation of standardized README files for skills by analyzing SKILL.md files, extracting structure and examples, validating quality standards, and generating comprehensive documentation. It ensures consistency across skill documentation while providing actionable validation feedback.

## When to Use This Skill

This skill is triggered when working with tasks related to skill-doc-generator.

**Common trigger scenarios:**
- documenting individual skills


## Skill Structure

- **Lines of documentation:** 208
- **Sections:** 16
- **Code examples:** 8

## Bundled Resources

### Scripts

- [`scripts/__pycache__/analyze_skill.cpython-312.pyc`](scripts/scripts/__pycache__/analyze_skill.cpython-312.pyc)
- [`scripts/__pycache__/validate_consistency.cpython-312.pyc`](scripts/scripts/__pycache__/validate_consistency.cpython-312.pyc)
- [`scripts/analyze_skill.py`](scripts/scripts/analyze_skill.py)
- [`scripts/document_directory.py`](scripts/scripts/document_directory.py)
- [`scripts/generate_readme.py`](scripts/scripts/generate_readme.py)
- [`scripts/validate_consistency.py`](scripts/scripts/validate_consistency.py)

### Reference Documentation

- [`references/consistency-rules.md`](references/references/consistency-rules.md)
- [`references/readme-template.md`](references/references/readme-template.md)
- [`references/terminology-standards.md`](references/references/terminology-standards.md)

## Key Sections

- **Skill Documentation Generator**
- **Workflow**
- **Document All User Skills With Validation**
- **Quick Pass Without Validation**
- **Script Reference**

## Usage Examples

### Example 1

```bash
python scripts/analyze_skill.py <skill_directory>
```

### Example 2

```bash
python scripts/validate_consistency.py <skill_directory> --verbose
```

### Example 3

```bash
python scripts/generate_readme.py <skill_directory> [output_path]
```

## Quality Validation

⚠️  **2 warning(s) found**

<details>
<summary>View validation details</summary>

- `INFO` Description: Description contains vague term 'multiple' - consider being more specific
- `INFO` Terminology: Found 'you should' - consider using imperative form (e.g., 'Use' instead of 'You should use')
- `WARNING` Resources: Script 'scripts/__pycache__/analyze_skill.cpython-312.pyc' exists but isn't referenced in SKILL.md
- `WARNING` Resources: Script 'scripts/__pycache__/validate_consistency.cpython-312.pyc' exists but isn't referenced in SKILL.md

</details>

---

_Documentation auto-generated from `SKILL.md`_

## Links discovered
- [`scripts/__pycache__/analyze_skill.cpython-312.pyc`](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/skill-doc-generator/scripts/scripts/__pycache__/analyze_skill.cpython-312.pyc)
- [`scripts/__pycache__/validate_consistency.cpython-312.pyc`](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/skill-doc-generator/scripts/scripts/__pycache__/validate_consistency.cpython-312.pyc)
- [`scripts/analyze_skill.py`](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/skill-doc-generator/scripts/scripts/analyze_skill.py)
- [`scripts/document_directory.py`](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/skill-doc-generator/scripts/scripts/document_directory.py)
- [`scripts/generate_readme.py`](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/skill-doc-generator/scripts/scripts/generate_readme.py)
- [`scripts/validate_consistency.py`](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/skill-doc-generator/scripts/scripts/validate_consistency.py)
- [`references/consistency-rules.md`](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/skill-doc-generator/references/references/consistency-rules.md)
- [`references/readme-template.md`](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/skill-doc-generator/references/references/readme-template.md)
- [`references/terminology-standards.md`](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/skill-doc-generator/references/references/terminology-standards.md)

--- skill-doc-generator/SKILL.md ---
---
name: skill-doc-generator
description: Auto-generates standardized README documentation from SKILL.md files, validates consistency (frontmatter, descriptions, terminology), and creates usage examples. Use when documenting individual skills, generating docs for multiple skills in a directory, or validating skill quality standards.
---

# Skill Documentation Generator

Auto-generate high-quality README documentation for skills with built-in consistency validation and example generation.

## Overview

This skill automates the creation of standardized README files for skills by analyzing SKILL.md files, extracting structure and examples, validating quality standards, and generating comprehensive documentation. It ensures consistency across skill documentation while providing actionable validation feedback.

## Workflow

### Single Skill Documentation

Generate documentation for one skill:

1. **Analyze the skill**:
   ```bash
   python scripts/analyze_skill.py <skill_directory>
   ```
   Extracts metadata, sections, code blocks, and resources.

2. **Validate consistency**:
   ```bash
   python scripts/validate_consistency.py <skill_directory> --verbose
   ```
   Checks frontmatter, description quality, and terminology.

3. **Generate README**:
   ```bash
   python scripts/generate_readme.py <skill_directory> [output_path]
   ```
   Creates README.md with validation results.

### Batch Documentation

Document multiple skills at once:

```bash
python scripts/document_directory.py <directory> [options]
```

**Options:**
- `--output <dir>`: Specify output directory
- `--no-recursive`: Don't search subdirectories
- `--no-index`: Skip index file generation
- `--no-validate`: Skip validation checks

**Example:**
```bash
# Document all user skills with validation
python scripts/document_directory.py /mnt/skills/user --output ./docs

# Quick pass without validation
python scripts/document_directory.py ./my-skills --no-validate
```

## Script Reference

### analyze_skill.py
Parses SKILL.md and extracts structured information.

**Usage**: `python scripts/analyze_skill.py <skill_directory>`

**Returns**:
- Metadata (name, description)
- Sections and structure
- Code blocks with language tags
- Referenced resources (scripts, references, assets)
- Statistics (line count, section count)

### validate_consistency.py
Validates skill quality against standards defined in references/consistency-rules.md.

**Usage**: `python scripts/validate_consistency.py <skill_directory> [--verbose]`

**Checks**:
- Frontmatter completeness and format
- Description quality (length, clarity, triggers)
- Structure appropriateness
- Terminology consistency
- Resource references
- Code example quality

**Severity Levels**:
- **ERROR**: Breaks functionality (missing required fields)
- **WARNING**: Quality issues (naming, unreferenced resources)
- **INFO**: Suggestions (style, optional improvements)

### generate_readme.py
Creates README.md from skill analysis.

**Usage**: `python scripts/generate_readme.py <skill_directory> [output_path]`

**Generates**:
- Title and description
- Overview from SKILL.md
- Trigger scenarios
- Structure statistics
- Bundled resource lists with links
- Key sections overview
- Usage examples (up to 3)
- Validation results (optional)

**Template**: See references/readme-template.md for structure.

### document_directory.py
Batch processes multiple skills in a directory.

**Usage**: `python scripts/document_directory.py <directory> [options]`

**Features**:
- Recursive skill discovery
- Parallel validation and documentation
- Index generation with categorization
- Summary statistics
- Error handling per skill

## Quality Standards

Validation enforces these standards:

### Frontmatter
- **name**: Lowercase with hyphens (e.g., `skill-name`)
- **description**: 50-500 chars, clear triggers
- Must start with capital letter
- Include "when" or "use" phrases

### Structure
- Body: 100+ chars minimum, <500 lines recommended
- Sections: Overview/workflow recommended
- Resources: All files referenced in SKILL.md

### Terminology
- Use imperative form: "Use" not "You should use"
- Capitalize "Claude" consistently
- Avoid vague terms: "various", "multiple"
- Active voice preferred

See references/consistency-rules.md and references/terminology-standards.md for complete standards.

## Reference Files

### readme-template.md
Standard README structure and best practices. Defines:
- Required sections
- Optional sections
- Formatting guidelines
- Link conventions

### consistency-rules.md
Detailed validation criteria. Covers:
- Frontmatter requirements
- Description quality metrics
- Structure guidelines
- Resource validation
- Error severity definitions

### terminology-standards.md
Standard vocabulary and style guide. Includes:
- Writing style (imperative form)
- Common terms and their usage
- Phrases to avoid
- Formatting conventions
- Consistency checklist

## Examples

### Example 1: Document a Single Skill
```bash
# Analyze
python scripts/analyze_skill.py ./my-skill

# Validate
python scripts/validate_consistency.py ./my-skill --verbose

# Generate README
python scripts/generate_readme.py ./my-skill
```

### Example 2: Batch Process with Index
```bash
# Document all skills in a directory
python scripts/document_directory.py /mnt/skills/user \
  --output ./documentation \
  --recursive
```

### Example 3: Quick Validation Pass
```bash
# Just validate without generating docs
python scripts/validate_consistency.py ./my-skill
```

## Common Use Cases

**New skill creation**: Generate documentation as part of skill development
**Quality audits**: Validate existing skills against standards
**Documentation updates**: Regenerate READMEs after SKILL.md changes
**Batch operations**: Document entire skill libraries
**CI/CD integration**: Automated validation in deployment pipelines

## Tips

- Run validation before generating documentation to catch issues early
- Use `--verbose` flag to see INFO-level suggestions
- Reference files provide the "why" behind validation rules
- Generated READMEs include validation results for transparency
- Index files help navigate large skill collections


--- skill-doc-generator/scripts/analyze_skill.py ---
#!/usr/bin/env python3
"""
Analyzes a SKILL.md file and extracts metadata, structure, and resources.
"""

import yaml
import re
import os
import sys
from pathlib import Path
from typing import Dict, List, Optional


def parse_frontmatter(content: str) -> tuple[Dict, str]:
    """Extract YAML frontmatter and remaining content."""
    frontmatter_pattern = r'^---\s*\n(.*?)\n---\s*\n(.*)$'
    match = re.match(frontmatter_pattern, content, re.DOTALL)
    
    if not match:
        return {}, content
    
    yaml_content = match.group(1)
    body = match.group(2)
    
    try:
        metadata = yaml.safe_load(yaml_content)
        return metadata or {}, body
    except yaml.YAMLError as e:
        print(f"Warning: Failed to parse YAML frontmatter: {e}", file=sys.stderr)
        return {}, content


def extract_sections(body: str) -> Dict[str, str]:
    """Extract major sections from markdown body."""
    sections = {}
    current_section = "introduction"
    current_content = []
    
    for line in body.split('\n'):
        if line.startswith('# '):
            if current_content:
                sections[current_section] = '\n'.join(current_content).strip()
            current_section = line[2:].strip().lower().replace(' ', '_')
            current_content = []
        elif line.startswith('## '):
            if current_content:
                sections[current_section] = '\n'.join(current_content).strip()
            current_section = line[3:].strip().lower().replace(' ', '_')
            current_content = []
        else:
            current_content.append(line)
    
    if current_content:
        sections[current_section] = '\n'.join(current_content).strip()
    
    return sections


def find_code_blocks(content: str) -> List[Dict[str, str]]:
    """Extract code blocks with their language tags."""
    code_blocks = []
    pattern = r'```(\w+)?\n(.*?)```'
    
    for match in re.finditer(pattern, content, re.DOTALL):
        language = match.group(1) or 'text'
        code = match.group(2).strip()
        code_blocks.append({
            'language': language,
            'code': code
        })
    
    return code_blocks


def find_references(body: str, skill_dir: Path) -> Dict[str, List[str]]:
    """Find references to bundled resources (scripts, references, assets)."""
    resources = {
        'scripts': [],
        'references': [],
        'assets': []
    }
    
    # Pattern for markdown links and file references
    link_pattern = r'\[(.*?)\]\((.*?)\)'
    
    for match in re.finditer(link_pattern, body):
        link_path = match.group(2)
        
        # Check if it's a relative path
        if not link_path.startswith('http'):
            for resource_type in resources.keys():
                if link_path.startswith(resource_type):
                    resources[resource_type].append(link_path)
    
    # Also check actual filesystem
    for resource_type in resources.keys():
        resource_dir = skill_dir / resource_type
        if resource_dir.exists():
            for file_path in resource_dir.rglob('*'):
                if file_path.is_file():
                    rel_path = file_path.relative_to(skill_dir)
                    path_str = str(rel_path)
                    if path_str not in resources[resource_type]:
                        resources[resource_type].append(path_str)
    
    return resources


def analyze_skill(skill_path: str) -> Dict:
    """
    Analyze a skill and return structured information.
    
    Args:
        skill_path: Path to skill directory or SKILL.md file
        
    Returns:
        Dictionary containing skill analysis
    """
    skill_path = Path(skill_path)
    
    # Handle both directory and file paths
    if skill_path.is_dir():
        skill_file = skill_path / 'SKILL.md'
        skill_dir = skill_path
    else:
        skill_file = skill_path
        skill_dir = skill_path.parent
    
    if not skill_file.exists():
        raise FileNotFoundError(f"SKILL.md not found at {skill_file}")
    
    content = skill_file.read_text(encoding='utf-8')
    metadata, body = parse_frontmatter(content)
    
    analysis = {
        'path': str(skill_dir),
        'metadata': metadata,
        'name': metadata.get('name', skill_dir.name),
        'description': metadata.get('description', ''),
        'sections': extract_sections(body),
        'code_blocks': find_code_blocks(body),
        'resources': find_references(body, skill_dir),
        'body_length': len(body),
        'line_count': len(body.split('\n'))
    }
    
    return analysis


if __name__ == '__main__':
    if len(sys.argv) != 2:
        print("Usage: python analyze_skill.py <skill_directory_or_SKILL.md>")
        sys.exit(1)
    
    skill_path = sys.argv[1]
    
    try:
        analysis = analyze_skill(skill_path)
        
        print(f"Skill Analysis: {analysis['name']}")
        print("=" * 60)
        print(f"Description: {analysis['description'][:100]}...")
        print(f"\nMetadata fields: {', '.join(analysis['metadata'].keys())}")
        print(f"Body length: {analysis['body_length']} chars, {analysis['line_count']} lines")
        print(f"\nSections found: {len(analysis['sections'])}")
        for section in analysis['sections'].keys():
            print(f"  - {section}")
        
        print(f"\nCode blocks: {len(analysis['code_blocks'])}")
        for i, block in enumerate(analysis['code_blocks'][:3], 1):
            print(f"  {i}. {block['language']} ({len(block['code'])} chars)")
        
        print("\nResources:")
        for resource_type, files in analysis['resources'].items():
            if files:
                print(f"  {resource_type}: {len(files)} file(s)")
                for f in files[:3]:
                    print(f"    - {f}")
        
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)


--- skill-doc-generator/scripts/document_directory.py ---
#!/usr/bin/env python3
"""
Generates documentation for all skills in a directory.
"""

import sys
import traceback
from pathlib import Path
from typing import List, Dict
from analyze_skill import analyze_skill
from generate_readme import generate_readme, save_readme
from validate_consistency import validate_skill


def find_skills(directory: str, recursive: bool = True) -> List[Path]:
    """
    Find all SKILL.md files in directory.
    
    Args:
        directory: Root directory to search
        recursive: Whether to search subdirectories
        
    Returns:
        List of paths to SKILL.md files
    """
    directory = Path(directory)
    
    if not directory.exists():
        raise FileNotFoundError(f"Directory not found: {directory}")
    
    skill_files = []
    
    if recursive:
        skill_files = list(directory.rglob('SKILL.md'))
    else:
        skill_files = list(directory.glob('*/SKILL.md'))
    
    return sorted(skill_files)


def create_index_file(skills: List[Dict], output_path: Path):
    """Generate an index/catalog of all skills."""
    lines = []
    
    lines.append("# Skills Documentation Index")
    lines.append("")
    lines.append(f"Documentation for {len(skills)} skills.")
    lines.append("")
    
    # Group by category if possible (based on path structure)
    categorized = {}
    uncategorized = []
    
    for skill in skills:
        path = Path(skill['path'])
        # Try to extract category from path
        parts = path.parts
        if len(parts) > 1 and parts[-2] not in ['skill-doc-generator', '.']:
            category = parts[-2]
        else:
            category = 'Other'
        
        if category not in categorized:
            categorized[category] = []
        categorized[category].append(skill)
    
    # Output by category
    for category in sorted(categorized.keys()):
        lines.append(f"## {category.title()}")
        lines.append("")
        
        for skill in sorted(categorized[category], key=lambda x: x['name']):
            name = skill['name']
            desc = skill['description'][:100] + "..." if len(skill['description']) > 100 else skill['description']
            
            # Link to README if it exists
            skill_path = Path(skill['path'])
            readme_path = skill_path / 'README.md'
            
            if readme_path.exists():
                try:
                    rel_path = readme_path.relative_to(output_path.parent)
                    lines.append(f"### [{name}]({rel_path})")
                except (ValueError, AttributeError):
                    lines.append(f"### [{name}](./README.md)")
            else:
                lines.append(f"### {name}")
            
            lines.append("")
            lines.append(f"{desc}")
            lines.append("")
            
            # Add quick stats
            lines.append(f"- **Lines:** {skill['line_count']}")
            lines.append(f"- **Resources:** {sum(len(v) for v in skill['resources'].values())} files")
            lines.append("")
    
    content = '\n'.join(lines)
    output_path.write_text(content, encoding='utf-8')
    return str(output_path)


def document_directory(
    directory: str,
    output_dir: str = None,
    recursive: bool = True,
    generate_index_file: bool = True,
    validate: bool = True
) -> Dict:
    """
    Document all skills in a directory.
    
    Args:
        directory: Directory containing skills
        output_dir: Optional output directory for documentation
        recursive: Whether to search subdirectories
        generate_index_file: Whether to create an index file
        validate: Whether to run validation
        
    Returns:
        Statistics dictionary
    """
    directory = Path(directory)
    
    if output_dir:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"Searching for skills in: {directory}")
    skill_files = find_skills(directory, recursive)
    
    if not skill_files:
        print("⚠️  No SKILL.md files found")
        return {'total': 0, 'successful': 0, 'failed': 0}
    
    print(f"Found {len(skill_files)} skill(s)")
    print("")
    
    stats = {
        'total': len(skill_files),
        'successful': 0,
        'failed': 0,
        'errors': 0,
        'warnings': 0
    }
    
    analyzed_skills = []
    
    for skill_file in skill_files:
        skill_dir = skill_file.parent
        skill_name = skill_dir.name
        
        try:
            print(f"Processing: {skill_name}...")
            
            # Analyze
            analysis = analyze_skill(skill_dir)
            analyzed_skills.append(analysis)
            
            # Validate if requested
            if validate:
                issues, has_errors = validate_skill(skill_dir)
                errors = [i for i in issues if i.severity == 'ERROR']
                warnings = [i for i in issues if i.severity == 'WARNING']
                
                stats['errors'] += len(errors)
                stats['warnings'] += len(warnings)
                
                if errors:
                    print(f"  ❌ {len(errors)} error(s)")
                elif warnings:
                    print(f"  ⚠️  {len(warnings)} warning(s)")
                else:
                    print(f"  ✅ Validated")
            
            # Generate README
            readme_content = generate_readme(analysis, include_validation=validate)
            
            # Save README
            if output_dir:
                # Save to output directory
                skill_output_dir = output_dir / skill_name
                skill_output_dir.mkdir(exist_ok=True)
                readme_path = skill_output_dir / 'README.md'
            else:
                # Save alongside SKILL.md
                readme_path = skill_dir / 'README.md'
            
            readme_path.write_text(readme_content, encoding='utf-8')
            print(f"  📄 README: {readme_path}")
            
            stats['successful'] += 1
            
        except Exception as e:
            print(f"  ❌ Failed: {e}")
            stats['failed'] += 1
        
        print("")
    
    # Generate index if requested
    if generate_index_file and analyzed_skills:
        index_path = output_dir / 'INDEX.md' if output_dir else directory / 'INDEX.md'
        create_index_file(analyzed_skills, index_path)
        print(f"📚 Index generated: {index_path}")
        print("")
    
    # Summary
    print("=" * 60)
    print("Summary:")
    print(f"  Total skills: {stats['total']}")
    print(f"  Successful: {stats['successful']}")
    print(f"  Failed: {stats['failed']}")
    if validate:
        print(f"  Total errors: {stats['errors']}")
        print(f"  Total warnings: {stats['warnings']}")
    
    return stats


if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("Usage: python document_directory.py <directory> [options]")
        print("\nOptions:")
        print("  --output <dir>     Output directory for documentation")
        print("  --no-recursive     Don't search subdirectories")
        print("  --no-index         Don't generate index file")
        print("  --no-validate      Skip validation checks")
        print("\nExamples:")
        print("  python document_directory.py /mnt/skills/user")
        print("  python document_directory.py ./skills --output ./docs")
        sys.exit(1)
    
    directory = sys.argv[1]
    
    # Parse options
    args = sys.argv[2:]
    output_dir = None
    recursive = '--no-recursive' not in args
    generate_index = '--no-index' not in args
    validate = '--no-validate' not in args
    
    if '--output' in args:
        idx = args.index('--output')
        if idx + 1 < len(args):
            output_dir = args[idx + 1]
    
    try:
        stats = document_directory(
            directory,
            output_dir=output_dir,
            recursive=recursive,
            generate_index_file=generate_index,
            validate=validate
        )
        
        sys.exit(0 if stats['failed'] == 0 else 1)
        
    except Exception as e:
        print(f"❌ Error: {e}", file=sys.stderr)
        traceback.print_exc()
        sys.exit(1)


## Links discovered
- [{name}](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/skill-doc-generator/scripts/{rel_path}.md)
- [{name}](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/skill-doc-generator/scripts/README.md)

--- skill-doc-generator/scripts/generate_readme.py ---
#!/usr/bin/env python3
"""
Generates a README.md file for a skill based on its analysis.
"""

import sys
from pathlib import Path
from typing import Dict, List
from analyze_skill import analyze_skill
from validate_consistency import validate_skill


def format_resource_list(resources: List[str], base_path: str = '') -> str:
    """Format a list of resources as markdown list items."""
    if not resources:
        return "_None_"
    
    lines = []
    for resource in sorted(resources):
        resource_path = Path(resource)
        if base_path:
            link = f"[`{resource}`]({base_path}/{resource})"
        else:
            link = f"`{resource}`"
        lines.append(f"- {link}")
    
    return '\n'.join(lines)


def extract_key_examples(analysis: Dict) -> List[Dict]:
    """Extract representative code examples from the skill."""
    code_blocks = analysis.get('code_blocks', [])
    
    # Limit to first 3 most substantial examples
    examples = []
    for block in code_blocks[:3]:
        if len(block['code']) > 20:  # Skip trivial examples
            examples.append(block)
    
    return examples


def generate_usage_section(analysis: Dict) -> str:
    """Generate usage/trigger examples section."""
    name = analysis['name']
    description = analysis['description']
    
    # Try to extract trigger phrases from description
    triggers = []
    if 'when' in description.lower():
        # Extract phrases after "when"
        import re
        when_matches = re.finditer(r'when\s+([^.,;]+)', description, re.IGNORECASE)
        for match in when_matches:
            triggers.append(match.group(1).strip())
    
    usage = f"This skill is triggered when working with tasks related to {name}.\n\n"
    
    if triggers:
        usage += "**Common trigger scenarios:**\n"
        for trigger in triggers[:3]:
            usage += f"- {trigger}\n"
    else:
        usage += f"The skill activates based on: {description[:200]}...\n"
    
    return usage


def generate_readme(analysis: Dict, include_validation: bool = True) -> str:
    """
    Generate README.md content from skill analysis.
    
    Args:
        analysis: Skill analysis dictionary
        include_validation: Whether to include validation results
        
    Returns:
        README.md content as string
    """
    name = analysis['name']
    description = analysis['description']
    sections = analysis.get('sections', {})
    resources = analysis.get('resources', {})
    
    readme = []
    
    # Title and description
    readme.append(f"# {name}")
    readme.append("")
    readme.append(f"> {description}")
    readme.append("")
    
    # Overview (from first section or description)
    overview_section = sections.get('overview', sections.get('introduction', ''))
    if overview_section:
        readme.append("## Overview")
        readme.append("")
        # Take first few paragraphs
        paragraphs = overview_section.split('\n\n')[:2]
        readme.append('\n\n'.join(paragraphs))
        readme.append("")
    
    # When to use this skill
    readme.append("## When to Use This Skill")
    readme.append("")
    readme.append(generate_usage_section(analysis))
    readme.append("")
    
    # Structure
    readme.append("## Skill Structure")
    readme.append("")
    readme.append(f"- **Lines of documentation:** {analysis['line_count']}")
    readme.append(f"- **Sections:** {len(sections)}")
    readme.append(f"- **Code examples:** {len(analysis['code_blocks'])}")
    readme.append("")
    
    # Resources
    if any(resources.values()):
        readme.append("## Bundled Resources")
        readme.append("")
        
        if resources.get('scripts'):
            readme.append("### Scripts")
            readme.append("")
            readme.append(format_resource_list(resources['scripts'], 'scripts'))
            readme.append("")
        
        if resources.get('references'):
            readme.append("### Reference Documentation")
            readme.append("")
            readme.append(format_resource_list(resources['references'], 'references'))
            readme.append("")
        
        if resources.get('assets'):
            readme.append("### Assets")
            readme.append("")
            readme.append(format_resource_list(resources['assets'], 'assets'))
            readme.append("")
    
    # Key sections
    if len(sections) > 1:
        readme.append("## Key Sections")
        readme.append("")
        # List main sections (skip introduction/overview)
        main_sections = [s for s in sections.keys() 
                        if s not in ['introduction', 'overview', name.lower().replace('-', '_')]]
        
        for section in main_sections[:5]:  # Limit to top 5
            section_title = section.replace('_', ' ').title()
            readme.append(f"- **{section_title}**")
        readme.append("")
    
    # Usage examples
    examples = extract_key_examples(analysis)
    if examples:
        readme.append("## Usage Examples")
        readme.append("")
        for i, example in enumerate(examples, 1):
            readme.append(f"### Example {i}")
            readme.append("")
            readme.append(f"```{example['language']}")
            readme.append(example['code'][:300])  # Truncate long examples
            if len(example['code']) > 300:
                readme.append("...")
            readme.append("```")
            readme.append("")
    
    # Validation results (optional)
    if include_validation:
        try:
            issues, has_errors = validate_skill(analysis['path'])
            
            readme.append("## Quality Validation")
            readme.append("")
            
            if not issues:
                readme.append("✅ **All validation checks passed**")
            else:
                errors = [i for i in issues if i.severity == 'ERROR']
                warnings = [i for i in issues if i.severity == 'WARNING']
                
                if errors:
                    readme.append(f"❌ **{len(errors)} error(s) found**")
                if warnings:
                    readme.append(f"⚠️  **{len(warnings)} warning(s) found**")
                
                readme.append("")
                readme.append("<details>")
                readme.append("<summary>View validation details</summary>")
                readme.append("")
                for issue in issues[:10]:  # Limit output
                    readme.append(f"- `{issue.severity}` {issue.category}: {issue.message}")
                readme.append("")
                readme.append("</details>")
            
            readme.append("")
        except Exception as e:
            pass  # Skip validation section if it fails
    
    # Footer
    readme.append("---")
    readme.append("")
    readme.append(f"_Documentation auto-generated from `SKILL.md`_")
    
    return '\n'.join(readme)


def save_readme(skill_path: str, readme_content: str, output_path: str = None) -> str:
    """
    Save README to file.
    
    Args:
        skill_path: Path to skill directory
        readme_content: README content
        output_path: Optional custom output path
        
    Returns:
        Path where README was saved
    """
    skill_path = Path(skill_path)
    
    if skill_path.is_file():
        skill_path = skill_path.parent
    
    if output_path:
        readme_path = Path(output_path)
    else:
        readme_path = skill_path / 'README.md'
    
    readme_path.write_text(readme_content, encoding='utf-8')
    return str(readme_path)


if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("Usage: python generate_readme.py <skill_directory> [output_path]")
        print("\nExamples:")
        print("  python generate_readme.py ./my-skill")
        print("  python generate_readme.py ./my-skill ./docs/MY_SKILL.md")
        sys.exit(1)
    
    skill_path = sys.argv[1]
    output_path = sys.argv[2] if len(sys.argv) > 2 else None
    
    try:
        print(f"Analyzing skill at: {skill_path}")
        analysis = analyze_skill(skill_path)
        
        print(f"Generating README for: {analysis['name']}")
        readme = generate_readme(analysis, include_validation=True)
        
        saved_path = save_readme(skill_path, readme, output_path)
        
        print(f"✅ README generated successfully: {saved_path}")
        print(f"   {len(readme.split(chr(10)))} lines, {len(readme)} characters")
        
    except Exception as e:
        print(f"❌ Error: {e}", file=sys.stderr)
        sys.exit(1)


## Links discovered
- [`{resource}`](https://github.com/Exploration-labs/Nates-Substack-Skills/blob/main/skill-doc-generator/scripts/{base_path}/{resource}.md)

--- skill-doc-generator/scripts/validate_consistency.py ---
#!/usr/bin/env python3
"""
Validates skill consistency: frontmatter format, description quality, and terminology.
"""

import sys
from pathlib import Path
from typing import List, Dict
from analyze_skill import analyze_skill


class ValidationIssue:
    """Represents a validation issue."""
    
    SEVERITY_ERROR = 'ERROR'
    SEVERITY_WARNING = 'WARNING'
    SEVERITY_INFO = 'INFO'
    
    def __init__(self, severity: str, category: str, message: str):
        self.severity = severity
        self.category = category
        self.message = message
    
    def __str__(self):
        return f"[{self.severity}] {self.category}: {self.message}"


class SkillValidator:
    """Validates skill structure and content."""
    
    def __init__(self):
        self.issues: List[ValidationIssue] = []
    
    def validate(self, skill_path: str) -> List[ValidationIssue]:
        """Run all validation checks on a skill."""
        self.issues = []
        
        try:
            analysis = analyze_skill(skill_path)
        except Exception as e:
            self.issues.append(ValidationIssue(
                ValidationIssue.SEVERITY_ERROR,
                'Parse Error',
                f"Failed to analyze skill: {e}"
            ))
            return self.issues
        
        self._validate_frontmatter(analysis)
        self._validate_description(analysis)
        self._validate_structure(analysis)
        self._validate_terminology(analysis)
        self._validate_resources(analysis)
        self._validate_examples(analysis)
        
        return self.issues
    
    def _validate_frontmatter(self, analysis: Dict):
        """Check required frontmatter fields and format."""
        metadata = analysis['metadata']
        
        # Required fields
        if 'name' not in metadata:
            self.issues.append(ValidationIssue(
                ValidationIssue.SEVERITY_ERROR,
                'Frontmatter',
                "Missing required field: 'name'"
            ))
        elif not metadata['name']:
            self.issues.append(ValidationIssue(
                ValidationIssue.SEVERITY_ERROR,
                'Frontmatter',
                "'name' field is empty"
            ))
        
        if 'description' not in metadata:
            self.issues.append(ValidationIssue(
                ValidationIssue.SEVERITY_ERROR,
                'Frontmatter',
                "Missing required field: 'description'"
            ))
        elif not metadata['description']:
            self.issues.append(ValidationIssue(
                ValidationIssue.SEVERITY_ERROR,
                'Frontmatter',
                "'description' field is empty"
            ))
        
        # Name format (should be lowercase with hyphens)
        if 'name' in metadata and metadata['name']:
            name = metadata['name']
            if not name.islower() or ' ' in name:
                self.issues.append(ValidationIssue(
                    ValidationIssue.SEVERITY_WARNING,
                    'Frontmatter',
                    f"Name '{name}' should be lowercase with hyphens (e.g., 'skill-name')"
                ))
    
    def _validate_description(self, analysis: Dict):
        """Check description quality and completeness."""
        description = analysis.get('description', '')
        
        if not description:
            return  # Already caught in frontmatter check
        
        # Length checks
        if len(description) < 50:
            self.issues.append(ValidationIssue(
                ValidationIssue.SEVERITY_WARNING,
                'Description',
                f"Description is very short ({len(description)} chars). Should be comprehensive and specific."
            ))
        
        if len(description) > 500:
            self.issues.append(ValidationIssue(
                ValidationIssue.SEVERITY_INFO,
                'Description',
                f"Description is quite long ({len(description)} chars). Consider if all content is essential for skill selection."
            ))
        
        # Content quality checks
        if not description[0].isupper():
            self.issues.append(ValidationIssue(
                ValidationIssue.SEVERITY_WARNING,
                'Description',
                "Description should start with a capital letter"
            ))
        
        # Check for specificity
        vague_terms = ['various', 'multiple', 'different', 'some', 'general']
        for term in vague_terms:
            if term.lower() in description.lower():
                self.issues.append(ValidationIssue(
                    ValidationIssue.SEVERITY_INFO,
                    'Description',
                    f"Description contains vague term '{term}' - consider being more specific"
                ))
                break
        
        # Check for trigger phrases
        if 'when' not in description.lower() and 'use' not in description.lower():
            self.issues.append(ValidationIssue(
                ValidationIssue.SEVERITY_INFO,
                'Description',
                "Consider adding trigger phrases ('when', 'use this when') to help with skill selection"
            ))
    
    def _validate_structure(self, analysis: Dict):
        """Check overall document structure."""
        sections = analysis.get('sections', {})
        body_length = analysis.get('body_length', 0)
        line_count = analysis.get('line_count', 0)
        
        # Check for empty body
        if body_length < 100:
            self.issues.append(ValidationIssue(
                ValidationIssue.SEVERITY_WARNING,
                'Structure',
                f"SKILL.md body is very short ({body_length} chars)"
            ))
        
        # Check for excessive length (suggests need for references/)
        if line_count > 500:
            self.issues.append(ValidationIssue(
                ValidationIssue.SEVERITY_WARNING,
                'Structure',
                f"SKILL.md is quite long ({line_count} lines). Consider moving detailed content to references/"
            ))
        
        # Check for common expected sections
        section_names = [s.lower() for s in sections.keys()]
        
        has_overview = any('overview' in s or 'about' in s for s in section_names)
        has_workflow = any('workflow' in s or 'usage' in s or 'how' in s for s in section_names)
        
        if not has_overview:
            self.issues.append(ValidationIssue(
                ValidationIssue.SEVERITY_INFO,
                'Structure',
                "Consider adding an 'Overview' section to introduce the skill"
            ))
    
    def _validate_terminology(self, analysis: Dict):
        """Check for consistent terminology and style."""
        # Get all text content
        body = '\n'.join(analysis.get('sections', {}).values())
        
        # Check for inconsistent capitalization of common terms
        terms_to_check = {
            'claude': ['Claude', 'claude'],  # Should be 'Claude'
            'skill': ['Skill', 'skill'],  # Can vary by context
        }
        
        # Check for imperative/infinitive form (per guidelines)
        non_imperative_starts = [
            'you should', 'you can', 'you must', 'you will',
            'we should', 'we can', 'we must', 'we will'
        ]
        
        for phrase in non_imperative_starts:
            if phrase.lower() in body.lower():
                self.issues.append(ValidationIssue(
                    ValidationIssue.SEVERITY_INFO,
                    'Terminology',
                    f"Found '{phrase}' - consider using imperative form (e.g., 'Use' instead of 'You should use')"
                ))
                break
    
    def _validate_resources(self, analysis: Dict):
        """Check bundled resources are properly referenced."""
        resources = analysis.get('resources', {})
        sections = analysis.get('sections', {})
        body = '\n'.join(sections.values())
        
        # Check if scripts exist but aren't mentioned
        scripts = resources.get('scripts', [])
        if scripts:
            for script in scripts:
                script_name = Path(script).name
                if script_name not in body:
                    self.issues.append(ValidationIssue(
                        ValidationIssue.SEVERITY_WARNING,
                        'Resources',
                        f"Script '{script}' exists but isn't referenced in SKILL.md"
                    ))
        
        # Check if references exist but aren't mentioned
        references = resources.get('references', [])
        if references:
            for ref in references:
                ref_name = Path(ref).name
                if ref_name not in body:
                    self.issues.append(ValidationIssue(
                        ValidationIssue.SEVERITY_WARNING,
                        'Resources',
                        f"Reference file '{ref}' exists but isn't mentioned in SKILL.md"
                    ))
    
    def _validate_examples(self, analysis: Dict):
        """Check for presence and quality of code examples."""
        code_blocks = analysis.get('code_blocks', [])
        
        if not code_blocks:
            self.issues.append(ValidationIssue(
                ValidationIssue.SEVERITY_INFO,
                'Examples',
                "No code examples found. Consider adding examples if they would help clarify usage."
            ))
        
        # Check for language tags on code blocks
        for i, block in enumerate(code_blocks, 1):
            if block['language'] == 'text':
                self.issues.append(ValidationIssue(
                    ValidationIssue.SEVERITY_INFO,
                    'Examples',
                    f"Code block {i} has no language tag. Consider adding one for syntax highlighting."
                ))


def validate_skill(skill_path: str, verbose: bool = False) -> tuple[List[ValidationIssue], bool]:
    """
    Validate a skill and return issues.
    
    Returns:
        Tuple of (issues, has_errors)
    """
    validator = SkillValidator()
    issues = validator.validate(skill_path)
    
    has_errors = any(issue.severity == ValidationIssue.SEVERITY_ERROR for issue in issues)
    
    return issues, has_errors


if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("Usage: python validate_consistency.py <skill_directory> [--verbose]")
        sys.exit(1)
    
    skill_path = sys.argv[1]
    verbose = '--verbose' in sys.argv
    
    issues, has_errors = validate_skill(skill_path, verbose)
    
    if not issues:
        print(f"✅ Skill validation passed with no issues!")
        sys.exit(0)
    
    # Group by severity
    errors = [i for i in issues if i.severity == ValidationIssue.SEVERITY_ERROR]
    warnings = [i for i in issues if i.severity == ValidationIssue.SEVERITY_WARNING]
    info = [i for i in issues if i.severity == ValidationIssue.SEVERITY_INFO]
    
    print(f"Validation Results for: {skill_path}")
    print("=" * 60)
    
    if errors:
        print(f"\n❌ ERRORS ({len(errors)}):")
        for issue in errors:
            print(f"  {issue}")
    
    if warnings:
        print(f"\n⚠️  WARNINGS ({len(warnings)}):")
        for issue in warnings:
            print(f"  {issue}")
    
    if info and verbose:
        print(f"\nℹ️  INFO ({len(info)}):")
        for issue in info:
            print(f"  {issue}")
    
    print(f"\nSummary: {len(errors)} errors, {len(warnings)} warnings, {len(info)} info")
    
    sys.exit(1 if has_errors else 0)


--- skill-performance-profiler/SKILL.md ---
---
name: skill-performance-profiler
description: Analyzes skill usage patterns across conversations to track token consumption, identify heavy vs. lightweight skills, measure invocation frequency, detect co-occurrence patterns, and suggest consolidation opportunities. Use when the user asks to analyze skill performance, optimize skill usage, identify token-heavy skills, find consolidation opportunities, or review skill metrics.
---

# Skill Performance Profiler

Comprehensive analysis tool for tracking and optimizing skill usage patterns, token consumption, and identifying opportunities for skill consolidation.

## When to Use This Skill

Use this skill when users request:
- Analysis of skill token usage or performance metrics
- Identification of "heavy" vs "lightweight" skills
- Skill consolidation opportunities or optimization suggestions
- Frequency analysis of skill invocations
- Co-occurrence patterns (skills used together)
- Time-based trends in skill usage
- Reports or visualizations of skill metrics

Trigger phrases include: "analyze my skills," "which skills use the most tokens," "skill performance," "consolidation opportunities," "optimize my skills," "skill usage report," etc.

## Analysis Process

The analysis involves three main steps:

1. **Data Collection**: Gather conversation history using recent_chats tool
2. **Analysis**: Process conversations to extract skill metrics
3. **Reporting**: Generate formatted output (markdown report, CSV, or visualization)

### Step 1: Collect Conversation Data

Use the `recent_chats` tool to gather conversations for analysis. The time range and number of conversations depends on the user's request:

```python
# For recent analysis (default - last 20 conversations)
recent_chats(n=20)

# For specific time periods
recent_chats(n=20, after="2025-10-01T00:00:00Z")

# For comprehensive analysis (iterate to get more)
recent_chats(n=20, before=earliest_timestamp_from_previous_call)
```

Extract the conversation content and metadata (especially `updated_at` timestamps) from the results.

### Step 2: Prepare Data for Analysis

Create a JSON file containing the conversation data in this format:

```json
{
  "conversations": [
    {
      "content": "full conversation text including tool calls and responses",
      "updated_at": "2025-10-22T10:30:00Z"
    }
  ]
}
```

Save this as `/home/claude/conversations.json`.

### Step 3: Run Analysis

Execute the analysis script:

```bash
cd /home/claude
python3 /mnt/skills/user/skill-performance-profiler/scripts/analyze_skills.py conversations.json
```

This produces `conversations_analysis.json` with comprehensive metrics including:
- Per-skill statistics (invocation count, token usage, averages)
- Skill categorization (Lightweight/Medium/Heavy/Very Heavy)
- Co-occurrence patterns
- Summary statistics
- Consolidation opportunities

### Step 4: Generate Reports

Create formatted output using the report generator:

```bash
# Generate markdown report
python3 /mnt/skills/user/skill-performance-profiler/scripts/generate_report.py conversations_analysis.json markdown

# Generate CSV export
python3 /mnt/skills/user/skill-performance-profiler/scripts/generate_report.py conversations_analysis.json csv

# Generate both formats
python3 /mnt/skills/user/skill-performance-profiler/scripts/generate_report.py conversations_analysis.json both
```

This creates:
- `conversations_report.md`: Comprehensive markdown report with all metrics
- `conversations_export.csv`: Tabular data for spreadsheet analysis

### Step 5: Present Results

Present the analysis to the user in the most appropriate format:

1. **For quick summaries**: Extract key findings from the JSON and present inline
2. **For detailed analysis**: Move the markdown report to `/mnt/user-data/outputs/` and provide a link
3. **For data exploration**: Create a spreadsheet (xlsx) or provide the CSV
4. **For visualization**: Consider creating a React artifact with charts (using recharts library)

## Key Metrics Explained

**Invocation Count**: Number of times a skill was used across analyzed conversations

**Token Consumption**:
- Total Tokens: Cumulative tokens consumed by the skill across all invocations
- Average Tokens: Mean token usage per invocation
- Min/Max Tokens: Range showing variability in skill usage

**Skill Categories** (by average tokens):
- Lightweight: < 500 tokens
- Medium: 500-2,000 tokens  
- Heavy: 2,000-5,000 tokens
- Very Heavy: > 5,000 tokens

**Co-occurrence Rate**: Percentage of time skills are used together, indicating potential consolidation opportunities

**Consolidation Opportunities**: Skill pairs used together ≥50% of the time, suggesting they might benefit from being merged into a single skill

## Example Usage Patterns

**Quick Performance Check**:
```
User: "Which of my skills are using the most tokens?"
→ Collect 20 recent chats, analyze, show top 5 heaviest skills
```

**Comprehensive Audit**:
```
User: "Give me a full analysis of my skill usage over the last month"
→ Collect conversations from last month (multiple calls to recent_chats)
→ Run full analysis
→ Generate markdown report and provide download link
```

**Consolidation Analysis**:
```
User: "Are there skills I should consolidate?"
→ Analyze conversation patterns
→ Focus on consolidation_opportunities in results
→ Present recommendations with supporting data
```

**Trend Analysis**:
```
User: "Show me skill usage trends over time"
→ Collect conversations across time periods
→ Analyze and group by time buckets
→ Create visualization artifact with trend charts
```

## Token Estimation Notes

Token counts are estimated using a 4:1 character-to-token ratio. This is an approximation since:
- Actual tokenization varies by content
- Skills are loaded into context but may not consume their full size
- Multiple skills may be loaded but only portions used

For more accurate analysis, actual token counts from the API would be ideal, but this estimation provides useful relative comparisons for optimization decisions.

## Output Recommendations

**Choose output format based on user needs**:

- **Inline summary**: For quick questions about specific metrics
- **Markdown report**: For comprehensive analysis requiring narrative explanation
- **CSV export**: When user wants to do their own analysis in Excel/Sheets
- **Visualization artifact**: For trend analysis or comparative visualizations
- **Spreadsheet (xlsx)**: For detailed data exploration with built-in charts

Move all generated files to `/mnt/user-data/outputs/` and provide computer:// links so users can download them.


--- skill-performance-profiler/scripts/analyze_skills.py ---
#!/usr/bin/env python3
"""
Skill Performance Profiler - Analyzes skill usage patterns and token consumption
"""

import json
import re
from collections import defaultdict, Counter
from datetime import datetime
from typing import Dict, List, Tuple, Set
import sys


class SkillAnalyzer:
    def __init__(self):
        self.skill_invocations = defaultdict(list)
        self.skill_tokens = defaultdict(list)
        self.skill_cooccurrences = defaultdict(Counter)
        self.conversation_skills = defaultdict(set)
        
    def estimate_tokens(self, text: str) -> int:
        """Estimate token count using rough approximation (4 chars ≈ 1 token)"""
        if not text:
            return 0
        return len(text) // 4
    
    def extract_skill_mentions(self, content: str) -> Set[str]:
        """Extract skill names from conversation content"""
        skills = set()
        
        # Pattern 1: Direct mentions like "using the docx skill"
        pattern1 = r'(?:using|use|used|invoking|invoked|calling|called|reading|read)\s+(?:the\s+)?([a-z0-9-]+)\s+skill'
        matches1 = re.findall(pattern1, content.lower())
        skills.update(matches1)
        
        # Pattern 2: File paths like /mnt/skills/.../skill-name/SKILL.md
        pattern2 = r'/mnt/skills/(?:public|user|examples)/([a-z0-9-]+)/SKILL\.md'
        matches2 = re.findall(pattern2, content.lower())
        skills.update(matches2)
        
        # Pattern 3: Tool calls to file_read with skill paths
        pattern3 = r'file_read.*?/mnt/skills/[^/]+/([a-z0-9-]+)/'
        matches3 = re.findall(pattern3, content.lower())
        skills.update(matches3)
        
        return skills
    
    def parse_conversation(self, chat_content: str, updated_at: str = None):
        """Parse a single conversation and extract skill usage"""
        # Extract timestamp
        timestamp = updated_at or datetime.now().isoformat()
        
        # Estimate total tokens in conversation
        total_tokens = self.estimate_tokens(chat_content)
        
        # Extract skills used in this conversation
        skills = self.extract_skill_mentions(chat_content)
        
        # Record invocations and token usage
        for skill in skills:
            self.skill_invocations[skill].append({
                'timestamp': timestamp,
                'total_conversation_tokens': total_tokens
            })
            
            # Estimate tokens for this skill (rough approximation)
            # Assume skill content is mentioned multiple times, allocate proportionally
            skill_token_estimate = total_tokens // max(len(skills), 1)
            self.skill_tokens[skill].append(skill_token_estimate)
        
        # Track co-occurrence patterns
        skills_list = list(skills)
        for i, skill1 in enumerate(skills_list):
            self.conversation_skills[timestamp].add(skill1)
            for skill2 in skills_list[i+1:]:
                self.skill_cooccurrences[skill1][skill2] += 1
                self.skill_cooccurrences[skill2][skill1] += 1
    
    def calculate_metrics(self) -> Dict:
        """Calculate comprehensive performance metrics"""
        metrics = {
            'skills': {},
            'summary': {},
            'consolidation_opportunities': [],
            'trends': {}
        }
        
        # Per-skill metrics
        for skill, invocations in self.skill_invocations.items():
            tokens = self.skill_tokens[skill]
            
            metrics['skills'][skill] = {
                'invocation_count': len(invocations),
                'total_tokens': sum(tokens),
                'average_tokens': sum(tokens) / len(tokens) if tokens else 0,
                'min_tokens': min(tokens) if tokens else 0,
                'max_tokens': max(tokens) if tokens else 0,
                'category': self._categorize_skill(sum(tokens) / len(tokens) if tokens else 0),
                'first_used': min(inv['timestamp'] for inv in invocations),
                'last_used': max(inv['timestamp'] for inv in invocations),
                'cooccurs_with': dict(self.skill_cooccurrences[skill].most_common(5))
            }
        
        # Summary statistics
        if metrics['skills']:
            all_invocations = sum(s['invocation_count'] for s in metrics['skills'].values())
            all_tokens = sum(s['total_tokens'] for s in metrics['skills'].values())
            
            metrics['summary'] = {
                'total_skills_used': len(metrics['skills']),
                'total_invocations': all_invocations,
                'total_tokens_consumed': all_tokens,
                'average_tokens_per_invocation': all_tokens / all_invocations if all_invocations else 0,
                'most_used_skill': max(metrics['skills'].items(), 
                                      key=lambda x: x[1]['invocation_count'])[0],
                'heaviest_skill': max(metrics['skills'].items(),
                                     key=lambda x: x[1]['average_tokens'])[0],
                'lightest_skill': min(metrics['skills'].items(),
                                     key=lambda x: x[1]['average_tokens'])[0]
            }
        
        # Consolidation opportunities
        metrics['consolidation_opportunities'] = self._find_consolidation_opportunities()
        
        return metrics
    
    def _categorize_skill(self, avg_tokens: float) -> str:
        """Categorize skill by token weight"""
        if avg_tokens < 500:
            return "Lightweight"
        elif avg_tokens < 2000:
            return "Medium"
        elif avg_tokens < 5000:
            return "Heavy"
        else:
            return "Very Heavy"
    
    def _find_consolidation_opportunities(self) -> List[Dict]:
        """Identify skills that are frequently used together"""
        opportunities = []
        
        # Find skill pairs with high co-occurrence
        processed_pairs = set()
        for skill1, cooccurs in self.skill_cooccurrences.items():
            for skill2, count in cooccurs.most_common():
                pair = tuple(sorted([skill1, skill2]))
                if pair in processed_pairs or count < 2:
                    continue
                
                processed_pairs.add(pair)
                
                # Calculate co-occurrence rate
                skill1_total = len(self.skill_invocations[skill1])
                skill2_total = len(self.skill_invocations[skill2])
                cooccurrence_rate = count / min(skill1_total, skill2_total)
                
                if cooccurrence_rate >= 0.5:  # Used together in 50%+ of cases
                    opportunities.append({
                        'skills': list(pair),
                        'cooccurrence_count': count,
                        'cooccurrence_rate': round(cooccurrence_rate * 100, 1),
                        'recommendation': f"Consider consolidating {pair[0]} and {pair[1]} - used together {cooccurrence_rate*100:.0f}% of the time"
                    })
        
        return sorted(opportunities, key=lambda x: x['cooccurrence_rate'], reverse=True)


def main():
    """Main analysis function"""
    if len(sys.argv) < 2:
        print("Usage: python analyze_skills.py <conversations_json_file>")
        print("\nExpected JSON format:")
        print('''{
  "conversations": [
    {
      "content": "conversation text with skill mentions...",
      "updated_at": "2025-10-22T10:30:00Z"
    }
  ]
}''')
        sys.exit(1)
    
    input_file = sys.argv[1]
    
    # Load conversation data
    try:
        with open(input_file, 'r') as f:
            data = json.load(f)
    except Exception as e:
        print(f"Error loading file: {e}")
        sys.exit(1)
    
    # Initialize analyzer
    analyzer = SkillAnalyzer()
    
    # Process conversations
    conversations = data.get('conversations', [])
    print(f"Processing {len(conversations)} conversations...")
    
    for conv in conversations:
        content = conv.get('content', '')
        updated_at = conv.get('updated_at')
        analyzer.parse_conversation(content, updated_at)
    
    # Calculate metrics
    metrics = analyzer.calculate_metrics()
    
    # Output results
    output_file = input_file.replace('.json', '_analysis.json')
    with open(output_file, 'w') as f:
        json.dump(metrics, f, indent=2)
    
    print(f"\n✅ Analysis complete!")
    print(f"   Results saved to: {output_file}")
    print(f"\n📊 Summary:")
    print(f"   Skills analyzed: {metrics['summary'].get('total_skills_used', 0)}")
    print(f"   Total invocations: {metrics['summary'].get('total_invocations', 0)}")
    print(f"   Total tokens: {metrics['summary'].get('total_tokens_consumed', 0):,}")
    
    if metrics['consolidation_opportunities']:
        print(f"\n💡 Consolidation opportunities found: {len(metrics['consolidation_opportunities'])}")


if __name__ == '__main__':
    main()


--- skill-performance-profiler/scripts/generate_report.py ---
#!/usr/bin/env python3
"""
Generate formatted reports from skill analysis data
"""

import json
import sys
from datetime import datetime


def generate_markdown_report(metrics: dict) -> str:
    """Generate a comprehensive markdown report"""
    report = []
    
    # Header
    report.append("# Skill Performance Analysis Report")
    report.append(f"\n*Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")
    
    # Executive Summary
    summary = metrics.get('summary', {})
    report.append("## Executive Summary\n")
    report.append(f"- **Skills Analyzed**: {summary.get('total_skills_used', 0)}")
    report.append(f"- **Total Invocations**: {summary.get('total_invocations', 0)}")
    report.append(f"- **Total Token Consumption**: {summary.get('total_tokens_consumed', 0):,}")
    report.append(f"- **Average Tokens per Invocation**: {summary.get('average_tokens_per_invocation', 0):.0f}")
    report.append(f"- **Most Frequently Used**: `{summary.get('most_used_skill', 'N/A')}`")
    report.append(f"- **Heaviest (Avg Tokens)**: `{summary.get('heaviest_skill', 'N/A')}`")
    report.append(f"- **Lightest (Avg Tokens)**: `{summary.get('lightest_skill', 'N/A')}`\n")
    
    # Skills by Category
    skills = metrics.get('skills', {})
    categories = {'Lightweight': [], 'Medium': [], 'Heavy': [], 'Very Heavy': []}
    
    for skill, data in skills.items():
        categories[data['category']].append((skill, data))
    
    report.append("## Skills by Weight Category\n")
    for category in ['Very Heavy', 'Heavy', 'Medium', 'Lightweight']:
        if categories[category]:
            report.append(f"### {category} ({len(categories[category])} skills)\n")
            for skill, data in sorted(categories[category], 
                                     key=lambda x: x[1]['average_tokens'], 
                                     reverse=True):
                report.append(f"**`{skill}`**")
                report.append(f"- Invocations: {data['invocation_count']}")
                report.append(f"- Average tokens: {data['average_tokens']:.0f}")
                report.append(f"- Total tokens: {data['total_tokens']:,}")
                report.append(f"- Token range: {data['min_tokens']:.0f} - {data['max_tokens']:.0f}\n")
    
    # Most Used Skills
    report.append("## Top 10 Most Frequently Invoked Skills\n")
    top_skills = sorted(skills.items(), 
                       key=lambda x: x[1]['invocation_count'], 
                       reverse=True)[:10]
    
    for i, (skill, data) in enumerate(top_skills, 1):
        report.append(f"{i}. **`{skill}`** - {data['invocation_count']} invocations "
                     f"({data['total_tokens']:,} total tokens)")
    report.append("")
    
    # Token Efficiency Analysis
    report.append("## Token Efficiency Rankings\n")
    report.append("### Most Efficient (Lowest Avg Tokens)\n")
    efficient = sorted(skills.items(), 
                      key=lambda x: x[1]['average_tokens'])[:5]
    for i, (skill, data) in enumerate(efficient, 1):
        report.append(f"{i}. **`{skill}`** - {data['average_tokens']:.0f} avg tokens")
    
    report.append("\n### Least Efficient (Highest Avg Tokens)\n")
    inefficient = sorted(skills.items(), 
                        key=lambda x: x[1]['average_tokens'], 
                        reverse=True)[:5]
    for i, (skill, data) in enumerate(inefficient, 1):
        report.append(f"{i}. **`{skill}`** - {data['average_tokens']:.0f} avg tokens")
    report.append("")
    
    # Consolidation Opportunities
    opportunities = metrics.get('consolidation_opportunities', [])
    if opportunities:
        report.append("## Consolidation Opportunities\n")
        report.append("Skills frequently used together may benefit from consolidation:\n")
        for opp in opportunities:
            skills_str = " + ".join(f"`{s}`" for s in opp['skills'])
            report.append(f"- {skills_str}")
            report.append(f"  - Co-occurrence: {opp['cooccurrence_count']} times "
                         f"({opp['cooccurrence_rate']}% rate)")
            report.append(f"  - {opp['recommendation']}\n")
    else:
        report.append("## Consolidation Opportunities\n")
        report.append("No significant consolidation opportunities detected.\n")
    
    # Co-occurrence Patterns
    report.append("## Skill Co-occurrence Patterns\n")
    report.append("Skills most frequently used together in conversations:\n")
    
    # Get skills with co-occurrences
    skills_with_cooccur = [(s, d) for s, d in skills.items() 
                           if d.get('cooccurs_with')]
    skills_with_cooccur.sort(key=lambda x: sum(x[1]['cooccurs_with'].values()), 
                            reverse=True)
    
    for skill, data in skills_with_cooccur[:10]:
        if data.get('cooccurs_with'):
            report.append(f"**`{skill}`** frequently appears with:")
            for partner, count in list(data['cooccurs_with'].items())[:3]:
                report.append(f"  - `{partner}` ({count} times)")
            report.append("")
    
    return "\n".join(report)


def generate_csv_export(metrics: dict) -> str:
    """Generate CSV export of skill metrics"""
    lines = []
    lines.append("Skill,Invocations,Total Tokens,Average Tokens,Min Tokens,Max Tokens,Category,First Used,Last Used")
    
    for skill, data in sorted(metrics.get('skills', {}).items()):
        lines.append(f"{skill},{data['invocation_count']},{data['total_tokens']},"
                    f"{data['average_tokens']:.2f},{data['min_tokens']:.0f},"
                    f"{data['max_tokens']:.0f},{data['category']},"
                    f"{data['first_used']},{data['last_used']}")
    
    return "\n".join(lines)


def main():
    """Generate reports from analysis results"""
    if len(sys.argv) < 2:
        print("Usage: python generate_report.py <analysis_json_file> [format]")
        print("Formats: markdown (default), csv, both")
        sys.exit(1)
    
    input_file = sys.argv[1]
    format_type = sys.argv[2] if len(sys.argv) > 2 else "markdown"
    
    # Load analysis data
    try:
        with open(input_file, 'r') as f:
            metrics = json.load(f)
    except Exception as e:
        print(f"Error loading file: {e}")
        sys.exit(1)
    
    # Generate reports
    base_name = input_file.replace('_analysis.json', '').replace('.json', '')
    
    if format_type in ['markdown', 'both']:
        markdown = generate_markdown_report(metrics)
        md_file = f"{base_name}_report.md"
        with open(md_file, 'w') as f:
            f.write(markdown)
        print(f"✅ Markdown report: {md_file}")
    
    if format_type in ['csv', 'both']:
        csv = generate_csv_export(metrics)
        csv_file = f"{base_name}_export.csv"
        with open(csv_file, 'w') as f:
            f.write(csv)
        print(f"✅ CSV export: {csv_file}")


if __name__ == '__main__':
    main()


--- skill-testing-framework/SKILL.md ---
---
name: skill-testing-framework
description: Provides test cases and validation tools for skills. Use when creating tests for a new skill, adding regression tests after skill updates, running test suites to verify skill functionality, or validating that skill outputs match expected results. Supports unit tests, integration tests, and regression tests with input/output pair validation.
---

# Skill Testing Framework

## Overview

This skill provides a comprehensive framework for testing skills at multiple levels: unit tests for individual components, integration tests for complete workflows, and regression tests to catch breaking changes. It includes scripts for generating test templates, running test suites, and validating outputs against baselines.

## When to Use This Skill

Use this skill when:
- Building a new skill and want to add tests from the start
- Updating an existing skill and need to verify it still works correctly
- Running regression tests to ensure changes don't break existing functionality
- Validating that skill outputs match expected results
- Creating a test suite for a skill

## Quick Start

### 1. Generate a Test Template

Start by generating a test template for your skill:

```bash
scripts/generate_test_template.py /path/to/your-skill --output your-skill-tests.json
```

This analyzes your skill structure and creates a template with unit, integration, and regression test sections.

### 2. Customize the Test Cases

Edit the generated test file to add specific test cases. Use `assets/test_template.json` as a reference for test case structure.

### 3. Run Tests

Execute the test suite:

```bash
scripts/run_tests.py your-skill-tests.json --skill-path /path/to/your-skill
```

Add `--verbose` flag for detailed output.

## Test Types

### Unit Tests

Test individual components in isolation (scripts, functions, modules).

**Example:**
```json
{
  "name": "Test PDF rotation script",
  "type": "script",
  "script": "rotate_pdf.py",
  "args": ["input.pdf", "output.pdf", "90"],
  "expected_exit_code": 0,
  "description": "Verify PDF rotation executes without errors"
}
```

### Integration Tests

Test complete workflows from start to finish.

**Example:**
```json
{
  "name": "Test document creation workflow",
  "type": "workflow",
  "description": "End-to-end test of document generation",
  "input": {
    "user_query": "Create a report with sections and formatting",
    "files": []
  },
  "expected_output": {
    "type": "docx",
    "validation": "Contains title, sections, and proper formatting"
  }
}
```

### Regression Tests

Compare outputs against known baselines to catch unintended changes.

**Example:**
```json
{
  "name": "Regression: Output format consistency",
  "description": "Ensure output format hasn't changed",
  "input": {
    "user_query": "Process sample data",
    "files": ["sample.csv"]
  },
  "baseline_file": "baselines/sample_output_v1.txt",
  "validation_method": "exact_match"
}
```

## Creating Test Cases

### Input/Output Pair Format

Test cases use input/output pairs to define expected behavior:

**Input:**
- User query or prompt that triggers the skill
- Input files or data
- Configuration parameters

**Expected Output:**
- Exit code (for scripts)
- Output content or patterns
- File existence and properties
- Validation criteria

### Validation Methods

Choose the appropriate validation method:

1. **exact_match** - Output must match exactly (for deterministic outputs)
2. **contains** - Output must contain specific content (for error messages)
3. **pattern** - Output must match regex pattern (for formatted data with variable values)
4. **structural_match** - Structure must match (for documents with dynamic content)

## Managing Baselines

### Creating Baselines

For regression tests, create baseline files from known good outputs:

```bash
# Run skill and capture output
./skill-script.py input.txt > output.txt

# Create baseline from output
scripts/validate_test_results.py --create-baseline output.txt baselines/
```

### Validating Against Baselines

Compare current output with baseline:

```bash
scripts/validate_test_results.py actual.txt baseline.txt --mode exact
```

### Updating Baselines

When skill behavior intentionally changes:
1. Review the changes carefully
2. Verify the changes are correct and intended
3. Update the baseline file
4. Document why the baseline changed (update notes in test case)

**Important:** Don't automatically update baselines when tests fail. Investigate first to ensure it's not a regression.

## Test Organization

Organize test files and data using this structure:

```
tests/
├── your-skill-tests.json          # Main test suite
├── fixtures/                      # Test input files
│   ├── sample_input.pdf
│   ├── test_data.csv
│   └── edge_case_data.txt
├── baselines/                     # Expected outputs for regression tests
│   ├── baseline_v1.txt
│   └── baseline_v2.json
└── outputs/                       # Actual test outputs (gitignored)
    └── test_run_*.txt
```

## Workflow Decision Tree

**Am I building a new skill?**
→ Generate test template → Customize tests → Run tests

**Am I updating an existing skill?**
→ Run existing tests → Add new test cases for new functionality → Update baselines if needed

**Am I debugging a failing test?**
→ Run with --verbose → Compare outputs using validate_test_results.py → Fix issue or update test

**Do I want to add regression tests?**
→ Capture current output as baseline → Create regression test case → Document what the baseline represents

## Available Scripts

### generate_test_template.py

Creates test case templates based on skill structure.

```bash
# Basic usage
generate_test_template.py /path/to/skill

# Specify output file
generate_test_template.py /path/to/skill --output my-tests.json

# YAML format
generate_test_template.py /path/to/skill --format yaml
```

### run_tests.py

Executes test suites and reports results.

```bash
# Run all tests
run_tests.py test-suite.json

# Run with skill path
run_tests.py test-suite.json --skill-path /mnt/skills/public/pdf

# Verbose output
run_tests.py test-suite.json --verbose
```

**Output:**
- Passes: ✅ Test name
- Failures: ❌ Test name with diff
- Summary: Total, passed, failed, success rate

### validate_test_results.py

Validates outputs against expected results and manages baselines.

```bash
# Compare two files (exact match)
validate_test_results.py actual.txt expected.txt

# Check if output contains string
validate_test_results.py output.txt expected.txt --mode contains

# Pattern matching
validate_test_results.py output.txt pattern.txt --mode pattern

# Create baseline
validate_test_results.py --create-baseline output.txt baselines/
```

## Best Practices

1. **Start with happy path tests** - Verify basic functionality first
2. **Add edge case tests** - Test boundary conditions and unusual inputs
3. **Test error handling** - Verify graceful failures with clear messages
4. **Use descriptive test names** - Names should explain what's being tested
5. **Document baselines** - Note what each baseline represents and when it was created
6. **Keep tests independent** - Tests shouldn't depend on each other
7. **Make tests maintainable** - Use fixtures and organize test data
8. **Review failures carefully** - Don't blindly update baselines

## Reference Documentation

For detailed guidance, see:

**references/test_patterns.md** - Examples of test cases for different skill types:
- Script-based skills (PDF manipulation, image processing)
- Document creation skills (DOCX, PPTX, XLSX)
- API integration skills
- Workflow-based skills
- Reference documentation skills

**references/writing_tests.md** - Best practices for effective testing:
- Test design principles
- Coverage strategy
- Test data management
- Writing test cases
- Debugging failed tests

## Example Test Suite

See `assets/test_template.json` for a complete example test suite with:
- Unit test examples
- Integration test examples
- Regression test examples
- Validation method examples
- Instructions and documentation


--- skill-testing-framework/scripts/generate_test_template.py ---
#!/usr/bin/env python3
"""
Test Template Generator

Creates test case templates for skills based on the skill structure.

Usage:
    generate_test_template.py <skill-path> [--output <output-file>] [--format json|yaml]

Examples:
    generate_test_template.py /mnt/skills/public/pdf
    generate_test_template.py /mnt/skills/public/docx --output docx-tests.json
    generate_test_template.py ../my-skill --format yaml
"""

import sys
import json
import yaml
from pathlib import Path
from typing import Dict, List, Any


def analyze_skill_structure(skill_path: Path) -> Dict[str, Any]:
    """Analyze a skill's structure to determine what tests to generate"""
    
    skill_md = skill_path / 'SKILL.md'
    scripts_dir = skill_path / 'scripts'
    
    structure = {
        'has_skill_md': skill_md.exists(),
        'has_scripts': scripts_dir.exists() and any(scripts_dir.iterdir()),
        'scripts': [],
        'skill_name': skill_path.name
    }
    
    # Find scripts
    if structure['has_scripts']:
        for script_file in scripts_dir.glob('*.py'):
            if script_file.name != '__pycache__':
                structure['scripts'].append(script_file.name)
    
    return structure


def generate_unit_tests(structure: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Generate unit test templates based on skill structure"""
    tests = []
    
    # Generate tests for each script
    for script in structure['scripts']:
        test = {
            "name": f"Test {script} executes successfully",
            "type": "script",
            "script": script,
            "args": [],
            "expected_exit_code": 0,
            "expected_output": None,
            "description": f"Verify that {script} runs without errors"
        }
        tests.append(test)
    
    return tests


def generate_integration_tests(structure: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Generate integration test templates"""
    skill_name = structure['skill_name']
    
    tests = [
        {
            "name": f"Test {skill_name} complete workflow",
            "type": "workflow",
            "description": f"End-to-end test of typical {skill_name} usage",
            "steps": [
                {
                    "action": "Setup test environment",
                    "details": "Create necessary test files and directories"
                },
                {
                    "action": "Execute workflow",
                    "details": "Run the main skill operation"
                },
                {
                    "action": "Validate output",
                    "details": "Check that output meets expectations"
                }
            ],
            "input": {
                "user_query": "Example user query that triggers this skill",
                "files": []
            },
            "expected_output": {
                "type": "Describe expected output type (file, text, etc.)",
                "validation": "Describe how to validate the output"
            }
        }
    ]
    
    return tests


def generate_regression_tests(structure: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Generate regression test templates"""
    skill_name = structure['skill_name']
    
    tests = [
        {
            "name": f"Regression: {skill_name} baseline behavior",
            "description": "Ensure skill behavior hasn't changed from established baseline",
            "input": {
                "user_query": "Example query with known expected behavior",
                "files": []
            },
            "baseline_file": f"baselines/{skill_name}_baseline_1.txt",
            "validation_method": "exact_match",  # or "contains", "pattern"
            "notes": "Update baseline_file path with actual baseline output location"
        }
    ]
    
    return tests


def generate_test_template(skill_path: Path, output_format: str = 'json') -> Dict[str, Any]:
    """Generate a complete test template for a skill"""
    
    structure = analyze_skill_structure(skill_path)
    
    template = {
        "skill_name": structure['skill_name'],
        "skill_path": str(skill_path),
        "test_version": "1.0",
        "description": f"Test cases for {structure['skill_name']} skill",
        
        "unit_tests": generate_unit_tests(structure),
        "integration_tests": generate_integration_tests(structure),
        "regression_tests": generate_regression_tests(structure),
        
        "_instructions": {
            "unit_tests": "Test individual components (scripts, functions) in isolation",
            "integration_tests": "Test complete workflows and interactions between components",
            "regression_tests": "Compare outputs against known baselines to catch regressions",
            "howto": "Fill in test details, then run with: run_tests.py <this-file>"
        }
    }
    
    return template


def save_template(template: Dict[str, Any], output_file: Path, output_format: str):
    """Save the template to a file"""
    
    with open(output_file, 'w') as f:
        if output_format == 'json':
            json.dump(template, f, indent=2)
        elif output_format == 'yaml':
            yaml.dump(template, f, default_flow_style=False, sort_keys=False)
    
    print(f"✅ Test template generated: {output_file}")
    print(f"\n📝 Next steps:")
    print(f"   1. Edit {output_file} to fill in test details")
    print(f"   2. Add expected outputs and validation criteria")
    print(f"   3. Create baseline files for regression tests if needed")
    print(f"   4. Run tests with: run_tests.py {output_file}")


def main():
    if len(sys.argv) < 2:
        print("Usage: generate_test_template.py <skill-path> [--output <output-file>] [--format json|yaml]")
        print("\nExamples:")
        print("  generate_test_template.py /mnt/skills/public/pdf")
        print("  generate_test_template.py /mnt/skills/public/docx --output docx-tests.json")
        print("  generate_test_template.py ../my-skill --format yaml")
        sys.exit(1)
    
    skill_path = Path(sys.argv[1])
    output_file = None
    output_format = 'json'
    
    # Parse arguments
    i = 2
    while i < len(sys.argv):
        if sys.argv[i] == '--output' and i + 1 < len(sys.argv):
            output_file = Path(sys.argv[i + 1])
            i += 2
        elif sys.argv[i] == '--format' and i + 1 < len(sys.argv):
            output_format = sys.argv[i + 1].lower()
            if output_format not in ['json', 'yaml']:
                print(f"❌ Error: Invalid format '{output_format}'. Use 'json' or 'yaml'")
                sys.exit(1)
            i += 2
        else:
            print(f"Unknown argument: {sys.argv[i]}")
            sys.exit(1)
    
    if not skill_path.exists():
        print(f"❌ Error: Skill path not found: {skill_path}")
        sys.exit(1)
    
    if not output_file:
        # Generate default output filename
        skill_name = skill_path.name
        extension = 'json' if output_format == 'json' else 'yaml'
        output_file = Path(f"{skill_name}-tests.{extension}")
    
    print(f"🔍 Analyzing skill: {skill_path.name}")
    print(f"📄 Output format: {output_format}")
    print()
    
    template = generate_test_template(skill_path, output_format)
    save_template(template, output_file, output_format)


if __name__ == "__main__":
    main()


--- skill-testing-framework/scripts/run_tests.py ---
#!/usr/bin/env python3
"""
Test Runner for Skills

Executes test cases defined in JSON/YAML format and validates outputs.

Usage:
    run_tests.py <test-file> [--skill-path <path>] [--verbose]

Examples:
    run_tests.py tests/pdf-skill-tests.json
    run_tests.py tests/docx-tests.json --skill-path /mnt/skills/public/docx --verbose
"""

import sys
import json
import yaml
from pathlib import Path
from typing import Dict, List, Any, Optional
import subprocess
import tempfile
import shutil


class TestResult:
    """Represents the result of a single test case"""
    def __init__(self, test_name: str, passed: bool, message: str = "", actual: Any = None, expected: Any = None):
        self.test_name = test_name
        self.passed = passed
        self.message = message
        self.actual = actual
        self.expected = expected


class SkillTestRunner:
    """Runs test cases against a skill"""
    
    def __init__(self, test_file: Path, skill_path: Optional[Path] = None, verbose: bool = False):
        self.test_file = test_file
        self.skill_path = skill_path
        self.verbose = verbose
        self.results: List[TestResult] = []
        
    def load_tests(self) -> Dict[str, Any]:
        """Load test cases from JSON or YAML file"""
        with open(self.test_file, 'r') as f:
            if self.test_file.suffix == '.json':
                return json.load(f)
            elif self.test_file.suffix in ['.yaml', '.yml']:
                return yaml.safe_load(f)
            else:
                raise ValueError(f"Unsupported test file format: {self.test_file.suffix}")
    
    def run_unit_test(self, test: Dict[str, Any]) -> TestResult:
        """Run a unit test (typically testing a single script or function)"""
        test_name = test.get('name', 'Unnamed test')
        test_type = test.get('type', 'script')
        
        if test_type == 'script':
            return self._run_script_test(test)
        elif test_type == 'function':
            return self._run_function_test(test)
        else:
            return TestResult(test_name, False, f"Unknown unit test type: {test_type}")
    
    def run_integration_test(self, test: Dict[str, Any]) -> TestResult:
        """Run an integration test (testing a complete workflow)"""
        test_name = test.get('name', 'Unnamed test')
        
        # Integration tests simulate a complete user interaction
        # This would typically involve:
        # 1. Setting up test environment
        # 2. Running the skill workflow
        # 3. Validating outputs
        
        return TestResult(test_name, True, "Integration test framework placeholder")
    
    def run_regression_test(self, test: Dict[str, Any]) -> TestResult:
        """Run a regression test (comparing against known good outputs)"""
        test_name = test.get('name', 'Unnamed test')
        
        # Regression tests compare current output with baseline
        input_data = test.get('input', {})
        expected_output = test.get('expected_output', None)
        baseline_file = test.get('baseline_file', None)
        
        if baseline_file:
            # Load baseline and compare
            baseline_path = self.test_file.parent / baseline_file
            if baseline_path.exists():
                with open(baseline_path, 'r') as f:
                    expected_output = f.read()
        
        # Run test and compare
        actual_output = self._execute_test(input_data)
        
        if self._compare_outputs(actual_output, expected_output):
            return TestResult(test_name, True, "Output matches expected")
        else:
            return TestResult(test_name, False, "Output differs from expected", 
                            actual=actual_output, expected=expected_output)
    
    def _run_script_test(self, test: Dict[str, Any]) -> TestResult:
        """Execute a script and validate its output"""
        test_name = test.get('name', 'Unnamed test')
        script_path = test.get('script', None)
        args = test.get('args', [])
        expected_exit_code = test.get('expected_exit_code', 0)
        expected_output = test.get('expected_output', None)
        
        if not script_path:
            return TestResult(test_name, False, "No script path specified")
        
        # Construct full script path
        if self.skill_path:
            full_script_path = self.skill_path / 'scripts' / script_path
        else:
            full_script_path = Path(script_path)
        
        if not full_script_path.exists():
            return TestResult(test_name, False, f"Script not found: {full_script_path}")
        
        # Execute script
        try:
            result = subprocess.run(
                [str(full_script_path)] + args,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            # Check exit code
            if result.returncode != expected_exit_code:
                return TestResult(test_name, False, 
                                f"Exit code mismatch: expected {expected_exit_code}, got {result.returncode}")
            
            # Check output if specified
            if expected_output is not None:
                actual_output = result.stdout.strip()
                if not self._compare_outputs(actual_output, expected_output):
                    return TestResult(test_name, False, "Output mismatch",
                                    actual=actual_output, expected=expected_output)
            
            return TestResult(test_name, True, "Script executed successfully")
            
        except subprocess.TimeoutExpired:
            return TestResult(test_name, False, "Script execution timed out")
        except Exception as e:
            return TestResult(test_name, False, f"Script execution failed: {str(e)}")
    
    def _run_function_test(self, test: Dict[str, Any]) -> TestResult:
        """Test a specific function within a module"""
        test_name = test.get('name', 'Unnamed test')
        # Placeholder for function testing
        return TestResult(test_name, True, "Function test placeholder")
    
    def _execute_test(self, input_data: Dict[str, Any]) -> Any:
        """Execute a test with given input data (placeholder for actual execution)"""
        # This would interface with Claude or run scripts
        return None
    
    def _compare_outputs(self, actual: Any, expected: Any) -> bool:
        """Compare actual and expected outputs"""
        if expected is None:
            return True  # No expected output specified
        
        # Handle different comparison types
        if isinstance(expected, dict) and 'pattern' in expected:
            # Regex pattern matching
            import re
            pattern = expected['pattern']
            return re.search(pattern, str(actual)) is not None
        elif isinstance(expected, dict) and 'contains' in expected:
            # Substring matching
            return expected['contains'] in str(actual)
        else:
            # Exact match
            return str(actual).strip() == str(expected).strip()
    
    def run_all_tests(self):
        """Execute all tests in the test file"""
        test_data = self.load_tests()
        
        print(f"\n🧪 Running tests from: {self.test_file.name}")
        print(f"{'='*60}\n")
        
        # Run unit tests
        if 'unit_tests' in test_data:
            print("📦 Unit Tests")
            print("-" * 40)
            for test in test_data['unit_tests']:
                result = self.run_unit_test(test)
                self.results.append(result)
                self._print_result(result)
        
        # Run integration tests
        if 'integration_tests' in test_data:
            print("\n🔗 Integration Tests")
            print("-" * 40)
            for test in test_data['integration_tests']:
                result = self.run_integration_test(test)
                self.results.append(result)
                self._print_result(result)
        
        # Run regression tests
        if 'regression_tests' in test_data:
            print("\n🔄 Regression Tests")
            print("-" * 40)
            for test in test_data['regression_tests']:
                result = self.run_regression_test(test)
                self.results.append(result)
                self._print_result(result)
        
        # Print summary
        self._print_summary()
    
    def _print_result(self, result: TestResult):
        """Print a single test result"""
        status = "✅ PASS" if result.passed else "❌ FAIL"
        print(f"{status} | {result.test_name}")
        
        if not result.passed or self.verbose:
            if result.message:
                print(f"     └─ {result.message}")
            if result.actual is not None and result.expected is not None:
                print(f"     └─ Expected: {result.expected}")
                print(f"     └─ Actual:   {result.actual}")
    
    def _print_summary(self):
        """Print test execution summary"""
        total = len(self.results)
        passed = sum(1 for r in self.results if r.passed)
        failed = total - passed
        
        print(f"\n{'='*60}")
        print(f"📊 Test Summary")
        print(f"{'='*60}")
        print(f"Total:  {total}")
        print(f"Passed: {passed} ✅")
        print(f"Failed: {failed} {'❌' if failed > 0 else ''}")
        print(f"Success Rate: {(passed/total*100) if total > 0 else 0:.1f}%")
        print()


def main():
    if len(sys.argv) < 2:
        print("Usage: run_tests.py <test-file> [--skill-path <path>] [--verbose]")
        print("\nExamples:")
        print("  run_tests.py tests/pdf-skill-tests.json")
        print("  run_tests.py tests/docx-tests.json --skill-path /mnt/skills/public/docx")
        print("  run_tests.py tests/all-tests.yaml --verbose")
        sys.exit(1)
    
    test_file = Path(sys.argv[1])
    skill_path = None
    verbose = False
    
    # Parse arguments
    i = 2
    while i < len(sys.argv):
        if sys.argv[i] == '--skill-path' and i + 1 < len(sys.argv):
            skill_path = Path(sys.argv[i + 1])
            i += 2
        elif sys.argv[i] == '--verbose':
            verbose = True
            i += 1
        else:
            print(f"Unknown argument: {sys.argv[i]}")
            sys.exit(1)
    
    if not test_file.exists():
        print(f"❌ Error: Test file not found: {test_file}")
        sys.exit(1)
    
    runner = SkillTestRunner(test_file, skill_path, verbose)
    runner.run_all_tests()
    
    # Exit with error code if any tests failed
    failed_count = sum(1 for r in runner.results if not r.passed)
    sys.exit(0 if failed_count == 0 else 1)


if __name__ == "__main__":
    main()


--- skill-testing-framework/scripts/validate_test_results.py ---
#!/usr/bin/env python3
"""
Test Results Validator

Validates test outputs against expected results and manages baseline files.

Usage:
    validate_test_results.py <actual-file> <expected-file> [--mode exact|contains|pattern]
    validate_test_results.py --create-baseline <output-file> <baseline-dir>

Examples:
    validate_test_results.py output.txt expected.txt
    validate_test_results.py result.json baseline.json --mode exact
    validate_test_results.py output.txt expected.txt --mode contains
    validate_test_results.py --create-baseline test_output.txt baselines/
"""

import sys
import json
import re
from pathlib import Path
from typing import Tuple, Optional
import difflib


class OutputValidator:
    """Validates test outputs against expectations"""
    
    def __init__(self, mode: str = 'exact'):
        self.mode = mode
        
    def validate(self, actual: str, expected: str) -> Tuple[bool, str]:
        """
        Validate actual output against expected output
        
        Returns: (passed, message)
        """
        if self.mode == 'exact':
            return self._validate_exact(actual, expected)
        elif self.mode == 'contains':
            return self._validate_contains(actual, expected)
        elif self.mode == 'pattern':
            return self._validate_pattern(actual, expected)
        else:
            return False, f"Unknown validation mode: {self.mode}"
    
    def _validate_exact(self, actual: str, expected: str) -> Tuple[bool, str]:
        """Exact string match"""
        actual_clean = actual.strip()
        expected_clean = expected.strip()
        
        if actual_clean == expected_clean:
            return True, "Exact match"
        else:
            # Generate diff for debugging
            diff = self._generate_diff(expected_clean, actual_clean)
            return False, f"Content mismatch:\n{diff}"
    
    def _validate_contains(self, actual: str, expected: str) -> Tuple[bool, str]:
        """Check if actual contains expected substring"""
        if expected in actual:
            return True, "Contains expected content"
        else:
            return False, f"Expected substring not found: '{expected}'"
    
    def _validate_pattern(self, actual: str, expected: str) -> Tuple[bool, str]:
        """Match against regex pattern"""
        try:
            if re.search(expected, actual, re.MULTILINE | re.DOTALL):
                return True, "Pattern matched"
            else:
                return False, f"Pattern not matched: {expected}"
        except re.error as e:
            return False, f"Invalid regex pattern: {e}"
    
    def _generate_diff(self, expected: str, actual: str) -> str:
        """Generate a unified diff between expected and actual"""
        expected_lines = expected.splitlines(keepends=True)
        actual_lines = actual.splitlines(keepends=True)
        
        diff = difflib.unified_diff(
            expected_lines,
            actual_lines,
            fromfile='expected',
            tofile='actual',
            lineterm=''
        )
        
        diff_text = ''.join(diff)
        if len(diff_text) > 500:
            diff_text = diff_text[:500] + "\n... (truncated)"
        
        return diff_text


def create_baseline(output_file: Path, baseline_dir: Path):
    """Create a baseline file from test output"""
    baseline_dir.mkdir(parents=True, exist_ok=True)
    
    baseline_file = baseline_dir / output_file.name
    
    if not output_file.exists():
        print(f"❌ Error: Output file not found: {output_file}")
        sys.exit(1)
    
    # Copy output to baseline
    import shutil
    shutil.copy2(output_file, baseline_file)
    
    print(f"✅ Baseline created: {baseline_file}")
    print(f"\n📝 This baseline can now be used in regression tests:")
    print(f'   "baseline_file": "{baseline_file}"')


def compare_files(actual_file: Path, expected_file: Path, mode: str = 'exact'):
    """Compare two files and report results"""
    
    if not actual_file.exists():
        print(f"❌ Error: Actual file not found: {actual_file}")
        sys.exit(1)
    
    if not expected_file.exists():
        print(f"❌ Error: Expected file not found: {expected_file}")
        sys.exit(1)
    
    # Read files
    with open(actual_file, 'r') as f:
        actual = f.read()
    
    with open(expected_file, 'r') as f:
        expected = f.read()
    
    # Validate
    validator = OutputValidator(mode)
    passed, message = validator.validate(actual, expected)
    
    # Print results
    print(f"\n📊 Validation Results")
    print(f"{'='*60}")
    print(f"Actual:   {actual_file}")
    print(f"Expected: {expected_file}")
    print(f"Mode:     {mode}")
    print(f"{'='*60}")
    
    if passed:
        print(f"✅ PASS: {message}")
        print()
        return 0
    else:
        print(f"❌ FAIL: {message}")
        print()
        return 1


def main():
    if len(sys.argv) < 2:
        print("Usage:")
        print("  validate_test_results.py <actual-file> <expected-file> [--mode exact|contains|pattern]")
        print("  validate_test_results.py --create-baseline <output-file> <baseline-dir>")
        print("\nExamples:")
        print("  validate_test_results.py output.txt expected.txt")
        print("  validate_test_results.py result.json baseline.json --mode exact")
        print("  validate_test_results.py output.txt expected.txt --mode contains")
        print("  validate_test_results.py --create-baseline test_output.txt baselines/")
        sys.exit(1)
    
    # Check for baseline creation mode
    if sys.argv[1] == '--create-baseline':
        if len(sys.argv) < 4:
            print("❌ Error: --create-baseline requires <output-file> <baseline-dir>")
            sys.exit(1)
        
        output_file = Path(sys.argv[2])
        baseline_dir = Path(sys.argv[3])
        create_baseline(output_file, baseline_dir)
        sys.exit(0)
    
    # Comparison mode
    actual_file = Path(sys.argv[1])
    expected_file = Path(sys.argv[2])
    mode = 'exact'
    
    # Parse additional arguments
    if len(sys.argv) >= 5 and sys.argv[3] == '--mode':
        mode = sys.argv[4]
        if mode not in ['exact', 'contains', 'pattern']:
            print(f"❌ Error: Invalid mode '{mode}'. Use 'exact', 'contains', or 'pattern'")
            sys.exit(1)
    
    exit_code = compare_files(actual_file, expected_file, mode)
    sys.exit(exit_code)


if __name__ == "__main__":
    main()
